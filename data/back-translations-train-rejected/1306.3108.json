{"id": "1306.3108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2013", "title": "Guaranteed Classification via Regularized Similarity Learning", "abstract": "Learning an appropriate (dis)similarity function from the available data is a central problem in machine learning, since the success of many machine learning algorithms critically depends on the choice of a similarity function to compare examples. Despite many approaches for similarity metric learning have been proposed, there is little theoretical study on the links between similarity metric learning and the classification performance of the result classifier. In this paper, we propose a regularized similarity learning formulation associated with general matrix-norms, and establish their generalization bounds. We show that the generalization error of the resulting linear separator can be bounded by the derived generalization bound of similarity learning. This shows that a good generalization of the learnt similarity function guarantees a good classification of the resulting linear classifier. Our results extend and improve those obtained by Bellet at al. \\cite{Bellet}. Due to the techniques dependent on the notion of uniform stability \\cite{Bous}, the bound obtained there holds true only for the Frobenius matrix-norm regularization, which has a strong dependence on the dimensionality of the input space. Our techniques using the Rademacher complexity \\cite{BM} and its related Khinchin-type inequality, in the cases of sparse $L^1$-norm and mixed $(2,1)$-norm regularization, enables us to obtain bounds that have a mild dependence on the input dimensionality.", "histories": [["v1", "Thu, 13 Jun 2013 13:47:51 GMT  (16kb)", "http://arxiv.org/abs/1306.3108v1", null], ["v2", "Thu, 29 Aug 2013 15:38:27 GMT  (18kb)", "http://arxiv.org/abs/1306.3108v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zheng-chu guo", "yiming ying"], "accepted": false, "id": "1306.3108"}, "pdf": {"name": "1306.3108.pdf", "metadata": {"source": "CRF", "title": "Guaranteed Classification via Regularized Similarity Learning", "authors": ["Zheng-Chu Guo"], "emails": ["gzhengchu@gmail.com", "mathying@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 6,31 08v1 [cs.L"}, {"heading": "1 Introduction", "text": "The success of many machine learning algorithms depends heavily on how the similarity or distance metric between the examples is defined. For example, the k-next neighbor x x x x classifier depends on a distance (dissimility) function to identify the closest neighbors for classification. However, most information retrieval methods rely on a similarity function to identify the data points that are most similar to a given query. Kernel methods rely on the core function to represent the similarity between examples. Therefore, how to learn an appropriate (dis) similarity function from the available reading data is a central problem in machine learning that we call similarity metric learning throughout the paper.Recently, a considerable amount of research has been devoted to similarity metric learning and many approaches, which can be briefly divided into three main approaches. The first approach is called metric learning [4, 11, 12, 31, which focuses on dialanolar learning]."}, {"heading": "2 Regularization Formulation and Main Results", "text": "In this section, we mainly present the regularized formulation of similarity and present our most important results. (...) Before doing so, we present some notations and some background information. (...) It is not that we apply a series of training programs ranging from a distribution to Z = X \u00b7 Y. Entrance room X is a domain in Rd and Y = (...) is referred to as an output room. (...) We consider KA (...) as a bilinear similarity parameterized by a symmetric matrix parametrix. (...) The symmetry of matrix A guarantees the symmetry of similarity Score KA, i.eKA (...)"}, {"heading": "3 Related Work", "text": "In this section, we discuss studies on similarity of metric learning related to our work. Many similarity metric learning methods were motivated by the intuition that the similarity value between examples in the same class should be greater than that of examples from different classes, see, for example, [4, 7, 12, 14, 18, 26, 29]. Jin et al. [14] established generalization limits for metric and similarity learning associated with the concept of uniform stability [5], which only works for strongly convex matrix regulation terms. A very recent paper [7] established generalization limits for metric and similarity learning associated with the general matrix-norm regulation [5], using techniques of Whedemacher averages and U statistics. However, there were no theoretical links between the similarity metric learning method and the generalization performance of classifiers based on learned similarity."}, {"heading": "4 Generalization Bounds for Similarity Learning", "text": "In this section, we establish generalization limits for the similarity formula (2) with general matrix-norm regularization terms. Remember that the goal of generalization analysis for similarity learning is to tie the true error E (az) with the empirical error Ez (az). (2) According to the definition (2) of Az, we know that Ez (az) + empirical error E (az) + empirical error E (az). (2) We know that Ez (az) + empirical error Ez (az), that Ez (az) + empirical error Ez (z), empirical error E (az). (2) empiriririrical error Ez (az), empirical error Ez (az)."}, {"heading": "5 Guaranteed Classification Via Good Similarity", "text": "In this section, we will examine the theoretical relationship between the generalization error of similarity and the generalization error of the linear classifier calculated from the learned similarity. \u2212 In particular, we will show that the generalization error of the linear SVM algorithm (6) was an upper limit for the generalization of the linear SVM algorithm (6) before we provide proof for Theorem 2 that we will first set the generalization limits for the linear SVM algorithm (6). \u2212 Recalling that the linear SVM algorithm (6) was defined as independent. \u2212 Before we provide proof for Theorem 2, we will set the generalization limits for the linear SVM algorithm (6). \u2212 Recalling that the linear SVM algorithm (6) was defined. \u2212 Taking into account that the linear SVM algorithm (6) was defined."}, {"heading": "6 Estimating Rademacher Averages", "text": "The most important theorems about this depend crucially on the estimate of the Rademacher average. In this section we determine its estimate and confirm the examples listed in section 2. (1) This results in the fact that it is the highest estimate of the i-th sample. (1) It results in the fact that the double standard of the L1 standard is the L-standard. (2) This results in the fact that the Rademacher average can be rewritten. (2) This results in. (2) From this results the x-standard. (2) From this results the x-standard. (2) From this result. (2) From this result. (2) From this result. (2) From this result. Bowel of the x-standard. (2) Bowel of the x-standard. (2) From the x-standard. (x) From the x-standard. (2) Bowel of the x-standard. (2)"}, {"heading": "7 Conclusion", "text": "In this paper, we looked at a regulated similarity learning formula (2), whose generalization limits were set for various matrix-norm regularization terms such as the Frobenius norm, the sparse L1 norm, and the mixed (2, 1) norm. We proved that the generalization error of the linear separator based on the learned similarity function can be limited by the derived generalization limit of similarity learning. This guarantees the quality of the generalization of similarity learning (2) with the general matrix-norm regularization and thus the classification of the resulting linear classification. Our techniques, which use the Rademacher complexity [5] and the important Khinchin type inequality for the Rademacher variables, enable us to set new limits for similarity learning, which show a slight dependence on the input dimensionality."}], "references": [{"title": "A theory of learning with similarity functions", "author": ["M.-F. Balcan", "A. Blum"], "venue": "COLT,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved gaurantees for learning via similarity functions", "author": ["M.-F. Balcan", "A. Blum", "N. Srebro"], "venue": "COLT,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Similarity learning for provably accurate sparse linear classification", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Rademacher and Gaussian complexities: risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "J. of Machine Learning Research, 3: 463\u2013482,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Stability and generalization", "author": ["O. Bousequet", "A. Elisseeff"], "venue": "J. of Machine Learning Research, 2: 499\u2013526,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Generalization bounds for metric and similarity learning", "author": ["Q. Cao", "Z.-C. Guo", "Y. Ying"], "venue": "Preprint,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Similaritybased classification: concepts and algorithms", "author": ["Y. Chen", "E.K. Garcia", "M.R. Gupta", "A. Rahimi", "L. Cazzanti"], "venue": "J. of Machine Learning Research, 10:747-776,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "J. of Machine Learning Research, 11: 1109-1135,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Is that you? Metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning distance metrics with contextual constraints for image retrieval", "author": ["S.C.H. Hoi", "W. Liu", "M.R. Lyu", "W.-Y. Ma"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2072\u20132078,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Supervised learning with similarity functions", "author": ["P. Jain", "P. Kar"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized distance metric learning: theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Similarity-based learning via data-driven embeddings", "author": ["P. Kar", "P. Jain"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "V. Panchenko"], "venue": "The Annals of Statistics, 30, 1\u20135,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Springer Press, New York,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning similarity with operator-valued large-margin classifiers", "author": ["A. Maurer"], "venue": "J. of Machine Learning Research, 9: 1049\u20131082,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Surveys in Combinatorics, Chapter On the methods of bounded differences, 148\u2013188", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Learning with non-positive kernels", "author": ["C.S. Ong", "X. Mary", "S. Canu", "A.J. Smola"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Decoupling: from Dependence to Independence", "author": ["V.H. De La Pe\u00f1a", "E. Gin\u00e9"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Protein homology detection using string alignment kernels", "author": ["H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu"], "venue": "Bioinformatics, 20: 1682\u20131689.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Regularization with dot-product kernels", "author": ["A.J. Smola", "Z.L. \u00d3v\u0301ari", "R.C. Williamson"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley, New York,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "On learning with dissimilarity functions", "author": ["L. Wang", "C. Yang", "J. Feng"], "venue": "ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbour classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "SVM soft margin classifiers: linear programming versus quadratic programming", "author": ["Q. Wu", "D.X. Zhou"], "venue": "Neural computation, 17: 1160\u20131187,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization networks with indefinite kernels", "author": ["Q. Wu"], "venue": "Journal of Approximation Theory, 166: 1\u201318,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning with application to clustering with side information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "NIPS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis of SVM with indefinite kernels", "author": ["Y. Ying", "M. Girolami", "C. Campbell"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrixnorm regularization, which has a strong dependence on the dimensionality of the input space.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality, in the cases of sparse L-norm and mixed (2, 1)-norm regularization, enables us to obtain bounds that have a mild dependence on the input dimensionality.", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 10, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 12, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 24, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 27, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 29, "context": "The first approach is referred to as metric learning [4, 11, 12, 14, 26, 29, 31] which often focuses on learning a Mahalanobis distance metric defined, for any x, x\u2032 \u2208 Rd, by dM (x, x\u2032) = \u221a (x\u2212 x\u2032)TM(x\u2212 x\u2032).", "startOffset": 53, "endOffset": 80}, {"referenceID": 7, "context": "This approach has been successfully applied to image searching [9] and object recognition [18].", "startOffset": 63, "endOffset": 66}, {"referenceID": 16, "context": "This approach has been successfully applied to image searching [9] and object recognition [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.", "startOffset": 87, "endOffset": 98}, {"referenceID": 12, "context": "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.", "startOffset": 87, "endOffset": 98}, {"referenceID": 16, "context": "The generalization bounds were recently established for metric and similarity learning [7, 14, 18] under different statistical assumptions on the data.", "startOffset": 87, "endOffset": 98}, {"referenceID": 12, "context": "In other words, it is not clear whether good generalization bounds for metric and similarity learning [14, 7] can lead to a good classification performance of the resultant k-NN classifiers.", "startOffset": 102, "endOffset": 109}, {"referenceID": 5, "context": "In other words, it is not clear whether good generalization bounds for metric and similarity learning [14, 7] can lead to a good classification performance of the resultant k-NN classifiers.", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "Such cases are quite common in applications such as hyperbolic tangent kernels [23], and the protein sequence similarity measures derived from SmithWaterman and BLAST score [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Such cases are quite common in applications such as hyperbolic tangent kernels [23], and the protein sequence similarity measures derived from SmithWaterman and BLAST score [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "Some methods [8, 30] learn a PSD kernel matrix from a prescribed indefinite kernel matrix, which are mostly restricted to the transductive settings.", "startOffset": 13, "endOffset": 20}, {"referenceID": 28, "context": "Some methods [8, 30] learn a PSD kernel matrix from a prescribed indefinite kernel matrix, which are mostly restricted to the transductive settings.", "startOffset": 13, "endOffset": 20}, {"referenceID": 25, "context": "Recent methods [27, 28] analyzed regularization networks such as ridge regression and SVM given a prescribed indefinite kernel, instead of aiming to learn an indefinite kernel function from data.", "startOffset": 15, "endOffset": 23}, {"referenceID": 26, "context": "Recent methods [27, 28] analyzed regularization networks such as ridge regression and SVM given a prescribed indefinite kernel, instead of aiming to learn an indefinite kernel function from data.", "startOffset": 15, "endOffset": 23}, {"referenceID": 2, "context": "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (\u03b5, \u03b3, \u03c4)good similarity functions introduced in [1, 2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (\u03b5, \u03b3, \u03c4)good similarity functions introduced in [1, 2].", "startOffset": 139, "endOffset": 145}, {"referenceID": 1, "context": "[3] proposed a regularized similarity learning approach, which is mainly motivated by the (\u03b5, \u03b3, \u03c4)good similarity functions introduced in [1, 2].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "However, due to the techniques dependent on the notion of uniform stability [6], the generalization bounds only hold true for the Frobenius matrix-norm regularization which usually has a strong dependence on the dimensionality of the input space.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Our techniques using the Rademacher complexity [5] and the important Khinchin-type inequality for the Rademacher variables, in the cases of sparse L1-norm, and mixed (2, 1)-norm regularization, enables us to derive bounds that have a mild dependence on the input dimensionality.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "A natural approach to achieve the above aim [1, 3] is to minimize the following empirical error Ez(A) = 1 m \u2211", "startOffset": 44, "endOffset": 50}, {"referenceID": 2, "context": "A natural approach to achieve the above aim [1, 3] is to minimize the following empirical error Ez(A) = 1 m \u2211", "startOffset": 44, "endOffset": 50}, {"referenceID": 2, "context": "Its special case with the Frobenius matrix norm was established in [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "It used the uniform stability techniques [6], which, however, can not deal with non-strongly convex matrix-norms such as the L1-norm, (2, 1)-mixed norm and trace norm.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "Our new analysis techniques are able to deal with general matrix norms, which depend on the concept of Rademacher averages [5] defined as follows.", "startOffset": 123, "endOffset": 126}, {"referenceID": 22, "context": "We show that the generalization bound for the similarity learning gives an upper bound for the generalization error of a linear classifier produced by the linear Support Vector Machine (SVM) [24] defined as follows: fz = argmin { 1 m \u2211", "startOffset": 191, "endOffset": 195}, {"referenceID": 2, "context": "Secondly, the bounds in Example 2 is consistent with that in [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "For instance, if the input space X = [0, 1]d, then we have supx\u2208X \u2016x\u2016\u221e = 1 and supx\u2208X \u2016x\u2016F = \u221a d.", "startOffset": 37, "endOffset": 43}, {"referenceID": 5, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 7, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 12, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 16, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 24, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 27, "context": "[4, 7, 9, 12, 14, 18, 26, 29].", "startOffset": 0, "endOffset": 29}, {"referenceID": 12, "context": "[14] established generalization bounds for regularized metric learning algorithms via the concept of uniform stability [5], which, however, only works for strongly convex matrix regularization terms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[14] established generalization bounds for regularized metric learning algorithms via the concept of uniform stability [5], which, however, only works for strongly convex matrix regularization terms.", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "A very recent work [7] established generalization bounds for the metric and similarity learning associated with general matrix norm regularization using techniques of Rademacher averages and U-statistics.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "For instance, the following pairwise empirical objective function was considered in [9, 7]: 1 m(m\u2212 1) m \u2211", "startOffset": 84, "endOffset": 90}, {"referenceID": 5, "context": "For instance, the following pairwise empirical objective function was considered in [9, 7]: 1 m(m\u2212 1) m \u2211", "startOffset": 84, "endOffset": 90}, {"referenceID": 1, "context": "[2] developed a theory of (\u01eb, \u03b3, \u03c4)-good similarity function defined as follows.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "It was mentioned in [2] that the linear separator can be estimated by solving the following linear programming if we have du potentially unlabeled sample and dl labeled sample, min \u03b1 { dl \u2211", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "[2] in the following two aspects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Secondly, although the separators are both trained from the linear SVM, the classification algorithm (13) in [2] was designed using two different sets of examples, a set of labeled samples of size dl to train the classification algorithm and another set of unlabeled samples with size du to define the mapping \u03c6 S .", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "[3] is mostly close to ours.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Generalization bounds for similarity learning were derived via uniform stability arguments [6] which can not deal with, for instance, the L1-norm and (2, 1)-norm regularization terms.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "In addition, the results about the relationship between the similarity learning and the performance of the learnt matrix in classification were quoted from [2] and hence requires two separate sets of samples to train the classifier.", "startOffset": 156, "endOffset": 159}, {"referenceID": 11, "context": "[13] and Kar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "[15] introduced an extended framework of [1, 2] in the general setting of supervised learning.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "The authors proposed a general goodness criterion for similarity functions, which can handle general supervised learning tasks and also subsumes the goodness of condition of [2].", "startOffset": 174, "endOffset": 177}, {"referenceID": 17, "context": "Applying the McDiarmid\u2019s inequality [19] (see Lemma 1 in the Appendix) to the term sup A\u2208A [ E(A)\u2212 Ez(A) ] , with probability at least 1\u2212 \u03b4, there holds sup A\u2208A [ E(A)\u2212 Ez(A) ] \u2264 Ez sup A\u2208A [ E(A)\u2212 Ez(A) ]", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "[5]), from the above estimation we can further estimate I2 as follows: I2 \u2264 1 r\u03bbEz sup x\u2208X \u2225 \u2225 \u2225 Ez\u2032 1 m \u2211", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The above generalization bound for similarity learning formulation (2) with the Frobenius norm regularization is consistent with that given in [3], where the result holds true under the assumption that supx\u2208X \u2016x\u2016F \u2264 1.", "startOffset": 143, "endOffset": 146}], "year": 2017, "abstractText": "Learning an appropriate (dis)similarity function from the available data is a central problem in machine learning, since the success of many machine learning algorithms critically depends on the choice of a similarity function to compare examples. Despite many approaches for similarity metric learning have been proposed, there is little theoretical study on the links between similarity metric learning and the classification performance of the result classifier. In this paper, we propose a regularized similarity learning formulation associated with general matrix-norms, and establish their generalization bounds. We show that the generalization error of the resulting linear separator can be bounded by the derived generalization bound of similarity learning. This shows that a good generalization of the learnt similarity function guarantees a good classification of the resulting linear classifier. Our results extend and improve those obtained by Bellet at al. [3]. Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrixnorm regularization, which has a strong dependence on the dimensionality of the input space. Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality, in the cases of sparse L-norm and mixed (2, 1)-norm regularization, enables us to obtain bounds that have a mild dependence on the input dimensionality.", "creator": "LaTeX with hyperref package"}}}