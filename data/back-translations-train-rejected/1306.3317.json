{"id": "1306.3317", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2013", "title": "Sparse Auto-Regressive: Robust Estimation of AR Parameters", "abstract": "In this paper I present a new approach for regression of time series using their own samples. This is a celebrated problem known as Auto-Regression. Dealing with outlier or missed samples in a time series makes the problem of estimation difficult, so it should be robust against them. Moreover for coding purposes I will show that it is desired the residual of auto-regression be sparse. To these aims, I first assume a multivariate Gaussian prior on the residual and then obtain the estimation. Two simple simulations have been done on spectrum estimation and speech coding.", "histories": [["v1", "Fri, 14 Jun 2013 07:49:44 GMT  (405kb)", "http://arxiv.org/abs/1306.3317v1", "4 pages, 4 figures"], ["v2", "Tue, 18 Aug 2015 16:59:06 GMT  (523kb)", "http://arxiv.org/abs/1306.3317v2", "4 pages, 4 figures"]], "COMMENTS": "4 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mohsen joneidi"], "accepted": false, "id": "1306.3317"}, "pdf": {"name": "1306.3317.pdf", "metadata": {"source": "CRF", "title": "Sparse Auto-Regressive: Robust Estimation of AR Parameters", "authors": [], "emails": [], "sections": [{"heading": null, "text": "The question about the \"why,\" according to the title, \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \"\", \",\" \",\", \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \"\", \",\" \",\" \",\", \",\" \",\" \"\", \",\" \",\", \"\" \",\", \"\" \",\", \"\", \"\" \",\", \"\", \",\", \"\" \",\", \"\" \",\", \",\", \",\" \",\", \",\", \"\" \",\" \",\", \",\" \",\", \"\" \",\" \",\" \",\", \",\", \"\", \"\", \"\", \",\", \"\" \",\", \"\", \",\", \"\" \",\", \"\" \",\" \",\" \"\", \"\", \"\", \"\", \"\", \"\" \",\", \"\", \"\", \",\" \"\" \",\", \",\" \",\" \",\" \"\", \"\", \"\" \",\", \"\", \",\" \",\" \",\" \"\" \",\" \",\" \",\" \",\" \",\" \"\", \",\", \"\" \"\" \",\", \"\" \",\" \"\" \"\", \"\", \",\" \""}, {"heading": "3. Relation to sparse representation", "text": "To understand how scanty residues relate to scanty representation in relation to over complete bases, I rewrite (8) in the following form: (11), (12), (12), is a scanty decomposition problem with scanty group constraints. Scanty making the coefficients corresponding to the identity matrix is tantamount to making the remnants of a regression problem too sparse. It is possible to use some other matrices instead of the identity matrix, which makes the residual scaneness better. But the approach of this paper is not this idea. If the compressive load does not take care, the research will be more successful from this point of view, because learning a suitable dictionary for different components of signals provides a suitable representation domain for processing. For example, if the residual amount of our problem is limited, then we may misjudge DCT or FFT problem."}, {"heading": "5. Application to time series with missing data", "text": "Problems based on minimizing MSE are very sensitive to missing data, as forgotten data are likely to have large errors and their square errors have a huge impact, while M estimators like the proposed estimate are capable of reducing the negative effects of gross errors. In this section, I will show only an intuitive experience of the applicability of the proposed estimate using an example. Figure 2-C shows a synthetic time series of 64 samples that have lost 25% of their samples. Figure 2-A shows the original row and the row of missed samples. Figure 2-B shows the estimated time series and its spectrum based on original time series. Figure 2-C shows the estimated time series and its spectrum based on missing time series calculated by solving Yule-Walker equations. Figure 2-D is similar to 2-C, but uses the proposed algorithm. As can be seen, the estimated resistances resolve the peaks of the spectrum based on its misalignment of time spectrum."}, {"heading": "6. Application to speech coding", "text": "As already described, the problem (8) differs from the traditional AR in terms of residual distribution from zero to zero. Thus, in the proposed problem, the residual amount of the Laplace distribution is less pronounced. Let me compare the entropy of the Gaussian and Laplace distributions. In this section, it is assumed that and are distributions of two residual samples, both corresponding to the regression of a signal. (14) Resistance resistance is as low as the number of resistance samples (16) [)] (17) \u221a (18) shows the entropy difference of two Gaussian and Laplace sources. Figure 3 shows both distributions with the same variance. The peak of the Laplace distribution is sharper than Gaussian. In other words, the number of samples of a Laplace source by zero is more than Gaussian. But it is obvious that the variance of resistance is minimized."}, {"heading": "7. Conclusion", "text": "As shown in this paper, a sparse residual quantity provides suitable properties for the residual signal for the parameter estimation of auto-regression. Only two applications of this approach have been investigated, but this robust estimate can be applied in many areas where auto-regression is a suitable model."}], "references": [{"title": "Variational Bayes for generalized autoregressive models", "author": ["S.J. Roberts", "W.D. Penny"], "venue": "Signal Processing, IEEE Transactions on , vol.50, no.9, pp. 2245- 2257, Sep 2002", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust Autoregression: Student-t Innovations Using Variational Bayes", "author": ["J. Christmas", "R. Everson"], "venue": "Signal Processing, IEEE Transactions on , vol.59, no.1, pp.48-57, Jan. 2011", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonparametric spectral density estimation with missing observations", "author": ["Lee, T.C.M.", "Zhengyuan Zhu"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on , vol., no., pp.3041-3044, 19-24 April 2009", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation of autoregressive spectra with randomly missing data", "author": ["P.M.T. Broersen", "S. de Waele", "R. Bos"], "venue": "Instrumentation and Measurement Technology Conference, 2003. IMTC '03. Proceedings of the 20th IEEE , vol.2, no., pp. 1154- 1159 vol.2, 20-22 May 2003", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A Generalized Least Absolute Deviation Method for Parameter Estimation of Autoregressive Signals", "author": ["Youshen Xia", "Kamel, M.S."], "venue": "Neural Networks, IEEE Transactions on , vol.19, no.1, pp.107-118, Jan. 2008", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "From Sparse Signals to Sparse Residuals for Robust Sensing", "author": ["V. Kekatos", "G.B. Giannakis"], "venue": "Signal Processing, IEEE Transactions on , vol.59, no.7, pp.3355-3368, July 2011", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual Reconstruction", "author": ["Andrew Blake", "Andrew Zisserman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Robust M-estimates and generalized Mestimates for autoregressive parameter estimation", "author": ["A. Basu", "K.K. Paliwal"], "venue": "TENCON '89. Fourth IEEE Region 10 International Conference , vol., no., pp.355-358, 22-24 Nov 1989", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "To address this problem, [1] suggest imposing the mixture of Gaussians distributions for the residual.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Another approach assumes the long tailed distributions, [2] exploited student-t distribution.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "[3] presented the application of autoregressive analysis in missing data problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In [4] an algorithm was suggested for estimation of AR parameters in missing data situation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "An approximation of the sparse regression is least absolutes regression that [5] exploited this regression for robust AR parameters estimation.", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "[6] has named this system of equations the \"robust sensing\" problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] has also showed that robust sensing is an NP-Hard problem and proved that the solution of robust sensing in a certain conditions equal to:", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "To solve it I exploit Graduated nonconvexity (GNC) technique [7] that will be described in Section 4.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "A well-known method to solve this type of problems is Iterative Reweighted Least Squares (IRLS) [9].", "startOffset": 96, "endOffset": 99}], "year": 2013, "abstractText": "In this paper I present a new approach for regression of time series using their own samples. This is a celebrated problem known as Auto-Regression. Dealing with outlier or missed samples in a time series makes the problem of estimation difficult, so it should be robust against them. Moreover for coding purposes I will show that it is desired the residual of auto-regression be sparse. To these aims, I first assume a multivariate Gaussian prior on the residual and then obtain the estimation. Two simple simulations have been done on spectrum estimation and speech coding.", "creator": "Microsoft\u00ae Word 2010"}}}