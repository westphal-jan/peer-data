{"id": "1306.3525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2013", "title": "Approximation Algorithms for Bayesian Multi-Armed Bandit Problems", "abstract": "In this paper, we consider several finite-horizon Bayesian multi-armed bandit problems with side constraints which are computationally intractable (NP-Hard) and for which no optimal (or near optimal) algorithms are known to exist with sub-exponential running time. All of these problems violate the \"idling bandit property\", which assumes that the reward from the play of an arm is not contingent upon when the arm is played. Not only are index policies suboptimal in these contexts, there has been little analysis of such policies in these problem settings. We show that if we consider near-optimal policies, in the sense of approximation algorithms, then there exists (near) index policies. Conceptually, if we can find policies that satisfy an approximate version of the idling bandit property, namely, that the reward from the play of an arm depends on when the arm is played to within a constant factor, then we have an avenue towards solving these problems. However such an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense. Clearly, such a property is not necessarily true o arbitrary single arm policies and finding such single arm policies is nontrivial. We show that by restricting the state spaces of arms we can find single arm policies and that these single arm policies can be combined into global (near) index policies where the approximate version of the idling bandit property is true in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability.", "histories": [["v1", "Fri, 14 Jun 2013 22:24:29 GMT  (48kb)", "https://arxiv.org/abs/1306.3525v1", "arXiv admin note: text overlap witharXiv:1011.1161"], ["v2", "Wed, 17 Jul 2013 19:16:47 GMT  (83kb,D)", "http://arxiv.org/abs/1306.3525v2", "arXiv admin note: text overlap witharXiv:1011.1161"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1011.1161", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["sudipto guha", "kamesh munagala"], "accepted": false, "id": "1306.3525"}, "pdf": {"name": "1306.3525.pdf", "metadata": {"source": "CRF", "title": "Approximation Algorithms for Bayesian Multi-Armed Bandit Problems\u2217", "authors": ["Sudipto Guha", "Kamesh Munagala"], "emails": ["sudipto@cis.upenn.edu.", "kamesh@cs.duke.edu."], "sections": [{"heading": null, "text": "We present a general solution framework that provides constant factor approximation algorithms for all of the above variants. Our framework goes further by formulating a weakly coupled linear programming relaxation, the solution of which produces a collection of compact strategies whose implementation is limited to a single arm. These one-armed strategies are more structured to ensure the polynomial calculation of relaxation time, and their implementation is then carefully coordinated so that the resulting global policies are not only practicable, but also produce a constant approximation. We show that relaxation can be solved using the same techniques as for calculating index policies; in fact, the final strategies we design are very close to being index policies themselves. Conceptually, we find strategies that satisfy an approximate version of the exchange property, namely that the reward from a game does not depend on game duration, but on a constant factor."}, {"heading": "1 Introduction.", "text": "In this paper, we look at the problem of the repeated allocation of resources when the effectiveness of a resource is uncertain and we have to make a series of allocation decisions based on past results. Since the underlying contributions from Wald [65] and Robbins [56], a huge literature that includes both optimal and near-optimal solutions, have been developed, we are able to take a series of measures that address both the celebrated Multi-Armed Bandit (MAB) problem and the allocation of resources between competing actions (arms) with uncertain rewards and only one action to take (play the arm).The game of the arm offers agents both a reward and some additional information about the state of the arm that needs to be updated. The agent's goal is to play the weapons (according to some specified terms) within limits."}, {"heading": "1.1 A Solution Recipe", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "1.2 Problem Definitions, Results, and Roadmap", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. It is. (...) It is. (...) It is. (... It is. (...) It is. (...) It is. It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. (... It is. (...) It is. It is. It is. (... It is. (...) It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. (). It is. It is. It is. (...). It is. It is. It is. It is. (...). (). It is. It is."}, {"heading": "1.3 Related Work", "text": "In this context, it should be noted that the solution to the problems is not just a red herring, but a red herring."}, {"heading": "2 Preliminaries: State Spaces, Budgets and Decision Policies", "text": "Remember the basic adjustment of the finite horizon of the Bayesian MAB problem. There are a number of n independent weapons. Arm i offers rewards in an i.i.d. manner from a parameterized distribution Di (\u03b8i). The parameter \u03b8i may be arbitrary. It is unknown at the beginning, but a previous Di is defined by possible values of \u03b8i. The precursors of different weapons are independent. At each step, a decision policy can play a number of weapons (as the limitations of the specific problem allow) and only observe the outcome of the weapons played. Based on the results, the priorities of the weapons played in the previous step are updated to the corresponding subordinate distributions according to the Bayes rule. The goal of decision policy (most often) is to maximize the expected sum of the rewards obtained over the T-time steps, placing expectation above the priories as well as the outcome of each game."}, {"heading": "2.1 State Space of an Arm and Martingale Rewards", "text": "Let us base the initial state, which corresponds to the initial previous Di-distribution. Let us base the initial state, which is the basis for the initial Di-distribution. Let us base the initial state. Let us base the initial state. Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base a posterior distribution.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state.Let us base the initial state."}, {"heading": "2.2 Modeling Budgets and Feedback", "text": "In fact, the fact is that we are able to hide, and that we are able to hide, we will be able to hide, \"he said in an interview with the\" New York Times. \""}, {"heading": "2.3 Decision Policies and Running Times", "text": "A Decision Policy P is a mapping of the current state of the overall system to an action that involves K-weapons being played in the basic version of the problem. This set of actions may be richer, as we discuss in the following sections. Note that the state of the overall system encompasses all the knowledge available to policy makers, including: 1. The Common States {u-Si} of all weapons; 2. The current time step, which also gives the remaining time horizon; 3. The remaining budget of all weapons; 4. Games that have been made in the past for which the feedback is outstanding (in connection with delayed feedback). Therefore, the \"state\" used by a decision-making policy could be much more complicated than the production space of Si, and one of our contributions in this paper is to simplify this state space and make it tractable. Running Times. Our goal will be to calculate decision-making policies at a time that polynomically depends on T and the size of the state space."}, {"heading": "2.3.1 Single Arm Policies", "text": "A single arm policy Pi for arm i is a policy whose implementation (measures and rewards) is limited only to the state space of arm i. In other words, it has one of several possible measures in each step; in the basic version of the problem, the measures available at each step involve either (i) playing the arm - if the arm is in the state u-Si, this brings reward ru; or (ii) ceasing to play and leaving. Furthermore, the measures of Pi depend only on the state variables limited to arm i - the current state u-Si, the remaining budget of arm i, the remaining time horizon, etc. In most situations, we will eliminate states that cannot be achieved in T-steps (the horizon). Formally; definition 1. Let Si (T) restrict the state space Si to T-steps, the remaining budget of arm i, the remaining time horizon, etc."}, {"heading": "3 Irrevocable Policies for Finite Horizon Bayesian MAB Problems", "text": "In this section we will consider the basic finite horizon Bayesian MAB problem (as defined in Section 1) with the additional limitation of irrevocability; that is, the games for each arm are contiguous. Recently, Farias and Madan [30] presented an 8 approach for this problem. The contribution of this section is the introduction of a range of basic techniques that illustrate how all (significantly more difficult) problems are in this paper. In terms of specific results we show that given a solution to the standard LP formulation for this problem, a simple termination algorithm provides a 2 approach (Theorem 4). In addition, we prove that this is the best possible result against the standard LP solution."}, {"heading": "3.1 Linear Programming Relaxations and Single arm Policies", "text": "Consider the following LP, the two variables wu and to each arm i and each u-Si (T). iDe LP, the two variables wu and to each arm i and each u-Si (T). iDe LP (T). iSe (T). iSe (T). iSe (T). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). iSe (D). (D). D. (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D. (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D. (D). (D). (D)."}, {"heading": "3.2 The Idea of Truncation", "text": "We now show that the time horizon of a one-armed policy can be reduced to T-steps to \u03b2T for constant \u03b2 \u2264 1, while we forego a constant factor in the reward. We note that although the statement appears simple, this theorem applies only to one-armed actions and does not apply to the global reward carried out over several arms. Proof of this theorem uses the marginal structure of the rewards and goes beyond a stop-time argument. A similar statement is proven in [30, Lemma2] using an inductive argument; however, the precise proof would require changes in the varied attitudes that we apply and therefore redefine and prove the theorematics. Definition 3. In view of a policy P, a decision path is a sequence of (observed reward) pairs encountered by the policy to the stop. Different decision paths are encountered by the policy with different probabilities that depend on the rewards observed."}, {"heading": "3.4 Weak Coupling, Efficient Algorithms and Compact Representations", "text": "We outline how to efficiently handle (LP1) a standard application of weak duality. (Recall, LP1 = Max {Pi-Ci (T)} {fic i-R (Pi). We note the Lagrangian of the coupling condition in order to achieve the optimal policy by solving LPLag (T). (Definition 4). Let's leave Qi (Pi) \u2212 \u03bbT (Pi). We now note that there are no constraints that bind the weapons, so the optimal policy is achieved by solving LPLag (T) separately for each weapon. (Definition 4) = MaxPi (T). (T). (Pi) \u2212 T (Pi) describes the optimal policy achieved by solving LPLag. (LPLag)"}, {"heading": "3.5 Comparison to Gittins Index", "text": "The most common policy for the discounted version of the multi-armed bandit problem is the Gittins index policy (Gittins index) (Gittins index) = (Gittins index) (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index) = (Gittins index = P) = (Gittins index = (Gittins index = P) = (Gittins index = (Gittins index = P) (Gittins index = (Gittins index = P) = (Gittins index = P) = (Gittins index = P) (Gittins index = (Gittins index = P) = (Gittins index = P) (Gittins index = P) = (Gittins index = (Gittins index = P) = (Gittins index = P) = (Gittins index = (Gittins index = = P) = (Gittins index = P) = (Gittins index = (Gittins index = = P) = (Gittins index = = P) = (Gittins index = P) = (Gittins index = (Gittins index = P) = (Gittins index = P) = (Gittins index = P) = (Gittins index = (Gittins index = P) = (Gittins index = P) = (Gittins index = (Gittins = P) = (Gittins index = P) = (Gittins index = P) = (Gittins index = (Gittins index = P) = (Gittins index = (Gittins = P) = (Gittins index = (Gittins = P) = (Gittins) = (Gittins) = (Gittins index = (Gittins)) = (Gittins) = (Gittins index ="}, {"heading": "4 Traversal Dependent Bayesian MAB Problems", "text": "In this section, we look at how the limitations for exceeding the various bandit problems can affect the approximation problems. A concrete example of such exceeding related limitations is the Bayesian MAB problem with switching the costs, where there are costs of switching between weapons. Name the cost of switching from arm i to arm j as'ij Z +. The system starts with an arm i0. The goal is to maximize the expected reward, which is subject to rigid limitations that (i) the total number of games is at most T and (ii) the total switching costs at most L in all decision paths. This problem has received considerable attention, see the discussion in section 1 and in [12] - however, efficient solutions with detectable limits in the Bayesian setting are difficult to attain.A classic example of such a switching problem can be if the costs \"ij define a distance metric that is naturally set in most navigation settings and was considered earlier."}, {"heading": "4.1 Arbitrary Order Irrevocable Policies for the Bayesian MAB Problem", "text": "The structure of this problem is similar to the finite-horizon MAB problem using irrevocable strategies discussed in Section 3. The only difference is that we cannot apply the arrangement of R (Pi) / T (Pi) as described in Figure 2 - instead, we will have to apply an arbitrary order after the weapons and corresponding strategies Pi have been chosen, i.e. step 2 will not be performed. Note that the limit of LP1 remains a valid upper limit of this problem - but Lemma 3 is not explicit and Theorem 7 is not implicitly useful. Below, we will prove Theorem 10 to replace it. The notation used will be the same as in Section 3.Theorem 10. The finite-horizon-Bayesian MAB problem using an arbitrary sequence of irrevocable strategies (Theorem 7 is implicitly not useful)."}, {"heading": "4.2 Bayesian MAB Problem with Metric Switching Costs", "text": "For the sake of simplicity, we will also assume K = 1 in this section, that is, the system may play only one arm at a time and observe only the result of that arm played. We will discuss K > 1 at the end of the subsection. The system starts with an arm i0. A policy takes one of the following decisions based on the results of actions taken so far (which determine the current state of all weapons): (i) plays the arm on which it is currently located; (ii) plays another arm (pays the distance cost of switching to that arm); or (iii) stops. Just as before, a policy receives a reward ru if it plays arm i in the state u-Si. Each policy is also subject to strict restrictions, so that the total number of games is at most T and the total distance cost at most L in all decision paths. First, we delete all arms j, so that \"i0j > L\" cannot reach such an arm without exceeding the distance budget."}, {"heading": "4.2.1 A (Strongly Coupled) Relaxation and its Lagrangian", "text": "We describe a sequence of relaxations to the optimal policy, culminating in a weakly coupled policy of detente. A priori, it is not clear how such a relaxation can be constructed, since the switching cost constraint links arms in a complicated way. We achieve a weak coupling via the Lagrange policy of natural LP detente, which we can consider a combinatorial problem called orientation orientation over a one-armed policy. Definition 5. Let C (L, T) denote the set of policy over all remaining arms, over a time horizon T, which can perform one of two actions: (1) Play current arm; or (2) Switch to different arm policies.Definition 5. Such measures have no limitations on the total number of games, but are necessary to have the distance cost L on all decision-making channels. Observe that the constraint corresponding to the distance paths of the distance."}, {"heading": "4.2.2 Structure of M2(\u03bb)", "text": "The critical insight, which explicitly utilizes the fact that in MAB the state of an inactive arm does not change and enables weak coupling, is as follows: Lemma 12. For each other, considering any P \u00b2 C, there is a P \u00b2 C that never repeats an arm it has already played and disconnected from, so that f\u03bb (P \u00b2) \u2265 arm (P).Proof. We will use the fact that Si is finite in our proof. Suppose that P \u00b2 C repeats an arm it has already played and disconnected in time, so that the policy on any arm i, makes a decision to switch to arm j and at a later date switch to some path of decision, verifies Arm i. Note that after time the subsequent decision policy PH fulfills the condition that no arm is revisited. Therefore, all games of arm j are temporary."}, {"heading": "4.2.3 Orienteering and the Final Algorithm", "text": "We show that the optimal solution for M2 (\u03bb) is a collection of one-armed strategies connected by a combinatorial optimization problem called orienteering. Definition 7. In the orientation problem [17, 13, 22] we will get a metric space G (V, E) in which each node v, V) has a reward. (There is a start node s and a distance bound L. The goal is that within a (1 + 2) approximation we can assume that we are integrers who are most O (n / \u03b5).A direct sequence of Lemma 12 is the following: Corollary 13. Define a graph G (V, E), where node i, V, V and V are."}, {"heading": "5 Multi-armed Bandits with Delayed Feedback", "text": "In this variant of the MAB problem, we have no guarantee of the expected reward. In this section, we assume that there will be no approximation or traversal-related costs. Considering the algorithms and analyses we propose for dealing with delayed feedback, such additional constraints can be handled using the ideas of the preceding sections. In this section, we assume that the idea of truncation is not (immediately) useful because a policy can (and should) be \"backloaded\" in the sense that we consider a single arm, most of the games are made toward the end of the horizon (possibly because the policy is confident that the reward of this arm is large and well separated from the alternatives). Therefore, truncating the horizon (according to each factor) may result in the elimination of these good games and, as a consequence, we have no guarantee of the expected reward."}, {"heading": "5.1 Weakly Coupled Relaxation and Block Structured Policies", "text": "This policy is now a (randomized) representation of the current state of the arm on one of the following measures: (i) make a play; (ii) wait a number of steps (less or equal to T) so that when the outcome of a previous game is known, the policy changes the state of the policy; (iii) wait a few steps and make a play (without additional information); or (iv) quit. Definition 8. Let Ci (T, i) be the set of all one-armed actions over a horizon of T-steps. As in Section 3, we will use the LP system (LPDelay) to tie the best collection of one-armed actions - the goal will be to find one policy per arm so that the total number of games and the expected reward is maximized."}, {"heading": "5.2 Delay Free Simulations and Small Delays", "text": "In this section, we will discuss the case when the delays are low compared to the horizon, i.e., maxi \u03b4i \u2264 \u221a T / 50. Next, we will introduce the idea of instantaneous simulation. Leave r = \u221a T / (2 maxi \u03b4i) and \u03b3 = 4r (maxi \u03b4i) 2 / T. The choice of these parameters will become clear shortly. Definition 10. Definition of a policy Pi that can currently be used in a delay-free mode. Notice that the truncation idea of Theorem 2 applies to a policy as soon as it is in a delay-free mode. Definition of a policy Pi (or earlier), the result of the current game is available at any time and can be applied subsequentially."}, {"heading": "5.3 Block Compaction and Larger Delays", "text": "In this section, we assume that T \u2265 21 (2\u03b4i + 1) (1 + log \u03b4i) (b) plays for all i. Let \u03b3 = 13, \u03c1 = 13. The next problem is that we can also shrink the horizon by increasing the number of games - which is intuitively synonymous with advancing the games in politics. (Tagma 21.) For all types of games that result in a policy, there is then a (\u2032 T, T) horizon policy (as defined in Lemma 19) P \u2032 i, that R (Pi) \u2264 R (P \u2032 i) and T (P \u2032 i) \u2264 i (Pi).Proof. Consider the execution of Pi. Without loss of generality, we can assume that all blocks in this policy have games in most cases, otherwise the policy may switch to delayed free mode, increasing the games by a factor of the games i. The total number of games in delay-free mode may be at most T."}, {"heading": "5.4 Constructing Block Structured Policies: Proof of Lemma 18", "text": "We will now show how to find a series of block structures designed so that each of them has a horizon of 2T maximum and together must fulfill the following characteristics: \"Most of them are in blocks of 2\u03b4i + 1 length and above a horizon of 2T steps.\" In each block, it makes a maximum of 2T steps a simple compilation from the description of Si, and then one waits for the feedback of these games. For each state of arm i, the policy determines the number of consecutive games that will be made in that state. Define the following quantities are a simple compilation from the description of Si, and the details are excluded. 1. Let ri (\") denotes the expected reward that will be achieved when\" consecutive games are made, and feedback very end.Let pi (\") denote the probability that we will receive the expected reward if.\""}, {"heading": "7 Future Utilization and Budgeted Learning", "text": "In this section, we look at the budgeted learning problem without returning to a result. We assume that K = 1 weapons can be played at any step. The goal of a policy here is to conduct a pure exploration for T-steps. In any such final state, we choose that arm i whose final state u-Si has maximum reward; this is the reward that the policy receives in this decision path. The goal is to find the policy that maximizes the expected reward, where the expectation about the execution of the policy is met."}, {"heading": "7.2 An Amortized Accounting", "text": "Consider the one-armed policy Pi (\u03bb), which corresponds to the value Q-i (\u03bb). Pi (\u03bb) performs one of three actions for each state u-Si: (i) Play the arm; (ii) select the arm as the final answer and stop or (iii) Quit. Therefore, the reward R (Pi (\u03bb) = Q-i (\u03bb) + \u03bb-i (I (Pi (\u03bb))) + T (Pi (\u03bb)) / T) can be amortized, so that for the state u-Si the reward is collected as follows: 1. A reward in advance of Q-i (\u03bb) when the game for the arm begins at the root."}, {"heading": "7.3 The Final Algorithm", "text": "The next problem is the crux of the whole analysis and follows the amortized accounting argument. The hurdle in using the argument is that, when the horizon is exhausted, the currently implemented one-armed policy is not fully implemented. (Note that this policy is not fully implemented, thus violating the horizontal policy.) The expected reward for this (unfeasible) policy is at least LPBud / (3 +). Consider three terminating conditions: (a) Politics has visited all weapons and has no more weapons to play, (b) Politics chooses an arm, or (c) Politics continues in the past, with the amortized argument that in the case of (a) the contribution to amortized accounting it is."}, {"heading": "8 Conclusions", "text": "In this paper, we have shown that we can formulate and solve weakly coupled LP relaxations for multiple variants of the multi-arm bandit problem with a limited horizon, and that the solutions of these relaxations can be used to design workable policy-making whose reward is within a fixed constant factor of optimal reward. This provides an analytical justification for the use of such relaxations to guide policy-making in practice, and the resulting strategies are comparable in complexity to standard index policies. In the latter case, the main open questions posed by our work are improving the performance limits for long delays in the delayed feedback model, or the results of the simultaneous feedback model for the MaxMAB problem. Note that in the latter case, we provide comparisons with the strong upper limits of the one-at-a-time model, which would require new techniques or a lower upper upper limit. This could include the formulation of more strongly coupled LP-related problems [for example, i54]."}, {"heading": "Acknowledgments", "text": "We thank Martin Pa \ufffd l for some helpful conversations."}], "references": [{"title": "Relaxations of weakly coupled stochastic dynamic programs", "author": ["D. Adelman", "A.J. Mersereau"], "venue": "Operations Research, 56(3):712\u2013727,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Explore/exploit schemes for web content optimization", "author": ["D. Agarwal", "B.-C. Chen", "P. Elango"], "venue": "Proceedings of ICDM, pages 1\u201310,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A measurement-based analysis of multihoming", "author": ["A. Akella", "B. Maggs", "S. Seshan", "A. Shaikh", "R. Sitaraman"], "venue": "Proceedings of SIGCOMM, pages 353\u2013364,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximizing sequence-submodular functions and its application to online advertising", "author": ["S. Alaei", "A. Malekian"], "venue": "CoRR abs/1009.4153,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential analysis with delayed observations", "author": ["T.W. Anderson"], "venue": "Journal of the American Statistical Association, 59(308):1006\u20131015,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1964}, {"title": "The search for optimality in clinical trials", "author": ["P. Armitage"], "venue": "International Statistical Review, 53(1):15\u201324,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1985}, {"title": "Bayes and minmax solutions of sequential decision problems", "author": ["K.J. Arrow", "D. Blackwell", "M.A. Girshick"], "venue": "Econometrica, 17:213\u2013244,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1949}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47(2-3):235\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM J. Comput., 32(1):48\u201377,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Automated experiment-driven management of (database) systems", "author": ["S. Babu", "N. Borisov", "S. Duan", "H. Herodotou", "V. Thummala"], "venue": "Proc. of HotOS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Denumerable-armed bandits", "author": ["J.S. Banks", "R.K. Sundaram"], "venue": "Econometrica, 60(5):1071\u2013 1096,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Switching costs and the gittins index", "author": ["J.S. Banks", "R.K. Sundaram"], "venue": "Econometrica, 62(3):687\u2013694,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Approximation algorithms for deadline-tsp and vehicle routing with time-windows", "author": ["N. Bansal", "A. Blum", "S. Chawla", "A. Meyerson"], "venue": "STOC, pages 166\u2013174,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Dynamic Programming and Optimal Control", "author": ["D. Bertsekas"], "venue": "Athena Scientific, second edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Conservation laws, extended polymatroids and multi-armed bandit problems: A unified polyhedral approach", "author": ["D. Bertsimas", "J. Nino-Mora"], "venue": "Math. of Oper. Res., 21(2):257\u2013306,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Restless bandits, linear programming relaxations, and a primal-dual index heuristic", "author": ["D. Bertsimas", "J. Ni\u00f1o-Mora"], "venue": "Oper. Res., 48(1):80\u201390,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Approximation algorithms for orienteering and discounted-reward tsp", "author": ["A. Blum", "S. Chawla", "D.R. Karger", "T. Lane", "A. Meyerson", "M. Minkoff"], "venue": "SIAM J. Comput., 37(2):653\u2013670,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "On sequential designs for maximizing the sum of n observations", "author": ["R.N. Bradt", "S.M. Johnson", "S. Karlin"], "venue": "Ann. Math. Statist., 27(4):1060\u20131074,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1956}, {"title": "Optimal learning and experimentation in bandit problems", "author": ["M. Brezzi", "T.-L. Lai"], "venue": "Journal of Economic Dynamics and Control, 27(1):87\u2013108,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "J. ACM, 44(3):427\u2013485,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Prediction, Learning and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved algorithms for orienteering and related problems", "author": ["C. Chekuri", "N. Korula", "M. P\u00e1l"], "venue": "SODA \u201908: Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms, pages 661\u2013670,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential decision for a binomial parameter with delayed observations", "author": ["S.C. Choi", "V.A. Clark"], "venue": "Biometrics, 26(3):411\u2013420,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1970}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & sons,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Approximating the stochastic knapsack problem: The benefit of adaptivity", "author": ["B.C. Dean", "M.X. Goemans", "J. Vondrak"], "venue": "Math. of Operations Research, 33(4):945\u2013964,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Reflective control for an elastic cloud appliation: An automated experiment workbench", "author": ["A. Demberel", "J. Chase", "S. Babu"], "venue": "Proc. of HotCloud,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "On a scheduling problem in sequential analysis", "author": ["S. Ehrenfeld"], "venue": "The Annals of Mathematical Statistics, 41(4):1206\u20131216,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1970}, {"title": "The two-armed bandit with delayed responses", "author": ["S.G. Eick"], "venue": "The Annals of Statistics, 16(1):254\u2013 264,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1988}, {"title": "Risk-sensitive online learning", "author": ["E. Even-Dar", "M.J. Kearns", "J.W. Vaughan"], "venue": "ALT, pages 199\u2013213,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "The irrevocable multiarmed bandit problem", "author": ["V.F. Farias", "R. Madan"], "venue": "Operations Research, 59(2):383\u2013399,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Online convex optimization in the bandit setting: Gradient descent without a gradient", "author": ["A. Flaxman", "A. Kalai", "H.B. McMahan"], "venue": "Annual ACM-SIAM Symp. on Discrete Algorithms,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "Wiley,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "Progress in statistics (European Meeting of Statisticians),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1972}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "Progress in statistics (European Meeting of Statisticians),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1972}, {"title": "How to probe for an extreme value", "author": ["A. Goel", "S. Guha", "K. Munagala"], "venue": "ACM Transactions of Algorithms, 7(1):12:1\u201320,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "The ratio index for budgeted learning, with applications", "author": ["A. Goel", "S. Khanna", "B. Null"], "venue": "Proceedings of SODA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive submodular optimization under matroid constraints", "author": ["D. Golovin", "A. Krause"], "venue": "CoRR abs/1101.4450,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation algorithms for budgeted learning problems", "author": ["S. Guha", "K. Munagala"], "venue": "Proc. of STOC,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Sequential design of experiments via linear programming", "author": ["S. Guha", "K. Munagala"], "venue": "CoRR, arxiv:0805.0766,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-armed bandits with metric switching costs", "author": ["S. Guha", "K. Munagala"], "venue": "Proceedings of ICALP,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximate indexability and bandit problems with concave rewards and delayed feedback", "author": ["S. Guha", "K. Munagala"], "venue": "Proceedings of APPROX-RANDOM,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterated allocations with delayed feedback", "author": ["S. Guha", "K. Munagala", "M. P\u00e1l"], "venue": "Manuscript, available at CoRR http://arxiv.org/abs/1011.1161,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximation algorithms for restless bandit problems", "author": ["S. Guha", "K. Munagala", "P. Shi"], "venue": "J. ACM, 58(1):3:1\u201350,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximation algorithms for correlated knapsacks and non-martingale bandits", "author": ["A. Gupta", "R. Krishnaswamy", "M. Molinaro", "R. Ravi"], "venue": "Proc. of FOCS,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Profiling, what-if analysis, and cost-based optimization of mapreduce programs", "author": ["H. Herodotou", "S. Babu"], "venue": "Proc. of VLDB,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "J. ACM, 48(2):274\u2013296,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2001}, {"title": "Job-search and the theory of turnover", "author": ["B. Jovanovich"], "venue": "J. Political Economy, 87:972990,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1979}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Proc. of 16th Conf. on Computational Learning Theory,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6:4\u201322,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1985}, {"title": "Budgeted learning of naive-bayes classifiers", "author": ["D. Lizotte", "O. Madani", "R. Greiner"], "venue": "Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI-03), pages 378\u201338,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimization flow control, i: Basic algorithm and convergence", "author": ["S. Low", "D.E. Lapsley"], "venue": "IEEE/ACM Transactions on Networks, 7(6), December", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Active model selection", "author": ["O. Madani", "D.J. Lizotte", "R. Greiner"], "venue": "UAI \u201904: Proc. 20th Conf. on Uncertainty in Artificial Intelligence, pages 357\u2013365,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Job-search and labor market analysis", "author": ["D. Mortensen"], "venue": "O. Ashenfelter and R. Layard, editors, Handbook of Labor Economics, volume 2, page 849919. North Holland, Amsterdam,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1985}, {"title": "Restless bandits, partial conservation laws and indexability", "author": ["J. Ni\u00f1o-Mora"], "venue": "Adv. in Appl. Probab., 33(1):76\u201398,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-UAV dynamic routing with partial observations using restless bandits allocation indices", "author": ["J.L. Ny", "M. Dahleh", "E. Feron"], "venue": "Proceedings of the 2008 American Control Conference,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin American Mathematical Society, 55:527\u2013535,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1952}, {"title": "A two-armed bandit theory of market pricing", "author": ["M. Rothschild"], "venue": "J. Economic Theory, 9:185202,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1974}, {"title": "Active learning in discrete input spaces", "author": ["J. Schneider", "A. Moore"], "venue": "34th Interface Symp.,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptive treatment assignment methods and clinical trials", "author": ["R. Simon"], "venue": "Biometrics, 33(4):743\u2013 749,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1977}, {"title": "An online algorithm for maximizing submodular functions", "author": ["M.J. Streeter", "D. Golovin"], "venue": "NIPS, pages 1577\u20131584,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2008}, {"title": "An asymptotically optimal algorithm for the max k-armed bandit problem", "author": ["M.J. Streeter", "S.F. Smith"], "venue": "Proceedings of AAAI,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "On sequential decision problems with delayed observations", "author": ["Y. Suzuki"], "venue": "Annals of the Institute of Statistical Mathematics, 18(1):229\u2013267,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1966}, {"title": "Tuple routing strategies for distributed eddies", "author": ["F. Tian", "D.J. DeWitt"], "venue": "Proc. of VLDB,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2003}, {"title": "A short proof of the Gittins index theorem", "author": ["J.N. Tsitsiklis"], "venue": "Annals of Appl. Prob., 4(1):194\u2013 199,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1994}, {"title": "Sequential Analysis", "author": ["A. Wald"], "venue": "Wiley, New York,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1947}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["P. Whittle"], "venue": "Appl. Prob., 25(A):287\u2013 298,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1988}], "referenceMentions": [{"referenceID": 37, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 138, "endOffset": 142}, {"referenceID": 38, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 181, "endOffset": 189}, {"referenceID": 41, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 181, "endOffset": 189}, {"referenceID": 64, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 55, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 20, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 31, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 13, "context": "However optimum index policies exist only in limited settings, see [14, 32] for further discussion.", "startOffset": 67, "endOffset": 75}, {"referenceID": 31, "context": "However optimum index policies exist only in limited settings, see [14, 32] for further discussion.", "startOffset": 67, "endOffset": 75}, {"referenceID": 6, "context": "One such general setting is the Bayesian Stochastic Multi-Armed Bandit formulation, which dates back to the results of [7, 18].", "startOffset": 119, "endOffset": 126}, {"referenceID": 17, "context": "One such general setting is the Bayesian Stochastic Multi-Armed Bandit formulation, which dates back to the results of [7, 18].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "This process therefore is a special case of the classic finite horizon multi-armed bandit problem [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "The Bayesian MAB formulation is therefore a canonical example of the Martingale Reward Bandit considered in recent literature such as [30]3.", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 13, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 31, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 33, "context": "For the above problems, the traditional and well understood index policies such as the Gittins index [34, 64] which are optimal for discounted infinite horizon settings, are either undefined or lead to poor performance bounds.", "startOffset": 101, "endOffset": 109}, {"referenceID": 63, "context": "For the above problems, the traditional and well understood index policies such as the Gittins index [34, 64] which are optimal for discounted infinite horizon settings, are either undefined or lead to poor performance bounds.", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "At a high level, our approach uses linear programming of a type that is similar to the widely used weakly coupled relaxation for bandit problems4 or \u201cdecomposable\u201d linear program (LP) relaxation [14, 66].", "startOffset": 195, "endOffset": 203}, {"referenceID": 65, "context": "At a high level, our approach uses linear programming of a type that is similar to the widely used weakly coupled relaxation for bandit problems4 or \u201cdecomposable\u201d linear program (LP) relaxation [14, 66].", "startOffset": 195, "endOffset": 203}, {"referenceID": 53, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 66, "endOffset": 74}, {"referenceID": 65, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 66, "endOffset": 74}, {"referenceID": 0, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 237, "endOffset": 240}, {"referenceID": 24, "context": "The gap between these two accounting processes is termed as the adaptivity gap (an example of such in the context of scheduling can be found in [25]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 34, "context": "For instance, for constraint (e), where the per-step reward is the maximum observed value from the set of plays is NP-Hard because it generalizes computing a subset of k distributions that maximizes the expected maximum [35].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "The bandit problem with switching costs (constraint c) is Max-SNP Hard6 since it generalizes an underlying combinatorial optimization problem termed orienteering [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 29, "context": "In this case, and many others we provide alternate analysis of existing heuristics which have been shown to perform well in practice [30].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "This problem was formalized by Farias and Madan [30].", "startOffset": 48, "endOffset": 52}, {"referenceID": 37, "context": "Using the algorithmic framework introduced in [38], Farias and Madan showed an 8 approximation algorithm which also works well in practice.", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "In Section 3, we provide a better analysis argument (based on revisiting [38] and newer ideas) and that argument improves the result to a factor (2 + )-approximation for any > 0 in time O(( \u2211 i#Edges(Si)) log(nT/ )) where #Edges(Si) is the number of edges that define the statespace Si.", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "It is widely acknowledged [12, 19] that the scenarios that call for the application of bandit problems typically have constraints/costs for switching arms.", "startOffset": 26, "endOffset": 34}, {"referenceID": 18, "context": "It is widely acknowledged [12, 19] that the scenarios that call for the application of bandit problems typically have constraints/costs for switching arms.", "startOffset": 26, "endOffset": 34}, {"referenceID": 11, "context": "Banks and Sundaram [12] provide an illuminating discussion even for the discounted reward version, and highlight the technical difficulty in designing reasonable policies.", "startOffset": 19, "endOffset": 23}, {"referenceID": 56, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 132, "endOffset": 136}, {"referenceID": 46, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 52, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 11, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 10, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 234, "endOffset": 242}, {"referenceID": 31, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 234, "endOffset": 242}, {"referenceID": 11, "context": "For instance [12, 11], consider a worker who has a choice of working in k firms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 10, "context": "For instance [12, 11], consider a worker who has a choice of working in k firms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 39, "context": "Except the conference paper [40], which is subsumed by this article.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "These result weakens if we use the 4-approximation in [17] instead of the 2 + approximation for the orienteering problem in [22].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "These result weakens if we use the 4-approximation in [17] instead of the 2 + approximation for the orienteering problem in [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "This notion was introduced by Anderson [5] in an early work in mid 1960s.", "startOffset": 39, "endOffset": 42}, {"referenceID": 61, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 54, "endOffset": 62}, {"referenceID": 26, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 5, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 58, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 27, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 1, "context": "As an example described in Agarwal etal in [2], consider the problem of presenting different websites/snippets to an user and induce the user to visit these web pages.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 116, "endOffset": 119}, {"referenceID": 62, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 196, "endOffset": 204}, {"referenceID": 44, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 196, "endOffset": 204}, {"referenceID": 25, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 243, "endOffset": 247}, {"referenceID": 54, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 277, "endOffset": 281}, {"referenceID": 2, "context": "As another example, consider an fault tolerant network application wishing to choose send packets along K independent routes [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 49, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 51, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 57, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 6, "context": "This application has also been hinted at in [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 23, "context": "Natural extensions of such correspond to Knapsack type constraints and similar problems have been discussed in the context of power allocation (when the arms are channels) [24] and optimizing \u201cTCP friendly\u201d network utility functions [51].", "startOffset": 172, "endOffset": 176}, {"referenceID": 50, "context": "Natural extensions of such correspond to Knapsack type constraints and similar problems have been discussed in the context of power allocation (when the arms are channels) [24] and optimizing \u201cTCP friendly\u201d network utility functions [51].", "startOffset": 233, "endOffset": 237}, {"referenceID": 55, "context": "Multi-armed bandit problems have been extensively studied since their introduction by Robbins in [56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 48, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 7, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 20, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 59, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 60, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 19, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 47, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 30, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 59, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 36, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 3, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 37, "context": "It may be appear Subsuming the main result of the conference paper [38].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "that we can avoid the issue by appealing to non-stochastic/adversarial MABs [9] \u2013 but they do not help in the presence of budget constraints which couples the action across various time steps.", "startOffset": 76, "endOffset": 79}, {"referenceID": 28, "context": "It is known that online convex analysis cannot be analyzed well in the presence of large number of arms and a \u201cstate\u201d that couples the time steps [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 60, "context": "The problem considered in [61] is similar in name but is different from the MaxMAB problem we have posed herein \u2014 that paper maximizes the single maximum value seen across all the arms and across all the T steps.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "The authors of [36] show that several natural index policies for the budgeted learning problem are constant approximations using analysis arguments which are different from those presented here.", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "The specific approximation ratios proved in [36] are improved upon by the results herein with better running times.", "startOffset": 44, "endOffset": 48}, {"referenceID": 43, "context": "The authors of [44] present a constant approximation for non-martingale finite horizon bandits; however, these problems require techniques that are orthogonal to those in this paper.", "startOffset": 15, "endOffset": 19}, {"referenceID": 42, "context": "The problems considered in [43] is an infinite horizon restless bandit problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "Our running times and the description of the final policies will be comparable (in a favorable way) to the time required for dynamic programming to compute the standard index policies [14, 34].", "startOffset": 184, "endOffset": 192}, {"referenceID": 33, "context": "Our running times and the description of the final policies will be comparable (in a favorable way) to the time required for dynamic programming to compute the standard index policies [14, 34].", "startOffset": 184, "endOffset": 192}, {"referenceID": 29, "context": "Recently, Farias and Madan [30] provided an 8-approximation for this problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "Previously, constant factor approximations were provided in [36, 40] for the finite horizon MAB problem (not necessarily using irrevocable policies).", "startOffset": 60, "endOffset": 68}, {"referenceID": 39, "context": "Previously, constant factor approximations were provided in [36, 40] for the finite horizon MAB problem (not necessarily using irrevocable policies).", "startOffset": 60, "endOffset": 68}, {"referenceID": 0, "context": "u\u2208Si(T ) ruzu \u2211n i=1 \u2211 u\u2208Si zu \u2264 KT \u2211 v\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i} zu \u2264 wu \u2200u \u2208 Si(T ),\u2200i zu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i", "startOffset": 108, "endOffset": 114}, {"referenceID": 65, "context": "A similar LP formulation was proposed for the multi-armed bandit problem by Whittle [66] and Bertsimas and Nino-Mora [54]; however, one key difference is that we ignore the time at which the arm is played in defining the LP variables.", "startOffset": 84, "endOffset": 88}, {"referenceID": 53, "context": "A similar LP formulation was proposed for the multi-armed bandit problem by Whittle [66] and Bertsimas and Nino-Mora [54]; however, one key difference is that we ignore the time at which the arm is played in defining the LP variables.", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "u\u2208Si(T ) (ru \u2212 \u03bb)zu \u2211 v\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i} zu \u2264 wu \u2200u \u2208 Si(T ),\u2200i zu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i (2)", "startOffset": 92, "endOffset": 98}, {"referenceID": 13, "context": "The solution presented in Lemma 6 shows that the optimum policy Li(\u03bb) satisfies zu = 0 (no play, or Gain(u) = 0) or zu = wu (play orGain(u) > 0); which are to expected using complementary slackness [14].", "startOffset": 198, "endOffset": 202}, {"referenceID": 45, "context": "By a standard application of weak duality (see for instance [46]), a (1+ ) approximate solution to (LP1) can be obtained by taking a convex combination of the solutions to LPLag(\u03bb) for two values \u03bb\u2212 and \u03bb+; these can be computed by binary search.", "startOffset": 60, "endOffset": 64}, {"referenceID": 32, "context": "The most widely used policy for the discounted version of the multi-armed bandit problem is the Gittins index policy [33].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "This problem has received significant attention, see the discussion in Section 1 and in [12] - however efficient solutions with provable bounds in the Bayesian setting has been elusive.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A classic example of such a switching cost problem can be when the costs `ij define a distance metric, which is natural in most navigational settings and was considered earlier in [40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 39, "context": "Here we will provide a X approximation for that problem improving the 12-approximation provided in [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 31, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 46, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 52, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 16, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 12, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "For any \u2208 (0, 1] the orienteering problem has a (2 + )-approximation that can be found in polynomial time [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "The authors of [17] showed that any c-approximation for K = 1 case (where we choose a single tour) extends to an (c+ 1)-approximation for the K > 1 case where we choose K tours11.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "We note that there exists approximations for asymmetric distances (directed graphs with triangle inequality) and other traversal problems in [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "We come a full circle from the discussion in Ehrenfeld [27], where the explicit connections between stopping rules for delayed feedback in the two bandit setting and scheduling policies were considered.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 2, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 1, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 2, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 14, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 53, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 15, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 43, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 144, "endOffset": 148}], "year": 2013, "abstractText": "In this paper, we consider several finite-horizon Bayesian multi-armed bandit problems with side constraints. These constraints include metric switching costs between arms, delayed feedback about observations, concave reward functions over plays, and explore-then-exploit models. These problems do not have any known optimal (or near optimal) algorithms in sub-exponential running time; several of the variants are in fact computationally intractable (NP-Hard). All of these problems violate the exchange property that the reward from the play of an arm is not contingent upon when the arm is played. This separation of scheduling and accounting of the reward is critical to almost all known analysis techniques, and yet it does not hold even in fairly basic and natural setups which we consider here. Standard index policies are suboptimal in these contexts, there has been little analysis of such policies in these settings. We present a general solution framework that yields constant factor approximation algorithms for all the above variants. Our framework proceeds by formulating a weakly coupled linear programming relaxation, whose solution yields a collection of compact policies whose execution is restricted to a single arm. These single-arm policies are made more structured to ensure polynomial time computability of the relaxation, and their execution is then carefully sequenced so that the resulting global policy is not only feasible, but also yields a constant approximation. We show that the relaxation can be solved using the same techniques as for computing index policies; in fact, the final policies we design are very close to being index policies themselves. Conceptually, we find policies that satisfy an approximate version of the exchange property, namely, that the reward from a play does not depend on time of play to within a constant factor. However such a property does not hold on a per-play basis and only holds in a global sense: We show that by restricting the state spaces of the arms, we can find single arm policies that can be combined into global (near) index policies that satisfy the approximate version of the exchange property analysis in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability. \u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42]. \u2020Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. sudipto@cis.upenn.edu. Research supported by NSF awards CCF-0644119, CCF-1117216. Part of this research was performed when the author was a visitor at Google. \u2021Department of Computer Science, Duke University, Durham, NC 27708-0129., kamesh@cs.duke.edu. Supported by an Alfred P. Sloan Research Fellowship, an award from Cisco, and by NSF grants CCF-0745761, CCF-1008065, and IIS-0964560. ar X iv :1 30 6. 35 25 v2 [ cs .D S] 1 7 Ju l 2 01 3", "creator": "LaTeX with hyperref package"}}}