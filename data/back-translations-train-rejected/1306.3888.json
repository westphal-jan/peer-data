{"id": "1306.3888", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2013", "title": "The SP theory of intelligence: an overview", "abstract": "This article is an overview of the \"SP theory of intelligence\". The theory aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme. It is conceived as a brain-like system that receives 'New' information and stores some or all of it in compressed form as 'Old' information. It is realised in the form of a computer model - a first version of the SP machine. The concept of \"multiple alignment\" is a powerful central idea. Using heuristic techniques, the system builds multiple alignments that are 'good' in terms of information compression. For each multiple alignment, probabilities may be calculated. These provide the basis for calculating the probabilities of inferences. The system learns new structures from partial matches between patterns. Using heuristic techniques, the system searches for sets of structures that are 'good' in terms of information compression. These are normally ones that people judge to be 'natural', in accordance with the 'DONSVIC' principle -- the discovery of natural structures via information compression. The SP theory may be applied in several areas including 'computing', aspects of mathematics and logic, representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information storage and retrieval, planning and problem solving, information compression, neuroscience, and human perception and cognition. Examples include the parsing and production of language including discontinuous dependencies in syntax, pattern recognition at multiple levels of abstraction and its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in Bayesian networks including 'explaining away', causal diagnosis, and the solving of a geometric analogy problem.", "histories": [["v1", "Thu, 13 Jun 2013 11:51:17 GMT  (406kb,D)", "https://arxiv.org/abs/1306.3888v1", "arXiv admin note: text overlap witharXiv:cs/0401009,arXiv:1303.2071,arXiv:cs/0307010,arXiv:1212.0229,arXiv:1303.2013"], ["v2", "Tue, 2 Jul 2013 16:31:15 GMT  (408kb,D)", "http://arxiv.org/abs/1306.3888v2", "arXiv admin note: text overlap witharXiv:cs/0401009,arXiv:1303.2071,arXiv:cs/0307010,arXiv:1212.0229,arXiv:1303.2013"], ["v3", "Sun, 8 Sep 2013 12:16:05 GMT  (404kb,D)", "http://arxiv.org/abs/1306.3888v3", "arXiv admin note: text overlap witharXiv:cs/0401009,arXiv:1303.2071,arXiv:cs/0307010,arXiv:1212.0229,arXiv:1303.2013"], ["v4", "Wed, 7 Jan 2015 11:44:26 GMT  (1527kb,AD)", "http://arxiv.org/abs/1306.3888v4", "arXiv admin note: text overlap witharXiv:cs/0401009,arXiv:1303.2071,arXiv:cs/0307010,arXiv:1212.0229,arXiv:1303.2013"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:cs/0401009,arXiv:1303.2071,arXiv:cs/0307010,arXiv:1212.0229,arXiv:1303.2013", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j gerard wolff"], "accepted": false, "id": "1306.3888"}, "pdf": {"name": "1306.3888.pdf", "metadata": {"source": "CRF", "title": "The SP theory of intelligence: an overview\u2217", "authors": ["J Gerard Wolff"], "emails": ["jgw@cognitionresearch.org;"], "sections": [{"heading": null, "text": "* Now published as The SP theory of intelligence: an overview (J G Wolff, Information, 4 (3), 283-341, 2013, doi: 10.3390 / info4030283). \u2020 Dr. Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch, Menai Bridge, UK; jgw @ cognitionresearch.org; + 44 (0) 1248 712962; + 44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org.ar Xiv: 130 6.38 88v4 [cs.AI] 7J an2 01its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in bayesian networks including \"explain away,\" causal diagnosis, and the solving of a geometric analogy problem.Keywords: information compression, intelligence, multiple reasability, probability, resolving, knowledge, natural."}, {"heading": "1 Introduction", "text": "The SP Theory of Intelligence, in development since about 1987.1, aims to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme. \"SP\" is an abbreviation for simplicity and power, since the compression of any body of information, I, can be seen as a process of reducing the informational \"redundancy\" within me, thus increasing its \"simplicity,\" while the most comprehensive description of the theory as it stands is contained with many examples in Wolff (2006a). But this book, with more than 450 pages, is too long to serve as an introduction to the theory. This article aims to meet this need."}, {"heading": "2 Origins and motivation", "text": "In the following subsections, the origins of the SP theory are outlined, how it relates to other research results and how it has developed."}, {"heading": "2.1 Information compression", "text": "Much of the inspiration for the SP theory is a body of research, pioneered by Fred Attneave (1954), Horace Barlow (1959; 1969), and others, which show that several aspects of the functioning of brains and nervous systems can be understood in terms of information compression.4 For example, if we look at a scene with two eyes, the image on the retina of the left eye is almost exactly the same as the image on the retina of the right eye, but our brain fuses the two images into a single perception and thus compresses the information (Barlow, 1969).5 More immediately, the theory has grown out of my own research, the development of models of unattended learning of a first language in which the importance of information compression became increasingly clear (e.g. Wolff, 1988).6 The theory also refers to principles of \"minimum length of coding,\" advanced by Solomonoff (1964), and others."}, {"heading": "2.2 The matching and unification of patterns", "text": "In SP theory, pattern matching and unification is considered closer to the foundation of information compression than more mathematical techniques such as wavelengths or arithmetic coding, and closer to the foundation of information processing and intelligence than concepts of probability, for example. A working hypothesis in this research program is that by sticking to and standardizing relatively simple, \"primitive\" concepts of matching patterns, there is a better chance of breaking through unnecessary complexity and gaining new insights and better solutions to problems. The mathematical basis of wavelengths, arithmetic coding, and probabilities can be based even on pattern matching and unification (BK, chapter 10)."}, {"heading": "2.3 Simplification and integration of concepts", "text": "In accordance with Occam's Razor, the SP system aims to combine conceptual simplicity with descriptive and explanatory power. Apart from this widely accepted principle, the drive for simplification and integration of concepts in this research program is motivated in part by Allen Newell's criticism of some types of research in cognitive science (Newell, 1973) and in part by the apparent fragmentation of research in artificial intelligence and mainstream computing, with their myriad concepts and many specializations. [8] And it plays with the resurgence of interest in understanding artificial intelligence as a whole (see, for example, Schmidhuber et al., 2011) and with research on \"natural computing\" (DodigCrnkovic et al., 2004)."}, {"heading": "2.4 Transparency in the representation of knowledge", "text": "This research assumes that knowledge in the SP system should normally be transparent or comprehensible, similar to the \"symbolic\" tradition of artificial intelligence (see also Section 5.2), and differs from the kind of \"subsymbolic\" representation of knowledge that is the rule, for example, in \"neural networks,\" as generally conceived in computer science. As we will see in Section 7 and elsewhere in this article, SP patterns in the context of multiple alignment can serve to represent a variety of kinds of knowledge in symbolic forms."}, {"heading": "2.5 Development of the theory", "text": "In developing the theory, it was obvious at an early stage that existing systems - such as my models of language acquisition 9 and systems such as Prolog - would need to be radically rethought in order to achieve the goal of simplifying and integrating ideas across a wide range. The first published version of SP theory (Wolff, 1990) described \"some unifying ideas in the computer field.\" Early work on the SP computer model focused on developing an improved version of the technique of \"dynamic programming\" to align two sequences (see BK, Appendix A) as a possible way to model human flexibility in pattern recognition, language analysis and similarity. Around 1992, it became clear that the explanatory field of theory could be greatly expanded by creating alignments of 2, 3 or more sequences, similar to the concept of \"multiple alignment\" in bioinformatics. This idea was developed and adapted in new versions of the SP model and incorporated into new methods for unattended learning."}, {"heading": "3 Introduction to the SP theory", "text": "The main elements of SP theory are: 9See www.cognitionresearch.org / lang learn.html \u2022 SP theory is conceived as an abstract brain-like system that can obtain \"new\" information about its senses in an \"input\" perspective, and stores some or all of it in its memory as \"old\" information that is schematically represented in Figure 1. There is also an \"output\" perspective described in Section 4.5. \u2022 The theory is realized in the form of a computer model introduced in Section 3.1, and will be described more fully later. \u2022 All new and old information is expressed as arrays (patterns) of atomic symbols in one or two dimensions."}, {"heading": "3.1 The SP computer model", "text": "SP theory is most fully realized in the SP70 computer model, with multi-alignment skills and unattended learning.11 This model is referred to as SP model, although in some cases examples come from a subset of the model or somewhat earlier predecessors of the model. SP model and its predecessors played a key role in developing the theory: \u2022 As an antidote to indeterminacy. As with all computer programs, the processes must be sufficiently detailed to ensure that the program actually functions.11 At the time this arXiv version of the article was created, the latest version of SP computer model is SP71. The source code for SP71 with an executable Windows file and some other files is located in the supplementary files in arXiv for this version of the article. \u2022 By providing a practical means of coding the simple but important mathematics underlying the SP theory, and performing relevant calculations, including quick calculations of the strengths proposed by the 2 and 3 means, can be tested."}, {"heading": "3.2 The SP machine", "text": "The SP model can be considered as the first version of the SP machine, as an expression of the SP theory and as a means of applying it. A useful step in the development of SP theory would be the creation of a highly parallel, open source version of the SP machine, which is accessible via the web and has a good user interface.12 This would provide researchers with a means to explore what can be done and improved with the system. How things can evolve is shown schematically in Figure 2.The highly parallel search mechanisms in all existing Internet search engines would probably be a good basis for the proposed development."}, {"heading": "3.3 Unfinished business", "text": "Like most theories, SP theory has shortcomings, but it seems that they can be overcome. Presently, the most immediate problems are: 12As in ordinary search engines, and indeed in the brains of humans and other animals, a high degree of parallelism is required to achieve fast processing with large amounts of data. Preliminary processing can be done using existing techniques for identifying perceptual characteristics at a low level (Prince, 2012, Chapter 13). \u2022 Unsupervised learning. One limitation of the SP computer model in its current form is that it cannot learn middle levels of abstraction in grammars (e.g. phrases and clauses) and cannot learn the kind of discontinuous dependencies in the natural language syntax described in Sections 8.1 to 8.3. I believe that these problems are solvable and that their solution will greatly improve the system's capabilities for unsupervised learning of structures in data (Section 5)."}, {"heading": "4 The multiple alignment concept", "text": "The concept of multiple alignment in bioinformatics has been adopted from a similar concept in bioinformatics, which involves a process of arranging two or more DNA sequences or amino acid sequences in rows or columns so that matching symbols - as many as possible - are orthogonally aligned in columns or rows. Multiple alignments such as these are normally used in the computational analysis of (symbolic representations of) sequences of DNA bases or sequences of amino acid remnants as part of the process of explaining the structure, function or evolution of the corresponding molecules. An example of this type of multiple alignment is shown in Figure 3.As shown in bioinformatics, multiple alignment in the SP system is an arrangement of two or more patterns in rows (or columns), with a pattern in each row (or column) that represents a new grammatic.13 The main difference between the two concepts is that in bioinformatics all sequences have the same status."}, {"heading": "4.1 Coding and the evaluation of an alignment in terms", "text": "An example of such a pattern in Figure 4 is \"D\" and \"0\" at the beginning of \"D 0 t h i s # D\" (line 6), and \"N\" and \"1\" at the beginning of \"N\" at the beginning of \"N\" and \"N\" at the beginning of \"N.\" (line 8) Associated with any type of symbol (where a \"type\" of symbol is a set of symbols that correspond exactly to each other) is a notional code or bit pattern used to distinguish the given type from all others."}, {"heading": "4.1.1 Compression difference and compression ratio", "text": "Given a code pattern such as \"S 0 1 0 1 0 # S,\" we can calculate a \"compression difference\" as follows: CD = BN \u2212 BE (1) or a \"compression ratio\" such as: CR = BN / BE, (2) where BN is the total number of bits in the New Pattern symbols that match old symbols in alignment, and BE is the total number of bits in the symbols in the code pattern, and the number of bits for each symbol is calculated using the above-mentioned Shannon-Fano-Elias scheme. CD and CR are each an indication of how effective the New Pattern (or those parts of the New Pattern that match symbols within old patterns in alignment) can be in relation to the old patterns that occur in the given multi-alignment."}, {"heading": "4.2 The building of multiple alignments", "text": "This section outlines how the SP model builds multiple alignments. Further details can be found in BK (Section 3.10). Multiple alignments are built up in stages, with pairs of alignments and pattern alignments. At each stage, any partially constructed multiple alignment can be processed as if it were a basic pattern and transferred to later stages. This is broadly some programs for creating multiple alignments in bioinformatics. 15 At all stages, the goal is to encode new information economically with respect to old information and to sort out multiple alignments that are poorly evaluated.The model can create old patterns for itself, as described in Section 5, but if the formation of multiple alignments is the focus of interest, old patterns can be provided by the user. In all cases, new patterns must be provided by the user."}, {"heading": "4.2.1 Finding good matches between patterns", "text": "Figure 5 shows a simple example of how the SP model finds good complete and partial matches between a \"query\" of atomic symbols (alphabetical characters in this example) and a \"database\" string: 1. The query is processed from left to right, one symbol at a time. 2. Each symbol in the query is sent de facto to to every symbol in the database to create a yes / no match in each case. 3. Every positive match (match) between a symbol from the query and a symbol in the database is recorded in a hit structure that is shown in the figure. 4. When the space allocated to the hit structure is exhausted at any point in time, the hit structure is cleaned up: The leaf nodes of the tree are sorted in reverse order of their probability values and each leaf node in the bottom half of the hit structure is extracted from space along with all nodes on its path that are not shared with any other path."}, {"heading": "4.2.2 Noisy data", "text": "Because of the way each model looks for a global optimum in the creation of multi-alignments, it does not depend on the presence or absence of certain features or combinations of features. Up to a certain point, plausible results can be achieved in the face of errors in omission, picking and substitution in the data, as illustrated by the two multi-alignments in Figure 6, where the New Pattern in line 0 of (b) is the same sentence as in (a) (\"t w o k i t t e n s p l a y\"), but without the \"w\" in \"t w o,\" the replacement of \"m\" in \"k i t t e n s\" and the addition of \"x\" within the word \"p l a y.\" Despite these errors, the best multi-alignment created by the SP model, as shown in (b), is probably the one that we intuitively consider \"right.\" This ability to understand loud data, despite its incomprehensible nature, is very strong in spite of the incomprehensibility and incomprehensibility of our language systems."}, {"heading": "4.3 Computational complexity", "text": "Considering the matching and unification of patterns, it is not difficult to see that for each body of information I, except for very small examples, there are a huge number of alternative ways in which patterns can be matched against each other, there will normally be many alternative ways in which patterns can be unified, and an exhaustive search is not comprehensible (BK, Section 2.2.8.4). However, this type of matching becomes quite practicable."}, {"heading": "4.4 Calculation of probabilities associated with multi-", "text": "As described in BK (Chapter 7), the creation of multiple alignments in the SP framework supports several kinds of probabilistic reasoning. The basic idea is that any old symbol in a multiple alignment that is not aligned to a new symbol is a conclusion that can be drawn from the multiple alignment. This section outlines how the probabilities for such conclusions can be calculated."}, {"heading": "4.4.1 Absolute probabilities", "text": "Each sequence of L symbols drawn from an alphabetical type of | A | alphabetical type represents a point in a series of N points where N is calculated as follows: N = | A | L. (5) If we assume that the sequence is random or almost so, 19 meaning that the N points are equally probable or almost so, the probability of any point (representing a sequence of length L) is close to: pABS = | A | \u2212 L. (6) This equation can be used to calculate the absolute probability of the code pattern that can be derived from a given multiple orientation (as described in Section 4.1). This number can also be considered as the absolute probability of any conclusions that can be drawn from the multiple orientation. In this calculation, L is the sum of all bits in the symbols of the code pattern and | A | is 2.As we see (Section 4.3), Equation 6 can be replaced with an advantage by a generalized value of L (see Section 1.1)."}, {"heading": "4.4.2 Relative probabilities", "text": "From a practical point of view, we are usually interested in the relative probability values calculated as follows: 1. for the multi-alignment that has the highest CD (which we will call reference multi-alignment), identify the reference set of symbols in New, i.e. the symbols from New that are encoded by multi-alignment, no more or less. 2. compute a multi-alignment reference set containing the reference multi-alignment and all other multi-alignments (if any) that encode exactly the reference set of the symbols from New, no more or less. 3. calculate the sum of the values for pABS in the multi-alignment reference set: pA SUM = i = R \u00b2 i = 1 pABSi = 1 pABSi (7), where R is the size of the reference set of the multi-alignment and pABSi is the value of pABS. For multi-alignment Li = R = R \u00b2 in the reference alignment (REM = each effective one)."}, {"heading": "4.4.3 A generalisation of the method for calculating absolute and relative probabilities", "text": "The value of L, calculated as described in Section 4.4.1, can be considered the informative \"cost\" of encoding the New Symbol or symbols that appear in the Multi-Alignment without those New Symbols that did not appear in the Multi-Alignment. That's fine, but it is somewhat restrictive because it means that if we want to calculate relative probabilities for two or more Multi-Alignments, they all have to encode the same symbol or symbols from New Alignment. (9) Where L is the total number of bits in the symbols in the code patterns (as in Section 4.4.1), Nnot is the total number of bits in the New Symbols that did not appear in the Multi-Alignment."}, {"heading": "4.4.4 Relative probabilities of patterns and symbols", "text": "It often happens that a certain pattern from the old or a certain symbol type occurs within patterns from the old in more than one of the multiple alignments of the reference set. In cases such as these, one would expect the relative probability of the pattern or symbol type to be higher than if it occurs only in one multiple alignment. To account for this type of situation, the SP model calculates relative probabilities for individual patterns and symbol types in the following way: 1. Compute a series of patterns from the old, each of which appears at least once in the reference set of multiple alignments. No single pattern from the old should appear more than once in the set. 2. Calculate a value for each pattern for its relative probability as the sum of the pREL values for the multiple alignments in which it appears. If a pattern appears more than once in a multiple alignment, it is counted only once for that multiple alignment. 3. Compile a series of symbol types that occur somewhere in the patterns determined in step 2.4."}, {"heading": "4.5 One system for both the analysis and the produc-", "text": "A potentially useful feature of the SP system is that the processes used to analyze or analyze a new pattern in relation to old patterns and generate an economic encoding of the new pattern can also work the other way around to create the new pattern from its encoding. This is the \"output\" perspective that is shown in Section 3.If the new pattern is the code sequence \"S 0 1 0 1 0 # S\" (as described in Section 4), and if the old patterns are the same as those shown in Figure 4, then the best multiple alignment the system has found is that shown in Figure 7. This multiple alignment contains the same words as the original sentence (\"t h i s b o y l o v e a t i r l\"), in the same order as the original. Readers familiar with Prolog will recognize that this process of recovering the original sentence from its encoding is similar."}, {"heading": "5 Unsupervised learning", "text": "As mentioned in Section 2.1, part of the inspiration for SP theory was a research program to develop models of unsupervised language learning. However, although the SNPR model (Wolff, 1982) has been quite successful in deriving plausible grammars from examples of English art language, it has proved quite unsuitable as a basis for SP theory. In order to take into account other aspects of intelligence such as pattern recognition, reasoning and problem solving, it was necessary to develop a completely new conceptual framework with multiple orientation at the centre.So there is now the curious paradox that while SP theory is based on work on unsupervised learning and this type of learning plays a central role in theory, the SP model essentially does the same things as the previous model, and with similar limitations (Sections 3.3 and 5.1.4).But I believe that the new conceptual framework has many advantages that it provides a much more solid foundation for further development."}, {"heading": "5.1 Outline of unsupervised learning in the SP model", "text": "The sketch of the SP model in this section aims to be sufficiently detailed to gain a good intuitive understanding of how it works. It can be found in much more detail in BK (Chapter 9). In addition to the processes for building multiple alignments, the SP model has processes for deriving old patterns from multiple alignments and evaluating newly created old patterns in terms of their effectiveness for economically coding new information and sorting out low scoring sets. The system not only records statistical information, it uses this information to learn new structures."}, {"heading": "5.1.1 Deriving Old patterns from multiple alignments", "text": "As mentioned in Section 3, the SP system is conceived as an abstract brain-like system that can receive \"new\" information about its senses in \"input mode,\" and store some or all of it as \"old\" information. [21] If the baby has never heard anything similar, then, if it is stored at all, the new information can be stored as a relatively simple copy, something like the old pattern shown in line 1 of the multiple alignment in part (a) of the figure. [21] Now imagine that the information has been stored, then, if it is stored at all, that the new information is stored as a relatively simple copy, something like the old pattern shown in line 1 of the multiple (a) of the figure. [Now] imagine that the information has been stored, and that at a later stage the baby hears the boundaries of # n, the grammars. \""}, {"heading": "5.1.2 Evaluating and selecting sets of newly-created Old patterns", "text": "The example just described shows how old patterns can be derived from multiple alignment, but it gives a highly misleading impression of how the SP model actually works. In practice, the program forms many multiple alignments that are much less tidy than the one shown, and it creates many old patterns that are clearly \"wrong.\" However, the program includes procedures for evaluating sample candidates (\"grammars\") and sorting out those that perform poorly economically in terms of their effectiveness in encoding the new information. Usually, from all the confusion, it can derive one or two \"best\" grammars, and these are usually those that intuitively appear to be \"correct,\" or almost so. Generally, the program can abstract one or more plausible grammars from a sample of the English language of art, including words, grammatical categories of words, and sentence structures that align with the principles of minimal length encoding (Solomonoff, 1964)."}, {"heading": "5.1.3 Plotting values for G, E and T", "text": "Figure 9 shows cumulative values for G, E and T, as the SP model looks for good grammars for a sequence of 8 new patterns, each representing a sentence. Each dot in each of the bottom three diagrams represents the relevant value (on the scale to the left) of the best grammar found after processing a given pattern from New. The graph called \"O\" shows cumulative values on the scale to the left for the sequence of new patterns. The graph called \"T / O\" shows the compression achieved (on the scale to the right)."}, {"heading": "5.1.4 Limitations in the SP model and how they may be overcome", "text": "As already mentioned (Section 3.3), there are two main weaknesses in the processes of unattended learning in the SP model in its current form: the model does not learn intermediate levels in grammar (phrases or sentences) or discontinuous dependencies of the kind described in Sections 8.1 to 8.3. It seems that a certain reorganization of the learning processes in the model solves both problems. What seems necessary is a greater focus on the principle that, with appropriately constructed old patterns, multiplexigrams can be created without the kind of mismatch between patterns that can be seen in Figure 8 (a) (\"g i r l\" and \"b o y\" do not match), and that such multiple adjustment can be treated as if it were a simple pattern. This reform should facilitate the discovery of multi-level structures and the discovery of discontinuous structures in the sense that they can bridge intervening structures."}, {"heading": "5.1.5 Computational complexity", "text": "As with multi-alignment building (Section 4.3), the computational complexity of learning in the SP model is kept under control by truncating the search tree in appropriate places to find grammars that are reasonably good and not necessarily perfect. In a serial processing environment, the temporal complexity of learning in the SP model is estimated at O (N2), with N being the number of patterns in New. In a parallel processing environment, the temporal complexity O (N) can approximate depending on how well parallel processing is applied. In serial or parallel environments, the spatial complexity is estimated at O (N)."}, {"heading": "5.2 The discovery of natural structures via informa-", "text": "In our dealings with the world, certain types of structures appear to be more prominent and useful than others: in natural languages, there are words, phrases, and sentences; we understand the visual and tactile worlds composed of individual \"objects\"; and conceptually, we recognize classes of things like \"person,\" \"house,\" \"tree,\" etc. It seems that these \"natural\" types of structures are significant in our thinking because they provide a means of compressing sensory information, and that compressing information provides the key to their learning or discovery. At first glance, this looks like nonsense, because popular information compression programs, such as those based on the LZW algorithm, or programs for JPEG compression of images, do not seem to recognize anything similar, words, objects, or classes. But these programs are designed to work quickly on low-performance computers, the static programs designed to be relatively thorough in their compression of information from natural structures to be revealed."}, {"heading": "5.3 Generalisation, the correction of overgeneralisa-", "text": "In this context, it should be noted that in the region between the middle and outer shell, we learn a \"correct\" version of our native language, even though it is marked in the figure as \"dirty data\" (sentences that are not complete, wrong starts, words that are mispronounced, and more).One possible answer is that mistakes by parents, teachers, and others are corrected, but the weight of the evidence is that children can learn their first language without this support (sentences that are incomplete, wrong approaches, words that are mispronounced)."}, {"heading": "5.4 One-trial learning and its implications", "text": "In many theories of learning, the process is considered gradual: behavior is progressively shaped by rewards or punishments or other types of experience. However, any theory of learning in which the process is necessarily gradual is at odds with our usual experience that we can and do learn things from a single experience, especially if that one experience is very important to us (BK, Section 11.4.4.1). SP theory takes into account one-step learning in the way that the system can directly store new information, and the gradual nature of, for example, language learning can be explained by the complexity of the process of sifting and sorting the many alternative patterns of candidates to find one or more sentences that are good in terms of information compression (BK, Section 11.4.4.2). 24If an error is not rare, it is likely to acquire the status of a dialect or idiolectic variation and no longer be considered an error."}, {"heading": "6 Computing, mathematics, and logic", "text": "In BK (chapters 4 to 11) I argued that the SP system is equivalent to a universal Turing machine (Turing, 1936) in the sense that everything that can be calculated with a Turing machine can in principle also be calculated with an SP machine. \"Principal\" qualification is necessary because the SP theory is not yet fully mature and there are still some weaknesses in the SP computer models. At the core of the argument is that the operation of a post-canonical system (Post, 1943) can be understood in the sense of SP theory and since it is accepted that the post-canonical system of the Turing machine (as a computer system) is equivalent, the Turing machine can also be understood in the sense of SP theory.The main differences between the SP theory and earlier theories of compression are that the earlier theory of compression is not based on compression, but rather on compression."}, {"heading": "6.1 Conventional computing systems", "text": "In conventional computer systems, the compression of information can be seen in the concordance of patterns with at least implicit unification of patterns that match each other - processes that occur in different forms (BK, Chapter 2); and three basic techniques for compressing information - chunking-with-codes, schema-plus-correction, and run-length coding - can be seen in various forms in the organization of computer programs (ibid.)."}, {"heading": "6.2 Mathematics and logic", "text": "Similarly, several structures and processes in mathematics and logic can be interpreted with respect to information compression by mapping and unifying patterns and the above-mentioned compression techniques (BK, Chapter 10). For example, multiplication (as repeated addition) and exponentiation (as repeated multiplication) can be seen as examples of log length coding; a function with parameters can be seen as an example of scheme plus correction; the chunking with codes technique can be seen in the organization of number systems, etc."}, {"heading": "6.3 Computing and probabilities", "text": "If it is indeed Turing's equivalent, as suggested above, and if the Turing machine is seen as a definition of \"computing,\" then we can conclude that computing is fundamentally probabilistic. \u2022 It seems that computing, mathematics, and logic are more likely than our ordinary experience with them might suggest. Gregory Chaitin wrote, \"I have recently been able to take another step down the path set by Go \ufffd del and Turing. By translating a particular computer program into an algebraic equation of a type familiar even to the ancient Greeks, I have shown that there is a randomness in the branch of pure mathematics known as number theory."}, {"heading": "7 Representation of knowledge", "text": "Within the multiple alignment framework (Section 4), SP patterns can be used to represent different types of knowledge, including grammars for natural languages, 26 ontologies, 27 class hierarchies with inheritance of attributes, including cross-classification or multiple inheritance, 28 sub-hierarchies and their integration with class hierarchies, 29 decision networks and trees, 30 relational tuples, 31 if-then rules, 32 associations of medical signs and symptoms, 33 causal relationships, 34 and concepts in mathematics and logic such as \"function,\" \"\" variable, \"\" value, \"quantity\" and \"type definition.\" 35The use of a simple format to represent knowledge facilitates the seamless integration of different types of knowledge."}, {"heading": "8 Natural language processing", "text": "One of the main strengths of the SP system is the processing of natural language (BK, Chapter 5): \u2022 As shown in Figures 4, 6 and 7, grammatical rules, including words and their grammatical markers, can be represented with SP patterns. \u2022 Both parsing and the production of natural language can be modelled by building multiple alignments (Section 4.5; BK, Section 5.7). \u2022 The system provides a simple but effective means of displaying discontinuous dependencies in syntax (Sections 8.1 to 8.3 below; BK, Sections 5.4 to 5.6). \u2022 The system can also model non-syntactic \"semantic\" structures (BK, Section 5.3)."}, {"heading": "8.1 Discontinuous dependencies in syntax", "text": "The way in which the SP system can record discontinuous dependencies in syntax can be seen in both parsars in Figure 6. The pattern in line 8 of each multiple alignment records the syntactic dependence between the plural noun phrase marked \"Vp\" (\"t w o k i t e n s\") which is the object of the sentence - marked \"Np\" - and the plural verb phrase (\"p l a y\") - marked \"Vp\" - which belongs there.This type of dependence is discontinuous because it can bridge arbitrarily large amounts of intervening structures such as \"from the West\" in a sentence such as \"winds from the West are strong.\" This method of marking discontinuous dependencies can take into account overlapping dependencies such as number dependencies and gender dependencies in languages such as French (BK, Section 5.4) as overlapping dependencies in the English structure."}, {"heading": "8.2 Two quasi-independent patterns of constraint in", "text": "The primary constraints can be expressed with this sequence of symbols, M H B V, which should be interpreted as follows: \u2022 Each letter represents a category for a single word: - \"M\" stands for \"modal\" verbs such as \"will,\" \"etc.-\" H \"stands for one of the different forms of the verb\" to have. \"-\" Each of the two instances of \"B\" stands for one of the different forms of the verb \"to be.\" - \"V\" stands for the main verb that can be. \"-\" H \"stands for one of the different forms of the verb\" to have. \"-\" Each letter stands for itself. \""}, {"heading": "8.3 Multiple alignments and English auxiliary verbs", "text": "Without giving all the details in BK (section 5,5), we can see from Figures 13 and 14 how the primary and secondary constraints can be applied in the multiple alignment frame. In each figure, the sentence to be analyzed is presented as a new pattern in column 0. Primary constraints are applied by matching the symbols in the old patterns in the remaining columns, with the patterns consistently interlocking so that they recognize sentences of the form \"M H B B V,\" with the options described above. In Figure 13,36, the secondary constraints apply as follows: \u2022 The first verb \"is\" is marked with the finite form (with the symbol \"FIN\" in columns 5 and 7). The same word is also a form of the verb \"to be\" (with the symbol \"B\" in columns 4, 5 and 6)."}, {"heading": "It will have been being washed", "text": "In Figure 14, the secondary constraints apply as follows: \u2022 The first verb \"is marked modally\" (with \"M\" in columns 7, 8, and 14) \u2022 The second verb \"to have\" is marked as an infinitive (with \"INF\" in columns 11 and 14) and it is also marked as the form of the verb \"to have\" (with \"H\" in columns 11, 12, and 15) \u2022 The fact that a modal verb must follow the infinitive form is marked with the pattern \"M INF\" in column 14. \u2022 The third verb \"to have\" is marked as the form of the verb \"to be\" (with \"B\" in columns 2, 3, and 16). Due to its position in the parsing, we know that it is an instance of the second \"B\" in the sequence. \"This verb is also marked as belonging to the en category (with\" EN \"in column 2,\" followed by \"the fourth\" in column H)."}, {"heading": "9 Pattern recognition", "text": "The system also has some useful features as a pattern recognition framework (BK, (Chapter 6): \u2022 It can model pattern recognition at multiple levels of abstraction, as described in BK (Section 6.4.1), and with the integration of class inclusion relationships with holistic hierarchies (Section 9.1; BK, Section 6.4.1). \u2022 The SP system can accommodate \"family resemblance\" or polythetic categories, which means that detection is not dependent on the presence of a particular feature or combination of features. \u2022 This is because there may be alternatives in any or all places within a pattern, and also in the way the system can tolerate errors in data (next point). \u2022 The system is robust in the face of errors of omission, assignment or substitution in data (Section 4.2.2). \u2022 The system facilitates seamless integration of pattern recognition with other aspects of intelligence: reasoning, learning, problem solving, etc. \u2022 A probability for a given area of identification or a given pattern pattern (Section 4.-) can be considered in a given pattern pattern pattern (Section 4.-)."}, {"heading": "9.1 Part-whole hierarchies, class hierarchies, and their", "text": "integrationA strength of the multiple alignment concept is that it provides a simple but effective vehicle for displaying and processing class-inclusion hierarchies, partial-whole hierarchies, and their integration. Figure 15 shows the best multi-alignment that the SP model has found with the New Pattern. From this multi-alignment we can conclude that the unknown being is an animal (column 1), a mammal (column 2), a cat (column 3), and the specific individual \"Tibs\" (column 4). From this multi-alignment we can conclude that the unknown being is an animal (column 1), a mammal (column 2), a cat (column 3), and the specific individual \"Tibs\" (column 4).The framework also provides for the representation of heterarchies or cross-classifications: a particular being, such as \"Jane\" (or a class of entities), is likely to be divided into two or several classes."}, {"heading": "9.2 Inference and inheritance", "text": "In the example just described, we can conclude very directly from the multiple arrangement that the plant provisionally identified as the meadow butterfly is photosynthesizing (column 2), has five petals (column 6), is toxic (column 5), and so on. As in other object-oriented systems, the first of these attributes was \"inherited\" from the class \"plants,\" the second from the class \"Ranunculus,\" and the third from the class \"Ranunculaceae.\" This kind of conclusion illustrates the often-mentioned close relationship between pattern recognition and reasoning (see also Pothos and Wolff, 2006).37 Although the term \"heterarchy\" is not widely used, it can be useful as a means of referring to hierarchies in which, as in the example in the text, a particular node may appear in two or more superordinate knots that are not themselves hierarchically related."}, {"heading": "10 Probabilistic reasoning", "text": "The SP system can model different types of thinking, including inheritance of attributes (as just described), single-level \"deductive\" thinking, abductive thinking, thinking with probable decision networks and decision trees, thinking with \"rules,\" non-monotonous thinking and thinking with default values, thinking in Bayesian networks (including \"clearing the way\"), causal diagnosis and reasoning that is not supported by evidence (BK, Chapter 7). Since these different types of thinking all proceed from a computational framework (multiple orientation), they can be considered aspects of a process that works individually or together without cumbersome limitations. Plausible lines of reasoning can be achieved even if relevant information is incomplete. Probabilities of conclusions can be calculated, including extreme values (0 or 1) in the case of logical \"derivatives.\" A selection of examples is described in the following subsections."}, {"heading": "10.1 Nonmonotonic reasoning and reasoning with de-", "text": "Conventional deductive thinking is monotonous, because conclusions drawn on the basis of current knowledge cannot be invalidated by new knowledge: the conclusion that \"Socrates is mortal,\" derived from \"All men are mortal\" and \"Socrates is human,\" remains valid for all time, regardless of what we learn later. In contrast, the conclusion that \"Tweety is likely to fly\" is based on the assertions \"Most birds fly\" and \"Tweety is a bird\" non-monotonous, because it can change when we learn, for example, that Tweety is a penguin. This section presents some examples showing how the SP system can accommodate non-monotonous thinking."}, {"heading": "10.1.1 Typically, birds fly", "text": "The idea that (all) birds can fly can be expressed with an SP pattern such as \"Bd Bird Name # Name Canfly Warm-blooded wing feathers... # Bd.\" This is, of course, an exaggerated simplification of the real facts, because although it is true that most birds fly, we know that there are flightless birds such as ostriches, penguins and kiwis. To model these facts more accurately, we need to change the pattern describing birds to be something like: \"Bd Bird Name # Name f Warm-blooded wing feathers... # Bd. And in our database of old patterns we need to add patterns such as this: Default Bd f Canfly # f # Bd # Default P Penguin Bd f Cannotfly # f # Bd... # P"}, {"heading": "O ostrich Bd f cannotfly #f #Bd ... #O.", "text": "The pair of symbols \"f # f\" in \"Bd bird name # name f # f warm-blooded wing feathers... # Bd\" works like a \"variable\" that can take on the value \"canfly\" if a particular bird class can fly and \"canotfly\" if a bird species cannot fly. \"P penguin Bd f canotfly # f # Bd... # P\" shows that penguins cannot fly and likewise the pattern \"O ostrich Bd f cannotfly # f # Bd... # O\" shows that ostriches cannot fly. The pattern \"Default Bd f canfly # f # Bd # Default,\" which has a much higher frequency than the other two patterns, represents the default value for the variable that is \"canfly.\" Note that all three of these patterns contain the pair of symbols \"Bd... # Bd,\" which shows that the corresponding classes are all subclasses of birds."}, {"heading": "10.1.2 Tweety is a bird so, probably, Tweety can fly", "text": "If the SP model with \"bird tweety\" in New and the same patterns in Old as before is modified as described above, the three best multiple alignments are those shown in Figures 17, 18 and 19. The first multiple alignment tells us that Tweety could be the typical bird that can fly with a relative probability of 0.66; the second multiple alignment tells us that Tweety could be an ostrich with a relative probability of 0.22 and as such could not fly; and the third multiple alignment tells us that Tweety could be a penguin with a relative probability of 0.12 and could not fly. The probability values in this simple example are derived from estimated frequencies that are almost certainly not ornithologically correct."}, {"heading": "10.1.3 Tweety is a penguin, so Tweety cannot fly", "text": "Figure 20 shows the best multiple alignment that the SP model will find when running again, with \"Penguin Tweety\" in New instead of \"Bird Tweety.\" This time, there is only one multiple alignment in the reference theorem and its relative probability is 1.0. Accordingly, all conclusions we can draw from this multiple alignment have a probability of 1.0. In particular, based on available knowledge, we can be certain that Tweety cannot fly. Similarly, if Tweety were an ostrich, we could say with confidence (p = 1.0) that he or she could not fly."}, {"heading": "10.2 Reasoning in Bayesian networks, including \u2018ex-", "text": "A Bayesian network is a directed acyclic graph, as shown in Figure 21, below, where each node has zero or more \"inputs\" (connections with nodes that can affect the given node) and one or more \"outputs\" (connections to other nodes that the given node can affect), each of which contains a set of conditional probability values, each of which can trigger the probability of a given initial value for a given input value or a combination of input values. With this information, conditional probabilities of alternative outputs for any node for any given combination of inputs can be calculated. By combining these calculations for sequences of nodes, probabilities can be spread through the network from one or more \"start nodes\" to one or more \"finish\" nodes. This section describes how the SP system can perform this type of probability calculations, and some advantages over Bayesian networks. Judea Pearl (1997) describes the true explanation for this phenomenon as \"B.\""}, {"heading": "10.2.1 Representing contingencies with patterns and frequencies", "text": "To see how this phenomenon can be understood in the sense of SP theory, we first look at the series of patterns shown in Figure 22, which are stored in old ones. Patterns in the figure show events that occur together in some mental sample of the \"world\" along with their frequencies of occurrence in the sample. As with other knowledge-based systems, we will assume that the \"closed world\" assumption applies, so that the absence of a pattern can be understood as meaning that the corresponding combination of events did not occur in the period in which observations were made. 38The first pattern (\"intrusion alarm\") shows that there were 1,000 occasions in which an intrusion took place and the alarm went off, and the second pattern (\"earthquake\") shows only 20 occasions in which there was an earthquake and the alarm went off (presumably triggered by the earthquake)."}, {"heading": "10.2.2 Approximating the temporal order of events", "text": "In these patterns, and in the multiple arrangements shown below, the left order of the symbols can be considered an approximation of the sequence 39Some of the frequencies shown in Figure 22 are intended to reflect the two probabilities suggested for this example in Pearl (1997, p. 49):... \"the [alarm] is earthquake sensitive and may be triggered accidentally (P = 0.20) by one... if an earthquake had occurred, it would certainly be (P = 0.40) in the [radio] messages.\" Events in time. Thus, in the first two patterns, events that can trigger an alarm can precede the alarm. Likewise, in the third pattern, the \"alarm\" (i.e. that the alarm has been triggered) can precede the \"telephone alarm\" (a telephone call to say that the alarm has been triggered).A single dimension can only approximate the sequence of events in time because it cannot represent events that intersect or occur simultaneously."}, {"heading": "10.2.3 Other considerations", "text": "Other points related to the patterns shown in Figure 22 are: \u2022 No attempt has been made to represent the notion that \"the last false alarm you remember was triggered by an earthquake\" (Pearl, 1997, p. 9). \u2022 These imaginary frequency values assume that intrusions (with a total frequency of occurrence of 1160) are much more common than earthquakes (with a total frequency of 100). As we will see, this difference reinforces the assumption that there was an intrusion when it is known that the alarm was triggered (but without additional knowledge of an earthquake). \u2022 According to Pearl's example (p. 49) (but in contrast to the phenomenon of looting during earthquakes), earthquakes and intrusions are assumed to be independent."}, {"heading": "10.2.4 Formation of alignments: the burglar alarm has sounded", "text": "Receiving a call to say that the burglar alarm has been triggered in your home can be displayed by placing the \"Telephone Alarm Call\" icon in New. Figure 23 above shows the best multiple alignment that the SP model has made in this case, with the patterns from Figure 22 in Old. The other two multiple alignments in the reference set are shown below the best multiple alignment, in order of the CD value and relative probability. The actual values for CD and relative probability are given in the caption to Figure 22."}, {"heading": "2 earthquake alarm 2", "text": "The most likely conclusion is the rather trivial conclusion that the alarm was actually triggered, reflecting the fact that in Figure 22 there is no pattern showing false positives for phone calls about the alarm. Apart from the conclusion that the alarm was triggered, the most likely conclusion (p = 0.328) is that there was indeed a break-in. However, there is a definite possibility that there was an earthquake - but the probability in this case (p = 0.016) is much lower than the probability of a break-in. These conclusions and their relative probabilities seem to be quite consistent with what you would naturally think after a phone call to say that the burglar alarm failed in your home (since you lived in a part of the world where earthquakes were not uncommon)."}, {"heading": "10.3 Formation of alignments: the burglar alarm has", "text": "In this example, the phenomenon of \"ringing away\" occurs when one learns not only that the intrusion alarm has been triggered, but that there has been an announcement on the radio that there has been an earthquake. In the sense of the SP model, the two events (the call about the alarm and the announcement about the earthquake) in New can be represented by a pattern like this: phone _ alarm _ call radio _ earthquake _ announcementor \"Radio Earthquake Alarm.\" The order of the two symbols makes no difference to the result. In this case, there is only one multiple alignment (shown above in Figure 24) that can \"explain\" all the information in New. As there is only this multiple alignment in the reference set for the best multiple alignment, the associated probabilities of the inferences, which can be read from the multiple alignment (\"alarm\" and \"earthquake\"), are best. \""}, {"heading": "10.3.1 Other possibilities", "text": "As mentioned above, the assumption of the closed world allows us to exclude the following possibilities: \u2022 an intrusion (that triggered the alarm) and simultaneously an earthquake (that led to a radio announcement) or \u2022 an earthquake that triggered the alarm and simultaneously led to a radio announcement and an intrusion that did not trigger the alarm. \u2022 And many other improbable possibilities of a similar nature (also discussed in Pearl, 1997, Section 2.2.4). Nevertheless, we can consider possibilities of this kind by combining multiple alignments as described in Section 7.8.7. But as a rule, this type of further analysis makes no difference to the original conclusion: the multiple alignment, originally judged to be the best interpretation of the available facts, has not been pushed out of this position, corresponding to the way we normally focus on the most probable explanations of events and ignore the many conceivable but unlikely alternatives."}, {"heading": "10.4 The SP framework and Bayesian networks", "text": "The above examples show that the SP framework is a viable alternative to Bayesian networks, at least in the situations described. Without diminishing Thomas Bayes \"performance in any way, his theorem appears to have shortcomings as a basis for theories about perception and cognition: \u2022 Inadequate complexity in storing statistical knowledge. Each node in a Bayesian network contains a table of conditional probabilities for all possible combinations of input factors, and these tables can be quite large. By contrast, the SP framework requires only a single measure of the frequency of each pattern. A focus on frequencies seems to have a general advantage in terms of simplicity over the representation of statistical knowledge in the form of bayones. \u2022 To divert attention from simpler alternatives by highlighting several categories of probabilities that differ from the hypothesis that objects stand for other universal theories (no structural theories in the form of conditional probabilities)."}, {"heading": "10.5 Causal diagnosis", "text": "In this section, we will consider a simple example of error diagnosis in an electronic circuit - described by Pearl (1997, pp. 263-272). Figure 25 shows the circuit with inputs on the left, outputs on the right and in between three multipliers (M1, M2 and M3) and two adders (M4 and M5). For the given inputs on the left, it is clear that output F is wrong and output G is correct. Figure 26 shows a causal network derived from the electronic circuit in Figure 25 (from Pearl, 1997, p. 264). In this diagram, X, Z, F and G are the outputs of components M1, M2, M4 and M5, respectable. 41 At the time of publication, permission to reproduce this diagram was applied twice but received no response."}, {"heading": "10.6 An SP approach to causal diagnosis", "text": "The main elements of the SP analysis presented here are as follows: \u2022 The input-output relationships of each component can be represented as a set of patterns, each associated with a measured or estimated frequency of occurrence. \u2022 A \"framework\" pattern (shown at the bottom of Figure 27) is necessary to ensure that appropriate multiple alignments can be built. \u2022 Figure 27 shows a set of patterns for the patterns shown in Figure 25. In the figure, the patterns beginning with the symbol \"M1\" represent the input relations for component M1, while those beginning with \"M2\" represent the input relations for the M2 component and also for the other patterns. \""}, {"heading": "10.7 Multiple alignments in causal diagnosis", "text": "Given appropriate patterns, the SP model constructs multiple alignments to be used for diagnoses. Figure 28 shows the best multi-alignment that the SP model has created with the new pattern shown in Figure 27 and \"TM1I1 TM1I2 TM2I1 TM2I2 TM3I1 TM3I2 FM4O TM5O.\" The first six symbols in this pattern express the idea that all inputs for the components M1, M2 and M3 are true. The penultimate symbol (\"FM4O\") indicates that the output of M4 is wrong and the last symbol (\"TM5O\") indicates that the output of M5 is true - in accordance with the results shown in Figure 25. From the multi-alignment shown in Figure 28, it can be concluded that the component M1 is bad and all other components are good. A total of seven alternative diagnoses can be derived from these multiple alignments, which encode all the symbols in Figure 28."}, {"heading": "11 Information storage and retrieval", "text": "SP theory provides a versatile model for database systems, with the ability to accommodate object-oriented structures as well as relational \"tuples\" and network and tree models of data (Wolff, 2007). It is most suitable for retrieving information in the type of sample query, but it seems to have the potential to support the use of natural language or query languages such as SQL. Unlike some ordinary database systems: \u2022 Storing and retrieving information is integrated with other aspects of intelligence such as pattern recognition, reasoning, planning, problem solving and learning - as set out elsewhere in this article. \u2022 The SP system provides a simple but effective means of combining class hierarchies with sub-hierarchies, with inheritance of attributes (Section 9.1). \u2022 It provides cross-classification with multiple inheritance. \u2022 There is flexibility and versatility in the representation of knowledge resulting from the fact that the system does not distinguish between \"9.bucket\" and \"section 1."}, {"heading": "12 Planning and problem solving", "text": "The SP frame provides a means of planning a route between two places, and, with the translation of square patterns into textual form, it can solve the type of geometric analogy problem that can be seen in some of the puzzle books and IQ tests (BK, chapter 8).Figure 29 shows an example of the latter type of problem. The task is to complete the relationship to B as C is to place one of the figures \"D,\" \"E\" or \"G\" in the position that is marked with \"E.\" The \"correct\" answer is unambiguous. \""}, {"heading": "13 Compression of information", "text": "Since information compression is central to the functioning of the SP system, it is natural to consider whether the system could provide useful insights in this area. In this context, the most promising aspects of the SP system seem to be: \u2022 The discovery of recurring data patterns by building multiple alignments with heuristic search to filter out the patterns that are most useful in terms of compression. \u2022 The potential of the system to detect and encode discontinuous data dependencies. It seems that there is potential to extract types of redundancy in information that are not accessible via standard methods of compression of information. In terms of the trade-off between the compression resources needed and the level of compression achievable, it is intended that the system will operate on the \"commercially available\" side of the spectrum - as opposed to LZW algorithms and the like, which are designed to be \"dirty and low performing\" for computers."}, {"heading": "14 Perception, cognition and neuroscience", "text": "Since much of the inspiration for the SP theory comes from evidence mentioned in Section 2.1 that the functioning of brains and nervous systems can be understood to a large extent from the point of view of information compression, the theory is about perception and cognition, as well as artificial intelligence and mainstream computing. However, in BK (Chapter 12) there is some discussion about how the SP concepts relate to a selection of questions of human perception and cognition. Of particular interest at the time of writing (after this chapter has been written) is the way in which SP theory can provide an alternative to quantum probability as an explanation for phenomena such as \"conjunctional fallacy\" (see, for example, Pothos and Busemeyer, 2013)."}, {"heading": "15 Conclusion", "text": "SP theory aims to simplify and integrate concepts in the fields of artificial intelligence, mainstream computing, and human perception and cognition, with information compression serving as a unifying theme. Matching and unifying patterns and the concept of multiple alignment are central ideas. In accordance with Occam's Razor, the SP system combines conceptual simplicity with descriptive and explanatory power. A relatively simple mechanism provides an interpretation for a number of concepts and phenomena in different areas, including concepts of \"computing,\" aspects of mathematics and logic, knowledge representation, natural language processing, pattern recognition, various types of probable reasoning, information storage and restoration, planning and problem solving, information compression, neuroscience, and human perception and cognition.As proposed in Section 3.2, the creation of a highly parallel open source version of the SP machine accessible via the Web would be a tool for further research."}, {"heading": "J. G. Wolff. Unifying Computing and Cognition: the SP Theory and Its", "text": "Applications. CognitionResearch.org, Menai Bridge, 2006a. ISBNs: 0- 9550726-0-3 (ebook edition), 0-9550726-1-1 (print edition). Distributors, including Amazon.com, are detailed at bit.ly / WmB1rs.J. G. Wolff. Medical Diagnosis as Pattern Recognition in Information Compression through Multiple Alignment, Unification, and Search. Decision Support Systems, 42: 608-625, 2006b. See bit.ly / XE7pRG.J. G. Wolff. SP Theory and Representation and Processing of Knowledge. In Z. Ma, editor, Soft Computing in Ontologies and Semantic Web, pages 79-101. Springer Publishers, Heidelberg, 2006c. See bit.ly / ZfrG4V.J. G. Wolff. Towards an intelligent database system based on the SP theory of computing and cognition. Data & Knowledge.bit.Engineering, Heidelberg, 2006b."}], "references": [{"title": "An integrated theory of the mind", "author": ["J.R. Anderson", "D. Bothell", "M.D. Byrne", "S. Douglass", "C. Lebiere", "Y. Qin"], "venue": "Psychological Review,", "citeRegEx": "Anderson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2004}, {"title": "Some informational aspects of visual perception", "author": ["F. Attneave"], "venue": "Psychological Review,", "citeRegEx": "Attneave.,? \\Q1954\\E", "shortCiteRegEx": "Attneave.", "year": 1954}, {"title": "Sensory mechanisms, the reduction of redundancy, and intelligence", "author": ["H.B. Barlow"], "venue": "Her Majesty\u2019s Stationery Office,", "citeRegEx": "Barlow.,? \\Q1959\\E", "shortCiteRegEx": "Barlow.", "year": 1959}, {"title": "Trigger features, adaptation and economy of impulses", "author": ["H.B. Barlow"], "venue": "Information Processes in the Nervous System,", "citeRegEx": "Barlow.,? \\Q1969\\E", "shortCiteRegEx": "Barlow.", "year": 1969}, {"title": "Redundancy reduction revisited", "author": ["H.B. Barlow"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Barlow.,? \\Q2001\\E", "shortCiteRegEx": "Barlow.", "year": 2001}, {"title": "Experiments in solving analogy problems using Minimal Length Encoding", "author": ["T. Belloti", "A. Gammerman"], "venue": "In Proceedings of Stream 1: \u201cComputational Learning and Probabilistic Reasoning\u201d,", "citeRegEx": "Belloti and Gammerman.,? \\Q1995\\E", "shortCiteRegEx": "Belloti and Gammerman.", "year": 1995}, {"title": "A First Language: The Early Stages", "author": ["R. Brown"], "venue": "Penguin, Harmondsworth,", "citeRegEx": "Brown.,? \\Q1973\\E", "shortCiteRegEx": "Brown.", "year": 1973}, {"title": "Randomness in arithmetic", "author": ["G.J. Chaitin"], "venue": "Scientific American,", "citeRegEx": "Chaitin.,? \\Q1988\\E", "shortCiteRegEx": "Chaitin.", "year": 1988}, {"title": "Syntactic Structures", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky.,? \\Q1957\\E", "shortCiteRegEx": "Chomsky.", "year": 1957}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q1991\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 1991}, {"title": "Significance of models of computation, from turing model to natural computation", "author": ["G. Dodig-Crnkovic"], "venue": "Minds & Machines,", "citeRegEx": "Dodig.Crnkovic.,? \\Q2011\\E", "shortCiteRegEx": "Dodig.Crnkovic.", "year": 2011}, {"title": "Ant colony system: a cooperative learning approach to the traveling salesman problem", "author": ["M. Dorigo", "L.M. Gambardella"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Dorigo and Gambardella.,? \\Q1997\\E", "shortCiteRegEx": "Dorigo and Gambardella.", "year": 1997}, {"title": "A program for the solution of a class of geometric-analogy intelligence-test questions", "author": ["T.G. Evans"], "venue": "Semantic Information Processing,", "citeRegEx": "Evans.,? \\Q1968\\E", "shortCiteRegEx": "Evans.", "year": 1968}, {"title": "The representation and manipulation of the algorithmic probability measure for problem solving", "author": ["A.J. Gammerman"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Gammerman.,? \\Q1991\\E", "shortCiteRegEx": "Gammerman.", "year": 1991}, {"title": "The Organization of Behaviour", "author": ["D.O. Hebb"], "venue": null, "citeRegEx": "Hebb.,? \\Q1949\\E", "shortCiteRegEx": "Hebb.", "year": 1949}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "ISBN 3-540-22139-5,", "citeRegEx": "Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Hutter.", "year": 2005}, {"title": "Introduction to special issue on context in natural language processing", "author": ["L. Iwanska", "W. Zadrozny"], "venue": "Computational Intelligence,", "citeRegEx": "Iwanska and Zadrozny.,? \\Q1997\\E", "shortCiteRegEx": "Iwanska and Zadrozny.", "year": 1997}, {"title": "The Soar Cognitive Architecture", "author": ["J.E. Laird"], "venue": null, "citeRegEx": "Laird.,? \\Q2012\\E", "shortCiteRegEx": "Laird.", "year": 2012}, {"title": "Understanding language without the ability to speak", "author": ["E.H. Lenneberg"], "venue": "Journal of Abnormal and Social Psychology,", "citeRegEx": "Lenneberg.,? \\Q1962\\E", "shortCiteRegEx": "Lenneberg.", "year": 1962}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2009\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2009}, {"title": "You can\u2019t play 20 questions with nature and win: projective comments on the papers in this symposium", "author": ["A. Newell"], "venue": "Visual Information Processing,", "citeRegEx": "Newell.,? \\Q1973\\E", "shortCiteRegEx": "Newell.", "year": 1973}, {"title": "The role of context in object recognition", "author": ["A. Oliva", "A. Torralba"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Oliva and Torralba.,? \\Q2007\\E", "shortCiteRegEx": "Oliva and Torralba.", "year": 2007}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1997\\E", "shortCiteRegEx": "Pearl.", "year": 1997}, {"title": "Definite Clause Grammars for language analysis - a survey of the formalism and a comparison with augmented transition networks", "author": ["F.C.N. Pereira", "D.H.D. Warren"], "venue": "Artificial Intelligence,", "citeRegEx": "Pereira and Warren.,? \\Q1980\\E", "shortCiteRegEx": "Pereira and Warren.", "year": 1980}, {"title": "Formal reductions of the general combinatorial decision problem", "author": ["E.L. Post"], "venue": "American Journal of Mathematics,", "citeRegEx": "Post.,? \\Q1943\\E", "shortCiteRegEx": "Post.", "year": 1943}, {"title": "Can quantum probability provide a new direction for cognitive modeling", "author": ["E.M. Pothos", "J.R. Busemeyer"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Pothos and Busemeyer.,? \\Q2013\\E", "shortCiteRegEx": "Pothos and Busemeyer.", "year": 2013}, {"title": "The Simplicity and Power model for inductive inference", "author": ["E.M. Pothos", "J.G. Wolff"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Pothos and Wolff.,? \\Q2006\\E", "shortCiteRegEx": "Pothos and Wolff.", "year": 2006}, {"title": "Computer Vision: Models, Learning, and Inference", "author": ["S.J.D. Prince"], "venue": null, "citeRegEx": "Prince.,? \\Q2012\\E", "shortCiteRegEx": "Prince.", "year": 2012}, {"title": "Time Warps, String Edits, and Macromolecules: the Theory and Practice of Sequence Comparisons", "author": ["D. Sankoff", "J.B. Kruskall"], "venue": null, "citeRegEx": "Sankoff and Kruskall.,? \\Q1983\\E", "shortCiteRegEx": "Sankoff and Kruskall.", "year": 1983}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Parts I and II. Information and Control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "A family of G\u00f6del machine implementations", "author": ["B. Steunebrink", "J. Schmidhuber"], "venue": "In Schmidhuber et al", "citeRegEx": "Steunebrink and Schmidhuber.,? \\Q2011\\E", "shortCiteRegEx": "Steunebrink and Schmidhuber.", "year": 2011}, {"title": "On computable numbers with an application to the Entscheidungsproblem", "author": ["A.M. Turing"], "venue": "Proceedings of the London Mathematical Society,", "citeRegEx": "Turing.,? \\Q1936\\E", "shortCiteRegEx": "Turing.", "year": 1936}, {"title": "The discovery of segments in natural language", "author": ["J.G. Wolff"], "venue": "British Journal of Psychology,", "citeRegEx": "Wolff.,? \\Q1977\\E", "shortCiteRegEx": "Wolff.", "year": 1977}, {"title": "Language acquisition and the discovery of phrase structure", "author": ["J.G. Wolff"], "venue": "Language & Speech,", "citeRegEx": "Wolff.,? \\Q1980\\E", "shortCiteRegEx": "Wolff.", "year": 1980}, {"title": "Language acquisition, data compression and generalization", "author": ["J.G. Wolff"], "venue": "Language & Communication,", "citeRegEx": "Wolff.,? \\Q1982\\E", "shortCiteRegEx": "Wolff.", "year": 1982}, {"title": "Learning syntax and meanings through optimization and distributional analysis", "author": ["J.G. Wolff"], "venue": "Categories and Processes in Language Acquisition,", "citeRegEx": "Wolff.,? \\Q1988\\E", "shortCiteRegEx": "Wolff.", "year": 1988}, {"title": "Simplicity and power\u2014some unifying ideas in computing", "author": ["J.G. Wolff"], "venue": "Computer Journal,", "citeRegEx": "Wolff.,? \\Q1990\\E", "shortCiteRegEx": "Wolff.", "year": 1990}, {"title": "Unifying Computing and Cognition: the SP Theory and Its Applications. CognitionResearch.org, Menai Bridge, 2006a. ISBNs: 09550726-0-3", "author": ["J.G. Wolff"], "venue": "(ebook edition),", "citeRegEx": "Wolff.,? \\Q2006\\E", "shortCiteRegEx": "Wolff.", "year": 2006}, {"title": "Medical diagnosis as pattern recognition in a framework of information compression by multiple alignment, unification and search", "author": ["J.G. Wolff"], "venue": "Decision Support Systems,", "citeRegEx": "Wolff.,? \\Q2006\\E", "shortCiteRegEx": "Wolff.", "year": 2006}, {"title": "The SP theory and the representation and processing of knowledge", "author": ["J.G. Wolff"], "venue": "Soft Computing in Ontologies and Semantic Web,", "citeRegEx": "Wolff.,? \\Q2006\\E", "shortCiteRegEx": "Wolff.", "year": 2006}, {"title": "Towards an intelligent database system founded on the SP theory of computing and cognition", "author": ["J.G. Wolff"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "Wolff.,? \\Q2007\\E", "shortCiteRegEx": "Wolff.", "year": 2007}, {"title": "The SP theory of intelligence: benefits and applications", "author": ["J.G. Wolff"], "venue": "2013a. Submitted for publication. See bit.ly/12YmQJW (PDF)", "citeRegEx": "Wolff.,? \\Q2013\\E", "shortCiteRegEx": "Wolff.", "year": 2013}, {"title": "Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision", "author": ["J.G. Wolff"], "venue": "2013b. In preparation. See bit.ly/Xj3nDY (PDF)", "citeRegEx": "Wolff.,? \\Q2013\\E", "shortCiteRegEx": "Wolff.", "year": 2013}, {"title": "Human Behaviour and the Principle of Least Effort", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf.,? \\Q1949\\E", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 32, "context": "The most comprehensive description of the theory as it stands now, with many examples, is in Wolff (2006a). But this book, with more than 450 pages, is too long to serve as an introduction to the theory.", "startOffset": 93, "endOffset": 107}, {"referenceID": 3, "context": "For example, when we view a scene with two eyes, the image on the retina of the left eye is almost exactly the same as the image on the retina of right eye, but our brains merge the two images into a single percept, and thus compress the information (Barlow, 1969).", "startOffset": 250, "endOffset": 264}, {"referenceID": 1, "context": "Much of the inspiration for the SP theory is a body of research, pioneered by Fred Attneave (1954), Horace Barlow (1959; 1969), and others, showing that several aspects of the workings of brains and nervous systems may be understood in terms of information compression.", "startOffset": 83, "endOffset": 99}, {"referenceID": 1, "context": "Much of the inspiration for the SP theory is a body of research, pioneered by Fred Attneave (1954), Horace Barlow (1959; 1969), and others, showing that several aspects of the workings of brains and nervous systems may be understood in terms of information compression. For example, when we view a scene with two eyes, the image on the retina of the left eye is almost exactly the same as the image on the retina of right eye, but our brains merge the two images into a single percept, and thus compress the information (Barlow, 1969). More immediately, the theory has grown out of my own research, developing models of the unsupervised learning of a first language, where the importance of information compression became increasingly clear (eg, Wolff, 1988). The theory also draws on principles of \u2018minimum length encoding\u2019 pioneered by Solomonoff (1964), and others.", "startOffset": 83, "endOffset": 856}, {"referenceID": 40, "context": "Also relevant and still of interest is Zipf\u2019s (1949) Human Behaviour and the Principle of Least Effort.", "startOffset": 39, "endOffset": 53}, {"referenceID": 20, "context": "Apart from this widely-accepted principle, the drive for simplification and integration of concepts in this research programme has been motivated in part by Allen Newell\u2019s critique of some kinds of research in cognitive science (Newell, 1973), and in part by the apparent fragmentation of research in artificial intelligence and mainstream computing, with their myriad of concepts and many specialisms.", "startOffset": 228, "endOffset": 242}, {"referenceID": 17, "context": "In attempting to simplify and integrate ideas, the SP theory belongs in the same tradition as unified theories of cognition such as Soar (Laird, 2012) and ACT-R (Anderson et al.", "startOffset": 137, "endOffset": 150}, {"referenceID": 0, "context": "In attempting to simplify and integrate ideas, the SP theory belongs in the same tradition as unified theories of cognition such as Soar (Laird, 2012) and ACT-R (Anderson et al., 2004).", "startOffset": 161, "endOffset": 184}, {"referenceID": 30, "context": "Although the SP programme shares some objectives with projects such as the G\u00f6del Machine (Steunebrink and Schmidhuber, 2011), and \u2018universal artificial intelligence\u2019 (Hutter, 2005), the approach is very different.", "startOffset": 89, "endOffset": 124}, {"referenceID": 15, "context": "Although the SP programme shares some objectives with projects such as the G\u00f6del Machine (Steunebrink and Schmidhuber, 2011), and \u2018universal artificial intelligence\u2019 (Hutter, 2005), the approach is very different.", "startOffset": 166, "endOffset": 180}, {"referenceID": 36, "context": "The first published version of the SP theory (Wolff, 1990) described \u2018some unifying ideas in computing\u2019.", "startOffset": 45, "endOffset": 58}, {"referenceID": 32, "context": "The first published version of the SP theory (Wolff, 1990) described \u2018some unifying ideas in computing\u2019. Early work on the SP computer model concentrated on developing an improved version of the \u2018dynamic programming\u2019 technique for the alignment of two sequences (see BK, Appendix A) as a possible route to modelling human-like flexibility in pattern recognition, analysis of language, and the like. About 1992, it became apparent that the explanatory range of the theory could be greatly expanded by forming alignments of 2, 3, or more sequences, much as in the \u2018multiple alignment\u2019 concept of bioinformatics. That idea was developed and adapted in new versions of the SP model, and incorporated in new procedures for unsupervised learning. Aspects of the theory, with many examples, have been developed in Wolff (2006a).", "startOffset": 46, "endOffset": 821}, {"referenceID": 9, "context": "This is calculated via the Shannon-Fano-Elias coding scheme (described in Cover and Thomas (1991)), using information about the frequency of occurrence of each Old pattern, so that the shortest codes represent the most frequent symbol types and vice versa.", "startOffset": 74, "endOffset": 98}, {"referenceID": 28, "context": "This is done with a process that is essentially a form of \u2018dynamic programming\u2019 (Sankoff and Kruskall, 1983), somewhat like the WinMerge utility for finding similarities and differences between files.", "startOffset": 80, "endOffset": 108}, {"referenceID": 32, "context": "(a) and (b) are reproduced from Figures 1 and 2 in Wolff (2007), with permission.", "startOffset": 51, "endOffset": 64}, {"referenceID": 11, "context": "An example of how effective this rough-and-ready approach can be is the way ant colonies can find reasonably good solutions to the travelling salesman problem via the simple technique of marking their routes with pheromones and choosing routes that are most strongly marked (Dorigo and Gambardella, 1997).", "startOffset": 274, "endOffset": 304}, {"referenceID": 34, "context": "But although the SNPR model (Wolff, 1982) is quite successful in deriving plausible grammars from samples of English-like artificial language, it has proved to be quite unsuitable as a basis for the SP theory.", "startOffset": 28, "endOffset": 41}, {"referenceID": 29, "context": "In accordance with the principles of minimum length encoding (Solomonoff, 1964), the aim of these processes of sifting and sorting is to minimise (G+E), where G is the size (in bits) of the grammar that is under development and E is the size (in bits) of the New patterns when they have been encoded in terms of the grammar.", "startOffset": 61, "endOffset": 79}, {"referenceID": 32, "context": "\u2022 Figure 10 shows part of a parsing of an unsegmented sample of natural language text created by the MK10 program (Wolff, 1977) using only the information in the sample itself and without any prior dictionary or other knowledge about the structure of language.", "startOffset": 114, "endOffset": 127}, {"referenceID": 33, "context": "\u2022 The same program does quite well\u2014significantly better than chance\u2014 in revealing phrase structures in natural language texts that have been prepared, as before, without spaces or punctuation\u2014but with each word replaced by a symbol for its grammatical category (Wolff, 1980).", "startOffset": 261, "endOffset": 274}, {"referenceID": 34, "context": "\u2022 The SNPR program for grammar discovery (Wolff, 1982) can, without supervision, derive a plausible grammar from an unsegmented sample of English-like artificial language, including the discovery of words, of grammatical categories of words, and the structure of sentences.", "startOffset": 41, "endOffset": 54}, {"referenceID": 32, "context": "Figure 10: Part of a parsing created by program MK10 (Wolff, 1977) from a 10,000 letter sample of English (book 8A of the Ladybird Reading Series) with all spaces and punctuation removed.", "startOffset": 53, "endOffset": 66}, {"referenceID": 32, "context": "Figure 10: Part of a parsing created by program MK10 (Wolff, 1977) from a 10,000 letter sample of English (book 8A of the Ladybird Reading Series) with all spaces and punctuation removed. The program derived this parsing from the sample alone, without any prior dictionary or other knowledge of the structure of English. Reproduced from Figure 7.3 in Wolff (1988), with permission.", "startOffset": 54, "endOffset": 364}, {"referenceID": 32, "context": "1 in Wolff (1988), with permission.", "startOffset": 5, "endOffset": 18}, {"referenceID": 18, "context": "Relevant evident comes from cases where children learn to understand language even though they have little or no ability to speak (Lenneberg, 1962; Brown, 1973)\u2014so that there is little or nothing for anyone to correct.", "startOffset": 130, "endOffset": 160}, {"referenceID": 6, "context": "Relevant evident comes from cases where children learn to understand language even though they have little or no ability to speak (Lenneberg, 1962; Brown, 1973)\u2014so that there is little or nothing for anyone to correct.", "startOffset": 130, "endOffset": 160}, {"referenceID": 34, "context": "In practice, the SNPR program, which is designed to minimise (G + E), has been shown to produce plausible generalisations, without over-generalising (Wolff, 1982).", "startOffset": 149, "endOffset": 162}, {"referenceID": 14, "context": "Such as: learning in the kinds of artificial neural network that are popular in computer science; Hebb\u2019s (1949) concept of learning; Pavlovian learning; and Skinnerian learning.", "startOffset": 98, "endOffset": 112}, {"referenceID": 31, "context": "In BK (Chapter 4), I have argued that the SP system is equivalent to a universal Turing machine (Turing, 1936), in the sense that anything that may be computed with a Turing machine may, in principle, also be computed with an SP machine.", "startOffset": 96, "endOffset": 110}, {"referenceID": 24, "context": "The gist of the argument is that the operation of a Post canonical system (Post, 1943) may be understood in terms of the SP theory and, since it is accepted that the Post canonical system is equivalent to the Turing machine (as a computational system), the Turing machine may also be understood in terms of the SP theory.", "startOffset": 74, "endOffset": 86}, {"referenceID": 32, "context": "Wolff (2006c), BK (Section 13.", "startOffset": 0, "endOffset": 14}, {"referenceID": 32, "context": "Wolff (2006c), BK (Section 13.4.3). BK (Section 6.4). Section 9.1; BK (Section 6.4). BK (Section 7.5). Wolff (2007), BK (Section 13.", "startOffset": 0, "endOffset": 116}, {"referenceID": 32, "context": "Wolff (2006c), BK (Section 13.4.3). BK (Section 6.4). Section 9.1; BK (Section 6.4). BK (Section 7.5). Wolff (2007), BK (Section 13.4.6.1). BK Section 7.6. Wolff (2006b). BK (Section 7.", "startOffset": 0, "endOffset": 170}, {"referenceID": 16, "context": "\u2022 The importance of context in the processing of language (Iwanska and Zadrozny, 1997) is accommodated in the way the system searches for a global best match for patterns: any pattern or partial pattern may be a context for any other.", "startOffset": 58, "endOffset": 86}, {"referenceID": 23, "context": "Despite the elegance and persuasiveness of his arguments, it turns out that the structure of English auxiliary verbs may be described with non-transformational rules in, for example, Definite Clause Grammars (Pereira and Warren, 1980), and also in the SP system, as outlined in the subsections that follow.", "startOffset": 208, "endOffset": 234}, {"referenceID": 8, "context": "It also provides a means of encoding the interesting system of overlapping and interlocking dependencies in English auxiliary verbs, described by Noam Chomsky in Syntactic Structures (1957). In that book, the structure of English auxiliary verbs is part of Chomsky\u2019s evidence in support of Transformational Grammar.", "startOffset": 151, "endOffset": 190}, {"referenceID": 8, "context": "In Chomsky\u2019s Syntactic Structures (1957), these forms were characterised as en forms and the same convention has been adopted here.", "startOffset": 3, "endOffset": 41}, {"referenceID": 21, "context": "\u2022 As in the processing of natural language (Section 8), the importance of context in recognition (Oliva and Torralba, 2007) is accommodated in the way the system searches for a global best match for patterns.", "startOffset": 97, "endOffset": 123}, {"referenceID": 32, "context": "One area of application is medical diagnosis (Wolff, 2006b), viewed as pattern recognition. There is also potential to assist in the understanding of natural vision and in the development of computer vision, as discussed in Wolff (2013b).", "startOffset": 46, "endOffset": 238}, {"referenceID": 40, "context": "The SP theory provides a versatile model for database systems, with the ability to accommodate object-oriented structures, as well as relational \u2018tuples\u2019, and network and tree models of data (Wolff, 2007).", "startOffset": 191, "endOffset": 204}, {"referenceID": 13, "context": "In more recent work (Belloti and Gammerman, 1996; Gammerman, 1991), minimum length encoding principles have been applied to good effect.", "startOffset": 20, "endOffset": 66}, {"referenceID": 11, "context": ", Evans\u2019 (1968) well-known heuristic algorithm).", "startOffset": 2, "endOffset": 16}, {"referenceID": 5, "context": "In more recent work (Belloti and Gammerman, 1996; Gammerman, 1991), minimum length encoding principles have been applied to good effect. This kind of problem may also be understood in terms of the SP concepts. As in most previous work, the proposed solution assumes that some mechanism is available which can translate the geometric forms in each problem into patterns of text symbols like other patterns in this article. For example, item \u2018A\u2019 in Figure 29 may be described as \u2018small circle inside large triangle\u2019. How this kind of translation may be done is not part of the present proposals (one such translation mechanism is described in Evans (1968)).", "startOffset": 21, "endOffset": 654}, {"referenceID": 13, "context": "noted elsewhere (Gammerman, 1991), successful solutions for this kind of problem require consistency in the way the translation is done.", "startOffset": 16, "endOffset": 33}, {"referenceID": 14, "context": "These proposals, which are adapted with modifications from Hebb\u2019s (1949) concept of a \u2018cell assembly\u2019, are very different from how artificial \u2018neural networks\u2019 are generally conceived in computer science.", "startOffset": 59, "endOffset": 73}], "year": 2015, "abstractText": "This article is an overview of the SP theory of intelligence, which aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme. It is conceived as a brain-like system that receives \u2018New\u2019 information and stores some or all of it in compressed form as \u2018Old\u2019 information; and it is realised in the form of a computer model, a first version of the SP machine. The matching and unification of patterns and the concept of multiple alignment are central ideas. Using heuristic techniques, the system builds multiple alignments that are \u2018good\u2019 in terms of information compression. For each multiple alignment, probabilities may be calculated for associated inferences. Unsupervised learning is done by deriving new structures from partial matches between patterns and via heuristic search for sets of structures that are \u2018good\u2019 in terms of information compression. These are normally ones that people judge to be \u2018natural\u2019, in accordance with the \u2018DONSVIC\u2019 principle\u2014the discovery of natural structures via information compression. The SP theory provides an interpretation for concepts and phenomena in several other areas including \u2018computing\u2019, aspects of mathematics and logic, the representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information storage and retrieval, planning and problem solving, information compression, neuroscience, and human perception and cognition. Examples include the parsing and production of language with discontinuous dependencies in syntax, pattern recognition at multiple levels of abstraction and \u2217Now published as The SP theory of intelligence: an overview (J G Wolff, Information, 4 (3), 283-341, 2013, doi:10.3390/info4030283). \u2020Dr Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw@cognitionresearch.org; +44 (0) 1248 712962; +44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org. 1 ar X iv :1 30 6. 38 88 v4 [ cs .A I] 7 J an 2 01 5 its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in Bayesian networks including \u2018explaining away\u2019, causal diagnosis, and the solving of a geometric analogy problem.", "creator": "LaTeX with hyperref package"}}}