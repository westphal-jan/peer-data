{"id": "1306.3895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2013", "title": "On-line PCA with Optimal Regrets", "abstract": "We carefully investigate the on-line version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we analyze two popular on-line algorithms, Gradient Descent (GD) and Exponentiated Gradient (EG). We show that both algorithms are essentially optimal in the worst-case. This comes as a surprise, since EG is known to perform sub-optimally when the instances are sparse. This different behavior of EG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as function of a loss budget, EG remains optimal and strictly outperforms GD. Next, we study the extension of the PCA setting, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that EG is optimal and strictly better than GD in this setting.", "histories": [["v1", "Mon, 17 Jun 2013 15:29:00 GMT  (47kb)", "https://arxiv.org/abs/1306.3895v1", null], ["v2", "Fri, 9 May 2014 05:28:39 GMT  (47kb)", "http://arxiv.org/abs/1306.3895v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiazhong nie", "wojciech kotlowski", "manfred k warmuth"], "accepted": false, "id": "1306.3895"}, "pdf": {"name": "1306.3895.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiazhong Nie", "Wojciech Kot", "Manfred K. Warmuth"], "emails": ["niejiazhong@cse.ucsc.edu", "manfred@cse.ucsc.edu", "vwkotlowski@cs.put.poznan.pl"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.38 95v2 [cs.LG] May 9 2Keywords: online learning, repentance limits, expert setting, k-sets, PCA, gradient descent and matrix exponentiated gradient algorithms."}, {"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 The online algorithms", "text": "The GD and MEG algorithms are both examples of the Mirror Descent algorithm [14]. Mirror Descent updates its parameters by minimizing a trade-off between a divergence from the parameter at the end of the last experiment and the loss of the current single instance, followed by a projection into the parameter set. Divergence is always a Bregman divergence. In Machine Learning, these updates were detected in [10,8]. Similarly, if we choose quantum relative entropy as the Bregman divergence, we get the matrix version of a multiplicative update algorithm called the Matrix Exponentiated Gradient Algorithm (here referred to as MEG). Similarly, the square Frobenius standard leads to an additive update algorithm called the Gradient Descent (GD). 3Sparse and dense instances: We call a symmetric positive matrix online, which are the values that are the constituents of most of our instances."}, {"heading": "2.1 The MEG algorithms", "text": "iiiiii. ii. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i."}, {"heading": "2.2 The GD algorithm", "text": "In this section we look at the GD algorithms (see e.g. [5,19]) which are characterized by the squared FP standard (WXt). (WXt). (WXt). (WXt). (WXt). (WXt). (WXT). (WXT). (WXT). (WXT). (WXT). (WXT). (WxT). (WxT). (WxT). (WXT). (WXT). (WxT). (WxT). (WXT). (WXT). (WxT). (WxT). (WxT). (WxT). (WxT). (WxT). (WxT). (WxT). (WxT). (WS. (WS). (WS. (WS). (WS. (WS). (WS. (WxT). (WS). (WS. (WxT). (WS. (WxT). (WS). (WS. (WxT). (WS). (WS. (WxT). (WS). (WS. (WxT). (WS). (WS. (WxT). (WS). (WS. (WS). (WS). (WS. (WxT). (WS). (WS). (WS. (WS). (WS). (WS). (WS). (WS. (WS. (WS). (WS). (WS). (WS). (WxT). (WS. (WS. (WS). (WS). (WS). (WxT). (WS. (WS). (Wxt). (WS). (Wxt). (WS). (W. (WS). (Wxt)."}, {"heading": "3 Lower bounds and optimal algorithms", "text": "In this section, we show the limits of regret for each algorithm that solves the problem. In particular, we show lower limits of regret for the online PCA and its generalization to the case of the dense instance. As argued in Section 2.2, it is sufficient to prove our lower limits for the m-set problem, which is the vector version of the online PCA and its generalization to dense instances. We show lower limits for the minimax regret, i.e. the minimum worst case that any algorithm can achieve against the best sentence: min alg. A with weighted Sm max sparse / dense loss seq. 1... T of the length TR (A, 1... T) loss of the Alg. A - loss of the best set-loss sequence 1... T.Recall that Sm are vectors in [0, 1] n of the total weight m representing mixtures of size m."}, {"heading": "3.1 Lower bounds for PCA with sparse instances", "text": "We start with the following technical problem for two experts. We start with the following technical problem for two experts. We start with the following technical problem for two experts. We start with the following technical problem for two experts. We assume that in an expert game one of the experts is randomly selected to suffer a unit of loss with a probability of 2p in each process, and with the probability of 1 \u2212 2p none of the experts suffers a loss. Then, according to T independent studies, E [loss of the winner of the tournament] one of the experts is randomly selected to suffer a unit of loss with the probability of 2p in each process, and with the probability of none of the experts suffering a loss. We are now ready to prove a lower limit for PCA. We first consider the case if k \u2264 n2.Theorem 3. For T-k and k \u2264 n2, in the T study online PCA problem with sparse instances."}, {"heading": "3.2 Lower bound for PCA with dense instances", "text": "The following lower boundary (m) The upper boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m) The lower boundary (m)"}, {"heading": "4 Conclusion", "text": "However, there is an alternative algorithm: the incremental offline algorithm [2] or the perturbed leader algorithm [9], which balances the total loss of all examples against the divergence to the initial distribution in its motivation. Note that both conversions follow their update with a projection into the parameter space. We suspect that the incremental offline conversion of GD is strictly better than the generally studied mirror descent conversion. The advantage of processing all examples over only the latter has now been shown in a number of different contexts: in Boosting it led to better algorithms [17] and it was also decisive for the objection of a kerelizable online PCA algorithm [13]. If there are only constraints of equality and the loss is linear, then the two versions of the G algorithm are not mandatory (7)."}], "references": [{"title": "A stochastic view of optimal regret through minimax duality", "author": ["J. Abernethy", "A. Agarwal", "P.L. Bartlett", "A. Rakhlin"], "venue": "COLT", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["K.S. Azoury", "M.K. Warmuth"], "venue": "Machine Learning 43(3), 211\u2013246", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "J. ACM 44(3),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P.M. Long", "M.K. Warmuth"], "venue": "IEEE Trans. Neural Netw. Learning Syst. 7(3), 604\u2013619", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P.M. Long", "M.K. Warmuth"], "venue": "IEEE Trans. Neural Netw. Learning Syst. 7(3), 604\u2013619", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning permutations with exponential weights", "author": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research 10, 1705\u20131736", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Tracking the best linear predictor", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research 1, 281\u2013309", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient algorithms for online decision problems", "author": ["A.T. Kalai", "S. Vempala"], "venue": "J. Comput. Syst. Sci. 71(3), 291\u2013307", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Inf. Comput. 132(1), 1\u201363", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Combining strategies efficiently: high-quality decisions from conflicting advice", "author": ["W.M. Koolen"], "venue": "Ph.D. thesis, Institute of Logic, Language and Computation (ILLC), University of Amsterdam", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Hedging structured concepts", "author": ["W.M. Koolen", "M.K. Warmuth", "J. Kivinen"], "venue": "COLT. pp. 93\u2013105", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online kernel pca with entropic matrix updates", "author": ["D. Kuzmin", "M.K. Warmuth"], "venue": "ICML. pp. 465\u2013472", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "On the universality of online mirror descent", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "NIPS. pp. 2645\u20132653", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex games in banach spaces", "author": ["K. Sridharan", "A. Tewari"], "venue": "Proceedings of the 23nd Annual Conference on Learning Theory (COLT)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["M.K. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research 9, 2287\u20132320", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Totally corrective boosting algorithms that maximize the margin", "author": ["M.K. Warmuth", "J. Liao", "G. R\u00e4tsch"], "venue": "ICML. pp. 1001\u20131008", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Leaving the span", "author": ["M.K. Warmuth", "S.V.N. Vishwanathan"], "venue": "COLT. pp. 366\u2013381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Fawcett, T., Mishra, N. (eds.) ICML. pp. 928\u2013936. AAAI Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "In this paper we consider the online version of centered PCA [16], where in each trial t = 1, .", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 87, "endOffset": 93}, {"referenceID": 18, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 204, "endOffset": 208}, {"referenceID": 15, "context": "In [16], a matrix version of the multiplicative update was applied to PCA, whose regret bound is logarithmic in the dimension n.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Beginning with some of the early work on linear regression [10], it is known that multiplicative updates are especially useful when the instances are dense.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "The advantage of GD in the sparse case is also supported by a general survey of Mirror Descent algorithms (to which GD and MEG belong) for the case when the loss vectors (which have negative components) lie in certain symmetric norm balls [14].", "startOffset": 239, "endOffset": 243}, {"referenceID": 13, "context": "This surprising performance of MEG comes from the fact that the losses in the PCA case are restricted to be non-negative, and therefore our results are qualitatively different from the cases studied in [14] where loss vectors are within a p\u2212norm ball, i.", "startOffset": 202, "endOffset": 206}, {"referenceID": 9, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 17, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 13, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 18, "context": "Linear losses are the least convex losses and in the regret bounds, convex losses are often approximated by first-order Taylor approximations [19] which are linear, and the gradient of the loss functions as the loss vector.", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "In the case when the parameter space and the space of loss vectors are convex and symmetric, the regret bounds are as expected: EG is optimal when the parameter space is 1-norm bounded and the loss vectors are infinity norm bounded, and GD is optimal when the both spaces are 2-norm bounded [15,14].", "startOffset": 291, "endOffset": 298}, {"referenceID": 13, "context": "In the case when the parameter space and the space of loss vectors are convex and symmetric, the regret bounds are as expected: EG is optimal when the parameter space is 1-norm bounded and the loss vectors are infinity norm bounded, and GD is optimal when the both spaces are 2-norm bounded [15,14].", "startOffset": 291, "endOffset": 298}, {"referenceID": 15, "context": "Previous lower bounds focused on the non-sparse case [16,12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 11, "context": "Previous lower bounds focused on the non-sparse case [16,12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 5, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 2, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 0, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "The GD and MEG algorithms are both examples of the Mirror Descent algorithm [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "In Machine Learning these updates were discovered in [10,8].", "startOffset": 53, "endOffset": 59}, {"referenceID": 7, "context": "In Machine Learning these updates were discovered in [10,8].", "startOffset": 53, "endOffset": 59}, {"referenceID": 15, "context": "These mixtures are generalized density matrices, which are symmetric, positive definite matrices with eigenvalues upper bounded by 1, and trace equal to k or m, respectively [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Similarly, tr(WXt) for W \u2208 Wk is the expected gain in trial t of the random projection matrix of rank k drawn from the mixture summarized by W \u2208 Wk, Note that the loss version of MEG corresponds to the original MEG algorithm developed in [16], where it was shown to have the following regret bound:", "startOffset": 238, "endOffset": 242}, {"referenceID": 4, "context": "[5,19]) which is motivated by the squared Frobenius norm (The loss and gain versions are the same in this case and we use the loss version below):", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[5,19]) which is motivated by the squared Frobenius norm (The loss and gain versions are the same in this case and we use the loss version below):", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "This algorithm is simple and a time dependent regret bound has been proved for arbitrary convex losses [19,14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "This algorithm is simple and a time dependent regret bound has been proved for arbitrary convex losses [19,14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 15, "context": "As already observed in [16], the PCA problem has the m-set problem as a special case.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "vector l \u2208 [0, 1], and the loss of a set is the total loss of all experts in the set.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "The algorithm maintains uncertainty over the m-sets by means of a weight vector w \u2208 [0, 1], such that \u2211n i=1 wi = m.", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "Recall that Sm were vectors in [0, 1] n of total weight m that represent mixtures of sets of size m.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "Let p \u2208 [0, 1] be such that p \u2264 1/4 and Tp \u2265 1/2.", "startOffset": 8, "endOffset": 14}, {"referenceID": 1, "context": "There is an alternate algorithm: the Incremental Off-line [2] or Follow the Perturbed Leader algorithm [9] that in its motivation trades off the total loss on all examples against the divergence to the initial distribution.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "There is an alternate algorithm: the Incremental Off-line [2] or Follow the Perturbed Leader algorithm [9] that in its motivation trades off the total loss on all examples against the divergence to the initial distribution.", "startOffset": 103, "endOffset": 106}, {"referenceID": 16, "context": "The advantage of processing all examples versus just the last one has now shown up in a number of different contexts: in Boosting it led to better algorithms [17] and it also was crucial for obtaining a kernelizable online PCA algorithm [13].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "The advantage of processing all examples versus just the last one has now shown up in a number of different contexts: in Boosting it led to better algorithms [17] and it also was crucial for obtaining a kernelizable online PCA algorithm [13].", "startOffset": 237, "endOffset": 241}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We carefully investigate the online version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we give regret bounds for two popular online algorithms, Gradient Descent (GD) and Matrix Exponentiated Gradient (MEG). We show that both algorithms are essentially optimal in the worst-case when the regret is expressed as a function of the number of trials. This comes as a surprise, since MEG is commonly believed to perform sub-optimally when the instances are sparse. This different behavior of MEG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as a function of a loss budget, MEG remains optimal and strictly outperforms GD. Next, we study a generalization of the online PCA problem, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that MEG is optimal and strictly better than GD in this setting.", "creator": "LaTeX with hyperref package"}}}