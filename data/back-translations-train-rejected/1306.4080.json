{"id": "1306.4080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2013", "title": "Parallel Coordinate Descent Newton Method for Efficient $\\ell_1$-Regularized Minimization", "abstract": "Parallel coordinate descent algorithms emerge with the growing demand for large-scale optimization. These algorithms are usually limited by their divergence under high parallelism or need data preprocessing to avoid divergence. In this paper, we propose a parallelized algorithm, termed as Parallel Coordinate Descent Newton (PCDN), to pursue more parallelism. It randomly partitions the feature set into $b$ subsets/bundles with size of $P$, then it sequentially processes each bundle by first computing the descent directions for each feature in the bundle in parallel and then conducting $P$-dimensional line search to obtain the stepsize of the bundle. We will show that: (1) PCDN is guaranteed to converge globally; (2) PCDN can converge to the specified accuracy $\\epsilon$ within the limited iteration number of $T_\\epsilon$, and the iteration number $T_\\epsilon$ decreases along with the increasing of parallelism (bundle size $P$). PCDN is applied to large-scale $L_1$-regularized logistic regression and $L_2$-loss SVM. Experimental evaluations over five public datasets indicate that PCDN can better exploit parallelism and outperforms state-of-the-art algorithms in speed, without losing test accuracy.", "histories": [["v1", "Tue, 18 Jun 2013 07:03:16 GMT  (2965kb)", "http://arxiv.org/abs/1306.4080v1", "25 pages, 25 figures"], ["v2", "Fri, 27 Dec 2013 08:41:37 GMT  (308kb,D)", "http://arxiv.org/abs/1306.4080v2", "28 pages, 27 figures"], ["v3", "Tue, 18 Mar 2014 14:55:49 GMT  (486kb,D)", "http://arxiv.org/abs/1306.4080v3", "30 pages, 36 figures"]], "COMMENTS": "25 pages, 25 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yatao bian", "xiong li", "yuncai liu", "ming-hsuan yang"], "accepted": false, "id": "1306.4080"}, "pdf": {"name": "1306.4080.pdf", "metadata": {"source": "CRF", "title": "Parallel Coordinate Descent Newton for Large-scale L1-Regularized Minimization", "authors": ["Yatao Bian", "Xiong Li", "Yuncai Liu"], "emails": ["bianyatao@sjtu.edu.cn", "lixiong@sjtu.edu.cn", "whomliu@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.40 80v1 [Keywords: Coordinate Descent, Parallel Optimization, Large Scale Optimization, L1Regularized Minimization"}, {"heading": "1. Introduction", "text": "It is not as if this is a real problem (Li and Osher, 2009). Various optimization methods such as trust models (Lin and More, 1999) have been developed to solve the L1 regulatory models that include Descent Descent Newton (CDN) (Yuan et al., 2010) is highly efficient and has shown superiority over other problems. Large dataset with high dimensional features or large number of samples for highly scalable and parallelized optimization algorithms in Langford et al. (2009); Large dataset with high dimensional features or."}, {"heading": "3. Parallel coordinate descent newton (PCDN)", "text": "SCDN does not guarantee its convergence if the number of features to be updated exceeds a threshold in parallel, i.e. P + n / p + p. To take advantage of higher parallelism, we propose a parallel algorithm based on a high-dimensional Armijo line that potentially ensures its convergence. The general approach of the proposed algorithm search for a bundle of features is summarized in Algorithm 3. Note: The key difference between PCDN and SCDN is the line search, i.e. PCDN performs P-dimensional searches for a bundle of features, while SCDN performs a one-dimensional Armijo line for each feature search."}, {"heading": "4. On the convergence of PCDN", "text": "In this section we will theoretically justify the convergence of the proposed PCDN from three aspects: the convergence of the P-dimensional line search, the global convergence, the convergence rate. We will include all the detailed proofs in Appendix A. Before analyzing the convergence of the PCDN, we will first of all the following terms: Lemma.Lemma 1 Let {wt}, {s}, {\u03b1t}, and {Bt}, the sequences derived from Alg. 3; \u03bb (Bt), the maximum element of the (XTX) jj, where j, the minimum element of the (XTX), jj, where j = 1, \u00b7 Bt), n, hold the following results. (EBt), the maximum element is the (XTX) jj, where j is the minimum element of the (XTX)."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental setup", "text": "Datasets Five public real datasets4 are used in our experiments. They are summarized in Table 2. news20, rcv1, a9a and real-sim are document datasets, whose samples are in unit vectors. gisette is a handwriting digit problem from NIPS 2003 feature selection challenge, whose features are linearly scaled to the [-1,1] interval.For each dataset, the optimal bundle size P. \u00b7 tdc + E [qt] \u00b7 tls (14) where # thread is the number of threads launch by each iteration of each iteration of PCDN can be approximated by, E [time)."}, {"heading": "6. Discussion and conclusion", "text": "In this paper, we have introduced a highly parallel algorithm, the Parallel Coordinate Descent Newton (PCDN), with a strong convergence guarantee and fast convergence rate with any possible parallelism to large-scale L1-regulated minimization. PCDN can be generalized as a general parallel framework to solve the problem of minimizing the sum of convex loss term and separable term. In our experiment, the ability of the PCDN is still limited by the usual multi-core implementation, where both the number of cores and the number of threads is low. In order to fully exploit the potential of parallelism (i.e. the downward direction of calculation in step 8 of algorithm 3), an implementation based on heterogeneous computing frameworks (Gaster et al., 2012) such as GPU and FPGA will be much more powerful."}, {"heading": "Appendix A. Proofs", "text": "In this appendix, we demonstrate the following lamas and theorems from Section 4.Proof of Lemma 1 (1) ProofWe first prove that EBt [\u03bb (Bt)] increases monotonically, while P and EBt (Bt) are constant. \u00b7 P, \u00b7 \u00b7 \u00b7 \u00b7 n, f (P) = EBt (Bt) = 1 = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 n (EBP) is the k-th minimum of (X TX) jj, j = 1, \u00b7 \u00b7 n, f (P) = EBt (Bt) = 1CPn (EBP \u2212 1 n \u2212 1). \u2212 P \u2212 1C P \u2212 1 n \u2212 2 + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 + \u00b7 + \u00b7 CP \u2212 1k \u2212 1 + \u00b7 \u00b7 \u00b7 p (P)."}, {"heading": "Appendix B. Details about P -dimensional Armijo line search", "text": "In this section, we will show that the time cost of a step of our P-dimensional line search tls remains approximately constant with different P by presenting the implementation details (in Alg. 4). For example, if we take logistic regression w.l.o.g, we maintain both dTxi and ew Txi, i = 1, \u00b7 \u00b7 \u00b7, s and follow the implementation technique of Fan etal (see Appendix G in Fan et al. (2008)), that is, we use the following form of sufficient reduction condition: f (w + \u03b2qd) \u2212 f (w) = w + \u03b2qd) \u2212 f (w) = \u03b2w + \u03b2qd: 1 \u2212 w (s), i = 1log (e (w + \u03b2 q d) Txi + 1e (w + e\u03b2qd) Txi + e\u03b2qdTxi) + \u03b2q: i = qd: yi = c = \u2212 1 Txi (dxi)."}, {"heading": "Appendix C. More experimental results for logistic regression", "text": "Further experimental results can be found in this section. Fig. 4 shows the track of the model nnz (number of unequal elements in w, the first line) and the function value Fc (w) (the second line) w.r.t. Fig. 5 and Fig. 6 show the temporal performance of the logistic regression on the dataset a9a or real-sim."}], "references": [{"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "OpenCL. Elsevier Science & Technology Books,", "citeRegEx": "Langford et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2012}, {"title": "Coordinate descent optimization for l1 minimization with", "author": ["Yingying Li", "Stanley Osher"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Li and Osher.,? \\Q2009\\E", "shortCiteRegEx": "Li and Osher.", "year": 2009}, {"title": "Hogwild: A lock-free", "author": ["Niu", "Benjamin Recht", "Christopher Re", "Stephen J. Wright"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2004}, {"title": "A coordinate gradient descent method for nonsmooth", "author": ["Paul Tseng", "Sangwoon Yun"], "venue": null, "citeRegEx": "Tseng and Yun.,? \\Q2009\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2009}, {"title": "Slow learners are fast", "author": ["Alex Smola", "John Langford"], "venue": "In NIPS,", "citeRegEx": "Zinkevich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": ", 2010) and compressed sensing (Li and Osher, 2009).", "startOffset": 31, "endOffset": 51}, {"referenceID": 3, "context": "Various optimization methods such as trust region (Lin and Mor\u00e9, 1999), coordinate gradient descent (Tseng and Yun, 2009) and stochastic gradient (Shalev-Shwartz and Tewari, 2009) have been developed to solve L1-regularized 1", "startOffset": 100, "endOffset": 121}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al.", "startOffset": 14, "endOffset": 56}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems.", "startOffset": 14, "endOffset": 81}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem.", "startOffset": 14, "endOffset": 230}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features.", "startOffset": 14, "endOffset": 408}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead.", "startOffset": 14, "endOffset": 753}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead. Is there a parallel coordinate descent algorithm with high parallelism and global convergence guarantee, but without needing data preprocessing? We in this paper present such an algorithm based on CDN of Yuan et al. (2010), termed as Parallel Coordinate Descent Newton (PCDN).", "startOffset": 14, "endOffset": 1096}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j \u2208 N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {\u2207jL(w)d+ 1 2 \u2207jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure.", "startOffset": 71, "endOffset": 361}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j \u2208 N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {\u2207jL(w)d+ 1 2 \u2207jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure. Let q be the line search step index, the stepsize \u03b1 = \u03b1(w,d) is determined as follows, \u03b1(w,d) = max q=0,1,2,\u00b7\u00b7\u00b7 {\u03b2 | Fc(w + \u03b2d)\u2212 Fc(w) \u2264 \u03b2\u03c3\u2206}, (5) where \u03b2 \u2208 (0, 1), \u03c3 \u2208 (0, 1), \u03b2 denotes \u03b2 to the power of q and, \u2206 = \u2207L(w)d+ \u03b3dHd+ \u2016w + d\u20161 \u2212 \u2016w\u20161, (6) where H \u2261 diag(\u22072L(w)), \u03b3 \u2208 [0, 1). This rule requires only function evaluations. According to Tseng and Yun (2009), larger stepsize will be accepted if we choose either \u03c3 near 0 or \u03b3 near 1.", "startOffset": 71, "endOffset": 796}, {"referenceID": 3, "context": "Proof of Lemma 1(3) Proof We follow the proof in Tseng and Yun (2009), from Eq.", "startOffset": 49, "endOffset": 70}, {"referenceID": 3, "context": "First, we prove that (following Lemma 5(b) in Tseng and Yun (2009)) the descent condition in Eq.", "startOffset": 46, "endOffset": 67}, {"referenceID": 3, "context": "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(\u22072L(w)).", "startOffset": 101, "endOffset": 122}, {"referenceID": 3, "context": "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(\u22072L(w)). Note that, the selection of bundle Bt in Eq. (7) is consistent with that used in CGD (Eq. (12) in Tseng and Yun (2009)).", "startOffset": 101, "endOffset": 265}, {"referenceID": 3, "context": "(6) in Tseng and Yun (2009)).", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "Then, we can use Theorem 1(e) in Tseng and Yun (2009) to prove the global convergence, which requires that {Bt} is chosen under the Gauss-Seidel rule and supt \u03b1 < \u221e.", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "According to Theorem 1(e) in Tseng and Yun (2009), any cluster point of {wt} is a stationary point of Fc(w).", "startOffset": 29, "endOffset": 50}], "year": 2017, "abstractText": "Parallel coordinate descent algorithms emerge with the growing demand for large-scale optimization. These algorithms are usually limited by their divergence under high parallelism or need data preprocessing to avoid divergence. In this paper, we propose a parallelized algorithm, termed as Parallel Coordinate Descent Newton (PCDN), to pursue more parallelism. It randomly partitions the feature set into b subsets/bundles with size of P , then it sequentially processes each bundle by first computing the descent directions for each feature in the bundle in parallel and then conducting P -dimensional line search to obtain the stepsize of the bundle. We will show that: (1) PCDN is guaranteed to converge globally; (2) PCDN can converge to the specified accuracy \u01eb within the limited iteration number of T\u01eb, and the iteration number T\u01eb decreases along with the increasing of parallelism (bundle size P ). PCDN is applied to large-scale L1-regularized logistic regression and L2-loss SVM. Experimental evaluations over five public datasets indicate that PCDN can better exploit parallelism and outperforms state-of-the-art algorithms in speed, without losing test accuracy.", "creator": "LaTeX with hyperref package"}}}