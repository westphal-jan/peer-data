{"id": "1306.4418", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2013", "title": "Structure Based Extended Resolution for Constraint Programming", "abstract": "Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art, called Lazy Clause Generation (LCG), uses resolution to derive nogoods expressing the reasons for each search failure. Such nogoods can prune other parts of the search tree, producing exponential speedups on a wide variety of problems. Nogood learning solvers can be seen as resolution proof systems. The stronger the proof system, the faster it can solve a CP problem. It has recently been shown that the proof system used in LCG is at least as strong as general resolution. However, stronger proof systems such as \\emph{extended resolution} exist. Extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution. The primary problem in using extended resolution is to figure out exactly which literals are useful to introduce. In this paper, we show that we can use the structural information contained in a CP model in order to introduce useful literals, and that this can translate into significant speedups on a range of problems.", "histories": [["v1", "Wed, 19 Jun 2013 04:18:45 GMT  (34kb,D)", "http://arxiv.org/abs/1306.4418v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["geoffrey chu", "peter j stuckey"], "accepted": false, "id": "1306.4418"}, "pdf": {"name": "1306.4418.pdf", "metadata": {"source": "CRF", "title": "Structure Based Extended Resolution for Constraint Programming", "authors": ["Geoffrey Chu", "Peter J. Stuckey"], "emails": ["gchu@csse.unimelb.edu.au", "pjs@csse.unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Definitions and Background", "text": "We start with the original problem at the root of the search tree. At each node in the search tree, we propagate the constraints to unlock variable / value pairs that can no longer be considered in this solution. We start with the original problem at the root of the search tree."}, {"heading": "3 Generality of Explanations", "text": "Since the nogoods derived from the resolution proof system are formed by the resolution of the explanations generated by the broadcasters, the more general the explanations are, the more general the nogood is derived from it. Having better explanations means that we can derive stronger nogoods for the same set of explanations, which clearly prove that a larger part of the search space has failed. However, the following definitions allow us to compare and estimate how good an explanation is: Definition 2. For two possible explanations E \u2264 l1 = logical. An explanation E \u00b2 p \u00b2 for the conclusion p, E \u00b2 is strictly more general than E \u00b2 xx \u00b2 language is strictly more general than E \u00b2 xxx \u00b2 language."}, {"heading": "4 Extending the Language", "text": "We will now consider how we can expand the language of resolution to give more general explanations. First, we will give a simple motivating example = Then it becomes possible. Example 4. Let us consider the 0-1 problem of the edge caused by x1,.., xn \u00b2, and pi is the gain of the edge i,. A normal CP solver will require O (2n) to solve this problem. It does not work better with an LCG solver because the size of the slightest proof of optimism with only equality, equality and inequality dictionaries on which xi is still exponentially in n. On the other hand, let us assume that we have introduced dictionaries to represent partial sums of form."}, {"heading": "4.1 Linear", "text": "Example 5: Considering the linear constraint x1 + 2x2 + 3x3 + 4x4 + 4x5 \u2264 30 of Example 1. Considering x1 \u2265 1, x2 \u2265 2, x3 \u2265 3, we can conclude x4 \u2264 4. There are several possible maximum general explanations in the standard LCG language, e.g. Jx2 \u2265 Jx3 \u2264 sum Jx3 \u2265 3K \u2192 Jx4 \u2264 4K or Jx1 \u2265 Jx2 \u2265 2K. However, none of them is the most general possible explanation. If we expand the language to include literals representing subtotals, we can now use the general general explanation: Jx1 + 2x2 + 3x3 \u2265 Jx3 \u2265 2K \u2192 Jx4 \u2264 4K \u2192 Jx4 \u2264 4K."}, {"heading": "4.2 Lex", "text": "Consider the global lexicographic constraint: lex less ([x1,..., xn], [y1,.., yn]), which results in x1,..., xn to be lexicographically less than y1,.., yn, i.e.: x1 < y1, (x1 = y1, x2, y2 < y2, x3 = 3. From the constraint we can conclude that y3, y1,., y1 \u2212 1, xn < yn). Consider a partial constraint x1, y1 = 1, x2 = 2, y2, y2, y2, x3 = 3. The maximum general explanation in the standard language LCG is: Jx1 = 1K, xxi xi, x2 = 2K, Jx2 = 2K, Jx3, y3, and we can conclude that y3, if we have such an extension."}, {"heading": "4.3 Disjunctive", "text": "Consider a disjunctive constraint disjunctively ([s1, s2], [5, 5]) over two tasks with start times, the current domains of s1,..., 8}, s2, s0,.., 4} and durations of d1 = d2 = 5. A global propagator would argue that task 1 must be planned after task 2, and therefore that s1 \u2265 5. There are several maximum general explanations in the standard LCG language, e.g. Js1 \u2265 0K-Js2 \u2264 4K-Js2 \u2264 0K \u2192 Js1 \u2265 5K. Then we can expand the language with words to show that task i runs before task j, written as Ji jK and channeled via: Ji jK \u2192 si + di \u2264 sj, \u00ac Ji jK \u2192 sj \u2264 0K. Then we can explain the conclusion by saying: J2 1K-Js1 > Js1-5K."}, {"heading": "4.4 Table", "text": "Let us take a table of constraints ([x1, x2, x3, x4], [1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1, 1]). Suppose we have x1 = 1, x2 = 2. Among other things, multiplication is x4 6 = 1. There are a number of different, maximum general explanations in the standard LCG language, e.g. Jx1 6 = 4K. Jx1 6 = 3K. Jx2 6 = 1K."}, {"heading": "5 Exploiting Global Structure for Linears", "text": "For lex, table, disjunctive, regular and bdd / mdd, the number of useful characters we have identified in Section 4 is only linear or square in terms size. Moreover, all these logical expressions have already been maintained internally by the global propagators and it is easy to change the propagators to use these letters and use them in explanations. So the overhead of adding these letters is quite low and it is fine to simply add them all to the language. Linear restrictions are extremely common and we know for certain language extensions can be useful to circumvent these limitations. On the other hand, there are possible divisible letters for a length n linear with the greatest coefficient and maximum domain size d, if we add too many of them, the cost of channeling them will squeeze out."}, {"heading": "6 Experiments", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "7 Related Work", "text": "These concerns are far less important in the context of the LCG."}, {"heading": "8 Conclusion", "text": "A significant amount of CP research focuses on improving the performance of the proof systems used in CP solvers by developing more powerful global propagators. However, such research may well reach its limits, as the optimal propagators for the most commonly used limitations are already known. Nogood learning provides an orthogonal way to improve the performance of the proof systems used in CP solvers. Extending the language of resolution changes the performance of the proof system used in nogood learning and can exponentially reduce the size of evidence of dissatisfaction or optimism, leading to a much faster CP solution. The primary difficulty in using advanced resolution is finding the correct language extensions. We have provided a framework for analyzing the universality of explanations given by global propagators in LCG solvers, and demonstrated that language extensions that enhance the universality of these explanations can be extremely good candidates for resolving linguistic problems."}], "references": [{"title": "Conflict directed lazy decomposition", "author": ["I. Abio", "P.J. Stuckey"], "venue": "In Proc. CP 2012,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A restriction of extended resolution for clause learning sat solvers", "author": ["Gilles Audemard", "George Katsirelos", "Laurent Simon"], "venue": "In Proc. of AAAI", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Efficient cnf encoding of boolean cardinality constraints", "author": ["Olivier Bailleux", "Yacine Boufkhad"], "venue": "In Proc. CP", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1957}, {"title": "Approximating treewidth, pathwidth, frontsize, and shortest elimination tree", "author": ["Hans L. Bodlaender", "John R. Gilbert", "Hj\u00e1lmtyr Hafsteinsson", "Ton Kloks"], "venue": "J. Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Boosting systematic search by weighting constraints", "author": ["Fr\u00e9d\u00e9ric Boussemart", "Fred Hemery", "Christophe Lecoutre", "Lakhdar Sais"], "venue": "In Procs. of ECAI04,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A short proof of the pigeon hole principle using extended resolution", "author": ["Stephen A. Cook"], "venue": "SIGACT News,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1976}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Translating pseudo-boolean constraints into sat", "author": ["Niklas E\u00e9n", "Niklas S\u00f6rensson"], "venue": "JSAT, 2(1-4):1\u201326,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Lazy clause generation reengineered", "author": ["Thibaut Feydy", "Peter J. Stuckey"], "venue": "In Proc. of CP 2009,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Solving talent scheduling with dynamic programming", "author": ["Maria Garcia de la Banda", "Peter J. Stuckey", "Geoffrey Chu"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The complexity of flowshop and jobshop scheduling", "author": ["Michael R Garey", "David S Johnson", "Ravi Sethi"], "venue": "Mathematics of operations research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1976}, {"title": "Extended clause learning", "author": ["Jinbo Huang"], "venue": "Artif. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unrestricted nogood recording in CSP search", "author": ["G. Katsirelos", "F. Bacchus"], "venue": "In Proc. of CP 2003,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Generalized nogoods in CSPs", "author": ["G. Katsirelos", "F. Bacchus"], "venue": "In The Twentieth National Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Boosting Search with Variable Elimination in Constraint Optimization and Constraint Satisfaction", "author": ["Javier Larrosa", "Rina Dechter"], "venue": "Problems. Constraints,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Symmetry breaking constraints for value symmetries in constraint satisfaction", "author": ["Y. Law", "J. Lee"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The challenge of exploiting weak symmetries", "author": ["Roland Martin"], "venue": "In Proc. of the International Workshop on Constraint Solving and Constraint Logic Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Solving strategies for highly symmetric csps", "author": ["Pedro Meseguer", "Carme Torras"], "venue": "In Proc. of IJCAI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Chaff: Engineering an Efficient SAT Solver", "author": ["Matthew W. Moskewicz", "Conor F. Madigan", "Ying Zhao", "Lintao Zhang", "Sharad Malik"], "venue": "In Proceedings of the 38th Design Automation Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Propagation via lazy clause generation", "author": ["Olga Ohrimenko", "Peter J. Stuckey", "Michael Codish"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "On the power of clause-learning sat solvers as resolution engines", "author": ["Knot Pipatsrisawat", "Adnan Darwiche"], "venue": "Artif. Intell.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Machine-oriented logic based on the resolution principle", "author": ["A.J. Robinson"], "venue": "Journal of the ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1965}, {"title": "Towards an optimal cnf encoding of boolean cardinality constraints", "author": ["Carsten Sinz"], "venue": "In Proc. CP", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Caching Search States in Permutation Problems", "author": ["Barbara M. Smith"], "venue": "In Proc. of CP 2005,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "On the complexity of derivation in propositional calculus, chapter Studies in Constructive Mathematics and Mathematical Logic", "author": ["G. Tseitin"], "venue": "Steklov Mathematical Institute,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1970}, {"title": "The complexity of propositional proofs", "author": ["Alasdair Urquhart"], "venue": "Bulletin of the EATCS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "An efficient algorithm for solving nonograms", "author": ["Chiung-Hsueh Yu", "Hui-Lung Lee", "Ling-Hwei Chen"], "venue": "Appl. Intell.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "The current state of the art is Lazy Clause Generation [21] (LCG).", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "It has recently been proved [22] that a SAT solver performing conflict directed clause learning and restarts has a proof system that is as powerful as general resolution [23].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "It has recently been proved [22] that a SAT solver performing conflict directed clause learning and restarts has a proof system that is as powerful as general resolution [23].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "Extended resolution [26] is a proof system even stronger than general resolution and is one of the most powerful proof systems for propositional logic [27].", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Extended resolution [26] is a proof system even stronger than general resolution and is one of the most powerful proof systems for propositional logic [27].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "It is well known that there can be an exponential separation between the size of the proof generated by extended resolution and general resolution on certain problems [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": ", [2, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [2, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 7, "context": "[8], chapter 6) only considers equality literals of the form x = v where x is a variable and v is a value.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Later on, disequality literals of the form x 6= v were introduced [14, 15].", "startOffset": 66, "endOffset": 74}, {"referenceID": 14, "context": "Later on, disequality literals of the form x 6= v were introduced [14, 15].", "startOffset": 66, "endOffset": 74}, {"referenceID": 20, "context": "More recently, inequality literals of the form x \u2265 v, x \u2264 v were introduced in LCG [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Consider a disjunctive constraint disjunctive([s1, s2], [5, 5]) over two tasks with start times having current domains of s1 \u2208 {2, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Consider a disjunctive constraint disjunctive([s1, s2], [5, 5]) over two tasks with start times having current domains of s1 \u2208 {2, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 3, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 3, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 24, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": ", [5]) to give a good variable ordering.", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "The concert hall problem [17] and talent scheduling problem [11] are scheduling problems with linear constraints.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "The concert hall problem [17] and talent scheduling problem [11] are scheduling problems with linear constraints.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "The PC-board problem [18] and the balanced incomplete block design problem (BIBD) [19] are matrix problems with linear constraints.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The PC-board problem [18] and the balanced incomplete block design problem (BIBD) [19] are matrix problems with linear constraints.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "The nonograms problem [28] is a board type problem with regular constraints.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "The jobshop scheduling problem [12] has disjunctive constraints.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "For nonograms and jobshop, we use the weighted degree search heuristic [6], which works well for these two problems.", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "(VSIDS) heuristic [20] adapted from SAT.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 2, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": "As a result, even if there are potentially an exponential number of possible literals and clauses, it is typically still fine, because during any one solve, only a very small proportion of those literals and clauses will need to be created, and they can be thrown away as soon as they are no longer useful [10].", "startOffset": 306, "endOffset": 310}, {"referenceID": 0, "context": "The closest related work to that presented here is conflict directed lazy decomposition [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "The only constraints for which lazy decomposition is defined in [1] are cardinality and pseudo-Boolean constraints.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "In [2], the extension considered are of the form: if l\u03041 \u2228\u03b1 and l\u03042 \u2228\u03b1 are two successively derived nogoods, then add the new literal z defined via z \u2194 l1 \u2228 l2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], another extension rule is proposed where they add a new literal z \u2194 d\u03041\u2228.", "startOffset": 3, "endOffset": 7}], "year": 2013, "abstractText": "Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art, called Lazy Clause Generation (LCG), uses resolution to derive nogoods expressing the reasons for each search failure. Such nogoods can prune other parts of the search tree, producing exponential speedups on a wide variety of problems. Nogood learning solvers can be seen as resolution proof systems. The stronger the proof system, the faster it can solve a CP problem. It has recently been shown that the proof system used in LCG is at least as strong as general resolution. However, stronger proof systems such as extended resolution exist. Extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution. The primary problem in using extended resolution is to figure out exactly which literals are useful to introduce. In this paper, we show that we can use the structural information contained in a CP model in order to introduce useful literals, and that this can translate into significant speedups on a range of problems.", "creator": "TeX"}}}