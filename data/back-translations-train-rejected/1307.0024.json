{"id": "1307.0024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2013", "title": "Investigation of \"Enhancing flexibility and robustness in multi-agent task scheduling\"", "abstract": "Wilson et al. propose a measure of flexibility in project scheduling problems and propose several ways of distributing flexibility over tasks without overrunning the deadline. These schedules prove quite robust: delays of some tasks do not necessarily lead to delays of subsequent tasks. The number of tasks that finish late depends, among others, on the way of distributing flexibility.", "histories": [["v1", "Fri, 28 Jun 2013 20:20:27 GMT  (32kb,D)", "http://arxiv.org/abs/1307.0024v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["daan wilmer"], "accepted": false, "id": "1307.0024"}, "pdf": {"name": "1307.0024.pdf", "metadata": {"source": "CRF", "title": "Empirical Evaluation of Algorithms: Final assignment", "authors": [], "emails": [], "sections": [{"heading": null, "text": "In this paper, I examine the different flexibility distributions proposed by Wilson et al. and the differences in the number of infringements (tasks that are completed too late). I show a factor in the cases that leads to differences in the number of infringements, as well as two properties of the flexibility distribution that cause them to behave differently. Based on these results, I propose three new flexibility distributions. Depending on the type of delays, these new flexibility distributions perform as well or better than the distributions by Wilson et al."}, {"heading": "1 Introduction", "text": "Planning problems are very common in the real world. Tasks can often be divided into several activities that need to be performed, with some activities depending on the outcome of other activities. Just as common in the real world are delays: Unforeseen circumstances regularly extend the time required for activities beyond their initial planning. The question is: How can you deal with these delays? When the completion time of the entire project is at stake, the answer is simple: Start each activity as quickly as possible so that the project is completed as quickly as possible. Sometimes, however, it is not just the project itself, but the individual activities that need to be completed on time - for example, due to contracts with other parties. In this case, the number of delayed activities should be minimized. Wilson, Witteveen, Klos and Huisman have examined this problem in [2] using the concept of flexibility: the margin on schedule. They suggest making the schedule more flexible, allocating it to tasks to increase robustness. Wilson algorithm."}, {"heading": "2 Problem", "text": "In this section, I will provide a more detailed picture of the problem area. I will begin by describing the problem to be solved, followed by the contributions of Wilson et al. Finally, I will describe the problem and the purpose of this essay."}, {"heading": "2.1 Project Scheduling Problem", "text": "The project planning problem can be defined as follows: Given a set of tasks T, each of which have a length lt, and a set of priority constraints C-T-T, assign each task a start time st, so that the tasks that need to be completed, leaving them (t, u).C: st + lt \u2264 su. In other words: There are a set of tasks with a certain length, and some tasks require that tasks be completed before they can begin. The tasks that need to be completed to verify Xiv: 130 7.00 24v1 [cs.DS] 28 Jun 2013 for another task to begin, his predecessors are called, and the tasks that require this task to be completed before they can begin are called his successors. To explain it in a more formal way: let Pt and St be the series of predecessors and successors of t, respectively successors of t. Then Pt = {u-T: (t).C} and St = {u-alksal.T = poly.T = This problem can be solved pretty easy in as.T (t) and.T =.T)."}, {"heading": "2.2 Algorithm and outcomes", "text": "Wilson et al. describe three main methods of allocating flexibility to the tasks: maximum, balanced and weighted. For the balanced distribution, they calculate the greatest possible flexibility for each task and then minimize the square loss of flexibility. For the weighted version, they assign weights to the various tasks before minimizing the loss of flexibility, resulting in increased flexibility for tasks with higher weight. They propose three types of weighting: by the number of direct predecessors, the number of predecessors N moves away (their predecessors, their predecessors, etc. up to a distance of N), and the total number of predecessors (their predecessors, their predecessors, etc., until there is no more). Finally, the maximum distribution maximizes the total amount of flexibility in the schedule and then balances this flexibility without reducing the total number of flexibilities.In total, this comes down to five distributions: maximum, balanced distribution, weighted, weighted by the number of predecessors, weighted by the number of outcomes, weighted by the number of outcomes and the outcomes weighted by the predecessors."}, {"heading": "2.3 Problem statement", "text": "The results in [2] are quite interesting, but there is no real explanation. The two questions I will try to answer in this post are: 1. What properties of the instances cause them to cause more or less injury than other instances and why? 2. Which properties of the flexibility distributions cause them to perform better or worse, and why?"}, {"heading": "3 Study", "text": "In order to develop hypotheses about the problem that answer the questions of the problem, I conducted a study to examine the algorithm and the results currently available. In this study, I looked for correlations between the characteristics of the problem and performance in terms of the number of violations. These correlations were calculated using the cor () function in R, 1 to calculate the correlation coefficient between two datasets. I chose this approach over graphs because of the amount of data points to compare: there are 600 cases and four algorithms. This creates a very dense cloud in which nothing is visible except for the strongest correlations."}, {"heading": "3.1 Instance properties", "text": "These instances have many measurable properties, but I limited the number of options to four. I selected them based on the network description - how much information the metric provides about the network of tasks - and the ease of implementation to increase the time available for research.Average Narrow Width The average number of simultaneous tasks per unit of time when all tasks are scheduled at their earliest start time. This metric is similar to the inversion of the I2 metric defined in [1] for measuring network properties, and it is counted for all time units in [at, bt + lt), which measures the extent of parallelism - the number of tasks that can be planned in parallel - of the instance. This metric is similar to the inversion of the I2 metric defined in [1] for measuring network properties. Average Filled Width The average number of tasks completed per cycle is calculated with one simultaneous time unit."}, {"heading": "3.1.1 Results", "text": "All correlation data collected in this part of the study can be found in Appendix A. In these numbers, the following three observations can be made: 1. It appears that with few and small delays, there appears to be no correlation at all. 3. As the size of the delays increases, the correlations grow more strongly, but much stronger as the number of delays increases. 2. The average narrow width does not seem to be correlated at all with the number of violations. 3. The complexity therefore seems to be fairly strongly correlated to the number of violations. Of these three observations, the last one shows the strongest correlation between a metric and the performance of the algorithm. I am therefore focusing on this observation. Obviously, instances with higher complexity tend to have more violations. Since the number of tasks is the same in all cases, 1http: / / www.r-project.org / 2See also https: / en.wikipedia.org / wiki / Cyclomatic _ complexitya higher complexity implies a higher complexity task that higher complexity is a higher task."}, {"heading": "3.2 Flexibility distribution", "text": "In order to gain more insight into the flexibility distribution, I first analyzed the distributions. The diagrams can be found in Section C. In these diagrams, the containers are selected [\u2212 2, 0], (0, 2, 3].., so that the first container only counts tasks with a flexibility of zero (negative flexibilities are not possible).The first thing that stands out is the large spike in the first container, which indicates that on average between 30% and 45% of the tasks receive no flexibility at all. As this varies by almost 15 percentage points, the number of tasks that do not receive flexibility is the first property of the flexibility distribution to be measured and correlate with the number of violations. I will add two more properties to be measured in order to further examine the two explanations from the previous section regarding the correlations between complexity and the number of violations. Combining these measures, the following list results: Number of zeros The number of tasks that have zero (flexibility)."}, {"heading": "3.2.1 Results", "text": "All correlation results can be found in Appendix B. Table 9 shows the correlations between these ratios and the number of violations, and some very interesting observations can be made. Depending on the number of delayed tasks and the amount by which they are delayed, the correlation between the number of tasks without flexibility and the number of violations can be quite strong. The correlation seems to be the strongest when there are many small delays, and the weakest when there are many large delays. I suspect that this is due to delay effects: If a task has no flexibility at all, any delay it gets is passed on to its successors. In this case, the number of zeros makes little difference: delays are spread further, resulting in more violations. However, if the total amount of delay becomes too high, this effect could be negated because tasks get more delays than can be handled through flexibility.In this case, the number of zeros makes little difference: delays are diluted anyway, which leads to few delays."}, {"heading": "4 Hypotheses", "text": "Finally, I propose two hypotheses that answer the main question: 1. Differences in the performance of algorithms in different cases are caused by differences in the complexity of the instance, because in cases of higher complexity tasks have more successors, so that more infringements are propagated, thus increasing the number of infringements. 2. Differences between algorithms are caused by two factors: \u2022 The number of tasks that have no flexibility, because when there are more tasks without flexibility, delays are increased, which leads to more infringements. \u2022 The assignment of flexibility to tasks that have many successors, because when a task has more delays, more tasks are propagated, which leads to more infringements."}, {"heading": "5 Experiments", "text": "To test the hypotheses, I conducted another study and conducted several experiments. For each hypothesis, I describe what I have done, show the results, and conclude whether the hypotheses are supported by the data."}, {"heading": "5.1 First hypothesis", "text": "To test the first hypothesis, I examined the original data. This is possible because the instances used have only three complexity values: 63, 100 and 137. I divided the instances by the complexity level and took the number of violations for the weighted distribution, based on all predecessors, with 80% of the tasks delayed by 80%. These settings were chosen because the correlations in these settings are the strongest of all measurements. Figure 1 shows the dispersion of the number of violations in instances with different complexity values. It is quite clear that the means of the three sets are different, which is confirmed by a t-test. However, the most important is the variance. For the combined amount, the variance is 99.3; the variance of the substances with a complexity of 63, 100 and 137 are 66.1, 49.6 and 30.0, respectively. This supports the statement that differences in the complexity of the instances cause a difference in the number of violations."}, {"heading": "5.2 Second hypothesis", "text": "To test the second hypothesis, I experimented with the distribution functions by adapting Michel Wilson's original code. I added three additional flexibility functions: wsucc Equalize the flexibility of tasks weighted by the number of successors. Finally, I balanced the flexibility while the schedule was completed before the deadline. Then, I maximized total flexibility in the schedule while keeping the flexibility of each task lower. Finally, I balanced the flexibility while ensuring maximum flexibility while maintaining the lower flexibility of each task."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, I have examined the findings of Wilson et al. [2]. I have raised two questions, both of which I have answered in this paper: 1. What properties of the instances cause them to cause more or less infringements than other cases, and why? I have found a strong correlation between the instances and the number of infringements that occur when delays are introduced. A causal relationship is likely, but has not yet been verified or refuted by further research. 2. What properties of the flexibility distributions cause them to perform better or worse, and why? The Flexibility Distribution Study has shown that the difference between performance as a function of the delays is caused by the number of tasks that receive zero flexibility and by the concentration of flexibility in tasks with many successors. Further experiments support this conclusion by improving the algorithms from the original paper in two different cases.The studies and experiments are by no means exhaustive, and therefore the answers to the questions that are maximized may not be complete."}, {"heading": "A Correlations of instance properties", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Correlations of flexibility distribution properties", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Flexibility distribution over tasks", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "An evaluation of the adequacy of project network generators with systematically sampled networks", "author": ["Mario Vanhoucke", "Jos Coelho", "Dieter Debels", "Broos Maenhout", "Lus V. Tavares"], "venue": "European Journal of Operational Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This metric is similar to the inverse of the I2 metric defined in [1] for measuring network characteristics.", "startOffset": 66, "endOffset": 69}], "year": 2013, "abstractText": "Wilson et al. propose a measure of flexibility in project scheduling problems and propose several ways of distributing flexibility over tasks without overrunning the deadline. These schedules prove quite robust: delays of some tasks do not necessarily lead to delays of subsequent tasks. The number of tasks that finish late depends, among others, on the way of distributing flexibility. In this paper I study the different flexibility distributions proposed by Wilson et al. and the differences in number of violations (tasks that finish too late). I show one factor in the instances that causes differences in the number of violations, as well as two properties of the flexibility distribution that cause them to behave differently. Based on these findings, I propose three new flexibility distributions. Depending on the nature of the delays, these new flexibility distributions perform as good as or better than the distributions by Wilson et al.", "creator": "LaTeX with hyperref package"}}}