{"id": "1307.0253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2013", "title": "Exploratory Learning", "abstract": "In multiclass semi-supervised learning (SSL), it is sometimes the case that the number of classes present in the data is not known, and hence no labeled examples are provided for some classes. In this paper we present variants of well-known semi-supervised multiclass learning methods that are robust when the data contains an unknown number of classes. In particular, we present an \"exploratory\" extension of expectation-maximization (EM) that explores different numbers of classes while learning. \"Exploratory\" SSL greatly improves performance on three datasets in terms of F1 on the classes with seed examples i.e., the classes which are expected to be in the data. Our Exploratory EM algorithm also outperforms a SSL method based non-parametric Bayesian clustering.", "histories": [["v1", "Mon, 1 Jul 2013 01:09:25 GMT  (825kb,D)", "http://arxiv.org/abs/1307.0253v1", "16 pages; European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2013"]], "COMMENTS": "16 pages; European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bhavana dalvi", "william w cohen", "jamie callan"], "accepted": false, "id": "1307.0253"}, "pdf": {"name": "1307.0253.pdf", "metadata": {"source": "CRF", "title": "Exploratory Learning", "authors": ["Bhavana Dalvi", "William W. Cohen", "Jamie Callan"], "emails": ["callan}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this case, it is sometimes the case that the number of classes present in the data is not known. Thus, for example, the task of classifying noun phrases into a large hierarchical group of categories such as \"person,\" \"organization,\" \"sports team,\" etc., as performed in broadly-dominated information systems (e.g., [5]). A sufficiently large corpus will certainly contain some unexpected natural clusters - e.g. types of musical scales or types of dental procedures. Therefore, it is unrealistic to assume that some examples have been provided for each class: a plausible assumption is that an unknown number of classes exist in the data, and that highlighted examples have been provided for some subgroups of these classes. This raises the natural question of how robust existing SSL methods are to unforeseen classes, as we experimentally show below, SSL methods can work quite poorly in this setting."}, {"heading": "2 Exploratory SSL Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 A Generic Exploratory Learner", "text": "In a typical EM setting, the Mstep finds the best parameters for adapting to the data, X l, Xu, and the E-step probably labels the unknown points with a distribution over the known classes C1, C2,. Ck. In some variants of EM, including the one we are looking at here, a \"hard\" mapping to classes is made, rather than an approach called classification EM [6]. Our exploratory version of EM differs in that it can introduce new classes Ck + 1. Cm during the E-step. Algorithm 1 EM algorithm for explorative learning with model selection 1: Function Exploratory EM (Xl, Y l, Xu): {C1. Ck}): {Ck + 1. Cm}, Y u 2: Input: Xl labeled data points; Y l labeling for datapoints Xl; Cu labeled classes (space as Xamek)."}, {"heading": "2.2 Discussion", "text": "Friedman [13] proposed the structural EM algorithm, which combines the standard EM algorithm that optimizes parameters with the structure search for model selection.This algorithm learns networks based on punished probability values in the absence of data. In each iteration, it evaluates multiple models based on the expected results of models with missing data and selects the model with the best expected score. This algorithm converges at local highs for the punished log probability (the score includes the penalty for increased model complexity).Similar to structural EM, in each iteration of algorithm 1, we evaluate two models, one with and one without additional classes. These two models are evaluated using a model selection criterion such as AIC or BIC, and the model with the best punished data probability number is selected in each iteration."}, {"heading": "2.3 Model Selection", "text": "For model penalties, we have tried several well-known criteria such as BIC, AIC and AICc. Burnham and Anderson [4] have experimented with AIC criteria and proposed AICc for datasets where the number of data points is less than 40 times the number of features. Formulas for evaluating a model on the basis of the three criteria we have tried are: BIC (g) = \u2212 2 \u0445 L (g) + v \u043a ln (n) (1) AIC (g) = \u2212 2 \u0445 L (g) + 2 \u0445 v (2) AICc (g) = AIC (g) + 2 \u0445 v (v + 1) / (n \u2212 v \u2212 1) (3) where g is the model to be evaluated, L (g) is the protocol probability of the given data g, v is the number of free parameters of the model and n is the number of data points. When comparing two models, a lower value is preferred."}, {"heading": "2.4 Exploratory versions of well-known SSL methods", "text": "In this section we will consider various SSL techniques and propose extensions of these algorithms, often referred to as \"K.\" Semi-Supervised Naive Bayes Nigam et al. [21] proposed semi-supervised version of the multinomial Naive Bayes. In this model P (Cj | x) the characteristics are considered as word events, and the number of results of multinomial production is the vocabulary size.This method can of course be used as an instance of exploration EM, using the multinomial model to P (Cj x) in line 1. The M step is also trivial, and only requires estimates of P (w | Cj)."}, {"heading": "2.5 Strategies for inducing new clusters/classes", "text": "In this section, we describe some possible strategies for introducing new classes in the E-step of the algorithm. They are detailed in algorithms 2 and 3, and each of these strategies is a possible implementation of the \"nearUniform\" subroutine of algorithm 1. As mentioned above, the intuition is that new classes should be introduced to keep x if the probability that x belongs to existing classes is roughly the same. In the JS criterion, we demand that Jensen-Shanon deviation 1 between the rear class distribution for x and the even distribution is less than 1k. The MinMax criterion is a slightly simpler approach to this intuition: A new cluster is introduced if the maximum probability is not more than twice as high as the minimum probability."}, {"heading": "2.6 Baseline Methods", "text": "Next, we will take a look at various basic methods we have implemented to measure the effectiveness of our proposed approach.1 The Jensen-Shannon divergence between p and q is the average deviation from p and q to a, the average of p and q, i.e., 12 (KL (p | a + KL (q | a))).Algorithm 4 Exploratory Gibbs Sampling with Chinese Restaurant Process 1: function GibbsCRP (Xl, Y l, Xu, {C1. Ck}): Ck + 1. Cm, Y 2: Input: Xl labeled data points of Xl; Xu unlabeled data points; {C1. Ck} set of known classes x's belong to; Pnew probability of creating a new class.3: Output: Cm newly discovered classes."}, {"heading": "3 Experimental Results", "text": "We are now trying to answer experimentally the questions raised in the introduction: How robust are the existing SSL methods if they are given incorrect information about the number of classes and seeds in the data for only some of these classes? Do the exploratory versions of the SSL methods perform better? How does Exploratory EM compare with the existing \"explorative\" method of the Gibbs sample with CRP? We used three publicly available records for our experiments: the first is the widely used data set of the 20 newsgroups [23]. We used the data set \"bydate,\" which contains a total of 18,774 text documents with a vocabulary size of 61,188. There are 20 non-overlapping classes and the entire data set is labeled. The second data set is the data set Delicious Sports, which was published by [9], which is an entity classification data set containing data sets extracted from 57K pages of the sports domain (the social HTML)."}, {"heading": "3.1 Exploratory EM vs. SemisupEM with few seed classes", "text": "This year, it is so far that it will only take a few days for it to come to a conclusion."}, {"heading": "3.2 Comparison with the Chinese Restaurant Process", "text": "As discussed in Section 2.6, a seed version of the Chinese Restaurant Process with Gibbs sampling (CRPGibbs) is an alternative exploratory learning algorithm. For visualization purposes, the classes introduced have been optimally aligned with the actual classes. In this section, we compare the performance of CRPGibbs with Explore KMeans and SemisupKMeans. We consider two versions of CRP-Gibbs, one using the standard CRP and one using our proposed modified CRP criterion for creating new classes, which is sensitive to the approximate uniformity of the rear class distribution. CRP-Gibbs uses the same instance representation as our K-Means variants, i.e. L1 normalized TFIDF characteristics. It is well known that CRP is sensitive to the Pnew concentration parameter. Figures 5 and 6 show the performance of all exploration methods, as well as SemisupKans plands, the concentration parameters of Exploration Parameters of 10 and Exoration Parameter \u2212 CRP \u2212 close \u2212 the results."}, {"heading": "4 Related Work", "text": "This year, it has come to the point where you are able to live in a country where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, and where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern such a country."}, {"heading": "5 Conclusion", "text": "In this paper, we examine and improve the robustness of SSL methods in an environment where the seeds are only available for a subset of classes - the subset of most interests for the end user. We have conducted systematic experiments for fully labeled multicultural problems in which the number of classes is known. We have shown that a user provides seeds for only some but not all classes, then the performance of SSL is degraded for several popular EM methods."}], "references": [{"title": "Clustering on the unit hypersphere using von mises-fisher distributions", "author": ["A. Banerjee", "I.S. Dhillon", "J. Ghosh", "S. Sra"], "venue": "In JMLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Semi-supervised clustering by seeding", "author": ["S. Basu", "A. Banerjee", "R. Mooney"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Adaptive mixture discriminant analysis for supervised learning with unobserved classes", "author": ["C. Bouveyron"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Multimodel inference understanding aic and bic in model selection", "author": ["K.P. Burnham", "D.R. Anderson"], "venue": "Sociological methods & research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Coupled semi-supervised learning for information extraction", "author": ["A. Carlson", "J. Betteridge", "R.C. Wang", "E.R. Hruschka", "Jr.", "T.M. Mitchell"], "venue": "In WSDM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A classification em algorithm for clustering and two stochastic versions", "author": ["G. Celeux", "G. Govaert"], "venue": "Computational statistics & Data analysis,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Intelligent choice of the number of clusters in k-means clustering: An experimental study with different cluster spreads", "author": ["M.M.-T. Chiang", "B. Mirkin"], "venue": "J. Classification,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Very fast similarity queries on semi-structured data from the web", "author": ["B. Dalvi", "W. Cohen"], "venue": "In SDM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Websets: Extracting sets of entities from the web using unsupervised information extraction", "author": ["B. Dalvi", "W. Cohen", "J. Callan"], "venue": "In WSDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Probabilistic dyadic data analysis with local and global consistency", "author": ["X.W. Deng Cai", "X. He"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning parameters of the k-means algorithm from subjective human", "author": ["H. Dutta", "R. Passonneau", "A. Lee", "A. Radeva", "B. Xie", "D. Waltz", "B. Taranto"], "venue": "annotation. FLAIRS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Web-scale information extraction in knowitall", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yates"], "venue": "In WWW,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A structural em algorithm for phylogenetic inference", "author": ["N. Friedman", "M. Ninio", "I. Pe\u2019er", "T. Pupko"], "venue": "Journal of Computational Biology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D. Griffiths", "M. Tenenbaum"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Learning the k in k-means", "author": ["G. Hamerly", "C. Elkan"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Emerging topic detection using dictionary learning", "author": ["S.P. Kasiviswanathan", "P. Melville", "A. Banerjee", "V. Sindhwani"], "venue": "In CIKM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Integrating novel class detection with classification for concept-drifting data streams", "author": ["M.M. Masud", "J. Gao", "L. Khan", "J. Han", "B. Thuraisingham"], "venue": "In ECML/PKDD", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Unsupervised discovery of negative categories in lexicon bootstrapping", "author": ["T. McIntosh"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A methodology for workload characterization of e-commerce sites", "author": ["D.A. Menasce", "V.A.F. Almeida", "R. Fonseca", "M.A. Mendes"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Discovering relations between noun categories", "author": ["T. Mohamed", "E. Hruschka Jr.", "T. Mitchell"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "A. Moore"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "New regularized algorithms for transductive learning", "author": ["P. Talukdar", "K. Crammer"], "venue": "In ECML-PKDD", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schrodl"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Bayesian k-means as a maximization-expectation algorithm", "author": ["M. Welling", "K. Kurihara"], "venue": "In ICDM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Textrunner: Open information extraction on the web", "author": ["A. Yates", "M. Cafarella", "M. Banko", "O. Etzioni", "M. Broadhead", "S. Soderland"], "venue": "In NAACL,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "In this paper, Exploratory EM is instantiated to produce exploratory versions of three well-known SSL methods: semi-supervised Naive Bayes, seeded K-Means, and a seeded version of EM using a von Mises-Fisher distribution [1].", "startOffset": 221, "endOffset": 224}, {"referenceID": 13, "context": "We then compare against an alternative exploratory SSL approach, namely Gibbs sampling with Chinese restaurant process [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "In some variants of EM, including the ones we consider here, a \u201chard\u201d assignment is made to classes instead, an approach named classification EM [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 12, "context": "2 Discussion Friedman [13] proposed the Structural EM algorithm that combines the standard EM algorithm, which optimizes parameters, with structure search for model selection.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "Say this model switch happens at iteration tswitch, then from iteration 1 to tswitch, Algorithm 1 acts like the structural EM algorithm [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "Burnham and Anderson [4] have experimented with AIC criteria and proposed AICc for datasets where, the number of datapoints is less than 40 times number of features.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "[21] proposed an EM-based semi-supervised version of multinomial Naive Bayes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Basu and Mooney [2] proposed a seeded version of K-Means, which is very analogous to Nigam et al\u2019s semi-supervised Naive Bayes, as another technique for semisupervised learning.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "[1], who described an EM algorithm that is directly inspired by K-Means and TFIDF-based representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "This natural extension of Banerjee et al[1]\u2019s work can be easily extended to our exploratory setting.", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "A seeded Gibbs sampler with CRP: The Exploratory EM method is broadly similar to non-parametric Bayesian methods, such as the Chinese Restaurant process (CRP) [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "The second dataset is the Delicious Sports dataset, published by [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For instance, Nigam et al\u2019s early work on SSL based EM with multinomial Naive Bayes [21] noted that adding too much unlabeled data sometimes hurt performance on SSL tasks, and discusses several reasons this might occur, including the possibility that there might not be a one-to-one correspondence between the natural mixture components (clusters) and the classes.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "More recently, McIntosh [18] described heuristics for introducing new \u201cnegative categories\u201d in lexicon bootstrapping, based on a domain-specific heuristic for detecting semantic drift with distributional similarity metrics.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 21, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 14, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 6, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 18, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 25, "context": "There has also been substantial work in the past to automatically decide the right \u201cnumber of clusters\u201d in unsupervised learning [11,22,15,7,19,27].", "startOffset": 129, "endOffset": 147}, {"referenceID": 24, "context": "There is also a substantial body of work on constrained clustering; for instance, Wagstaff et al [26] describe a constrained clustering variant of K-Means \u201cmust-link\u201d and \u201ccannot-link\u201d constraints between pairs.", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "In the modified adsorption algorithm [25], one such graphbased label propagation method, each datapoint can be marked with one or more known labels, or a special dummy label meaning \u201cnone of the above\u201d.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "One of our benchmark tasks, entity classification, is inspired by the NELL (Never Ending Language Learning) system [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "One subproject within NELL [20] uses a clustering technique for discovering new relations between existing noun categories\u2014relations not defined by the existing handdefined ontology.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Another line of research considers the problem of \u201copen information extraction\u201d, in which no classes or seeds are used at all [28,12,9].", "startOffset": 126, "endOffset": 135}, {"referenceID": 11, "context": "Another line of research considers the problem of \u201copen information extraction\u201d, in which no classes or seeds are used at all [28,12,9].", "startOffset": 126, "endOffset": 135}, {"referenceID": 8, "context": "Another line of research considers the problem of \u201copen information extraction\u201d, in which no classes or seeds are used at all [28,12,9].", "startOffset": 126, "endOffset": 135}, {"referenceID": 15, "context": "[16] assumes the number of novel topics is given as input to the algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] develop techniques on streaming data to predict whether next data chunk is novel or not.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Bouveyron [3] worked on the EM approach to model unknown classes, but the entire EM algorithm is run for multiple numbers of classes.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "[24] defines a problem of learning a function over the data space that isolates outliers from class instances.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "In multiclass semi-supervised learning (SSL), it is sometimes the case that the number of classes present in the data is not known, and hence no labeled examples are provided for some classes. In this paper we present variants of well-known semi-supervised multiclass learning methods that are robust when the data contains an unknown number of classes. In particular, we present an \u201cexploratory\u201d extension of expectation-maximization (EM) that explores different numbers of classes while learning. \u201cExploratory\u201d SSL greatly improves performance on three datasets in terms of F1 on the classes with seed examples\u2014i.e., the classes which are expected to be in the data. Our Exploratory EM algorithm also outperforms a SSL method based non-parametric Bayesian clustering.", "creator": "LaTeX with hyperref package"}}}