{"id": "1307.1275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice", "abstract": "This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing three-level representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.", "histories": [["v1", "Thu, 4 Jul 2013 11:10:45 GMT  (261kb)", "http://arxiv.org/abs/1307.1275v1", "6 pages, 1 figure, Presented at the Workshop on Representation Learning, ICML 2013"]], "COMMENTS": "6 pages, 1 figure, Presented at the Workshop on Representation Learning, ICML 2013", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["fangxiang feng", "ruifan li", "xiaojie wang"], "accepted": false, "id": "1307.1275"}, "pdf": {"name": "1307.1275.pdf", "metadata": {"source": "META", "title": "Constructing Hierarchical Image-tags Bimodal Representations  for Word Tags Alternative Choice", "authors": ["Fangxiang Feng", "Ruifan Li", "Xiaojie Wang"], "emails": ["f.fangxiang@gmail.com", "rfli@bupt.edu.cn", "xjwang@bupt.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "The multimodal learning challenge of ICML 2013 aims to develop a prediction system for word tags using bimodal data: images and text. Specifically, the data used in this competition includes two groups: the Small ESP Game Dataset (by Ahn & Dabbish, 2004), which was presented at the ICML Workshop on Representation Learning, Atlanta, Georgia, USA, 2013. ESP consists of 100,000 labeled images with tags. The GF consists of 1,000 test examples, which are presented in three versions: one image and two annotations, i.e. a correct description and an incorrect one. The GF is still evenly divided into public test sets and private test sets. The performance of the predictions is based on the accuracy of predicting two images, each showing two major differences between these two sets of data."}, {"heading": "2. System Architecture", "text": "The main idea of our solution is that we strive to construct hierarchical representations of bimodal data for the selection of word tags. In the training phase, we present the data in three successive steps. In the first stage, the low representations for these two data types are obtained in each case. For images, the characteristics released by the competition organizer, which are extracted from four descriptors in MPEG-7, and the basic characteristics of the images are combined to form the level-1 representations. For tagwords, the typical bag-of-words model is used for level-1 representations. In the second stage, the level-1 representations for picture and tag words are distilled to form the level-2 representation with two stacked Boltzmann machines (RBMs). In the third stage, we propose a quasi-siamese auto-encoder for learning the level-3-like representations of these pictorial data. In this section, the details of our architecture are presented with the three images."}, {"heading": "3. Obtaining Level 1 Representations", "text": "Due to the bi-modal nature of this contest, we represent our input data from two perspectives: image and text. For image representation, we adopt three types of characteristics: the characteristics of the contest organizer, the MPEG-7, and gist descriptors. The competition organizer released some extracted image characteristics with 816 dimensions. We remove the invalid 408 all-zero dimensions, reducing the size of the characteristics from 816 to 408. In addition, we use MPEG-7 and gist descriptors. Part of MPEG-7 is a standard for visual descriptors, which are defined in MPEG-7 for image representation: Color Layout (CL), Color Structure (CS), Edge Histogram (EH), and Scalable Color Color (SC)."}, {"heading": "4. Learning Level 2 Representations", "text": "In the second step, we use RBMs to construct the Level 2 representations. These Level 1 representations, which were achieved in the first stage for images and keywords, have different properties. That is, the Level 1 representations of images have real values and those of keywords have multiple 1 / 0 values. We model these two types of data using different variants of keywords: Gaussian Bernoulli RBM or Replicated softmax. Below, we describe some key points of these learning machines."}, {"heading": "4.1. Restricted Boltzmann Machines", "text": "RBM (Smolensky, 1986) is an undirected graphical model with stochastic binary units in the visible layer and hidden layer, but no connections between units within these two layers. Since there are n visible units v and m hidden units h, and each unit is distributed by the Bernoulli distribution with logistic activation function \u03c3 (x) = 1 / (1 + exp (\u2212 x), we then define a common probable distribution of visible units v and hidden units hp (v, h) = 1Z exp (\u2212 E (v, h)) (1), in which Z is the normalization constant and E (v, h) the energy function that can be learned by configuring all units asE (v, h) = \u2212 n \u00b2 i = 1 m \u00b2 j = 1 wijvihj \u2212 n = 1 wijvihj = 1 bivi \u2212 m \u00b2 viltj = 1 viltj (2) viltj = 1 viltj (2) by maximizing the input parameters (2)."}, {"heading": "4.2. Modeling Real-valued Data", "text": "We model the really valuable data using Gaussian RBM, which is an extension of the binary RBM and replaces the Bernoulli distribution with the Gaussian distribution for the visible data (Welling et al., 2004). The energy function of various configurations of visible and hidden units is E (v, h) = \u2212 n \u2211 i = 1 m \u2211 i = 1 wij vi \u03c3i hj + n \u2211 i = 1 (vi \u2212 bi) 2 2\u03c32 \u2212 m \u0445 j = 1 cjhj (6) The gradient of the log probability function is: \u2202 log p (v) \u0445 wij = vi \u03c3i hj data \u2212 vi \u03c3i hj model (7) Normally we define the variances \u03c32 = 1 for all visible units."}, {"heading": "4.3. Modeling Count Data", "text": "For counting data, we use Replicated Softmax Model (Salakhutdinov & Hinton, 2009) to model these sparse vectors.The energy function of the Sate configurations is defined as follows: E (v, h) = \u2212 n \u2211 i = 1 m \u2211 j = 1 wijvihj \u2212 n \u2211 i = 1 bivi \u2212 M m \u2211 j = 1 cjhj (8), where M is the total number of words in a document. Note that this replicated Softmax model can be interpreted as an RBM model that uses a single visible multinomic unit with support 1,..., K, which is scanned M times. That is, for each document, we create a separate RBM with as many Softmax units as there are words in the documentation. We can learn the entire model efficiently by applying the contrasting divergence approximation (Hinton, 2002).In our solution, we stack the two modalities for each one, we learn to split the two."}, {"heading": "5. Learning Level 3 Representations", "text": "In the third stage, these Siamese neural networks are used for a single representation; the Siamese architecture of the neural networks is originally proposed for signature verification (Bromley et al., 1993); the network takes a pair of signature patterns either from the same person or not as inputs; the simple distance for approaching the \"semantic\" distance in the input space is achieved by mapping these two patterns using these non-linear sub-networks; the Siamese architecture has been successfully applied to facial recognition (Chopra et al., 2005), dimensionality reduction (Salakhutdinov & Hinton, 2007), and speech recognition (Chen & Salman, 2011)."}, {"heading": "6. Choosing Alternatives", "text": "We have two strategies: a general strategy and a data-specific strategy. The general strategy is straightforward. To be precise, we can include a pair of image-Pi and one of its keywords Qi into the network, the compatibility LC (Pi, q-i) between the image Pi and the other keywords q-i. Finally, the keywords with greater compatibility are chosen as the correct link for this image. Although this general strategy is applicable, we can work out another more precise strategy by looking at characteristics of the data.The data-specific strategy is based on an observation. To emphasize the compatibility, for each image in GF, the incorrect link of image is chosen. Although this general strategy is applicable, we highlight another more precise strategy by considering characteristics of the data.The data-specific strategy is based on an observation."}, {"heading": "7. Experiments and Final Results", "text": "In this section, we report on our experimental details and their results. In all our experiments, we use only the ESP and GF datasets provided by the organizer, although additional datasets can be used to train this model. Descriptions and some features of the ESP and GF datasets were given in Section 1. We publish our implementation code at https: / / github.com / FangxiangFeng / deepnet, which is based on Nitish Srivastava's DeepNet library. The ESP dataset only has the correct keywords for each image. Therefore, we need to generate an incorrect counterpart for word marks for each image in this dataset. This can be achieved by randomly selecting one of the correct keywords of the remaining images, while ensuring that each of the keywords occurs for only one time.In the training phase, the level 1, level 2, level 3 representations of Eu.The settings for learning level 1 are automatically extracted."}, {"heading": "8. Discussion and Conclusions", "text": "Our results show that the solution to this problem is effective. We believe that the strategy applied in the choice of alternatives is important, and in moderate representations it is sufficient to make an accurate choice. Therefore, we have not fine-tuned the parameters very carefully and the learning cycles are reduced to speed up the whole learning process. Finally, we construct a hierarchical bimodal representation and data-specific strategy for the choice of the word mark. These bimodal representations are achieved by three-stage extractions. In the first stage, the level-1 representations are achieved by extracting from images and texts using typical methods. In the second stage, the level-2 representations are learned by two consecutive RBMs for each modality. In the third stage, a quasi-Siamese auto-encoder is proposed to learn the level-3 representations. In the selection of alternatives, we strive to find the maximum discrepancy between linking images from a data feature to observation."}, {"heading": "Acknowledgments", "text": "We thank the organizers for organizing this interesting competition and Nitish Srivastava for providing his DeepNet library. Part of the work was supported by the National Sciences Foundation of China (No. 61273365) and the Fundamental Research Funds for the Central Universities (No. 2013RC0304)."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Signature verification using a \u201dSiamese\u201d time delay neural network", "author": ["J. Bromley", "I. Guyon", "Y. Le Cun", "E. Saeckinger", "R. Shah"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Extracting speaker-specific information with a regularized Siamese deep network", "author": ["K. Chen", "A. Salman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chen and Salman,? \\Q2011\\E", "shortCiteRegEx": "Chen and Salman", "year": 2011}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "In Proceedings of the 2005 IEEE", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Sight and sound converge to form modality-invariant representations in temporoparietal cortex", "author": ["K. Man", "J.T. Kaplan", "A. Damasio", "K. Meyer"], "venue": "Journal of Neuroscience,", "citeRegEx": "Man et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Man et al\\.", "year": 2012}, {"title": "Multimodal deep learning", "author": ["J Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1. chapter Information processing in dynamical systems: foundations of harmony theory, pp. 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Multimodal learning with deep Boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2012\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov", "year": 2012}, {"title": "Labeling images with a computer game", "author": ["L. von Ahn", "L. Dabbish"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Ahn and Dabbish,? \\Q2004\\E", "shortCiteRegEx": "Ahn and Dabbish", "year": 2004}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Welling et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2004}, {"title": "Predicting structured data. chapter A Tutorial on Energy-Based Learning, pp. 1\u201359", "author": ["Yann LeCun", "Sumit Chopra", "Huang", "Fu-Jie"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "RBM (Smolensky, 1986) is an undirected graphical model with stochastic binary units in visible layer and hidden layer but without connections between units within these two layers.", "startOffset": 4, "endOffset": 21}, {"referenceID": 13, "context": "We model the real-valued data using Gaussian RBM, which is an extension of the binary RBM replacing the Bernoulli distribution with Gaussian distribution for the visible data (Welling et al., 2004).", "startOffset": 175, "endOffset": 197}, {"referenceID": 4, "context": "We can efficiently learn all the model by using the Contrastive Divergence approximation (CD) (Hinton, 2002).", "startOffset": 94, "endOffset": 108}, {"referenceID": 5, "context": "These two-layer stacked RBMs can be trained by greedy layer-wise method (Hinton et al., 2006; Bengio et al., 2007).", "startOffset": 72, "endOffset": 114}, {"referenceID": 0, "context": "These two-layer stacked RBMs can be trained by greedy layer-wise method (Hinton et al., 2006; Bengio et al., 2007).", "startOffset": 72, "endOffset": 114}, {"referenceID": 1, "context": "The Siamese architecture of neural networks is originally proposed for signature verification (Bromley et al., 1993).", "startOffset": 94, "endOffset": 116}, {"referenceID": 3, "context": "Incorporated by deep learning, the Siamese architecture has been successfully applied to face recognition (Chopra et al., 2005), dimensionality reduction (Salakhutdinov & Hinton, 2007), and speech recognition (Chen & Salman, 2011).", "startOffset": 106, "endOffset": 127}, {"referenceID": 7, "context": "Recent advances in multimodal deep learning have seen a trend to learn a joint representation by fusing different modalities (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).", "startOffset": 125, "endOffset": 179}, {"referenceID": 6, "context": "(Man et al., 2012) suggests that information from different sensory channels converges somewhere in the brain to possibly form modalityinvariant representations.", "startOffset": 0, "endOffset": 18}], "year": 2013, "abstractText": "This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing threelevel representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.", "creator": "LaTeX with hyperref package"}}}