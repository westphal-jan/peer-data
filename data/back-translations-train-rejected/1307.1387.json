{"id": "1307.1387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Examining the Classification Accuracy of TSVMs with ?Feature Selection in Comparison with the GLAD Algorithm", "abstract": "Gene expression data sets are used to classify and predict patient diagnostic categories. As we know, it is extremely difficult and expensive to obtain gene expression labelled examples. Moreover, conventional supervised approaches cannot function properly when labelled data (training examples) are insufficient using Support Vector Machines (SVM) algorithms. Therefore, in this paper, we suggest Transductive Support Vector Machines (TSVMs) as semi-supervised learning algorithms, learning with both labelled samples data and unlabelled samples to perform the classification of microarray data. To prune the superfluous genes and samples we used a feature selection method called Recursive Feature Elimination (RFE), which is supposed to enhance the output of classification and avoid the local optimization problem. We examined the classification prediction accuracy of the TSVM-RFE algorithm in comparison with the Genetic Learning Across Datasets (GLAD) algorithm, as both are semi-supervised learning methods. Comparing these two methods, we found that the TSVM-RFE surpassed both a SVM using RFE and GLAD.", "histories": [["v1", "Thu, 4 Jul 2013 16:06:25 GMT  (400kb)", "http://arxiv.org/abs/1307.1387v1", "UKCI 2011, the 11th Annual Workshop on Computational Intelligence, Manchester, pp 7-12"]], "COMMENTS": "UKCI 2011, the 11th Annual Workshop on Computational Intelligence, Manchester, pp 7-12", "reviews": [], "SUBJECTS": "cs.LG cs.CE", "authors": ["hala helmi", "jon m garibaldi", "uwe aickelin"], "accepted": false, "id": "1307.1387"}, "pdf": {"name": "1307.1387.pdf", "metadata": {"source": "CRF", "title": "Examining the Classification Accuracy of TSVMs with Feature Selection in Comparison with the GLAD Algorithm", "authors": ["Hala Helmi", "Jonathan M. Garibaldi", "Uwe Aickelin"], "emails": ["@cs.nott.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTIONData mining techniques have traditionally been used to extract hidden predictive information in many different contexts. Normally, datasets contain thousands of examples. Recently, growth in biology, medicine and DNA analysis has led to the accumulation of huge amounts of biomedical data that require in-depth analysis. After years of research and development, many data mining, machine learning, statistical analysis systems and tools that can be used in biodata analysis are available. Consequently, this work will explore a relatively new technique in data mining called Transductive Supervised Support Vector Machines [2], also known as Semi-Supervised Support Vector Machines S3VMs, located between supervised learning with fully labeled training data and unsupervised learning without labeled training data [1]. In this method, we have used both labeled and unlabeled Support Vector Vector Machines [2], Semi-Supervised Support Vector Vector Vector Machines S3S3M."}, {"heading": "A. Support Vector Machines", "text": "Support Vector Machine (SVM) is a supervised machine learning technique that has proven itself in several areas of biological research, including the evaluation of microarray expression data [4], the detection of remote protein homologies [5], and the detection of translation initiation sites [6]. SVMs have not only demonstrated the ability to correctly classify entities into appropriate classes, but also to identify cases where established classification is not supported by the data [7]. SVMs are a technique where samples are used to determine in advance which data should be pooled [4]."}, {"heading": "B. Tranductive Support Vector Machines", "text": "Transductive learning is a method that is strongly associated with semi-supervised learning, where semi-supervised learning is mediated between supervised and unsupervised learning. Vapnik introduced Semi-Supervised Learning for Support Vector Machines in the 1990s. He believed that transduction (TSVM) is preferable to induction (SVM) because induction has to solve a more general problem (derive a function) before it solves a more detailed one (computational results for new cases) [8] [9]. Transductive Support Vector Machines try to maximize the hyper-level classifier between two classes by means of marked training data; at the same time, this forces the hyperplane to be far away from the unmarked samples. TSVMs seem to be a perfect semi-supervised learning algorithm because they combine the regulation of Support Vector Machines with a direct implementation of cluster adoption [10]."}, {"heading": "C. Recursive Feature Elimination", "text": "Most predictive model algorithms are less effective when the size of the data set is large. There are several methods for reducing the amount of the feature set. From these methods, we selected a technique called Recursive Feature Elimination (RFE). The basis for RFE is to start with all the features that are least useful, select the feature that is least useful, remove that feature, and then repeat until a stop condition is reached. Finding the best feature elimination (RFE) is too expensive, so RFE reduces the difficulty of feature selection by being \"greedy\" [11]. III. METHODSThis section of the paper focuses on the description of TSVMRFE, the problem motivated by the task of classifying biomedical data. The goal is to examine classification errors using the Transductive Support Vector Machines Method (GLAD) to determine whether this method is an algorithm in combination with the Recursive Feature (RE) learning algorithm in the RLAD (Acminative Feature)."}, {"heading": "A. Support Vector Machines", "text": "The purpose of the SVM is to find a classifier with the largest difference between the samples, which relate to two different classes, in which the training error is minimized. To achieve this, we have a series of - dimensional training samples = {characterized by the core function: and their mapping by the core function ()."}, {"heading": "B. Transductive Support Vector Machines", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "C. Recursive Feature Elimination", "text": "In fact, it is not the case that we are able to analyze and analyze the results of the TSVM-RFE study. (...) The results of the TSVM-RFE study are unambiguous. (...) In addition, it is possible to analyze the results of the TSVM-RFE study. (...) This study first filters out some characteristics based on the results of the TSVM-RFE study. (...) The results of the TSVM-RFE study show that the TSVM-RFE study is able to analyze the results of the TSVM-RFE study. (...) The results of the TSVM-RFE study show that the TSVM-RFE study is able to analyze the results of the TSVM study. (...) The results of the TSVM-RFE study analyze the results of the SVM-RFE study. (...) The results of the TSVM-RFE study show that the SVM-RFE study is able to analyze the results of the SVM-RFE study."}, {"heading": "D. Genetic Learning Across Datasets (GLAD)", "text": "The GLAD algorithm differs from previous algorithms of semi-supervised learning. The GLAD algorithm was used as a wrapper method for the selection of characteristics. A Genetic Algorithm (GA) was implemented for these two types of data of cluster algorithms [14]. A unique two-term scoring function resulted in the labeled and unlabeled data samples being independently evaluated. \u2212 Generally, the score was calculated as a weighted average of the two terms shown below. \u2212 ScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreSolls-ScoreSolls-ScoreScoreSolls-ScoreSolls-ScoreScoreScoreSolls-ScoreScoreSolls-ScoreSolls-ScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreScoreSolls-ScoreSolls-ScoreSolls-ScoreSolls-Score-ScoreSolls-Score-ScoreSolls-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Solls-Score-Score-Score-Solls-Score-Score-Score-Solls-Score-Score-Score-Score-Solls-ScoreSolls-Score-Score-Score-Score-Score-Score-Solls-Solls-Score-Score-Score-Score-Score-Solls-Score-Score-Solls-Score-Score-Score-Score-Score-Solls-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-"}, {"heading": "A. Datasets", "text": "Leukemia (AML-ALL): including 7129 probes, two types of leukemia are available: acute myeloblastic leukemia (AML), 25 samples; and acute lymphoblastic leukemia (ALL), 47 samples [15]. Lymphoma (DLBCL): consisting of 7129 genes and 58 DLBCL samples. Diffuse large B-cell lymphoma (DLBCL) and 19 samples follicular lymphoma (FL) [16]. Chronic myeloid leukemia (CML): contained 30 samples (18 severe emphysema, 12 mild or no emphysema) with 22,283 human probes [17]."}, {"heading": "B. TSVM Recursive Feature Elimination (TSVM-RFE) Result", "text": "Leukemia (AML-ALL): The results for the leukemia ALL / AML dataset are summarized in Figure 1 in the diagram on the left. TSVM-RFE returned the lowest minimum error of 3.68% and compassionately smaller errors compared to SVM-RFE: 3.97% for 30, 40,..., 70 genes. Interestingly, both methods showed the lowest error in our experiments when 60 genes were used, which provided a reasonable suggestion for the number of relevant genes to be used for the leukemia data. Lymphoma (DLBCL): The results for the lymphoma dataset (DLBCL) are summarized in Figure 1 in the mean diagram. TSVM-RFE returned the smallest minimum error of 3.89% and fairly firmly smaller errors compared to the SVM-RFFE: 4.72% for 30, 40,..., 70 genes."}, {"heading": "C. Comparing the TSVM Algorithm result with the GLAD Algorithm", "text": "In fact, it is not as if it is a way in which people are able to determine themselves. In fact, in which they are able, it is as if they are able to determine themselves. In fact, it is as if people are able to determine themselves. In fact, it is as if people are able to determine themselves. In fact, it is as if people are able to determine themselves. In fact, it is as if people are able to determine themselves what they determine. In fact, it is not as if people are able to determine themselves. In fact, it is as if people are able to determine themselves."}], "references": [{"title": "Semisupervised Learning for Computational Linguistics", "author": ["S. Abney"], "venue": "(1st ed.). Chapman & Hall/CRC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Semi-Supervised Learning Tutorial", "author": ["X. Zhu"], "venue": "Department of Computer Sciences University of Wisconsin, Madison, USA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Biomarker discovery across annotated and unannotated microarray datasets using semi-supervised learning", "author": ["C. Harris", "N. Ghaffari"], "venue": "BMC Genomics 9 Suppl 2:S7,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Knowledge-based analysis of microarray gene expression data by using support vector machines", "author": ["M.P. Brown", "W.N. Grundy", "D. Lin", "N. Cristianini", "C.W. Sugnet", "T.S. Furey", "Ares", "M. Jr.", "D. Haussler"], "venue": "Proc. Natl. Acad. Sci. USA", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Using the Fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "D. Haussler", "M. Diekhans"], "venue": "In Proceedings of ISMB,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Engineering support vector machine kernels that recognize translation initiation", "author": ["A. Zien", "G. Ratsch", "S. Mika", "B. Scholkopf", "C. Lemmen", "A. Smola", "T. Lengauer", "Muller", "K.-R"], "venue": "sites\", Bioinformatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Pattern Recognition Techniques in Microarray Data Analysis", "author": ["F. Valafar"], "venue": "Annals of the New York Academy of Sciences", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Learning by Transduction", "author": ["A. Gommerman", "V. Vovk", "V. Vapnik"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Large Scale Transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "J. Mach. Learn. Res", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Least Square Transduction Support Vector Machine.", "author": ["R. Zhang", "W Wang"], "venue": "Neural Processing Letters", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Gene Selection for Cancer Classification using Support Vector Machines Machine Learning", "author": ["I. Guyon", "J Weston"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In NIPS,1998", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceedings of ICML-", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Data Mining: Concepts and Techniuqes", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring", "author": ["Golub", "T.R"], "venue": "Science, 286:531-537,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Diffuse large B-cell lymphoma outcome prediction by gene expression profiling and supervised machine learning", "author": ["Shipp", "M.A"], "venue": "Nature Medicine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Molecular profiling of CD34+ cells identifies low expression of CD7, along with high expression of proteinase 3 or elastase, as predictors of longer survival in patients with CML", "author": ["Yong", "A.S.M"], "venue": "Blood 2006,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "In Advances in Kernel Methods: Support Vector Machines,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}], "referenceMentions": [{"referenceID": 1, "context": "This technique is called Transductive Supervised Support Vector Machines [2], also named Semi-Supervised Support Vector Machines SVMs, located between supervised learning with fully-labelled training data and unsupervised learning without any labelled training data [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "This technique is called Transductive Supervised Support Vector Machines [2], also named Semi-Supervised Support Vector Machines SVMs, located between supervised learning with fully-labelled training data and unsupervised learning without any labelled training data [1].", "startOffset": 266, "endOffset": 269}, {"referenceID": 2, "context": "In section 3, the TSVM algorithm combined with RFE is detailed, as well as a brief summary of the GLAD algorithm based on a recently published paper [3] which aims to compare the prediction accuracy of these two algorithms.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 267, "endOffset": 270}, {"referenceID": 6, "context": "SVMs have demonstrated the ability not only to separate the entities correctly into appropriate classes, but also to identify instances where established classification is not supported by the data [7].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "SVMs are a technique that makes use of training that utilizes samples to determine beforehand which data should be clustered together [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "His view was that transduction (TSVM) is preferable to induction (SVM), since induction needs to solve a more general problem (inferring a function) before solving a more detailed one (computing outputs for new cases) [8] [9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "His view was that transduction (TSVM) is preferable to induction (SVM), since induction needs to solve a more general problem (inferring a function) before solving a more detailed one (computing outputs for new cases) [8] [9].", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "TSVMs seem to be a perfect semi-supervised learning algorithm because they combine the regularization of Support Vector Machines with a straight forward implementation of the clustering assumption [10].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Finding the best subset features is too expensive, so RFE decreases the difficulty of feature selection by being \u2018greedy\u2019 [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "The 1-norm and 2-norm TSVMs have been discussed in [12] and [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "The 1-norm and 2-norm TSVMs have been discussed in [12] and [13].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "It is worth mentioning the l\u0b36TSVM implemented in the SVMLight [18] [8].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "It is worth mentioning the l\u0b36TSVM implemented in the SVMLight [18] [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "The following estimation suggested in [11] is easier to measure.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "Linear Discriminant Analysis (LDA) and K-means (K = 2) for these two data forms of cluster algorithms were used [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "\uf0b7 Leukaemia (AML-ALL): including 7129 probes, two variants of leukaemia are available: acute myeloblastic leukaemia (AML), 25 samples; and acute lymphoblastic leukaemia (ALL), 47 samples [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "Diffuse large B-cell lymphoma (DLBCL) and 19 samples of follicular lymphoma (FL) [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "\uf0b7 Chronic Myeloid Leukaemia (CML): contained 30 samples (18 severe emphysema, 12 mild or no emphysema) with 22,283 human gene probe sets [17].", "startOffset": 137, "endOffset": 141}], "year": 2013, "abstractText": "Gene expression data sets are used to classify and predict patient diagnostic categories. As we know, it is extremely difficult and expensive to obtain gene expression labelled examples. Moreover, conventional supervised approaches cannot function properly when labelled data (training examples) are insufficient using Support Vector Machines (SVM) algorithms. Therefore, in this paper, we suggest Transductive Support Vector Machines (TSVMs) as semi-supervised learning algorithms, learning with both labelled samples data and unlabelled samples to perform the classification of microarray data. To prune the superfluous genes and samples we used a feature selection method called Recursive Feature Elimination (RFE), which is supposed to enhance the output of classification and avoid the local optimization problem. We examined the classification prediction accuracy of the TSVM-RFE algorithm in comparison with the Genetic Learning Across Datasets (GLAD) algorithm, as both are semi-supervised learning methods. Comparing these two methods, we found that the TSVM-RFE surpassed both a SVM using RFE and GLAD.", "creator": "Microsoft Word - UKCI-Hala Helmi.docx"}}}