{"id": "1307.1391", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm", "abstract": "Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded several criticisms about its underlying structure and operation. As a result, several alterations and fixes have been suggested in the literature to correct for these findings. A contribution of this work is to investigate the effects of replacing the classification stage of the DCA (which is known to be flawed) with a traditional machine learning technique. This work goes on to question the merits of those unique properties of the DCA that are yet to be thoroughly analysed. If none of these properties can be found to have a benefit over traditional approaches, then \"fixing\" the DCA is arguably less efficient than simply creating a new algorithm. This work examines the dynamic filtering property of the DCA and questions the utility of this unique feature for the anomaly detection problem. It is found that this feature, while advantageous for noisy, time-ordered classification, is not as useful as a traditional static filter for processing a synthetic dataset. It is concluded that there are still unique features of the DCA left to investigate. Areas that may be of benefit to the Artificial Immune Systems community are suggested.", "histories": [["v1", "Thu, 4 Jul 2013 16:19:21 GMT  (1118kb,D)", "http://arxiv.org/abs/1307.1391v1", "Proceedings of the 10th International Conference on Artificial Immune Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 173-186, 2011"]], "COMMENTS": "Proceedings of the 10th International Conference on Artificial Immune Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 173-186, 2011", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["feng gu", "jan feyereisl", "robert oates", "jenna reps", "julie greensmith", "uwe aickelin"], "accepted": false, "id": "1307.1391"}, "pdf": {"name": "1307.1391.pdf", "metadata": {"source": "CRF", "title": "Quiet in Class : Classification, Noise and the Dendritic Cell Algorithm", "authors": ["Feng Gu", "Jan Feyereisl", "Robert Oates", "Jenna Reps", "Julie Greensmith", "Uwe Aickelin"], "emails": ["fxg@cs.nott.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that the solution to these problems is not a purely theoretical question, but a purely theoretical one."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The Dendritic Cell Algorithm", "text": "The first phase of the DCA is an anomalous detection phase, in which the classification decisions of the population are monitored to identify anomalies; the second phase attempts to correlate the antigen sampled by the cells with the occurrence of detected anomalies. DCA receives two types of input, signal and antigen. Signals are represented as vectors of real values and are periodic measurements of characteristics within the problem environment. An assumption of the algorithm is that the presence or absence of an anomaly in the cell can be detected."}, {"heading": "2.2 Machine Learning Concepts", "text": "In our analysis, the classification level of the DCA is replaced by a tractable classifier based on the operation of vector machines (SVM). < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "2.3 Signal Processing Concepts", "text": "Filters can be considered algorithms or structures that apply an amplification (output-to-input ratio) to their input signal to generate a new output signal. Where filters differ from a simple amplifier, the amplification applied is a function of the frequency of the input. However, the mathematical function in relation to the frequency is referred to as the \"transmission function\" of the filter. In the field of signal processing, it is common practice to express filters by providing their transmission functions. For completeness, the filters used for this work are given here.The filter with the most analogous behavior to the DCA is the sliding window filter [18]. A sliding window filter is called as it is translated as a Bounding box along the input data. At each step t, the output of the sliding window filter is the average transmission size contained within the window. This is expressed in Equation 9, ot = 1W = miscellaneous."}, {"heading": "3 Research Aims", "text": "In order to justify future work on the DCA, it is necessary to assess the importance of their new features. In the literature, three new properties of the DCA remain unvalidated: the effect of antigen; the effect of dynamic filtering; and the effect of a population of classifiers. However, it is conceivable that the effect of dynamic filtering is most important. This is because the antigen effect does not yield positive results when the hypotheses-hypotheses-detection phase is insufficient and the classification phase probably does not yield positive results when the dynamic filters turn out to be insufficient. In order to verify the necessity of a filter of any kind, it is important to determine whether the issuance of a hypotheses-hypotheses-hypotheses improves the results of the classification when the use of time-driven hypotheses-hypotheses-data sets are considered insufficient by the population."}, {"heading": "4 Algorithmic Details", "text": "To investigate the merits of the sliding window effect of the DCA, it is necessary to separate it from the rest of the algorithm and use it in conjunction with a better understood classifier. In this investigation, two movable window functions are used as filters for processing the decisions made by a linear SVM. For a given training set, the linear SVM finds an optimal decision boundary and returns the predefined orthogonal distance from the decision boundary to each data point as defined in Equation 2. The movable window functions initialize either a set of window sizes or a set of decision thresholds and label the data instances within each generated moving window. An error function is used to find the optimal window size or decision threshold. The knowledge acquired in the training is then applied to classify data instances within the test situation.For clarity, the algorithmic combinations of a linear SVM with a static and dynamic sliding function are defined in the subsequent experiments to be used in the experiments."}, {"heading": "4.1 Static Moving Window Function", "text": "The function to determine the class name of each data instance in relation to a window size \u03b1l is defined as c: Rn \u00b7 N \u00b7 {\u00b1 1}, c (f (x), \u03b1l, i) = d n\u03b1l e = d n\u03b1l e partitions. The function to determine the class name of each data instance in relation to a window size \u03b1l is defined as c: Rn \u00b7 N \u00b7 N \u2192 {\u00b1 1}, c (f (x), \u03b1l, i) = d n\u03b1l e partitions. The function to determine the class name of each data instance in relation to a window size \u03b1l is defined as c: Rn \u00b7 N \u2192 {\u00b1 1}, c (f (x), \u03b1l, i) = d n\u03b1l e: k = 1 1Sk (i), sgn: c: c: c: c: c), c: c: c: c: c: c: c), c: c: c: c: c: c: c: c: c, c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c"}, {"heading": "4.2 Dynamic Moving Window Function", "text": "The dynamic window size of a decision threshold is defined by a series of initial decision thresholds (life span), and k [1, b, f (xi) \u03b2l] \u03b2l is the index of a moving window as a function of the decision threshold \u03b2l, where b \u00b7 c denotes the floor function. For each decision threshold \u03b2l, the moving windows are found by the following inequality, bk + 1l = arg maxa, n (bkl, n) | a (bkl, n), i = bkl, as a function of the floor function."}, {"heading": "5 Experimental Design", "text": "This section describes the techniques for implementing the interesting algorithms and the synthetic data required to test the null hypotheses outlined in Section 3. Details of the raw data, experimental results and statistical analyses included in this paper can be found in [11]."}, {"heading": "5.1 Synthetic Datasets", "text": "Synthetic data sets based on two Gaussian distributions are common practice in machine learning, as shown in [22]. This is due to the fact that variation in the distance between distributions allows control over the separability of the data. In the experiments in Musselle's work [16], where the temporal nature of the data is important, the author uses a Markov chain to generate synthetic data sets, in which the probability of a state change dictates the relative concentrations of normal and anomalous behavior, for which both the separability and the temporal order are important. Therefore, it was decided to use a data set based on two Gaussian distributions, and then to introduce an artificial temporal sequence, which is achieved by generating temporally varying signals representing the class characteristics. Each data set is divided into quarters, with the first and third quarters belonging to class I and the second and fourth quarters belonging to class."}, {"heading": "5.2 Algorithm Setup", "text": "The parameters used in the linear SVM are the default values of the R packet core laboratory [15] and remain the same for both moving window functions. For the static moving window function, the cardinality is the set of initial window sizes | A | 100, and a window size \u03b1l [1, 100]. For the dynamically moving window function, the cardinality is the set of initial decision thresholds | B | also 100, and a decision threshold \u03b2l is calculated as, \u03b2l = arg max xi-X [f (xi) | p (xi) | p (20), where \u03bb is a scaling factor that controls the window sizes taken into account by the parameter setting. If \u03bb = 1, windows are limited to values normally used in the DCA literature. With \u03bb = 100, parameters corresponding to those used by the static and dynamic moving window functions are also taken into account by the tuning process."}, {"heading": "5.3 Statistical Tests", "text": "All results are tested using the Shapiro-Wilk normalcy test to determine whether parametric or non-parametric statistical tests are appropriate [5]. All null hypotheses in Section 3 are formulated as the absence of a detectable significant difference between pairs of results. The two-sided student-t test is used for normally distributed samples, and the two-sided Wilcoxon-signed ranking test for abnormally distributed [5]. Where differences are detected, the one-sided versions of the relevant difference test are used to determine the relative performance of the results. A significance level of \u03b1 = 0.05 is considered sufficient for all statistical tests."}, {"heading": "6 Results and Analysis", "text": "The results of the experiments are presented with respect to the error rates, which correspond to the number of misclassified data instances divided by the total number of instances in the tested dataset. The error rates of the six methods tested across all datasets are measured against the Euclidean distance between the two class entroids in Fig. 2. For inseparable cases, the classification performance differs from method to method. To determine whether these differences are statistically significant, statistical tests are performed as follows. Shapiro-Wilk tests confirm that the data is not normally distributed (p values are less than 0.05), and therefore Wilcoxon tests are used to assess the statistical significance of both the bilateral and one-sided comparisons described above. As all p values are less than 0.05, we reject the null hypotheses of all two-sided Wilcoxon tests with a 95% certainty and conclude that significant differences between the three methods exist."}, {"heading": "7 Discussion and Future Work", "text": "The experimental results show that filtering the decisions of a linear classifier with contemporary and noise input data significantly alters and improves its classification performance. This was to be expected, because even if the data sets in the attribute space are inseparable, the temporal arrangement is such that the hyperplane has greater accuracy that the average of several instances of the same class tends towards the correct class designation. This can also be viewed from the frequency domain by introducing a high frequency noise component into the signal, which can be removed by filtering. The classification performance of the DCA is substantially different from that of a linear classifier, filtered or otherwise, because it is inseparable."}], "references": [{"title": "The Danger Project", "author": ["U. Aickelin", "S. Cayzer", "P. Bentley", "J. Greensmith", "J. Kim", "G. Tedesco", "J. Twycross"], "venue": "In http://ima.ac.uk/danger,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Behavioural Correlation for Malicious Bot Detection", "author": ["Y. Al-Hammadi"], "venue": "PhD thesis, School of Computer Science, University of Nottingham,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A Tutorial on Support Vector Mahinces for Pattern Recognition", "author": ["C.J.C. Buerges"], "venue": "Data Mining and Knowledge Discovery, 2:121\u2013167,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "FDCM: A Fuzzy Dendritic Cell Algorithm", "author": ["Z. Chelly", "Z. Elouedi"], "venue": "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 102\u2013115,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistics: An Introduction Using R", "author": ["M.J. Crawley"], "venue": "Wiley Blackwell,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Blackwell, 2nd edition,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Practical Methods of Optimization", "author": ["R. Fletcher"], "venue": "John Wiley and Sons,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1987}, {"title": "Mixing Linear SVMs for Nonlinear Classification", "author": ["Z.Y. Fu", "A. Robles-Kelly", "J. Zhou"], "venue": "IEEE Transactions on Neural Networks, 21(12):1963 \u2013 1975,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The Dendritic Cell Algorithm", "author": ["J. Greensmith"], "venue": "PhD thesis, School of Computer Science, University of Nottingham,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "The Deterministic Dendritic Cell Algorithm", "author": ["J. Greensmith", "U. Aickelin"], "venue": "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 291\u2013303,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Documentation of ICARIS 2011 paper (raw data, experimental results and statistical analysis)", "author": ["F. Gu", "J. Feyereisl", "R. Oates", "J. Reps", "J. Greensmith", "U. Aickelin"], "venue": "http://www.cs.nott.ac.uk/~fxg/icaris_paper2011.html,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploration of the Dendritic Cell Algorithm with the Duration Calculus", "author": ["F. Gu", "J. Greensmith", "U. Aickelin"], "venue": "In Proceedings of the 8th International Conference on Artificial Immune Systems (ICARIS), pages 54\u201366,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Artificial immune systems and kernel methods", "author": ["T.S. Guzella", "T.A. Mota-Santos", "W.M. Caminhas"], "venue": "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 303\u2013315,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Digital Signal Processing: A Practical Approach", "author": ["E. Ifeachor", "P.B. Jervis"], "venue": "Prentice Hall, 2nd edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel-based machine learning lab", "author": ["A. Karatzoglou", "A. Smola", "K. Hornik"], "venue": "Technical report, Department of Statistics and Probability Theory, Vienna University of Technology,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Insight into the Antigen Sampling Component of the Dendritic Cell Algorithm", "author": ["C. Musselle"], "venue": "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 88\u2013101,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "The Suitability of the Dendritic Cell Algorithm for Robotic Security Applications", "author": ["R. Oates"], "venue": "PhD thesis, School of Computer Science, University of Nottingham,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Frequency Analysis for Dendritic Cell Population Tuning: Decimating the Dendritic Cell", "author": ["R. Oates", "G. Kendall", "J. Garibaldi"], "venue": "Evolutionary Intelligence, 1(2):145\u2013 157,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "The Limitations of Frequency Analysis for Dendritic Cell Population Modelling", "author": ["R. Oates", "G. Kendall", "J. Garibaldi"], "venue": "In Proceedings of the 7th International Conference on Artificial Immune Systems (ICARIS), pages 328\u2013339,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Classifying in the presence of uncertainty: a DCA persepctive", "author": ["R. Oates", "G. Kendall", "J. Garibaldi"], "venue": "In Proceedings of the 9th International Conference on Artificial Immune Systems (ICARIS), pages 75\u201387,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning with Kernels : Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Scholkopf", "A.J. Smola"], "venue": "The MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Geometrical insights into the dendritic cell algorithm", "author": ["T. Stibor", "R. Oates", "G. Kendall", "J. Garibaldi"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 1275\u20131282,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm developed as part of the Danger Project [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Despite being applied to a number of applications, it was originally designed and used as an anomaly detection and attribution algorithm [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 75, "endOffset": 81}, {"referenceID": 16, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 99, "endOffset": 107}, {"referenceID": 8, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 99, "endOffset": 107}, {"referenceID": 16, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 99, "endOffset": 107}, {"referenceID": 11, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 133, "endOffset": 146}, {"referenceID": 15, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 133, "endOffset": 146}, {"referenceID": 17, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 133, "endOffset": 146}, {"referenceID": 21, "context": "Since its first version [9] the DCA has been subject to many modifications [4,17], empirical tests [2,9,17] and theoretical analyses [12,16,18,22].", "startOffset": 133, "endOffset": 146}, {"referenceID": 17, "context": "For example, it has been shown that the structure of a single dendritic cell within the DCA is similar in function to the operation of a filter with a dynamically changing transfer function [18,19].", "startOffset": 190, "endOffset": 197}, {"referenceID": 18, "context": "For example, it has been shown that the structure of a single dendritic cell within the DCA is similar in function to the operation of a filter with a dynamically changing transfer function [18,19].", "startOffset": 190, "endOffset": 197}, {"referenceID": 21, "context": "One such property is that its classification stage is functionally equivalent to a statically weighted, linear classifier [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].", "startOffset": 241, "endOffset": 251}, {"referenceID": 19, "context": "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].", "startOffset": 241, "endOffset": 251}, {"referenceID": 21, "context": "Within the literature, it has been suggested that replacing the classification stage of the DCA with a trainable, nonlinear, machine learning algorithm would negate much of the criticism made of the DCA while preserving its novel properties [13,20,22].", "startOffset": 241, "endOffset": 251}, {"referenceID": 9, "context": "The algorithmic details can be found in [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "It is demonstrated in [22] that both the classification boundary and the position of the decision boundary can be expressed as hyperplanes, akin to those found in linear classifiers.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "As the classification performed by a cell is performed using the history of the sampled signals rather than an instantaneous sample of the environmental features, it can be shown that the DCA exhibits a filtering property which allows it to remove high frequency noise from the input signals [18].", "startOffset": 292, "endOffset": 296}, {"referenceID": 2, "context": "In our investigation, the classification stage of the DCA is replaced by a trainable classifier, which is based on the operation of Support Vector Machines (SVM) [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "SVM models can be described using linear discriminant functions [6], quadratic optimisation [7], and kernel methods [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "The primal form LP is differentiable with respect to w and b, and an equivalent dual form, known as the Wolfe dual [7], can be derived.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "The optimisation becomes a convex quadratic programming problem, and all data points that satisfy the constraints also form a convex set [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 20, "context": "where \u03a6 is a mapping from the original input feature space X to a higher dimensional (inner product) feature space F , where nonlinearly separable problems become more separable [21].", "startOffset": 178, "endOffset": 182}, {"referenceID": 20, "context": "Depending on the applications, a number of kernel functions are available, including linear kernels, polynomial kernels, and Gaussian kernels [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "It is more computationally efficient than other SVM models that use more complicated kernel functions [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 17, "context": "The filter with the most analogous behaviour to the DCA is the sliding window filter [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "The transfer function of the sliding window filter is given in Equation 10 [14],", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "A dendritic cell acts like a sliding window filter which only reports its output every W steps [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Details of the raw datasets, experimental results and statistical analyses involved in this paper can be found in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "Synthetic datasets based on two Gaussian distributions are common practice in machine learning, as shown in [22].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "For the experiments in Musselle\u2019s work [16], where the temporal nature of the data is important, the author uses a Markov chain to generate synthetic datasets, where the probability of state change dictates the relative concentrations of the normal and anomalous behaviour.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Parameters used in the linear SVM are the default values of the R package kernlab [15], and kept the same for both moving window functions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "For the static moving window function, the cardinality of the set of initial window sizes |A| is 100, and a window size \u03b1l \u2208 [1, 100]\u2229N.", "startOffset": 125, "endOffset": 133}, {"referenceID": 0, "context": "Firstly the two input features are normalised into a range [0, 1] through min-max normalisation.", "startOffset": 59, "endOffset": 65}, {"referenceID": 9, "context": "The remaining parameters are chosen according to the values suggested in [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "All results will be tested using the Shapiro-Wilk normality test to verify if parametric or nonparametric statistical tests are suitable [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "The two-sided student t-test will be used for normally distributed samples, and the two-sided Wilcoxon signed rank test will be used for non-normally distributed ones [5].", "startOffset": 167, "endOffset": 170}], "year": 2013, "abstractText": "Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded several criticisms about its underlying structure and operation. As a result, several alterations and fixes have been suggested in the literature to correct for these findings. A contribution of this work is to investigate the effects of replacing the classification stage of the DCA (which is known to be flawed) with a traditional machine learning technique. This work goes on to question the merits of those unique properties of the DCA that are yet to be thoroughly analysed. If none of these properties can be found to have a benefit over traditional approaches, then \u201cfixing\u201d the DCA is arguably less efficient than simply creating a new algorithm. This work examines the dynamic filtering property of the DCA and questions the utility of this unique feature for the anomaly detection problem. It is found that this feature, while advantageous for noisy, time-ordered classification, is not as useful as a traditional static filter for processing a synthetic dataset. It is concluded that there are still unique features of the DCA left to investigate. Areas that may be of benefit to the Artificial Immune Systems community are suggested.", "creator": "LaTeX with hyperref package"}}}