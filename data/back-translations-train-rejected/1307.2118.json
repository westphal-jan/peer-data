{"id": "1307.2118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2013", "title": "A PAC-Bayesian Tutorial with A Dropout Bound", "abstract": "This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, $L_2$ regularization, {\\em provides a bound for dropout training}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.", "histories": [["v1", "Mon, 8 Jul 2013 15:03:03 GMT  (13kb,D)", "http://arxiv.org/abs/1307.2118v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david mcallester"], "accepted": false, "id": "1307.2118"}, "pdf": {"name": "1307.2118.pdf", "metadata": {"source": "CRF", "title": "A PAC-Bayesian Tutorial with A Dropout Bound", "authors": ["David McAllester"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2 An Occam Bound", "text": "Let us assume that a set of \"rules,\" S is a set of \"situations,\" Lmax > 0 is a real number (> f), and L is a loss function, so that for a rule h-H and a situation s-S we have a distribution that occurs in nature, and P as a learner bias on rules. There is no assumed relationship between D and P-1. We are interested in drawing a sequence S of N situations IID of D (S-S-S) and then selecting h-based on S to minimize the \"generalization loss\" L (h) = Es-D (h, s). If the sample S-S is clear, we will for 1N-S-S (h, s).1We should assume that the loss function L-S is measurable."}, {"heading": "2.1 Finite Precision Bounds", "text": "As an example of the application of (1), we can use rules of the form h\u044b for some pairs of parameters > bits for each bit (1), each component of the bit being represented with a B-bit finite precision representation. In this case, we can consider the previous P to be uniform for the 2bd possible rules, and (1) then indicates that there is a probability of at least 1 \u2212 \u03b4 over the drawing of the sample that the following numbers will be drawn simultaneously for all such types. L (h\u044b) \u2264 inf \u03bb > 1211 \u2212 1 2\u03bb (L-Bit) + \u03bbLmax N ((((ln 2) bd + ln 1\u03b4). We can also consider sparse representations \u2212 Rd say that we have a savings level s if most s components are not zero. We can then represent a sparse vector by first selecting the spareness s and then listing the spareness of the s-bit ability, each of which is not specified by a zero component."}, {"heading": "3 A PAC-Bayesian Bound", "text": "Let us let H, S, Lmax, L, D and P be defined as in section 2 > Q (Q) is uniform (uncountable); let us let Q be a variable extending beyond distributions (quantities) in control room H. For s \u00b2 S, we define the loss L (Q, s) as Eh \u00b2 Q [L (h, s). We have L (Q, s) is the loss of a stochastic process that hypothesizes h according to the distribution Q. We define L (Q) as E \u00b2 D [L (Q, s)]. Faced with a sample S = {s1,... \u2212 sN}, we define L (Q) as simultaneous S (Q, s). L (Q, s) finally, let us write D (Q) for the cullback-Leibler divergence from Q to P. D (Q, P) = Eh \u00b2 Q (h) P (h)."}, {"heading": "3.1 An Infinite Precision L2 Bound", "text": "As in section 2.1, we consider the rules h\u03c9 with \u0438\u0435 Rd. Here, we also assume that the rule is scale invariant - that the rule h\u043e depends only on the direction of the vector \u03c9. For example, linear predictors of formh\u03c9 (x) = argmax y\u03c9\u0445 (x, y) are scale invariant. For scale invariant rules, it is natural to consider the uniform distribution over the directions of \u03c9. This uniform distribution can be formalized as an isotropic unit variance before P = N (0, 1) d, where N (0, 1) is the mean unit variance of the Gaussian distribution. For area variance Rd, we define the isotropic unit variance Gauss with the emphasis on area variance. Since only the direction of quantities matters, we should consider P as the uniform distribution over directions and consider the uneven distribution over directions."}, {"heading": "3.2 Binary and Multi-Class classification", "text": "As an example, we can consider the linear binary classification. In this case, we have that each situation is a pair (x, y) with y (1, 1, y) and we have (x) = characters (x, y) where it is a characteristic card, so we also have 0-1 lossL (h, (x, y) = 1h (x) 6 = y. In this case, we have that (4) is very similar to the objective definition of a support vector machine, but where the hinge loss is replaced by a (non-convex) sigmoidal loss function (x), x haristic loss function (the cumulative one of Gaussian).In practice, the rule is used horizontally, where the support vector machine is very similar, but where the hinge loss is replaced by a (non-convex) sigmoidal loss function (x), x haristic loss function (the cumulative one of Gaussian)."}, {"heading": "3.3 Dropouts", "text": "We now present a dropout pattern inspired by the recent success of dropout training in deep neural networks (Q1). This dropout pattern is the only original contribution of this learning process. For a given dropout rate from 0 to 1 and a vector from 0 to 1, we can stochastically create a vector with Rd by selecting for each coordination the value 0 with probability 1 (dropping the coordination rate i) or with probability 1 \u2212 1 + with probability 1 = i + with probability N (0, 1). We have the distribution determined on vectors defined by this generational process. To apply the PAC-Bayesian limit, we take Q1 as the previous distribution and Q1 as the subsequent distribution. The PAC-Bayesian theorem then implies that for a dropout rate of 1 that we selected before drawing the sample, we have this probability of at least 1 \u2212 d."}, {"heading": "3.4 The PAC-Bayesian Posterior", "text": "It is important to note that (2) has a closed solution for the distribution Q which minimizes the limited.Q \u043a = argmin Q L (Q) + \u03bbLmax N D (Q, P) In the case where the control space H is finite, we have the restriction that p = HQ (h) = 1 and a simple application of the KTT conditions leads to the following. Q \u0445 (h) = Q\u03bb (h) = 1Z\u03bb P (h) e \u2212 NL (h) \u03bbLmaxZ\u03bb = Eh \u0445 P [e \u2212 N = Lmax L (h)] Here we can imagine Q\u03bb as \"the\" PAC \u2212 Bayesian posterior distribution for regulation parameters \u03bb. It is important to note that the choice of Lmax strongly influences the posterior distribution. In the case of (3) we can consider it as the following optimization. Q \u0445 = Q\u03bbi \u0432 = argmin1 \u2264 i \u03bbi (Qi) (Qi) (Qi) + \u03bbi (11)."}, {"heading": "4 A Training-Variance Bound", "text": "For a given learning algorithm A, we now consider the expected loss ES \u0445 DN [L (QA (S)]] and the expected posteriorQ \u0445 A (h) = ES \u0445 DN [QA (S)]. Theorem 3. For each fixed learning algorithm A (h) = ES \u0445 DN [QA (S) (h). The training variance is the following, where we will write ES [f (S)] for ES \u0445 DN (f (S)]. Theorem 3. For each fixed learning algorithm A (h) = ES (S) > 1 (2), we have ES [L (QA (S)))."}, {"heading": "5 Applying the Training-Variance Bound", "text": "We now look at the learning algorithm that maps a sample S (> Q) to the PAC-Bayesian limit (2). At this point, we should mention a parameter of the learning algorithm. We should note that although the analysis given here replaces the PAC-Bayesian limit (2), it seems unlikely that Q\u03bb is the algorithm that optimizes the training variance limit (6). Also, as we will see below, the analysis given here is somewhat more loose. Following Catoni [4] and Lever et al. [9], we approach the Q limit (2) with the following. Q-Bayesian (h) = 1 Z value P (h) e \u2212 NL value (h) LmaxZ value (4) and Lever et al. [9] we approach the Q value (2) with the following."}, {"heading": "6 Incorporating Empirical Loss Variance", "text": "For a given rule h and sample S = {s1,.., sn} one can measure an empirical loss variance (h) = 1N \u2212 1 N \u2211 i = 1 (L (h, si) \u2212 L (h) 2It is natural to ask whether narrower limits are possible if we allow the limits to contain more than 2 (h). Audibert, Munos and Szepesvari [2] indicate a limit motivated by this question. Consider a random variable x [0, Lmax] with expectation \u00b5 and an IID sample {x1,.., xn} with empirical average variance."}, {"heading": "7 Conclusion", "text": "This paper focuses on three generalization boundaries - an Occam boundary, a PAC-Bayesian boundary, and a training variance boundary. Occam and PAC-Bayesian boundaries seem to be important primarily because they provide the conceptual basis for establishing the training variance boundary that dominates the other two. While the rear PAC-Bayesian boundary defines a learning algorithm that optimizes the PAC-Bayesian boundary, there is no known analog optimal algorithm for the training variance boundary, which seems to suggest methods for reducing variance, such as boosting. Clearly, there is room for an improved theoretical understanding of the consequences of the training variance boundary."}, {"heading": "A Proof of Theorem 2", "text": "All the proofs in these annexes are adjusted by Catoni [4], with the exception of the proof of Q = Q (Q), which is straightforward. Theorem states that for all numbers chosen before the drawing of the sample Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all set sizes) + Q (for all types of sizes) + ln (for all types of sizes). We will consider the case of Lmax = 1, the general case of Lmax = 1, recalculating the loss function. For real numbers p, q), q), p), p), to determine the deviation from a Bernoulli variable with a procedure q (for Bernoulli variable with a procedure p.D (q), q =, p)."}, {"heading": "B Proof of Theorem 4", "text": "The theorem states that for the distribution P according to the rules any algorithm A and for \u03bb > 12 ES [L (QA (S)) \u2264 11 \u2212 1 2\u03bb (ES [L (QA (S)))] + \u03bbLmaxN ES [D (QA (S), P)].The proof is a slight modification of theorem 2 according to Section A. By shifting the scale lemma (20) we have for each specified sample the following: S.Eh (QA (S) [ND\u03b3 (L (h), L (h))] \u2264 D (QA (S), P) + ln Eh (P [eND\u03b3 (L (h), L (h))].By the common convexity of D\u03b3 (S) we then have the common convexity of D\u03b3 (L (L) (L) (L), L (S) and L (S), L (L)."}, {"heading": "C Proof of (10) and (13)", "text": "(10) is the following state. ES [D (Q\u03bb (S), Q \u03bb)] \u2264 N \u03bbLmax (ES) [L (Q\u03bb (S)))] \u2212 ES [L (Q\u03bb (S))]] Proof.ES [D (Q\u03bb (S), Q \u03bb)] = ES, h \u0445 Q\u03bb (S) [ln Q\u03bb (S) + ln Z (S) (h) Q (h) \u2212 ES (L) [N\u03bbLmax L (h) \u2212 N (h) \u2212 ES (L) \u2212 L (S) \u2212 L (S) \u2212 S (S) \u2212 S) \u2212 ES (L) \u2212 L (Q\u03bb (S)) \u2212 L (L) \u2212 L (L) before (S) \u2212 L (L) before (L) + ln (Z) (L) before (L)."}, {"heading": "D Proof of Theorem 6", "text": "The theorem states that with a probability of at least 1 \u2212 \u043c above the drawing of the sample, we have that the following applies to all h, so the probability is 2 (h) = 0, L (h) \u2264 L (h) + Lmax (ln 1P (h) + ln 1 \u03b4) N \u2212 1Proof. Consider a sample S = {s1,.., sn}. We can then use the \"feasible\" standard analysis via the sample {s2,.., sN} to limit the breakout rate. Specifically, let \u00b5 (h) be the probability that a new drawing of a situation of D is an outlier of h.\u00b5 (h)."}], "references": [{"title": "Fast probabilistic algorithms for hamiltonian circuits and matchings", "author": ["Dana Angluin", "Leslie G Valiant"], "venue": "In Proceedings of the ninth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1977}, {"title": "Use of variance estimation in the multi-armed bandit problem", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesvari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pac-bayesian supervised classification: the thermodynamics of statistical learning", "author": ["Olivier Catoni"], "venue": "arXiv preprint arXiv:0712.0248,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Li Deng", "Geoffrey Hinton", "Brian Kingsbury"], "venue": "Proc. ICASSP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Pac-bayesian learning of linear classifiers", "author": ["Pascal Germain", "Alexandre Lacasse", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Tutorial on practical prediction theory for classification", "author": ["John Langford"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Microchoice bounds and self bounding learning algorithms", "author": ["John Langford", "Avrim Blum"], "venue": "In Proceedings of the twelfth annual conference on Computational learning theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Distribution-dependent pac-bayes priors", "author": ["Guy Lever", "Fran\u00e7ois Laviolette", "John Shawe-Taylor"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A note on the pac-bayesian theorem", "author": ["Andreas Maurer"], "venue": "arXiv preprint cs/0411099,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Pac-bayesian model averaging", "author": ["David A. McAllester"], "venue": "In COLT, pages 164\u2013170,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Pac-bayesian generalisation error bounds for gaussian process classification", "author": ["Matthias Seeger"], "venue": "J. Mach. Learn. Res,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 10, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 5, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 2, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 3, "context": "It also provides bounds for a form of dropout learning [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "It is of course possible to rescale any bounded loss function into the interval [0, 1].", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "For a given h \u2208 H the relative Chernoff bound [1] states that", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": "The first PAC-Bayesian theorem was given in [11].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 8, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 5, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 2, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 4, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 2, "context": "Here we will focus on the following PAC-Bayesian version of the Occam bound (1) which can be derived as a corollary of statements by Catoni [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "We now present a dropout bound inspired by the recent success of dropout training in deep neural networks [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "For a given dropout rate \u03b1 \u2208 [0, 1] and vector \u0398 \u2208 R we can stochastically generate a vector w \u2208 R by selecting, for each coordinate wi, the value 0 with probability \u03b1 (dropping the coordinate \u03c9i) or with probability 1 \u2212 \u03b1 setting \u03c9i = \u0398i + with \u223c N (0, 1).", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "The training-variance bound is an immediate corollary of the following more general theorem which is implicit in Catoni [4] and which is proved in appendix B.", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "It was observed by Langford [8] that the rule distribution P minimizing ES [D(QA(S), P )] is Q\u0304A.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Following Catoni [4] and Lever et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "[9] we approximate Q\u0304\u03bb with the following.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "We now have the following bound from Catoni [4] and whose proof is given in appendix C.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Audibert, Munos and Szepesvari [2] give a bound motivated by this question.", "startOffset": 31, "endOffset": 34}], "year": 2013, "abstractText": "This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, L2 regularization, provides a bound for dropout training, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound \u2014 a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.", "creator": "LaTeX with hyperref package"}}}