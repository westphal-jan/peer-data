{"id": "1307.2191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2013", "title": "A Knowledge-based Treatment of Human-Automation Systems", "abstract": "In a supervisory control system the human agent knowledge of past, current, and future system behavior is critical for system performance. Being able to reason about that knowledge in a precise and structured manner is central to effective system design. In this paper we introduce the application of a well-established formal approach to reasoning about knowledge to the modeling and analysis of complex human-automation systems. An intuitive notion of knowledge in human-automation systems is sketched and then cast as a formal model. We present a case study in which the approach is used to model and reason about a classic problem from the human-automation systems literature; the results of our analysis provide evidence for the validity and value of reasoning about complex systems in terms of the knowledge of the system agents. To conclude, we discuss research directions that will extend this approach, and note several systems in the aviation and human-robot team domains that are of particular interest.", "histories": [["v1", "Mon, 8 Jul 2013 18:07:31 GMT  (581kb)", "http://arxiv.org/abs/1307.2191v1", "39 pages, 1 figure"]], "COMMENTS": "39 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["yoram moses", "marcia k shamo technion - israel institute of technology)"], "accepted": false, "id": "1307.2191"}, "pdf": {"name": "1307.2191.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Marcia K. Shamo"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is such that most people who are in a position to decide for themselves what they want to do and what they want to do, are left to themselves. In fact, it is such that they are not willing to take power, to decide whether they want to take power or not. In fact, it is such that they are not ready to take power. In fact, it is such that they are not willing to take power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, to seize the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power, the power."}, {"heading": "II. HUMAN KNOWLEDGE IN COMPLEX SYSTEMS", "text": "This year it has come to the point that it will only be a matter of time before we will be able to do it again, until we will be able to find a solution that will enable us to do it."}, {"heading": "B. When Does An Operator Know Enough?", "text": "If our goal is to provide a practically useful methodology and set of tools for designing and analyzing complex systems from the perspective of agents \"knowledge, we must provide not only a formal means of describing the knowledge in these systems, but also formal indicators by which knowledge in systems can be rigorously evaluated. Although human agents\" knowledge of the possible behavior of a complex system is by definition incomplete due to the size of the state space, we propose that the knowledge of the human agent must be at least solid and adequate. Next, we describe these characteristics and note that, if they exist, we will say that the knowledge of the human agent is satisfactory."}, {"heading": "III. REASONING ABOUT KNOWLEDGE \u2013 AN INTRODUCTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Elements of the Framework", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "IV. DEFINING BOUNDED KNOWLEDGE IN A COMPLEX HUMAN-AUTOMATION SYSTEM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. An Appropriate 3-Tiered Syntactic Model of Human Knowledge", "text": "The question of whether this is a way in which the knowledge of people is put in a certain way into the world is not only a question of perception, but also a question of perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception, perception,"}, {"heading": "B. Knowledge of Bad System Behavior", "text": "A supervisor must know at all times whether the behavior of the system is within certain and acceptable limits. In addition to system-specific knowledge formulas that will be part of the pistemic setup of the human agent, the agent is expected to know that there is a problem in any local state where the current global state does not conform to system specifications. We define pbad as a phrase that is \"not acceptable to the current behavior of the system,\" where the term \"acceptable\" depends on application. Furthermore, we require that \"hK pbad\" be included in any state of the system that is not guaranteed to be within acceptable limits. In the above example, if we assume that there is nothing that excludes the circumvention maneuver, then the current behavior of the system is acceptable and the formula \"hK pbad\" is not part of the pilot's knowledge in that local state."}, {"heading": "C. The Epistemic System Model", "text": "The mathematical model in which we can define both the truth of the formulas and the knowledge of the human agent is an epistemic system E = (R, h) in which R is the set of possible runs. At a given time m in a run r we define: (E, r, m) | = p (for p) iff (r (m))) iff (r (m))))) = true (E, r, m) | = iff both (E, r, m) | = (E, r, m) | = (E, r, m) | = (E, m) iff (E, r, m)."}, {"heading": "D. Criteria for Satisfactory Bounded Knowledge", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "E. Human Knowledge and Possible Worlds Knowledge", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "V. THE THERAC-25 DEVICE PROBLEM  OVERVIEW", "text": "The case study is presented at two levels of detail: the first, immediately following, is a high-level review discussion aimed at improving an intuitive understanding of our approach and its value for thinking about complex human automation systems; and the second, included in the appendix, is a full formal presentation of the problem and solution."}, {"heading": "A. Problem Description", "text": "\"Between June 1985 and January 1987, a computer-controlled radiotherapy machine called Therac-25 massively overdosed six people, which were described as the worst accidents in the 35-year history of medical accelerators.\" [10] In investigating the Therac-25 failure, a common factor in two of the accidents was the operator of the machine. In both cases, the operator had started and completed the task of patient input and then returned so quickly to processing one or more values before treatment began. Analysis showed that the operator's input speed was the cause of the machine's behavior - while processing the data was a permissible function, the operator was able to edit the data and return to a state of \"complete data input\" that the machine did not record and therefore never knew that the data processing had taken place. The operator then initiated the treatment because he believed that the dosage would correspond to the newly edited parameter values, while the actual dosage was virtually free to the patient according to the internal data of the treatment system."}, {"heading": "B. Modeling the System", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "C. Satisfactory Knowledge and Possible-Worlds Knowledge in the Therac-25 Problem", "text": "After modeling the behavior of the system and the limited knowledge of the operator, we use our definitions of soundness and appropriateness to analyze the system and to show that the limited knowledge of the operator of Therac-25 as modeled is not satisfactory for supervisory control. Furthermore, the investigation of the model within the scope of possible worlds shows that the design of the Therac-25 is flawed, making the satisfactory knowledge of the operator impossible. At the outset, we recall that the operator of the Therac device fully knows that the treatment data can be modified before the initiation of the patient's treatment. We assume that the operator has modified previously entered data, the device processes the data and displays its \"status,\" and the operator initiates the device as expected and treats the patient in accordance with the modified data; the operator knows that the system is acceptable to the behavior of the operator."}, {"heading": "D. Problem Resolution", "text": "Our approach not only supports a fine-grained analysis of a complex human automation system, but also provides a clear direction for problem-solving. In the case of the Therac-25 problem, the knowledge-based model and analysis suggest the need for a design change that provides the operator with the information necessary to know truthfully that the device has processed the data processed by the operator and is thus actually ready for treatment.4 This modification would allow the operator to easily determine the actual ready-to-treat status of the device and should therefore exclude the possibility of initiating treatment with incorrect treatment data."}, {"heading": "E. Supporting Design Modification Via Bounded Knowledge", "text": "It remains to be proved, using our formal framework, that this change actually results in a system in which the available knowledge for supervisory control is satisfactory. Suppose once again that there is a state g in which the treatment data has been processed and processed by the device, and a state g 'in which the treatment data has been processed but not processed by the device. Suppose that the epistemic system modifies the knowledge of the operator and the device in the Therac-25 system, as described in Section 5.4. In the local state l, the epistemic setup of the operator contains h (l) = dh (ah (fh (l))), now a knowledge formula called \"hK\" which represents the limited knowledge of the operator about the readiness to treat the global system. In order for this system to be solid, i.e. for what the operator undoubtedly knows to be true, the operator must know that the system is ready to handle it within the possible world framework."}, {"heading": "F. Summary", "text": "Although for most complex systems of interest neither the model nor the solution will be nearly as easy to define, this Therac 25 example shows how to model a \"real\" system and its interface between humans and automation4 We do not propose that adding indicators for every fact that humans need to know is a desirable or even practicable solution option. Display optimization for the knowledge identified as necessary is (far) outside the scope of this research. Design is evaluated using the knowledge of its operator. For the interested reader, the full detailed model and analysis is included in the appendix."}, {"heading": "VI. DISCUSSION AND DIRECTIONS FOR FUTURE RESEARCH", "text": "In this article, we have made a first attempt to use a well-established formal theory of knowledge and action in multi-agent systems to perform a knowledge-based analysis of a complex human automation system, which allows us to think cleanly and rigorously about the design and performance of a system depending on one of its most basic resources - agent knowledge. Therac-25 analysis revealed the existence of design flaws in the system that prevented the human agent from being an effective controller, and identified the lack of knowledge. No additional theories of performance or behavior were required, and the human and automation agents were presented on the basis of an expressive common construct without losing important and unique characteristics of any of these different agents. Example, has shown that our approach provides a formal, expressive and economical methodology for designing and analyzing complex systems, and we believe that this is a significant contribution."}, {"heading": "A. A Richer Notion of Human Knowledge", "text": "A first direction for further research is to expand and fully define an idea of human knowledge within our formal framework. For example, we would like to consider a classification of human knowledge in terms of its content in addition to the typical taxonomy defined in the current paper. Another aspect of human knowledge that is often used in cognitive modeling is the distinction between knowledge and belief. In this way, we can gain a multidimensional model of human knowledge that makes our approach useful in a broader range of problem areas. Another aspect of human knowledge that is important in regulatory control is the distinction between knowledge and belief."}, {"heading": "B. Extending the Formal Model", "text": "Our formalization of human knowledge has relied on various approaches in epistemic logic to create a framework that is expressive enough to capture the unique characteristics of human action knowledge without sacrificing the rigor of formal logic. Our approach can also formally represent different kinds of reasoning that a human agent could do, perhaps under different circumstances or in other systems. Previously, we found that our current definition of a \"one-time\" round of thinking can capture an intuitive idea of how a human agent might argue in a control situation. That is, since complex systems are usually dynamic systems in which control decisions must be made and implemented in a timely manner, the operator's argumentation activities cannot require multiple rounds of thinking."}, {"heading": "C. Knowledge of Groups of Agents in Human-Automation Systems", "text": "One of the most significant contributions of knowledge formalism is its ability to express notions of the knowledge of agents, such as the knowledge of other agents, distributed knowledge (knowledge is distributed among agents in the system), and general knowledge (that is, all agents know a fact that is common knowledge, and know that all agents p know, and so on. This expressivity supports the analysis of central system characteristics such as the need for an agent to know what another agent knows, the additional knowledge provided by a fact, the system failure when the system fails."}, {"heading": "D. Applications", "text": "The final test of any formal approach to modeling and analysis is its applicability to real problems in the intended areas. The Therac-25 device problem discussed in this article served as a benchmark for demonstrating that our approach (1) can explicitly capture important characteristics of human and non-human agents, (2) can allow us to answer questions about what human and non-human agents know and do not know, and (3) can be used to draw significant conclusions about the design of a complex system despite the dissimilarity of its agents. While these results provide important \"proof of concept,\" the problem is a neatly defined and thoroughly researched design-flawed scenario. What is the value of our formality?"}, {"heading": "VII. CONCLUSION", "text": "As mentioned in the introduction to this paper, the critical nature of many human automation systems requires a clear need for rigorous design and analysis tools that are practically useful. This is widely acknowledged, and the development of methods and tools has been an active area of research for many years. Unfortunately, the highly complex nature of many of these tools all too often leads to their isolation in academic and scientific arenas. Subsequently, practical system design remains largely an ad hoc process. To this end, we have introduced and described the initial development of a novel approach to modeling and arguing about these systems, which should satisfy both the need for formal rigor and be sufficiently intuitive for practical application. One of the most significant aspects of this framework is that agent knowledge is attributed and analyzed with respect to the automaton representing the complete given human automation system. Our initial results suggest that thinking about these systems from the perspective of agent knowledge is actually a useful and valuable approach."}, {"heading": "VIII. THE THERAC-25 DEVICE PROBLEM \u2013 DETAILED ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Description", "text": "\"Between June 1985 and January 1987, a computer-controlled radiotherapy machine, called Therac-25, massively overdosed six people, and these accidents were described as the worst in the [then] 35-year history of medical accelerators.\" [10] In investigating the Therac-25 failure, a common factor in two of the accidents was the operator of the machine. In both cases, the operator had started and completed the task of patient input and then returned so quickly to processing one or more values before treatment began, the analysis found that the operator's input speed was the cause of the machine's behavior - while processing the data was a permissible function, the operator was able to process the data and return to a state of \"full data input\" that the machine never knew that the data processing had taken place. Thereupon, the operator initiated treatment because he believed that the dosing would match the newly edited parameter values on the user interface, while the actual dosing values were close to the operating system's."}, {"heading": "B. Modeling the System", "text": "In fact, the data available in the individual countries are not local data, but local data that are able to record the status of data entries and changes, and a variable that represents the status of data. (...) It is a question of whether the data is that of people who are able to identify themselves. (...) It is a question of whether it is data that are able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are not able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are able to identify themselves. (...) It is a question of data that are able to identify themselves."}, {"heading": "D. Problem Resolution", "text": "Our approach not only identifies problems with agent knowledge, but also provides insights into problem solving and the means to determine whether resolution is appropriate. For the Therac-25 system, the modeling and analysis process suggests possible and demonstrably correct solutions. In the situation described above, it was shown that the operator had no way to determine whether the device had processed the data and that he was aware of the poor state as soon as treatment was initiated before data processing. A simple design solution would be to modify the interface so that information about the data processing status of the device would always be available. For example, changing the input data by the operator could block further system action and turn the system-ready indicator red. Only when new data has been fully entered and then processed by the device would the device send a data-ready signal to the control panel in the local view of the operator.Finished signal to the control panel would be able to know when treatment could be initiated."}, {"heading": "E. Conclusion", "text": "This Therac-25 problem model and its proposed solution have been simplified to serve as a simple example of how our approach can be applied. Note that from several iterations of the analysis and solution definition process, a system design is derived that is demonstrably satisfactory to regulatory control. Although neither the model nor the solution will be nearly as easy to define for most complex systems of interest 5, the Therac-25 example has shown how a \"real\" system can be modeled and its human-automation interface design can be evaluated using the knowledge of its operator. 5 Nor do we suggest that adding indicators for every fact that humans need to know is a desirable or even practicable solution option.The optimization of displays for the knowledge identified as necessary is (far) outside the scope of this research."}, {"heading": "IX. REFERENCES", "text": "[1] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi, \"Knowledge-based programs,\" presented at PODC 95, Ottawa, 1995. [2] J. Y. Halpern and R. Fagin, \"Modeling Knowledge and common knowledge in a distributed environment,\" Distributed Computing, Vol. 3, pp. 159-179, 1989. [3] J. Y. Halpern and Y. Moses, \"Knowledge and common knowledge in a distributed environment,\" Journal of the ACM, vol. 37, pp. 549-587, 1990. [4] C. Dwork and Y. Hobes, \"Knowledge and common knowledge in a byzantine environment: crash failures,\" Information and Computation, vol. 88, pp. 156-186, 1990. [5] R. I. Brafman, J.-C. Latombe, Y."}], "references": [{"title": "Modelling knowledge and action in distributed systems", "author": ["J.Y. Halpern", "R. Fagin"], "venue": "Distributed Computing, vol. 3, pp. 159- 179, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Knowledge and common knowledge in a distributed environment", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Journal of the ACM, vol. 37, pp. 549-587, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge and common knowledge in a byzantine environment: crash failures", "author": ["C. Dwork", "Y. Moses"], "venue": "Information and Computation, vol. 88, pp. 156-186, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge as a tool in motion planning under uncertainty", "author": ["R.I. Brafman", "J.-C. Latombe", "Y. Moses", "Y. Shoham"], "venue": "presented at Theoretical Aspects of Reasoning about Knowledge: Proc. Fifth Conference, San Francisco, California, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Know means no: Incorporating knowledge into discrete-event control systems", "author": ["S.L. Ricker", "K. Rudie"], "venue": "IEEE Transactions on Automatic Control, vol. 45, pp. 1656-1668, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimal communication in a distributed discrete-event system", "author": ["K. Rudie", "S. Lafortune", "F. Lin"], "venue": "IEEE Transactions on Automatic Control, vol. 48, pp. 957-975, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Guaranteeing temporal validity with a real-time logic of knowledge", "author": ["S. Anderson", "J.K. Filipe"], "venue": "presented at Proceedings of the 23rd IEEE International Conference on Distributed Computing Systems Workshops, Providence, Rhode Island, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Timed knowledge-based modelling and analysis: on the dependability of sociotechnical systems", "author": ["J.K. Filipe", "M. Felici", "S. Anderson"], "venue": "presented at Proceedings of the 8th International Conference on Human Aspects of Advanced Manufacturing: Agility & Hybrid Automation, Rome, Italy, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Knowledge Representation: Logical, Philosophical, and Computational Foundations", "author": ["J.F. Sowa"], "venue": "Pacific Grove: Brooks/Cole Thomson Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Representations of Commonsense Knowledge", "author": ["E. Davis"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Knowledge and Belief: An Introduction to the Logic of the Two Notions", "author": ["J. Hintikka"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1962}, {"title": "A formal theory of knowledge and action", "author": ["R.C. Moore"], "venue": "Formal Theories of the Commonsense World, J. R. Hobbs and R. C. Moore, Eds. Norwood New Jersey: Ablex, 1985.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1985}, {"title": "Humans and Automation: System Design and Research Issues", "author": ["T.B. Sheridan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Human supervisory control", "author": ["T.B. Sheridan"], "venue": "Handbook of Systems Engineering and Management, A. P. Sage and W. B. Rouse, Eds. New York: John Wiley & Sons, Inc., 1999.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Engineering Psychology and Human Performance, Third Edition ed", "author": ["C.D. Wickens", "J.G. Hollands"], "venue": "Upper Saddle River,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Mental models in human-computer interaction", "author": ["J.M. Carroll", "J.R. Olson"], "venue": "Handbook of Human-Computer Interaction, M. Helander, Ed. Amsterdam: Elsevier, 1988.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Some observations on mental models", "author": ["D.A. Norman"], "venue": "Mental Models, D. Gentner and A. Stevens, Eds. Hillsdale, New Jersey: Lawrence Erlbaum Associates, 1983.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1983}, {"title": "Workload: An examination of the concept", "author": ["D. Gopher", "E. Donchin"], "venue": "Handbook of Perception and Performance, vol. 2, K. Boff, L. Kauffman, and J. Thomas, Eds. New York: Wiley, 1986, pp. 41-1 to 41-49.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}, {"title": "When mental models go wrong: co-occurrences in dynamic, critical systems", "author": ["D. Besnard", "D. Greathead", "G. Baxter"], "venue": "International Journal of Human-Computer Studies, vol. 60, pp. 117-128, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "The Psychology of Proof: Deductive Reasoning in Human Thinking", "author": ["L.J. Rips"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Processing resources in attention", "author": ["C.D. Wickens"], "venue": "Varieties of Attention, R. Parasuraman and R. Davies, Eds. New York: Academic, 1984, pp. 63-98.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "Using interference models to predict performance in a multiple-task UAV environment - 2 UAVs", "author": ["C.D. Wickens", "S. Dixon", "D. Chang"], "venue": "Aviation Human Factors Division Institute of Aviation, Savoy, Illinois, Technical Report AHFD-03- 9/MAAD-03-1, April 2003 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Using counterfactuals in knowledge-based programming", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Distributed Computing, vol. 17, pp. 91- 106, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Resource-bounded knowledge (extended abstract)", "author": ["Y. Moses"], "venue": "presented at Proc. Second Conference on Theoretical Aspects of Reasoning About Knowledge, San Francisco, Calif., 1988.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1988}, {"title": "Knowledge and communication (a tutorial)", "author": ["Y. Moses"], "venue": "presented at Theoretical Aspects of Reasoning About Knowledge: Proc. Fourth Conference, San Francisco: Calif., 1992.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1992}, {"title": "Belief as defeasible knowledge", "author": ["Y. Moses", "Y. Shoham"], "venue": "Artificial Intelligence, vol. 64, pp. 299-322, 1993.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1993}, {"title": "Introduction to Mathematical Logic, Fourth Edition ed", "author": ["E. Mendelson"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "Formal Theories of the Commonsense World", "author": ["J.R. Hobbs", "R.C. Moore"], "venue": "Ablex Series in Artificial Intelligence, Ablex, Ed. Norwood, New Jersey: Ablex Publishing Company, 1985.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1985}, {"title": "Algorithmic knowledge", "author": ["J.Y. Halpern", "Y. Moses", "M.Y. Vardi"], "venue": "presented at Theoretical Aspects of Reasoning About Knowledge: Proceedings of the Fifth Conference, San Francisco, Calif, 1994.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "The Atomic Components of Thought", "author": ["J.R. Anderson", "C. Lebiere"], "venue": "Mahwah, NJ: Lawrence Erlbaum Associates,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Using computational cognitive modeling to diagnose possible sources of aviation error", "author": ["M.D. Byrne", "A. Kirlik"], "venue": "Aviation Human Factors Division, Institute of Aviation, University of Illinois Technical Report AHFD-03-14/NASA-03-4, 2003.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "The role of shared mental models in developing team situation awareness: implications for training", "author": ["R. Stout", "J.A. Cannon-Bowers", "E. Salas"], "venue": "Training Research Journal, vol. 2, pp. 85-116, 1996.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Belief and incompleteness", "author": ["K. Konolige"], "venue": "Formal Theories of the Commonsense World, J. R. Hobbs and R. C. Moore, Eds. Norwood NJ: Ablex, 1985.  39", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1985}, {"title": "Mental models and counterfactual thoughts about what might have been", "author": ["R.M.J. Bryne"], "venue": "Trends in Cognitive Science, vol. 6, pp. 426-431, 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Counterfactual and prefactual conditionals", "author": ["R.M.J. Byrne", "S.M. Egan"], "venue": "Canadian Journal of Experimental Psychology, vol. 58, pp. 113-120, 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Reasoning counterfactually: making inferences about things that didn't happen", "author": ["V.A. Thompson", "R.M.J. Byrne"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, vol. 28, pp. 1154-1170, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Types of representation", "author": ["L. Bainbridge"], "venue": "Tasks, Errors and Mental Models, L. P. Goodstein, H. B. Anderson, and S. E. Olsen, Eds. London: Taylor and Francis Ltd., 1988.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1988}, {"title": "Cognitive systems engineering", "author": ["D.D. Woods", "E.M. Roth"], "venue": "Handbook of Human-Computer Interaction, M. Helander, Ed. New York: North-Holland, 1988.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1988}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["McCarthy", "P.J. Hayes"], "venue": "Machine Intelligence, vol. 6, 1969.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1969}, {"title": "How to make automated systems team players", "author": ["K. Christoffersen", "D.D. Woods"], "venue": "Advances in Human Performance and Cognitive Engineering Research, vol. 2: Elsevier Science Ltd., 2002, pp. 1-12.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Fostering common ground in human-robot interaction", "author": ["S. Kiesler"], "venue": "presented at Proceedings of the IEEE International Workshop on Robots and Human Interactive Communication (RO-MAN), 2005.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Common ground and coordination in joint activity", "author": ["G. Klein", "P.J. Feltovich", "J.M. Bradshaw", "D.D. Woods"], "venue": "Organizational Simulation, W. R. Rouse and K. B. Boff, Eds. New York City, NY: John Wiley, 2004, pp. (pp. in press).", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Final report for the DARPA/NSF interdisciplinary study on human-robot interaction", "author": ["J.L. Burke", "R.R. Murphy", "E. Rogers", "V.J. Lumelsky", "J. Scholtz"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, vol. 34, pp. 103-112, 2004.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Human-robot interaction in rescue robotics", "author": ["R.R. Murphy"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 34, pp. 138-153, 2004.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "In this paper we introduce a formal approach to reasoning about knowledge in human-automation systems that is based on a model of knowledge developed by Halpern and Moses and their colleagues [1]-[3].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 176, "endOffset": 179}, {"referenceID": 5, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "The value of reasoning about knowledge in the analysis of socio-technical systems has already been noted in the literature [8], [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "The value of reasoning about knowledge in the analysis of socio-technical systems has already been noted in the literature [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Philosophers from ancient times have considered fundamental questions such as the nature of knowledge and the origin of its existence, and began the development of logics and formal modeling in order to more precisely reason about these complex and often subtle concepts [11].", "startOffset": 271, "endOffset": 275}, {"referenceID": 9, "context": "Valuable outgrowths of work in these and other important fields include both strong support for the critical value of knowledge as a formal construct, and richly descriptive tools for precisely describing and reasoning about knowledge [12]-[15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 11, "context": "Valuable outgrowths of work in these and other important fields include both strong support for the critical value of knowledge as a formal construct, and richly descriptive tools for precisely describing and reasoning about knowledge [12]-[15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "What is the knowledge that is required for supervisory control? Sheridan notes that to function as a supervisory controller, the human agent must be able to command the system in accordance with defined specifications, monitor the system\u2019s behavior to ensure that it performs as required, and identify and correct anomalous system behaviors [16], [17].", "startOffset": 341, "endOffset": 345}, {"referenceID": 13, "context": "What is the knowledge that is required for supervisory control? Sheridan notes that to function as a supervisory controller, the human agent must be able to command the system in accordance with defined specifications, monitor the system\u2019s behavior to ensure that it performs as required, and identify and correct anomalous system behaviors [16], [17].", "startOffset": 347, "endOffset": 351}, {"referenceID": 14, "context": "While the relationship between displayed information and useful knowledge may depend on non-trivial factors such as the agent correctly perceiving the display and understanding its meaning, the compatibility of the display format to the task, and so forth [18], we assume here that the display type, quality, and information saliency are adequate for the task at hand.", "startOffset": 256, "endOffset": 260}, {"referenceID": 15, "context": "A second type of knowledge needed for human-supervisory control is the 'knowledge-in-the-head' that the human agent possesses regarding the global behaviors and properties of the physical system with which he is interacting [19]-[21].", "startOffset": 224, "endOffset": 228}, {"referenceID": 16, "context": "A second type of knowledge needed for human-supervisory control is the 'knowledge-in-the-head' that the human agent possesses regarding the global behaviors and properties of the physical system with which he is interacting [19]-[21].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "Mental model knowledge is required both for reasoning about and interpreting current interface information as well as for future-oriented supervisory control tasks such as planning and scheduling, troubleshooting, and decision-making [16]-[18].", "startOffset": 234, "endOffset": 238}, {"referenceID": 14, "context": "Mental model knowledge is required both for reasoning about and interpreting current interface information as well as for future-oriented supervisory control tasks such as planning and scheduling, troubleshooting, and decision-making [16]-[18].", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "We suggest that this knowledge is \u2018automatic\u2019 in nature and thus requires almost no effortful thinking or reasoning action [18], [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "We suggest that this knowledge is \u2018automatic\u2019 in nature and thus requires almost no effortful thinking or reasoning action [18], [22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "perhaps incorrect) beliefs [23].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Precisely how humans reason, the rules of inference that guide their reasoning activities, or indeed whether humans use any form of mental logic at all are on-going and strongly debated questions in the literature [24].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 524, "endOffset": 528}, {"referenceID": 17, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 530, "endOffset": 534}, {"referenceID": 20, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 536, "endOffset": 540}, {"referenceID": 21, "context": "the time- or safety-critical environments within which most complex systems function [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 51, "endOffset": 54}, {"referenceID": 22, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "The primary references for this part are [1]-[3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 26, "context": "two [31].", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "5) Knowledge as truth in all possible worlds The formal definition of knowledge is traditionally framed within the notion of possible worlds [13], [14].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "2 This point is most succinctly emphasized by Konolige [37] who notes that if humans were capable of unlimited reasoning, a chess player would know the outcome of a chess match immediately following the first move.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Rather than using possible-worlds semantics as the primary component, we will present a syntactic model in which the known facts will be obtained based on a restricted amount of reasoning, and in which what is boundedly known may not be true (for related approaches to syntactic knowledge see [13], [32]).", "startOffset": 299, "endOffset": 303}, {"referenceID": 28, "context": "(See [33] for definitions of knowledge that take computational complexity into account.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "For instance, we may wish to consider a classification of human knowledge in terms of its contents in addition to the type-based taxonomy defined in the current paper, such as the declarative / procedural dimension often used in cognitive modeling [34]-[36].", "startOffset": 248, "endOffset": 252}, {"referenceID": 31, "context": "For instance, we may wish to consider a classification of human knowledge in terms of its contents in addition to the type-based taxonomy defined in the current paper, such as the declarative / procedural dimension often used in cognitive modeling [34]-[36].", "startOffset": 253, "endOffset": 257}, {"referenceID": 25, "context": "Belief has been formally represented using a number of techniques [30], [37], and a valuable goal is to identify and build on the technique most appropriate for modeling a notion of human belief in domains of interest.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "Belief has been formally represented using a number of techniques [30], [37], and a valuable goal is to identify and build on the technique most appropriate for modeling a notion of human belief in domains of interest.", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "An additional element of human knowledge and reasoning that is particularly important in the supervisory control context is the concept of counterfactual reasoning (\u2018if p were to hold then q would be true\u2019) [38].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "When an operator plans future actions, especially error recovery actions, counterfactual reasoning supports the operator\u2019s consideration of conditional alternatives and the outcomes of hypothetical scenarios [39], [40].", "startOffset": 208, "endOffset": 212}, {"referenceID": 35, "context": "When an operator plans future actions, especially error recovery actions, counterfactual reasoning supports the operator\u2019s consideration of conditional alternatives and the outcomes of hypothetical scenarios [39], [40].", "startOffset": 214, "endOffset": 218}, {"referenceID": 22, "context": "Preliminary work suggests that incorporating an existing formalization of counterfactual reasoning [27] into our approach will provide designers with a means to do so.", "startOffset": 99, "endOffset": 103}, {"referenceID": 36, "context": "If qualitative differences in knowledge distinguish between novice and expert human operators [41], [42], for example, this would allow a designer to gauge the vulnerability of the system to novice supervisory control and might make salient required emphases in training.", "startOffset": 94, "endOffset": 98}, {"referenceID": 37, "context": "If qualitative differences in knowledge distinguish between novice and expert human operators [41], [42], for example, this would allow a designer to gauge the vulnerability of the system to novice supervisory control and might make salient required emphases in training.", "startOffset": 100, "endOffset": 104}, {"referenceID": 38, "context": "Again, the goal should not be to draw a true and faithful picture of human knowledge, but rather to develop a representation that is epistemically adequate [43] and that allows us to reason expressively and formally about important properties of human knowledge within the context of complex systems.", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": ") [3].", "startOffset": 2, "endOffset": 5}, {"referenceID": 39, "context": "The importance of being able to reason formally about the knowledge of groups of agents in the design and analysis of human-automation systems is significant, as noted in [44]-[46].", "startOffset": 171, "endOffset": 175}, {"referenceID": 41, "context": "The importance of being able to reason formally about the knowledge of groups of agents in the design and analysis of human-automation systems is significant, as noted in [44]-[46].", "startOffset": 176, "endOffset": 180}, {"referenceID": 42, "context": "What are the theoretical issues of shared knowledge relevant to a human-automation system? It is natural to say that an operator may need to know what the automation knows, but when is it useful (and meaningful) to talk about an automation agent knowing what a human agent knows or what other automation agents know? Considering various forms of human-robot teams, for example, the need for a robot to know what the human agent knows seems clear in the case of search and rescue, personal assistant, or physical therapy robots [47].", "startOffset": 527, "endOffset": 531}, {"referenceID": 40, "context": "On the one hand, the importance of capturing notions of agent knowledge and common knowledge, and the need for a formal method for reasoning about knowledge in this domain, have already been noted in the literature [45], [48], and so the potential value of our approach is clear.", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "On the one hand, the importance of capturing notions of agent knowledge and common knowledge, and the need for a formal method for reasoning about knowledge in this domain, have already been noted in the literature [45], [48], and so the potential value of our approach is clear.", "startOffset": 221, "endOffset": 225}], "year": 2013, "abstractText": "In a supervisory control system the human agent\u2019s knowledge of past, current, and future system behavior is critical for system performance. Being able to reason about that knowledge in a precise and structured manner is central to effective system design. In this paper we introduce the application of a wellestablished formal approach to reasoning about knowledge to the modeling and analysis of complex humanautomation systems. An intuitive notion of knowledge in human-automation systems is sketched and then cast as a formal model. We present a case study in which the approach is used to model and reason about a classic problem from the human-automation systems literature; the results of our analysis provide evidence for the validity and value of reasoning about complex systems in terms of the knowledge of the system\u2019s agents. To conclude, we discuss research directions that will extend this approach, and note several systems in the aviation and human-robot team domains that are of particular interest.", "creator": "Microsoft\u00ae Word 2010"}}}