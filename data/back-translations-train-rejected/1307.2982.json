{"id": "1307.2982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2013", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "abstract": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is straightforward to implement and storage efficient. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speed-ups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.", "histories": [["v1", "Thu, 11 Jul 2013 05:52:21 GMT  (572kb,D)", "http://arxiv.org/abs/1307.2982v1", null], ["v2", "Sun, 15 Dec 2013 02:36:21 GMT  (303kb,D)", "http://arxiv.org/abs/1307.2982v2", null], ["v3", "Fri, 25 Apr 2014 01:31:55 GMT  (306kb,D)", "http://arxiv.org/abs/1307.2982v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.DS cs.IR", "authors": ["mohammad norouzi", "ali punjani", "david j fleet"], "accepted": false, "id": "1307.2982"}, "pdf": {"name": "1307.2982.pdf", "metadata": {"source": "CRF", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "authors": ["Mohammad Norouzi", "Ali Punjani", "David J. Fleet"], "emails": ["norouzi@cs.toronto.edu", "alipunjani@cs.toronto.edu", "fleet@cs.toronto.edu"], "sections": [{"heading": null, "text": "Index Terms - Binary Codes, Hamming Removal, Search for Next Neighbor, Multi-Index Hashing.F"}, {"heading": "1 INTRODUCTION", "text": "The problem is that the number of such problems requiring searching near the universe is often increasing to facilitate quick searching near neighbors and matching of functions in vision applications (e.g. [2], [7], [30], [31], [19]). Binary codes are memory efficient and comparisons require only a small number of machine instructions. Millions of binary codes can be compared to a query in less than a second. However, the most compelling reason for binary codes is their use as direct indexes (addresses) in a hash table, resulting in a dramatic increase in search speed compared to an exhaustive linear scan (e.g. [35], [24]). Nevertheless, the use of binary codes as direct hash indices is not necessarily efficient. To find close neighbors, one must examine all radius table entries (or buckets) within some hammering balls around the query."}, {"heading": "1.1 Background: Problem and Related Work", "text": "In fact, most of them are able to reform themselves in the way that they do it: in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it."}, {"heading": "2 MULTI-INDEX HASHING", "text": "Our approach is a form of multi-index hashing. Binary codes from the database are indexed m times in m different hash tables, based on m disjunct binary substrings. In the face of a query code, entries that come close to the query in at least one such substring are considered neighbor candidates. Candidates are then validated based on the entire binary code to remove all non-R neighbors. In order to be workable for large rows, the substrings must be selected so that the number of candidates is small and the memory requirements are reasonable. We also demand that all real neighbors be found. The key idea here stems from the fact that with n binary codes of q bits the vast majority of the possible 2q buckets in a full hash table are empty, as 2q n. It seems expensive to examine all L (q, r) buckets within r bits of a query, as most of them are not comprised of any items (instead, many of them are empty)."}, {"heading": "2.1 Substring Search Radii", "text": "In more detail, any binary code h that includes q bits follows directly from the hole that is larger than any other. (1) The key idea is based on the following statement: If two binary codes h and g differ by r bits or less, then we must assume that q bits are divisible by m, and that the substrings comprise contiguous bits. (1) The key idea is based on the following statement: If two binary codes h and g bits differ by r bits or less each, then they must differ by at most br / mc bits in at least one of their m substrings. This leads to the first statement: If the two bits h-g-bits differ h-bits or less, then H denotes the hamming distance between h and g-bits, then it is 1 \u2264 z-bits bits (z)."}, {"heading": "3 PERFORMANCE ANALYSIS", "text": "Next, we develop an analytical model of search performance to answer two key questions: (1) How do the search costs depend on q = q length of the substrings and thus the number of substrings? (2) How do runtime and memory complexity depend on the size of the database, code length and search radius? To answer these questions, we use a well-known number tied to the sum of binomial coefficients [9]; i.e., for each 0 < \u2264 12 and \u03b7 \u2265 1.b number of searches () \u2264 2H, (5) where H () -log2 \u2212 log2 (1 \u2212) log2 (1 \u2212) is the entropy of a Bernoulli distribution with probability."}, {"heading": "3.1 Optimal Substring Length", "text": "As mentioned in Section 2.2 above, finding a good length of the sub-strings is crucial to the efficiency of multi-index hashing. If the length of the sub-strings is too large or too small, the approach will not be effective. To find a good length of the sub-strings, we first note that the division of costs into Eqn. (7) by q has no effect on the optimal s, which are called s *. Accordingly, s * can be considered a function of two quantities, namely the number of items n and the search ratio r / q.Figure 2 shows costs as a function of the length of the sub-strings s, for 240-bit codes, different database sizes n and different search radii (expressed as a fraction of the code length q). Striked curves represent the cost (s) in (7), while solid curves of the same color represent the upper limit in (8)."}, {"heading": "3.2 Run-Time Complexity", "text": "The choice of s near log2 n provides a characterization of the retrieval time complexity. If s = log2 n is used, the upper limit of the number of reference books (6) also becomes a limit for the number of candidates. In particular, if we replace log2 n with s in (8), then we find the following upper limit for the cost, now as a function of database size, code length and search radius: Cost (s) \u2264 2 q log2 n nH (r / q). (9) Thus, with an even distribution over binary codes, the expected query time complexity is O (q nH (r / q) / log2 n). If a small ratio of r / q is sublinear in n. For example, if we choose m to make s log2 n, then H (.11) <.5, and the runtime complexity becomes O (b / log2)."}, {"heading": "3.3 Storage Complexity", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "5 EXPERIMENTS", "text": "The first is a high-end machine with a 2.9 Ghz dual quad-core Intel Xeon processor, 20 MB L2 cache and 128 GB RAM. The difference in the size of the L2 cache has a big impact on the runtime of the linear scan, as the effectiveness of the linear scan depends heavily on the L2 cache lines. At about ten times the L2 cache, the linear scan on the Intel platform is about twice as fast as on the AMD machines. In comparison, multi-index hashing does not have a serial memory access pattern, so the cache size has no such pronounced effect. The actual runtimes for multi-index hashing on the Intel platforms and AMD are within 20% of one uniform.Both linear hashing dates can be implemented with a linear cache size of 1 MB +."}, {"heading": "5.1 Datasets", "text": "The fact is that we are able to assert ourselves, that we are able, that we are going to be able, that we are going to be able, that we are going to be able, \"he said.\" We have to be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in. \""}, {"heading": "5.2 Results", "text": "In fact, it is the case that most people are able to go in search of another country rather than being able to go in search of another country. (...) The other countries that go in search of another country must go in search of another country. (...) The other countries that go in search of another country must go in search of another country. (...) The other countries that go in search of another country must go in search of another country. (...) The other countries that go in search of another country must go in search of another country. (...)"}, {"heading": "5.4 Direct lookups with a single hash table", "text": "In fact, it is as if we are dealing with an infinite vastness in which most of us are able to save the world."}, {"heading": "5.5 Substring Optimization", "text": "For LSH and MLH, this is equivalent to randomly assigning bits to substrings. It is natural to ask whether further efficiency gains are possible by optimizing this assignment. Specifically, by carefully assigning bits to substrings, one can maximize the discriminability of the various substrings. In other words, while the radius of the substring search, and thus the number of lookups, is determined by the desired search radius on the complete code, by optimizing the assignment of bits to substrings, one can reduce the number of candidates that need to be validated. To explore this idea, we considered a simple method of assigning substrings to individual bits, with bits being greedily assigned based on correlations between substrings. In particular, of those bits that are not yet assigned, the next substring is assigned to the bit that minimizes the already assigned maximum connection between bits and the other bits."}, {"heading": "6 CONCLUSION", "text": "The algorithm is a form of multi-index hashing that has been proven to have sublinear runtime behavior for evenly distributed codes. It is memory-efficient and easy to implement. We demonstrate empirical performance on binary code datasets derived from 1 billion SIFT, and 80 million gist characteristics. With these datasets, we find that for 64-bit and 128-bit codes our new multi-index hashing candidates are often more than two orders of magnitude faster than a linear scan baseline.While the basic algorithm is being developed in this paper, there are several interesting avenues for future research. For example, our preliminary research shows that log2 n is a good choice for substrate length, and it should be possible to formulate a solid mathematical basis for this choice."}, {"heading": "APPENDIX A IMPLEMENTATION DETAILS", "text": "The reason why it has come to this is that most of them are not a group, but a group of people who are able to store and store their data, most of them are able to store and store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store their data, most of them are able to store themselves, most of them are able to store their data."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would also include Mohamed Aly, Rob Fergus, Ryan Johnson, Abbas Mehrabian, and Pietro Perona for useful discussions about this work.REFERENCES Conference on Computer Vision and Pattern Recognition, 2012. [3] M. Aly, M. Munich, and P. Perona. Distributed kd-trees for retrieval from very large image collections. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2012. [3] M. Aly, M. Perona. Distributed kd-trees for retrieval from very large image collections. In Proc. British Machine Vision Conference, 2011. [4] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate near."}], "references": [{"title": "Freak: Fast retina keypoint", "author": ["A. Alahi", "R. Ortiz", "P. Vandergheynst"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed kd-trees for retrieval from very large image collections", "author": ["M. Aly", "M. Munich", "P. Perona"], "venue": "Proc. British Machine Vision Conference,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Communications of the ACM, 51(1):117\u2013122,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The inverted multi-index", "author": ["A. Babenko", "V. Lempitsky"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A. Bergamo", "L. Torresani", "A. Fitzgibbon"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 24,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Brief: Binary robust independent elementary features", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "Proc. European Conference on Computer Vision, page 778792,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "ACM Symposium on Theory of Computing. ACM,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Parameterized Complexity Theory", "author": ["J. Flum", "M. Grohe"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. Int. Conf. Very Large Databases,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymmetric distances for binary embeddings", "author": ["A. Gordo", "F. Perronnin"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 729\u2013736,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-index hashing for information retrieval", "author": ["D. Greene", "M. Parnas", "F. Yao"], "venue": "IEEE Symposium on Foundations of Computer Science, pages 722\u2013731,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Compact hashing with joint optimization of search accuracy and time", "author": ["J. He", "R. Radhakrishnan", "S.-F. Chang", "C. Bauer"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "ACM Symposium on Theory of Computing, pages 604\u2013613,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Proc. European Conference on Computer Vision, volume I, pages 304\u2013317,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. PAMI, 33(1):117\u2013128,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Searching in one billion vectors: re-rank with source coding", "author": ["H. J\u00e9gou", "R. Tavenard", "M. Douze", "L. Amsaleg"], "venue": "IEEE Acoustics, Speech and Signal Processing, pages 861\u2013864. IEEE,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation propagation in imagenet", "author": ["D. Kuettel", "M. Guillaumin", "V. Ferrari"], "venue": "Proc. European Conference on Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 22,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Perceptrons", "author": ["M. Minsky", "S. Papert"], "venue": "MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1969}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D. Lowe"], "venue": "International Conference on Computer Vision Theory and Applications,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "Proc. International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming space metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 25,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["M. Norouzi", "A. Punjani", "D. Fleet"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, 42(3):145\u2013175,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 22,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "Proc. IEEE International Conference on Computer Vision, volume 2,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "LDAHash: improved matching with smaller descriptors", "author": ["C. Strecha", "A. Bronstein", "M. Bronstein", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1):66\u201378,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Trans. PAMI, 30(11):1958\u20131970,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S. Chang"], "venue": "In Proc. International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 21,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 28, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": ", [35], [29], [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [35], [29], [24]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": ", [35], [29], [24]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [33], [29], [20], [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [33], [29], [20], [24]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": ", [33], [29], [20], [24]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": ", [33], [29], [20], [24]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 155, "endOffset": 158}, {"referenceID": 17, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": ", [24], [30], [29], [33]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": ", [24], [30], [29], [33]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": ", [24], [30], [29], [33]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [24], [30], [29], [33]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "The 1-NN problem in Hamming space was called the Best Match problem by Minsky and Papert [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": ", [3], [17], [23], [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": ", [3], [17], [23], [5]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": ", [3], [17], [23], [5]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": ", [3], [17], [23], [5]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "Binary codes with 64 and 128 bits were obtained by random projections (LSH) from the SIFT descriptors [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "The second problem is to find all codes in a dataset H that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem [13], or Point Location in Equal Balls (PLEB) [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "The second problem is to find all codes in a dataset H that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem [13], or Point Location in Equal Balls (PLEB) [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 31, "context": ", [33]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": ", [14], [34] ), but in most cases of interest the desired search radius is larger than is currently feasible (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": ", [14], [34] ), but in most cases of interest the desired search radius is larger than is currently feasible (e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Our work is inspired in part by the multi-index hashing results of Greene, Parnas, and Yao [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "One example is Hamming Locality Sensitive Hashing [15], [10], which aims to solve the (r, )neighbors decision problem: determine whether there exists a binary code h \u2208 H such that \u2016h \u2212 g\u2016H \u2264 r, or whether all codes in H differ from g in (1 + )r bits or more.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "One example is Hamming Locality Sensitive Hashing [15], [10], which aims to solve the (r, )neighbors decision problem: determine whether there exists a binary code h \u2208 H such that \u2016h \u2212 g\u2016H \u2264 r, or whether all codes in H differ from g in (1 + )r bits or more.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": ", see [13], [15]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": ", see [13], [15]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "We next develop an analytical model of search performance to help address two key questions: (1) How does search cost depend on substring length, and hence the number of substrings? (2) How do run-time and storage complexity depend on database size, code length, and search radius? To help answer these questions we exploit a wellknown bound on the sum of binomial coefficients [9]; i.", "startOffset": 378, "endOffset": 381}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] entails storage complexity that is super-linear in n.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our approach is more memory-efficient than that of [13] because we do not enforce exact equality in substring matching.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 119, "endOffset": 122}, {"referenceID": 22, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "Figure 3 depicts empirical distributions of search radii needed for 10-NN and 1000-NN on three sets of binary codes obtained from 1B SIFT descriptors [18], [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "Figure 3 depicts empirical distributions of search radii needed for 10-NN and 1000-NN on three sets of binary codes obtained from 1B SIFT descriptors [18], [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 2, "context": "In all cases, for 64 and 128-bit codes, and for hash functions based on LSH [4] and MLH [24], there is a substantial variance in the search radius.", "startOffset": 76, "endOffset": 79}, {"referenceID": 22, "context": "In all cases, for 64 and 128-bit codes, and for hash functions based on LSH [4] and MLH [24], there is a substantial variance in the search radius.", "startOffset": 88, "endOffset": 92}, {"referenceID": 30, "context": "We consider two well-known large-scale vision corpora: 80M Gist descriptors from 80 million tiny images [32] and 1B SIFT features from the BIGANN dataset [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "We consider two well-known large-scale vision corpora: 80M Gist descriptors from 80 million tiny images [32] and 1B SIFT features from the BIGANN dataset [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "SIFT vectors [21] are 128D descriptors of local image structure in the vicinity of feature points.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Gist features [27] extracted from from 32 \u00d7 32 images capture global image structure in 384D vectors.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 134, "endOffset": 137}, {"referenceID": 22, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Recall rates for BIGANN dataset [18] (1M and 1B subsets) obtained by kNN on 64- and 128-bit MLH and LSH codes.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "For MLH the training set is used to optimize hash function parameters [25].", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "In particular, we plot the fraction of Euclidean 1 nearest neighbors found, by kNN in 64-bit and 128bit LSH [8] and MLH [25] binary codes.", "startOffset": 108, "endOffset": 111}, {"referenceID": 23, "context": "In particular, we plot the fraction of Euclidean 1 nearest neighbors found, by kNN in 64-bit and 128bit LSH [8] and MLH [25] binary codes.", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Finally, recent results have shown that for many datasets in which the binary codes are the result of some form of vector quantization, an asymmetric Hamming distance is attractive [12], [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "Finally, recent results have shown that for many datasets in which the binary codes are the result of some form of vector quantization, an asymmetric Hamming distance is attractive [12], [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 24, "context": "This approach was used in [26], and hence their storage requirement was large.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "The increase in speed of multi-index hashing results reported here compared to those in [26] are attributed to libhugetlbfs.", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is straightforward to implement and storage efficient. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speed-ups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.", "creator": "LaTeX with hyperref package"}}}