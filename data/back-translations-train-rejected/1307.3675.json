{"id": "1307.3675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2013", "title": "Minimum Error Rate Training and the Convex Hull Semiring", "abstract": "We describe the line search used in the minimum error rate training algorithm MERT as the \"inside score\" of a weighted proof forest under a semiring defined in terms of well-understood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation.", "histories": [["v1", "Sat, 13 Jul 2013 19:38:09 GMT  (17kb,D)", "http://arxiv.org/abs/1307.3675v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chris dyer"], "accepted": false, "id": "1307.3675"}, "pdf": {"name": "1307.3675.pdf", "metadata": {"source": "CRF", "title": "Minimum Error Rate Training and the Convex Hull Semiring\u2217", "authors": ["Chris Dyer"], "emails": ["cdyer@cs.cmu.edu"], "sections": [{"heading": null, "text": "We describe the line search used in the minimal error rate training algorithm (Och, 2003) as an \"inside score\" of a weighted forest of evidence under a semicircle defined in terms of well-understood computer geometry operations, resulting in a simple complexity analysis of the dynamic programming algorithms MERT by Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation."}, {"heading": "1 Introduction", "text": "Och's (2003) Minimum Error Rate Algorithm Training (MERT) is widely used in the direct loss minimization of linear translation models. It is based on an efficient and optimal line search and can optimize non-differentiated, corpus-like loss functions. While the original algorithm used n-best hypotheses lists to learn from it, more recent work has developed dynamic programming variants that utilize much larger amounts of hypotheses encoded in finite state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011). Although MERT has several attractive properties (\u00a7 2) and is widely used in previous work, it has not explicated its close relationship to more familiar inference algorithms, and as a result it is less well understood than many other optimization algorithms."}, {"heading": "2 Minimum error rate training", "text": "The goal of MERT is to find a weight vector that minimizes the cost of weight loss reduction (in relation to a development group D). (1). (1). (2). (2). (2). (2). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4). (4).). (4). (4).). (4). (4).). (4). (4).). (4).). (4). (4).). (4).). (4). (4).).). (4).). (4).). (4). (4. (4).). (4). (4). (4).). (4). (4).). (4). (4).).). (4).). (4). (4).). (4). (4). (4).). (4).).). (4). (4). (4).)."}, {"heading": "2.1 Point-line duality", "text": "To determine which lines (and the corresponding hypotheses) are involved, we turn to standard algorithms from the calculation geometry. While algorithms exist to directly calculate the upper hull of a series of lines, we assume that the calculation of the upper hull has a dual problem that can be solved instead: determining the lower convex hull of a series of points (de Berg et al., 2010).The dual representation of a line of the form y = mx + b is the point (m, \u2212 b).This line shows for a given output w0, v and the feature vector H how the model value of the initial hypothesis varies, being represented simply by the point (v > H, \u2212 w > 0 H)."}, {"heading": "3 The Convex Hull Semiring", "text": "Definition 1. A semiring K is a quintuple < K,, 0, 1 > that consists of a set K, an addition operator that is associative and commutative, a multiplication operator that is associative, and the values 0 and 1 in K that are additive and multiplicative identities, respectively. Furthermore, an addition from A to B (a) and B (b) a) must take place (c) a). Furthermore, an addition from K to K must apply to any number of K."}, {"heading": "4 Complexity", "text": "Common structures such as finite state machines and context-free grammars encode an exponential number of different derivatives in polynomial space. Since the values of the convex envelope are set themselves, it is important to understand how their size grows. Fortunately, we can specify the following narrow limits, which guarantee that growth in the size of the input grammar is linear in the worst case scenario: Theorem 2. | A-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B-B."}, {"heading": "Acknowledgements", "text": "We thank David Mount for proposing the dot-line duality and pointing out the relevant literature in computer geometry and Adam Lopez for the TikZ MERT numbers."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Allauzen et al.2007] C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri"], "venue": "In Proc. of CIAA,", "citeRegEx": "Allauzen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Regularization and search for minimum error rate training", "author": ["Cer et al.2008] D. Cer", "D. Jurafsky", "C.D. Manning"], "venue": "In Proc. ACL", "citeRegEx": "Cer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cer et al\\.", "year": 2008}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["de Berg et al.2010] M. de Berg", "M. van Kreveld", "M. Overmars", "O. Schwarzkopf"], "venue": null, "citeRegEx": "Berg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2010}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Dyer et al.2010] C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Datalog 2.0, chapter Dyna: Extending Datalog", "author": ["Eisner", "Filardo2011] J. Eisner", "N.W. Filardo"], "venue": null, "citeRegEx": "Eisner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisner et al\\.", "year": 2011}, {"title": "Optimal search for minimum error rate training", "author": ["Galley", "Quirk2011] M. Galley", "C. Quirk"], "venue": "In Proc. of EMNLP", "citeRegEx": "Galley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2011}, {"title": "Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices", "author": ["S. Kumar", "W. Macherey", "C. Dyer", "F. Och"], "venue": "In Proc. of ACL-IJCNLP", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Z. Li", "C. Callison-Burch", "C. Dyer", "S. Khudanpur", "L. Schwartz", "W. Thornton", "J. Weese", "O. Zaidan"], "venue": "In Proc. of the Fourth Workshop on Statistical Machine Translation", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Translation as weighted deduction", "author": ["A. Lopez"], "venue": "In Proc. of EACL", "citeRegEx": "Lopez.,? \\Q2009\\E", "shortCiteRegEx": "Lopez.", "year": 2009}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Macherey et al.2008] W. Macherey", "F.J. Och", "I. Thayer", "J. Uszkoreit"], "venue": "In Proc. of EMNLP", "citeRegEx": "Macherey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "In Proc. of ACL", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Minimum error rate training semiring", "author": ["Sokolov", "Yvon2011] A. Sokolov", "F. Yvon"], "venue": "In Proc. of AMTA", "citeRegEx": "Sokolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sokolov et al\\.", "year": 2011}, {"title": "Feasibility of human-inthe-loop minimum error rate training", "author": ["Zaidan", "Callison-Burch2009] O.F. Zaidan", "C. Callison-Burch"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zaidan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zaidan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "We describe the line search used in the minimum error rate training algorithm (Och, 2003) as the \u201cinside score\u201d of a weighted proof forest under a semiring defined in terms of wellunderstood operations from computational geometry.", "startOffset": 78, "endOffset": 89}, {"referenceID": 8, "context": "This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al.", "startOffset": 109, "endOffset": 132}, {"referenceID": 6, "context": "(2008) and Kumar et al. (2009) and practical approaches to implementation.", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "While the original algorithm used n-best hypothesis lists to learn from, more recent work has developed dynamic programming variants that leverage much larger sets of hypotheses encoded in finite-state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011).", "startOffset": 240, "endOffset": 307}, {"referenceID": 6, "context": "While the original algorithm used n-best hypothesis lists to learn from, more recent work has developed dynamic programming variants that leverage much larger sets of hypotheses encoded in finite-state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011).", "startOffset": 240, "endOffset": 307}, {"referenceID": 10, "context": "In this paper, we show that the both the original (Och, 2003) and newer dynamic programming algorithms given by Macherey et al.", "startOffset": 50, "endOffset": 61}, {"referenceID": 8, "context": "(2009) can be understood as weighted logical deductions (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011) using weights from a previously undescribed semiring, which we call the convex hull semiring (\u00a73).", "startOffset": 56, "endOffset": 110}, {"referenceID": 0, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 7, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 3, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 4, "context": "In this paper, we show that the both the original (Och, 2003) and newer dynamic programming algorithms given by Macherey et al. (2008) and Kumar et al.", "startOffset": 112, "endOffset": 135}, {"referenceID": 4, "context": "(2008) and Kumar et al. (2009) can be understood as weighted logical deductions (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011) using weights from a previously undescribed semiring, which we call the convex hull semiring (\u00a73).", "startOffset": 11, "endOffset": 31}, {"referenceID": 8, "context": "Zaidan and Callison-Burch (2009) exploit this and find that it is even feasible to solicit human judgments while evaluating \u03b4! Macherey et al. (2008) recommend selecting the midpoint of the segment with the best loss, but Cer et al.", "startOffset": 127, "endOffset": 150}, {"referenceID": 1, "context": "(2008) recommend selecting the midpoint of the segment with the best loss, but Cer et al. (2008) suggest other strategies.", "startOffset": 79, "endOffset": 97}, {"referenceID": 2, "context": "5 in de Berg et al. (2010). From these inequalities, it follows straightforwardly that the number of points in a derivation forest\u2019s total convex hull is upper bounded by |E|.", "startOffset": 8, "endOffset": 27}], "year": 2013, "abstractText": "We describe the line search used in the minimum error rate training algorithm (Och, 2003) as the \u201cinside score\u201d of a weighted proof forest under a semiring defined in terms of wellunderstood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation.", "creator": "LaTeX with hyperref package"}}}