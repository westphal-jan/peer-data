{"id": "1307.5322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2013", "title": "Ontology alignment repair through modularization and confidence-based heuristics", "abstract": "Ontology Matching aims to find a set of semantic correspondences, called an alignment, between related ontologies. In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies. However, most of the alignments produced for large ontologies are logically incoherent. It was only recently that the use of repair techniques to improve the quality of ontology alignments has been explored. In this paper we present a novel technique for detecting incoherent concepts based on ontology modularization, and a new repair algorithm that minimizes the incoherence of the resulting alignment and the number of matches removed from the input alignment. An implementation was done as part of a lightweight version of AgreementMaker system, a successful ontology matching platform, and evaluated using a set of four benchmark biomedical ontology matching tasks. Our results show that our implementation is efficient and produces better alignments with respect to their coherence and f-measure than the state of the art repairing tools. They also show that our implementation is a better alternative for producing coherent silver standard alignments.", "histories": [["v1", "Fri, 19 Jul 2013 16:15:41 GMT  (17kb)", "http://arxiv.org/abs/1307.5322v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["emanuel santos", "daniel faria", "c\\'atia pesquita", "francisco couto"], "accepted": false, "id": "1307.5322"}, "pdf": {"name": "1307.5322.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 7.53 22v1 [cs.AI] 1"}, {"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 Our Setting", "text": "In fact, we are able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "3 Ontology Modularization", "text": "In order to resolve an inconsistency, we have to determine which images are the culprits. (O) Determining all possible culprits is a very demanding task which has to be accomplished in coping with large ontologies. (O) Determining all possible culprits is a very demanding task which has to be applied in coping with large ontologies. (O) Answering the question of scalability (10, 12, 5). (O) Answering the question of answering the question of answering the question of answering the question of answering the question of answering the question of answering the question of answering the question of answering the answer of answering the question of answering the answer of answering the answer of answering the question of answering the answer of answering the answer of answering the answer of the answer of answering the question of answering the answer of the answer of answering the question of answering the answer of the answer of the answer of the answer of the question of answering the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the question of the answer of the answer of the answer of the answer of the answer of the answer of the answer of the question of the answer of"}, {"heading": "4 Alignment Repair", "text": "With an incoherent alignment, the goal of a repair procedure is to remove the mappings from the input alignment so that the resulting set is coherent. Typically, a repair procedure ensures minimal impact on the input, for example by minimizing the number of remote mappings or the sum of the trust values of the remote mappings. There are two main approaches to repairing alignments: global and local. A global repair determines the minimal impact by taking into account all classes and relationships of the matched ontologies. Although this approach delivers better results, it is usually not scalable for large ontologies. This approach is followed by ALCOMO.Local repair is performed by determining the minimal impact in small subsets of matched ontologies. This approach is more efficient, but generates a greater impact on the input alignment than the global approach. LogMap follows this approach and applies it during its ontology alignment process. Our repair process is divided into three main tasks: the input direction of the filtering and the main one: the input direction of the input alignment."}, {"heading": "4.1 Conflict sets of mappings", "text": "Our implementation uses the fragment extraction suggested in Section 3, but also the data structures of AgreementMakerLight. In order to calculate all possible causes of incoherence, we perform a complete depth search in the core fragment structure for each class in the checkset. In this way, we are able to identify all the minimum sets of mappings, so-called conflict sets, that are responsible for the coherences. Formally, we perform ontologies O1 and O2, as well as a series of mappings M for each checkset class A and the disjointed classes B and C, the minimum set of mappings M \u2032 M so that O1, O2, M \u2032 (A C). Note that in order to remove all found incoherences, we must remove at least one mapping from each conflict set."}, {"heading": "4.2 Filtering", "text": "These values are calculated during ontology synchronization and are typically good reliability indicators. They can also be used in the repair process when we need to decide, for example, which mapping to remove in a conflict set. Our repair algorithm uses this information to resolve possible connections during the selection process (see Section 4.3), but also to perform an initial filtering of the conflict sets. The main idea is to solve conflict sets that seem to have a simple solution based on the respective confidence values. For example, if a conflict set contains a mapping with a very low confidence value compared to the other mappings in the set, the problem is to determine a value for which the lowest confidence value in a conflict set should be compared with the other confidence values. As this value should indicate how reliable the confidence values are, we call it trust interval."}, {"heading": "4.3 Removing Mappings", "text": "Given all the conflicting sets (or only a portion of them after filtering), we need to determine which sets of contradictory mappings should be removed. Therefore, we used two main approaches: (1) calculate all the disjointed clusters of contradictory sets. That is, we split the initial group of contradictory sets into sets of contradictory sets that have at least one mapping in common. In this way, however, we are able to independently determine the mappings to be removed for each of these clusters. In some cases, this allows us to verify that the resulting repair is actually a global minimum set. However, since it is not a scalable approach and some associated problems have a huge number of conflict sets, it may not be applicable in every case. For example, with respect to large biomedical sets of OAEI and UMLS-based reference alignments [see Table 1], we are able to resolve these conflicts, and some associated problems are not huge in each case, having a number of conflict sets."}, {"heading": "4.4 The Repair Algorithm", "text": "Algorithm 1 shows a description of our repair algorithm. Its input consists of: (1) a list of contradictory sets of mappings, C. This list contains all contradictory sets for a particular pair of ontologies and input alignments, as described in Section 4.1. Thus, instead of taking the matching ontologies as input, the core algorithm receives the corresponding contradictory sets; (2) the original set of mappings, setMaps. This set is used to keep track of the remote mappings and return them after the repair process; (3) a confidence interval for which filtering is performed as described in Section 4.2; (4) a search depth value, sDepth. This value specifies the depth of the search when handling bindings, as described in Section 4.3; and finally, the verification method, f, is performed."}, {"heading": "5 Evaluation and Discussion", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "5.1 Repairing Silver Standard Alignments", "text": "The construction of a gold or silver standard alignment for an ontology match problem is a very complex task. Even after several automated and manual refinements, alignments still contain errors or incomplete information. In the case of large ontologies, this problem is even greater because manual refinement becomes impractical. OAEI Large Biomedical Track uses a silver standard alignment based on the UMLS Metathesaurus. As the resulting silver standard alignment was incoherent, repaired versions of the alignment of ALCOMO and LogMap were created and used to evaluate the competing alignment systems. Note that the produced silver standard alignment does not have confidence values associated with each of the imaging. Therefore, the repair algorithms cannot take advantage of this information. In this context, we evaluate the quality of AMLR repairs by: (1) determining the degree of incoherence of alignment by the effects of the ALINohm count (25R); (3) the effects of the ALINohm count (25R)."}, {"heading": "5.1.1 FMA-NCI", "text": "In terms of number of mappings, AMLR and LogMap2 lead to narrow results, with 2901 and 2902 mappings respectively. ALCOMO eliminates 80 mappings more. However, in terms of incoherence, ALCOMO performs a repair with only 10 incoherent classes, the same number as AMLR. LogMap and LogMap2 lead to alignments with a high number of incoherent classes. Results show that AMLR significantly improves the incoherence level of LogMap and LogMap2 by reducing them to 10 incoherent classes, as AMLR and ALCOMO produce optimal and near-optimal mappings. Results show that AMLR significantly improves the incoherence level of LogMap and LogMap2 by reducing them to 10 incoherent mappings, as AMLR and ALCOMO produce optimal mappings."}, {"heading": "5.1.2 FMA-SNMD", "text": "In terms of the number of mappings, AMLR provides by far the best results with 8349 mappings, the second best is ALCOMO with 8132. In terms of incoherence, AMLR is the only one that produces a fully coherent alignment. Furthermore, only ALCOMO produces a comparably lower number of incoherent classes. LogMap and LogMap2 did not produce a qualitative alignment. In this case, we also fixed the resulting alignments of the other systems. In all cases, we can significantly improve their results. For example, by removing 6, 4 and 14 mappings from the LogMap, Logmap2 and ALCOMO alignment, we were able to achieve fully coherent alignments."}, {"heading": "5.1.3 SNMD-NCI", "text": "The SNMD-NCI task is very demanding in terms of memory, so both ALCOMO and our incoherence check failed to produce results. This was an exception, as the UMLS-based alignment for this matching problem has more than twice as many mappings in relation to the FMA-SNMD case, which took an average of 10 hours to verify the coherence of the individual mappings. Nevertheless, in terms of the number of mappings AMLR, we achieved an alignment with fewer mappings than LogMap and LogMap2. By applying AMLR to the repairs generated by LogMap and LogMap2, we also obtained a lower number of mappings. Given the results of the FMA-NCI and FMA-SNMD cases, this indicates that these mappings exhibit a much higher level of incoherence. For example, AMLR removes 324 mappings from the LogMap alignment, which indicates that the AMR coefficient is still the majority of the AMR card."}, {"heading": "5.2 Repairing alignments", "text": "In addition to ensuring the coherence of an alignment, a repair process is also used to improve the quality of the alignment with respect to the f-measure. As the repair process cannot, by its very nature, improve the recall, its goal is to improve precision without reducing the recall. Therefore, its application leads to better results when the input alignment is low precision. To evaluate the impact of AMLR on the f-measure of the input alignments, we consider the alignments created by OMSZ for OAEI anatomy and large biomedical traces. In terms of anatomy, we use the provided gold standard alignment, which is coherent and considered accurate. In this case, we use an alignment generated by OMSZ in the initial phase of its alignment process, where the precision is low. With respect to the large biomedical gauge, we show in Section 5.1 that AMLR produces much better results than the other systems."}, {"heading": "5.2.1 Anatomy", "text": "Considering the degree of coherence of the resulting alignments (see Table 3), we once again conclude that AMLR delivers the best results with 0 incoherent classes. LogMap2 produces an alignment with almost as many incoherent classes as the original unrepaired alignment. In terms of f-readings, the resulting values are closer to the original alignment values due to the low initial alignment. However, it is AMLR that achieves the best results in one of its settings, namely 67.1% f-measurement, which is a marked improvement of 0.7% over the original f-readout. Note that the worst of the four AMLR settings has the same f-readout as ALCOMO, 66.7%, and is still a better measure than LogMap2, 66.6%."}, {"heading": "5.2.2 FMA-NCI", "text": "In this evaluation (see Table 4), ALCOMO and AMLR achieved similar results in terms of coherence and f-measurement. These results were excluded because the original alignment already had a high precision value. Both ALCOMO and AMLR produce an alignment with 83.8% f-measurement or more and with only 2 incoherent classes. LogMap2 produces the worst results by producing an alignment with 147 incoherent classes and the lowest f-measurement."}, {"heading": "5.2.3 FMA-SNOMED", "text": "The results show (see Table 5) that the different settings of OMSZ can lead to very different results. For example, by not using a filter, OMSZ generates an alignment with an f-measure that is 1.1% higher than the original alignment. However, by applying a filter with a confidence interval of 0.0 or 0.05, OMSZ generates the worst alignment with respect to the f-measure. LogMap2 also generates an alignment with a lower f-measure than the original alignment. The contrasting results generated by the different confidence intervals can be explained by the number of filtered confidence sets. In this case, we filter 2845, 6398, 13832 contradictory sets to a total of 13932 with regard to confidence intervals of 0.1, 0.05 and 0.0, respectively. In the case of a confidence interval of 0.0, most of the contradictory sets were filtered."}, {"heading": "5.2.4 SNOMED-NCI", "text": "As in Section 5.1.3, we could not determine the degree of coherence of the alignment. Results show (see Table 6) that OMSZ delivers better results than the original alignment. Its best settings result in an alignment with an improvement of the f-measure of the original alignment by 0.9%. LogMap2 also improves the f-measure of the original alignment, but only by 0.3%. Regarding various settings of the AMLR test, the results show that it is not clear how to set the confidence interval. However, it is clear that by filtering the contradictory sets we can achieve better and more efficient results."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we introduced a new modularization-based technique to extract the core fragments of ontologies involved in incoherence, and a new repair algorithm that uses heuristics and filtering strategies to determine near-optimal solutions to ensure coherent alignment. We conducted a comprehensive evaluation comparing our implementation with the state of the art of repair systems, and the results show that our repair implementation achieved better results in terms of coherence, i.e. number of incoherent classes, and input alignment impacts, i.e. the number of images removed. In fact, our implementation achieved remarkably better results than the repaired silver-standard images of the OAEI Large biomedical Track. Thus, in addition to producing coherent silver-standard alignments, the results also show that our filtration strategy can achieve good results when paired with confidence mappings."}], "references": [{"title": "The unified medical language system (umls): integrating biomedical terminology", "author": ["O. Bodenreider"], "venue": "Nucleic Acids Research, 32(Database-Issue):267\u2013270,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Agreementmaker: efficient matching for large real-world schemas and ontologies", "author": ["I.F. Cruz", "F.P. Antonelli", "C. Stroe"], "venue": "Proc. VLDB Endow., 2(2):1586\u20131589, Aug.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The agreementmakerlight ontology matching system", "author": ["E.S.M.P.I.C. Daniel Faria", "Catia Pesquita", "F. Couto"], "venue": "In The 12th International Conference on Ontologies, DataBases, and Applications of Semantics, ODBASE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Matching directories and owl ontologies with aroma", "author": ["J. David", "F. Guillet", "H. Briand"], "venue": "Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM \u201906, pages 830\u2013831, New York, NY, USA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Ontology module extraction for ontology reuse: an ontology engineering perspective", "author": ["P. Doran", "V.A.M. Tamma", "L. Iannone"], "venue": "M. J. Silva, A. H. F. Laender, R. A. Baeza-Yates, D. L. McGuinness, B. Olstad, \u00d8. H. Olsen, and A. O. Falc\u00e3o, editors, CIKM, pages 61\u201370. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Instance-based matching of large ontologies using locality-sensitive hashing", "author": ["S. Duan", "A. Fokoue", "O. Hassanzadeh", "A. Kementsietsidis", "K. Srinivas", "M.J. Ward"], "venue": "P. Cudr\u00e9-Mauroux, J. Heflin, E. Sirin, T. Tudorache, J. Euzenat, M. Hauswirth, J. X. Parreira, J. Hendler, G. Schreiber, A. Bernstein, and E. Blomqvist, editors, International Semantic Web Conference (1), volume 7649 of Lecture Notes in Computer Science, pages 49\u201364. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Yam: a schema matcher factory", "author": ["F. Duchateau", "R. Coletta", "Z. Bellahsene", "R.J. Miller"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, CIKM \u201909, pages 2079\u20132080, New York, NY, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Results of the ontology alignment evaluation initiative", "author": ["J. Euzenat", "A. Ferrara", "W.R. van Hage", "L. Hollink", "C. Meilicke", "A. Nikolov", "D. Ritze", "F. Scharffe", "P. Shvaiko", "H. Stuckenschmidt", "O. Sv\u00e1b-Zamazal", "C.T. dos Santos"], "venue": "CEUR Workshop Proceedings. CEUR-WS.org,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Ontology matching", "author": ["J. Euzenat", "P. Shvaiko"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting modules from ontologies: A logic-based approach", "author": ["B.C. Grau", "I. Horrocks", "Y. Kazakov", "U. Sattler"], "venue": "C. Golbreich, A. Kalyanpur, and B. Parsia, editors, OWLED, volume 258 of CEUR Workshop Proceedings. CEUR-WS.org,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Logmap: Logic-based and scalable ontology matching", "author": ["E. Jim\u00e9nez-Ruiz", "B.C. Grau"], "venue": "L. Aroyo, C. Welty, H. Alani, J. Taylor, A. Bernstein, L. Kagal, N. F. Noy, and E. Blomqvist, editors, International Semantic Web Conference (1), volume 7031 of Lecture Notes in Computer Science, pages 273\u2013288. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale interactive ontology matching: Algorithms and implementation", "author": ["E. Jim\u00e9nez-Ruiz", "B.C. Grau", "Y. Zhou", "I. Horrocks"], "venue": "L. D. Raedt, C. Bessi\u00e8re, D. Dubois, P. Doherty, P. Frasconi, F. Heintz, and P. J. F. Lucas, editors, ECAI, volume 242 of Frontiers in Artificial Intelligence and Applications, pages 444\u2013 449. IOS Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Gomma: a component-based infrastructure for managing and analyzing life science ontologies and their evolution", "author": ["T. Kirsten", "A. Gross", "M. Hartung", "E. Rahm"], "venue": "J. Biomedical Semantics, 2:6,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient approximation algorithms for repairing inconsistent databases", "author": ["A. Lopatenko", "L. Bravo"], "venue": "R. Chirkova, A. Dogac, M. T. \u00d6zsu, and T. K. Sellis, editors, ICDE, pages 216\u2013225. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Alignment Incoherence in Ontology Matching", "author": ["C. Meilicke"], "venue": "University Mannheim,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Journal on data semantics xi. chapter Exploring the Semantic Web as Background Knowledge for Ontology Matching, pages 156\u2013190", "author": ["M. Sabou", "M. D\u2019Aquin", "E. Motta"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Ontology matching: State of the art and future challenges", "author": ["P. Shvaiko", "J. Euzenat"], "venue": "IEEE Trans. Knowl. Data Eng., 25(1):158\u2013176,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Matching large ontologies based on reduction anchors", "author": ["P. Wang", "Y. Zhou", "B. Xu"], "venue": "T. Walsh, editor, IJCAI, pages 2343\u20132348. IJCAI/AAAI,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].", "startOffset": 224, "endOffset": 237}, {"referenceID": 6, "context": "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].", "startOffset": 224, "endOffset": 237}, {"referenceID": 8, "context": "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].", "startOffset": 224, "endOffset": 237}, {"referenceID": 16, "context": "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].", "startOffset": 224, "endOffset": 237}, {"referenceID": 15, "context": "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].", "startOffset": 285, "endOffset": 289}, {"referenceID": 11, "context": "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].", "startOffset": 116, "endOffset": 135}, {"referenceID": 14, "context": "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].", "startOffset": 116, "endOffset": 135}, {"referenceID": 12, "context": "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].", "startOffset": 116, "endOffset": 135}, {"referenceID": 17, "context": "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].", "startOffset": 116, "endOffset": 135}, {"referenceID": 5, "context": "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].", "startOffset": 116, "endOffset": 135}, {"referenceID": 7, "context": "The Ontology Alignment Evaluation Initiative (OAEI) [8] has been the major playfield for ontology alignment, with the participation of state of the art ontology matching", "startOffset": 52, "endOffset": 55}, {"referenceID": 10, "context": "With respect to large ontologies alignments, the degree of incoherency is typically higher, and only one participant, LogMap [11, 12], detects incoherencies and uses repair techniques to improve the quality of the resulting alignment.", "startOffset": 125, "endOffset": 133}, {"referenceID": 11, "context": "With respect to large ontologies alignments, the degree of incoherency is typically higher, and only one participant, LogMap [11, 12], detects incoherencies and uses repair techniques to improve the quality of the resulting alignment.", "startOffset": 125, "endOffset": 133}, {"referenceID": 14, "context": "To the best of our knowledge, there are only two systems that perform alignment repair: LogMap and ALCOMO [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Since there is no gold standard alignment, a silver standard alignment based on the UMLS Metathesaurus [1] is provided for evaluating each matching problem.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "In an ontology matching setting we say that an alignment M between ontologies O1 and O2 is coherent if there is no class or property in O1 or O2 that is unsatisfiable due to M (see [15] for a formal definition).", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "An implementation of our algorithms was done as part of AgreementMakerLight [3], a lightweight version of AgreementMaker [2], a successful ontology matching platform.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "An implementation of our algorithms was done as part of AgreementMakerLight [3], a lightweight version of AgreementMaker [2], a successful ontology matching platform.", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].", "startOffset": 107, "endOffset": 118}, {"referenceID": 11, "context": "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].", "startOffset": 107, "endOffset": 118}, {"referenceID": 4, "context": "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].", "startOffset": 107, "endOffset": 118}, {"referenceID": 9, "context": "In comparison to the module proposed by [10] and implemented by LogMap2, which computes fragments that contain 37% of the classes in FMA and 38% of the classes in NCI, there is a considerably improvement - only 5% of the total classes of FMA and NCI.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "A similar strategy has been applied for repairing inconsistent databases [14].", "startOffset": 73, "endOffset": 77}], "year": 2013, "abstractText": "Ontology Matching aims to find a set of semantic correspondences, called an alignment, between related ontologies. In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies. However, most of the alignments produced for large ontologies are logically incoherent. It was only recently that the use of repair techniques to improve the quality of ontology alignments has been explored. In this paper we present a novel technique for detecting incoherent concepts based on ontology modularization, and a new repair algorithm that minimizes the incoherence of the resulting alignment and the number of matches removed from the input alignment. An implementation was done as part of a lightweight version of AgreementMaker system, a successful ontology matching platform, and evaluated using a set of four benchmark biomedical ontology matching tasks. Our results show that our implementation is efficient and produces better alignments with respect to their coherence and f-measure than the state of the art repairing tools. They also show that our implementation is a better alternative for producing coherent silver standard alignments.", "creator": "LaTeX with hyperref package"}}}