{"id": "1307.5730", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2013", "title": "A New Strategy of Cost-Free Learning in the Class Imbalance Problem", "abstract": "In this work, we define cost-free learning (CFL) formally in comparison with cost-sensitive learning (CSL). The main difference between them is that a CFL approach seeks optimal classification results without requiring any cost information, even in the class imbalance problem. In fact, several CFL approaches exist in the related studies, such as sampling and some criteria-based pproaches. However, to our best knowledge, none of the existing CFL and CSL approaches are able to process the abstaining classifications properly when no information is given about errors and rejects. Based on information theory, we propose a novel CFL which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers. Using the strategy, we can deal with binary/multi-class classifications with/without abstaining. Significant features are observed from the new strategy. While the degree of class imbalance is changing, the proposed strategy is able to balance the errors and rejects accordingly and automatically. Another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the \"equivalent\" costs in binary classifications. The connection between rejection thresholds and ROC curve is explored. Empirical investigation is made on several benchmark data sets in comparison with other existing approaches. The classification results demonstrate a promising perspective of the strategy in machine learning.", "histories": [["v1", "Mon, 22 Jul 2013 14:36:03 GMT  (162kb)", "http://arxiv.org/abs/1307.5730v1", "Classification, class imbalance, cost-free learning, cost-sensitive learning, abstaining, mutual information, ROC"]], "COMMENTS": "Classification, class imbalance, cost-free learning, cost-sensitive learning, abstaining, mutual information, ROC", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaowan zhang", "bao-gang hu"], "accepted": false, "id": "1307.5730"}, "pdf": {"name": "1307.5730.pdf", "metadata": {"source": "CRF", "title": "A New Strategy of Cost-Free Learning in the Class Imbalance Problem", "authors": ["Xiaowan Zhang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 7.57 30v1 [cs.LG] 2 2Ju l 201 3Index Terms - Classification, Class Imbalance, Free Learning, Cost Sensitive Learning, Abstinence, Mutual Information, ROC."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "1.1 Related Work", "text": "If the costs are unequal and unknown, Maloof [20] uses ROC curve to show the performance of binary classifications under different cost settings; the study can be considered a comparison of classifiers rather than finding an optimal operating point; cost curve [21], [14] can be used to visualize the optimal expected costs across a number of cost settings, but it does not fit multi-class problems; Zadrozny and Elkan [22] apply multiple linear regression to estimate costs; however, the method requires cost information of training sets to be known; it is proposed to select from a limited set of cost values, and the final decisions are made by users; there are some CFL approaches to the class balance problem; different sampling strategies [24], [25], [26] attempt to modify the unbalanced class distributions."}, {"heading": "1.2 Paper Organization", "text": "The rest of this paper is broken down as follows: Section 2 gives a brief overview of NI; Section 3 presents our CFL strategy; Section 4 analyzes the relationships between optimal parameters and cost conditions; and presents the graphical interpretations of ROC curve diagrams; and the experimental results are presented in Section 5. Finally, we conclude this work in Section 6.3."}, {"heading": "2 REVIEW: NORMALIZED MUTUAL INFORMATION", "text": "Normalized mutual information (NI = j) was used as an evaluation criterion to measure the degree of dependence between objectives T and decision outputs Y (i.e., it becomes asNI (T, Y) = I (T, Y) H (T), where I (T, Y) is the mutual information between objectives T and Y, H (T) is the Shannon entropy of T. Note that NI (T, Y) is in the range [0, 1]. Suppose the Anm class dispenses with classification, where each class is called 1, 2,. m, and the rejected class is called m + 1. The value of the target variables T ranges from 1 to m, while the decision variables Y ranges from 1 to m + 1. Then haveI (T, Y) = m."}, {"heading": "3 NI-BASED CLASSIFICATION", "text": "In this paper, we distinguish between two types of classifications, \"no classification\" for no rejection and \"abandonment of classification\" for rejection. From the phenomenon that different error types and types of rejection have different effects on NI, we can conclude that NI considers costs to be unequal as opposed to accuracy. In fact, cost information is hidden in NI, and we use its bias against the minority class. If we leave x = [x1, x2,..., xn] T stands for a data matrix with n instances to be classified, xl stands for the intervention mark vector, l = 2,., n. The target vector is called a vector."}, {"heading": "3.1 Non-Abstaining Classification", "text": "If the classification is not maintained, the first condition for deriving yl in (2) should only be met, i.e., yl = argmax i (i (xl) \u03c4i), 0 < \u03c4i \u2264 1, i = 1, 2,.., m, l = 1, 2,.. Then we have the following: \u03c6i (xl) = \u03b1ii (xl) = \u03c4m i (xl) \u03c4i. It is obvious that argmaxi \u03c6i (xl) = argmaxi (i (xl) \u03c4i) and \u03b1m = 1. The optimal decision for yl remains the same. The effect of allocating weights to the probable results is the same as setting decision thresholds."}, {"heading": "3.2 Abstaining Classification", "text": "We refer to Tr = [Tr1, Tr2,.., Trm] T-Rm as a reject threshold vector in dealing with the abandonment of classification. Let us leave 1 \u2212 Tri = \u03c4i, Tri is in the range [0, 1), i = 1, 2,.., m. Decision output for yl = f (xl), Tr) lies within m + 1 classes. Then we propose NI (t, y = f (x), Tr))), subject toyl = argmax i (i (xl) 1 \u2212 Tri) ifmax (i (xl) 1 \u2212 Tri) \u2265 1, m + 1 otherwise, (5) 0 \u2264 Tri < 1, 0 \u2264 m \u00b2 i = 1Tri < m \u2212 1, Tri < m \u2212 l = 2,."}, {"heading": "3.3 Optimization Algorithm", "text": "We use a general optimization algorithm called the \"Powell algorithm,\" which is a direct method for nonlinear optimization without calculating the derivatives [41]. It is also frequently used in image registration to find the optimal registration parameters. Algorithm 1 Learning algorithm Input: Probabilistic outputs resulting from x, target designations t, D as degree of freedom in.Output: Result 1: Starting point 1: Starting point 1 as a random vector in the range of derivatives, d1, d2,.., dD as linear independent vectors, number of iterations W = 0, D as degree of freedom in.Output: Starting point 1: Starting point 1 as a random vector in the range of derivatives, d1, d2,."}, {"heading": "4 RELATIONS IN BINARY CLASSIFICATION", "text": "The previous section completes the essence of the current framework. It can be considered a general way to make traditional learning algorithms information theoretical, with optimal parameters reflecting the degree of bi-implication of NI and revealing cost information to a certain extent. In this section, we focus on binary classification and analyze the relationships between optimal parameters and cost conditions, and discover some graphical interpretations of benchmarks on the ROC curve that allow users to adjust the parameters more conveniently using the ROC curve."}, {"heading": "4.1 Normalized Cost Matrix", "text": "Friedel et al. [14] derive the normalized cost matrix on the basis of the total risk that is written asRisk = \u2211 i, j\u03bbijp (\u03b2 | i) p (i), (7) where \u03bbij is the original cost in the common cost matrix that assigns an instance of class i to class j isRisk (j | xl), p (j | i) is the true prior probability of class i. The conditional risk of assigning an instance xl to class j isRisk (j | xl) = m \u2211 i = 1\u03bbijp (i | xl), (8) where p (i | xl) is the true posterior probability of class i in the face of xl. By applying the method of transformation costs [14] we find that the normalization method for the total risk is also applicable to the conditional risk."}, {"heading": "4.2 Optimal Weight and Misclassification Cost", "text": "The relationship between decision thresholds and costs was derived by Elkan [7] by minimizing conditional risk. Taking into account the normalized cost matrix in (9), the decision threshold in (9) can be presented as a threshold for optimal decision-making. Otherwise, the optimal weight vector cannot be derived or displayed correctly. In our current work, the optimal weight vector is considered a threshold for optimal weight-making. We apply it in the decision rule for the maximum weighted probability. & lt\u03b1 P cannot be derived or incorrect. In our current work, the optimal weight vector is considered a threshold for the optimal weight condition N, 1) T. We apply it in the decision rule for the maximum weighted probability. < The optimal prediction is the positive class when the optimal weight threshold for the optimal weight threshold N, 1)."}, {"heading": "4.3 Optimal Rejection Thresholds and Costs", "text": "If the binary classification is abstained, the relationships between the rejection thresholds and costs can be represented in a form of explicit formulas [18]. With the optimal rejection threshold, these relationships can be represented as vector T * r = [T * rN, T * rP] T and the normalized cost matrix in (10). In addition, the value of the \"equivalent\" rejection costs derived from (13) can be used as foreknowledge assuming cost consistency. Definition 4. Given the \"equivalent\" misclassification costs, we can define the \"equivalent\" rejection costs."}, {"heading": "4.4 Graphical Interpretations of ROC Curve Plots with/without Abstaining", "text": "In binary classification, an ROC chart becomes complete information about the performance of each class [42], so that a total performance metric [43], such as AUC, can be formed. \u2212 This is a preferred feature in the processing class Imbalance Problems [44]. In addition, an ROC curve can also provide the graphic interpretation rate for non-abstaining and abstaining from classifications in Figure 2 (where TPR and FPR are true positive rates. We refer to A, CR, E and Rej as accuracy, correct detection rate and rejection rate, respectively. CN and CP are the total numbers of negatives and positives, respectively, CFN, CTN, CRN and CRP are the numbers of false negatives, false positives, true negatives, true positives, and negative positives."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Configuration", "text": "Most of the data sets are from the UCI Machine Learning Repository1, Ism is from [25], Rooftop is from [20], and Phoneme is from the KEEL Datasets2. Furthermore, Table 3 lists the procedure of our NI-based experiments and is reactivated in the range [0, 1]. We perform triple cross-validation, and all experiments are repeated ten times to get the average results."}, {"heading": "5.2 Evaluation Criterion", "text": "In order to clearly represent the changes of each class, egg and reji are used as the error rate or rejection rate within their ith class. Also, the total error rate (\"E\") and the total rejection rate (\"Rej\") are used. \"A\" is the abbreviation for the total accuracy. \"G\" is the abbreviation for G mean with the formula G \u2212 mean = (Hm i = 1 Ai) 1m, where Ai represents the accuracy within their ith class. In binary class tasks, we also evaluate the F measure (\"F\" for short)."}, {"heading": "5.3 Binary Class Tasks", "text": "The results of the binary class data sets are presented in Table 4. Both conventional classifiers have high accuracy and low error rates of the negative class. - Cost-sensitive learning performs well under current cost settings, but its accuracy is lowest if the class distribution differs greatly. - On Nursery and Letter, the error rate is zero with cost-sensitive learning at the price of high error rate of the negative class. - Cost-sensitive learning at the price of high error rate of the negative class. - Cost-sensitive learning at the price of high error rate of the negative class. - Cost-sensitive learning at the price of high error rate of the negative class. - Mean no rej no rej and NI no rej no rej - Rej no rej - Rej no rej (%) - Rej no rej (%) G9090 - Classification of two classes. - If a89 (continuation of previous page) Data set MethjEN (9jY%) (9SME) (94.69%) (9SME) (94.9%) (9SME) (4P) N9090 - Classification of two classes."}, {"heading": "5.4 Multi-Class Tasks", "text": "The detailed results of the multi-class data sets can be seen from Table 7 to Table 9, including performance ratings and the values of the optimal parameters. Compared to conventional classifications, which have a high minority class error rate, SMOTE is effective in reducing these errors. Cost-sensitive learning classifies all instances of the class that have the minimum number of instances. Both Gmean no rej and NI no rej score well on low minority class error rates, high G averages and high NI. Chow's rejection and Gmean rej reject many minority class instances; in addition, Gmean rej rejects many minority class instances. Overall, our NI rej performs best with low minority class error rates, a certain rejection rate, high G averages and the highest NI. In summary, the above observations suggest that 12 (a) the rating of the methods is ineffective: \"each performance is not the best.\""}, {"heading": "6 CONCLUSION", "text": "In this paper, we propose a new strategy of the CFL to solve the problem of class imbalance. Based on the specific property of mutual information that can distinguish different types of errors and distinguish types of rejections, we strive to maximize this as a general rule for dealing with binary / multiclass classifications with / without abstention. A peculiarity is the abandonment of classifications when information about errors and rejections is not known. To the best of our knowledge, no other existing approach to this scenario comes into question. In addition, we can derive the \"equivalent\" costs of binary classifications. Generally, the \"equivalent\" costs are changed according to the distributions of the given data sets. Therefore, the current strategy represents an \"objective\" reference for CSL when users wish to evaluate the methods \"-\": unavailable, the best performance in each cell is weighed. (b) Optimal weights and rejection thresholds are listed as standard values (deviation)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is partially supported by the NSFC (No. 61075051) and the SPRP-CAS (No. XDA06030300)."}], "references": [{"title": "The Class Imbalance Problem: A Systematic Study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis, vol. 6, pp. 429-449, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning from Imbalanced Data: Evaluation Matters", "author": ["T. Raeder", "G. Forman", "N.V. Chawla"], "venue": "Data Mining: Foundations and Intelligent Paradigms, D. E. Holmes and L. C. Jain, Eds., New York: Springer, pp. 315-331, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Test-Cost Sensitive Naive Bayes Classification", "author": ["X. Chai", "L. Deng", "Q. Yang", "C.X. Ling"], "venue": "Proc. IEEE Int\u2019l Conf. Data Mining, pp. 51-58, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "MetaCost: A General Method for Making Classifiers Cost-Sensitive", "author": ["P. Domingos"], "venue": "Proc. ACM SIGKDD Int\u2019l Conf. Knowledge Discovery and Data Mining, pp. 155-164, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Cost-Sensitive Learning by Cost-Proportionate Example Weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "Proc. IEEE Int\u2019l Conf. Data Mining, pp. 435-442, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "An Instance-Weighting Method to Induce Cost- Sensitive Trees", "author": ["K.M. Ting"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 14, no. 13, pp. 659-665, May, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "The Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "Proc. Int\u2019l Joint Conf. Artificial Intelligence, pp. 973-978, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Thresholding for Making Classifiers Cost-Sensitive", "author": ["V.S. Sheng", "C.X. Ling"], "venue": "Proc. AAAI National Conf. Artificial Intelligence, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from Imbalanced Data", "author": ["H. He", "E.A. Garcia"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 21, no. 9, pp. 1263-1284, Sept. 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On Optimum Recognition Error and Reject Tradeoff", "author": ["C. Chow"], "venue": "IEEE Trans. Information Theory, vol. 16, no. 1, pp. 41-46, 1970.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1970}, {"title": "Classification of Intrusion Detection Alerts Using Abstaining Classifiers", "author": ["T. Pietraszek"], "venue": "Intelligent Data Analysis, vol. 11, no. 3, pp. 293-316, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Aggregating Abstaining and Delegating Classiers for Improving Classication Performance: An Application to Lung Cancer Survival Prediction", "author": ["M.-R. Temanni", "S.-A. Nadeem", "D. Berrar", "J.-D. Zucker"], "venue": "[Online]. Available: http://camda.bioinfo.cipf.es/camda07/agenda/detailed.html", "citeRegEx": "12", "shortCiteRegEx": null, "year": 0}, {"title": "The Interaction between Classification and Reject Performance for Distance- Based Reject-Option Classifiers", "author": ["T.C. Landgrebe", "D.M. Tax", "P. Pacl\u0131\u0301k", "R.P. Duin"], "venue": "Pattern Recognition Letters, vol. 27, no. 8, pp. 908-917, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Cost Curves for Abstaining Classifiers", "author": ["C.C. Friedel", "U. R\u00fcckert", "S. Kramer"], "venue": "Proc. Workshop on ROC Analysis in Machine Learning. Int\u2019l Conf. Machine Learning, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the Classification Cost of Support Vector Classifiers Through an ROC-Based Reject Rule", "author": ["F. Tortorella"], "venue": "Pattern Anal. Applic., vol. 7, pp. 128-143, 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Local Estimation of Posterior Class Probabilities to Minimize Classification Errors", "author": ["A. Guerrero-Curieses", "J. Cid-Sueiro", "R. Alaiz-Rodr\u0131\u0301guez", "A.R. Figueiras-Vidal"], "venue": "IEEE Trans. Neural Networks, vol. 15, no. 2, pp. 309-317, Mar. 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-Sensitive Learning and the Class Imbalance Problem", "author": ["C.X. Ling", "V.S. Sheng"], "venue": "Encyclopedia of Machine Learning, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "What are the Differences between Bayesian Classifiers and Mutual-Information Classifiers", "author": ["B.-G. Hu"], "venue": "arXiv:1105.0051v2 [cs.IT], 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning When Data Sets are Imbalanced and When Costs are Unequal and Unknown", "author": ["M.A. Maloof"], "venue": "Proc. Workshop on Learning from Imbalanced Data Sets II. Int\u2019l Conf. Machine Learning, 2003.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Explicitly Representing Expected Cost: An Alternative to ROC Representation", "author": ["C. Drummond", "R.C. Holte"], "venue": "Proc. ACM SIGKDD Int\u2019l Conf. Knowledge Discovery and Data Mining, pp. 198-207, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning and Making Decisions When Costs and Probabilities are Both Unknown", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proc. ACM SIGKDD Int\u2019l Conf. Knowledge Discovery and Data Mining, pp. 204- 213, 2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Cost-Sensitive Face Recognition", "author": ["Y. Zhang", "Z.-H. Zhou"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 10, pp. 1758-1769, Oct. 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Matwin,\u201cAddressing the Curse of Imbalanced Training Sets: One-Sided Selection,", "author": ["S.M. Kubat"], "venue": "Proc. Int\u2019l Conf. Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "SMOTE: Synthetic Minority Over-Sampling Technique", "author": ["N.V. Chawla", "K.W. Bowyer", "W.P. Kegelmeyer"], "venue": "Journal Of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Yao,\u201cDynamic Sampling Approach to Training Neural Networks for Multiclass Imbalance Classification,", "author": ["M. Lin", "K. Tang"], "venue": "IEEE Trans. Neural Networks and Learning Syst.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Learning on the Border: Active Learning in Imbalanced Data Classification", "author": ["S. Ertekin", "J. Huang", "L. Bottou", "L. Giles"], "venue": "Proc. ACM Conf. Information and Knowledge Management, pp. 127-136, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature Selection for Text Categorization on Imbalanced Data", "author": ["Z. Zheng", "X. Wu", "R. Srihari"], "venue": "ACM SIGKDD Explor. Newsl., vol. 6, pp. 80-89, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Combating the Small Sample Class Imbalance Problem Using Feature Selection", "author": ["M. Wasikowski", "X.-W. Chen"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 22, no. 10, pp. 1388-1400, Oct. 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Relationships between Diversity of Classification Ensembles and Single-Class Performance Measures", "author": ["S. Wang", "X. Yao"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 25, no. 1, pp. 206-219, Jan. 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensembles of \u03b1-Trees for Imbalanced Classification Problems", "author": ["Y. Park", "J. Ghosh"], "venue": "IEEE Trans. Knowledge and Data Eng., preprint, Dec. 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "One-Class Svms for Document  Classification", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 139- 154, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Supervised Versus Unsupervised Binary-Learning by Feedforward Neural Networks", "author": ["N. Japkowicz"], "venue": "Machine Learning, vol. 42, pp. 97-122, 2001.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting for Learning Multiple Classes with Imbalanced Class Distribution", "author": ["Y. Sun", "M.S. Kamel", "Y. Wang"], "venue": "Proc. IEEE Int\u2019l Conf. Data Mining, pp. 592-602, 2006.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimizing Abstaining Classifiers using ROC Analysis", "author": ["T. Pietraszek"], "venue": "Proc. Int\u2019l Conf. Machine Learning, pp. 665-672, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Reject Option with Multiple Thresholds", "author": ["G. Fumera", "F. Roli", "G. Giacinto"], "venue": "Pattern Recognition, vol. 33, no. 12, pp. 2099-2101, 2000.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2000}, {"title": "Confidence-Based Classifier Design", "author": ["M. Li", "I.K. Sethi"], "venue": "Pattern Recognition, vol. 39, no. 7, pp. 1230-1240, 2006.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Information-Theoretic Measures for Objective Evaluation of Classifications", "author": ["B.-G. Hu", "R. He", "X.-T. Yuan"], "venue": "Acta Automatica Sinica, vol. 38, pp. 1170-1182, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Information Theory, Inference and Learning Algorithms", "author": ["D. MacKay"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Learning from Examples with Information-Theoretic Criteria", "author": ["J. Principe", "D. Xu", "Q. Zhao", "J. Fisher"], "venue": "Journal of VLSI Signal Processing Systems, vol. 26, no. 1/2, pp. 61-77, Aug. 2000.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2000}, {"title": "An Efficient Method for Finding the Minimum of a Function of Several Variables without Calculating Derivatives", "author": ["M.J.D. Powell"], "venue": "Computer Journal, vol. 7, no. 2, pp. 155-162, 1964.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1964}, {"title": "A Survey on Graphical Methods for Classification Predictive Performance Evaluation", "author": ["R.C. Prati", "G.E.A.P.A. Batista", "M.C. Monard"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 23, no. 11, pp. 1601-1618, Nov. 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluation Criteria Based on Mutual Information for Classifications Including Rejected Class", "author": ["B.-G. Hu", "Y. Wang"], "venue": "Acta Automatica Sinica, vol. 34, no. 11, pp. 1396-1403, Nov. 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "An Introduction to ROC Analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters, vol. 27, pp. 861-874, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust Classification for Imprecise Environments", "author": ["F. Provost", "T. Fawcett"], "venue": "Machine Learning, vol. 42, no. 3, pp. 203-231, 2001.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Rejection Strategies 15 and Confidence Measures for a k-NN Classifier in an OCR Task", "author": ["J. Arlandis", "J.C. Perez-Cortes", "J. Cano"], "venue": "Proc. Int\u2019l Conf. Pattern Recognition, pp. 576-579, 2002.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2002}, {"title": "Restricted Bayes Optimal Classifiers", "author": ["S. Tong"], "venue": "Proc. AAAI National Conf. Artificial Intelligence, pp. 658-664, 2000.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION IMBALANCED data sets [1], [2] arise frequently in a variety of real-world applications, such as medicine, biology, finance, and computer vision.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "1 INTRODUCTION IMBALANCED data sets [1], [2] arise frequently in a variety of real-world applications, such as medicine, biology, finance, and computer vision.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 5, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 220, "endOffset": 223}, {"referenceID": 6, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 258, "endOffset": 261}, {"referenceID": 7, "context": "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].", "startOffset": 263, "endOffset": 266}, {"referenceID": 8, "context": "A comprehensive review of learning in the class imbalance problem is provided by He and Garcia [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "When there exist some uncertainties in the decision, it may be better to apply abstaining classification [10] to", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Significant benefits have been obtained from abstaining classification, particularly in very critical applications [11], [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 11, "context": "Significant benefits have been obtained from abstaining classification, particularly in very critical applications [11], [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "The possibility of designing loss functions for classifiers with a reject option is also explored [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "Based on the definition in [17], we extend it below by including the situation of abstaining.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We extend Hu\u2019s [18] study on mutual information classifiers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "When costs are unequal and unknown, Maloof [20] uses ROC curve to show the performance of binary classifications under different cost settings.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "Cost curve [21], [14] can be used to visualize optimal expected costs over a range of cost settings, but it does not suit multi-class problem.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Cost curve [21], [14] can be used to visualize optimal expected costs over a range of cost settings, but it does not suit multi-class problem.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Zadrozny and Elkan [22] apply least-squares multiple linear regression to estimate the costs.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Cross validation [23] is proposed to choose from a limited set of cost values, and the final decisions are made by users.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.", "startOffset": 113, "endOffset": 117}, {"referenceID": 28, "context": "Besides, ensemble learning methods [30], [31] are used to improve the generalization of predicting the minority class.", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "Besides, ensemble learning methods [30], [31] are used to improve the generalization of predicting the minority class.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "The recognitionbased methods [32], [33] that train on a single class are proposed as alternatives to the discrimination-based methods to avoid the influence of imbalanced distributions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "The recognitionbased methods [32], [33] that train on a single class are proposed as alternatives to the discrimination-based methods to avoid the influence of imbalanced distributions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 32, "context": "[34] can get costs for multi-class data sets through maximizing the geometric mean (Gmean) or F-measure which has the ability of balancing the performance of each class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Pietraszek [35] proposes a bounded-abstaintion model with ROC analysis, and Fumera et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "[36] seek to maximize accuracy while keeping the reject rate below a given value.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Li and Sethi [37] restrict the maximum error rate of each class, but the rates may conflict when they are arbitrarily given.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Note that NI(T, Y ) is in the range [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 36, "context": "[38] apply empirical estimations to compute NI based on the confusion matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "To avoid unchanged value of NI if rejections are made within only one class, the formula of NI is proposed as [38]", "startOffset": 110, "endOffset": 114}, {"referenceID": 38, "context": "[40] present a schematic diagram of information theory learning (ITL) and they mention that maximizing mutual information as the target function makes the decision outputs correlate with the targets as much as possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Recently, a study [18] confirms that ITL opens a new perspective for classifier design.", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "MacKay [39] recommends mutual information for its single rankable value which makes more sense than error rate.", "startOffset": 7, "endOffset": 11}, {"referenceID": 36, "context": "[38] study theoretically for the first time on both error types and reject types in binary classifications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "We apply a general optimization algorithm called \u201cPowell Algorithm\u201d which is a direct method for nonlinear optimization without calculating the derivatives [41].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "[14] derive normalized cost matrix based on the overall risk which is written as", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "By applying the way of transforming costs [14], we find that the normalization way for the overall risk is also applicable for the conditional risk.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "The relation between the decision thresholds and the costs has been derived by Elkan [7] through minimizing the conditional risk.", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "In abstaining binary classification, the relations between the rejection thresholds and the costs can be presented in a form of explicit formulae [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Based on [18], one can have the relations \u03bb\u0304TN < \u03bb\u0304RN < \u03bb\u0304FP and \u03bb\u0304TP < \u03bb\u0304RP < \u03bb\u0304FN .", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "In binary classification, an ROC curve plot presents complete information about the performance of each class [42], so that an overall performance measure [43], such as AUC, can be formed.", "startOffset": 110, "endOffset": 114}, {"referenceID": 41, "context": "In binary classification, an ROC curve plot presents complete information about the performance of each class [42], so that an overall performance measure [43], such as AUC, can be formed.", "startOffset": 155, "endOffset": 159}, {"referenceID": 42, "context": "This is a preferred feature in processing class imbalance problems [44].", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "For a theoretical ROC curve which is concave, the decision is made by K, the slope of ROC curve, in the form of [45]:", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "However, the situation in (18) can never appear from using the present strategy, because it will result in a zero value of mutual information [39], [43].", "startOffset": 142, "endOffset": 146}, {"referenceID": 41, "context": "However, the situation in (18) can never appear from using the present strategy, because it will result in a zero value of mutual information [39], [43].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "Two abstaining slopes, KN and KP , are generally given in the forms of [15]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "(19) confirms the finding in [18] that at most two independent parameters will determine the rejection range in binary classifications.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "There exist relations between rejection thresholds in the posterior curve plot [18] and abstaining slopes in the ROC curve plot.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Their relations and the associated constraint are derived from [18]:", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "Most of the data sets are obtained from the UCI Machine Learning Repository, Ism is from [25], Rooftop is from [20], and Phoneme is from KEEL Datasets.", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "Most of the data sets are obtained from the UCI Machine Learning Repository, Ism is from [25], Rooftop is from [20], and Phoneme is from KEEL Datasets.", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "All of them have continuous attributes and are rescaled to be in the range [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 9, "context": "To illustrate the effectiveness of our strategy, we adopt kNN and Bayes classifier as the conventional classifiers, and we compare our methods with SMOTE, Cost-sensitive learning, Chow\u2019s reject [10] methods and the G-mean based methods (\u201cGmean no rej\u201d and \u201cGmean rej\u201d) besides two conventional classifications.", "startOffset": 194, "endOffset": 198}, {"referenceID": 44, "context": "In kNN classifier, we apply Euclidean distance and use the confidence values [47], [19] as the probabilistic", "startOffset": 77, "endOffset": 81}, {"referenceID": 45, "context": "In Bayes classifier, we derive the estimated class-conditional density from the Parzen-window estimation with Gaussian kernel [48] and apply Bayes rule to classification.", "startOffset": 126, "endOffset": 130}, {"referenceID": 42, "context": "3 shows the ROC convex hull (ROCCH) of kNN for Diabetes generated from 90 validation sets by threshold averaging [44].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "And we use them to approximatively locate the parameters [15].", "startOffset": 57, "endOffset": 61}], "year": 2013, "abstractText": "In this work, we define cost-free learning (CFL) formally in comparison with cost-sensitive learning (CSL). The main difference between them is that a CFL approach seeks optimal classification results without requiring any cost information, even in the class imbalance problem. In fact, several CFL approaches exist in the related studies, such as sampling and some criteria-based approaches. However, to our best knowledge, none of the existing CFL and CSL approaches are able to process the abstaining classifications properly when no information is given about errors and rejects. Based on information theory, we propose a novel CFL which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers. Using the strategy, we can deal with binary/multi-class classifications with/without abstaining. Significant features are observed from the new strategy. While the degree of class imbalance is changing, the proposed strategy is able to balance the errors and rejects accordingly and automatically. Another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the \u201cequivalent\u201d costs in binary classifications. The connection between rejection thresholds and ROC curve is explored. Empirical investigation is made on several benchmark data sets in comparison with other existing approaches. The classification results demonstrate a promising perspective of the strategy in machine learning.", "creator": "LaTeX with hyperref package"}}}