{"id": "1307.5736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2013", "title": "Speaker Independent Continuous Speech to Text Converter for Mobile Application", "abstract": "An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system, VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.", "histories": [["v1", "Fri, 19 Jul 2013 05:27:46 GMT  (782kb)", "http://arxiv.org/abs/1307.5736v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE cs.SD", "authors": ["r sandanalakshmi", "p abinaya viji", "m kiruthiga", "m manjari", "m sharina"], "accepted": false, "id": "1307.5736"}, "pdf": {"name": "1307.5736.pdf", "metadata": {"source": "CRF", "title": "Speaker Independent Continuous Speech to Text Converter for Mobile Application", "authors": [], "emails": ["*sandanalakshmi@pec.edu"], "sections": [{"heading": null, "text": "This year, it is more than ever in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2. PREPROCESSING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 SPEECH ACQUISITION", "text": "An acoustic voice signal exists as pressure fluctuations in the air. A microphone converts these pressure fluctuations into electrical current. In order to process the voice signal digitally, it is necessary to make the analog waveform discrete in both time (sample) and amplitude (quantity). This A-to-D conversion is usually done by digital signal processing hardware on the sound card of the computer. The spoken speech frequency is between 300 and 3400 Hertz. Therefore, a sampling frequency of 8 KHz is chosen to meet the Nyquist criterion. The voice signal is recorded at a sampling rate of 8 KHz. And the data is stored with a unique name as. \"wav\" as an extension."}, {"heading": "2.2 VOICE ACTIVITY DETECTION", "text": "In general, the acquired voice signal of one second duration (8000 samples) begins and ends with silence, which amounts to almost 6000 samples. VAD can be performed by two algorithms: the first algorithm uses signal characteristics based on energy level, and the second algorithm uses signal characteristics based on the rate of zero intersections, the combination of which results in a good result, which is used in the proposed system. A basic VAD works on the principle of extracting measured characteristics from the incoming audio signal, which is divided into frame size of 150 ms duration and frame size of 40 ms duration. The extracted signal characteristics are based on energy level and zero intersection rate from the audio signal, are then compared with a threshold and then calculated VAD decision. STEP 1: If the input image characteristic exceeds the estimated threshold, a VAD decision (VAD = 1) is calculated, which declares that language is predetermined."}, {"heading": "2.3 PRE-EMPHASIS", "text": "In order to flatten the speech spectrum, a preconcreting filter is used before the spectral analysis, the aim of which is to compensate for the high frequency part of the speech signal suppressed during the human sound production mechanism. Thus, high frequency signals have a lower amplitude, which leads to a negative spectral slope and is compensated by corresponding preconcreting filters.The ztransformation of the filter results from [2]: - (1) where \"a\" is the filter coefficient and is selected as 0.9375.The output signal after the preconcreting is derived from [2]: - (2) (2.2) (2.1) Where y (n) - output signal s (n) - input signal for speech"}, {"heading": "2.4 FRAMING", "text": "Language is a highly non-stationary signal. From now on, voice analysis must be carried out on short segments over which the voice signal is assumed to be stationary. For this purpose, the voice signal is divided into frames of short duration, typically 20 to 40ms with overlaps of 10 to 15ms for short-term spectral analysis."}, {"heading": "2.5 WINDOWING", "text": "Windows minimize discontinuities by reducing the signals to relatively small values (almost zeros) at the edges of a frame. Hammering window is also referred to as a raised cosine window. Hammering window is defined as [3] - (3) The output voice signal can be multiplied as (4) Any frame s (n) by hammering window w (n) of 25ms duration. 3. FEATURE EXTRACTIONFeature Extraction is the process of distinguishing features from the input signal. It is about reducing the dimensionality of the input vector while maintaining the uniqueness of the signal. Good features have the attributes of being easy to extract, invariant to time ratios and general amplitude variations, and being able to distinguish different phonetic categories. Features are extracted using the MFCC technique [4]."}, {"heading": "3.1 MEL-FREQUENCY CEPSTRAL COEFFICIENT ALGORITHM", "text": "MFCCs are usually derived as follows: on short time scales, the audio signal does not change much; the chosen frame time is 20-40ms frames; the power spectrum of each frame is routed through the FFT block. FFT is used to identify the frequencies present in the frame; logarithmic compression is done tomatch features closer to what humans actually hear; the periogram spectral estimate still contains a lot of information that is not required for automatic speech recognition (ASR); for this reason, we take clumps of periodogram bins and add them up to get an idea of how much energy is available in different frequency ranges; this is done by our Mel filter bank. The Mel scale tells us exactly how to place our filter banks and how wide they should be. Mel's inverse fast Fourier transformation of log power gives the MFCC coefficients."}, {"heading": "4. SPEECH RECOGNIZER USING NEURAL NETWORK", "text": "A neural network is a computer program designed to model the way the brain performs a particular task or function of interest; the network is usually implemented by electronic components or simulated in software on a digital computer."}, {"heading": "4.1 PROPOSED NN RECOGNIZER", "text": "The object of speech regression analysis is to tomograph a detector by providing a finite series of observations of the language and its associated target values, so that the most likely output value for an unknown language input is given. Regression equation can be expressed as [9] (5) (6), where wij is the target output corresponding to the input training vector xi and the input output. The pattern layer has one neuron for each pattern layer. During training, all variations of input vectors and the associated desired output or target (wij) are made available to the network. The neuron in the pattern layer passes the input vector to each neuron in the pattern layer. The neuron in the pattern layer calculates Hi using Ci and Xi as shown below. (7) (8) The neuron in the pattern layer calculates the neuron unit for the read function D."}, {"heading": "5. DATABASE CREATION", "text": "To facilitate training and testing of the recognition device, a speech database is required. Different language samples were obtained from different speakers to form the speech database. Three types of text recognition databases with numerical names Transcription of Title5,1 Digit Note Recognition were created: The numerical database consists of ten expressions from \"zero\" to \"nine,\" which are collected from both male and female speakers. The database is divided into Training and Test. The training set is used to train the neural network. The test set is used to test the performance of neural networks. Excellent recognition accuracy is achieved for speaker-dependent recognition, which is closer to 100%. This recognition device can be adapted very well for language selection."}, {"heading": "6. RESULTS AND DISCUSSION", "text": "6.1 Syllable Level Word Recognition: To perform a syllenbased implementation of the word cognition, a separate database was created with voice samples of 2 female speakers. 36 syllables, namely \"be,\" com, \"in\" out, \"\" ply, \"\" pose, \"\" press, \"\" side, \"\" sup, \"in\" vi, \"\" ta, \"\" ti, \"ma,\" \"no,\" re, po, \"si,\" lo, \"de,\" di, \"\" di, \"\" \",\" ca, \"car,\" na, \"mo,\" go, \"go,\" go, \"\" lo, \"la,\" ra, \"\" ing, \"re,\" va, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \"do,\" do, \""}, {"heading": "7. SUMMARY AND CONCLUSION", "text": "The proposed model has several advantageous features such as fast learning, flexible network size and robustness over speaker variability (ability to recognize the same words pronounced in different manners).GRNN promises to be a successful and powerful alternative to traditional speech recognition systems.The designed recognition system with all the above features has been used to develop a number recognition system, a syllable recognition system and a transcription system.Digital recognition can find applications in speech dialogue systems.And it must be a speaker-independent system. In the syllable recognition system, the recognition mechanism has been trained on syllables, which have been separately recognized and linked to output the full word as text. In the transcription system, the duration of an input speech signal is extended and the corresponding text output calculated."}], "references": [{"title": "A simple but efficient real-time Voice Activity Detection algorithm", "author": ["M.H. Moattar", "M.M. Homayounpour"], "venue": "European Signal Processing Conference (EUSIPCO),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Enhanced Voice Activity Detection Using Acoustic Event Detection", "author": ["Namgook Cho", "Eun-Kyoung Kim"], "venue": "And Classification\u201d,IEEE Transactions On Consumer Electronics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Chiu-Sing CHOY and Kong-Pang PUN, \u201cAn Efficient MFCC Extraction Method in Speech Recognition", "author": ["Wei HAN", "Cheong-Fat CHAN"], "venue": "IEEE International Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Mahesh Chandra", "author": ["A.N. Mishra"], "venue": "Astik Biswas, S. N. Sharan , \u201cRobust Features for Connected Hindi Digits Recognition\u201d, International Journal of Signal Processing, Image Processing and Pattern Recognition ,Vol. 4, No. 2, June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Digit Recognition Using Neura l Networks", "author": ["Chin Luh Tan", "Adznan Jantan"], "venue": "Malaysian Journal Of Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Neural Networks Used For Speech Recognition", "author": ["Wouter Gevaert", "Georgi Tsenov", "Valeri Mladenov"], "venue": "Journal Of Automatic Control, University of Belgrade,Vol.20,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "But at the same time, the VAD algorithm [2] should be of low complex to facilitate real time application.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "This demands to use a VAD that also calculates energy of high frequency part separately as ZCR to differentiate noise from speech [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "MFCC [4] overcomes this limitation.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "The usability of neural network for speech recognizer proved to be a suitable technology [6] and it outperforms traditional recognizers like Hidden Markov Model (HMM), Dynamic Time Warping (DTW), Vector Quantization (VQ) etc.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Moreover RBF gives more accurate result than MLP [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The ztransform of the filter is given by [2]:", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "9375 The output signal after pre-emphasis is given by [2]:", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "The Hamming window defined as [3]", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "Features are extracted using MFCC technique [4].", "startOffset": 44, "endOffset": 47}], "year": 2013, "abstractText": "An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system , VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.", "creator": "Microsoft\u00ae Office Word 2007"}}}