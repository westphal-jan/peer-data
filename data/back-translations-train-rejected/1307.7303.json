{"id": "1307.7303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2013", "title": "Learning to Understand by Evolving Theories", "abstract": "In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition.", "histories": [["v1", "Sat, 27 Jul 2013 20:33:34 GMT  (172kb,D)", "http://arxiv.org/abs/1307.7303v1", "KRR Workshop at ICLP 2013"]], "COMMENTS": "KRR Workshop at ICLP 2013", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["martin e mueller", "madhura d thosar"], "accepted": false, "id": "1307.7303"}, "pdf": {"name": "1307.7303.pdf", "metadata": {"source": "CRF", "title": "Learning to Understand by Evolving Theories", "authors": ["Martin E. M\u00fcller", "Madhura D. Thosar"], "emails": ["m.e.mueller@acm.org,", "madhura.thosar@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "\"Semantics\" usually refers to the meaning of something, but since there is no such thing as understanding in a computer, there is no semantics either. Nevertheless, a robot (causes events) acts and senses the environment (identifies things), but thinking about actions requires language, and therefore semantics. There are several main inspirations for our work: firstly, John Locke's concept of learning from the ground up [1] and Bertrand Russell's \"Theory of Knowledge.\" [2] [3] It also includes several theories of cognition related to learning the semantics of an action. Since a model is defined as a formal description of the observable effects of an action, we add to the model the fundamental problem of identifying a change in the environment as an effect of an action. Both the environment and innate knowledge are presented in relations as logical programs, so that we can describe the formation of more complex models as a learning process similar to the inductive logic of programming [6]."}, {"heading": "2 Learning to Understand Actions", "text": "Understanding and learning require knowledge. In this article, we assume that an agent has a minimum of \"knowledge,\" as described in a given set of terminological formulas that describe facts, properties, and rules that apply within the agent's world model."}, {"heading": "2.1 Actions as state changes", "text": "Since we do not know the world itself, but only a model of it, we have to live with interpretations of the partial data collected by sensors. < < < < < \"Targeted Action\" (Planned, Rational Xiv: 130 7.73 03v1 [cs.LG] 2 7Ju l 201 3behavior) therefore requires anticipating the results of an action, and we do so by assuming that when we do something, the real effects put us in a situation where our sensors produce results that correspond to what we wanted to achieve: wi action / / \u03b1iwi + 1 \u03b1ii + 1\u03b4lti operator / / / \u03b4i + 1 (1) All \u03b4i are state descriptions consisting of sets of logical formulas (in our case, horn clauses) via a signature. \"The \u03b1i represent the interpretation of sensory data and thus the mapping of the variables in \u03b4i. & lti & ti & ti\" An operator & ti function is a function op, and thus we modify the situation < < < we take < < the condition < &lti."}, {"heading": "2.2 Representing States", "text": "We assume that the world is represented at a point t in time by a snapshot of the state. Snapshots of the state are essentially total functions \u03b1t, which assign values corresponding to a set of variables representing certain sensory inputs. The set of variables is Var = VarO-VarI with VarO = {X-i-nO} as a set of (global) observable variables and VarI = {Xj: j-nI} as a set of (local) internal variables. In addition, each variable is assigned a type to X-Var. A status snapshot st at the time t is defined as a sequence test: = [< X, \u03b1t (X) >: X-Var]."}, {"heading": "2.3 Bootstrapping initial theories", "text": "Symbol degradation or lexical semantics does not work until a set of general, atomic symbols of meaning can be agreed upon from which to construct larger ones. Whatever we perceive, we must be able to put it into words in order to think about it. And what we will learn can only be learned if we have a sufficient repository of basic categories from which to build a desired concept. Our initial knowledge consists of a type system and a set of primitive typical relations and (arithmetic) operations. It contains the definitions of relations such as greater than, smaller than; arithmetic relations such as addition, subtraction, multiplication, division; spatial relations such as position, orientation, proximity; and predicates of logical expressions including equality. The type system can be easily adapted to the domain: Example; arithmetic relations such as addition, subtraction, multiplication, division; spatial relations such as position, orientation, predicate, including equality."}, {"heading": "2.4 Actions", "text": "An action is something a robot does. Normally, each action is the result of a command and causes several effects (hopefully all and only desired). A command is the instantiation of a robot (c.f. STRIPS, [13]): MOVE obj: Obj, num: x PRE {at (Obj, pos: X, Y) EFF [Power (0,8); waiting time (c \u00b7 x); performance (0)] DEL {at (Obj, pos: X, Y)} ADD {at (Obj, pos: X, Y, Y, Y cx cos Z)} You could say that the semantics of motion is quite simple: it is the program as it is defined in the EFF slot of the operator definition, but our goal is to explain what it means to do EFF."}, {"heading": "2.5 Learning by Observing", "text": "If we want a robot to learn the semantics of its actions, we reverse the line of argument: MOVE means that certain variables change in some way. In general, this leads to two learning problems: 1. What is the (smallest) subset V Var, which contains all the relevant variables to describe the effects of an action? 2. Once we know where to look for changes, what is the underlying system in the way the values change? The first question seems to be simple to answer: first, the meaning of an action could be precisely to maintain a certain value rather than change it, and second, there are many other reasons why a variable could be changed without any connection with the execution of the action."}, {"heading": "2.6 Related work", "text": "DIDO, [19] explores unfamiliar areas without external supervision. The task is to build a representation of the domain that can be used to predict the results of their motor operations with maximum probability of accuracy. Its2 Note that one is working on state descriptions in this paradigm. In a closed, deterministic and fully observed world, such a description (is assumed) corresponds to reality. In the real world, however, situations can change without changing their description and there could be a different description for the same world. 3 A first step towards dealing with asynchronous processes in the context of learning has been discussed in [14]. Key features - unattended inductive learning of actions and state descriptions without prior knowledge - are the basic principles on which EVOLT is based, with the main difference between the proposed system and DIDO being that EVOLT learns through the observation of the environment."}, {"heading": "3 EVOLT: Evolving Theories", "text": "Learning the meaning of an action in EVOLT involves three stages of the following steps: 41. Identifying relevant observable variables. 2. Introduce a new hypothesis describing the action performed. 3. Refine existing hypotheses. In this first study, and in order to do justice to the enormous amount of flat data, we have focused on one aspect of the ILP and expanded the generalization process to deal with typewritten terms. Furthermore, simple data collections leave us with only positive examples. Finally, the sentence head of the target predicate is sub-specified in terms of mode description, for example, used in such a way [24] that no previous candidates are available for the sentence head sought. EVOLT exhaustively searches for the relationships in which each kinship definition is scanned in background knowledge to verify whether it entails changes in the state variables caused by an action. \"The increase in the search effort due to the brute force type 2 is enabled by our use of the efficiency of the variable, however."}, {"heading": "3.1 Evolving theories by stepwise refinement", "text": "We will explain this by the following example: Each of us represents a value associated with a variable. (Suppose each of us would perform an action and the immediate preceding and subsequent states are st \u2212 1 and st + 1, we define an effect and an ineffective setEff \u2212 1). (Suppose each of us has a command to perform an action (\"EFF\"). (7) Eff t: = VarO \u2212 Eff t (8) or (that is, between t \u2212 1 and t + 1) a command structure that performs all effects (\"EFF\") came into effect. It was also assumed that the action of parameters was called a0., ak \u2212 1. We therefore expect that actt (a0, ak \u2212 1) causes some change. (a0, ak \u2212 1)."}, {"heading": "3.2 Example", "text": "In fact, it is as if most people who are able to live and work in the USA are not able to stay in the USA. (...) It is not as if they were able to stay in the USA. (...) It is as if they were staying in the USA. (...) It is as if they were staying in the USA. (...) It is as if they were staying in the USA. (...) It is as if they were staying in the United States. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (). (). (). (.). (). (). (). ().). (. (). (. (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ("}, {"heading": "3.3 Results", "text": "The primary objective of the experimental evaluation was to investigate the applicability and behavior of EVOLT in different environment settings. The example of such an environment setting is given below. The data (state and action correlations) were collected as follows: A robot was controlled to move in a simulated environment (using openRAVE).We captured all state descriptions (by omitting the variables of the simulation system and presenting them as facts.In our case, the position is expressed using a three-dimensional coordinate system (adding the height to our previous code examples).We tried to learn semantic descriptions of several primitive actions that move forward, left / right, grab / drop. In our case, the position is expressed using a three-dimensional coordinate system (adding the height to our previous code examples).We tried to learn semantic descriptions of several primitive actions that move forward, left / right, grab / drop. In our case, the position is expressed using a three-dimensional coordinate system (adding the height to our previous code examples).We tried to learn semantic descriptions of several primitive actions that move forward, left / right, grab / drop. In our case, the position is expressed using a three-dimensional coordinate system (adding the height to our previous code examples).We tried to learn semantic descriptions of several primitive actions that move forward, left / right, grab / right, grab / fall."}, {"heading": "4 Conclusion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Summary", "text": "We have proposed an approach to learn the semantics of actions by describing the observed effects in terms of atomic arithmetic / logical operations. An action is considered to be an event that causes the current state to change, with the state being a series of characteristics that represent sensory inputs. EVOLT's working principles are inspired by the paradigm of the evolution of theories, while learning itself is implemented using principles of inductive logic programming. EVOLT uses an exhaustive or crude search to search the relationships in which the search is guided by the search bias in the form of typical variables. Another contribution is that our systems, motivated by their use in robotics, work on unmarked examples. In addition, EVOLT has a minimum of background knowledge, which is \"general knowledge,\" in the sense that there are no task-specific and only atomic operations on these sensory data. Moreover, EVOLT has a minimum of background knowledge, which is \"general knowledge,\" in which means that it does not result in any task-specific and only atomic operations on these sensory data being accepted, although the evaluation of the system is quite reasonable, for example, the evaluation of which is still in its results."}, {"heading": "4.2 Prospects", "text": "Our immediate future goal is to make the system more complex by improving its data processing and adding additional definitions of more complex relationships to the background knowledge. Further possibilities for improvement arise through a type system that allows for the typing of complex terms - adding timestamps to the action theory facts for sequence learning - building transitional sentences on an entire level of abstraction by combining several observable values - a more complex refinement of theory that goes beyond a mere collection of common phenomena of variable mapping changes - with the increasing complexity of the search space we also need to introduce more complex biases - also reducing the dimensionality of the hypotheses space (as determined by the number of observable variables), while at the same time enabling the system to present a 2.5] larger number of characteristics of method selection with the help of a context."}], "references": [{"title": "An essay concerning human understanding", "author": ["J. Locke"], "venue": "The Pennsylvania State University", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1690}, {"title": "Theory of Knowledge: The 1913 Manuscript", "author": ["B.A.W. Russell"], "venue": "Routledge", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Being aware: Where we think the Action is", "author": ["M.E. M\u00fcller"], "venue": "Cognition, Technology, and Work 9(2)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Xpero - learning by experimentation (d1.6)", "author": ["B. Kahl", "T. Henne", "U. K\u00f6ckemann", "C. Shahazad", "E. Prassler"], "venue": "Workpackage final report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Towards iterative learning of autonomous robots using ilp", "author": ["N. Akhtar", "M. Fueller", "B. Kahl", "T. Henne"], "venue": "15th Intl. Conf. on Advanced Robotics (ICAR).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Relational Knowledge Discovery", "author": ["M.E. M\u00fcller"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Programs with commonsense", "author": ["J. McCarthy"], "venue": "Proceedings of the Teddington Conference on the Mechanization of Thought Processes. Her Majesty\u2019s Stationery Office, London", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1959}, {"title": "Making robots conscious", "author": ["J. McCarthy"], "venue": "In Furukawa, K., Michie, D., Muggleton, S., eds.: Machine Intelligence 15: Intelligent Agents. Oxford University Press, Oxford", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "On sentences which are true of direct unions of algebras", "author": ["A. Horn"], "venue": "Journal of Symbolic Logic 16(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1951}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["J. McCarthy", "P.J. Hayes"], "venue": "Machine Intelligence 4", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1969}, {"title": "Tree of Knowledge: Biological Roots of Human Understanding", "author": ["H.R. Maturana", "F.J. Varela"], "venue": "Shambala", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "Vehicles: Experiments in Synthetic Psychology", "author": ["V. Braitenberg"], "venue": "The MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Strips: A new approach to the application of theorem proving to problem solving", "author": ["N.J. Nilsson", "R.E. Fikes"], "venue": "Technical Report 43, Stanford Research Institute, SRI, Menlo Park, CA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1970}, {"title": "Relational Cognitive Structures for Intelligent Agent and Robot Control", "author": ["M.E. M\u00fcller", "F. Krebs", "F. Hielscher"], "venue": "Systems, Man, and Cybernetics (SMC-2008), IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Induction of Horn-clauses: methods and the plausible generalization algorithm", "author": ["W. Buntine"], "venue": "26", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Towards constructive induction in first-order predicate calculus", "author": ["S. Muggleton", "W. Buntine"], "venue": "TIRM 88-03, The Turing Institute, Glasgow", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1988}, {"title": "Logical and Relational Learning", "author": ["L.D. Raedt"], "venue": "Cognitive Technologies. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning novel domains through curiosity and conjecture", "author": ["P.D. Scott", "S. Markovitch"], "venue": "In Proceedings of International Joint Conference for Artificial Intelligence, Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Complementary discrimination learning with decision lists", "author": ["W.M. Shen"], "venue": "In Proceedings Tenth National Conference on Artificial Intelligence (pp. 153-158). Menlo Park, Ca, AAAI Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Discovery as autonomous learning from the environment", "author": ["W.M. Shen"], "venue": "Machine Learning, Computer Science Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "An experiment in robot discovery with ilp", "author": ["G. Leban", "J. \u017dabkar", "I. Bratko"], "venue": "Proceedings of the 18th international conference on Inductive Logic Programming. ILP \u201908, Berlin, Heidelberg, Springer-Verlag", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "The Aleph Manual", "author": ["A. Srinivasan"], "venue": "University of Oxford.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Inverting entailment and Progol", "author": ["S. Muggleton"], "venue": "In Furukawa, K., Michie, D., Muggleton, S., eds.: Machine Intelligence 14. Oxford University Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "There are several prime inspirations for our work: First, John Locke\u2019s concept of learning from scratch, [1], and Bertrand Russell\u2019s \u201cTheory of knowledge\u201d, [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "There are several prime inspirations for our work: First, John Locke\u2019s concept of learning from scratch, [1], and Bertrand Russell\u2019s \u201cTheory of knowledge\u201d, [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "[3] also summarises several theories of cognition related to learning the semantics of an action.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Both the environment and the innate knowledge is represented relationally as logic programs so we can describe the formation of more complex models as a learning process similar to inductive logic programming, [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 9, "context": "the frame problem and determinism, [10]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "STRIPS, [13]): \uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 MOVE obj : Obj , num : x PRE { at(Obj , pos : \u3008 \u1e8a, \u1e8e \u3009 ) }", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 15, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 16, "context": "4) and Horn logic as representation formalisms, Inductive Logic Programming (ILP), [15,16,17,18], is a prime candidate for learning the semantics of action.", "startOffset": 83, "endOffset": 96}, {"referenceID": 17, "context": "DIDO, [19], explores unfamiliar domains without any external supervision.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "3 A first step towards dealing with state snapshots of asynchronous processes in the context of learning was discussed in [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "LIVE, [20,21], uses complementary discrimination learning which is inspired by Piaget\u2019s child development theories to learn the prediction models of effects of an action.", "startOffset": 6, "endOffset": 13}, {"referenceID": 19, "context": "LIVE, [20,21], uses complementary discrimination learning which is inspired by Piaget\u2019s child development theories to learn the prediction models of effects of an action.", "startOffset": 6, "endOffset": 13}, {"referenceID": 20, "context": "Like DIDO, HYPER [22] shall learn by exploring the domain.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "HYPER uses standard ILP (Aleph, [23]) to induce knowledge about movement (movability, obstacle identification, DoF).", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "In the course of the XPERO project [4] the same ILP learner was used.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "[5] focus on generating negative examples by introducing a bias that determines feature value ranges.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "used [24] such that there are no prior candidates for the sought clause head available.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "state spec(31, [ [r pos, [9, 14]:pos], [obj num, 1:num], [obj grab, [none]:obj, 0:truthVal], [obj pos, [obst]:obj, [13,3]:pos] ]).", "startOffset": 115, "endOffset": 121}, {"referenceID": 12, "context": "This allows to assign complex values to a variable: [obj pos, [obst]: obj, [13, 3]: pos] means that  \u0307 Obj Loc is of a type obj\u00d7pos.", "startOffset": 75, "endOffset": 82}, {"referenceID": 2, "context": "This allows to assign complex values to a variable: [obj pos, [obst]: obj, [13, 3]: pos] means that  \u0307 Obj Loc is of a type obj\u00d7pos.", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "The idea is that, based on the state description for t = 31 and an according state description at t = 33 including [r pos, [9, 20]:pos]], and with background knowledge", "startOffset": 123, "endOffset": 130}, {"referenceID": 18, "context": "The idea is that, based on the state description for t = 31 and an according state description at t = 33 including [r pos, [9, 20]:pos]], and with background knowledge", "startOffset": 123, "endOffset": 130}, {"referenceID": 5, "context": "One method for a feature selection procedure in the context of relational representation of data is rough set data analysis, [6]; see also sections 2.", "startOffset": 125, "endOffset": 128}], "year": 2013, "abstractText": "In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition.", "creator": "LaTeX with hyperref package"}}}