{"id": "1307.7382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jul-2013", "title": "Learning Frames from Text with an Unsupervised Latent Variable Model", "abstract": "We develop a probabilistic latent-variable model to discover semantic frames---types of events and their participants---from corpora. We present a Dirichlet-multinomial model in which frames are latent categories that explain the linking of verb-subject-object triples, given document-level sparsity. We analyze what the model learns, and compare it to FrameNet, noting it learns some novel and interesting frames. This document also contains a discussion of inference issues, including concentration parameter learning; and a small-scale error analysis of syntactic parsing accuracy.", "histories": [["v1", "Sun, 28 Jul 2013 16:55:27 GMT  (5563kb,D)", "http://arxiv.org/abs/1307.7382v1", "21 pages; technical report for Data Analysis Project requirement, Machine Learning Department, Carnegie Mellon University"]], "COMMENTS": "21 pages; technical report for Data Analysis Project requirement, Machine Learning Department, Carnegie Mellon University", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["brendan o'connor"], "accepted": false, "id": "1307.7382"}, "pdf": {"name": "1307.7382.pdf", "metadata": {"source": "CRF", "title": "Learning Frames from Text with an Unsupervised Latent Variable Model", "authors": ["Brendan O\u2019Connor"], "emails": [], "sections": [{"heading": null, "text": "Note: This paper was originally published in October 2012 as part of the CMU MLD Data Analysis Project Requirements. This version does not contain any new experiments or results, but has added some discussions of new related work."}, {"heading": "1 Introduction", "text": "Semantic frames - types of events and their participants - are a key development in linguistic theories of semantics (Fillmore, 1982) and, sometimes referred to as \"scripts\" or \"schemas,\" have become strongly established in research into the understanding of natural languages (Schank and Abelson, 1977; Lehnert and Ringle, 1982). Recently, interest has increased in the search for frames in text data, including frame-semantic parsing (Das et al., 2010; Gildea and Jurafsky, 2002) following the conventions of FrameNet (Fillmore and Baker, 2001) and in the discovery of narrative structures (Chambers and Jurafsky, 2009). In this paper, we attempt to discover semantic frames - types of events or relationships and their participants - using probability variable models. This approach focuses on verbs with their subjects and objects and is inspired by models driven by models of selective preference and argument structure."}, {"heading": "2 Models", "text": "Verbs, subjects and objects form a basic syntactical encoding of actions, events and their participants. We are interested in modeling a dataset of document VSO tuples (DocID, w (verb), w (subj), w (obj)). We present two models for capturing documentary and syntactical context information in text generation."}, {"heading": "2.1 \u201cModel 0\u201d: Independent tuples", "text": "Previous work in model-based syntactic distribution clusters, which usually aim at modeling selection preferences, has modelled syntactic tuples as independent (or rather conditionally independent given the model parameters). (Rooth et al. (1993) and Rooth et al. (1999) model a corpus of (verb, subject) pairs with a latent variable for each tupel and different word distributions for each argument and class. (Rooth et al. experiment with different syntactic relationships, but always use pairs; e.g. (verb, subject).) To situate our model, we slightly generalize these approaches (verb, subject, object) triples and add symmetrical dirichlet priors as follows. (arg) f denotes a word multinomially for argument type arg and frame ifi; and there are three argument types (verb, subject, object) that are treated separately as dirichles \u2022."}, {"heading": "2.2 Model 1: Document-tuples", "text": "Following the intuition of latent dirichlet allocation (Blei et al., 2003) - that each individual document tends to use a small subset of available latent semantic factors (\"topics\") - we propose that the frames of a document are similarly derived from a sparse-inducing dirichlet procedure. Our document tuple model uses the same frame lexicon structure as above, but enriches the document generation: \u2022 F frames and dirichlet prizes \u03b1, \u03b2 \u2022 Frame lexicon: For each frame f-1.. F, and argument positions such a frame model f-1, 2, 3}, \u2022 Draw-word multinomial structure (\u03b2) \u2022 Document tuple dimensions dimensions dimensions dimensions dimensions dimensions dimensions dimensions dimensions dimensions dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-dimensions-the intuition of latent latent latent latent sements-sements-semantic semantic semantic semantic semantic semantic semantics-semantic semantic semantic semantic semantics-semantic semantic semantic semantic semantic semantic semantic semantic semantic semantic semantic semantic seman"}, {"heading": "2.3 Model 2: Cross-cutting semantic classes", "text": "Model 1: \"There are no word statistics with each other, so each word class has to be redefined for its arguments.\" Model 2: \"There is only one model for each word class.\" (Model 2:) \"There is only one model for each word class.\" (Model 2:) \"There is only one model for each word class.\" (Model 2:) \"There is only one word for each word class.\" (Model 2:) \"There is only one model for each word class.\" (Model 2:) \"There is only one model for each word class.\" (Model 2:) \"There is only one word for each word class.\" (Model 2:) \"There is only one word for each word class.\" (Model 2: \"There is only one model for each word class.\") \"(Model 2:\" There is one model for each word class. \")\" (Model 2: \"There is only one word.\" (Model 2: \"There is only one word class.\") \"(Model 2:\" There is only one word for each word class. \")\" (Model 2: \"There is only one word for each word class.\") \"(Model 2:\" There is only one word. \"There is only one word.\" (Model 2: \"There is only one word.\")"}, {"heading": "3 Inference", "text": "The second term of the CPT factors shows the two soft constraints that the Gibbs sampler works have to satisfy: the left term p (f / d) to ensure that the document frame ensures coherence - it puts pressure on you to select a frame used elsewhere in the document; the second term p (f / fa) exerts syntactical coherence - to choose compatibility with all syntactic arguments. Thus, the Model 1 combines selective preferences with the document modeling.Model 2's Gibbs sampling equations c (f / a)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "In fact, it is as if most of them are able to deny themselves that they are not doing it. (...) It is not as if they are doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it."}, {"heading": "5 Example Results", "text": "As a matter of fact, most of them are able to survive themselves by blaming themselves and others. (...) Most of them are very well able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are able to survive themselves. (...)"}, {"heading": "6 Comparison to FrameNet", "text": "We are primarily interested in the quality of our induced frame lexicon. Evaluation is difficult; an automatic measurement, sustained probability, cannot always correlate with subjective semantic coherence (Chang et al., 2009). And while subjective coherence judgments are often collected to evaluate word clusters or argumentation compatibilities, it is unclear to us which task would support the analysis of frame quality. Our primary goal is to achieve a better understanding of what our model is and is not learning. We propose a method to compare the similarities and differences between a learned frame model and an existing lexicon."}, {"heading": "6.1 Comparing verb wordsets", "text": "How can we analyze the similarity and differences between two different word clusters? Our idea of \"clusters\" does not necessarily have to be a proper partition: it consists only of a certain number of word clusters that can overlap and 3http: / / brenocon.com / dap / materials / not necessarily cover the entire vocabulary. (These are often referred to as \"word clusters,\" but for clarification we always use \"wordset.\") Many lexical resources and unattended models can be discredited and converted into this representation. We perform a fundamental analysis and compare the verb groups that FrameNet implies to our model. Verb word groups (verbsets) are extracted from FrameNet by taking the sentences for each frame, so there are 171 verb sets. We discredit our model by analyzing, for each word class, the words that are analyzed at least 5 in the Gibbs sample."}, {"heading": "7 Note on MUC", "text": "Besides FrameNet, it might be worth making a comparison with a verb more akin to the Levin classes (1993); for example, Sun et al. (2008) and Sun and Korhonen (2009) construct a series of 204 verbs in Levin-style clusters and evaluate cluster methods against it. It would be interesting to compare the work of Chambers and Jurafsky (2011b), who trigger frames with multiple stages of ad-hoc clusters on unlabeled newswire data, and compare their learned frames with frames and extractions from MUC-4, a domain of news wire reporting on terrorist and political events. We conducted a headroom test for the extraction accuracy of MUC and found that our approach to this task is bad - looking at all role-filler instances in the text and checking how often they correlate with a nominal subject or object according to the Stanford dependency parser and our syntactical extraction rules."}, {"heading": "8 Conclusion", "text": "We have illustrated a probabilistic model that learns frames from text by combining document and syntactic contexts in a Dirichlet multinomial latent variable model. Many other extensions are possible. First, the document context could be enriched with various metadata - the context of a document in time, space, and author attributes can be easily integrated into the framework of graphical models. Second, the constraints on syntactic constructions between subject and object must be loosened to capture the types of arguments seen in semantic role designation and information extraction."}, {"heading": "Acknowledgments", "text": "Thanks to Noah Smith and Nathan Schneider for advice and help with model analysis. 4http: / / brenocon.com / muc4 _ proc / and http: / / github.com / brendano / muc4 _ proc 5https: / / docs.google.com / document / pub? id = 1erEEsWI9V0SapEecbn1AMy69Fy6TgSIdYVTsKRaF8vM"}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Dice, F-measure, and set similarity", "text": "Its harmonious mean, the F1 measure, is the same as the cube coefficient: F1 (A, B) = 21 / P + 1 / R = 2 | A | / | AB | + | B | / | AB | Cube (A, B) = 2 | AB | (A, B) = 21 / P + 1 / R = 2 | A | / | B | / | AB | + 12 | A\\ B | + 12 | B\\ A | B\\ A | | This illustrates the close relationship of the cube with the Jaccard metric Jacc (A, B) = | AB | | | A \u00b2 B | = B\\ A | = AB | | + | A\\ B | + | B\\ A | + 12 | B\\ A | J = Nitude / (2 \u2212 D) and D = 2J / J (J) = | J (A, B) = | A \u00b2 B | = positive, B | | | | AB | | | | AB | | | AB | | AB | + | A\\ B | | | | B\\ B | | | | B\\ B | | | B\\ B\\ B and B\\ B\\ B | | | | B | B\\ B\\ B\\ B\\ A\\ B and B\\ B\\ B\\ A\\ B\\ A\\ B\\ B = A = A (A, A, A, A, B, B, B, B\\ A, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B."}, {"heading": "9.2 Appendix: Dirichlet-multinomial conjugacy and the DM", "text": "D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D ="}, {"heading": "9.3 DM PMF in LDA hyperparameter sampling", "text": "Traversing the full DM is not necessary to derive the collapsed Gibbs sample equations, but it is necessary for the hyperparameter sampling, which requires the evaluation of the probability of the entire dataset under various hyperparameters \u03b1. LDA under collapse can therefore be regarded as a series of DM drawings: \u2022 For each d, the sample vector z {i: di = d} \u0445 DMPath (\u03b2) can be selected, with \"DMPath\" indicating that a random sequence is selected with the counting of a DM draw; its PMF is the DM1 function of its counter vector. (This could be calculated by progressing through a Polya urn process a.k.a. (finite) Chinese restaurant process.) Therefore, it is difficult for their Gibbs update steps to implement the hyperparameter probability: p (z | \u03b1) = DMPath (di: d}."}], "references": [{"title": "On smoothing and inference for topic", "author": ["Arthur Asuncion", "Max Welling", "Padhraic Smyth", "Yee Whye Teh"], "venue": null, "citeRegEx": "Asuncion et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Template-based information extraction without the templates", "author": ["Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chambers and Jurafsky.,? \\Q2011\\E", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2011}, {"title": "Template-based information extraction without the templates", "author": ["Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chambers and Jurafsky.,? \\Q2011\\E", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2011}, {"title": "Template-based information extraction without the templates", "author": ["Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chambers and Jurafsky.,? \\Q2011\\E", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2011}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "J. Boyd-Graber", "S. Gerrish", "C. Wang", "D. M Blei"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Chang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Probabilistic frame induction", "author": ["Jackie Chi Kit Cheung", "Hoifung Poon", "Lucy Vanderwende"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Cheung et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "Topic model diagnostics: Assessing domain relevance via topical alignment", "author": ["Jason Chuang", "Sonal Gupta", "Christopher D. Manning", "Jeffrey Heer"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Chuang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chuang et al\\.", "year": 2013}, {"title": "Validation of software for bayesian models using posterior quantiles", "author": ["S.R. Cook", "A. Gelman", "D.B. Rubin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Cook et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2006}, {"title": "Probabilistic frame-semantic parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 948\u2013956", "author": ["Dipanjan Das", "Nathan Schneider", "Desai Chen", "Noah A. Smith"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Das et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Das et al\\.", "year": 2010}, {"title": "The stanford typed dependencies representation", "author": ["M. C de Marneffe", "C. D Manning"], "venue": "In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,", "citeRegEx": "Marneffe and Manning.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["M. C De Marneffe", "B. MacCartney", "C. D Manning"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "A latent variable model for geographic lexical variation", "author": ["Jacob Eisenstein", "Brendan O\u2019Connor", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Eisenstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2010}, {"title": "Frame semantics. In Linguistics in the Morning Calm", "author": ["Charles Fillmore"], "venue": "Hanshin Publishing Company, Seoul, South Korea,", "citeRegEx": "Fillmore.,? \\Q1982\\E", "shortCiteRegEx": "Fillmore.", "year": 1982}, {"title": "Frame semantics for text understanding", "author": ["Charles J. Fillmore", "Collin F. Baker"], "venue": "In Proceedings of WordNet and Other Lexical Resources Workshop,", "citeRegEx": "Fillmore and Baker.,? \\Q2001\\E", "shortCiteRegEx": "Fillmore and Baker.", "year": 2001}, {"title": "Probabilistic models of verb-argument structure", "author": ["Daniel Gildea"], "venue": "In Proceedings of COLING,", "citeRegEx": "Gildea.,? \\Q2002\\E", "shortCiteRegEx": "Gildea.", "year": 2002}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea and Jurafsky.,? \\Q2002\\E", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Unsupervised discovery of a statistical verb lexicon", "author": ["T. Grenager", "C. D Manning"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grenager and Manning.,? \\Q2006\\E", "shortCiteRegEx": "Grenager and Manning.", "year": 2006}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["T.L. Griffiths", "M. Steyvers", "D.M. Blei", "J.B. Tenenbaum"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2005}, {"title": "Unsupervised induction of semantic roles. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 939\u2013947", "author": ["Joel Lang", "Mirella Lapata"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Lang and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Lang and Lapata.", "year": 2010}, {"title": "English verb classes and alternations: A preliminary investigation, volume 348", "author": ["Beth Levin"], "venue": "University of Chicago press Chicago,", "citeRegEx": "Levin.,? \\Q1993\\E", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M. P Marcus", "B. Santorini", "M. A Marcinkiewicz"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "Topic models conditioned on arbitrary features with dirichletmultinomial regression", "author": ["David. Mimno", "Andrew McCallum"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Mimno and McCallum.,? \\Q2008\\E", "shortCiteRegEx": "Mimno and McCallum.", "year": 2008}, {"title": "Unsupervised induction of frame-semantic representations", "author": ["Ashutosh Modi", "Ivan Titov", "Alexandre Klementiev"], "venue": "In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,", "citeRegEx": "Modi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Modi et al\\.", "year": 2012}, {"title": "Bayesian mixture modeling", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "Neal.,? \\Q1991\\E", "shortCiteRegEx": "Neal.", "year": 1991}, {"title": "Distributed algorithms for topic models", "author": ["David Newman", "Arthur Asuncion", "Padhraic Smyth", "Max Welling"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Newman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2009}, {"title": "Effective information extraction with semantic affinity patterns and relevant regions", "author": ["S. Patwardhan", "E. Riloff"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Patwardhan and Riloff.,? \\Q2007\\E", "shortCiteRegEx": "Patwardhan and Riloff.", "year": 2007}, {"title": "Distributional clustering of english words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424\u2013434", "author": ["Alan Ritter", "Oren Etzioni"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Ritter and Etzioni.,? \\Q2010\\E", "shortCiteRegEx": "Ritter and Etzioni.", "year": 2010}, {"title": "Inducing a semantically annotated lexicon via EM-based clustering", "author": ["M. Rooth", "S. Riezler", "D. Prescher", "G. Carroll", "F. Beil"], "venue": "In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "Rooth et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Rooth et al\\.", "year": 1999}, {"title": "Framenet ii: Extended theory and practice", "author": ["J. Ruppenhofer", "M. Ellsworth", "M.R.L. Petruck", "C.R. Johnson", "J. Scheffczyk"], "venue": "International Computer Science Institute,", "citeRegEx": "Ruppenhofer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ruppenhofer et al\\.", "year": 2006}, {"title": "Scripts, Plans, Goals, Understanding: An Inquiry Into Human Knowledge Structures", "author": ["Roger C. Schank", "Robert P. Abelson"], "venue": null, "citeRegEx": "Schank and Abelson.,? \\Q1977\\E", "shortCiteRegEx": "Schank and Abelson.", "year": 1977}, {"title": "Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435\u2013444", "author": ["Diarmuid O. S\u00e9aghdha"], "venue": "Association for Computational Linguistics,", "citeRegEx": "S\u00e9aghdha.,? \\Q2010\\E", "shortCiteRegEx": "S\u00e9aghdha.", "year": 2010}, {"title": "Improving verb clustering with automatically acquired selectional preferences", "author": ["Lin Sun", "Anna Korhonen"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume", "citeRegEx": "Sun and Korhonen.,? \\Q2009\\E", "shortCiteRegEx": "Sun and Korhonen.", "year": 2009}, {"title": "Verb class discovery from rich syntactic data", "author": ["Lin Sun", "Anna Korhonen", "Yuval Krymolowski"], "venue": "Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Sun et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "A Bayesian approach to unsupervised semantic role induction", "author": ["I. Titov", "A. Klementiev"], "venue": "Proceedings of EACL,", "citeRegEx": "Titov and Klementiev.,? \\Q2012\\E", "shortCiteRegEx": "Titov and Klementiev.", "year": 2012}, {"title": "A bayesian model for unsupervised semantic parsing", "author": ["Ivan Titov", "Alexandre Klementiev"], "venue": "In Proceedings of ACL,", "citeRegEx": "Titov and Klementiev.,? \\Q2011\\E", "shortCiteRegEx": "Titov and Klementiev.", "year": 2011}, {"title": "Rethinking lda: Why priors matter", "author": ["Hanna Wallach", "David Mimno", "Andrew McCallum"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Topic modeling: beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Wallach.,? \\Q2006\\E", "shortCiteRegEx": "Wallach.", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "Semantic frames\u2014types of events and their participants\u2014are a key development in linguistic theories of semantics (Fillmore, 1982) and, sometimes called \u201cscripts\u201d or \u201cschemata,\u201d have figured heavily in natural language understanding research (Schank and Abelson, 1977; Lehnert and Ringle, 1982).", "startOffset": 113, "endOffset": 129}, {"referenceID": 33, "context": "Semantic frames\u2014types of events and their participants\u2014are a key development in linguistic theories of semantics (Fillmore, 1982) and, sometimes called \u201cscripts\u201d or \u201cschemata,\u201d have figured heavily in natural language understanding research (Schank and Abelson, 1977; Lehnert and Ringle, 1982).", "startOffset": 241, "endOffset": 293}, {"referenceID": 9, "context": "There has been a recent surge in interest in finding frames in text data, including frame-semantic parsing (Das et al., 2010; Gildea and Jurafsky, 2002) following the conventions of FrameNet (Fillmore and Baker, 2001), and discovery of narrative structure (Chambers and Jurafsky, 2009).", "startOffset": 107, "endOffset": 152}, {"referenceID": 16, "context": "There has been a recent surge in interest in finding frames in text data, including frame-semantic parsing (Das et al., 2010; Gildea and Jurafsky, 2002) following the conventions of FrameNet (Fillmore and Baker, 2001), and discovery of narrative structure (Chambers and Jurafsky, 2009).", "startOffset": 107, "endOffset": 152}, {"referenceID": 14, "context": ", 2010; Gildea and Jurafsky, 2002) following the conventions of FrameNet (Fillmore and Baker, 2001), and discovery of narrative structure (Chambers and Jurafsky, 2009).", "startOffset": 73, "endOffset": 99}, {"referenceID": 29, "context": "Pereira et al. (1993) and Rooth et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 29, "context": "Pereira et al. (1993) and Rooth et al. (1999) model a corpus of (verb, object) pairs with a latent variable for each tuple, and different word distributions for for each argument and class.", "startOffset": 0, "endOffset": 46}, {"referenceID": 34, "context": "More recent work extends the Rooth approach to model other syntactic relation pairs (S\u00e9aghdha, 2010) and web-extracted triple relations (Ritter and Etzioni, 2010).", "startOffset": 84, "endOffset": 100}, {"referenceID": 30, "context": "More recent work extends the Rooth approach to model other syntactic relation pairs (S\u00e9aghdha, 2010) and web-extracted triple relations (Ritter and Etzioni, 2010).", "startOffset": 136, "endOffset": 162}, {"referenceID": 40, "context": "(2010); Mimno and McCallum (2008)) In this respect, this model could be seen as a \u201csemantic\u201d or \u201csyntactic tuple\u201d topic model, along the lines of previous work that has added various forms of sentence-internal structure to LDA\u2019s document generation process, such as bigrams (Wallach, 2006), HMM\u2019s (Griffiths et al.", "startOffset": 274, "endOffset": 289}, {"referenceID": 20, "context": "(2010); Mimno and McCallum (2008)) In this respect, this model could be seen as a \u201csemantic\u201d or \u201csyntactic tuple\u201d topic model, along the lines of previous work that has added various forms of sentence-internal structure to LDA\u2019s document generation process, such as bigrams (Wallach, 2006), HMM\u2019s (Griffiths et al., 2005), or certain forms of syntax (Boyd-Graber and Blei, 2008).", "startOffset": 297, "endOffset": 321}, {"referenceID": 12, "context": "Blei and Lafferty (2006); Eisenstein et al. (2010); Mimno and McCallum (2008)) In this respect, this model could be seen as a \u201csemantic\u201d or \u201csyntactic tuple\u201d topic model, along the lines of previous work that has added various forms of sentence-internal structure to LDA\u2019s document generation process, such as bigrams (Wallach, 2006), HMM\u2019s (Griffiths et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 12, "context": "Blei and Lafferty (2006); Eisenstein et al. (2010); Mimno and McCallum (2008)) In this respect, this model could be seen as a \u201csemantic\u201d or \u201csyntactic tuple\u201d topic model, along the lines of previous work that has added various forms of sentence-internal structure to LDA\u2019s document generation process, such as bigrams (Wallach, 2006), HMM\u2019s (Griffiths et al.", "startOffset": 26, "endOffset": 78}, {"referenceID": 15, "context": "Other related models include (Gildea, 2002; Titov and Klementiev, 2012); most relevantly, the recently published work of Modi et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 37, "context": "Other related models include (Gildea, 2002; Titov and Klementiev, 2012); most relevantly, the recently published work of Modi et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 29, "context": "Titov and Klementiev (2011)\u2019s model of semantic frames also uses cross-cutting word classes, embedded in a more complex model that also learns clusters of syntactic relations, and recursively generates dependency trees.", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Grenager and Manning (2006) and Lang and Lapata (2010) present related models for unsupervised PropBank-style semantic role labeling, where a major focus is grouping or clustering syntactic argument patterns.", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Grenager and Manning (2006) and Lang and Lapata (2010) present related models for unsupervised PropBank-style semantic role labeling, where a major focus is grouping or clustering syntactic argument patterns.", "startOffset": 0, "endOffset": 55}, {"referenceID": 11, "context": "Other related models include (Gildea, 2002; Titov and Klementiev, 2012); most relevantly, the recently published work of Modi et al. (2012) compares a similar Bayesian model directly to FrameNet, and Cheung et al.", "startOffset": 30, "endOffset": 140}, {"referenceID": 3, "context": "(2012) compares a similar Bayesian model directly to FrameNet, and Cheung et al. (2013) integrates discourse into a frame model, capturing the script-style structures that were previously explored in ad-hoc approaches by Chambers and Jurafsky (2011a).", "startOffset": 67, "endOffset": 88}, {"referenceID": 2, "context": "(2013) integrates discourse into a frame model, capturing the script-style structures that were previously explored in ad-hoc approaches by Chambers and Jurafsky (2011a). Finally, while we do not enforce any relationship between syntactic argument position and the classes, in practice, most classes are exclusively either verbs or nouns, since words that can be verbs often cannot appear as nouns.", "startOffset": 140, "endOffset": 170}, {"referenceID": 6, "context": "(This analysis technique was inspired by Cook et al. (2006)\u2019s Bayesian software validation method.", "startOffset": 41, "endOffset": 60}, {"referenceID": 6, "context": "(This analysis technique was inspired by Cook et al. (2006)\u2019s Bayesian software validation method.) The hyperparameter sampling substantially improves likelihood. This is in line with Wallach et al. (2009) and Asuncion et al.", "startOffset": 41, "endOffset": 206}, {"referenceID": 0, "context": "(2009) and Asuncion et al. (2009), which found that learning these Dirichlet hyperparameters for LDA gave much better solutions than fixing them\u2014this makes sense, since a researcher can\u2019t have much idea of good values to arbitrarily pick.", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": "(2009) and Asuncion et al. (2009), which found that learning these Dirichlet hyperparameters for LDA gave much better solutions than fixing them\u2014this makes sense, since a researcher can\u2019t have much idea of good values to arbitrarily pick. (An alternative approach, cross-validated grid search (used in e.g. (Griffiths and Steyvers, 2004a)), is far more expensive than fitting the hyperparameters on the training data\u2014it is infeasible once there are several concentration paramaters, as there are here. Asuncion et al. found that direct hyperparameter learning performed as well as grid search.) Furthermore, the recent work of Chuang et al. (2013) found that hyperparameter learning also gave near-optimal semantic coherency for LDA (according to expert judgments of topical semantic quality).", "startOffset": 11, "endOffset": 648}, {"referenceID": 0, "context": "(2009) and Asuncion et al. (2009), which found that learning these Dirichlet hyperparameters for LDA gave much better solutions than fixing them\u2014this makes sense, since a researcher can\u2019t have much idea of good values to arbitrarily pick. (An alternative approach, cross-validated grid search (used in e.g. (Griffiths and Steyvers, 2004a)), is far more expensive than fitting the hyperparameters on the training data\u2014it is infeasible once there are several concentration paramaters, as there are here. Asuncion et al. found that direct hyperparameter learning performed as well as grid search.) Furthermore, the recent work of Chuang et al. (2013) found that hyperparameter learning also gave near-optimal semantic coherency for LDA (according to expert judgments of topical semantic quality). These works use a slightly different method for hyperparameter learning (empirical Bayes optimization, as opposed to MCMC sampling); but we suspect the choice of method makes little difference. What matters is getting away from the human-chosen initial value, which will be pretty poor. Interestingly, most of the movement tends to happen early in the MCMC chain, then the hyperparameter stabilizes as the rest of the model is still moving. For one experimental setting (a subset of CRIMENYT, described below), we checked if the outcome was initializer dependent by starting three different MCMC chains that were identical except for three different \u03b1 initializers: Figure 4. Reassuringly, they all converged on the same region of values. This robustness to initialization was exactly what we wanted. For larger datasets, we implemented a parallelized sampling scheme for f and c similar to Newman et al. (2009) where individual processors use stale counts and synchronize once per iteration by sending count update messages to all other processors.", "startOffset": 11, "endOffset": 1706}, {"referenceID": 14, "context": "First, inspired by examples in the crime reporting domain of extracting narrative event structures (Chambers and Jurafsky, 2009) and work on the FrameNet corpus (Fillmore and Baker, 2001), we select news articles for which any category labels contain any of the words crime, crimes, or criminal, resulting in a targeted corpus", "startOffset": 161, "endOffset": 187}, {"referenceID": 23, "context": "Finally, we also performed experiments on two pre-parsed corpora from the Penn Treebank (Marcus et al., 1994), cointaining tokenizations, part-of-speech tags, and parses: The Wall Street Journal (all sections; WSJPTB), and the PTB\u2019s subset of the Brown corpus\u2014consisting mainly of literature and essays (BROWNPTB).", "startOffset": 88, "endOffset": 109}, {"referenceID": 2, "context": "We were curious if the model could find word classes that took the subject position in some frames, but the object position in others\u2014Chambers and Jurafsky (2009) demonstrate interesting examples of this in their learned schemas.", "startOffset": 134, "endOffset": 163}, {"referenceID": 5, "context": "Evaluation is difficult; one automatic measure, held-out likelihood, may not always correlate to subjective semantic coherency (Chang et al., 2009).", "startOffset": 127, "endOffset": 147}, {"referenceID": 14, "context": "Seeking a resource that is more general, more lexicon-focused, can be used to compare different corpus domains we turn to FrameNet (Fillmore and Baker, 2001; Ruppenhofer et al., 2006), a well-documented lexical resource of actions/situations and their typical participant types.", "startOffset": 131, "endOffset": 183}, {"referenceID": 32, "context": "Seeking a resource that is more general, more lexicon-focused, can be used to compare different corpus domains we turn to FrameNet (Fillmore and Baker, 2001; Ruppenhofer et al., 2006), a well-documented lexical resource of actions/situations and their typical participant types.", "startOffset": 131, "endOffset": 183}, {"referenceID": 2, "context": "Chambers and Jurafsky (2011c) compare their learned frames to MUC templates in the domain of news reports about terrorist activities.", "startOffset": 0, "endOffset": 30}, {"referenceID": 28, "context": "We uncovered a number of discrepancies between the evaluation done by Chambers and Jurafsky versus the previous work they compare to (Patwardhan and Riloff, 2007); after a number of email exchanges with all previous co-authors, Chambers modified his evaluation implementation and reports minor changes to their evaluation numbers.", "startOffset": 133, "endOffset": 162}, {"referenceID": 18, "context": "Besides FrameNet, it may be worth comparing to a verb clustering more like the Levin (1993) classes; for example, Sun et al.", "startOffset": 79, "endOffset": 92}, {"referenceID": 18, "context": "Besides FrameNet, it may be worth comparing to a verb clustering more like the Levin (1993) classes; for example, Sun et al. (2008) and Sun and Korhonen (2009) construct a set of 204 verbs in Levin-style clusters and evaluate clustering methods against them.", "startOffset": 79, "endOffset": 132}, {"referenceID": 18, "context": "Besides FrameNet, it may be worth comparing to a verb clustering more like the Levin (1993) classes; for example, Sun et al. (2008) and Sun and Korhonen (2009) construct a set of 204 verbs in Levin-style clusters and evaluate clustering methods against them.", "startOffset": 79, "endOffset": 160}, {"referenceID": 2, "context": "It would be interesting to compare to the work of Chambers and Jurafsky (2011b), which induces frames with several stages of ad-hoc clustering on unlabeled newswire data, and compares its learned frames to frames and extractions from MUC-4, a domain of newswire reports of terrorist and political events.", "startOffset": 50, "endOffset": 80}, {"referenceID": 2, "context": "It would be interesting to compare to the work of Chambers and Jurafsky (2011b), which induces frames with several stages of ad-hoc clustering on unlabeled newswire data, and compares its learned frames to frames and extractions from MUC-4, a domain of newswire reports of terrorist and political events. We did perform a headroom test for MUC extraction accuracy and found our approach is poor for this task\u2014by looking at all role-filler instances in the text, and checking how often they corresponded to a nominal subject or object according to the Stanford dependency parser plus our syntactic extraction rules. This was 42% of the time (on the DEV data), which constitutes an upper bound on recall. Many of the MUC instances (as well as FrameNet annotations) use noun-noun, implicit, and other syntactic indicators of semantic relations. The approach of Cheung et al. (2013) is superior for this task: they use a flexible set of dependency relations integrated with discourse-level sequence model, and compare favorably to Chambers and Jurafsky\u2019s work.", "startOffset": 50, "endOffset": 879}], "year": 2013, "abstractText": "We develop a probabilistic latent-variable model to discover semantic frames\u2014types of events and their participants\u2014from corpora. We present a Dirichlet-multinomial model in which frames are latent categories that explain the linking of verb-subject-object triples, given document-level sparsity. We analyze what the model learns, and compare it to FrameNet, noting it learns some novel and interesting frames. This document also contains a discussion of inference issues, including concentration parameter learning; and a small-scale error analysis of syntactic parsing accuracy. Note: this work was originally posted online October 2012 as part of CMU MLD\u2019s Data Analysis Project requirement. This version has no new experiments or results, but has added some discussion of new related work.", "creator": "LaTeX with hyperref package"}}}