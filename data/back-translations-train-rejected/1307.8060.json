{"id": "1307.8060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2013", "title": "Extracting Information-rich Part of Texts using Text Denoising", "abstract": "The aim of this paper is to report on a novel text reduction technique, called Text Denoising, that highlights information-rich content when processing a large volume of text data, especially from the biomedical domain. The core feature of the technique, the text readability index, embodies the hypothesis that complex text is more information-rich than the rest. When applied on tasks like biomedical relation bearing text extraction, keyphrase indexing and extracting sentences describing protein interactions, it is evident that the reduced set of text produced by text denoising is more information-rich than the rest.", "histories": [["v1", "Tue, 30 Jul 2013 17:36:53 GMT  (16kb)", "http://arxiv.org/abs/1307.8060v1", "26th Canadian Conference on Artificial Intelligence (CAI-2013), Regina, Canada, May 29-31, 2013"]], "COMMENTS": "26th Canadian Conference on Artificial Intelligence (CAI-2013), Regina, Canada, May 29-31, 2013", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["rushdi shams"], "accepted": false, "id": "1307.8060"}, "pdf": {"name": "1307.8060.pdf", "metadata": {"source": "CRF", "title": "Extracting Information-rich Part of Texts using Text Denoising", "authors": ["Rushdi Shams"], "emails": ["rshams@uwo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 130 7,80 60v1 [cs.IR] 3 0Ju l 201 3"}, {"heading": "1 Introduction", "text": "In order to test the scalability of a method and its performance across all text genres, there is often a need to process large amounts of text data across many disciplines of the NLP, be it in the extraction of textual relations, summaries or meta-tagging. [?] It has been reported by many researchers [?] that machine learning and rule-based approaches have improvements over their benchmarks with increased training data. However, the use of large amounts of data can cause several bottlenecks. One is technical - the processing of large data, such as those from biomedical texts, slows down many algorithms; another is more important - algorithms may have reduced accuracy due to noise, which is irrelevant, or redundant data for a given classification task, which is added by low-information parts of text. There are several statistics, such as the tf-idf word marker and the sentence marker, which help to identify the wealth of information, although these have significant limitations on the degree of use of the phrases, which are required for their popularity."}, {"heading": "2 Proposed Method", "text": "Among the text readability values, the following five metrics are considered as benchmarks - Fog Index (hereinafter FI) [?], Flesch reading ease score (FRES) [?], Smog Index [?], Forcast Index [?] and Flesch-Kincaid readability index (FKRI) [?]. The choice of using text readability as an informational wealth statistic is motivated by the results of an experiment by Duff and Kabance [?]. In their experiment, a passage with no more than two phrases was converted into primary prose and FI to test its readability, and they found that the score was low (i.e. the prose was extremely easy to read). The authors concluded that simple texts obscure the connections and ideas by de-emphasizing both. In contrast, difficult texts emphasize relationships and ideas that lead to low readability."}, {"heading": "3 Text Denoising on Relation Extraction", "text": "The only rule I set for this task was to extract 30% of the hard-to-read sentences from the texts; this threshold is called the denocialization threshold and the texts are called the denocialized text; the rest is called the note text. However, this threshold was set hazardously, taking into account the stability in the frequency of occurrence of related concepts in the corpus; other than 30%, which is called results with different denocialization thresholds ranging from 10% to 50%, was not satisfactory. I ranked the pairs of concepts in the denocized texts using their frequency."}, {"heading": "4 Text Denoising on Keyphrase Extraction", "text": "I investigated the usability of denoised texts as training data for machine learning using keyphrase indexers called KEA [?], KEA + + [?] and Maui [?]. I applied the indexers with their classifiers, which are derived from denoised training data, to three data sets, namely FAO-780, CERN-290 and NLM-500. These data sets consist of texts from the fields of agriculture, physics and biomedical science. I compared the result with their benchmark performance achieved by using the full-text training data. Convincing in a 10-fold cross-validation experiment, both KEA and KEA + + with their classifiers derived from denoised training data exceeded their respective benchmark F values [?]. Maui, on the other hand, had mixed results and its denoised text denoised classifier comparable to its benchmark."}, {"heading": "5 Text Denoising on Extracting Protein Relations bearing Sentences", "text": "In an attempt to eliminate the denocialization threshold, which depends on the writing style (Section 4), I decided to develop a machine learning version of text denociation. However, the classification task in hand consisted of annotating sentences of a series of texts with either positive or negative labels based on the presence of protein interactions. However, the chosen functionality consisted of 35 features, such as various parameters of readability indices, term frequency, inverse sentence frequency, biomedical designated unit, verbs and acronyms, stopwords, semantic words and sentence positions. After applying a number of well-known classification features such as the Bayesian classifiers, Random Forest, SVM, AdaBoost and dredging of the classifiers that best segments, it was chosen to stack with Random Forest, semantic words and sentence positions. The company that is used for this experiment is BioDRP, BioDRP, BioDRP and BioDRP, and BioDRP, the F000."}, {"heading": "6 Conclusion and Future Work", "text": "The proposed method of text denocialization was essentially the same for several tasks and types of text: text reduction according to readability improved relation mining, keyphrase indexing, and extraction of sentences describing protein inclusions. This result suggests that hard-to-read sentences are more informative than the rest. The effect of text denociation has yet to be studied on text categorization and summary. I am currently investigating the effect of readability on the detection of email spam. The results so far are interesting, as I label spam and ham only on the basis of the readability of email text content (i.e. without looking at the mail header). I also intend to train benchmark summarizers with denocized texts and see how they fare against gold standard summaries."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "The aim of this paper is to report on a novel text reduction technique, called Text Denoising, that highlights information-rich content when processing a large volume of text data, especially from the biomedical domain. The core feature of the technique, the text readability index, embodies the hypothesis that complex text is more informationrich than the rest. When applied on tasks like biomedical relation bearing text extraction, keyphrase indexing and extracting sentences describing protein interactions, it is evident that the reduced set of text produced by text denoising is more information-rich than the rest.", "creator": "LaTeX with hyperref package"}}}