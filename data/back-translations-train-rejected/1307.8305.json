{"id": "1307.8305", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2013", "title": "The Planning-ahead SMO Algorithm", "abstract": "The sequential minimal optimization (SMO) algorithm and variants thereof are the de facto standard method for solving large quadratic programs for support vector machine (SVM) training. In this paper we propose a simple yet powerful modification. The main emphasis is on an algorithm improving the SMO step size by planning-ahead. The theoretical analysis ensures its convergence to the optimum. Experiments involving a large number of datasets were carried out to demonstrate the superiority of the new algorithm.", "histories": [["v1", "Wed, 31 Jul 2013 12:38:20 GMT  (25kb)", "http://arxiv.org/abs/1307.8305v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tobias glasmachers"], "accepted": false, "id": "1307.8305"}, "pdf": {"name": "1307.8305.pdf", "metadata": {"source": "CRF", "title": "The Planning-ahead SMO Algorithm", "authors": ["Tobias Glasmachers"], "emails": ["tobias.glasmachers@ini.rub.de"], "sections": [{"heading": null, "text": "ar Xiv: 130 7,83 05v1 [cs.LG] 3 1Ju l 2Keywords: Sequential Minimal Optimization, Quadratic Programming, Support of Vector Machine Training"}, {"heading": "1 Introduction", "text": "Suppose we get a training dataset (x1, y1),.., (x, y) consisting of inputs xi-X and binary designations yi-1, a positive semi-definitive Mercer kernel function k: X-X-R on input range X and a regularization parameter value C > 0. Then the dual SVM training problem is given for this problem by maximizing f (\u03b1) = yT\u03b1-12 \u03b1TK\u03b1 (1) s.t.0 (equality constraint) and Li \u2264 \u03b1i \u2264 Ui \u2264 1 \u2264 i (box constraint). Here, the vector y = (y1, y) T-R value is composed relatively precisely from the labels, the positive semi-defined problem Graasim (box constraint) for this problem."}, {"heading": "2 State of the Art SMO Algorithm", "text": "The sequential minimum optimization (SMO) is an iterative resolution algorithm (1) that uses minimum sets of size two. This quantity is minimal to keep the current solution workable. (1) The algorithm explicitly uses the specific structure of the problem (1) and shows very good performance in practice. (1) For every possible point (2) we define the index setsIup (2) = short-term. (.) The canonical form of the SMO algorithm is (using the usual Karush-KuhnTucker (KKT) violation of equality) can be expressed as follows: Algorithm 1: General SMO algorithms mInput: Feasible starting points (0), Accuracy."}, {"heading": "3 Behavior of the SMO Algorithm", "text": "We want to make several empirical and theoretical statements about the overall behavior of the SMO algorithm, including some motivation for the later presented algorithm. We start with theoretical results.It is well known that the algorithm converges to an optimum for a number of selection strategies of the working environments.In addition to convergence evidence for important special cases [8, 15, 5, 3] evidence procedures for general classes of selection strategies have been studied [6, 10, 1].Chen et al. [1] have shown that under some technical conditions there are problems with selection t0 such that no SMO step ends at the limit for iterations that are free for iterations t > t0. For these iterations, the authors derive a linear convergence rate. However, the prerequisites exclude the relevant case that the optimum is not isolated. Upper limits for t0 are not known, and in experiments the algorithm is rare to enter these variations."}, {"heading": "4 Planning Ahead", "text": "Without loss of universality, we consider the iteration t = 1 in this section. Suppose we get the current working principle B (1) = (i (1), j (1) and for some reason we know the working principle B (2) = (i (2), j (2)) to be selected in the next iteration. Furthermore, we assume that the solutions of both involved sub-problems are not borderline, that is, we can simply ignore the box constraints. From Equation (4), we know with \u00b5 (1) = l1 / Q11 and \u00b5 (2) that both free steps together lead to the gaing2 step: = f (2) \u2212 f (0) \u2212 f (0) = 12 Q11 (1)."}, {"heading": "5 Algorithms", "text": "We will present a first simple version and a refinement that focuses on the convergence of the general algorithm to an optimal solution. These modifications are all based on the SMO algorithm 1. Therefore, we will only substitute for the selection of the workset step 1 and the updated step 2. In the previous section, it is left open how we can know the choice of the workset in the upcoming iteration. If we try to calculate this workset based on a step size, it turns out that we need to execute the selection algorithm of the workset. That is, the next workgroup depends on the current step size and it takes linear time to determine the workset for a given step size. This makes a search for the optimal combination of step size and workset impractical. Instead, we propose a very simple heuristic method. We propose to reuse the previous workgroup for two reasons: First, the chance that the corresponding ratings are highest."}, {"heading": "6 Convergence of the Method", "text": "In this section, we will show that the PA-SMO algorithm converges to the optimal f function (1). First, we will introduce some notations and introduce some definitions."}, {"heading": "7 Experiments", "text": "The focus of the experiments is on the comparison of the PA-SMO algorithm with the standard (greedy) SMO algorithm. The latest LIBSVM version 2.84 implements algorithm 1. For comparison, we have converted the changes described in algorithm 3 and algorithm 4 directly into LIBSVM. Note that in the first iteration starting from \u03b1 (0) = (0,.., 0) T the components yi = \u00b1 1 of the gradient-f (\u03b1 (0) = y take only two possible values. The absolute values of these components are equal and they all point into the box. Therefore, the workset selection algorithm could select any i (1) \u2022 Iup (\u03b1 (0))) as the first index, because the gradient components of all indices are maximum. Thus, there is a freedom of choice for the first iteration. LIBSVM chooses arbitrarily i (1) = max (Iup (\u03b1 (0)))."}, {"heading": "7.1 Results", "text": "The results are summarized in Table 2.There is a clear trend in these results. For some datasets, the PA-SMO algorithm significantly outperforms the SMO algorithm, while for other datasets there is no significant difference. Most importantly, PA-SMO does not perform worse than standard SMO in any case. The number of iterations is significantly reduced in almost all cases. This result is not surprising. It basically means that the algorithm works as expected. However, the runtime of the PA-SMO algorithm takes much longer than late iterations after shrinkage has more or less identified the interesting variables. Therefore, it is only natural that the number of iterations is only a weak indicator of runtime. The runtime of the PA-SMO algorithm is usually slightly reduced. This difference is significant in 5 cases."}, {"heading": "7.2 Influence of Planning-Ahead vs. Working Set Selection", "text": "Remember that we changed two parts of the SMO algorithm: the abbreviated Newton step was replaced by the Planning Pre-Selection Algorithm 4, and the Workset selection was modified accordingly by Algorithm 3. It is possible to use the second modification without the first, but hardly vice versa. Therefore, we operated the SMO algorithm with the modified Workset selection, but without planning beforehand to get a handle on the impact of these changes on overall performance. That is, we took care that the algorithm selects the workset before two iterations, if it is a feasible direction, and g-maximizes the Newton step forward. While the results of the comparison to the standard SMO were completely unclear, the PA-SMO algorithm proved to be clearly superior. So, the reason for the acceleration of the PA-SMO is not the changed workset selection, but the planning in advance."}, {"heading": "7.3 Planning-Ahead Step Sizes", "text": "To understand how the planning run is actually used by the algorithm, we measured the quantity \u00b5 / \u00b5 \u043a \u2212 1, i.e. the size of the planning step in relation to the Newton step. In free SMO steps, this quantity is always 0, in larger steps it is positive, in smaller steps it is negative, and in steps in the opposite direction it is even smaller than \u2212 1. We present some representative histograms in Figure 3. These histograms show that most planning steps are only slightly increased compared to the Newton step size, but there are cases where the algorithm chooses a step that is increased by a factor of several thousand. However, very few steps are reduced, or even reversed, if at all. Obviously, the step size histograms are far from symmetrical, so it is only natural to ask whether a very simple increase in the Newton step size can be a good strategy that cannot be an SMO strategy."}, {"heading": "7.4 Multiple Planning-Ahead", "text": "We now turn to the variant of the PA-SMO algorithm, which uses more than one current workset to plan ahead. This variant, as explained at the end of Section 5, plans with several workgroups. In addition, these workgroups are additional candidates for selecting the workset. We can assume that the number of iterations decreases as more workgroups are used in this way. However, the calculations per iteration naturally increase, so that too many workgroups will slow down the entire algorithm. Thus, there has been a balance between the number of iterations and the time required per iteration. Now, the interesting question arises whether there is a uniform best number of workgroups for all problems. We conducted experiments with the 2, 3, 5, 10 and 20 most recent workgroups. It turned out that the strategies that take into account the most recent two or three workgroups are comparable to standard PA-SMO, and even slightly better."}, {"heading": "8 Conclusion", "text": "At first glance, it is surprising that the abbreviated Newton step used in all existing variants of the SMO algorithm can be exceeded, as evidenced by the greedy nature of the decomposition siterations. Experimental evaluation clearly shows the benefits of the new algorithm. As we have never observed a drop in performance, we recommend PA-SMO as the default algorithm for SVM training. PA-SMO is easy to implement based on existing SMO solvers. Due to the guaranteed convergence of the algorithm with an optimal solution, the method is widely applicable. Furthermore, convergence detection introduces a general technique to address the convergence of hybrid algorithms."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "The sequential minimal optimization (SMO) algorithm and variants<lb>thereof are the de facto standard method for solving large quadratic<lb>programs for support vector machine (SVM) training. In this paper<lb>we propose a simple yet powerful modification. The main emphasis<lb>is on an algorithm improving the SMO step size by planning-ahead.<lb>The theoretical analysis ensures its convergence to the optimum. Ex-<lb>periments involving a large number of datasets were carried out to<lb>demonstrate the superiority of the new algorithm.", "creator": "LaTeX with hyperref package"}}}