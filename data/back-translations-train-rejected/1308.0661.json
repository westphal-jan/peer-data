{"id": "1308.0661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2013", "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical Texts", "abstract": "Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating Wikipedia articles. We then select publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.", "histories": [["v1", "Sat, 3 Aug 2013 05:57:48 GMT  (245kb)", "http://arxiv.org/abs/1308.0661v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["samet atda\\u{g}", "vincent labatut"], "accepted": false, "id": "1308.0661"}, "pdf": {"name": "1308.0661.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["samet2@gmail.com,", "vlabatut@gsu.edu.tr)."], "sections": [{"heading": null, "text": "In fact, most of them are able to play by the rules they have imposed on themselves."}, {"heading": "II. EXISTING NER TOOLS", "text": "As mentioned above, many methods and tools have been designed for identifying named entities. Furthermore, it is not possible to list them all here, but three main families can be distinguished [10]: handmade rules-based methods, machine learning-based methods, and hybrid methods. First, we use manually constructed finite state patterns [11]; second, we treat NER as a classification process [10], and third, we are a mixture of these two approaches. We used three criteria for selecting suitable NER tools. First, they must be publicly and freely available; second, we prefer proven tools that are already well established in the NER community; and finally, we focus on tools that are capable of handling at least people, locations, and organizational titles.In the end, we selected four different tools. Stanford Named Entity Recognizer (SNER) [6] is based on linear chain random fields."}, {"heading": "III. EVALUATION METHODS", "text": "For a given text, the output of a NER tool is a list of units and their corresponding types, and the basic truth takes exactly the same form. In order to judge the performance of the tool, one essentially wants to compare both lists. Depending on the destination and context, different approaches can be used [1]. In this section, we will first review the traditional approach and then propose a variant that is adapted to our own context."}, {"heading": "A. Traditional Evaluation", "text": "In fact, it is the case that we are able to solve the problems mentioned without being able to get to grips with them."}, {"heading": "B. Considering Partial Matches", "text": "The traditional approach to spatial performance assessment requires complete agreement to count some of the cases presented previously: the boundaries of the estimated and actual units must be exactly the same. However, in practice, it is also possible to obtain partial matches [1], i.e. an estimated unit that overlaps with an actual unit but whose boundaries do not perfectly match. For example, this fact seems a bit too strict for us, but the estimate only includes the word Canada. A partial agreement is a significant piece of information: the NI tool has detected something, even if it is not exactly the expected unit. It is completely ignored at a later stage of our project, and we will also develop a method to efficiently combine the results of several NI tools to improve overall performance. From this perspective, it is important to look at the information represented by partial matches, and this is why we are presenting an extension of the existing measures."}, {"heading": "IV. DESCRIPTION OF THE CORPUS", "text": "Most studies use some standard corporations that are generally designed for conferences or competitions, while some commercial tools are equipped with their own data. [12] The New York Times Annotated Corpus [14] is a popular resource that consists of manually annotated articles published in this journal. However, access is subject to payment of a fee, and we decided to focus on freely available tools in this work. [15] The Conference on News Understanding proposed different corporations for NER. However, not all of them are freely available, and those that focus on texts that are very different from biographies (terrorist attacks, plane crashes, etc.) The National Institute of Standards and Technology designed an NER corpus based on Newswires [16], but it is no longer accessible from the Web. The Conference on Computational Natural Language Learning is an institution that deals with the needs of biographies (terrorist attacks, Newswires aircrashes, etc.] but is not accessible from Newswires."}, {"heading": "V. RESULTS AND DISCUSSIONS", "text": "We applied the NER tools described in Section II to our body from Section IV, using the measures described in Section III to evaluate their performance; the values obtained for the measurements are presented in Table IV or Table V for spatial or typical assessment, respectively. In the case of NER tools that propose several prefabricated models, we present only those that have achieved the best performance. In order to examine in detail the behavior of the tested NER tools, we have processed their performance not only for the entire body, but also by entity type and article category."}, {"heading": "A. Overall Performance", "text": "Consider first the overall performance. From a spatial perspective (Table IV), there is a clear hierarchy between tools ET rather than 91. Taking into account the total measures, i.e. the sum of complete and partial measures, SNER comes second for precision (0.88) and first for recall (0.93). Furthermore, the portion of partial matches in these results is very low. LIPI has the third precision (0.81) and the second recall (0.89), but the portion of partial matches is much higher. INET is fourth for precision (0.79) and third for recall (0.78), and the percentage of partial matches are even more important (more than one-third of the total performance). Note the fact that the balance between complete and partial matches changes from one tool to another shows that it is a relevant criterion of performance evaluation. We manually examined the texts commented on by INET and found the two main levels of matches."}, {"heading": "B. Performance by Entity Type", "text": "Let us focus on the performance of people who are able to put themselves centre stage."}, {"heading": "C. Performance by Article Category", "text": "Certain article categories have an impact on the performance of certain tools. Taking into account the SNER, there is no impact on the categories military, politics and science. However, art and others result in slightly lower performance, both in terms of space and types. On the contrary, the spatial performance is much higher than the overall level for sports (and it also applies to typical performance, to a lesser extent). This seems to be due to the fact that sports-related biographies generally contain a lot of person names, such as teammates, opponents, coaches, etc. SNER is particularly good at recognizing person names, which is why its performances are higher than in this category. Art-related articles contain many titles of artworks that are generally confusing to the organization."}, {"heading": "D. General Comments", "text": "Several interesting conclusions can be drawn from our findings and observations. Firstly, even if the overall performance seems to indicate SNER as the best tool, it is difficult to evaluate it when looking at the detailed performance. Thus, the fact that individual measures may be insufficient to properly assess the quality of NER tools and to compare them, the various aspects that we all considered useful to characterize the tools in a relevant way: partially identical, entity types, article categories. As a result, these tools can be considered complementary."}, {"heading": "VI. CONCLUSION", "text": "In this article, we focus on the problem of selecting a suitable designated entity recognition (NER) tool for biographical texts. Many NER tools exist, most of them are based on generic approaches capable of handling any type of text. So, their performance on these specific data must be compared to make a choice. However, existing corporations do not consist of biographies. For this reason, we designed our own and applied a selection of publicly available NER tools to it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12]. To highlight the importance of partial matches, we evaluated their performance using custom measures that allow them to be taken into account. Our results show a clear hierarchy between the tools tested: first Stanford NER, then LingPipe, Illinois NET and finally OpenCalais. The latter achieves particularly low recall values. When studying the details of these categories, it turns out they are non-uniform across generic types."}, {"heading": "ACKNOWLEDGMENT", "text": "The first version of our platform was developed by YasaAkbulut, the first version of our body by Burcu K\u00fcpelio\u011flu. Both tasks were carried out in our research team."}], "references": [{"title": "Semi-Supervised Named Entity Recognition: Learning to Recognize 100 Entity Types with Little Supervision", "author": ["D. Nadeau"], "venue": "Ottawa- Carleton Institute for Computer Science, School of Information Technology and Engineering, University of Ottawa, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "GENETAG: a tagged corpus for gene/protein named entity recognition", "author": ["L. Tanabe", "N. Xie", "L.H. Thom", "W. Matten", "W.J. Wilbur"], "venue": "BMC Bioinformatics, vol. 6, p. S3, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A Review of Relation Extraction", "author": ["N. Bach", "S. Badaskar"], "venue": "Literature review for Language and Statistics II, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "13th Conference on Computational Natural Language Learning, 2009, pp. 147-155.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Named Entity Work in IMPACT", "author": ["F. Landsbergen"], "venue": "presented at the IMPACT Final Conference 2011, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "43rd Annual Meeting on ACL, 2005, pp. 363-370.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "BANNER: an executable survey of advances in biomedical named entity recognition", "author": ["R. Leaman", "G. Gonzalez"], "venue": "Pacific Symposium on Biocomputing, vol. 13, pp. 652-663, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Assigning geographical scopes to Web pages", "author": ["B. Martins", "M. Chaves", "M.J. Silva"], "venue": "LNCS, vol. 3408, pp. 564-567, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Named Entity Recognition Approaches", "author": ["A. Mansouri", "L. Suriani Affendey", "A. Mamat"], "venue": "International Journal of Computer Science and Network Security, vol. 8, pp. 339-344, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Named entity recognition using an HMMbased chunk tagger", "author": ["G.D. Zhou", "J. Su"], "venue": "40th Annual Meeting on ACL, 2001, pp. 473- 480.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Introduction to the CoNLL-2002 shared task: language-independent named entity recognition", "author": ["E.F.T.K. Sang"], "venue": "6th Conference on Natural language Learning, 2002, pp. 1-4.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "The New York Times Annotated Corpus", "author": ["E. Sandhaus"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Message Understanding Conference- 6: a brief history", "author": ["R. Grishman", "B. Sundheim"], "venue": "16th conference on Computational linguistics, 1996, pp. 466-471.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "The Automatic Content Extraction (ACE) Program: Tasks, Data, and Evaluation", "author": ["G. Doddington", "A. Mitchell", "M. Przybocki", "L. Ramshaw", "S. Strassel", "R. Weischedel"], "venue": "4th International Conference on Language Resources and Evaluation, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to the CoNLL-2003 shared task: language-independent named entity recognition", "author": ["E.F.T.K. Sang", "F. de Meulder"], "venue": "7th conference on Natural Language Learning, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Extracting personal names from email: applying named entity recognition to informal text", "author": ["E. Minkov", "R.C. Wang", "W.W. Cohen"], "venue": "HLT/EMNLP, 2005, pp. 443-450.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Identifying and categorizing strings of text into different classes is a process defined as named entity recognition (NER) [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "First, NER is used directly in many applied research domains [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "For instance, proteins and genes can be considered as named entities, and many works in medicine focus on the analysis of scientific articles to find out hidden relationships between them, and drive experimental research [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "But NER is also used as a preprocessing step by more advanced NLP tools, such as relationship or information extraction [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 4, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 5, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 6, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Fourth, some are implemented as libraries [6], and some take various other forms such as Web services [9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "It is not possible to list them all here, but one can distinguish three main families [10]: hand-made rule-based methods, machine learning-based methods, and hybrid methods.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "The first use manually constructed finite state patterns [11]; the second treat NER as a classification process [10], and the third are a mix of those two approaches.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "The first use manually constructed finite state patterns [11]; the second treat NER as a classification process [10], and the third are a mix of those two approaches.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "Stanford Named Entity Recognizer (SNER) [6] is based on linear chain conditional random fields.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Illinois Named Entity Tagger (INET) [4] relies on several supervised learning methods: hidden Markov models, multilayered neural networks and other statistical methods.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Different approaches can be used for this purpose, depending on the goal and context [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "Those are used to process two distinct measures: Precision and Recall [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "FN are actual entities for which the tool selected the wrong type, or no type at all [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "However, in practice it is also possible to obtain partial matches [1], i.", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "The New York Times Annotated Corpus [14] is a popular resource, constituted of manually annotated articles published in this journal.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "The Message Understanding Conference [15] proposed various corpora for NER.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "The National Institute of Standards and Technology designed a NER corpus based on newswires [16], but it is not accessible from the web anymore.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "The Conference on Computational Natural Language Learning constituted NER corpora in 2002 and 2003, the latter in English [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "In [18], four different corpora were constituted from emails, for the purpose of NER assessment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "For this reason, we designed our own one and applied a selection of publicly available NER tools on it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "For this reason, we designed our own one and applied a selection of publicly available NER tools on it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12].", "startOffset": 135, "endOffset": 138}], "year": 2013, "abstractText": "Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating 247 Wikipedia articles. We then select 4 publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.", "creator": "Acrobat PDFMaker 11 pour Word"}}}