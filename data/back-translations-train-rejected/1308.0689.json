{"id": "1308.0689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2013", "title": "Measure Transformer Semantics for Bayesian Machine Learning", "abstract": "The Bayesian approach to machine learning amounts to computing posterior distributions of random variables from a probabilistic model of how the variables are related (that is, a prior distribution) and a set of observations of variables. There is a trend in machine learning towards expressing Bayesian models as probabilistic programs. As a foundation for this kind of programming, we propose a core functional calculus with primitives for sampling prior distributions and observing variables. We define measure-transformer combinators inspired by theorems in measure theory, and use these to give a rigorous semantics to our core calculus. The original features of our semantics include its support for discrete, continuous, and hybrid measures, and, in particular, for observations of zero-probability events. We compile our core language to a small imperative language that is processed by an existing inference engine for factor graphs, which are data structures that enable many efficient inference algorithms. This allows efficient approximate inference of posterior marginal distributions, treating thousands of observations per second for large instances of realistic models.", "histories": [["v1", "Sat, 3 Aug 2013 12:28:23 GMT  (69kb)", "https://arxiv.org/abs/1308.0689v1", "An abridged version of this paper appears in the proceedings of the 20th European Symposium on Programming (ESOP'11), part of ETAPS 2011"], ["v2", "Fri, 6 Sep 2013 11:45:21 GMT  (72kb)", "http://arxiv.org/abs/1308.0689v2", "An abridged version of this paper appears in the proceedings of the 20th European Symposium on Programming (ESOP'11), part of ETAPS 2011"], ["v3", "Fri, 20 Sep 2013 18:48:34 GMT  (71kb)", "http://arxiv.org/abs/1308.0689v3", "An abridged version of this paper appears in the proceedings of the 20th European Symposium on Programming (ESOP'11), part of ETAPS 2011"], ["v4", "Mon, 23 Sep 2013 08:01:06 GMT  (71kb)", "http://arxiv.org/abs/1308.0689v4", "An abridged version of this paper appears in the proceedings of the 20th European Symposium on Programming (ESOP'11), part of ETAPS 2011"]], "COMMENTS": "An abridged version of this paper appears in the proceedings of the 20th European Symposium on Programming (ESOP'11), part of ETAPS 2011", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.PL", "authors": ["johannes borgstr\\\"om", "rew d gordon", "michael greenberg", "james margetson", "jurgen van gael"], "accepted": false, "id": "1308.0689"}, "pdf": {"name": "1308.0689.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["JOHANNES BORGSTR\u00d6Ma", "ANDREW D. GORDONb", "MICHAEL GREENBERG", "JAMES MARGETSONd", "JURGEN VAN GAEL", "J. VAN GAEL", "Csoft", "FACTORIE", "Figaro", "HANSEI", "HBC", "IBAL"], "emails": ["borgstrom@acm.org", "adg@microsoft.com,", "jfdm1@roundwood.org", "mgree@seas.upenn.edu", "jurgen.vangael@gmail.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "Over the past 15 years, statistical machine learning has unified many seemingly unrelated methods through the Bayesian paradigm. With a solid understanding of the theoretical fundamentals, advances in algorithms for inferences, and numerous applications, the Bayesian paradigm is now state-of-the-art in data learning. The theme of this paper is the idea of expressing Bayesian models as probabilistic programs that were pushed by BUGS [14] and are gaining popularity recently, 2012 ACM CCS: [Theory of computation]: Semantics and reasoning - Program constructs; [Computing methodologies]: Machine learning - Machine learning approaches.Key words and phrases: Probabilistic Programming, Model-based Machine Languages, Denotational Semantics, Program constructs - Program constructs; [Computing methodologies]: Machine learning - Machine immansions - Machine learning approaches.Keywords and phrases: Probabilistic Machine construtics - Semotming program, Machine-based Semantics - Program anguages."}, {"heading": "In a tournament of three games, Alice beats Bob, Bob beats Cyd, and Alice beats Cyd. What are", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is"}, {"heading": "2. BAYESIAN MODELS AS PROBABILISTIC EXPRESSIONS", "text": "We introduce the idea of expressing a probabilistic model as code in a functional language, Fun, with primitives for generating and observing random variables. To illustrate, we first consider a subset, Bernoulli Fun, limited to weighted Boolean decisions. We describe in elementary terms operative semantics for Bernoulli Fun, which allows us to calculate the conditional probability that the expression will yield a given value, since the run was valid. 2.1. syntax, informal semantics, and baisian reading. Expressions are typed strongly, with types t, u being built up from the base types b and pair types. We let c pass over constant scale type data, n over integers, and r over real numbers. We write ty (c) = t to mean that the constant c is the type. For each base type b, we define a zero element 0b with 0bool, 0br = true, and 0u = methu (we have oletic)."}, {"heading": "Types, Constant Data, and Zero Elements:", "text": "b: = bool | real basic formula t, u: = unit | b | (t * u) real success (t * u) real success (t * u) compound type ty ((()) = unit (true) = bool ty (n) = random (r) = real 0int = 0 0real = 0.0Signatures of arithmetic and logical operators::: b1, b2 \u2192 b3 &, |, =: bool, bool \u2192 bool >, =: int, int \u2192 bool +, \u2212,%: int, int >: int >: real, real \u2192 bool +, \u2212, bool: real, real, realWe have several default probability distributions as primitive: D: t \u2192 u takes parameters in t and returns a random value in u. The names below: Uniint: only document the meaning of the parameters."}, {"heading": "Example: Two Coins, Not Both Tails", "text": "We have two different expressions, each with the probability of 1 / 4, according to the possible combinations of Booleans heads2 and heads2. All of these expressions are valid, apart from the one where heads1 = false and heads2 = false (which represents two tails), since the observation (false) is not a valid observation. The sampling semantics of this expression is a probability distribution that assigns probabilities 1 / 3 to the values (true, true), and (false, false), and (false). The sampling semantics of this expression is a probability distribution that associates probabilities (true, true), and (true), and (false, false)."}, {"heading": "Epidemiology: Odds of Disease Given Positive Test", "text": "let has disease = random (Bernoulli (0.01)) let positive result = if has diseasethen random (Bernoulli (0.8))) else random (Bernoulli (0.096) observe positive result has diseaseThe following applies to this expression: \u2022 prob (\u03c9tt) = 0.01 x 0.8 = 0.008 (true positive) \u2022 prob (\u03c9c1c2) = 0.01 x 0.2 = 0.002 (false negative) \u2022 prob (extent f t) = 0.99 x 0.096 = 0.009504 (false positive) \u2022 prob (\u03c9tt) = 0.01 x 0.008 (true positive) \u2022 prob (\u03c9t f) = 0.01 x 0.02 (false negative) \u2022 prob (\u03c9 f t) = 0.99 x 0.006 (false positive f) \u2022 prob (\u03c9 f) = 0.99 x 0.904 = 0.89496 (true negative) The semantics P [value = true | valid] is the conditional probability that we have the disease, the test is evident."}, {"heading": "3. SEMANTICS AS MEASURE TRANSFORMERS", "text": "Another difficulty is the need to observe events with a zero probability, a common situation in machine learning. Consider, for example, the naive Bayesian classifier, a common, simple probabilistic model. In the training phase, objects are given to it along with their classes and the values of their relevant characteristics. In the following, we show the training for a single characteristic: the weight of the object. Zero probability events are weight measurements that are assumed to be normally distributed around the class mean value. The result of the training is the posterior weight distributions for the different classes."}, {"heading": "Naive Bayesian Classifier, Single Feature Training:", "text": "The reason why we do not write x = y is that the events of the zero probability are marked without specifying the random variables, which we do not generally well-defined, cf. Borel's paradox."}, {"heading": "Semantics of Types as Measurable Spaces:", "text": "\"We are not in a position to bring about a solution.\" \"We are not in a position to bring about a solution.\" \"We are not in a position to bring about a solution.\" \"We are not in a position to bring about a solution.\" \"We are in a position to bring about a solution.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \".\".. \"..\" \"\" \"\".. \"\" \"..\" \"\" \"We.\".. \".\".. \".\". \".\" \"We.\". \".\" \"We.\". \"\". \"\" We. \".\" \"\" We. \"\". \"\" We. \".\" \"\" We. \".\" \"\" We. \"\". \"\" We. \".\". \"\" We. \".\". \"\" We. \".\". \".\" \"\" We. \".\" \".\" \"We.\". \"\". \"\" We. \".\". \"\" \"\" \".\" \"We.\". \"..\" We. \"\". \"\" \".\" \"\" \"We.\".. \"\" \"..........\" \"\" We.... \"\" \"\" We.... \"\" \"\" \"\" We......... \"\" \"\"........... \"\" \"\" \"\" \"\" We..... \"\" \"\" \"\" \"\" \"\" \"...........\" \"\" \"\" \"\" \"\" \"\" \"\" \"...............................\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "Measure Transformer Combinators:", "text": "It is not as if we raise a purely measurable function to a messtransformator, we use the combinator pure (t). (t) o) o (t) o (t) o (t) o (t) o (t) o (n) o (n) o (n) o (t) o (t) o (t) o (t) o (n) o (n) o (n) o (n) o (n) o (n) o (n) o (n) o o (n) o o (n) o o (n) o o (n) o (o) o o (n) o o o (n) o o (n) o o o o o (n) o o o o o o (n) o o o o (n) o o o o (n) o o o (n n n o o o o o (n) o o (n) o o o (n) o o (n) o o o (n o o o (n) o o (n) o o (n) o o o o (n (n) o o o o (n) o (n) o o o (n) o o (n) o o (n) o o (n) o (n) o o o (n) o o (n) o o (n (n) o o o o (n) o (n) o o o o o (n) o o o (n) o o o o (n) o o o o (n) o o (n) o (n) o o o (n) o o o (n) o o o o (n) o o o o (n) o o o o o) o o o (n) o (n) o o o (n) o o o (n) o o (n) o o o o o o (n) o o o o (n) o o o o (n) o o o (n) o o o o (n) o o o o o (n) o o o (n) o o o o o o o o o (n) o o o o o o o (n (n) o o o o o o o (n) o o o o o o o o (n) o o o"}, {"heading": "Auxiliary Operations on States and Pairs:", "text": "Add x (s, c), s (x) if ty (c) = t and x / dom (s), s others. search x s, s (x) if x (s) dom (s), () others. drop X s, {(x 7 \u2192 c) does | x / X} fst ((x, y), x snd ((x, y), yWe write s | X for drop (dom (s)\\ X) s. We use these combinators to give fun programs semantics as measurement transformers. We assume that all bound variables in a program are different from the free variables and from each other. In the following, V [V] s indicates the rating of V in state s, and A [[M] indicates the measurement transformer given by M."}, {"heading": "Measure Transformer Semantics of Fun:", "text": "[x] s, lookup x s V [V] s [[c] s, c > V [>] s (V1, V2)] s, (V [V1] s, V [V2] s, V [V2] s, A [V1] s, V1 [V] s, S [V] s, S [V] s, S [V] s, S [V2] s, V [V] s, S [V1] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S [S] s, S [S] s, S [S] s, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s, S [S] s] s, S [S [S] s] s, S [S] s, S, S [S] s, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "Continuous Observation:", "text": "The second program observes instead that a Boolean variable is true. This has zero probability of occurrence, and since the Boolean type is discrete, the resulting measurement variable is zero. Discrete observation: let x = random (Gaussian (0.0, 1.0)) in let b = (x = 0.0) in let = observation b in xThese examples show the need for observations on the real type, as well as on the type bool. (This also clearly distinguishes between observations based on the assumption of introspection.) Discrete observations amount to filtering. One consequence of theorem 3.3 is that our measurement semantics is a generalization of sampling semantics for discrete probabilities. For this theorem, it is crucial that denotes observe unstandardized conditioning (filtering)."}, {"heading": "Medical Trial:", "text": "let medicalTrial nTrial nControl cTrial = let pTrial = random (Beta (1.0,1.0))) observe (cTrial = = random (Binomial (nTrial, pTrial)))); let pControl = random (Beta (1.0,1.0))) observe (cControl = = random (Binomial (nControl, pControl)))); pTrial, pControlWe can then compare this model with one in which the treatment is ineffective, i.e. in which the members of the experimental group and the control group have the same probability of recovery. Again, we give a uniform prediction of the probability that the treatment will be effective; the subsequent distribution of this variable depends on the Bayean evidence for the different models, that is, the ratio between the probabilities of the observed result in the two models. This way of performing the model comparison depends crucially on the non-normalized nature of discrete observations as filtering."}, {"heading": "Model Selection:", "text": "The model selection nTrial nTrial nTrial cControl = let pEffective = = > (Beta (1,0,1,0) | | if random (Bernoulli (pEffective) \u00b2 thenmedicalTrial nTrial cControl () else let pAll = random (Beta (1,0,1,0))) Observation (cTrial = = = random (Binomial (nTrial, pAll))))) Observation (cControl = = = random (Binomial (nControl, pAll))))) pEffectiveObservation of Derived Variable. The following example has highlighted regularity problems with our original definition of observation [8] due to Chung-Chieh Shan. Observation of the derived variables: let x = random (Beta (1,0, 1,0))) Observation of the derived variables in let y = x (x \u2212 0,5 in observation of ltx; lt.lt.lt.x.ltx.x] this program should yield a score of x = 0.5."}, {"heading": "4. SEMANTICS BY COMPILATION TO CSOFT", "text": "A naive implementation of the transformer semantics of the previous section would work directly with state measurements, the size of which, even in a discrete case, could be exponential in the number of variables in the scope. In the case of large models, this becomes insoluble. In this section, instead, we give semantics to fun programs by translating them into the simple imperative language Imp. We consider Imp to be a sublanguage of Csoft; the Csoft program is then evaluated by Infer.NET by constructing a suitable factor graph [28] whose size will be linear to the size of the program. The advantage of implementing the translation from F # to Csoft over the simple generation of factor graphs directly [32] is that the translation preserves the structure of the input model (including array processing in our full language), which can be exploited by the various inference algorithms supported by Infer.NET.NET.4.1."}, {"heading": "Syntax of Imp:", "text": "l, l \u2032,.. location (variable) in global business E, F:: = c | l | (l'l) expression I:: = statementl \u2190 E assignment l s \u2190 \u2212 D (l1,.., ln) random assignment observeb l observation, if l then makes C1 otherwise C2 conditional local l: b in C local declaration (perimeter of l is C) C:: = nil | I | (C; C) composite assignment observeb observation, we make the type b of the observed location explicit. In a local declaration, local l: b in C, the place l is bound, perimeter C. Next, we derive an extended form of local, which introduces a sequence of local variables."}, {"heading": "Extended Form of local:", "text": "The rules for typing Imp are standard. We consider environments for typing Imp as a special case of fun environments in which variables (places) are always assigned to the base types. If \u03a3 = \u03b5, l1: b1,..., ln: bn, say, \u03a3 is well formed and writes \u03a3 to mean that the places li are different in pairs. Assessment \u03a3 E: b means that the expression E has in the environment \u03a3."}, {"heading": "Judgments of the Imp Type System:", "text": "The expression \"environment\" is well-formed: \"E: b\" in \"A,\" expression \"E\" has the type \"B.\" \"C:\" given, \"statement\" C \"assigns\" A. \""}, {"heading": "Typing Rules for Imp Expressions and Commands:", "text": "(IMP CONST). (c) (IMP LOC) (l: b) l: b (IMP OP) l1: b1 (b2) (IMP RANDOM) D: (x1: b1,.. l2: b3 (IMP ASSIGN) E: b l / dom (\u03a3) l s E: (\u03b5, l: b) (IMP RANDOM) D: (x1: b1,.., xn: bn) l: l1 (IMP OBSERVE) l: l observeb l: (IMP SEQ) C1 (IL RANDOM). l: (IMP OBSERVE) l: l: (IMP SEQ) l: (IMP SEQ) l: l (IMP) l: l (IMP) l: l (IMP)."}, {"heading": "Typing Rule for Extended Form of local:", "text": "(SH EMP)..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "Semantics of Extended Form of local:", "text": "I [[[local in C]], I [[C]] > > > pure (drop (dom (\u03a3))) 4.3. Translating Fun to Imp. Translating Fun to Imp is a mostly routine compilation of functional code into mandatory code. The main point of interest is that Imp places only contain values of the base type, while Fun variables can contain tuples. We rely on patterns p and layouts \u03c1 to track the Imp places that correspond to Fun environments."}, {"heading": "Notations for the Translation from Fun to Imp:", "text": "p:: = l | () | (p, p) pattern: group of Imp locations to represent Fun value \u03c1:: = (xi 7 \u2192 pi) i \u01921.. n layout: finite map from Fun variables to patterns \u03a3 p: t in environment \u03a3, pattern p stands for Fun value of type t \u03a3 \u03c1: \u044b in environment \u03a3, layout \u03c1 stands for environment \u0432 M \u21d2 C, p given \u03c1, expression M translated for C and pattern p"}, {"heading": "Typing Rules for Patterns \u03a3 \u22a2 p : t and Layouts \u03a3 \u22a2 \u03c1 : \u0393:", "text": "The rules (PAT UNIT) represent products according to a pattern for their respective components. The rule (PAT LOC) requires that each entry in a pattern is mapped to a specific location. (PAT LOC) The rules (PAT LOC) represent values of the base type according to a specific location. (PAT UNIT) The rules (PAT UNIT) and (PAT PAIR) represent products according to a specific pattern for their respective components. (PAT LOC) The rule (PAT LOC) requires that each entry is mapped to a suitable pattern."}, {"heading": "5. ADDING ARRAYS AND COMPREHENSIONS", "text": "To be useful for machine learning, our language needs to support large data sets. To this end, we are adding arrays and understandings to Fun and Imp. We offer three examples, according to which we present the formal semantics based on unrolling. 5.1. Understanding examples in fun. Previously, we tried to estimate the skill level of three competitors in head-to-head games. Using understanding examples, we can model the skill level for any number of players and games:"}, {"heading": "TrueSkill:", "text": "The results: (bool * int * int * int * * *) is the result of a game between two players. < p (for players \u2192 random (Gaussian (10,00,0)))] for (w, p1, p2) in results dolet perf1 = random (Gaussian (Skills) [p1], 1.0))) let perf2 = random (Gaussian (Skills) distributions (Skills) for each player: we assume that skills are normally distributed by 10.0, with variance 20.0. Then we look at each of the results - this is the understanding of head-to-head matches. The result of the two games is an array of triples: one Boolean and two indexes. If the first index is true, then the second one represents the winner and the second one."}, {"heading": "Extended Syntax of Fun:", "text": "To simplify the formulation, we demand here that the body M of understanding contains neither an array nor an array that we understand; we do not measure the results.;.; Vn] array literal V1. [V2] r Indexing [for x inr V \u2192 M] comprehensionFirst, we do not consider arrays as a type: t [r] is an array of elements of type t across the range r. In the array type t [r] we require that type t does not contain an array of type t \u2032 [r \u2032], that is, we do not consider nested arrays. Indexing, V1. [V2] r, extracts elements from an array where the index V2 modulo expands the size of array V1. An understanding [for x inr V \u2192 M] maps about an array V that produces a new array V in which each element is determined by the evaluation of array V."}, {"heading": "Extended Typing Rules for Fun Expressions: \u0393 \u22a2 M : t", "text": "FUN ARRAY. FUN ARRAY. (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN ARRAY). (FUN). (FUN ARRAY). (FUN ARRAY). (FUN). (FUN). (FUN). (FUN). (FUN). (FUN). (FUN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). FUN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN). (UN)."}, {"heading": "Extended Syntax of Imp:", "text": "In such an iteration, any assignment to an array variable must be at index r. We also expand patterns to include area-indexed locations, and write (p1, p2) [r] for (p1 [r], p2 [r] for (p2]). Our compiler translates understandings about array-type variables as iterations about the translation of the understanding variable. We add that the understanding variable corresponds to the array variable indexed by the area. We invent a new array result pattern p \"and assign the result of the translated body to the translation of the understanding variable. Finally, we hide the local variables of the translation variable. This variable corresponds to the array variable indexed by the area. We invent a refreshing array result pattern p\" and assign the result of the translated body to the translation variable p. \""}, {"heading": "6. IMPLEMENTATION EXPERIENCE", "text": "We have written two backends for Imp: an exact inference algorithm based on a direct implementation of measurement transformers for discrete measurements, and an approximate inference algorithm for continuous measurements, using Infer.NET [37]. The translation of Section 4 formalizes our translation from Fun to Imp. Translating Imp to Infer.NET is relatively simple and runs on a syntax-oriented series of calls to Infer.NET's object-oriented API. The front end of our compiler accepts (a subset of) actual F # codes as its input. To do this, we use F # s reflective definitions that allow programmatic access to ASTs. This implementation strategy is beneficial in several ways."}, {"heading": "Summary of our Basic Test Suite:", "text": "LOC Observations Variables Time Naive Bayes 28 9 3 < 1sMixture 33 3 3 < 1s TrueSkill 68 15,664 84 6sadPredictor 78 300,752 299,594 3m30sIn summary, our implementation strategy allowed us to quickly and easily build an effective prototype: The entire compiler consists of only 2079 lines of F #; the Infer.NET backend consists of 600 lines; the discrete backend is 252 lines. However, our implementation is only a prototype and has limitations. Our discrete backend is limited to small models using only finite measurements. Infer.NET only supports a limited number of operations on specific combinations of probable and deterministic arguments. In the future, it would be useful to have an improved type system that is able to detect errors that arise from illegal combinations of operators in Infer.NET. There is no need to include an arrow definition option in the # 24."}, {"heading": "7. RELATED WORK", "text": "There is a long history of formal semantics for probable languages with sampling primitives, which are often combined with recursive computation. (One of the first semantics is for Probabilistic LCF [49], which, however, expands the core functional language LCF with weighted binary selections to define discrete distributions. (Apart from its inclusion of observations, Bernoulli Fun is another operative LCF and the other denotational semantics is descriptive LCF.) Kozen [27] develops probable semantics for currency programs with random allocations. He develops two demonstrably equivalent semantics and the other denotational semantics with partially ordered banach spaces. Imp is easier than Kozen's language, as Imp does not have unlimited statements, while the sectics of Imp need not deal with non-mination."}, {"heading": "8. CONCLUSION", "text": "We advocate probabilistic functional programming with observations and understandings as the modeling language for Bayesian thinking. We developed an idea-based system, invented new formal semantics to establish correctness, and evaluated the system based on a number of typical sequential problems. Our direct contribution is a strict semantics for a probabilistic programming language with zero probability observations for continuous variables. We have shown that probabilistic functional programs with iteration via arrays, but without the complexity of general recursion, represent a concise representation of complex probability distributions that arise in machine learning. One implication of our work for the machine learning community is that probabilistic programs can be written directly within an existing declarative language (Fun - a subset of F #), coupled with understandings to large data sets, and compiled at lower levels of Bayesian follow-up engines.We propose some new programming practices for our new directions."}, {"heading": "APPENDIX A. DETAILED PROOFS", "text": "Our evidence is structured as follows. \u2022 Appendix A.1 provides evidence for adoption 4.3. \u2022 Appendix A.2 provides evidence for theory 4.4.A.1. Evidence for adoption 4.3. We begin with a series of lemmas.Lemma A.1 (mitigating model agreement). (1) If the model agreement is mitigating. (2) If the model agreement is mitigating. (2) If the model agreement is mitigating. (3) If the model agreement is mitigating. (3) Appendix A.2:, then final, then final. (2) Appendix E: b (2)."}], "references": [{"title": "Reconciling two views of cryptography (the computational soundness of formal encryption)", "author": ["M. Abadi", "P. Rogaway"], "venue": "J. Cryptology, 15(2):103\u2013127,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Noncomputable conditional distributions", "author": ["N.L. Ackerman", "C.E. Freer", "D.M. Roy"], "venue": "LICS, pages 107\u2013116,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Proofs of randomized algorithms in Coq", "author": ["P. Audebaud", "C. Paulin-Mohring"], "venue": "Science of Computer Programming, 74(8):568\u2013589,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Formal certification of code-based cryptographic proofs", "author": ["G. Barthe", "B. Gr\u00e9goire", "S.Z. B\u00e9guelin"], "venue": "POPL, pages 90\u2013101. ACM,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A type theory for probability density functions", "author": ["S. Bhat", "A. Agarwal", "R.W. Vuduc", "A.G. Gray"], "venue": "J. Field and M. Hicks, editors, POPL, pages 545\u2013556. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Probability and Measure", "author": ["P. Billingsley"], "venue": "Wiley, 3rd edition,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Composable Probabilistic Inference with Blaise", "author": ["K.A. Bonawitz"], "venue": "PhD thesis, MIT,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Measure transformer semantics for Bayesian machine learning", "author": ["J. Borgstr\u00f6m", "A.D. Gordon", "M. Greenberg", "J. Margetson", "J. Van Gael"], "venue": "European Symposium on Programming (ESOP\u201911), volume 6602 of LNCS, pages 77\u201396. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic databases: diamonds in the dirt", "author": ["N.N. Dalvi", "C. R\u00e9", "D. Suciu"], "venue": "Commun. ACM, 52(7):86\u201394,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Markov logic", "author": ["P. Domingos", "S. Kok", "D. Lowd", "H. Poon", "M. Richardson", "P. Singla"], "venue": "L. De Raedt, P. Frasconi, K. Kersting, and S. Muggleton, editors, Probabilistic inductive logic programming, pages 92\u2013117. Springer-Verlag, Berlin, Heidelberg,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Functional pearls: Probabilistic functional programming in Haskell", "author": ["M. Erwig", "S. Kollmansberger"], "venue": "J. Funct. Program., 16(1):21\u201334,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "On the definition of probability densities and sufficiency of the likelihood map", "author": ["D.A.S. Fraser", "P. McDunnough", "A. Naderi", "A. Plante"], "venue": "J. Probability and Mathematical Statistics, 15:301\u2013310,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "A language and program for complex Bayesian modelling", "author": ["W.R. Gilks", "A. Thomas", "D.J. Spiegelhalter"], "venue": "The Statistician, 43:169\u2013178,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Church: a language for generative models", "author": ["N. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "Uncertainty in Artificial Intelligence (UAI\u201908), pages 220\u2013229. AUAI Press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "A modellearner pattern for Bayesian reasoning", "author": ["A.D. Gordon", "M. Aizatulin", "J. Borgstr\u00f6m", "G. Claret", "T. Graepel", "A. Nori", "S. Rajamani", "C. Russo"], "venue": "POPL, pages 403\u2013416,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft\u2019s Bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "International Conference on Machine Learning, pages 13\u201320,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic processes as concurrent constraint programs", "author": ["V. Gupta", "R. Jagadeesan", "P. Panangaden"], "venue": "POPL, pages 189\u2013202,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "TrueSkilltm: A Bayesian skill rating system", "author": ["R. Herbrich", "T. Minka", "T. Graepel"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201906), pages 569\u2013576,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Formal verification of probabilistic algorithms", "author": ["J. Hurd"], "venue": "PhD thesis, University of Cambridge, 2001. Available as University of Cambridge Computer Laboratory Technical Report UCAM\u2013CL\u2013TR\u2013566, May", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Probability Theory: The Logic of Science, chapter 15.7 The Borel-Kolmogorov paradox, pages", "author": ["E.T. Jaynes"], "venue": "CUP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "A probabilistic powerdomain of evaluations", "author": ["C. Jones", "G.D. Plotkin"], "venue": "Logic in Computer Science (LICS\u201989), pages 186\u2013195. IEEE Computer Society,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Embedded probabilistic programming", "author": ["O. Kiselyov", "C. Shan"], "venue": "Domain-Specific Languages, pages 360\u2013384,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Monolingual probabilistic programming using generalized coroutines", "author": ["O. Kiselyov", "C. Shan"], "venue": "Uncertainty in Artificial Intelligence (UAI\u201909),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic Graphical Models", "author": ["D. Koller", "N. Friedman"], "venue": "The MIT Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Effective Bayesian inference for stochastic programs", "author": ["D. Koller", "D.A. McAllester", "A. Pfeffer"], "venue": "AAAI/IAAI, pages 740\u2013747,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Semantics of probabilistic programs", "author": ["D. Kozen"], "venue": "Journal of Computer and System Sciences, 22(3):328\u2013350,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1981}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "venue": "IEEE Transactions on Information Theory, 47(2):498\u2013519,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Quantitative analysis with the probabilistic model checker PRISM", "author": ["M.Z. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Quantitative Aspects of Programming Languages (QAPL 2005), volume 153(2) of ENTCS, pages 5\u201331,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Quantifying information flow", "author": ["G. Lowe"], "venue": "CSFW, pages 18\u201331. IEEE Computer Society,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["D.J.C. MacKay"], "venue": "CUP,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201909), pages 1249\u20131257,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Abstraction, refinement and proof for probabilistic systems", "author": ["A. McIver", "C. Morgan"], "venue": "Monographs in computer science. Springer,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["F. McSherry"], "venue": "SIGMOD Conference, pages 19\u201330. ACM,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "On the formalization of the Lebesgue integration theory in HOL", "author": ["T. Mhamdi", "O. Hasan", "S. Tahar"], "venue": "Interactive Theorem Proving (ITP 2010),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Blog: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S.J. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "L. P. Kaelbling and A. Saffiotti, editors, IJCAI, pages 1352\u20131359. Professional Book Center,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Software available from http://research.microsoft.com/infernet", "author": ["T. Minka", "J. Winn", "J. Guiver", "A. Kannan"], "venue": "Infer.NET 2.3,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Expectation Propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Uncertainty in Artificial Intelligence (UAI\u201901), pages 362\u2013369. Morgan Kaufmann,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian Modeling Using WinBUGS", "author": ["I. Ntzoufras"], "venue": "Wiley,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Labelled Markov processes", "author": ["P. Panangaden"], "venue": "Imperial College Press,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "A probabilistic language based upon sampling functions", "author": ["S. Park", "F. Pfenning", "S. Thrun"], "venue": "POPL, pages 171\u2013 182. ACM,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "IBAL: A probabilistic rational programming language", "author": ["A. Pfeffer"], "venue": "B. Nebel, editor, International Joint Conference on Artificial Intelligence (IJCAI\u201901), pages 733\u2013740. Morgan Kaufmann,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "The design and implementation of IBAL: A general-purpose probabilistic language", "author": ["A. Pfeffer"], "venue": "L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Practical probabilistic programming", "author": ["A. Pfeffer"], "venue": "P. Frasconi and F. A. Lisi, editors, Inductive Logic Programming (ILP 2010), volume 6489 of Lecture Notes in Computer Science, pages 2\u20133. Springer,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Report on the probabilistic language scheme", "author": ["A. Radul"], "venue": "Proceedings of the 2007 symposium on Dynamic languages (DLS\u201907), pages 2\u201310. ACM,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic lambda calculus and monads of probability distributions", "author": ["N. Ramsey", "A. Pfeffer"], "venue": "POPL, pages 154\u2013165,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance makes the types grow stronger: A calculus for differential privacy", "author": ["J. Reed", "B.C. Pierce"], "venue": "ICFP, pages 157\u2013168,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "A First Look at Rigorous Probability Theory", "author": ["J.S. Rosenthal"], "venue": "World Scientific, 2nd edition,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic LCF", "author": ["N. Saheb-Djahromi"], "venue": "Mathematical Foundations of Computer Science (MFCS), volume 64 of LNCS, pages 442\u2013451. Springer,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1978}, {"title": "AutoBayes program synthesis system users manual", "author": ["J. Schumann", "T. Pressburger", "E. Denney", "W. Buntine", "B. Fischer"], "venue": "Technical Report NASA/TM\u20132008\u2013215366, NASA Ames Research Center,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Expert F", "author": ["D. Syme", "A. Granicz", "A. Cisternino"], "venue": "Apress,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic programming with Infer.NET", "author": ["J. Winn", "T. Minka"], "venue": "Machine Learning Summer School lecture notes, available at http://research.microsoft.com/~minka/papers/mlss2009/,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Variational message passing", "author": ["J.M. Winn", "C.M. Bishop"], "venue": "Journal of Machine Learning Research, 6:661\u2013694,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "An intuitive explanation", "author": ["E.S. Yudkowsky"], "venue": "Bayesian reasoning,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2003}], "referenceMentions": [{"referenceID": 12, "context": "The theme of this paper is the idea of expressing Bayesian models as probabilistic programs, which was pioneered by BUGS [14] and is recently gaining in popularity,", "startOffset": 121, "endOffset": 125}, {"referenceID": 48, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 104, "endOffset": 107}, {"referenceID": 34, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 127, "endOffset": 131}, {"referenceID": 50, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 154, "endOffset": 158}, {"referenceID": 42, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 180, "endOffset": 184}, {"referenceID": 40, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 201, "endOffset": 205}, {"referenceID": 39, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 233, "endOffset": 237}, {"referenceID": 10, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 243, "endOffset": 247}, {"referenceID": 43, "context": "witness the following list of probabilistic programming languages: AutoBayes [50], Alchemy [11], Blaise [7], BLOG [36], Church [15], Csoft [52], FACTORIE [32], Figaro [44], HANSEI [24], HBC [10], IBAL [42], \u03bb\u25e6 [41], Probabilistic cc [18], PFP [12], and Probabilistic Scheme [45].", "startOffset": 274, "endOffset": 278}, {"referenceID": 50, "context": "In particular, we draw inspiration from Csoft [52], an imperative language where programs denote factor graphs [28], data structures that support efficient inference algorithms [25].", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "In particular, we draw inspiration from Csoft [52], an imperative language where programs denote factor graphs [28], data structures that support efficient inference algorithms [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "In particular, we draw inspiration from Csoft [52], an imperative language where programs denote factor graphs [28], data structures that support efficient inference algorithms [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 35, "context": "NET [37], a software library for Bayesian reasoning.", "startOffset": 4, "endOffset": 8}, {"referenceID": 17, "context": "Consider a simplified form of TrueSkill [19], a large-scale online system for ranking computer gamers.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "A classic computational method to compute an approximate posterior distribution of each of the skills is Monte Carlo sampling [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 41, "context": "recent versions of IBAL [43], are based on nondeterministic inference using some form of Monte Carlo sampling.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "Inference algorithms based on factor graphs [28, 25] are an efficient alternative to Monte Carlo sampling.", "startOffset": 44, "endOffset": 52}, {"referenceID": 23, "context": "Inference algorithms based on factor graphs [28, 25] are an efficient alternative to Monte Carlo sampling.", "startOffset": 44, "endOffset": 52}, {"referenceID": 49, "context": "We designed Fun to be a subset of the F# dialect of ML [51], for implementation convenience: F# reflection allows easy access to the abstract syntax of a program.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "The technical report version of our paper [8] includes additional details, including the code of an F# implementation of measure transformers in the discrete case.", "startOffset": 42, "endOffset": 45}, {"referenceID": 39, "context": "[41]); the formal semantics for the general case comes later.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "If a subject is positive, what are the odds they have the disease? [54]", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "Borel\u2019s paradox [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "To give a formal semantics to such observations, as well as to mixtures of continuous and discrete distributions, we turn to measure theory, following standard sources [6, 48].", "startOffset": 168, "endOffset": 175}, {"referenceID": 46, "context": "To give a formal semantics to such observations, as well as to mixtures of continuous and discrete distributions, we turn to measure theory, following standard sources [6, 48].", "startOffset": 168, "endOffset": 175}, {"referenceID": 33, "context": "To machine-check our theory, one might build on a recent formalization of measure theory and Lebesgue integration in higher-order logic [35].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Given a finite measure \u03bc on T[[t]] and c \u2208 Vu, we let Fc : t \u2192 R be defined by the limit below (following [13]) Fc(d), lim i\u2192\u221e \u03bc(Rd \u2229 p(Bi))/\u03bbu(Bi) (3.", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "1) exists almost everywhere [13], that is, there is a set C with \u03bc(C) = 0 such that c \u2208 C if Fc(d) is undefined.", "startOffset": 28, "endOffset": 32}, {"referenceID": 3, "context": "This choice is a generalization of the (discrete) semantics of pWHILE [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 44, "context": "This contrasts with Ramsey and Pfeffer [46], where the semantics of an open program takes a variable valuation and returns a (monadic computation yielding a) distribution of return values.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "Our direct implementation of the measure transformer semantics, described in the technical report version of our paper [8], explicitly constructs the valuation.", "startOffset": 119, "endOffset": 122}, {"referenceID": 35, "context": "As another example, let us consider a simple Bayesian evaluation of a medical trial [37].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "The following example, due to Chung-Chieh Shan, highlighted regularity problems with our original definition of observation [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 26, "context": "NET by constructing a suitable factor graph [28], whose size will be linear in the size of the program.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "The implementation advantage of translating F# to Csoft, over simply generating factor graphs directly [32], is that the translation preserves the structure of the input model (including array processing in our full language), which can be exploited by the various inference algorithms supported by Infer.", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "NET [37].", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "As a third example, consider the adPredictor component of the Bing search engine, which estimates the click-through rates for particular users on advertisements [17].", "startOffset": 161, "endOffset": 165}, {"referenceID": 35, "context": "NET [37].", "startOffset": 4, "endOffset": 8}, {"referenceID": 47, "context": "One of the first semantics is for Probabilistic LCF [49], which augments the core functional language LCF with weighted binary choice, for discrete distributions.", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": ") Kozen [27] develops a probabilistic semantics for while-programs augmented with random assignment.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "Jones and Plotkin [22] investigate the probability monad, and apply it to languages with discrete probabilistic choice.", "startOffset": 18, "endOffset": 22}, {"referenceID": 44, "context": "Ramsey and Pfeffer [46] give a stochastic \u03bb -calculus with a measure-theoretic semantics in the probability monad, and provide an embedding within Haskell; they do not consider observations.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "The probabilistic concurrent constraint programming language Probabilistic cc of Gupta, Jagadeesan, and Panangaden [18] is also intended for describing probability distributions using independent sampling and constraints.", "startOffset": 115, "endOffset": 119}, {"referenceID": 31, "context": "McIver and Morgan [33] develop a theory of abstraction and refinement for probabilistic while programs, based on weakest preconditions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Ackerman, Freer, and Roy [2] show the uncomputability of conditional distributions in general, establishing limitations on constructive foundations of probabilistic programming.", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "Recent work on semantics of probabilistic programs within interactive theorem provers includes the mechanization of measure theory [20] and Lebesgue integration [35] in HOL, and a framework for proofs of randomized algorithms in Coq [3] which also allows for discrete observations.", "startOffset": 131, "endOffset": 135}, {"referenceID": 33, "context": "Recent work on semantics of probabilistic programs within interactive theorem provers includes the mechanization of measure theory [20] and Lebesgue integration [35] in HOL, and a framework for proofs of randomized algorithms in Coq [3] which also allows for discrete observations.", "startOffset": 161, "endOffset": 165}, {"referenceID": 2, "context": "Recent work on semantics of probabilistic programs within interactive theorem provers includes the mechanization of measure theory [20] and Lebesgue integration [35] in HOL, and a framework for proofs of randomized algorithms in Coq [3] which also allows for discrete observations.", "startOffset": 233, "endOffset": 236}, {"referenceID": 24, "context": "[26] proposed representing a probability distribution using first-order functional programs with discrete random choice, and proposed an inference algorithm for Bayesian networks and stochastic context-free grammars.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Their work was subsequently developed by Pfeffer into the language IBAL [43], which has observations and uses a factor graph semantics, but only works with discrete datatypes.", "startOffset": 72, "endOffset": 76}, {"referenceID": 39, "context": "[41] propose \u03bb\u25e6, the first probabilistic language with formal semantics applied to actual machine learning problems involving continuous distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "HANSEI [24, 23] is an embedding of a probabilistic language as a programming library in OCaml, based on explicit manipulation of discrete probability distributions as lists, and sampling algorithms based on coroutines.", "startOffset": 7, "endOffset": 15}, {"referenceID": 21, "context": "HANSEI [24, 23] is an embedding of a probabilistic language as a programming library in OCaml, based on explicit manipulation of discrete probability distributions as lists, and sampling algorithms based on coroutines.", "startOffset": 7, "endOffset": 15}, {"referenceID": 35, "context": "NET [37] is a software library that implements the approximate deterministic algorithms expectation propagation [38] and variational message passing [53], as well as Gibbs sampling, a nondeterministic algorithm.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "NET [37] is a software library that implements the approximate deterministic algorithms expectation propagation [38] and variational message passing [53], as well as Gibbs sampling, a nondeterministic algorithm.", "startOffset": 112, "endOffset": 116}, {"referenceID": 51, "context": "NET [37] is a software library that implements the approximate deterministic algorithms expectation propagation [38] and variational message passing [53], as well as Gibbs sampling, a nondeterministic algorithm.", "startOffset": 149, "endOffset": 153}, {"referenceID": 50, "context": "NET models are written in a probabilistic subset of C#, known as Csoft [52].", "startOffset": 71, "endOffset": 75}, {"referenceID": 43, "context": "Probabilistic Scheme [45] is a probabilistic form of the untyped functional language Scheme, limited to discrete distributions, and with a construct for reifying the distribution induced by a thunk as a value.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Church [15] is another probabilistic form of Scheme, equipped with conditional sampling and a mechanism of stochastic memoization.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "WinBUGS [39] is a popular implementation of the BUGS language [14] for explicitly describing distributions suitable for MCMC analysis.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "WinBUGS [39] is a popular implementation of the BUGS language [14] for explicitly describing distributions suitable for MCMC analysis.", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "FACTORIE [32] is a Scala library for explicitly constructing factor graphs.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Blaise [7] is a software library for building MCMC samplers in Java, that supports compositional construction of sophisticated probabilistic models, and decouples the choice of inference algorithm from the specification of the distribution.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "A recent paper [16] based on Fun describes a model-learner pattern which captures common probabilistic programming patterns in machine learning, including various sorts of mixture models.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 27, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 173, "endOffset": 181}, {"referenceID": 45, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 173, "endOffset": 181}, {"referenceID": 28, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "Probabilistic languages with formal semantics find application in many areas apart from machine learning, including databases [9], model checking [29], differential privacy [34, 47], information flow [30], and cryptography [1].", "startOffset": 223, "endOffset": 226}, {"referenceID": 38, "context": "A recent monograph on semantics for labelled Markov processes [40] focuses on bisimulation-based equational reasoning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "The syntax and semantics of Imp is modelled on the probabilistic language pWhile [4] without observations.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "Erwig and Kollmansberger [12] describe a library for probabilistic functional programming in Haskell.", "startOffset": 25, "endOffset": 29}], "year": 2013, "abstractText": "The Bayesian approach to machine learning amounts to computing posterior distributions of random variables from a probabilistic model of how the variables are related (that is, a prior distribution) and a set of observations of variables. There is a trend in machine learning towards expressing Bayesian models as probabilistic programs. As a foundation for this kind of programming, we propose a core functional calculus with primitives for sampling prior distributions and observing variables. We define measure-transformer combinators inspired by theorems in measure theory, and use these to give a rigorous semantics to our core calculus. The original features of our semantics include its support for discrete, continuous, and hybrid measures, and, in particular, for observations of zero-probability events. We compile our core language to a small imperative language that is processed by an existing inference engine for factor graphs, which are data structures that enable many efficient inference algorithms. This allows efficient approximate inference of posterior marginal distributions, treating thousands of observations per second for large instances of realistic models.", "creator": "LaTeX with hyperref package"}}}