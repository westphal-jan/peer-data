{"id": "1308.0702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2013", "title": "Universal Empathy and Ethical Bias for Artificial General Intelligence", "abstract": "Rational agents are usually built to maximize rewards. However, AGI agents can find undesirable ways of maximizing any prior reward function. Therefore value learning is crucial for safe AGI. We assume that generalized states of the world are valuable - not rewards themselves, and propose an extension of AIXI, in which rewards are used only to bootstrap hierarchical value learning. The modified AIXI agent is considered in the multi-agent environment, where other agents can be either humans or other \"mature\" agents, which values should be revealed and adopted by the \"infant\" AGI agent. General framework for designing such empathic agent with ethical bias is proposed also as an extension of the universal intelligence model. Moreover, we perform experiments in the simple Markov environment, which demonstrate feasibility of our approach to value learning in safe AGI.", "histories": [["v1", "Sat, 3 Aug 2013 14:40:36 GMT  (162kb)", "http://arxiv.org/abs/1308.0702v1", "AGI Impacts conference 2012 paper"]], "COMMENTS": "AGI Impacts conference 2012 paper", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexey potapov", "sergey rodionov"], "accepted": false, "id": "1308.0702"}, "pdf": {"name": "1308.0702.pdf", "metadata": {"source": "CRF", "title": "Universal Empathy and Ethical Bias for Artificial General Intelligence", "authors": ["Alexey Potapov", "Sergey Rodionov"], "emails": ["potapov@aideus.com", "rodionov@aideus.com"], "sections": [{"heading": "Universal Empathy and Ethical Bias for Artificial General Intelligence", "text": "Alexey Potapov1,2, Sergey Rodionov1,31AIDEUS, Russia2National Research University of Information Technology, Mechanics and Optics, St. Petersburg, Russia3Aix Marseille Universit\u00e9, CNRS, LAM (Laboratoire d'Astrophysique de Marseille) UMR 7326, 13388, Marseille, France {potapov, rodionov} @ aideus.com"}, {"heading": "Universal Empathy and Ethical Bias for Artificial General Intelligence", "text": "We believe that generalized states of the world are valuable - not rewards themselves - and therefore propose an extension of AIXI, in which rewards are only used to initiate hierarchical learning of values. AIXI is modified in a multi-agent environment where other agents can be either human or other \"mature\" agents whose values should be revealed and adopted by the \"young\" AGI. The general framework for designing such an empathic agent with ethical bias is also proposed as an extension of the universal intelligence model. Furthermore, we conduct experiments in the simple Markov environment that demonstrate the feasibility of our approach to value learning in safe AGI. Keywords: AIXI, safe AGI, empathy, representation, multi-agent environment"}, {"heading": "Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "General framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Universal intelligence approach", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Foster values", "text": "Consider the following most simplified and yet relevant case. Let Markov environment with a series of states (Q) Q (Q) Q (Q) + Q (Q) + Q (Q) + Q (Q) is given as additional reward. This environment is described by the matrix of probabilities), | (assP) of handing over to the state s \"from the state s\" after performing the reward a, and the matrix of rewards), | () 1 (assR) 1 (assR) level is a dangerous state, but it is not reflected in the reward function, so that the actor cannot simply stop the exploration. We want to follow it social values, even if the transfer of social rewards is stopped, but also the dynamic consideration of somatic rewards. Consider SARSA with its boring strategy."}, {"heading": "Multi-agent Markov environments", "text": "If these models are truly identifiable, and will this ethical bias be appropriate? Consider the first part of this question. To answer this question, one should compare the length of the description of the I / O presence. If these models are truly identifiable, it is a smaller agent if there is another agent presence, e.g. (Tan, 1993), (Choi & Ahn, 2010). However, conventional MARL implies that the maximization of rewards is the goal of any agent who can pursue cooperative or competitive strategies (or ignore the presence of all other agents). Here, we assume that only one of two agents attempts to implement fixed reward models (\"adult\" agents including humans may already know better values), and the task of another agent is to disclose the presence of that agent and act according to his abilities. As has been said, is the prior representation for multi-agent environments, including external value models, that will be \"ethical\" and \"will this question really be taken into account?"}, {"heading": "Empathic policies", "text": "Consider the Markov environment for two agents, in which one agent tries to reconstruct \"good\" states while another agent tries to maximize the function of the \"true\" value; the first agent must indicate which actions are more or less desirable for the second agent; more specifically, this can be formulated as follows: Let both agents receive appropriate rewards r (1) and r (2); the goal of the first agent is to maximize, say, r (1) + r (2) without directly receiving r (2); the first (empathic) agent requires a special exploration strategy to demonstrate the desirability of individual actions in each state; one can suggest the following simple exploration policy: \u2022 Perform the same action in the same state for some time without directly obtaining r (2). \u2022 Calculate frequency of visits in this state depends on the action. \u2022 Compare frequency of visits in dependence."}, {"heading": "Conclusion", "text": "We started from the assertion that generalized states of the world are valuable - not the rewards themselves. Thus, true values of states should be learned and tied to generalized representations; the active ingredient can be directly supplied with special rewards (from which it reconstructs \"true values\") or it can reconstruct which generalized states of the environment are desired by other actors who already have better value systems; the use of learned true values ensures that the active ingredient performs safe actions.We have made methodological considerations and proposed general mathematical models by introducing corresponding modifications in AIXI. These models cannot be applied directly in practice, but they provide an appropriate starting point. In particular, simplifications of these models have been implemented in Markov environments. Their experimental study has shown that the models developed are suitable to prove the presence of other active ingredients by reconstructing and adopting their values without permanently obtaining external \"true\" rewards."}, {"heading": "Acknowledgments", "text": "This work was supported by the Scholarship Council of the President of the Russian Federation (MD1072.2013.9) and the Ministry of Education and Science of the Russian Federation."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": null, "creator": null}}}