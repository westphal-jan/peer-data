{"id": "1308.0850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2013", "title": "Generating Sequences With Recurrent Neural Networks", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "histories": [["v1", "Sun, 4 Aug 2013 21:04:36 GMT  (3854kb,D)", "http://arxiv.org/abs/1308.0850v1", null], ["v2", "Sun, 11 Aug 2013 21:20:24 GMT  (3854kb,D)", "http://arxiv.org/abs/1308.0850v2", null], ["v3", "Fri, 21 Feb 2014 17:51:54 GMT  (3854kb,D)", "http://arxiv.org/abs/1308.0850v3", null], ["v4", "Wed, 2 Apr 2014 16:22:09 GMT  (3854kb,D)", "http://arxiv.org/abs/1308.0850v4", null], ["v5", "Thu, 5 Jun 2014 16:04:02 GMT  (3887kb,D)", "http://arxiv.org/abs/1308.0850v5", "Thanks to Peng Liu and Sergey Zyrianov for various corrections"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["alex graves"], "accepted": false, "id": "1308.0850"}, "pdf": {"name": "1308.0850.pdf", "metadata": {"source": "CRF", "title": "Generating Sequences With Recurrent Neural Networks", "authors": ["Alex Graves"], "emails": ["graves@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people who work for the rights of women and men have to burden themselves and their rights and obligations. It is not as if the women and men who work for the rights and obligations of men and women have the same rights and obligations as the men. It is as if the women and men who work for the rights and obligations of women have the same rights and obligations. It is as if the women and men who work for the rights and obligations of women and men have the same rights and obligations. It is as if the women and the rights of men and the rights of women and the rights of women are brought to the fore. It is as if the women and the rights of women and the rights of men and women are put at the centre. It is as if the rights of women and the rights of women are put at the centre, and it is as if the women are put in the centre."}, {"heading": "2 Prediction Network", "text": "In fact, it is not surprising that the PISA study, which examined the PISA results of the PISA study, so that the PISA results of the PISA study in the PISA study were incorporated into the PISA study. (...) The PISA study showed that the PISA results of the PISA study were incorporated into the PISA study by the PISA study. (...) The PISA study shows that the PISA results of the PISA study were incorporated into the PISA study. (...) The PISA study shows that the PISA results of the PISA study were incorporated into the PISA study. (...) The PISA study shows that the PISA study was incorporated into the PISA study. (...)"}, {"heading": "2.1 Long Short-Term Memory", "text": "In most RNNs, the hidden layer function H is an elementary application of a sigmoid function. However, we have found that the Long Short-Term Memory (LSTM) architecture [16], which uses purpose-bound memory cells to store information, is better at finding and exploiting extensive dependencies in the data. Fig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in this paper [7], H is implemented by the following composite function: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict \u2212 1 + bi) (7) ft = \u03c3 (Wxfxt + Whfht \u2212 1 + Wcfct \u2212 1 + bf) (8) ct = ftct \u2212 1 + it tanh (Wxcxt + Whcht \u2212 1 + bc) ot = Wcict \u2212 1 + bi (7) ft = Wcict + Whfht \u2212 1) \u2212 xf1 + Whfct \u2212 7 (Wxf1) \u2212 1 \u2212"}, {"heading": "3 Text Prediction", "text": "Text data is discrete and is typically presented to neural networks using \"onehot\" input vectors. That is, if there are altogether K text classes and the class k is fed in due course, then xt is a length-K vector whose inputs are all zero, except the kth, which is one. Pr (xt + 1 | yt) is therefore a multinomial distribution that can of course be parameterized by a softmax function at the output level: Pr (xt + 1 = k | yt) = ykt = exp (y-kt). Kk \u2032 = 1 exp (y-k \u2032 t). (12) Substitute into the equation. (6) We see that the prediction of L (x) = 1log y xt + 1 t (13) = \u21d2 L (x)."}, {"heading": "3.1 Penn Treebank Experiments", "text": "The first set of text predictions focused on the Penn Treebank part of the Wall Street Journal corpus, which is about networking, and this was a preliminary study whose main purpose was to measure the predictive power of the network rather than generate interesting sequences. Although a relatively small text corpus (a little more than a million words in total), the Penn Treebank database is widely used as a language modeling benchmark. It has a training volume of 930,000 words, validation contains 74,000 words, and the test sentence contains 82,000 words. The vocabulary is limited to 10,000 words, with all other words associated with a specific \"unknown word.\" The sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequ"}, {"heading": "3.2 Wikipedia Experiments", "text": "In 2006, Marcus Hutter, Jim Bowery, and Matt Mahoney organized the following challenge, commonly known as the Hutter Prize: In order to compress the first 100 million bytes of full English Wikipedia data (as it was at one point on March 3, 2006), the file had to include not only the compressed data, but also the code that implements word compression, so its size can be considered a benchmark for the minimum description length. [13] The data, which includes a two-part encryption of the data, is interesting because it not only contains a huge range of words from the dictionary, but also many character sequences that are not normally included in text corporations traditionally used for language modeling. For example, foreign words (including letters from non-Latin alphabets such as Arabic and Chinese), XML are used to define metadress data."}, {"heading": "By the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the in tention of navigation the ISBNs, all encoding [[Transylvania International Organ isation for Transition Banking|Attiking others]] it is in the westernmost placed", "text": "[[Mt. Costall Leyton]] is one of the two largest Calashia universities in the world, all of which are equipped with the usual calorie-fed clipper. [[Astr onomical Classification Railway]] LACN645 Snowshore val - a rocket that has not been used by every club in the last century. [[Astr onomical Classification Railway]] I 'is another communist submarine & quot; a [[Della missile]] more than the [[Royal Society]] (12-258): & quot; Glide sun wag [[lubricant]]."}, {"heading": "In the United States, there is no hard resort in computation significantly.", "text": "In [[1868]], the [[Italo Capital Territories Unit]] began construction of the [[Continental Rail Way Centre]], and the students wrote against the deployment of the [[elective missile]]; [[Steam]], a [[quot; 20 to night & quot; and [[Fia Citation Quantity Logo]] (since 1967); [[Steam]], a $175 million [[Lochroom River | Tri-]] missile system resolution specified by [[Quarry]]; and [[Lochroom River | Tri-]], an English-language network that struggled with the Lombard capital city of Silvio and Murray from 1985 to 1999."}, {"heading": "In [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Handwriting Prediction", "text": "In this context, it should be noted that the writing is recorded as a sequence of time zones deducted from the Online Manuscript Database (IAM-OnDB). [21] IAM-OnDB consists of handwritten lines collected by the authors. [21] The authors were asked to write forms from the Online Manuscript Database (IAM-OnDB). [21] The IAM-OnDB consists of handwritten lines collected by the authors. [21] The authors were asked to write forms out of time."}, {"heading": "4.1 Mixture Density Outputs", "text": "The idea of mixing density networks [2] is to use the outputs of a neural network to parameterise a mix distribution. A subset of outputs is used to define the mixing weights, while the remaining outputs are used to parameterise the individual mixing components. Mixing weight outputs are normalized with a softmax function to ensure that they form a valid discrete distribution, and the other outputs are guided by appropriate functions to keep their values within a reasonable range (for example, the exponential function is typically applied to outputs that must be positive). Mixing density networks are trained by maximizing the log probability density of the targets among the induced distributions."}, {"heading": "4.2 Experiments", "text": "Each point in the data sequences consisted of three numbers: the x and y offset over the previous point and the binary balance = 42. The network input layer was therefore size 3. Coordinate compensation was normalized to 0, dev. 1 over the training set. 20 mixing components were used to model the offsets, giving a total of 120 mixing parameters per time step (20 weights, 40 averages, 40 standard deviations, and 20 correlations). Another parameter was used to model the probability at the end of the training set, giving an output layer of size 121. Two network architectures were compared for the hidden layers: one with three hidden layers, each consisting of 400 LSTM cells, and one with a single hidden layer of 900 LSTM cells. Both networks had about 3.4M weights. Both networks had a three-layer network, which is the i balance."}, {"heading": "1 layer none -1025.7 0.40", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 layer none -1041.0 0.41", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 layer adaptive weight noise -1057.7 0.41", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3 Samples", "text": "Fig. 11 shows examples of handwriting generated by the prediction network. It appears that the network has learned to model strokes, letters, and even short words (especially common ones like \"of\" and \"the\"). It also appears to have learned a basic language model at the sign level, since the words it invented (\"eald,\" \"bryoes,\" \"lenrest\") seem reasonably plausible in English. Given that the average character takes more than 25 time periods, this once again demonstrates the network's ability to generate coherent long-distance structures."}, {"heading": "5 Handwriting Synthesis", "text": "Handwriting synthesis is the generation of handwriting for a given text. Clearly, the prediction networks described so far are not able to do so, as there is no way to restrict which letters the network writes. This section describes an extension that allows a prediction network to generate data sequences based on a high-grade annotation sequence (a string, in the case of handwriting synthesis), the resulting sequences being sufficiently convincing that they are often indistinguishable from the real handwriting. Furthermore, this realism is achieved without sacrificing the variety of writing style shown in the previous section. The biggest challenge in conditioning predictions to the text is that the alignment between the two is not known until the data is generated. A neural network model that is able to make sequential predictions based on two sequences of different length is the RNN converter [9] which allows transducers to make two choices, but not the one."}, {"heading": "5.1 Synthesis Network", "text": "As with the forecast, the hidden layers are not stacked on top of each other, but scattered up to the top level, and there are no connections from the inputs to all the hidden layers and from all the hidden layers to the outputs. The difference is defined by the following discrete convolution with a mixture of K-1 functions. (t, u) The length U-2 data sequence x, the soft window wt in c at the outputs. (1) The following discrete convolution with a mixture of K-1 functions is defined. (t, u) The length U-2 data sequence x, the soft window wt in c. (46) wt = U-2-2 (t, u). (47) Where the constellation (t, u) the window weights of cu are timestep."}, {"heading": "5.2 Experiments", "text": "The full transcriptions contain 80 different characters (uppercase, lowercase, digits and punctuation).The network architecture, however, was as similar to the best prediction network as possible: three hidden layers of 400 LSTM cells each, 20 bivariable Gaussian mixture components at the output level, and one input layer of size 3. The character sequence was coded with a uniform \"non-letter\" label. The network architecture was as similar to the best prediction network as possible: three hidden layers of 400 LSTM cells each, 20 bivariable Gaussian mixture components at the output level, and an input layer of size 3. The character sequence was coded with uniform vectors, and therefore the window vectors were able to resize 57. A mixture of 10 Gaussian functions was used for the window parameters requiring a size of 30 parameters. The total number of network weights was increased to approximately 3.4 with the weight of the network."}, {"heading": "5.3 Unbiased Sampling", "text": "Since c, an unbiased sample can be selected from Pr (x | c) by iteratively dragging xt + 1 from Pr (xt + 1 | yt), just like for the predictive network. The only difference is that we also have to decide when the synthesis network has finished writing the text and should not make future errors. To do this, we use the following heuristics: Once \u03c6 (t, U + 1) > \u03c6 (t, u), the current input text is defined as the end of the sequence and the sampling ends. Examples of unbiased synthesis samples are shown in Fig. 15. These and all subsequent numbers were generated using the synthesis network with adaptive weight noise. Note how stylistic features such as font size, slant, italicity, etc., the distortion system, etc. that arises from the unforeseen text by creating a disordered, disordered, disordered, disordered, disordered, disordered, disordered, etc."}, {"heading": "5.4 Biased Sampling", "text": "One problem with unbiased samples is that they tend to be hard to read (partly because real handwriting is hard to read, and partly because the network is an imperfect model). Intuitively, we would expect the network to give a good handwriting a higher probability because it tends to be smoother and more predictable than bad handwriting. If true, we should aim to output more likely elements of Pr (x | c) if we want the samples to be easier to read independently of each other. A principle-based search for highly likely samples could lead to a difficult problem of inference, since the probability of any output depends on all previous results. However, simple heuristics, where the sample tends to make more likely predictions at each step, generally show good results. Define the probability default b as a real number greater than or equal to zero. Before subtracting a sample from Pr (xt + 1 | yt), any error in the mix will be calculated as any deviation from the palate."}, {"heading": "5.5 Primed Sampling", "text": "Another reason to restrict scanning would be to create a handwriting in the style of a particular writer (and not in a randomly chosen style); the easiest way to do this would be to retrain only that writer; but even without retraining, it is possible to imitate a particular style by \"priming\" the network with a real sequence and then creating an extension with the real sequence that is still in the network's memory; this can be achieved for a real x, c and a synthesis string by setting the string to c + s and jamming the data inputs for the first T periods to x, then scanning as usual until the sequence ends. Examples of prepared samples are shown in Fig. 18 and 19. The fact that priming works proves that the network is able to memorize stylistic features previously identified in the sequence; this technique never seems to work better for the scans seen in the training data."}, {"heading": "6 Conclusions and Future Work", "text": "This work has shown that long short-term memory recurrent neural networks are capable of generating both discrete and real sequences with a complex, far-reaching structure by making predictions in the next step. Furthermore, a novel revolutionary mechanism has been introduced that allows a recursive network to condition its predictions to an additional annotation sequence and to use this approach to synthesize multiple and realistic examples of online handwriting. Furthermore, it has shown how these samples can be biased due to the larger dimensionality of the data points and how they can be modelled based on the style of a particular writer. Another is to gain a better insight into the internal representation of the data and use it to directly manipulate the sample distribution. It would also be interesting to develop a mechanism to automatically extract high-grade annotations from these examples in order to write more information than just string them together."}, {"heading": "Acknowledgements", "text": "Thanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geoffrey Hinton and other colleagues from the University of Toronto for numerous useful comments and suggestions. This work was supported by a Global Scholarship from the Canadian Institute for Advanced Research."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Mixture density networks", "author": ["C. Bishop"], "venue": "Technical report", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural Networks for Pattern Recognition", "author": ["C. Bishop"], "venue": "Oxford University Press, Inc.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the Twenty-nine International Conference on Machine Learning (ICML\u201912)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Ian", "author": ["J.G. Cleary"], "venue": "and I. H. Witten. Data compression using adaptive coding and partial string matching. IEEE Transactions on Communications, 32:396\u2013402", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F. Gers", "N. Schraudolph", "J. Schmidhuber"], "venue": "Journal of Machine Learning Research, 3:115\u2013143", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "ICML Representation Learning Worksop", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18:602\u2013610", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems, volume 21", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "The Minimum Description Length Principle (Adaptive Computation and Machine Learning)", "author": ["P.D. Gr\u00fcnwald"], "venue": "The MIT Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G. Hinton"], "venue": "Technical report", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-term Dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A Field Guide to Dynamical Recurrent Neural Networks", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "The Human Knowledge Compression Contest", "author": ["M. Hutter"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "An analysis of noise in recurrent neural networks: convergence and generalization", "author": ["K.-C. Jim", "C. Giles", "B. Horne"], "venue": "Neural Networks, IEEE Transactions on, 7(6):1424 \u20131438", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "and G", "author": ["S. Johansson", "R. Atwell", "R. Garside"], "venue": "Leech. The tagged LOB corpus user\u2019s manual; Norwegian Computing Centre for the Humanities", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "A machine learning perspective on predictive coding with paq", "author": ["B. Knoll", "N. de Freitas"], "venue": "CoRR, abs/1108.3298,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard", "author": ["M. Liwicki", "H. Bunke"], "venue": "Proc. 8th Int. Conf. on Document Analysis and Recognition, volume 2, pages 956\u2013 961", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS, 19(2):313\u2013330", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "H. Le", "S. Kombrink", "J. Cernocky"], "venue": "Technical report, Unpublished Manuscript", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["A. Mnih", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, volume 21", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "Proceedings of the 29th International Conference on Machine Learning, pages 1751\u20131758", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Lowrank matrix factorization for deep neural network training with highdimensional output targets", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. ICASSP", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Better generative models for sequential data problems: Bidirectional recurrent mixture density networks", "author": ["M. Schuster"], "venue": "pages 589\u2013595. The MIT Press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "pages 1601\u20131608", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "ICML", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["G.W. Taylor", "G.E. Hinton"], "venue": "Proc. 26th Annual International Conference on Machine Learning, pages 1025\u20131032", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["R. Williams", "D. Zipser"], "venue": "In Back-propagation: Theory, Architectures and Applications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1995}], "referenceMentions": [{"referenceID": 3, "context": "Recurrent neural networks (RNNs) are a rich class of dynamic models that have been used to generate sequences in domains as diverse as music [6, 4], text [30] and motion capture data [29].", "startOffset": 141, "endOffset": 147}, {"referenceID": 28, "context": "Recurrent neural networks (RNNs) are a rich class of dynamic models that have been used to generate sequences in domains as diverse as music [6, 4], text [30] and motion capture data [29].", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "Recurrent neural networks (RNNs) are a rich class of dynamic models that have been used to generate sequences in domains as diverse as music [6, 4], text [30] and motion capture data [29].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "This distinguishes them from n-gram models and compression algorithms such as Prediction by Partial Matching [5], whose predictive distributions are determined by counting exact matches between the recent history and the training set.", "startOffset": 109, "endOffset": 112}, {"referenceID": 13, "context": "In practice however, standard RNNs are unable to store information about past inputs for very long [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "One remedy that has been proposed for conditional models is to inject noise into the predictions before feeding them back into the model [31], thereby increasing the model\u2019s robustness to surprising inputs.", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "Long Short-term Memory (LSTM) [16] is an RNN architecture designed to be better at storing and accessing information than standard RNNs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "LSTM has recently given state-of-the-art results in a variety of sequence processing tasks, including speech and handwriting recognition [10, 12].", "startOffset": 137, "endOffset": 145}, {"referenceID": 10, "context": "LSTM has recently given state-of-the-art results in a variety of sequence processing tasks, including speech and handwriting recognition [10, 12].", "startOffset": 137, "endOffset": 145}, {"referenceID": 0, "context": "by reducing the number of processing steps between the bottom of the network and the top, and thereby mitigating the \u2018vanishing gradient\u2019 problem [1].", "startOffset": 146, "endOffset": 149}, {"referenceID": 31, "context": "The partial derivatives of the loss with respect to the network weights can be efficiently calculated with backpropagation through time [33] applied to the computation graph shown in Fig.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "(LSTM) architecture [16], which uses purpose-built memory cells to store information, is better at finding and exploiting long range dependencies in the data.", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "For the version of LSTM used in this paper [7] H is implemented by the following composite function:", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "The original LSTM algorithm used a custom designed approximate gradient calculation that allowed the weights to be updated after every timestep [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "However the full gradient can instead be calculated with backpropagation through time [11], the method used in this paper.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "In the case of softmax models, a further difficulty is the high computational cost of evaluating all the exponentials during training (although several methods have been to devised make training large softmax layers more efficient, including tree-based models [25, 23], low rank approximations [27] and stochastic derivatives [26]).", "startOffset": 260, "endOffset": 268}, {"referenceID": 21, "context": "In the case of softmax models, a further difficulty is the high computational cost of evaluating all the exponentials during training (although several methods have been to devised make training large softmax layers more efficient, including tree-based models [25, 23], low rank approximations [27] and stochastic derivatives [26]).", "startOffset": 260, "endOffset": 268}, {"referenceID": 25, "context": "In the case of softmax models, a further difficulty is the high computational cost of evaluating all the exponentials during training (although several methods have been to devised make training large softmax layers more efficient, including tree-based models [25, 23], low rank approximations [27] and stochastic derivatives [26]).", "startOffset": 294, "endOffset": 298}, {"referenceID": 24, "context": "In the case of softmax models, a further difficulty is the high computational cost of evaluating all the exponentials during training (although several methods have been to devised make training large softmax layers more efficient, including tree-based models [25, 23], low rank approximations [27] and stochastic derivatives [26]).", "startOffset": 326, "endOffset": 330}, {"referenceID": 28, "context": "Character-level language modelling with neural networks has recently been considered [30, 24], and found to give slightly worse performance than equivalent word-level models.", "startOffset": 85, "endOffset": 93}, {"referenceID": 22, "context": "Character-level language modelling with neural networks has recently been considered [30, 24], and found to give slightly worse performance than equivalent word-level models.", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "The first set of text prediction experiments focused on the Penn Treebank portion of the Wall Street Journal corpus [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "Since both networks overfit the training data, we also experiment with two types of regularisation: weight noise [18] with a std.", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "075 applied to the network weights at the start of each training sequence, and adaptive weight noise [8], where the variance of the noise is learned along with the weights using a Minimum description Length (or equivalently, variational inference) loss function.", "startOffset": 101, "endOffset": 104}, {"referenceID": 21, "context": "Overall the results compare favourably with those collected in Tomas Mikolov\u2019s thesis [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following challenge, commonly known as Hutter prize [17]: to compress the first 100 million bytes of the complete English Wikipedia data (as it was at a certain time on March 3rd 2006) to as small a file as possible.", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "Its size can therefore be considered a measure of the minimum description length [13] of the data using a two part coding scheme.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "This form of truncated backpropagation has been considered before for RNN language modelling [23], and found to speed up training (by reducing the sequence length and hence increasing the frequency of stochastic weight updates) without affecting the network\u2019s ability to learn long-range dependencies.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "variant of the PAQ-8 compression algorithm [20]) achieves 1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "47 when the RNN was combined with a maximum entropy model [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 28, "context": "Being able to correctly open and close quotation marks and parentheses is a clear indicator of a language model\u2019s memory, because the closure cannot be predicted from the intervening text, and hence cannot be modelled with shortrange context [30].", "startOffset": 242, "endOffset": 246}, {"referenceID": 19, "context": "All the data used for this paper were taken from the IAM online handwriting database (IAM-OnDB) [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "The writers were asked to write forms from the Lancaster-Oslo-Bergen text corpus [19], and the position of their pen was tracked using an infra-red device in the corner of the board.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "The idea of mixture density networks [2, 3] is to use the outputs of a neural network to parameterise a mixture distribution.", "startOffset": 37, "endOffset": 43}, {"referenceID": 2, "context": "The idea of mixture density networks [2, 3] is to use the outputs of a neural network to parameterise a mixture distribution.", "startOffset": 37, "endOffset": 43}, {"referenceID": 12, "context": "Note that the densities are normalised (up to a fixed constant) and are therefore straightforward to differentiate and pick unbiased sample from, in contrast with restricted Boltzmann machines [14] and other undirected models.", "startOffset": 193, "endOffset": 197}, {"referenceID": 26, "context": "Mixture density outputs can also be used with recurrent neural networks [28].", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "The three layer network was retrained with adaptive weight noise [8], with all std.", "startOffset": 65, "endOffset": 68}, {"referenceID": 30, "context": "The networks were trained with rmsprop: a form of stochastic gradient descent where the gradients are divided by a running average of their recent magnitude [32].", "startOffset": 157, "endOffset": 161}, {"referenceID": 7, "context": "One neural network model able to make sequential predictions based on two sequences of different length is the RNN transducer [9].", "startOffset": 126, "endOffset": 129}], "year": 2014, "abstractText": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "creator": "LaTeX with hyperref package"}}}