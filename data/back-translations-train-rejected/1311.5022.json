{"id": "1311.5022", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2013", "title": "Extended Formulations for Online Linear Bandit Optimization", "abstract": "On-line linear optimization on combinatorial action sets (d-dimensional actions) with bandit feedback, is known to have complexity in the order of the dimension of the problem. The exponential weighted strategy achieves the best known regret bound that is of the order of $d^{2}\\sqrt{n}$ (where $d$ is the dimension of the problem, $n$ is the time horizon). However, such strategies are provably suboptimal or computationally inefficient. The complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set. Mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition. Entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. Here, we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called `extended formulation'. Such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis. We show that with such formulations, exponential weighting can achieve logarithmic dependence on the dimension of the set.", "histories": [["v1", "Wed, 20 Nov 2013 11:39:26 GMT  (357kb,D)", "http://arxiv.org/abs/1311.5022v1", null], ["v2", "Tue, 26 Nov 2013 12:25:29 GMT  (336kb,D)", "http://arxiv.org/abs/1311.5022v2", null], ["v3", "Wed, 30 Sep 2015 16:43:29 GMT  (335kb,D)", "http://arxiv.org/abs/1311.5022v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["shaona ghosh", "adam prugel-bennett"], "accepted": false, "id": "1311.5022"}, "pdf": {"name": "1311.5022.pdf", "metadata": {"source": "META", "title": "Extended Formulations for Online Linear Bandit Optimization ", "authors": [], "emails": [], "sections": [{"heading": null, "text": "000 002 003 004 005 007 008 009 010 011 013 014 015 016 017 019 020 021 022 024 025 027 028 029 031 032 034 035 036 038 039 041 042 044 045 045 048 049 050 052 053 054055 056 058 059 061 063 064 066 067 067 070 072 074 075 078 079 079 080 082 086 087 089 091 092 093 096 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of us are able to hide when they are in a position to outdo ourselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "1.1. Contribution", "text": "Specifically, our contributions are the following: \u2022 We present the \"advanced\" exponentially weighted algorithm. Our algorithm has an exponential weighting method for the various actions; the weights in each round form the slack matrix (extended formulation). \u2022 We introduce an inherent regularity measurement that weights the actions, which is a strikingly good indicator of the repentance gap. Our method defines slack variables or inequalities given by the linear scale loss in each round. \u2022 We introduce an inherent regularity measurement that weights the actions, which is a strikingly good indicator of the repentance gap. Our method defines slack variables or inequalities given by the linear scale loss in each round."}, {"heading": "2. Relation with Previous Work", "text": "They have also shown that without further assumptions, for the use of a limited log, the limit is not unlikely for A = [\u2212 1] d. This exploration strategy was later demonstrated by Bianchi et al. (2012), for combinatorial symmetrical action sets (2012), by using a uniform distribution across actions, while proving that regret is tied to order. (2012) This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetrical action sets (2012), by using a uniform distribution across actions, while proving that regret is tied to order, is not unlikely in general for concrete decisions of A; but the limit is suboptimal in the route planning. (2008) and the strategy of Bianchi is (2012)."}, {"heading": "2.1. Extended Formulations", "text": "An extended formulation (Conforti et al., 2010) is a way to present the practicable set of solutions, which is, of course, exponential in the size of the data, up to a formulation polynomial in the natural parameters of the problem by introducing a polynomial number of new variables. Where possible, the extended formulation or its approximation simplifies the computational complexity associated with the problem in the paradigm of linear programming. Yannakakis \"(1991) groundbreaking work naturally characterizes the size of the extended formulation by indicating the smallest size of the extension, which well represents the original set. Recent work by Gouveia et al. (2011) and Fiorini et al. (2012) generalizes the theorem from the paradigm of linear programming to convex optimization. The techniques are also known as Lift-and-Project (Balas et al., 1993) and Linear Techniques (Chandrakarania 2013) dealing with this structural optimization."}, {"heading": "3. Extended Formulations in Linear Bandits", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Setup", "text": "In the formal constellation of the combinatorial bandit optimization problem, we consider an adversarial environment with limited feedback. The prediction game, as described in the preceding sections, proceeds in a series of rounds t = 1,.., T. The plot set forms a subset of a hypercube A [0, 1] d that includes a series of all possible actions, d being the dimension of the hypercube. If the feasible solutions to a combinatorial optimization problem are encoded as 0 / 1 points in Rd, they result in a convex envelope polytope of the resulting points. The loss is as-Ov3V2V1P QAv \u2264 b3 3Av \u2264 b1 (a) (-1, -1) (-1.1) (1, -1) PQO (b) Figure 1."}, {"heading": "3.2. Slack based Regularization", "text": "t definition 1. P = conv 349 351 354 358 358 358 359 371 371 364 360."}, {"heading": "4. Algorithms and Results", "text": "In algorithm 1, an exponentially weighted technique is used to weight the actions based on the uniformity measure defined in definition 2. This defines the weight matrix or the sleep matrix Mt + 1 in each round that has a positive rank. Definition 4. As long as there is a positive rank for the weight matrix Mt, a guaranteed non-negative factorization is possible, so that M = TU, where T and Ualgorithm 2 have an extended extended Exp2 with sampling input. Let the initial covariance matrix P1 = cov (A) set the dimensional rank (A). Repeat p1 = (1,.., 1). R | A | p |. Let the non-negative rank r =. Let the initial covariance matrix P1 = cov (A), set the dimensional rank (A). Repeat p1 = p. Repeat p1. R | A | A | p = p. Let the non-negative rank r =. Let the initial covariance matrix P1 = cov (A) p p p p p p p p p = cov (A), set the dimensional rank (A). Repeat p1, p = p = p = 1 to do pi, t p1, p = p on, T, p = p = p, T, p = k, p = p = k, p = p = p p p, p = p = 1, p at, p = p = p = p p p = p p p p p, p = p p p = p = p p p, p = 1, p p p = p = p p = p = p, p, p at, p = p = p = p = p = p, p = p, p = 1, p = p = p, p = p = p = p, p = p at, p, p = p = p = p, p = p, p = p = t, p = t, p = p = p = p = p = p."}, {"heading": "4.1. Sampling of low rank approximations", "text": "The definition 4 leads directly to the technique of lower rank approximations of the action platform. Typically, in the linear optimization algorithms 478 the decisive step is the loss-estimation-matrices-matrices-matrices-matrices-matrices-matrices-matrices-matrices-matrices-matrices-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern-pattern"}, {"heading": "4.2. Complexity of Extension", "text": "It turns out that the non-negative rank of the sleep matrix S of the order log2 d is the expansion complexity of the polytopia (in our case the hypercube) of action P. In reality, the non-negative rank of a sleep matrix S of the order log2 d is the expansion complexity of the dimensionality. The rank of the sleep matrix provides lower limits for the complexity of the expansion. The lower limit of the non-negative rank for a regular n-gon with an Rr + lift is given by r = O (log2 (d)) (Gouveia et al., 2011)."}, {"heading": "4.3. Information Theoretic - Entropic treatment", "text": "Although, unlike mirror scenario algorithms, we are in the exponential weight setting, where an entropic regulatory function is carefully selected to impose the marginal penalty, regulation is inherent in our case to the treatment of the problem. It has been shown that a randomized communication protocol can result in a stronger lower limit in the order of the logarithm of the non-negative position of the expectant calculated slack matrix (communication matrix) (Faenza et al., 2012). Moreover, the uniform sampling of the non-negative ranking index ri in our case has a corresponding entropy H (ri) = log | S |. In fact, the uniform sampling of the low-rank Pri matrices with bias has an entropy H (wt) = wt log 1wt (1 \u2212 wt) log (1 \u2212 wt), which is similar to the entropy in the case of the mirror scenario."}, {"heading": "4.4. Analysis", "text": "Lemma 4.1. The learning rate on both sides of inequality is 0, for all an \"A\" problem and the \"A\" opponent playing with the law (Section A.1.1), we have an \"A\" problem, we have an \"A\" problem, \"we have an\" A \"problem,\" we have an \"A\" problem, \"we have an\" A \"problem,\" we have an \"A\" problem, \"we have an\" A \"problem,\" we have an \"A\" problem, \"we have an\" A problem, \"we have an\" A problem, \"we have an\" A problem, \"we have an\" A problem, \"we have an\" A problem, \"we have an\" (Section A.1.1), we have an \"L problem.\""}, {"heading": "5. Experiments", "text": "We compared the advanced exponentially weighted approach with state-of-the-art exponentially weighted algorithms in the opposing linear optimization band setting."}, {"heading": "5.1. Simulations", "text": "In the first experiment, in the middle of a d-dimensional network of routes, the optimal route through the learning algorithm should be selected. Typically, we choose d to vary between 10 and 15. The environment is an oblivious opponent of the player's actions, simulated to choose fixed but unknown losses in each round. Losses are limited and within range [0, 1]. The learning algorithm is executed using our basic Exp algorithm. Each action is represented by a d-dimensional incident vector indicating whether there is an edge or a path in the route."}, {"heading": "5.2. Empirical Datasets", "text": "The dataset is the Jester Online Joke Recommendation dataset (Goldberg, 2003) from the University of Berkeley, which collects data from 24,983 users with ratings of 36 or more jokes. We look at the ratings of 24,983 users for 20 jokes that form the dense matrix. Ratings vary in the range [\u2212 10.00, 10.00], including unrated jokes. We scale and normalize ratings in the range [0, 1]. For the purpose of our problem, each user presents a d-dimensional decision problem, with d = 20. We do not make the ratings available to the algorithm, instead the ratings are provided by the environment based on the user. In other words, we assume that the non-conscious attitude towards the opponent changes when the user or action selected is selected. The aim of the algorithm is to identify the user who has rated the worst ratings on all jokes."}, {"heading": "6. Conclusion and Future Work", "text": "96 949 929 928 949 96 96 996 96 96 96 96 96 96 28 898 957 96 96 96 96 96 996 996 28 949 28 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 96 949 949 949 949 949 949 949 96 949 949 949 949 96 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 949 996 949 949 949 949 949 949 949 949 949 949 949 949 949 949 996 949 949 949 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 949 949 949 949 949 949 949 949 949 949 949 949 996 949 949 996 996 949 949 996 996 949 996 996 949 996 996 996 996 996 996 949 996 996 996 996 996 996 949 996 996 996 996 949 996 996 996 996 996 996 949 996 996 996 949 996 949 996 996 996 996 996 996 949 996 949 949 949 996 949 996 949 996 996 949 949 949 996 949 996 996 996"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Abernethy", "Jacob", "Hazan", "Elad", "Rakhlin", "Alexander"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Minimax policies for combinatorial prediction games", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien", "Lugosi", "G\u00e1bor"], "venue": "arXiv preprint arXiv:1105.4871,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A lift-and-project cutting plane algorithm for mixed 0\u20131 programs", "author": ["Balas", "Egon", "Ceria", "Sebasti\u00e1n", "Cornu\u00e9jols", "G\u00e9rard"], "venue": "Mathematical programming,", "citeRegEx": "Balas et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Balas et al\\.", "year": 1993}, {"title": "An elementary introduction to modern convex geometry", "author": ["Ball", "Keith"], "venue": "Flavors of geometry,", "citeRegEx": "Ball and Keith.,? \\Q1997\\E", "shortCiteRegEx": "Ball and Keith.", "year": 1997}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo", "Kakade", "Sham M"], "venue": "JMLR Workshop and Conference Proceedings Volume", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["Cesa-Bianchi", "Nicolo"], "venue": null, "citeRegEx": "Cesa.Bianchi and Nicolo.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Nicolo.", "year": 2006}, {"title": "Combinatorial bandits", "author": ["Cesa-Bianchi", "Nicolo", "Lugosi", "G\u00e1bor"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Mirror descent meets fixed share (and feels no regret)", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Gaillard", "Pierre", "Lugosi", "G\u00e1bor", "Stoltz", "Gilles"], "venue": "arXiv preprint arXiv:1202.3323,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Chandrasekaran", "Venkat", "Jordan", "Michael I"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2013}, {"title": "Extended formulations in combinatorial optimization", "author": ["Conforti", "Michele", "Cornu\u00e9jols", "G\u00e9rard", "Zambelli", "Giacomo"], "venue": null, "citeRegEx": "Conforti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Conforti et al\\.", "year": 2010}, {"title": "Extended formulations, non-negative factorizations and randomized communication protocols", "author": ["Conforti", "Michele", "Faenza", "Yuri", "Fiorini", "Samuel", "Grappe", "Roland", "Tiwary", "Hans Raj"], "venue": "arXiv preprint arXiv:1105.4127,", "citeRegEx": "Conforti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Conforti et al\\.", "year": 2011}, {"title": "The price of bandit information for online optimization", "author": ["Dani", "Varsha", "Hayes", "Thomas", "Kakade", "Sham M"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Extended formulations, nonnegative factorizations, and randomized communication protocols", "author": ["Faenza", "Yuri", "Fiorini", "Samuel", "Grappe", "Roland", "Tiwary", "Hans Raj"], "venue": "In Combinatorial Optimization,", "citeRegEx": "Faenza et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Faenza et al\\.", "year": 2012}, {"title": "Linear vs. semidefinite extended formulations: exponential separation and strong lower bounds", "author": ["Fiorini", "Samuel", "Massar", "Serge", "Pokutta", "Sebastian", "Tiwary", "Hans Raj", "de Wolf", "Ronald"], "venue": "In Proceedings of the 44th symposium on Theory of Computing,", "citeRegEx": "Fiorini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fiorini et al\\.", "year": 2012}, {"title": "Computers and intractability a guide to the theory of np-completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "Garey and Johnson,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson", "year": 1979}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Goemans", "Michel X", "Williamson", "David P"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Goemans et al\\.", "year": 1995}, {"title": "Anonymous Ratings from the Jester Online Joke", "author": ["Goldberg", "Ken"], "venue": "Recommender System. http://eigentaste. berkeley.edu/dataset/,", "citeRegEx": "Goldberg and Ken.,? \\Q2003\\E", "shortCiteRegEx": "Goldberg and Ken.", "year": 2003}, {"title": "Lifts of convex sets and cone factorizations", "author": ["Gouveia", "Joao", "Parrilo", "Pablo A", "Thomas", "Rekha"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Gouveia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gouveia et al\\.", "year": 2011}, {"title": "Interior-point polynomial algorithms in convex programming, volume", "author": ["Nesterov", "Yurii", "Nemirovskii", "Arkadii Semenovich", "Ye", "Yinyu"], "venue": null, "citeRegEx": "Nesterov et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 1994}, {"title": "Expressing combinatorial optimization problems by linear programs", "author": ["Yannakakis", "Mihalis"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yannakakis and Mihalis.,? \\Q1991\\E", "shortCiteRegEx": "Yannakakis and Mihalis.", "year": 1991}], "referenceMentions": [{"referenceID": 3, "context": "Online linear optimization is a natural generalization of the the basic adversarial (non-stochastic) or worst case multiarm bandit framework (Auer et al., 2002), to the domain of convex optimization, where the set of actions is replaced by a compact action set A \u2282 R and the loss is a linear function on the action set A.", "startOffset": 141, "endOffset": 160}, {"referenceID": 6, "context": "The best known bound is of the order d \u221a n (Bubeck et al., 2012) with a computationally hard John\u2019s exploration technique (Ball, 1997).", "startOffset": 43, "endOffset": 64}, {"referenceID": 6, "context": "The best known bound is of the order d \u221a n (Bubeck et al., 2012) with a computationally hard John\u2019s exploration technique (Ball, 1997). Specifically, in the line of research of online combinatorial optimization, we address the following open questions posed by Bubeck et al. (2012). For the combinatorial bandit optimization where actions are d-dimensional, can we have an optimal regret bound with a computationally efficient strategy? Is there a natural way one can characterize the combinatorial action sets for which such optimal regret bounds are obtained under bandit feedback?", "startOffset": 44, "endOffset": 282}, {"referenceID": 20, "context": "Particularly, we use a technique called \u2018extended formulation\u2019- a lift and projection technique where the complicated combinatorial convex set (action set) is approximated by a simpler convex set whose linear image is the original convex set (Gouveia et al., 2011).", "startOffset": 242, "endOffset": 264}, {"referenceID": 21, "context": "Particularly, we are interested in the approximation of the convex set based on the orthant or semidefinite/non-negative cone that has an associated natural barrier penalty (Nesterov et al., 1994; Chandrasekaran & Jordan, 2013).", "startOffset": 173, "endOffset": 227}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy.", "startOffset": 177, "endOffset": 196}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem.", "startOffset": 178, "endOffset": 533}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al.", "startOffset": 178, "endOffset": 862}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995).", "startOffset": 178, "endOffset": 888}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995). However, for a set of k arms, Exp3 scales with \u221a nN lnn, and for discretised k into d as in the combinatorial setting, N possible actions is exponential in d.", "startOffset": 178, "endOffset": 980}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995). However, for a set of k arms, Exp3 scales with \u221a nN lnn, and for discretised k into d as in the combinatorial setting, N possible actions is exponential in d. This work was completed by Bubeck et al. (2012), where an optimal exploration using John\u2019s theorem (Ball, 1997) from convex geometry is used on an adaptation of Exp3 called Exp2, to obtain optimal regret bound of \u221a dn logA for any set of finite actions.", "startOffset": 178, "endOffset": 1188}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers.", "startOffset": 129, "endOffset": 153}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n).", "startOffset": 130, "endOffset": 555}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting.", "startOffset": 130, "endOffset": 620}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting. Their work showed that when the action set is combinatorial A \u2282 [0, 1] and loss L = [0, 1], the minimax regret in the full information and semi-bandit case is of the order d \u221a n while with bandit feedback the order is the sub-optimal d \u221a n. The optimal regret bound is also obtained by the mirror descent strategies on the simplex and the Euclidean ball action sets. OSMD for the Euclidean ball in Bubeck (2012), (Ball, 1997), achieves regret of the order \u221a dn.", "startOffset": 130, "endOffset": 1113}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting. Their work showed that when the action set is combinatorial A \u2282 [0, 1] and loss L = [0, 1], the minimax regret in the full information and semi-bandit case is of the order d \u221a n while with bandit feedback the order is the sub-optimal d \u221a n. The optimal regret bound is also obtained by the mirror descent strategies on the simplex and the Euclidean ball action sets. OSMD for the Euclidean ball in Bubeck (2012), (Ball, 1997), achieves regret of the order \u221a dn. For more information on combinatorial linear optimization, interested reader may refer (Bubeck & Cesa-Bianchi, 2012). Cesa Bianchi et al. have established in (2012), the unified analysis of mirror descent and fixed share of weights between experts in the online setting.", "startOffset": 130, "endOffset": 1328}, {"referenceID": 12, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables.", "startOffset": 24, "endOffset": 47}, {"referenceID": 4, "context": "The techniques are also known as lift-and-project (Balas et al., 1993) and approximation techniques (Chandrasekaran & Jordan, 2013).", "startOffset": 50, "endOffset": 70}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well.", "startOffset": 25, "endOffset": 471}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well. The recent work of Gouveia et al. (2011) and Fiorini et al.", "startOffset": 25, "endOffset": 670}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well. The recent work of Gouveia et al. (2011) and Fiorini et al. (2012) generalizes the theorem from linear programming paradigm for convex optimization.", "startOffset": 25, "endOffset": 696}, {"referenceID": 20, "context": "from (Gouveia et al., 2011).", "startOffset": 5, "endOffset": 27}, {"referenceID": 13, "context": "the Pri,t\u2019s compute the slack matrix M in expectation (see (Conforti et al., 2011)) over the randomization of the player and the adversary.", "startOffset": 59, "endOffset": 82}, {"referenceID": 20, "context": "The lower bound on the non-negative rank for a regular n-gon that has a R+ lift is given by r = O (log2 (d)) (Gouveia et al., 2011).", "startOffset": 109, "endOffset": 131}, {"referenceID": 15, "context": "It has been shown that a randomized communication protocol can give a stronger lower bound in the order of base-2 logarithm of the non-negative rank of the slack matrix (communication matrix) computed in expectation (Faenza et al., 2012).", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "We implemented Algorithm 1, Exp2 (Bubeck & Cesa-Bianchi, 2012), Exp3 (Auer et al., 2002), Exp3.", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "P (Auer et al., 2002), and CombBand (Cesa-Bianchi & Lugosi, 2012).", "startOffset": 2, "endOffset": 21}, {"referenceID": 6, "context": "We could not compare with Exp2 with John\u2019s exploration (Bubeck et al., 2012) as the authors state its computational inefficiency.", "startOffset": 55, "endOffset": 76}], "year": 2017, "abstractText": "On-line linear optimization on combinatorial action sets (d-dimensional actions) with bandit feedback, is known to have complexity in the order of the dimension of the problem. The exponential weighted strategy achieves the best known regret bound that is of the order of d \u221a n (where d is the dimension of the problem, n is the time horizon ). However, such strategies are provably suboptimal or computationally inefficient. The complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set. Mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition. Entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. Here, we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called \u2018extended formulation\u2019. Such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis. We show that with such formulations, exponential weighting can achieve logarithmic dependence on the dimension of the set.", "creator": "LaTeX with hyperref package"}}}