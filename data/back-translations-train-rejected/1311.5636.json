{"id": "1311.5636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2013", "title": "Learning Non-Linear Feature Maps", "abstract": "Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches.", "histories": [["v1", "Fri, 22 Nov 2013 01:49:26 GMT  (140kb,D)", "http://arxiv.org/abs/1311.5636v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dimitrios athanasakis", "john shawe-taylor", "delmiro fernandez-reyes"], "accepted": false, "id": "1311.5636"}, "pdf": {"name": "1311.5636.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Linear Feature Maps", "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "emails": ["dathanasakis@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"], "sections": [{"heading": null, "text": "Feature selection plays a central role in learning, especially in areas where frugal traits can provide insight into the underlying process, such as biology. Recent approaches to nonlinear trait selection through greedy optimization of the Centred Kernel Target Alignment (KTA) can become mathematically prohibitive for high-dimensional datasets. We propose randSel, a randomized trait selection algorithm with attractive scaling characteristics. Our theoretical analysis of randSel provides strong probable guarantees for the correct identification of relevant traits. Experimental results from real and artificial data show that the method successfully identifies effective traits and performs better than a number of competing approaches."}, {"heading": "1 Introduction", "text": "The selection of characteristics is an important aspect in the implementation of machine learning methods. Appropriate selection of informative characteristics can reduce generalization errors as well as the storage and processing requirements for large data sets. In addition, economical models can provide valuable insights into the relationships underlying the elements studied. There is a wealth of literature on feature selection when the relationship between variables is linear. Unfortunately, nonlinear feature selections become substantially nuanced. Core methods excel in modeling nonlinear relationships. Unsurprisingly, a number of kernel-based feature selection algorithms have been proposed. Early suggestions, such as recursive feature elimination (RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-ranking cores may fail, using nonlinearity in the underlying relationship to encapsulate linear features [1] may be computationally prohibitive, while attempts to learn a convex combination of low-ranking cores may fail, but may not use newer approaches that encapitalize on linear relationships, which can expand explicit."}, {"heading": "1.1 Related Work", "text": "The work in [4] provides the basis for using the orientation of centered kernel matrices as the basis for measuring statistical dependencies. The Hilbert-Schmidt independence criterion is the basis for further work in [5], which uses greedy optimization of centered orientation for feature selection. [5] In addition, it identifies numerous connections with other existing feature selection algorithms that can be considered as instances of the frame. Stability selection [6] is a general framework for variable selection and structural estimation of high-dimensional data. The core principle of stability selection is to combine subsampling with a sparse variable selection algorithm. By repeatedly estimating over a number of different subsample samples, the framework tracks the number of times each variable was used, thus obtaining an estimate of the importance of each feature."}, {"heading": "2 Preliminaries", "text": "We consider the supervised learning problem in modelling the relationship between an m \u00b7 n input matrix X and a corresponding m \u00b7 n \u00b2 output matrix Y. The simplest example of such a problem is the binary classification, where the learning problem consists in learning a function f: x \u2192 y that maps input vectors x to the desired outputs y. In the binary case, we are confronted with a m \u00b7 n matrix X and a vector of outputs y. By limiting discrimination functions to linear classifiers, we would like to find a classification of the characteristics that we can generalize to the nonlinear setting by using a nonlinear characteristic card (x), which leads to the kernel formula: f (x) = < w = < p < p < p < p (xi), the equation of learning through the use of a nonlinear characteristic card."}, {"heading": "3 Development of key ideas", "text": "The approach we will take will be based on the following known observation, which links the orientation of the nucleus to the degree to which an input space contains a linear projection that correlates with the target."}, {"heading": "4 Properties of the algorithm", "text": "With m \u00b7 n input matrix X and corresponding output matrix Y, randSel assumes that the individual contribution of the characteristics is estimated by estimating the alignment of a number of random sub-samples comprising n2 and n 2 + 1 randomly selected characteristics, resulting in an estimate of the expected alignment contribution of a characteristic. The algorithm is parameterized by the number of boot straps N, a boot straps Sizenb and a percentage z% of the characteristics dropped after N boot straps. The algorithm proceeds iteratively until only two characteristics remain. Optionally, the algorithm can be further parameterized by having characteristics that are in the uppermost percentile Y and a percentage z of the characteristics that fall after N boot straps. This option increases the probability of detecting non-linear dependencies between them if they are present."}, {"heading": "5 Results", "text": "In this section, we will present several experiments that compare our approach to feature selection with a number of competing methods. We have used three synthetic datasets to better illustrate the performance characteristics of these algorithms before moving on to experiments with real-world data generated when creating profiles for infectious diseases."}, {"heading": "5.1 Experimental Setup", "text": "In both synthetic and real datasets, we used nested 10-fold cross-validation to perform feature selection, and repeated the simulations on three different rearrangements of the dataset to account for variance. For each iteration, we estimate the validation error after selecting the characteristics before proceeding with the test at hand. Internal cross-validation loop determines the number of characteristics to be used in classifying the test set for optimal accuracy. If two or more models are tied in terms of performance, the more favorable model is preferable.We compare our proposed approach with kernel-based algorithms such as RFE, FoHsic, and BaHsic, as well as a filtering approach based on correlation coefficient and stability selection, using the lasso as the underlying sparse selection algorithm. The same range of gaussian core bandwidths was examined in all algorithms, and the resulting 1."}, {"heading": "5.2 Synthetic Data", "text": "All three synthetic datasets contain 300 samples with a dimensionality of 100 characteristics. The linear and non-linear Webston datasets were created according to [7] and consist of 5 relevant and 95 interference characteristics. Neither the linear nor the non-linear Weston datasets exhibit a non-linear interdependence between characteristics. To simulate this scenario, we have created a simple XOR sample dataset. Along with the accuracy of the test set and sparseness, we also capture the precision and retrieval of selection algorithms. Similar to the query for information, we define precision as the number of relevant characteristics selected from the feature selection method, across the total number of selected characteristics, and remember as the number of relevant characteristics selected from the total number of relevant characteristics."}, {"heading": "5.3 Real Data", "text": "We conduct experiments in real data sets that are able to explore the underlying mechanisms."}, {"heading": "5.4 Learning Deep Representations with LPBoostMKL", "text": "A final experimental application in which we applied randomised selection was Black Box Learning [10] [13] in the most recent challenge. After taking a first unattended step towards learning traits from the original dataset using sparse filtering [11], we performed randomised selection in the resulting representation and created cores that matched the remaining traits after each iteration of the trait selection algorithm. Treating each kernel as the definition of a class of weak learners, we used LPBoost to learn multiple kernels (LPBoostMKL [12]), and the resulting classifier beat many of our other approaches and was one of the most powerful in the challenge."}, {"heading": "6 Conclussions", "text": "In this paper, we propose randSel, a new algorithm for nonlinear feature selection based on randomized HSIC estimates. RandSel stochastically estimates the expected importance of features in each iteration and continues to cull uninformative features at the end of each iteration. Our theoretical analysis provides strong guarantees for the expected performance of this method, which is further demonstrated by testing a number of real and artificial datasets. This, combined with the attractive scaling properties of the algorithm, makes randSel a good proposal for use in areas such as quantitative biology, where data volumes are increasing at breakneck speed."}], "references": [{"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning 46,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "On kernel target alignment. Advances in neural information processing systems", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Feature selection via dependence maximization", "author": ["Song", "Le", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Feature selection for SVMs. Advances in neural information processing systems: 668-674", "author": ["Weston", "Jason", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Discriminating Active from Latent Tuberculosis in Patients Presenting to Community Clinics, PLOS ONE", "author": ["Gurjinder Sandhu", "Francesca Battaglia", "Barry K. Ely", "Dimitrios Athanasakis", "Rosario Montoya", "Teresa Valencia", "Robert H. Gilman", "Carlton A. Evans", "Jon S. Friedland", "Delmiro Fernandez-Reyes", "Daniel D"], "venue": "Agranoff", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Multiple Kernel Learning on the Limit Order Book", "author": ["Tristan Fletcher", "Zakria Hussain", "John Shawe-Taylor"], "venue": "Proceedings of the First Workshop on Applications of Pattern Analysis", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Fast dependency-aware feature selection in very-highdimensional pattern recognition", "author": ["Somol", "Petr", "Jiri Grim", "Pavel Pudil"], "venue": "In Systems, Man, and Cybernetics (SMC) 2011 IEEE International Conference on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Challenges in Representation Learning: A report on three machine learning contests", "author": ["I.J. Goodfellow", "D. Erhan", "P.L. Carrier", "A. Courville", "M. Mirza", "B. Hamner", "Y. Bengio"], "venue": "Proceedings of the 20th International Conference on Neural Information Processing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3].", "startOffset": 66, "endOffset": 71}, {"referenceID": 2, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3].", "startOffset": 66, "endOffset": 71}, {"referenceID": 3, "context": "The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where greedy optimisation of centred alignment is employed for feature selection.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "The dependence estimation of random subsets of variables is similar to the approach of [13], which is extended through bootstrapping and carefully controlled feature set sizes.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "This approach is a straightforward extension of the idea of BaHsic [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "The linear and non-linear weston datasets were generated according to [7], and consist of 5 relevant and 95 noise features.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The first TB dataset consists of 523-dimensional mass-spectrometry proteomic profiles of blood plasma [8], and consists of 100 active TB samples, 40 symptomatic controls, and 49 samples of patients with TB-Like symptoms with a co-existing latent TB infection (LTBI).", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "A final experimental application where we employed randomised selection was in the recent Black Box Learning challenge [10][13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "Treating each kernel as defining a class of weak learners, we used LPBoost to perform multiple kernel learning (LPBoostMKL [12]).", "startOffset": 123, "endOffset": 127}], "year": 2013, "abstractText": "Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches. Feature Selection, Kernels", "creator": "LaTeX with hyperref package"}}}