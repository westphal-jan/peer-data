{"id": "1311.6041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2013", "title": "No Free Lunch Theorem and Bayesian probability theory: two sides of the same coin. Some implications for black-box optimization and metaheuristics", "abstract": "Challenging optimization problems, which elude acceptable solution via conventional calculus methods, arise commonly in different areas of industrial design and practice. Hard optimization problems are those who manifest the following behavior: a) high number of independent input variables; b) very complex or irregular multi-modal fitness; c) computational expensive fitness evaluation. This paper will focus on some theoretical issues that have strong implication for practice. I will stress how an interpretation of the No Fee Lunch theorem leads naturally to a general Bayesian optimization framework. The choice of a prior over the space of functions is a critical and inevitable step in every black-box optimization.", "histories": [["v1", "Sat, 23 Nov 2013 19:19:37 GMT  (156kb,D)", "https://arxiv.org/abs/1311.6041v1", "This paper has been presented at Gulf International Conference On Applied Mathematics, November 19-21 2013, Kuwait"], ["v2", "Wed, 27 Nov 2013 04:18:38 GMT  (156kb,D)", "http://arxiv.org/abs/1311.6041v2", "This paper has been presented at Gulf International Conference On Applied Mathematics, November 19-21 2013, Kuwait"], ["v3", "Sun, 1 Dec 2013 06:02:58 GMT  (157kb,D)", "http://arxiv.org/abs/1311.6041v3", "Multiple changes throughout the paper"]], "COMMENTS": "This paper has been presented at Gulf International Conference On Applied Mathematics, November 19-21 2013, Kuwait", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["loris serafino"], "accepted": false, "id": "1311.6041"}, "pdf": {"name": "1311.6041.pdf", "metadata": {"source": "CRF", "title": "No Free Lunch Theorem and Bayesian probability theory: two sides of the same coin. Some implications for black-box optimization and metaheuristics\u2217", "authors": ["Loris Serafino", "L. Serafino"], "emails": ["l.serafino@ack.edu.kw"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}], "references": [{"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Why Is Optimization Difficult?, Nature-Inspired Algorithms for Optimisation", "author": ["T. Weise", "M. Zapf", "R. Chiong", "A.J. Nebro Urbaneja"], "venue": "Raymond Chiong (Ed.),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Survey of modeling and optimization strategies to solve high-dimensional design problems with computationally-expensive black-box functions Structural and Multidisciplinary Optimization", "author": ["S. Shan", "G. Gary Wang"], "venue": "Volume 41, Issue", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A computational intelligence algorithm for expensive engineering optimization problems", "author": ["Y. Tenne"], "venue": "Eng. Appl. Artif. Intell. 25,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An Introduction to Genetic Algorithms", "author": ["M. Mitchell"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Review of Metaheuristics and Generalized Evolutionary Walk Algorithm", "author": ["X.S. Yang"], "venue": "J. Bio-Inspired Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Parameter Control in Evolutionary Algorithms", "author": ["E. Eiben A", "R. Hinterding", "Z. Michalewicz"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Arbitrary function optimisation with metaheuristics", "author": ["C. Garca-Martnez", "F.J. Rodriguez", "M. Lozano"], "venue": "Soft Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Design and Modeling for Computer Experiments (Computer Science & Data Analysis)", "author": ["A. Sudjianto", "K.T. Fang", "R. Li"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A comprehensive survey of fitness approximation in evolutionary computation", "author": ["Y. Jin"], "venue": "Soft Computing A Fusion of Foundations, Methodologies and Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Introduction to Derivative-Free Optimization", "author": ["A.R. Conn", "K. Scheinberg", "L.N. Vicente"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "CoRR abs/1012.2599,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A framework for optimization under limited information", "author": ["T Alpcan"], "venue": "J Glob Optim 55:681706,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Portfolio Allocation for Bayesian Optimization", "author": ["M. Hoffman", "E. Brochu", "N. de Freitas"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "The CMA evolution strategy: a comparing review, Towards a new evolutionary computation. Advances on estimation of distribution algorithms", "author": ["N. Hansen"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "This fact, which is frequently named the curse of dimensionality, is well known by practitioners that have to handle problems with hundreds of variables [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "ruggedness [2].", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "Some good reviews are [2, 3, 4].", "startOffset": 22, "endOffset": 31}, {"referenceID": 2, "context": "Some good reviews are [2, 3, 4].", "startOffset": 22, "endOffset": 31}, {"referenceID": 3, "context": "Some good reviews are [2, 3, 4].", "startOffset": 22, "endOffset": 31}, {"referenceID": 4, "context": "For example just to mention one class, nature-inspired algorithms, like the well known genetic algorithm [5], are based on the idea of mimicking some natural phenomena that leads to the maximization of some defined quantity.", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Ant Colonies, Simulated Annealing and many other families [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "We will start with the usual formal statement of the NFLT for optimization [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Several related methods are now commonly used for fitness approximation: the Kriging model, radial-basis-function networks and the support vector machines (for a general reference, see [10, 11]).", "startOffset": 185, "endOffset": 193}, {"referenceID": 10, "context": "Several related methods are now commonly used for fitness approximation: the Kriging model, radial-basis-function networks and the support vector machines (for a general reference, see [10, 11]).", "startOffset": 185, "endOffset": 193}, {"referenceID": 11, "context": "After the model has been developed some classical derivative-based or derivative-free methods can be applied [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "This technique conceptually combines surrogate estimation with optimum localization (for a good introduction see [13, 14, 15]).", "startOffset": 113, "endOffset": 125}, {"referenceID": 13, "context": "This technique conceptually combines surrogate estimation with optimum localization (for a good introduction see [13, 14, 15]).", "startOffset": 113, "endOffset": 125}, {"referenceID": 14, "context": "This technique conceptually combines surrogate estimation with optimum localization (for a good introduction see [13, 14, 15]).", "startOffset": 113, "endOffset": 125}, {"referenceID": 9, "context": "\u2021For Design of Experiments techniques see [10]", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "For function with low causality they tend to suffer [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Many automated self-adapting strategies able to change the internal parameters values according to partial results obtained during the search process have been studied in recent years [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 15, "context": "Just for mentioning one, Covariance Matrix Adaptation Evolutionary Strategies (CMAES) is one of the most famous in the field of continuous global optimization [16].", "startOffset": 159, "endOffset": 163}], "year": 2013, "abstractText": "Challenging optimization problems, which elude acceptable solution via conventional calculus methods, arise commonly in different areas of industrial design and practice. Hard optimization problems are those who manifest the following behavior: a) high number of independent input variables; b) very complex or irregular multi-modal fitness; c) computational expensive fitness evaluation. This paper will focus on some theoretical issues that have strong implications for practice. I will stress how an interpretation of the No Free Lunch theorem leads naturally to a general Bayesian optimization framework. The choice of a prior over the space of functions is a critical and inevitable step in every black-box optimization. key words: No free lunch theorem, Metaheuristics, Bayesian optimization", "creator": "LaTeX with hyperref package"}}}