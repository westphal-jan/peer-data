{"id": "1311.6396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2013", "title": "A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds", "abstract": "We study sequential prediction of real-valued, arbitrary and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios we achieve matching lower and upper bounds demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result we also prove that for the worst case scenario, the performance of randomized algorithms can be achieved by sequential algorithms so that randomized algorithms does not improve the performance.", "histories": [["v1", "Mon, 25 Nov 2013 18:36:26 GMT  (12kb)", "https://arxiv.org/abs/1311.6396v1", null], ["v2", "Wed, 22 Jan 2014 21:00:52 GMT  (12kb)", "http://arxiv.org/abs/1311.6396v2", "Submitted to IEEE Transactions on Neural Networks and Learning Systems"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["n denizcan vanli", "suleyman s kozat"], "accepted": false, "id": "1311.6396"}, "pdf": {"name": "1311.6396.pdf", "metadata": {"source": "CRF", "title": "A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds", "authors": ["N. Denizcan Vanli"], "emails": ["vanli@ee.bilkent.edu.tr,", "kozat@ee.bilkent.edu.tr)."], "sections": [{"heading": null, "text": "\"We have the possibility that we see ourselves in a position to hide ourselves in a position,\" he said to the \"world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"The world.\" (\"The world\") \"(\" The world \").\" (\"The world\"). \"(\" The world \").\" (\"The world\"). \"(\"). \"(\" The world \"). (\"). \"(\" The world. \"(\") (\"The world.\" (\").\" (\"). (\" The world. \"(\"). (\") (\" The world. \"(\"). (\") (\" The world. \"(\"). \"(\"). \"(\" The world. \"(\"). \"(\"). \"(\" The world. \"(\"). \"(\"). \"(\" The world. \"(\")"}, {"heading": "II. LOWER BOUNDS", "text": "In this section, we examine the worst case performance of sequential algorithms in order to obtain guaranteed lower limits. \"(2) For this regret, we have the following theorems, which relate the performance of each sequential algorithm to the general class of parametric predictors. (2) We have the following theory, which relates to the general class of parametric predictors. (3) While proving these theorems, we also provide a generic approach to find lower limits on remorse in (2), and later we use this method to derive lower limits for parametric classes, including the classes of univariant polynomial predictors."}, {"heading": "B. Multivariate Polynomial Prediction", "text": "Suppose the prediction function is used by wTfx (x-1 t-a) for the prediction function and uniformity."}, {"heading": "III. A COMPREHENSIVE APPROACH TO REGRET MINIMIZATION", "text": "In this section, we present a prediction function that can be used to predict a limited, arbitrary problem and identify a universal sequence. We derive the uppermost limits of this algorithm in such a way that our prediction function is not inferior to the upper limits presented. However, in some cases, by achieving matching upper and lower limits, we prove that this algorithm is optimal in a strong minimax sense that the worst prediction function cannot be further improved. We limit the prediction functions to be separable, i.e., f (w, xt \u2212 a) = fw (w) T fx (x \u2212 1 t \u2212 a), where fx (w) and fx \u2212 a \u2212 a) vector functions of size m \u00b7 1 are integer for some m. To avoid any confusion we simply denote \u03b2, fw (w), where \u03b2, the same prediction function can be written as f (w)."}, {"heading": "B. Multivariate Polynomial Prediction", "text": "The upper limit for a multivariate polynomial prediction function fx (x t \u2212 1 t \u2212 a) follows exactly the upper limit of the univariate polynomial prediction mth order, which has an upper limit n \u2211 t = 1 (x [t] \u2212 x [t] \u2212 u [t]) 2 \u2264 min\u03b2 Rm {n \u2211 t = 1 (x [t] \u2212 \u03b2Tfx (x t \u2212 1 t \u2212 a)) 2 + \u03b4 | \u03b2 | 2} + A2m ln (1 + A2n\u03b4).6 C. k-ahead mth-order Linear PredictionFor k-ahead mth-order prediction, the prediction class is given by f (w, xt \u2212 1t \u2212 a) = \u03b2 Tfx (x t \u2212 1 t \u2212 a) = \u03b2Tx [t \u2212 k] where x [t \u2212 k] where x [t \u2212 k] [t [t \u2212 n \u2212 k],"}, {"heading": "IV. RANDOMIZED OUTPUT PREDICTIONS", "text": "In this section, we examine the performance of randomized output algorithms for the worst-case scenario with respect to linear predictors using the same deplorable value in (2). We emphasize that the randomized output algorithms are a superset of deterministic sequential predictors and the derivatives here can be easily generalized to include any predictive class. In particular, we consider randomized output algorithms f (xt \u2212 11), x \u2212 1), so that the randomization parameters Rm can be a function of the entire past. Hence, a randomized sequential algorithm, introduces randomization or uncertainty in its output, so that the output also depends on a random element. Note that such methods are widely used in applications with safety considerations. As an example, let us assume that there are prediction algorithms running in parallel to the prediction of the observation sequence."}], "references": [{"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 6, pp. 1411\u20131423, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonparametric estimation and classification using radial basis function nets and empirical risk minimization", "author": ["L. Devroye", "T. Linder", "G. Lugosi"], "venue": "IEEE Transactions on Neural Networks, vol. 7, no. 2, pp. 475\u2013487, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Radial basis function networks and complexity regularization in function learning", "author": ["A. Krzyzak", "T. Linder"], "venue": "IEEE Transactions on Neural Networks, vol. 9, no. 2, pp. 247\u2013256, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P. Long", "M. Warmuth"], "venue": "IEEE Transactions on Neural Networks, vol. 7, no. 3, pp. 604\u2013619, 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Universal linear prediction by model order weighting", "author": ["A. Singer", "M. Feder"], "venue": "IEEE Transactions on Signal Processing, vol. 47, no. 10, pp. 2685\u20132699, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Universal linear least-squares prediction in the presence of noise", "author": ["G.C. Zeitler", "A. Singer"], "venue": "IEEE/SP 14th Workshop on Statistical Signal Processing, 2007. SSP \u201907, 2007, pp. 611\u2013614.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal linear least squares prediction: upper and lower bounds", "author": ["A. Singer", "S. Kozat", "M. Feder"], "venue": "IEEE Transactions on Information Theory, vol. 48, no. 8, pp. 2354\u20132362, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Model complexity control for regression using VC generalization bounds", "author": ["V. Cherkassky", "X. Shao", "F. Mulier", "V. Vapnik"], "venue": "IEEE Transactions on Neural Networks, vol. 10, no. 5, pp. 1075\u20131089, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Journal of Information and Computation, vol. 132, no. 1, pp. 1\u201362, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Competitive on-line statistics", "author": ["V. Vovk"], "venue": "International Statistical Review, vol. 69, pp. 213\u2013248, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Universal prediction of individual binary sequences in the presence of noise", "author": ["T. Weissman", "N. Merhav"], "venue": "IEEE Transactions on Information Theory, vol. 47, no. 6, pp. 2151\u20132173, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Universal FIR MMSE filtering", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 3, pp. 1068\u20131083, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Competitive on-line linear FIR MMSE filtering", "author": ["\u2014\u2014"], "venue": "IEEE International Symposium on Information Theory, 2007. ISIT 2007., 2007, pp. 1126\u20131130.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive polynomial filters", "author": ["V. Mathews"], "venue": "Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, 1991.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability, Random Processes, and Estimation Theory for Engineers", "author": ["H. Stark", "J. Woods"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION In this brief paper, we investigate the generic sequential (online) prediction problem from an individual sequence perspective using tools of computational learning theory, where we refrain from any statistical assumptions either in modeling or on signals [1]\u2013[4].", "startOffset": 269, "endOffset": 272}, {"referenceID": 3, "context": "INTRODUCTION In this brief paper, we investigate the generic sequential (online) prediction problem from an individual sequence perspective using tools of computational learning theory, where we refrain from any statistical assumptions either in modeling or on signals [1]\u2013[4].", "startOffset": 273, "endOffset": 276}, {"referenceID": 0, "context": "Since we do not impose any statistical assumptions on the underlying data, we, motivated by recent results from sequential learning [1]\u2013[4], define the performance of a sequential algorithm with respect to a comparison class, where the predictors of the comparison class are formed by observing the the entire sequence in hindsight, under the squared error loss, i.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "Since we do not impose any statistical assumptions on the underlying data, we, motivated by recent results from sequential learning [1]\u2013[4], define the performance of a sequential algorithm with respect to a comparison class, where the predictors of the comparison class are formed by observing the the entire sequence in hindsight, under the squared error loss, i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "for an arbitrary length of data n, and for any possible sequence {x[t]}t\u22651, where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212 1] for prediction, and x\u0302c[t] is the prediction at time t of", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "1 Although the parameters of the parametric prediction function f(w, xt\u22121 t\u2212a) can be set arbitrarily, even by observing all the data {x[t]}t\u22651 a priori, the function is naturally restricted to use only the sequential data xt\u22121 1 in prediction [5]\u2013[7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 6, "context": "1 Although the parameters of the parametric prediction function f(w, xt\u22121 t\u2212a) can be set arbitrarily, even by observing all the data {x[t]}t\u22651 a priori, the function is naturally restricted to use only the sequential data xt\u22121 1 in prediction [5]\u2013[7].", "startOffset": 248, "endOffset": 251}, {"referenceID": 6, "context": "found such that the upper bound of the regret of that algorithm matches with the lower bound, then that algorithm is optimal in a strong minimax sense such that the actual convergence performance cannot be further improved [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 0, "context": "To this end, the minimax sense optimality of different parametric learning algorithms such as the well-known prediction algorithms, least mean squares (LMS) [8], recursive least squares (RLS) [8], and online sequential extreme learning machine (OS-ELM) of [1] can be determined using the lower bounds provided in this paper.", "startOffset": 256, "endOffset": 259}, {"referenceID": 7, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 186, "endOffset": 190}, {"referenceID": 4, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "These results are then extended to the filtering problems [13], [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "These results are then extended to the filtering problems [13], [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 3, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 232, "endOffset": 235}, {"referenceID": 3, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 203, "endOffset": 206}, {"referenceID": 8, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 213, "endOffset": 217}, {"referenceID": 13, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 3, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 272, "endOffset": 275}, {"referenceID": 6, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 276, "endOffset": 279}, {"referenceID": 8, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 281, "endOffset": 285}, {"referenceID": 10, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 286, "endOffset": 290}, {"referenceID": 13, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 292, "endOffset": 296}, {"referenceID": 14, "context": "Yet, for a specific distribution on x1 , the best predictor is the conditional mean on x1 under the squared error [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "The squared-error loss E [ (x[t]\u2212x\u0302s[t]) 2 ] is minimized with the well-known minimum mean squared error (MMSE) predictor given by [16]", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": ", x[1] ] = E [ x[t] \u2223 \u2223xt\u22121 1 ] , (4)", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "(8) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, w = [w1, .", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "After following the lines in [5], we obtain a lower bound of the form O(ln(n)).", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, and w is the parameter for prediction.", "startOffset": 96, "endOffset": 99}, {"referenceID": 13, "context": "We emphasize that this class of predictors are not only the super set of univariate polynomial predictors, but also widely used in many signal processing applications to model nonlinearity such as Volterra filters [15].", "startOffset": 214, "endOffset": 218}, {"referenceID": 6, "context": "After this line the derivation follows similar lines to [7], giving a lower bound of the form O(ln(n)) for the regret.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "(9) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212k] for prediction for some integer k, w = [w1, .", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "For this purpose we define the following parametric distribution on x1 as in [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "After this point the derivation exactly follows the lines in [5] resulting a lower bound of the form O(ln(n)).", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "For k-ahead mth-order prediction, we generalize the lower bound obtained for k-ahead first-order prediction and following the lines in [5], we obtain a lower bound of the form O(m ln(n)).", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "The derivations follow similar lines to [5], [10], hence only main points are presented.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The derivations follow similar lines to [5], [10], hence only main points are presented.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Following similar lines to [7] with a predictor of \u03b2fx(x t\u22121 t\u2212a) we obtain", "startOffset": 27, "endOffset": 30}], "year": 2014, "abstractText": "We study sequential prediction of real-valued, arbitrary and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios we achieve matching lower and upper bounds demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result we also prove that for the worst case scenario, the performance of randomized algorithms can be achieved by sequential algorithms so that randomized algorithms does not improve the performance.", "creator": "LaTeX with hyperref package"}}}