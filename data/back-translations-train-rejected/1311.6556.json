{"id": "1311.6556", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "Double Ramp Loss Based Reject Option Classifier", "abstract": "We consider the problem of building classifiers with the option to reject i.e., not return a prediction on a given test example. Adding a reject option to a classifier is well-known in practice; traditionally, this has been accomplished in two different ways. One is the {\\em decoupled} method where an optimal base classifier (without the reject option) is build first and then the rejection boundary is optimized, typically in terms of a band around the separating surface. The {\\em coupled} method is based on finding both the classifier as well as the rejection band at the same time. Existing coupled approaches are based on minimizing risk under an extension of the classical $0-1$ loss function wherein a loss $d \\in (0,.5)$ is assigned to a rejected example. In this paper, we propose a {\\bf double ramp loss} function which gives a continuous upper bound for $(0-d-1)$ loss described above. Our coupled approach is based on minimizing regularized risk under the double ramp loss which is done using difference of convex (DC) programming. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets.", "histories": [["v1", "Tue, 26 Nov 2013 05:13:18 GMT  (270kb)", "http://arxiv.org/abs/1311.6556v1", null], ["v2", "Mon, 8 Dec 2014 07:34:34 GMT  (250kb)", "http://arxiv.org/abs/1311.6556v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["naresh manwani", "kalpit desai", "sanand sasidharan", "ramasubramanian sundararajan"], "accepted": false, "id": "1311.6556"}, "pdf": {"name": "1311.6556.pdf", "metadata": {"source": "CRF", "title": "Double Ramp Loss Based Reject Option Classifier", "authors": ["Naresh Manwani", "Kalpit Desai", "Sanand Sasidharan", "Ramasubramanian Sundararajan"], "emails": ["naresh.manwani@ge.com", "kalpit.desai@ge.com", "sanand.sasidharan@ge.com", "ramasubramanian.sundararajan@ge.com"], "sections": [{"heading": null, "text": "The question is whether the diagnosis is a miscalculation, which in many real-life situations may be wise to reject an example, i.e. whether the symptoms are either ambiguous or rare enough to be inexplicable without further investigation, then the doctor might choose to misdiagnose the patient (which could lead to further complications), ask for further medical tests to be performed, or refer the case to a suitable specialist. Similarly, when confronted with a customer's credit application, a banker might decide not to make a decision."}], "references": [{"title": "Trading convexity for scalability", "author": ["Ronan Collobert", "Fabian Sinz", "Jason Weston", "L\u00e9on Bottou"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2006}, {"title": "Support vector machines with embedded reject option", "author": ["Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines,", "citeRegEx": "Fumera and Roli.,? \\Q2002\\E", "shortCiteRegEx": "Fumera and Roli.", "year": 2002}, {"title": "Support vector machines with a reject option", "author": ["Yves Grandvalet", "Alain Rakotomamonjy", "Joseph Keshet", "St\u00e9phane Canu"], "venue": "In NIPS,", "citeRegEx": "Grandvalet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2008}, {"title": "Classification with reject option", "author": ["Radu Herbei", "Marten H. Wegkamp"], "venue": "The Canadian Journal of Statistics,", "citeRegEx": "Herbei and Wegkamp.,? \\Q2006\\E", "shortCiteRegEx": "Herbei and Wegkamp.", "year": 2006}, {"title": "kernlab \u2013 an S4 package for kernel methods in R", "author": ["Alexandros Karatzoglou", "Alex Smola", "Kurt Hornik", "Achim Zeileis"], "venue": "Journal of Statistical Software,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2004}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Systems, Man and Cybernetics: Part\u2013B,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "Learning sparse classifiers with difference of convex functions algorithms", "author": ["Cheng Soon Ong", "Le Thi Hoai An"], "venue": "Optimization Methods and Software,", "citeRegEx": "Ong and An.,? \\Q2012\\E", "shortCiteRegEx": "Ong and An.", "year": 2012}, {"title": "A conservative approach to perceptron learning", "author": ["Ramasubramanian Sundararajan", "Asim K. Pal"], "venue": "WSEAS Transactions on Systems,", "citeRegEx": "Sundararajan and Pal.,? \\Q2004\\E", "shortCiteRegEx": "Sundararajan and Pal.", "year": 2004}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "Support vector machines with a reject", "author": ["Marten Wegkamp", "Ming Yuan"], "venue": "option. Bernaulli,", "citeRegEx": "Wegkamp and Yuan.,? \\Q2011\\E", "shortCiteRegEx": "Wegkamp and Yuan.", "year": 2011}, {"title": "Lasso type classifiers with a reject option", "author": ["Marten H. Wegkamp"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Wegkamp.,? \\Q2007\\E", "shortCiteRegEx": "Wegkamp.", "year": 2007}, {"title": "Robust truncated hinge loss support vector machines", "author": ["Yichao Wu", "Yufeng Liu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wu and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Wu and Liu.", "year": 2007}, {"title": "Classification methods with reject option based on convex risk minimization", "author": ["Ming Yuan", "Marten Wegkamp"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan and Wegkamp.,? \\Q2010\\E", "shortCiteRegEx": "Yuan and Wegkamp.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Since D(x, y) is generally assumed to be fixed but unknown, the empirical risk minimization principle (with its attendant caveats on minimizing complexity or structural risk) is used (Vapnik, 1998).", "startOffset": 183, "endOffset": 197}, {"referenceID": 9, "context": "L0\u2212d\u22121 in this case is described as follows (Wegkamp and Yuan, 2011; Herbei and Wegkamp, 2006):", "startOffset": 44, "endOffset": 94}, {"referenceID": 3, "context": "L0\u2212d\u22121 in this case is described as follows (Wegkamp and Yuan, 2011; Herbei and Wegkamp, 2006):", "startOffset": 44, "endOffset": 94}, {"referenceID": 3, "context": "For 2-class problems, the risk under L0\u2212d\u22121 is minimized by generalized Bayes discriminant (Herbei and Wegkamp, 2006; Chow, 1970) which is as below: f d (x) = \uf8f1", "startOffset": 91, "endOffset": 129}, {"referenceID": 12, "context": "Classifier h(f(x), \u03c1) (equation (3))is shown to be infinite sample consistent with respect to the generalized Bayes classifier f d (x) described in equation (5) (Yuan and Wegkamp, 2010).", "startOffset": 161, "endOffset": 185}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way.", "startOffset": 0, "endOffset": 52}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 265}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 281}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 310}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier. It is shown that a minimizer of risk under LGH is consistent to the generalized Bayes classifier f d (Bartlett and Wegkamp, 2008). Grandvalet et al. (2008) propose a risk minimization scheme under double hinge loss LDH (see Table 1) and show that resulting classifier is strongly universally consistent to the generalized Bayes classifier f d .", "startOffset": 0, "endOffset": 583}, {"referenceID": 5, "context": "However, SVM and other convex loss based classification algorithms are not robust to label noise in the data (Manwani and Sastry, 2013).", "startOffset": 109, "endOffset": 135}, {"referenceID": 11, "context": "ramp loss based risk minimization for classifier learning is more robust to noise (Wu and Liu, 2007) and gives sparse solutions compared to hinge loss based SVM.", "startOffset": 82, "endOffset": 100}, {"referenceID": 0, "context": "This makes it more suitable for scalability (Collobert et al., 2006; Ong and An, 2012).", "startOffset": 44, "endOffset": 86}, {"referenceID": 6, "context": "This makes it more suitable for scalability (Collobert et al., 2006; Ong and An, 2012).", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 151}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 180}, {"referenceID": 2, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al. (2008)).", "startOffset": 181, "endOffset": 206}, {"referenceID": 4, "context": "For solving the dual D(l) at every iteration, we have used the kernlab package (Karatzoglou et al., 2004) in R.", "startOffset": 79, "endOffset": 105}], "year": 2017, "abstractText": "We consider the problem of building classifiers with the option to reject i.e., not return a prediction on a given test example. Adding a reject option to a classifier is well-known in practice; traditionally, this has been accomplished in two different ways. One is the decoupled method where an optimal base classifier (without the reject option) is build first and then the rejection boundary is optimized, typically in terms of a band around the separating surface. The coupled method is based on finding both the classifier as well as the rejection band at the same time. Existing coupled approaches are based on minimizing risk under an extension of the classical 0 \u2212 1 loss function wherein a loss d \u2208 (0, .5) is assigned to a rejected example. In this paper, we propose a double ramp loss function which gives a continuous upper bound for (0 \u2212 d \u2212 1) loss described above. Our coupled approach is based on minimizing regularized risk under the double ramp loss which is done using difference of convex (DC) programming. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets.", "creator": "LaTeX with hyperref package"}}}