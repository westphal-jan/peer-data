{"id": "1311.6809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost", "abstract": "We introduce a novel family of adaptive filtering algorithms based on a relative logarithmic cost. The new family intrinsically combines the higher and lower order measures of the error into a single continuous update based on the error amount. We introduce important members of this family of algorithms such as the least mean logarithmic square (LMLS) and least logarithmic absolute difference (LLAD) algorithms that improve the convergence performance of the conventional algorithms. However, our approach and analysis are generic such that they cover other well-known cost functions as described in the paper. The LMLS algorithm achieves comparable convergence performance with the least mean fourth (LMF) algorithm and extends the stability bound on the step size. The LLAD and least mean square (LMS) algorithms demonstrate similar convergence performance in impulse-free noise environments while the LLAD algorithm is robust against impulsive interferences and outperforms the sign algorithm (SA). We analyze the transient, steady state and tracking performance of the introduced algorithms and demonstrate the match of the theoretical analyzes and simulation results. We show the extended stability bound of the LMLS algorithm and analyze the robustness of the LLAD algorithm against impulsive interferences. Finally, we demonstrate the performance of our algorithms in different scenarios through numerical examples.", "histories": [["v1", "Tue, 26 Nov 2013 10:02:20 GMT  (271kb)", "http://arxiv.org/abs/1311.6809v1", "Submitted to IEEE Transactions on Signal Processing"]], "COMMENTS": "Submitted to IEEE Transactions on Signal Processing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["muhammed o sayin", "n denizcan vanli", "suleyman s kozat"], "accepted": false, "id": "1311.6809"}, "pdf": {"name": "1311.6809.pdf", "metadata": {"source": "CRF", "title": "A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic Cost", "authors": ["Muhammed O. Sayin", "N. Denizcan Vanli", "Suleyman S. Kozat"], "emails": ["sayin@ee.bilkent.edu.tr,", "vanli@ee.bilkent.edu.tr,", "kozat@ee.bilkent.edu.tr)."], "sections": [{"heading": null, "text": "This year, the time has come: The European Commission has left the EU Presidency of the Council of the European Union to leave the EU Council Presidency to leave the EU Council Presidency. To leave the EU Presidency of the Council of the European Union to support the EU Presidency of the Council of the European Union."}, {"heading": "II. COST FUNCTION WITH LOGARITHMIC ERROR", "text": "12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,"}, {"heading": "III. NOVEL ALGORITHMS", "text": "Based on the gradient of J (et), we obtain the general steepest descendant progress aswt + 1 = steady noise convergence progress (.eu).Note 3.1: In the previous section, we motivate the logarithm-based margin of error framework as a continuous generalization of the switched standard algorithms. The connected standard update includes a cut-off \u03b3 compared to the error amount. Similarly, we use a design parameter \u03b1 in (1) to determine the asymptotic cut-off value. For example, a larger \u03b1 reduces the weight of the logarithmic term in the costs (1) and the resulting algorithm behaves more like minimizing the costs F (et).In the performance analysis, we show that a sufficiently small design parameter, i.e. \u03b1 = 1, has no determining influence on the constant state of noise convergence progressive adjustment."}, {"heading": "A. The Least Mean Logarithmic Square (LMLS) Algorithm", "text": "For F (et) = E [e2t], the stochastic gradient update results in wt + 1 = wt + \u00b5xtet e2t1 + e2t = wt + \u00b5 xte3 t1 + e2t. (8) Note that we include the multiplier \"2,\" which comes from the gradient ete2t = 2t, in the step variable \u00b5. The algorithm (8) resembles a smallest fourth update for the smallest error values, while it behaves like the smallest mean square algorithm for large errors of the error. Thus, a smaller stationary mean square error is obtained thanks to the statistics of the fourth order of the error for small errors and the stability of the smallest square algorithms for large errors. Therefore, the LMLS algorithm intrinsically combines the smallest mean square and the smallest mean fourth algorithm based on the error set of mixed LF + LMS combinations that need [11] in the LMS algorithms."}, {"heading": "B. The Least Logarithmic Absolute Difference (LLAD) Algorithm", "text": "The SA uses F (et) = E [| et |] as the cost function, which provides robustness against impulsive interference [1]. However, the SA has a lower convergence rate, since the L1 standard represents the smallest possible error for a convex cost function. In the logarithmic cost framework, F (et) = E [| et |], (7) for F (et) + 1 = wt + \u00b5xtsign (et) | et | 1 + | et | = wt + \u00b5 xtet1 + | et |. (9) The algorithm (9) combines the LMS algorithm and SA into a single robust algorithm with improved convergence performance. We note that in Section V we achieve the optimum to achieve better convergence performance than the SA in the impulsive noise environment, who choose optimum."}, {"heading": "C. Normalized Updates", "text": "We introduce normalized updates to the regressor signal to ensure independence from the input data correlation statistics under certain settings. We define the new objective function asJnew (et) - 1 \u03b1 ln (1 + \u03b1F (et-xt))), 5 for example F (et-xt) = E [e2t-xt]. The Hessian matrix of the new cost function Jnew (et) is also positively semi-definitive, provided that the Hessian matrix of F (et-xt) is positively semi-definitive, as shown in Remark 2.2.2. the steepest slope is represented by wt + 1 = wt-\u00b5-wF (et-xt) \u03b1F (et-xt) 1 + \u03b1F (et-xt). For F (et-xt) = E [et-xt) 2] we get the normalized lowest average logarithmic state."}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "We define a priority estimation error and the weighted form asea, t-T = xTt w-T and e-T-T = xTt-W-T, where w-T = wo-wt and \u03a3 is a symmetrical positive definitive weighting matrix. Different selection of \u03a3 leads to the different performance metrics of the algorithm [1]. In the analyses, we include the design parameter \u03b1-T to facilitate the theoretical analysis, and after a certain algebra, we obtain the weighted energy recursion [1], [22], [23] asE-W-T-T-T-T-T-T-T-T-T-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-T-T-T-T-T-T-T-T-T-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-e-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p)."}, {"heading": "A. Transient Analysis", "text": "In the following, we will consider the question of whether we will be able, in the next two years, to take the next five billion euros in our hands over the next three years over the next five years. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros. (...) We will take the next five billion euros."}, {"heading": "B. Steady State Analysis", "text": "In the steady state (11) and (15) the following state (et) = 2 hG (et) = 2 hG (et) E [e2a, t] = 2 hG (et) hG (et) = 2 Tr (R) hU (et) hG (et) (et). (18) Assuming 1, the steady state MSD is given by [23] s. (E) = 2 hG (et) = 2 Tr (R) hU (et) hG (et) hG (et). (18) Assuming 1, the steady state MSD is given by [23] s."}, {"heading": "C. Tracking Performance", "text": "In this subsection we examine the tracking performance of the introduced algorithms in a non-stationary environment. We proceed from a random walk model [1] for where, t + 1 = where, t + qt + qt (23), where qt-Rp is a zero-mean vector process with covariance matrix E [qtq T] = Q. We note that the model (23) has not changed the definitions of a priority error. Consequently, the tracking of the LMLS EMSs is identical to the tracking of the EMSs of the LMF and is given approximately by [1].LMLS = 3\u03b1\u00b5\u03c34nTr (R) + 1Tr (Q). Similarly, by adopting 6, we obtain the tracking of the EMSs of the LLLAD as its LLAD = \u03b1\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043d\u043e\u0441\u043e\u0441\u043d\u043e\u0441\u043d\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043e\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043e\u0441\u043d\u0435\u0441\u0441\u0441\u043d\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u043d\u0435\u0441\u0441\u043d\u0435\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u0435\u0441\u043d\u043d\u0435\u0441\u043d\u043d\u0435"}, {"heading": "V. COMPARISON WITH THE CONVENTIONAL ALGORITHMS", "text": "We emphasize again that the cost function J (et) essentially combines the costs F (et) and F 2 (et) based on the relative error amount, since the updates for minor errors of the error mainly use the costs F 2 (et). Based on our stochastic gradient approach, i.e. eliminating the expectation in the gradient descent, F 2 (et) and F (e2t) yield the same algorithm. Therefore, in this section we compare the stability of the LMLS algorithm with the LMF and LMS algorithms and analyze the robustness of the LLAD algorithm in impulsive noise environments."}, {"heading": "A. Stability Bound for the LMLS Algorithm", "text": "We refer once again to the stochastic gradient update (7), which we describe independently of the design parameter \u03b1. Therefore, we can intuitively determine that for the introduced algorithms the step size limit is at least as large as the step size limit for the corresponding conventional algorithm. Analytically, the step size \u00b5 should be satisfactory for stable updates E [e2a, t] w + 2] \u2264 E [e2b, t] p [e2c] p [e2c] p \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t t \u00b2 t \u00b2 t \u00b2 t t t \u00b2 t \u00b2 t \u00b2 t t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t t t t \u00b2 t \u00b2 t \u00b2 t t t t \u00b2 t \u00b2 t \u00b2 t"}, {"heading": "B. Robustness Analysis for the LLAD Algorithm", "text": "Although the performance analysis of the adaptive filters is based on white Gaussian noise signals, impulsive noise is assumed to be a common problem in practical applications [8]. To analyze the performance in impulsive noise environments, we use the following impulsive noise model: We model noise as a summation of two independent random terms [28], [29] asnt = no, t + btni, t, where no, t is the ordinary noise signal, Gaussian is with variance \u03c32no and ni, t is impulse noise, which is also Gaussian with significantly large variance \u03c32ni. Here bt is generated by a Bernoulli random process and determines the occurrence of impulses in the noise signal with pB (bt = 1) = pendi and pB (bt = 0) = 1 \u2212 empirical noise in the noise signal."}, {"heading": "VI. NUMERICAL EXAMPLES", "text": "In this section we compare in particular the convergence rate of the algorithms for the same stationary state MSD = 43 by the specific choice of step variables for a fair comparison. Here we have a stationary data dt = wTo xt + nt, where xt is zero Gaussian i.i.d. regression signal with variance \u03c32x = 1, representing zero-mean i.i.d. noise signal and the parameters of interest where R5 is randomly selected. In the following scenarios we compare the algorithms under Gaussian noise and impulsive noise models subsequentially. Scenario 1 (impulse-free environment): In this scenario we use a zero-mean noise i.i.d. noise signal with the variance \u03c32n = 0.01 and the design parameter MS = 1. In Fig. 7 we compare the convergence rate of the LMLS, LMF and LMS algorithms for relatively small increments."}, {"heading": "VII. CONCLUDING REMARKS", "text": "In this article, we present a novel family of adaptive filter algorithms based on the logarithmic error cost framework. We propose key members of the new family, namely the LMLS and LLAD algorithms. The LMLS algorithm achieves a comparable convergence performance with the LMF algorithm with far greater stability tied to the step size. In impulse-free environments, the LLAD algorithm exhibits a similar convergence performance with the LMS algorithm. In addition, the LLAD algorithm is robust against impulsive interference and exceeds the SA. In addition, we offer comprehensive performance analyses of the implemented algorithms, which are consistent with our simulation results. For example, the stationary state in impulse-free and impulse noise environments analyses. Finally, we show the improved convergence performance of the new algorithms in various system identification scenarios 11."}, {"heading": "APPENDIX A", "text": "The LMLS algorithm: We have the last term in (28) as follow E [11 + \u03b1e2t] and the first line of the equation follows the definition of g (et) in (12). According to assumption 2 we get the last term in (28) as follow E [11 + \u03b1e2t] = 1 \u03b54t and the first line of the equation follows the definition of g (\u2212 e 2 t2\u03c32t) in (12). According to assumption 2 we get the last term in (28) as follow E [11 + \u03b1e2t] = 1 \u03b54t \u00b2 and the first line in \u00b2. \u2212 \u2212 Exp (\u2212 e 2 t2\u03c32e) det = 1 \u043e\u0441\u0441\u0442u2) in (2 + u2 du) and the last line in (2 + u2 du)."}, {"heading": "APPENDIX B", "text": "EVALUATION OF hU (et) The LMLS algorithm: We havehU (et) = E [\u03b12e6t (1 + \u03b1e2t) 2] = E [\u2212 \u03b12 \u2202 \u03b1 (e4t 1 + \u03b1e2t)] = \u2212 \u03b12 \u2202 \u03b1 (E [e4t 1 + \u03b1e2t]), in the last line we have used the exchange of integration and differentiation properties since \u03b8 (et, \u03b1) = e4t 1 + \u03b1e2tand \u03b8 (et, \u03b1) \u03b1 (both applied continuously in R2. From Appendix A, weobtainhU (et) = \u2212 \u03b12 \u0445 \u03b1 (\u03b1 \u2212 1E [\u03b1e4t 1 + \u03b1e2t])))) = \u2212 \u03b12 \u0432 (\u03b1 \u2212 1 + \u03b52ehG (et) = \u03c32ehG (et) = \u03c32e (1 \u2212 2\u03bb + 2)."}], "references": [{"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "The least mean fourth (LMF) adaptive algorithm and its family,", "author": ["E. Walach", "B. Widrow"], "venue": "IEEE Trans. Inform. Theory, vol. 30,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1984}, {"title": "When is the least-mean fourth algorithm mean-square stable?", "author": ["V. Nascimento", "J. Bermudez"], "venue": "in Acoustics, Speech, and Signal Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Probability of divergence for the least-mean fourth algorithm,", "author": ["V. Nascimento", "J.C.M. Bermudez"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A mean-square stability analysis of the least mean fourth adaptive algorithm,", "author": ["P. Hubscher", "J. Bermudez", "V. Nascimento"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A simple model for the effect of normalization on the convergence rate of adaptive filters,", "author": ["V. Nascimento"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Signal processing with fractional lower order moments: stable processes and their applications,", "author": ["M. Shao", "C. Nikias"], "venue": "Proceedings of the IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Adaptive robust impulse noise filtering,", "author": ["S.R. Kim", "A. Efron"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Improved convergence analysis of stochastic gradient adaptive filters using the sign algorithm,", "author": ["V.J. Mathews", "S.-H. Cho"], "venue": "IEEE Trans. Acoust., Speech, Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "Least mean mixednorm adaptive filtering,", "author": ["J. Chambers", "O. Tanrikulu", "A. Constantinides"], "venue": "Electron. Lett., vol. 30,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "A robust mixed-norm adaptive filter algorithm,", "author": ["J. Chambers", "A. Avlonitis"], "venue": "IEEE Signal Processing Lett., vol. 4,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Separate-variable adaptive combination of lms adaptive filters for plant identification,", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "M. Martinez-Ramon", "A. Figueiras-Vidal"], "venue": "IEEE 13th Workshop on Neural Networks for Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "New algorithms for improved adaptive convex combination of lms transversal filters,", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "A. Figueiras-Vidal"], "venue": "IEEE Trans. Instrumentation and Measurement,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Mean-square performance of a convex combination of two adaptive filters,", "author": ["J. Arenas-Garcia", "A. Figueiras-Vidal", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Improving the tracking capability of adaptive filters via convex combination,", "author": ["M.T.M. Silva", "V. Nascimento"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Steady-state mse performance analysis of mixture approaches to adaptive filtering,", "author": ["S. Kozat", "A. Erdogan", "A. Singer", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Adaptive combination of normalised filters for robust system identification,", "author": ["J. Arenas-Garcia", "A. Figueiras-Vidal"], "venue": "Electron. Lett., vol. 41,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Robust huber adaptive filter,", "author": ["P. Petrus"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Scherbert, Introduction to Real Analysis", "author": ["D.R.R.G. Bartle"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A normalized least mean squares algorithm with a step-size scaler against impulsive measurement noise,", "author": ["I. Song", "P. Park", "R. Newcomb"], "venue": "IEEE Trans. Circuits Syst. II: Express Briefs, vol. 60,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Transient analysis of data-normalized adaptive filters,", "author": ["T.Y. Al-Naffouri", "A. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "A useful theorem for nonlinear devices having gaussian inputs,", "author": ["R. Price"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1958}, {"title": "An extension of price\u2019s theorem (corresp.),", "author": ["E. McMahon"], "venue": "IEEE Trans. Inform. Theory, vol. 10,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1964}, {"title": "Efficient methods of estimate correlation functions of gaussian processes and their performance analysis,", "author": ["T. Koh", "E. Powers"], "venue": "IEEE Trans. Acoust., Speech, Signal Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1985}, {"title": "Joint channel estimation and symbol detection in rayleigh flat-fading channels with impulsive noise,", "author": ["X. Wang", "H. Poor"], "venue": "IEEE Comm. Lett.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "A recursive least m-estimate algorithm for robust adaptive filtering in impulsive noise: fast algorithm and convergence performance analysis,", "author": ["S.-C. Chan", "Y.-X. Zou"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "The least mean square (LMS) and normalized least mean square (NLMS) algorithms are the members of this class [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "In the literature, different powers of the error are commonly used as the cost function in order to provide stronger convergence or steady-state performance than the least-squares algorithms under certain settings [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 1, "context": ", E[e t ] [2].", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "transient and steady-state performance, however, has stability issues [3]\u2013[5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "transient and steady-state performance, however, has stability issues [3]\u2013[5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "On the other hand, the stability of the conventional LMS algorithm depends only on the input power for a given step-size [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "The normalized filters improve the performance of the algorithms under certain settings by removing dependency to the input statistics in the updates [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": ", in applications involving high power noise signals [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "In this context, we define robustness as the insensitivity of the algorithms against the impulsive interferences encountered in the practical applications and provide a theoretical framework [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 8, "context": "However, the SA usually exhibits slower convergence performance especially for highly correlated input signals [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "The mixed-norm algorithms minimize a combination of different error norms in order to achieve improved convergence performance [11], [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "The mixed-norm algorithms minimize a combination of different error norms in order to achieve improved convergence performance [11], [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "For example, [12] combines the robust L1 norm and the more sensitive but better converging L2 norm through a mixing parameter.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "On the other hand, the mixture of experts algorithms adaptively combine different algorithms and provide improved performance irrespective of the environment statistics [13]\u2013 [16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "On the other hand, the mixture of experts algorithms adaptively combine different algorithms and provide improved performance irrespective of the environment statistics [13]\u2013 [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "However, note that such mixture approaches require to operate several different algorithms on parallel, which may be infeasible in different applications [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "In [18], authors propose an adaptive combination of L1 and L2 norms of the error in parallel, however, the resulting algorithm demonstrates impulsive perturbations on the learning curves.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In general, the samples contaminated with impulses contain little useful information [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "The switched-norm algorithms switch between the L1 and L2 norms based on the error amount such as the robust Huber filter [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "We particularly choose the logarithm function as the normalizing diminishing return function [20] in our cost definitions since the logarithmic function is differentiable and results efficient and mathematically tractable adaptive algorithms.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": ", the mean square error E[et ] [1].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The different powers of et [2], [10] or a linear combination of different error powers [11], [12] are also widely used.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "1: In [21], the authors propose a stochastic cost function using the logarithm function as follows", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "J [21](et) \u25b3 = 1 2\u03b3 ln (", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Note that the cost function J [21](et) is the subtracted term in (1) for F (et) = e2t \u2016xt\u2016 .", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The Hessian matrix of J [21](et) is given by H (", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "J [21](et) )", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "J [21](et) )", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "On the other hand, we show that the new cost function in (1) is a convex function enabling the use of the diminishing return property [20] of the logarithm function for stable and robust updates.", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "As an example, the Huber objective function combining L1 and L2 norms of the error is defined as [19] \u03c1(et) \u25b3 = { 1 2e 2 t for |et| \u2264 \u03b3, \u03b3|et| \u2212 1 2\u03b32 for |et| > \u03b3, (5) where \u03b3 > 0 denotes the cut-off value.", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "Hence, the LMLS algorithm intrinsically combines the least mean-square and least-mean fourth algorithms based on the error amount instead of mixed LMF + LMS algorithms [11] that need artificial combination parameter in the cost definition.", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The Least Logarithmic Absolute Difference (LLAD) Algorithm The SA utilizes F (et) = E[|et|] as the cost function, which provides robustness against impulsive interferences [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Different choice of \u03a3 leads to the different performance measures of the algorithm [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "After some algebra, we obtain the weighted-energy recursion [1], [22], [23] as E [", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "After some algebra, we obtain the weighted-energy recursion [1], [22], [23] as E [", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "The Gaussian estimation error assumption is acceptable for sufficiently small step size \u03bc and through the Assumption 1 [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 21, "context": "(14) Proof: The proof of Lemma 1 follows from the Price\u2019s result [24], [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "(14) Proof: The proof of Lemma 1 follows from the Price\u2019s result [24], [25].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "That is, for any Borel function g(b) we can write E[xg(y)] = E[xy] E[y2] E[yg(y)], where x and y are zero-mean jointly Gaussian random variables [26].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "We assume a random walk model [1] for wo,t such that wo,t+1 = wo,t + qt (23) where qt \u2208 R is a zero-mean vector process with covariance matrix E[qtq T t ] = Q.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Hence, by the Assumption 5, the tracking EMSE of the LMLS is the same with the tracking EMSE of the LMF and is approximately given by [1] \u03b6 LMLS \u2248 3\u03b1\u03bc\u03c3 nTr(R) + \u03bc Tr(Q) 6\u03c32 n .", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "We re-emphasize that the LMLS extends the stability bound of the LMS algorithm (the same bound with \u03b2 = 1) while performing comparable performance with the LMF algorithm, which has several stability issues [3]\u2013[5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "We re-emphasize that the LMLS extends the stability bound of the LMS algorithm (the same bound with \u03b2 = 1) while performing comparable performance with the LMF algorithm, which has several stability issues [3]\u2013[5].", "startOffset": 210, "endOffset": 213}, {"referenceID": 6, "context": "Robustness Analysis for the LLAD Algorithm Although the performance analysis of the adaptive filters assumes the white Gaussian noise signals, in practical applications the impulsive noise is a common problem [8].", "startOffset": 209, "endOffset": 212}, {"referenceID": 24, "context": "Impulsive noise model: We model the noise as a summation of two independent random terms [28], [29] as nt = no,t + btni,t, where no,t is the ordinary noise signal that is zero-mean Gaussian with variance \u03c3 no and ni,t is the impulse-noise that is also zero-mean Gaussian with significantly large variance \u03c3 ni .", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "Impulsive noise model: We model the noise as a summation of two independent random terms [28], [29] as nt = no,t + btni,t, where no,t is the ordinary noise signal that is zero-mean Gaussian with variance \u03c3 no and ni,t is the impulse-noise that is also zero-mean Gaussian with significantly large variance \u03c3 ni .", "startOffset": 95, "endOffset": 99}], "year": 2013, "abstractText": "We introduce a novel family of adaptive filtering algorithms based on a relative logarithmic cost. The new family intrinsically combines the higher and lower order measures of the error into a single continuous update based on the error amount. We introduce important members of this family of algorithms such as the least mean logarithmic square (LMLS) and least logarithmic absolute difference (LLAD) algorithms that improve the convergence performance of the conventional algorithms. However, our approach and analysis are generic such that they cover other well-known cost functions as described in the paper. The LMLS algorithm achieves comparable convergence performance with the least mean fourth (LMF) algorithm and extends the stability bound on the step size. The LLAD and least mean square (LMS) algorithms demonstrate similar convergence performance in impulse-free noise environments while the LLAD algorithm is robust against impulsive interferences and outperforms the sign algorithm (SA). We analyze the transient, steady state and tracking performance of the introduced algorithms and demonstrate the match of the theoretical analyzes and simulation results. We show the extended stability bound of the LMLS algorithm and analyze the robustness of the LLAD algorithm against impulsive interferences. Finally, we demonstrate the performance of our algorithms in different scenarios through numerical examples.", "creator": "LaTeX with hyperref package"}}}