{"id": "1311.7385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2013", "title": "Algorithmic Identification of Probabilities", "abstract": "The problem is to identify a probability mass function associated with a discrete set, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function is computable and has a finite mean, then there is an algorithm to almost surely identify the target in the limit. If the probability mass function is incomputable, has a finite mean, and the drawing is i.i.d., then there is an algorithm which pointwise converges to the target almost surely. Here we use the strong law of large numbers. If the elements of the sequence are dependent and the sequence is typical (random) for at least one computable measure and the set is finite, then there is an algorithm to almost surely identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). Here we use the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions.", "histories": [["v1", "Thu, 28 Nov 2013 17:44:45 GMT  (20kb)", "https://arxiv.org/abs/1311.7385v1", "21 pages LaTeX. arXiv admin note: substantial text overlap witharXiv:1208.5003"], ["v2", "Thu, 5 Jun 2014 17:10:33 GMT  (18kb)", "http://arxiv.org/abs/1311.7385v2", "19 pages LaTeX.Corrected errors which also resulted inarXiv:1405.5139. arXiv admin note: text overlap witharXiv:1208.5003"], ["v3", "Fri, 11 Jul 2014 17:10:27 GMT  (18kb)", "http://arxiv.org/abs/1311.7385v3", "19 pages LaTeX.Corrected errors and rewrote the entire paper. arXiv admin note: text overlap witharXiv:1208.5003"]], "COMMENTS": "21 pages LaTeX. arXiv admin note: substantial text overlap witharXiv:1208.5003", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["paul m b vitanyi", "nick chater"], "accepted": false, "id": "1311.7385"}, "pdf": {"name": "1311.7385.pdf", "metadata": {"source": "CRF", "title": "Algorithmic Identification of Probabilities", "authors": ["Paul M.B. Vit\u00e1nyi"], "emails": ["paulv@cwi.nl.", "Nick.Chater@wbs.ac.uk."], "sections": [{"heading": null, "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "A. Preliminaries", "text": "N denotes the natural numbers and R denotes the real numbers. We say that we identify a function f in the boundary if we have an algorithm that generates an infinite sequence f1, f2,... of functions and fi = f for all, but an infinite number i. This corresponds to the concept of \"identification in the boundary\" in [6], [9], [16], [20]. In this term, an object is produced at each step and after a finite number of steps the target object is produced at each step. However, we do not know this finite number. It is as if you ask for instructions and the answer is \"turn right at the last intersection,\" but you do not know which intersection is the last one. In the sequence we often \"dovetail\" a calculation. This is a technique that intersects the steps of various calculations that ensure the progress of each individual calculation."}, {"heading": "B. Related work", "text": "In this case it is so that it is a purely calculated total function. (i) Each Qi is a probability mass function. (ii) We have a calculated total function. (i, x, x) We give one such that Qi (x) -r-r-r-r-e-r-e-r-e-r-e-e-r-e-e-r-e-e-r-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e-e"}, {"heading": "C. Results", "text": "The set of bearing algorithms for calculable probabilities (or quantities) is not c.e., Lemma 1 in Appendix A. This complicates the algorithms and the analysis of the results. In Section II there is a calculable probability mass function (the target) on a series of natural numbers. We get an infinite sequence of elements of this set, which are i.e. drawn, and are asked to identify the target. An algorithm is presented that almost certainly identifies the target in the limit, provided the target is an element of a c.e. or co-c.e. set of bearing algorithms for probability mass functions (theorem 1). This partially supports the result announced in [8]. The technical tool is the strong law of large numbers. In Section III the set of natural numbers is finite and the elements of the sequence calculation may be dependent."}, {"heading": "II. COMPUTABLE PROBABILITY MASS FUNCTIONS AND I.I.D. DRAWING", "text": "To get an approximate probability in the i.i.d. setting is known and a simple example illustrating our problem. One does this by an algorithm calculation that shows the probability p (a) in the limit for all A (L). If the infinite sequence x1, x2,.. of the data i.i.d. of the data drawn by L (2), the probability is that the different values of pn sum are to exactly 1, 2,.. the output is a sequence pn (a) equal to the frequency of occurrences in x1, x2,.., xn. the different values of pn sum are to exactly 1, 2,. The output is a sequence p1, p2,. of probability mass functions as we have limn."}, {"heading": "III. COMPUTABLE MEASURES", "text": "As far as the authors are aware, there is neither an approximation as in Section II nor an analogy of the strong law of large numbers for general metrics. However, there is an idea of the typicality of an infinite data sequence for a typical metric in the Martin-Lo-f theory of sequential tests [15] based on Kolmogorov complexity, and this is what we are using. Let's use the simpler notation \u00b5 (x). We get a sequence in L \u00b2 that is typical (definition 1) for a number of maintenance algorithms for calculable metrics. In this paper, instead of the usual notation \u00b5 (x) we use the simpler notation \u00b5 (x). We get a sequence in L \u00b2 that is typical. The constituent elements of the sequence may be dependent."}, {"heading": "IV. PREDICTION", "text": "In Section II, the data i.i.d. is given according to a probability mass function p on the elements of L. Given p, we can predict the probability p (a | x1,.., xn) that the next draw will yield an element a if the previous draws yield x1,.., xn. The resulting measurement on L \u221e is called the i.i.d. measurement. In general measurements such as in Section III, which allow dependent data, the situation is quite different. We can encounter the so-called black swan phenomenon of [17]. Let's take a simple example. The data sequence is a, a,.. is typical (definition 1) for the measurement \u00b51, which is defined by \u00b51 (x) = 1 for each data sequence x, consisting of a finite or infinite string of a and \u00b51 (x) = 0 otherwise. But a, a,.. is also typical for predicting \u00b50 (x) = 12 for each string of characters that follows either a finite string of a number or a finite string of a string of a line or 0.1."}, {"heading": "V. CONCLUSION", "text": "Using an infinite sequence of elements from a series of natural numbers, algorithms are presented that identify in the boundary the probability distribution associated with this set in two cases: (i) the target distribution is a probability mass function (i.e. a measurement variable) in a series of calculable probability mass functions (calculable i.e. measurement variables) and the elements of the sequence are drawn according to this probability (theorem 1); (ii) the underlying quantity is finite and the infinite sequence may be dependent and typical of a calculable measurement variable in a series of calculable quantities (theorem 2). In the case of the target, the calculable probability mass function is almost certainly identified in the boundary, i.e. in the dependent case, the target is determined - it is one of a series of calculable quantities."}, {"heading": "A. Computability", "text": "A real function f with rational argument is lower semicomputable if it is defined by a rationally rated computable function \u03c6 (x, k). This means that f can be a calculable number and k a nonnegative integer approaching from below (see [14], p. 35). A function f is upper semicomputable if \u2212 f is semicomputable if it is semicomputable. If a real function is both lower semicomputable and upper semicomputable. A function f is a semiprobable mass function if we write x (x) a probability mass function if we write (x) f (x) for f (x)."}, {"heading": "B. Kolmogorov Complexity", "text": "We need the Kolmogorov complexity theory [14] (originally in [12]) and the prefix version we use here in [13]. A Turing machine is a Turing machine with a one-way read-only input tape with a distinguishable tape cell named Origin, a finite number of two-way write-work tapes on which the calculation is performed, an auxiliary tape on which the auxiliary string y [0, 1} \"is written, and a one-way-only output tape. At the beginning of the calculation, the input tape is executed infinitely x x,\" and the input head is at origin x. The machine works with a binary alphabet. If the machine then holds an input head, it has scanned a segment of the input tape."}, {"heading": "C. Measures, Semimeasures, and Computability", "text": "If one considers a finite sequence x = x1, x2,.., xn of the elements of L, we consider the set of finite sequences beginning with x. Therefore, the set of all such sequences is written as \"x,\" the cylinder of X. However, we associate a probability (x) with the event that an element of X. The probabilities associated with these subsets are derived from the probabilities of the cylinder sequences and write \"x.\" The transitive closure of the intersection, the complement and the countable union of cylinders results in a set of subsets of X. The probabilities associated with these subsets are derived from the probabilities of the cylinder sequences. [10]. A semimeasure \u00b5 fulfills the following elements (x\u00b5)."}, {"heading": "D. Proofs of the Theorems", "text": "Proof: The strong law of large, originally 11 divided states (which we have almost shared) [2] [3] [4] [4] Let L'N, and X1, X2,.. be a sequence of mutually independent random variables, each of which is a copy of a single random variable X with the probability mass function P (X = a) = p (a) for a random variable P (a) > 0 for all is a random variable L. Let # a (x1, x2, xn) the number of random variables X = a (1).CLAIM 1: If the results of the random variables X1, X2, X2. are x2, then we almost certainly have a whole set of random variables L (a)."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank Laurent Bienvenu for pointing out an error in the earlier version and explaining the comments. Drafts of this essay have been written since 2012 in various correctness states from arXiv: 1208.5003 to arXiv: 1311.7385."}], "references": [{"title": "Identifying languages from stochastic examples", "author": ["D. Angluin"], "venue": "Yale University, Dept. of Computer Science, Technical report, New Haven, Conn., USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Minimum complexity density estimation", "author": ["A.R. Barron", "T.M. Cover"], "venue": "IEEE Trans. Inform. Th., 4:37", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Merging of opinions with increasing information", "author": ["D Blackwell", "L Dubins"], "venue": "The Annals of Mathematical Statistics, 33:3", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1962}, {"title": "Limiting recursion", "author": ["E.M. Gold"], "venue": "J. Symb. Logic, 30", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1965}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Inform. Contr., 10", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1967}, {"title": "An Introduction to Probability", "author": ["W. Feller"], "venue": "Theory and Its Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1968}, {"title": "The probabilistic analysis of language acquisition: Theoretical", "author": ["A. Hsu", "N. Chater", "P.M.B. Vit\u00e1nyi"], "venue": "computational, and experimental analysis, Cognition,120", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Systems that Learn", "author": ["S. Jain", "D.N. Osherson", "J.S. Royer", "A. Sharma"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Grundbegriffe der Wahrscheinlichkeitsrechnung", "author": ["A.N. Kolmogorov"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1933}, {"title": "Sur la loi forte des grandes nombres", "author": ["A.N. Kolmogorov"], "venue": "C. r. Acad. Sci. Paris, 191(1930), 910\u2013912. See also A.N. Kolmogorov, Grundbegriffe der Wahrscheinlichkeitsrechnung, Springer-Verlag, Berlin, 1933. See also F.P. Cantelli, Sulla probabilit\u00e1 come limite della frequenza, Rendiconti della R. Academia dei Lincei, Classe di scienze fisische matematiche e naturale, Serie 5a, 26", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1917}, {"title": "Three approaches to the quantitative definition of information", "author": ["A.N. Kolmogorov"], "venue": "Problems Inform. Transmission, 1:1", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1965}, {"title": "Laws of information conservation (non-growth) and aspects of the foundation of probability theory", "author": ["L.A. Levin"], "venue": "Problems Inform. Transmission, 10", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1974}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications, Springer-Verlag", "author": ["M. Li", "P.M.B. Vit\u00e1nyi"], "venue": "New York,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "The definition of random sequences", "author": ["P. Martin-L\u00f6f"], "venue": "Inform. Control, 9:6", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1966}, {"title": "Formal models of language learning", "author": ["S. Pinker"], "venue": "Cognition 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1979}, {"title": "The Logic of Scientific Discovery", "author": ["K.R. Popper"], "venue": "Hutchinson, London", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1959}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "part 1 and part 2, Inform. Contr., 7:1\u201322, 224\u2013254", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1964}, {"title": "On computable numbers", "author": ["A.M. Turing"], "venue": "with an application to the Entscheidungsproblem, Proc. London Mathematical Society 2, 42(1936), 230\u2013265, \u201dCorrection\u201d, 43", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1937}, {"title": "Learning recursive functions: a survey", "author": ["T. Zeugmann", "S. Zilles"], "venue": "Theoret. Comput. Sci., 397", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "The learnability of a language under various computational assumptions is the subject of an immensely influential approach in [5] and especially [6], or the review [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "We shall consider identification in the limit (following, for example, [6], [9], [16]).", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "We shall consider identification in the limit (following, for example, [6], [9], [16]).", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "We shall consider identification in the limit (following, for example, [6], [9], [16]).", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "In the realm of algorithmic information theory, in particular in Solomonoff induction [18] and here, we reason as follows.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "The possible strategies of learners are computable in the sense of Turing [19], that is, they are computable functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "This corresponds to the notion of \u201cidentification in the limit\u201d in [6], [9], [16], [20].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "This corresponds to the notion of \u201cidentification in the limit\u201d in [6], [9], [16], [20].", "startOffset": 72, "endOffset": 75}, {"referenceID": 14, "context": "This corresponds to the notion of \u201cidentification in the limit\u201d in [6], [9], [16], [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "This corresponds to the notion of \u201cidentification in the limit\u201d in [6], [9], [16], [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "In [1] (citing previous more restricted work) a target probability mass function was identified in the limit when the data are drawn i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2] computability questions are apparently ignored.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "If L(q) is identified with the Kolmogorov complexity K(q), as in Section IV of this reference, then it is incomputable as already observed by Kolmogorov in [12] (for the plain Kolmogorov complexity; the", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "The results hold (contrary to what is claimed in the Conclusion of [2] and other parts of the text) not for the set of computable probability mass functions since they are not c.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "The sentence \u201cyou know but you don\u2019t know you know\u201d on the second page of [2] does not hold", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "This underpins partially the result announced in [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "The technical tool is the Martin-L\u00f6f theory of sequential tests [15] based on Kolmogorov complexity.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "In order that it is computable we only require that the probability mass function is finitely describable and there is a computable process producing it [19].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "stronger than the result in [1] referred to in Section I-B.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "It is more theoretical but strictly stronger than [2] that does not give identification in the limit for classes of computable functions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 13, "context": "However, there is a notion of typicality of an infinite data sequence for a computable measure in the Martin-L\u00f6f theory of sequential tests [15] based on Kolmogorov complexity, and this is what we use.", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "We can meet the so-called black swan phenomenon of [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "In [3] Blackwell and Dubin show that under certain conditions predictions of two measures merge asymptotically almost surely.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "For the dependent case we use typicality according to the theory developed by Martin-L\u00f6f in [15] embedded in theory of Kolmogorov complexity.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "This means that f can be computably approximated arbitrary close from below (see [14], p.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "We need the theory of Kolmogorov complexity [14] (originally in [12] and the prefix version we use here in [13]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "To distinguish machines like U from other universal machines, Kolmogorov [12] called machines like U optimal.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "It can be shown that K(x|y) is incomputable [12].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "the probabilities of the cylinders in standard ways [10].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "The sequence is typical for a computable measure \u03bc if it passes all computable sequential tests (known and unknown alike) for randomness with respect to \u03bc in the sense of Martin-L\u00f6f [15].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "One of the highlights of the theory of Martin-L\u00f6f is that the sequence passes all these tests iff it passes a single universal test, [14] Corollary 4.", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "2 on p 315, see also [15].", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "The theory and properties of such sequences for computable measures are extensively treated in [14] Chapter 4.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Proof: The strong law of large numbers (originally in [11]) states that if we perform the same experiment a large number of times, then almost surely the number of successes divided by the number of trials goes to the expected value, provided the mean exists, see the theorem on top of page 260 in [7].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Proof: The strong law of large numbers (originally in [11]) states that if we perform the same experiment a large number of times, then almost surely the number of successes divided by the number of trials goes to the expected value, provided the mean exists, see the theorem on top of page 260 in [7].", "startOffset": 298, "endOffset": 301}, {"referenceID": 5, "context": "np(a)p(\u0101) < \u221a 2\u03bb lg n for every \u03bb > 1 and n is large enough for all a \u2208 L, see [7] p.", "startOffset": 79, "endOffset": 82}, {"referenceID": 12, "context": "1 in [14] (originally in [21], [13]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "1 in [14] (originally in [21], [13]).", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "1 of [14] pp.", "startOffset": 5, "endOffset": 9}], "year": 2014, "abstractText": "The problem is to identify a probability associated with a set of natural numbers, given an infinite data sequence of elements from the set. If the given sequence is drawn i.i.d. and the probability mass function involved (the target) belongs to a computably enumerable (c.e.) or co-computably enumerable (co-c.e.) set of computable probability mass functions, then there is an algorithm to almost surely identify the target in the limit. The technical tool is the strong law of large numbers. If the set is finite and the elements of the sequence are dependent while the sequence is typical in the sense of Martin-L\u00f6f for at least one measure belonging to a c.e. or co-c.e. set of computable measures, then there is an algorithm to identify in the limit a computable measure for which the sequence is typical (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We give the algorithms and consider the associated predictions.", "creator": "LaTeX with hyperref package"}}}