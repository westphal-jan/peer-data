{"id": "1311.7679", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2013", "title": "Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches", "abstract": "The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, user's aggregated purchase history and competitive online travel agency information for each potential hotel choice. This paper describes the solution of team \"binghsu &amp; MLRush &amp; BrickMover\". We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model's output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition.", "histories": [["v1", "Fri, 29 Nov 2013 20:01:10 GMT  (314kb,D)", "http://arxiv.org/abs/1311.7679v1", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xudong liu", "bing xu", "yuyu zhang", "qiang yan", "liang pang", "qiang li", "hanxiao sun", "bin wang"], "accepted": false, "id": "1311.7679"}, "pdf": {"name": "1311.7679.pdf", "metadata": {"source": "CRF", "title": "Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches", "authors": ["Xudong Liu", "Bing Xu", "Yuyu Zhang", "Qiang Yan", "Liang Pang", "Qiang Li", "Hanxiao Sun", "Bin Wang"], "emails": ["pl8787}@gmail.com", "4yanqiang.yq@taobao.com", "NDCG@38"], "sections": [{"heading": null, "text": "I. INTRODUCTIONICDM Challenge 2013 requires learning the ranking of hotels in order to maximize purchases for given hotel requests by Expedia.com. The data set provided by Expedia.com includes hotel characteristics, the attractiveness of the hotel, the totality of the purchase history and competitive OTA information for each searchid-hotel pair. Hotels for each user request are assigned relevance scores as follows: 5 for the user who purchased a room; 1 for the user who clicked on the hotel information and 0 for the user who neither clicked nor booked. The data will be randomly divided by the organizers. The training data includes 399,344 unique search lists and 9,917,530 points. The test data contain 266,230 search lists and 6,622,629 points. 25% of the test data will be used for evaluation in the public ranking and the remaining 75% will be used as final private test data. The Cain Rating Metric is usually used in this competition (the G)."}, {"heading": "II. FRAMEWORK", "text": "This section introduces the architecture and software we have used in our system, and then discusses the internal validation set from the training set, which is important for model evaluation and combination of different models."}, {"heading": "A. System Overview", "text": "The data infrastructure is based on pandas [4]. In this step, we use pandas to store data. And, at this stage, we do some feature engineering. The output of the data infrastructure can be done in various formats, such as numpy binary array, SVMRank [5] text format and LIBSVM [6] text format. In the second step, we examine various approaches to generate different models, including logistic regression, support vector engine, random forest, gradient boosting machine, factorization engine, LambdaMART and deep neural network. In the last step, we combine all the different results on the internal validation set and test set, using a list-based approach, a linear approach and a deep neural network.We use LIBLINEAR [7] and SVMRank [5] for pairwise logistic regression; Random Forest [8] from scikit-learn [2]; ranking algorithms like Aht.org [8]."}, {"heading": "III. PREPROCESSING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Listwise Feature for Point/Pairwise Model", "text": "Some of the features in the list are used as a single feature for each hotel choice. Here are the main features of the list review. \u2022 Price usd \u2022 Prop Starrating \u2022 Prop Location score2Other features of the list review that we suggested but could not rate, including: Rank of Exp (Prop Log Historical Price) price etc, Rank of Click / Booking bias etc. Listwise Features is a bridge to bring list-by-list information to point / pair models."}, {"heading": "B. Composite Features", "text": "Composite Features is a method that combines two different features: For example, we now combine srch number of rooms and srch booking window, the feature count window corresponds to srch number of rooms \u0445 max (srch booking window) + srch booking window. 5http: / / www.libfm.org /"}, {"heading": "C. Dealing Missing Feature Values", "text": "There are many missing characteristic values in the data, such as the value of the support site2. We use the first quartile calculated by the country where the data is located to represent the missing data."}, {"heading": "D. Use 10% data", "text": "We sampled 10% of the data by srch id to generate new training data. Using the new training data, we can train a model with very little difference to the model that is trained by the overall training data."}, {"heading": "E. Use Balanced Data", "text": "Since there are only 4.4% positive data points among the 9.9 million data points, we choose a positive example and randomly choose a negative example. In this way, we can train tree-based models with a large number of trees in a reasonable time."}, {"heading": "F. Split Data by prop country id", "text": "Based on the Prop Country ID, we split the data into 172 parts and train independent models on each piece. This method significantly reduces the time spent training tree-based models."}, {"heading": "G. Use Bucket to Binarize Float Feature", "text": "Algorithm 1 Bucket (feature, bucket number) Required: An integer BUCKET > 0description = {} binary feature = zeros (((Feature.size, bucket number) for i = 1 to bucket number do description [i] = feature. Quantile (i / bucket number) end for i = 0 to feature. size do j = 1 while feature.at (i) < description [j] do j = j + 1end while binary feature [i] [j] = 1end for the return of binary features."}, {"heading": "IV. MODELS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Logistic Regression", "text": "As a classic binary classification model, logistic regression is used as our first attempt in this competition. We have tried both binary logistic regression and multinomial logistic regression, while the former obviously performs better. In this part, therefore, we will only introduce our approach to binary logistic regression. We first process the data by merging the clicked and booked items within each query as positive instances, while all remaining items are considered as negative instances. With some feature engineering work, which will be discussed in more detail later, all instances can be presented as a series of feature vectors. Now, it is becoming a standard binary classification problem, although the data here is very unbalanced as the negative instances are overwhelming. Therefore, we then adjust the weight in the cross entropy error function (also known as negative log likelihood function) to address the problem of data imbalance as the error function is relatively overbalanced in the negative instances."}, {"heading": "B. Pairwise Logistic Regression", "text": "The FTRLProximal Algorithm [18] can be considered a hybrid of FOBOS and RDA algorithms, significantly exceeding both on a large, real dataset. We use the entire training set to train this model with only seven traits. The feature list is srch id, prop id, srch destination id, prop starrating, prop location score1, prop location score2, price usd. And this single FTRL model archives 0.51273 NDCG @ 38 on validation set."}, {"heading": "C. Random Forest", "text": "After creating a split dataset in 172 parts using the Prop-Country-ID, we balanced each piece. For each balanced Prop-Country-ID dataset, we train an independent random forest model [8] with 3200 trees. Using listwise ranking characteristics, the 172 random forest models in the internal validation set reach almost 0.51 NDCG @ 38 points. Some failures occurred in the prediction for some countries, so we simply combine the value with a paired logistic regression, which is trained by a liblinar [7] model with 0.47 NDCG @ 38 points in validation, and then the mixed balanced random forest model reaches almost 0.52 NDCG @ 38 in validation."}, {"heading": "D. Gradient Boosting Machine", "text": "Gradient Boosting Machine (GBM) [12] is a machine learning technique for regression problems that produces a predictive model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model gradually, as other boosting methods do, and generalizes it by allowing the optimization of any differentiable loss function. The Gradient Boosting method can also be used for classification problems by reducing it to regression with a suitable loss function. We use GBM in R language with the balanced data. The 20 most relevant features are shown in Table I. Fm-Score and Lr-Score is the rank predicted by simple Factorization Machine and Linear Regression, using only visitor and query characteristics. Date time is transformed into Unix stamp as a continuous feature. The suffix name with the suffix means that the trait is a combination of positive and positive characteristics."}, {"heading": "E. Extreme Randomized Trees", "text": "Extremely Randomized Trees (ERT) is proposed by [13]. This method is similar to the Random Forests algorithm in that it is based on selecting a random subset of K attributes on each node to decide on the division. Unlike the Random Forests method, each tree is randomly selected from the complete study sample (without a bootstrap copy) and, most importantly, for each of the attributes (randomly selected on each internal node) a discretion threshold (cut point) to define a division, rather than choosing the best cut point based on the local sample (as in Tree Bagging or the Random Forests method)."}, {"heading": "F. Factorization Machine", "text": "The factorization engine [15] is often used in the Recommender System. It is also a kind of regression model and can therefore be used as a useful model. We have done a lot of work in the Feature Engineering and in the Individual Model Archives 0.5171 NDCG @ 38 on the validation set. As the feature engineering and the model itself are very diverse, it works well together. Some of the features are in Tab. II. Our features are structured differently for the model of the Factorization Engine. Some of the features are standardized (e.g. per location score1), others are divided into containers (e.g. srch booking window). The ranking (e.g. price rank) means the ranking value of the identical query, which works quite well in this model. The central point of this model is that features should be used in the right way and ranking features bring list information to this point-oriented model."}, {"heading": "G. LambdaMART", "text": "LambdaMART model [10] [11] with CTR (click rates Eq 2) and CVR (booking rates Eq 3) features and original features at full rate. CTR and CVR are calculated in two scales, prop id and discrete price usd (fig. 2). This model archives 0.51149 NDCG @ 38 on validation set.CTRi = # (Clicki) # (Presentationi) (2) CV Ri = # (Bookingi) # (Clicki) (3) Two of the team members use LambdaMART independently of each other. The other LambdaMART model is based on normalized features, ranking features and result characteristics. Some features are listed in Tab. III and this single LambdaMART model archives 0.5243 NDCG @ 38 on validation. All of these features except fm score and lr score are introduced in the factorization part."}, {"heading": "H. Deep Learning Approach", "text": "In practice, we first transform selected features by using algorithm 1. To make it easier, we choose to build independent models for each country using the same network frame in the figure. 3. We choose Maxout network [16] because it provides much more regulation than other network layer. We discover the following situations during pre-training of denoising autoencoders: 1) reconstruction errors that are fixed by a large number (in most cases). 2) reconstruction errors are gradually becoming small (especially for the prop country id = 219) As the result shown above, composite characteristic is important. We want to use the 3-layer denozing autoencoder to find at least 3 layers and robust composite characteristics."}, {"heading": "V. ENSEMBLE MODELS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. z-score", "text": "We have experimented with several linear methods to combine, and the linear combination is nothing more than selecting several effective rangers and assigning them a set of parameters to help them achieve a better result than ever before a single model. The simplest, but also best method we tried was the Z-Score Ensemble Eq 4. We added a corresponding score to each ranker in the ensemble evaluator at the beginning, but found that it did not work well. Z (x) = x \u2212 x-Score (x-x) = x-Score of the ensemble is not sufficient to normalize the score of the ensemble. It is unreasonable to simply add them without normalization, so we used z-Score to normalize the ranking. Z (x) = x \u2212 x-Score (x-x) = x-Score of the ensemble is not sufficient to normalize the score of the ensemble."}, {"heading": "B. GBM Ensemble", "text": "To get a better result, we also include some significant features, such as the value of prop locations 1, the value of prop locations 2 and the price. We use 30 models and 120 trees and archive 0.53573 NDCG @ 38 on the local test set, but only 0.53053 NDCG @ 38 on the online test set."}, {"heading": "C. Deep Learning", "text": "By using models with a local NDCG score of 0.505, 0.508, 0.510, 0.511, 0.512, 0.513,0,519, 0.521, our model achieves about 0.526 in the private ranking. And logistic regression achieves the best score in the private ranking of 0.52729 NDCG @ 38. They are still weaker than other ensemble methods."}, {"heading": "D. Listwise Ensemble", "text": "Previous ensemble methods do not include the list information. Therefore, we try to ensembles all models with LambdaMART. By normalizing the z-score, our final result is achieved: 0.53249 NDCG @ 38 on the public ranking and 0.53102 on the last private ranking."}, {"heading": "VI. CONCLUSION AND FURTHER DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Conclusion", "text": "Due to the large amount of data that Expedia provides, we use both random sampling and balanced sampling methods to create a reliable validation set. In this paper, various ranking models are described, including modified logistic regression, random forest, gradient boosting machine, extreme randomized trees, factorization machines, and LambdaMART. We also try to apply a deep learning approach, which will be discussed further in the next subsection. With these individual ranking models, we present our ensemble methods to combine individual models, including Z-Score, GBM, Deep Learning, and Listwise Ensemble.The combination of models significantly improves ranking accuracy with respect to NDCG on both public and private leaderboards."}, {"heading": "B. Lessons Learned in Deep Learning", "text": "Based on the shared data, there is a serious problem of a lack of training data, and reconstruction errors may not reflect the best optimization direction. We suggest that in later practice, we check to see if pre-training finds distributed representations that are not just based on the reconstruction error. Bucket may not be an optimized solution to normalize feature input Better normalization may lead to a better generalization capacity for the deep networks."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank Prof. Russ Greiner of the University of Alberta for his advice in preparing this report and Prof. Hong Hu of the Institute of Computing Technology for his advice on data analysis."}], "references": [{"title": "Learning to rank for information retrieval. Foundations and Trends in Information Retrieval", "author": ["Liu", "Tie-Yan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Pedregosa", "Fabian", "Gal Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Frdric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Training Linear SVMs in Linear Time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "Chih-Chung", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan", "Rong-En"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 (1): 532", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Adarank: a boosting algorithm for information retrieval", "author": ["Xu", "Jun", "Hang Li"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["Burges", "Chris"], "venue": "Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Adapting boosting for information retrieval measures", "author": ["Q. Wu", "C.J. Burges", "K.M. Svore", "J. Gao"], "venue": "Information Retrieval 13.3 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["Friedman", "Jerome H"], "venue": "Annals of Statistics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Extremely randomized trees", "author": ["Geurts", "Pierre", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Learning to rank with extremely randomized trees", "author": ["Geurts", "Pierre", "Gilles Louppe"], "venue": "JMLR: Workshop and Conference Proceedings", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Factorization machines with libFM", "author": ["Rendle", "Steffen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Geoffrey E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization", "author": ["McMahan", "H. Brendan"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Horizontal and Vertical Ensemble with Deep Representation for Classification", "author": ["Xie", "Jingjing", "Bing Xu", "Zhang Chuang"], "venue": "arXiv preprint arXiv:1306.2759", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The evaluation metric for this competition is Normalized Discounted Cumulative Gain (NDCG), which is commonly used in ranking[1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "The output of the data infrastructure can be in different format like numpy binary array, SVMRank[5] text format and LIBSVM[6] text format.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "The output of the data infrastructure can be in different format like numpy binary array, SVMRank[5] text format and LIBSVM[6] text format.", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 138, "endOffset": 141}, {"referenceID": 8, "context": "We use LIBLINEAR[7] and SVMRank[5]for pairwise logistic regression; Random Forest[8] from scikit-learn[2]; Ranking algorithms like AdaRank[9], LambdaMART[10] from RankLib2.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "And we also use Gradient Boosting Machine3[12], Extremely Randomized Trees4[13] from R, deep neural net-", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "And we also use Gradient Boosting Machine3[12], Extremely Randomized Trees4[13] from R, deep neural net-", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "work implementation from PyLearn2[3] and Factorization Machine libFM5[15].", "startOffset": 33, "endOffset": 36}, {"referenceID": 13, "context": "work implementation from PyLearn2[3] and Factorization Machine libFM5[15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "In order to use full train set as pairwise in logistic regression model, we use FTRL-Proximal algorithm and liblinear[7] and SVMRank[5] build in function.", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "In order to use full train set as pairwise in logistic regression model, we use FTRL-Proximal algorithm and liblinear[7] and SVMRank[5] build in function.", "startOffset": 132, "endOffset": 135}, {"referenceID": 15, "context": "The FTRLProximal algorithm [18], can be seen as a hybrid of FOBOS and RDA algorithms, and significantly outperforms both on a large, realworld dataset.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "For each unique balanced prop country id data piece we train an independent random forest model [8] with 3200 trees.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "Some failed cases happened in predicting for some countries, so to make the example count of test and internal validation set equal we simply combine the score with a pairwise logistic regression trained by liblinear[7] model with 0.", "startOffset": 216, "endOffset": 219}, {"referenceID": 10, "context": "Gradient Boosting Machine Gradient Boosting Machine(GBM) [12] is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "Extremely Randomized Trees (ERT) is proposed by [13].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "ERT model using in learning to rank task is proposed by [14].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Factorization Machine [15] is widely used in Recommender System.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "LambdaMART model [10] [11] with CTR(clickthrough rates Eq 2) and CVR(Booking rates Eq 3) features and original features in full train set.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "LambdaMART model [10] [11] with CTR(clickthrough rates Eq 2) and CVR(Booking rates Eq 3) features and original features in full train set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "Vertical ensemble[19] helps improve a little but still can not make deep network as a strong single model.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Treat each model\u2019s output on local test set as a feature of GBM model [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "With ReLU[17] network or Maxout network, our model achieves around 0.", "startOffset": 9, "endOffset": 13}], "year": 2013, "abstractText": "The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, users aggregated purchase history and competitive online travel agency (OTA) information for each potential hotel choice. This paper describes the solution of team \u201dbinghsu & MLRush & BrickMover\u201d. We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model\u2019s output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition.", "creator": "LaTeX with hyperref package"}}}