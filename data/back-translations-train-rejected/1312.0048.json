{"id": "1312.0048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2013", "title": "Stochastic Optimization of Smooth Loss", "abstract": "In this paper, we first prove a high probability bound rather than an expectation bound for stochastic optimization with smooth loss. Furthermore, the existing analysis requires the knowledge of optimal classifier for tuning the step size in order to achieve the desired bound. However, this information is usually not accessible in advanced. We also propose a strategy to address the limitation.", "histories": [["v1", "Sat, 30 Nov 2013 01:07:25 GMT  (13kb)", "http://arxiv.org/abs/1312.0048v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rong jin"], "accepted": false, "id": "1312.0048"}, "pdf": {"name": "1312.0048.pdf", "metadata": {"source": "CRF", "title": "Stochastic Optimization of Smooth Loss", "authors": ["Rong Jin"], "emails": ["rongjin@cse.msu.edu"], "sections": [{"heading": null, "text": "It is not the first time that we find a solution. & # 8222; It is the first time that we find a solution. & # 8220; & # 8222; It is the second time that we find a solution. & # 8220; & # 8222; It is the first time that we find a solution. & # 8220; & # 8220; & # 8222; It is the second time that we find a solution. & # 8220; & # 8222; It is the first time that we find a solution. & # 8220; & # 8220; & # 8220; & # 8222; & # 8222; It is the second time that we find a solution. & # 8220; & # 8222; It is the second time that we find a solution. & # 8220; & # 8220; & # 8222; & # 8222; It is the first time that we find a solution."}], "references": [{"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In (Srebro et al., 2010), the authors were able to show that a simple stochastic optimization method, with an appropriate choice of step size \u03b7, can achieves the following generalization error bound in expectation, i.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "There are two limitations with the analysis in (Srebro et al., 2010).", "startOffset": 47, "endOffset": 68}, {"referenceID": 0, "context": "In the draft presented in this work, we improve the analysis in (Srebro et al., 2010) by addressing these two limitations.", "startOffset": 64, "endOffset": 85}], "year": 2013, "abstractText": "Let \u03c6(z) be a smooth loss function, with |l\u2032(z)| \u2264 L and |l\u2032(z)\u2212 l\u2032(z\u2032)| \u2264 \u03b3|z \u2212 z\u2032|. Let \u03a9 = { w \u2208 R : |w| \u2264 R } be the solution domain. Let (xi, yi), i = 1, . . . , n be the sequence of i.i.d samples used for training, where xi \u2208 R and yi \u2208 {\u22121,+1}. Our goal is to find a solution \u0175 with a good generalization performance. More specifically, let l(w) be the expected loss for any solution w, i.e. l(w) = E[l(ywx)]. Our goal is to minimize l(w). A straightforward approach is to optimize l(w) by stochastic optimization. Let w1 = 0 be the initial. At each iteration t, we receive a training example (xi, yi), and update the current solution wt by wt+1 = argmin\u03c0\u03a9 (wt \u2212 \u03b7\u2207lt(wt)) where \u03b7 > 0 is the stepsize and lt(w) = \u03c6(ytw xt). The final solution \u0175 will be the average of all the solutions, i.e. \u0175 = \u2211T t=1 wt/T . In (Srebro et al., 2010), the authors were able to show that a simple stochastic optimization method, with an appropriate choice of step size \u03b7, can achieves the following generalization error bound in expectation, i.e.", "creator": "LaTeX with hyperref package"}}}