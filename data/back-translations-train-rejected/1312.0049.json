{"id": "1312.0049", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2013", "title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "abstract": "One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research.", "histories": [["v1", "Sat, 30 Nov 2013 01:52:36 GMT  (441kb)", "http://arxiv.org/abs/1312.0049v1", "24 pages + 11 pages of references, 8 figures"]], "COMMENTS": "24 pages + 11 pages of references, 8 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["shehroz s khan", "michael g madden"], "accepted": false, "id": "1312.0049"}, "pdf": {"name": "1312.0049.pdf", "metadata": {"source": "CRF", "title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "authors": ["SHEHROZ S.KHAN", "MICHAEL G.MADDEN"], "emails": ["shehroz@gmail.com,", "michael.madden@nuigalway.ie"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.00 49v1 [cs.LG] 3 0N ov2 01One-class classification (OCC) algorithms are aimed at building classification models when the negative class is either missing, poorly sampled or not well defined. This unique situation impedes the learning of efficient classifiers by defining class boundaries only with the knowledge of the positive class. The OCC problem has been considered and applied among many research topics, such as outlier / novelty detection and concept learning. In this paper, we present a unified view of the overall problem of the OCC by presenting a taxonomy of the study for OCC problems, based on the availability of training data, algorithms used and the areas of application applied. We delve further into each of the categories of proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methods with an emphasis on their importance, limitations and applications, by discussing our current research issues in the OCC section and our open paper."}, {"heading": "1 Introduction to One-class Classification", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 One-class Classification Vs Multi-class Classification", "text": "Most conventional classifiers assume more or less balanced data classes and do not work well when each class is severely underrepresented or completely absent. Minter (1975) seems to be the first to use the term \"single-class classification\" four decades ago, in the context of learning Bayes classifiers, who only need labeled data from the \"class of interest.\" Much later, Moya et al. (1993) The term \"one-class classification\" comes from researchers who use other terms such as \"outlier detection\" and \"gallegos\" that are only labeled."}, {"heading": "2.1 Measuring Classification Performance of One-class Classifiers", "text": "As mentioned in Tax (2001), a confusion matrix (see Table 1) can be constructed to calculate the classification performance of single-class classifiers. To estimate the true error (as it is calculated for multi-class classifiers), the full probability density of both classes should be known. In the case of single-class classification, the probability density is known only to the positive class. This means that only the number of positive class objects that are not accepted by the single-class classifier (i.e. the false negatives, F +) can be minimized. In the absence of examples and sample distributions of outliers, it is not possible to estimate the number of outliers accepted by the single-class classifier (the false positives, F +). Furthermore, since T + + F \u2212 = 1 and T \u2212 F + = 1 are the main complication in the OCC metric, nothing is known about this class, and only the positive class and \u2212 \u2212 can be estimated."}, {"heading": "3 Related Review Work in OCC", "text": "In recent years, there has been a considerable amount of research carried out in the field of OCC. Researchers have proposed several OCC algorithms to deal with various classification problems. Mazhelis (2006) presents a review of OCC algorithms and analyses their suitability in the context of mobile recognition. In this paper, Mazhelis proposes a taxonomy of classification techniques based on: (i) the internal model used by classifiers (density, reconstruction or boundary); (ii) the type of data (numerical or symbolic); and (iii) the ability of classifiers to consider temporal relationships between characteristics (yes or no). Mazhelis \"study of OCC describes a number of algorithms and techniques; however, it covers a subspectrum of the problems in the field of OCC."}, {"heading": "4 Proposed Taxonomy", "text": "Based on research carried out in the field of OCC using different algorithms, methods and application areas, we present a consistent approach to the OCC by proposing a taxonomy for the study of OCC issues. Taxonomy is divided into three broad categories (see Figure 1): (i) Availability of training data: Only learning with positive data or learning with positive and unlabelled data and / or a certain number of outlier examples. (ii) Methodology applied: algorithms based on one-class support vector machines (OSVMs) or methods based on algorithms other than OSVMs. (iii) Application domain: OCC in the field of text / document classification or other application areas. The proposed categories are not mutually exclusive, so that there may be overlap between research carried out in each of these categories."}, {"heading": "4.1 Category 1: Availability of Training Data", "text": "The availability of training data plays a central role in any OCC algorithm. Researchers have studied OCC extensively under three broad categories: (a) Learning with positive examples only. (b) Learning with positive examples and a certain amount of poorly sampled negative examples or artificially generated outliers. (c) Learning with positive and unlabeled data. (c) Learning with positive and unlabeled data is a matter of great research interest within the text / document classification community (Liu et al., 2003; Li and Liu, 2003), which is detailed in Section 4.3.1. Taxes and Duin (1999a, b) and Scho \ufffd lkopf et al. (1999b) have developed various algorithms based on support vector machines to address the problem of OCC with positive examples; for a detailed discussion of them, refer to Section 4.2.1. The main idea behind these strategies is to set a decision limit to construct the data in order to calculate them from positive examples."}, {"heading": "4.2 Category 2: Algorithms Used", "text": "Most of the major OCC algorithms that have been developed can be divided into two broad categories, as has been done either through: \u2022 One-class Support Vector Machines (OSVMs) or \u2022 Non-OSVM methods (including various variants of neural networks, decision trees, closest neighbors, and others). At first glance, this category may seem to represent a biased view, classifying algorithms according to whether or not they are based on OSVM or non-OSVM. However, we have found that the advances, applications, significance, and differences that OSVM-based algorithms have demonstrated open them up as a separate field of research. Nevertheless, OSVM-based OCC algorithms have not been used to solve specific research problems; the details are presented in the following subsections."}, {"heading": "4.2.1 One-class Support Vector Machine (OSVM)", "text": "It is not as if it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about both and in which it is about a way in which it is about both and in which it is about both and in which it is about a way and in which it is about a way in which it is about both and in which it is about both, in which it is about a way in which it is about both and in which it is about both, in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about both and in which it is about both and in which it is about both in which it is about a way and in which it is about which it is about both in which it is about which it is about both in which it is about and in which it is about which it is about which it is about which it is about which it is about which it is about both in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which is about which it is about which it is about which is about which it is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which it is about which is about which is about which is about which is about which is about which is about which is about which is about which it is about which is about which is about which is about which is about which is about which is about which is"}, {"heading": "4.2.2 One-Class Classifiers other than OSVMs", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "4.3 Category 3: Application Domain Applied", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Text / Document Classification", "text": "It is, of course, possible to name some negative examples manually, but depending on the scope of application, this can be a labor-intensive and time-consuming task. However, it is a common practice to build text classifiers that have a set of negative examples that include a comprehensive characterization of everything that is \"not\" the target concept adopted by a conventional binary classifier. It is a common practice to build text classifiers using positive and undescribed examples."}, {"heading": "4.3.2 Other Application Domains", "text": "In this section we will highlight some of the other applications of one-class classification methods that may not necessarily apply lessons learned from positive and unmarked data, some of which are: Handwriting Detection (Tax and Duin, 2001b; Tax, 2001; Scholelkopf et al., 2000; Hempstalk et al., 2008); Information Retrieval (Tax and Duin, 2001b; Chen et al., 2001; Missing Data / Data Correction, 2007); Face / Object Recognition Applications (Wang et al., 2004; Detection Database Retrieval (Tax and Duin, 2001b; Chen et al., 2001; Gondra et al., 2004; Shio et al. Applications (Wang et al., 2004; Zeng et al., 2006; Bicego et al., al., 2005), Remote Sensing (Li et al., 2011)."}, {"heading": "5 Conclusions and Open Research Questions", "text": "It's not that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it. It's that they don't want it."}], "references": [{"title": "One-class support vector machines for proteinprotein interactions prediction", "author": ["H.T. Alashwal", "S. Deris", "R.M. Othman"], "venue": "Intternational Journal of Biomedical Sciences,", "citeRegEx": "Alashwal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alashwal et al\\.", "year": 2006}, {"title": "Implementing multi-class classifiers by one-class classification methods", "author": ["T. Ban", "S. Abe"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Ban and Abe.,? \\Q2006\\E", "shortCiteRegEx": "Ban and Abe.", "year": 2006}, {"title": "Anomaly, novelty, one-class classification: A comprehensive introduction", "author": ["A.M. Bartkowiak"], "venue": "International Journal of Computer Information Systems and Industrial Management Applications,", "citeRegEx": "Bartkowiak.,? \\Q2011\\E", "shortCiteRegEx": "Bartkowiak.", "year": 2011}, {"title": "An empirical comparison of voting classification algorithms: Bagging, boosting, and variants", "author": ["E. Bauer", "R. Kohavi"], "venue": "Machine Learning,", "citeRegEx": "Bauer and Kohavi.,? \\Q1999\\E", "shortCiteRegEx": "Bauer and Kohavi.", "year": 1999}, {"title": "Fusion of biometric systems using one-class classification", "author": ["C. Bergamini", "L.S. Oliveira", "A.L. Koerich", "R. Sabourin"], "venue": "In IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Bergamini et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bergamini et al\\.", "year": 2008}, {"title": "Combining different biometric traits with one-class classification", "author": ["C. Bergamini", "A.L. Koerich", "R. Sabourin"], "venue": "Signal Processing,", "citeRegEx": "Bergamini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bergamini et al\\.", "year": 2009}, {"title": "Face authentication using one-class support vector machines", "author": ["M. Bicego", "E. Grosso", "M. Tistarelli"], "venue": "In Advances in Biometric Person Authentication: Int. Workshop on Biometric Recognition Systems, in conjunction with Int. Conf. On Computer Vision,", "citeRegEx": "Bicego et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bicego et al\\.", "year": 2005}, {"title": "Novelty detection and neural network validation", "author": ["C. Bishop"], "venue": "In IEEE Proceedings on Vision, Image and Signal Processing,", "citeRegEx": "Bishop.,? \\Q1994\\E", "shortCiteRegEx": "Bishop.", "year": 1994}, {"title": "Semi-supervised novelty detection", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blanchard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Proc. of 11th Annual conference on Computation Learning Theory,", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "A fuzzy one class classifier for multi layer model", "author": ["G.L. Bosco", "L. Pinello"], "venue": "Fuzzy Logic and Application,", "citeRegEx": "Bosco and Pinello.,? \\Q2009\\E", "shortCiteRegEx": "Bosco and Pinello.", "year": 2009}, {"title": "An evaluation of one-class classification techniques for speaker verification", "author": ["A. Brew", "M. Grimaldi", "P. Cunningham"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Brew et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brew et al\\.", "year": 2007}, {"title": "A novel method for one-class classification based on the nearest neighbor data description and structural risk minimization", "author": ["G.G. Cabral", "A.L.I. Oliveira", "C.B.G. Cahu"], "venue": "In Proc. of International Joint Conference on Neural Networks,", "citeRegEx": "Cabral et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cabral et al\\.", "year": 2007}, {"title": "Combining nearest neighbor data description and structural risk minimization for one-class classification", "author": ["G.G. Cabral", "A.L.I. Oliveira", "C.B.G. Cahu"], "venue": "Neural Computing and Applications,", "citeRegEx": "Cabral et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cabral et al\\.", "year": 2009}, {"title": "Positive unlabelled learning with applications in computational biology", "author": ["B. Calvo"], "venue": "Phd thesis, University of the Basque Country,", "citeRegEx": "Calvo.,? \\Q2008\\E", "shortCiteRegEx": "Calvo.", "year": 2008}, {"title": "Learning bayesian classifiers from positive and unlabeled examples", "author": ["B. Calvo", "P. Larra\u00f1aga", "J.A. Lozano"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Calvo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Calvo et al\\.", "year": 2007}, {"title": "A linear programming approach to novelty detection", "author": ["C. Campbell", "K.P. Bennett"], "venue": "In Advances in Neural Information Processing,", "citeRegEx": "Campbell and Bennett.,? \\Q2001\\E", "shortCiteRegEx": "Campbell and Bennett.", "year": 2001}, {"title": "Learning gene regulatory networks from only positive and unlabeled data", "author": ["L. Cerulo", "C. Elkan", "M. Ceccarelli"], "venue": "BMC Bioinformatics,", "citeRegEx": "Cerulo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cerulo et al\\.", "year": 2010}, {"title": "Outlier detection - a survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "A new method for fMRI data processing: Neighborhood independent component correlation algorithm and its preliminary application", "author": ["H.F. Chen", "D.Z. Yao", "S. Becker", "Y. Zhou", "M. Zeng", "L. Chen"], "venue": "Science in China Series F,", "citeRegEx": "Chen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2002}, {"title": "Survey of Preference Elicitation Methods", "author": ["L. Chen", "P. Pu"], "venue": "Technical report,", "citeRegEx": "Chen and Pu.,? \\Q2004\\E", "shortCiteRegEx": "Chen and Pu.", "year": 2004}, {"title": "One-class SVM for learning in image retrieval", "author": ["Y. Chen", "X. Zhou", "T.S. Huang"], "venue": "In Proc IEEE International Conference on Image Processing,", "citeRegEx": "Chen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2001}, {"title": "Pruned random subspace method for one-class classifiers", "author": ["V. Cheplygina", "D.M.J. Tax"], "venue": "In 10th International Workshop, MCS 2011,", "citeRegEx": "Cheplygina and Tax.,? \\Q2011\\E", "shortCiteRegEx": "Cheplygina and Tax.", "year": 2011}, {"title": "Video summarization using fuzzy one-class support vector machine", "author": ["Y.S. Choi", "K.J. Kim"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Choi and Kim.,? \\Q2004\\E", "shortCiteRegEx": "Choi and Kim.", "year": 2004}, {"title": "An application of one-class support vector machines to nosocomial infection detection", "author": ["G. Cohen", "M. Hilario", "H. Sax", "S. Hugonnet", "C. Pellegrini", "A. Geissbuhler"], "venue": "In In Proc. of Medical Informatics,", "citeRegEx": "Cohen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2004}, {"title": "Learning bayesian network classifiers for facial expression recognition both labeled and unlabeled data", "author": ["I. Cohen", "N. Sebe", "F.G. Gozman", "M.C. Cirelo", "T.S. Huang"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings,", "citeRegEx": "Cohen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2003}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y.L. Cun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "Cun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1989}, {"title": "Characteristic Concept Representations", "author": ["P. Datta"], "venue": "PhD thesis, University of California Irvine,", "citeRegEx": "Datta.,? \\Q1997\\E", "shortCiteRegEx": "Datta.", "year": 1997}, {"title": "Positive and unlabeled examples help learning", "author": ["F. De Comit\u00e9", "F. Denis", "R. Gilleron", "F. Letouzey"], "venue": "In Proc. of the 10th International Conference on Algorithmic Learning Theory,", "citeRegEx": "Comit\u00e9 et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Comit\u00e9 et al\\.", "year": 1999}, {"title": "One-class methods for separating plant/pathogen sequences. In VI Congreso Espaol sobre Metaheursticas, Algoritmos Evolutivos y Bioinspirados", "author": ["A. de Haro-Garca", "N. Garca-Pedrajas", "J.A. Romero del Castillo", "M.D. Garca-Pedrajas"], "venue": null, "citeRegEx": "Haro.Garca et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Haro.Garca et al\\.", "year": 2009}, {"title": "An experimental comparison of one-class classification methods", "author": ["D. de Ridder", "D.M.J. Tax", "R.P.W. Duin"], "venue": "In Proc. of the 4th Annual Conference of the Advanced School for Computing and Imaging,", "citeRegEx": "Ridder et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Ridder et al\\.", "year": 1998}, {"title": "PAC learning from positive statistical queries", "author": ["F. Denis"], "venue": "In Proc. of the 9th International Conference on Algorithmic Learning Theory,", "citeRegEx": "Denis.,? \\Q1998\\E", "shortCiteRegEx": "Denis.", "year": 1998}, {"title": "Text classification from positive and unlabeled examples", "author": ["F. Denis", "R. Gilleron", "M. Tommasi"], "venue": "In 9th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems,", "citeRegEx": "Denis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2002}, {"title": "Text classification and co training from positive and unlabeled examples. In Proc. of the ICML Workshop: the Continuum from Labeled Data to Unlabeled Data in Machine Learning and Data Mining, pages", "author": ["F. Denis", "A. Laurent", "R. Gilleron", "M. Tommasi"], "venue": null, "citeRegEx": "Denis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2003}, {"title": "A random forest based approach for one class classification in medical imaging", "author": ["C. D\u00e9sir", "S. Bernard", "C. Petitjean", "L. Heutte"], "venue": "In Machine Learning in Medical Imaging,", "citeRegEx": "D\u00e9sir et al\\.,? \\Q2012\\E", "shortCiteRegEx": "D\u00e9sir et al\\.", "year": 2012}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees, bagging, boosting and randomization", "author": ["T.G. Dietterich"], "venue": "Machine Learning,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Domingos and Hulten.,? \\Q2000\\E", "shortCiteRegEx": "Domingos and Hulten.", "year": 2000}, {"title": "Optimal single-class classication strategies - google scholar", "author": ["R. El-Yaniv", "M. Nisenson"], "venue": "In Proc. of the 2006 NIPS Conference,", "citeRegEx": "El.Yaniv and Nisenson.,? \\Q2007\\E", "shortCiteRegEx": "El.Yaniv and Nisenson.", "year": 2007}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": "In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Elkan and Noto.,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto.", "year": 2008}, {"title": "One class classification using implicit polynomial surface fitting", "author": ["A. Ercil", "B. Buke"], "venue": "In Proc. of the 16th International Conference on Pattern Recognition,", "citeRegEx": "Ercil and Buke.,? \\Q2002\\E", "shortCiteRegEx": "Ercil and Buke.", "year": 2002}, {"title": "Fuzzy ROC curves for the 1 class SVM: application to intrusion detection", "author": ["P.F. Evangelista", "P. Bonnisone", "M.J. Embrechts", "B.K. Szymanski"], "venue": "In Application to Intrusion Detection,", "citeRegEx": "Evangelista et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evangelista et al\\.", "year": 2005}, {"title": "Combining one-class classifiers for robust novelty detection in gene expression data", "author": ["Ferreira de Carvalho", "A.C.P. L"], "venue": "In Brazilian Symposium on Bioinformatics,", "citeRegEx": "Carvalho and L.,? \\Q2005\\E", "shortCiteRegEx": "Carvalho and L.", "year": 2005}, {"title": "One-class novelty detection for seizure analysis from intracranial EEG", "author": ["B. Gardner", "A.M. Krieger", "G. Vachtsevanos", "B. Litt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gardner et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2006}, {"title": "Combining one class fuzzy KNN\u2019s", "author": ["V.D. Ges\u00f9", "G.L. Bosco"], "venue": "Applications of Fuzzy Sets Theory, LNCS 4578:152\u2013160,", "citeRegEx": "Ges\u00f9 and Bosco.,? \\Q2007\\E", "shortCiteRegEx": "Ges\u00f9 and Bosco.", "year": 2007}, {"title": "A one class classifier for signal identification: A biological case study", "author": ["V.D. Ges\u00fa", "G.L. Bosco", "L. Pinello"], "venue": "In Proc. of the 12th international conference on Knowledge-Based Intelligent Information and Engineering Systems,", "citeRegEx": "Ges\u00fa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ges\u00fa et al\\.", "year": 2008}, {"title": "A multi-layer method to study genome-scale positions of nucleosomes", "author": ["V.D. Ges\u00f9", "G.L. Bosco", "L. Pinello", "G.C. Yuan", "D.F.V. Corona"], "venue": null, "citeRegEx": "Ges\u00f9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ges\u00f9 et al\\.", "year": 2009}, {"title": "Network intrusion detection by combining one-class classifiers", "author": ["G. Giacinto", "R. Perdisci", "F. Roli"], "venue": "Image Analysis and Processing \u2013 ICIAP 2005,", "citeRegEx": "Giacinto et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Giacinto et al\\.", "year": 2005}, {"title": "Analysis of the effect of unexpected outliers in the classification of spectroscopy data", "author": ["F.G. Glavin", "M.G. Madden"], "venue": "In Artificial Intelligence and Cognitive Science", "citeRegEx": "Glavin and Madden.,? \\Q2009\\E", "shortCiteRegEx": "Glavin and Madden.", "year": 2009}, {"title": "Improving image retrieval performance by inter-query learning with one-class support vector machines", "author": ["I. Gondra", "D.R. Heisterkamp", "J. Peng"], "venue": "Neural Computation and Applications,", "citeRegEx": "Gondra et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gondra et al\\.", "year": 2004}, {"title": "Identification of egg freshness using near infrared spectroscopy and one class support vector machine algorithm", "author": ["L. Hao", "Z.J. Wen", "C.Q. Sheng", "C.J. Rong", "Z. Ping"], "venue": "Spectroscopy and Spectral Analysis,", "citeRegEx": "Hao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hao et al\\.", "year": 2010}, {"title": "Fuzzy one-class support vector machines", "author": ["P.Y. Hao"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "Hao.,? \\Q2008\\E", "shortCiteRegEx": "Hao.", "year": 2008}, {"title": "fMRI analysis via one-class machine learning techniques", "author": ["D.R. Hardoon", "L.M. Manevitz"], "venue": "In Proc. of 19th International Joint Conference on Aritifical Intelligence,", "citeRegEx": "Hardoon and Manevitz.,? \\Q2005\\E", "shortCiteRegEx": "Hardoon and Manevitz.", "year": 2005}, {"title": "One-class machine learning approach for fMRI analysis", "author": ["D.R. Hardoon", "L.M. Manevitz"], "venue": "Postgraduate Research Conference in Electronics,", "citeRegEx": "Hardoon and Manevitz.,? \\Q2005\\E", "shortCiteRegEx": "Hardoon and Manevitz.", "year": 2005}, {"title": "Naive bayes classifier for positive unlabeled learning with uncertainty", "author": ["J. He", "Y. Zhang", "X. Li", "Y. Wang"], "venue": "In Proceedings of the Tenth SIAM International Conference on Data Mining,", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "One-class classification by combining density and class probability estimation", "author": ["K. Hempstalk", "E. Frank", "I.H Witten"], "venue": "In Proceedings of the 12th European Conference on Principles and Practice of Knowledge Discovery in Databases and 19th European Conference on Machine Learning,", "citeRegEx": "Hempstalk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hempstalk et al\\.", "year": 2008}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ho.,? \\Q1998\\E", "shortCiteRegEx": "Ho.", "year": 1998}, {"title": "Kernel methods for machine learning with applications to the analysis of raman spectra", "author": ["T. Howley"], "venue": "Phd thesis,", "citeRegEx": "Howley.,? \\Q2007\\E", "shortCiteRegEx": "Howley.", "year": 2007}, {"title": "An evolutionary approach to automatic kernel construction", "author": ["T. Howley", "M.G. Madden"], "venue": "In Proc. of ICANN 2006,", "citeRegEx": "Howley and Madden.,? \\Q2006\\E", "shortCiteRegEx": "Howley and Madden.", "year": 2006}, {"title": "Concept-Learning in the absence of counterexamples: an autoassociation-based approach to classification", "author": ["N. Japkowicz"], "venue": "PhD thesis,", "citeRegEx": "Japkowicz.,? \\Q1999\\E", "shortCiteRegEx": "Japkowicz.", "year": 1999}, {"title": "Learning to Recognise. A study on one-class classification and active learning", "author": ["P. Juszczak"], "venue": "PhD thesis, Delft University of Technology,", "citeRegEx": "Juszczak.,? \\Q2006\\E", "shortCiteRegEx": "Juszczak.", "year": 2006}, {"title": "Combining one-class classifiers to classify missing data", "author": ["P. Juszczak", "R.P.W. Duin"], "venue": "In Proc. of the 5th International Workshop MCS,", "citeRegEx": "Juszczak and Duin.,? \\Q2004\\E", "shortCiteRegEx": "Juszczak and Duin.", "year": 2004}, {"title": "Minimum spanning tree based one-class", "author": ["P. Juszczak", "D.M.J. Tax", "E. P\u0229kalska", "R.P.W. Duin"], "venue": "classifier. Neurocomputing,", "citeRegEx": "Juszczak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Juszczak et al\\.", "year": 2009}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "In Proceedings IEEE International Conference on Neural Networks,", "citeRegEx": "Kennedy and Eberhart.,? \\Q1995\\E", "shortCiteRegEx": "Kennedy and Eberhart.", "year": 1995}, {"title": "Credit scoring: Solving the low default portfolio problem using one-class classification", "author": ["K. Kennedy", "B. Mac Namee", "S.J. Delany"], "venue": "Proceedings of the 20th Irish Conference on Artificial Intelligence and Cognitive Science,", "citeRegEx": "Kennedy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kennedy et al\\.", "year": 2009}, {"title": "Kernels for One-Class Nearest Neighbour Classification and Comparison of Chemical Spectral Data", "author": ["S.S. Khan"], "venue": null, "citeRegEx": "Khan.,? \\Q2010\\E", "shortCiteRegEx": "Khan.", "year": 2010}, {"title": "A survey of recent trends in one class classification", "author": ["S.S. Khan", "M.G. Madden"], "venue": "In Lecture Notes in Artificial Intelligence,", "citeRegEx": "Khan and Madden.,? \\Q2009\\E", "shortCiteRegEx": "Khan and Madden.", "year": 2009}, {"title": "Bayesian multiple imputation approaches for oneclass classification - springer", "author": ["S.S. Khan", "J. Hoey", "Daniel Lizotte"], "venue": "In Proc. Advances in Artificial Intelligence,", "citeRegEx": "Khan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2012}, {"title": "Towards the detection of unusual temporal events during activities using hmms", "author": ["S.S. Khan", "Michelle E. Karg", "Jesse Hoey", "Dana Kulic"], "venue": "In Proceedings of the 2012 ACM Conference on Ubiquitous Computing,", "citeRegEx": "Khan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2012}, {"title": "One class classifiers for process monitoring illustrated by the application to online HPLC of a continuous process", "author": ["S. Kittiwachana", "D.L.S. Ferreira", "G.R. Lloyd", "L.A. Fido", "D.R. Thompson", "R.E.A. Escott", "R.G. Brereton"], "venue": "Journal of Chemometrics,", "citeRegEx": "Kittiwachana et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kittiwachana et al\\.", "year": 2010}, {"title": "Authorship verification as a one-class classification problem", "author": ["M. Koppel", "J. Schler"], "venue": "In Proc. of the 21st international conference on Machine learning,", "citeRegEx": "Koppel and Schler.,? \\Q2004\\E", "shortCiteRegEx": "Koppel and Schler.", "year": 2004}, {"title": "One class SVM for yeast regulation prediction", "author": ["A. Kowalczyk", "B. Raskutti"], "venue": "In ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Kowalczyk and Raskutti.,? \\Q2002\\E", "shortCiteRegEx": "Kowalczyk and Raskutti.", "year": 2002}, {"title": "Using one-class SVMs for relevant sentence extraction", "author": ["C. Kruengkrai", "C. Jaruskulchai"], "venue": "In International Symposium on Communications and Information Technologies,", "citeRegEx": "Kruengkrai and Jaruskulchai.,? \\Q2003\\E", "shortCiteRegEx": "Kruengkrai and Jaruskulchai.", "year": 2003}, {"title": "Rodrguez. Classifier ensembles with a random linear oracle", "author": ["J.J.L.I. Kuncheva"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Kuncheva,? \\Q2007\\E", "shortCiteRegEx": "Kuncheva", "year": 2007}, {"title": "On combining one-class classifiers for image database retrieval", "author": ["C. Lai", "D.M.J. Tax", "R.P.W. Duin", "E. P\u0229kalska", "P. Paclk"], "venue": "In Proc. of the Third International Workshop on Multiple Classifier Systems,", "citeRegEx": "Lai et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2002}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["W. Lee", "B. Liu"], "venue": "In Proc. of the 20th International Conference on Machine Learning,", "citeRegEx": "Lee and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Lee and Liu.", "year": 2003}, {"title": "Learning from positive and unlabeled examples", "author": ["F. Letouzey", "F. Denis", "R. Gilleron"], "venue": "In Proc. of 11th International Conference on Algorithmic Learning Theory, Sydney, Australia,", "citeRegEx": "Letouzey et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Letouzey et al\\.", "year": 2000}, {"title": "Bagging one-class decision trees", "author": ["C. Li", "Y. Zhang"], "venue": "In 5th International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "Li and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Li and Zhang.", "year": 2008}, {"title": "OcVFDT: one-class very fast decision tree for one-class classification of data streams", "author": ["C. Li", "Y. Zhang", "X. Li"], "venue": "In Proceedings of the Third International Workshop on Knowledge Discovery from Sensor Data, SensorKDD", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Improving one-class SVM for anomaly detection", "author": ["K. Li", "H. Huang", "S. Tian", "W. Xu"], "venue": "In Proc. of the Second International conference on Machine Learning and Cybernetics,", "citeRegEx": "Li et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Li et al\\.", "year": 2003}, {"title": "A positive and unlabeled learning algorithm for one-class classification of remote-sensing data", "author": ["W. Li", "Q. Guo", "C. Elkan"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Learning to classify texts using positive and unlabeled data", "author": ["X. Li", "B. Liu"], "venue": "In Proc. of 18th International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Li and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Li and Liu.", "year": 2003}, {"title": "Cost-sensitive learning", "author": ["C.X. Ling", "V.S. Sheng"], "venue": "Encyclopedia of Machine Learning,", "citeRegEx": "Ling and Sheng.,? \\Q2010\\E", "shortCiteRegEx": "Ling and Sheng.", "year": 2010}, {"title": "Partially supervised classification of text documents", "author": ["B. Liu", "W.S. Lee", "P.S. Yu", "X. Li"], "venue": "In Proc. of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2002}, {"title": "Building text classifiers using positive and unlabeled examples", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "P.S. Yu"], "venue": "In Proceedings of the 3rd IEEE International Conference on Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2003}, {"title": "One-class based uncertain data stream learning", "author": ["B. Liu", "Y. Xiao", "L. Cao", "P.. S. Yu"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Linear and Nonlinear Programming", "author": ["D.G. Luenberger"], "venue": "Addison-Wesley, Amsterdam, 2nd edition edition,", "citeRegEx": "Luenberger.,? \\Q1984\\E", "shortCiteRegEx": "Luenberger.", "year": 1984}, {"title": "Research on cost-sensitive learning in one-class anomaly detection algorithms", "author": ["J. Luo", "L. Ding", "Z. Pan", "G. Ni", "G. Hu"], "venue": "In Autonomic and Trusted Computing,", "citeRegEx": "Luo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2007}, {"title": "Steganalysis using color wavelet statistics and one class support vector machines", "author": ["S. Lyu", "H. Farid"], "venue": "In Proc. of SPIE,", "citeRegEx": "Lyu and Farid.,? \\Q2004\\E", "shortCiteRegEx": "Lyu and Farid.", "year": 2004}, {"title": "A machine learning application for classification of chemical spectra", "author": ["M.G. Madden", "T. Howley"], "venue": "In Proc. of 28th SGAI International Conference,", "citeRegEx": "Madden and Howley.,? \\Q2008\\E", "shortCiteRegEx": "Madden and Howley.", "year": 2008}, {"title": "Document classification on neural networks using only positive examples", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "In Proc. of 23rd annual international ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Manevitz and Yousef.,? \\Q2000\\E", "shortCiteRegEx": "Manevitz and Yousef.", "year": 2000}, {"title": "Learning from positive data for document classification using neural networks", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "In Proc. of 2nd Bar-Ilan Workshop on Knowledge Discovery and Learning,", "citeRegEx": "Manevitz and Yousef.,? \\Q2000\\E", "shortCiteRegEx": "Manevitz and Yousef.", "year": 2000}, {"title": "One-class SVMs for document classification", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Manevitz and Yousef.,? \\Q2001\\E", "shortCiteRegEx": "Manevitz and Yousef.", "year": 2001}, {"title": "Novelty detection: A review - part 1: Statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing,", "citeRegEx": "Markou and Singh.,? \\Q2003\\E", "shortCiteRegEx": "Markou and Singh.", "year": 2003}, {"title": "Novelty detection: A review - part 2: Neural networks based approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing,", "citeRegEx": "Markou and Singh.,? \\Q2003\\E", "shortCiteRegEx": "Markou and Singh.", "year": 2003}, {"title": "One-class classifiers: A review and analysis of suitability in the context of mobilemasquerader detection", "author": ["O. Mazhelis"], "venue": "South African Computer Journal (SACJ), ARIMA & SACJ Joint Special Issue on Advances in End-User Data-mining Techniques,", "citeRegEx": "Mazhelis.,? \\Q2006\\E", "shortCiteRegEx": "Mazhelis.", "year": 2006}, {"title": "Single-class classification", "author": ["T.C. Minter"], "venue": "In Symposium on Machine Processing of Remotely Sensed Data, pages 2A12\u20132A15, Indiana,", "citeRegEx": "Minter.,? \\Q1975\\E", "shortCiteRegEx": "Minter.", "year": 1975}, {"title": "One-class classifier networks for target recognition applications", "author": ["M.R. Moya", "M.W. Koch", "L.D. Hostetler"], "venue": "In International Neural Network Society,", "citeRegEx": "Moya et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Moya et al\\.", "year": 1993}, {"title": "Learning from the positive data", "author": ["S. Muggleton"], "venue": "Machine Learning,", "citeRegEx": "Muggleton.,? \\Q2001\\E", "shortCiteRegEx": "Muggleton.", "year": 2001}, {"title": "Multi-class and single-class classification approaches to vehicle model recognition from images", "author": ["D.T. Munroe", "M.G. Madden"], "venue": "In In Proc. of Irish Conference on Artificial Intelligence and Cognitive Science, Portstewart,", "citeRegEx": "Munroe and Madden.,? \\Q2005\\E", "shortCiteRegEx": "Munroe and Madden.", "year": 2005}, {"title": "Classification of cancerous cells based on the oneclass problem approach", "author": ["N. Murshed", "F. Bortolozzi", "R. Sabourin"], "venue": "In SPIE Conference on Applications and Science of Artificial Neural Networks II,", "citeRegEx": "Murshed et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Murshed et al\\.", "year": 1996}, {"title": "Experimental comparison of one-class classifiers for online signature verification", "author": ["L. Nanni"], "venue": "Neurocomputing, 69:869\u2013873,", "citeRegEx": "Nanni.,? \\Q2006\\E", "shortCiteRegEx": "Nanni.", "year": 2006}, {"title": "An application of support vector machines to anomaly detection", "author": ["B.V. Nguyen"], "venue": "Technical Report CS681,", "citeRegEx": "Nguyen.,? \\Q2002\\E", "shortCiteRegEx": "Nguyen.", "year": 2002}, {"title": "Learning pattern classification tasks with imbalanced data sets", "author": ["H. Nguyen", "Giang", "Abdesselam Bouzerdoum", "L. Phung", "Son"], "venue": "Pattern Recognition. InTech,", "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Positive unlabeled learning for time series classification", "author": ["M.N. Nguyen", "X.L. Li", "S.K. Ng"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "Nguyen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine Learning,", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "One class support vector machine based non-relevance feedback document retrieval", "author": ["T. Onoda", "H. Murata", "S. Yamada"], "venue": "In International Joint Conference on Neural Networks", "citeRegEx": "Onoda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Onoda et al\\.", "year": 2005}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "In Proc. of 8th IEEE International Conference on Data Mining,", "citeRegEx": "Pan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2008}, {"title": "Nearest neighbor algorithm for positive and unlabeled learning with uncertainty", "author": ["S. Pan", "Y. Zhang", "X. Li", "Y. Wang"], "venue": "Journal of Computer Science and Frontiers,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "One-class LP classifiers for dissimilarity representations", "author": ["E. P\u0229kalska", "D.M.J. Tax", "R.P.W. Duin"], "venue": "Advances in Neural Info. Processing Systems,", "citeRegEx": "P\u0229kalska et al\\.,? \\Q2003\\E", "shortCiteRegEx": "P\u0229kalska et al\\.", "year": 2003}, {"title": "Combining dissimilarity representations in oneclass classifier problems", "author": ["E. P\u0229kalska", "M. Skurichina", "R.P.W. Duin"], "venue": "In Proceedings Fifth International Workshop MCS 2004,", "citeRegEx": "P\u0229kalska et al\\.,? \\Q2004\\E", "shortCiteRegEx": "P\u0229kalska et al\\.", "year": 2004}, {"title": "Text classification from positive and unlabeled documents based on GA", "author": ["T. Peng", "W. Zuo", "F. He"], "venue": "In Proc. of VECPAR\u201906,", "citeRegEx": "Peng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2006}, {"title": "Using an ensemble of one-class SVM classifiers to harden payload-based anomaly detection systems", "author": ["R. Perdisci", "G. Gu", "W. Lee"], "venue": "In Proc. of the 16th International Conference on Data Mining,", "citeRegEx": "Perdisci et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Perdisci et al\\.", "year": 2006}, {"title": "Application of SVMs for colour classification and collision detection with AIBO robots", "author": ["J.M. Quinlan", "S.K. Chalup", "R.H. Middleton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Quinlan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Quinlan et al\\.", "year": 2003}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "Improved one-class SVM classifier for sounds classification", "author": ["A. Rabaoui", "M. Davy", "S. Rossignol", "Z. Lachiri", "N. Ellouze"], "venue": "AVSS", "citeRegEx": "Rabaoui et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rabaoui et al\\.", "year": 2007}, {"title": "Using one-class SVMs and wavelets for audio surveillance systems", "author": ["A. Rabaoui", "M. Davy", "S. Rossignol", "N. Ellouze"], "venue": "IEEE Trans. on Information Forensic and Security,", "citeRegEx": "Rabaoui et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rabaoui et al\\.", "year": 2008}, {"title": "Constructing boosting algorithms from SVMS: an approach to one-class classification", "author": ["G. R\u00e4tsch", "S. Mika", "B. Sch\u00f6lkopf", "K.R. M\u00fcller"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2002\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2002}, {"title": "Outliers in statistical pattern recognition and an application to automatic chromosome classification", "author": ["G. Ritter", "M. Gallegos"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Ritter and Gallegos.,? \\Q1997\\E", "shortCiteRegEx": "Ritter and Gallegos.", "year": 1997}, {"title": "Relevant feedback in information retrieval", "author": ["J. Rocchio"], "venue": "Englewood Cliffs,", "citeRegEx": "Rocchio.,? \\Q1971\\E", "shortCiteRegEx": "Rocchio.", "year": 1971}, {"title": "Steganography anomaly detection using simple one-class classification", "author": ["B.M. Rodriguez", "G.L. Peterson", "S.S. Agaian"], "venue": "In Proceddings of the SPIE,", "citeRegEx": "Rodriguez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2007}, {"title": "Multiple Imputation for Non response in Surveys", "author": ["D.B. Rubin"], "venue": null, "citeRegEx": "Rubin.,? \\Q1987\\E", "shortCiteRegEx": "Rubin.", "year": 1987}, {"title": "Parallel distributed processing : Exploration in the microstructure of cognition, volume 1 & 2", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": null, "citeRegEx": "Rumelhart and McClelland.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart and McClelland.", "year": 1986}, {"title": "One-class support vector machines for the classification of bioacoustic time series", "author": ["A. Sachs", "C. Thiel", "F. Schwenker"], "venue": "In INFOS\u201906,", "citeRegEx": "Sachs et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sachs et al\\.", "year": 2006}, {"title": "Fault detection in reactive ion etching systems using one-class, support vector machines", "author": ["T. Sarmiento", "S.J. Hong", "G.S. May"], "venue": "In Advanced Semiconductor Manufacturing Conference and Workshop,", "citeRegEx": "Sarmiento et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sarmiento et al\\.", "year": 2005}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Feund", "P.L. Bartlett", "W.S. Lee"], "venue": "The Annals of Statistics,", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Learning to filter junk e-mail from positive and unlabeled examples", "author": ["K.M. Schneider"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Schneider.,? \\Q2004\\E", "shortCiteRegEx": "Schneider.", "year": 2004}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "SV estimation of a distribution\u2019s support", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J.S. Taylor"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Estimating the support of a high dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Technical Report MSR-TR-99-87, Microsoft Research,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J.S. Taylor", "J.C. Platt"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Weighted bagging for graph based one-class classifiers", "author": ["S. Seg\u00f9\u0131", "L. Igual", "J. Vitri\u00e0"], "venue": "Multiple Classifier Systems,", "citeRegEx": "Seg\u00f9\u0131 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Seg\u00f9\u0131 et al\\.", "year": 2010}, {"title": "An application of one-class support vector machines in content based image retrieval", "author": ["K. Seo"], "venue": "Expert Systems with Applications,", "citeRegEx": "Seo.,? \\Q2007\\E", "shortCiteRegEx": "Seo.", "year": 2007}, {"title": "How to improve the reliability of artificial neural networks", "author": ["A.J.C. Sharkey", "N.E. Sharkey"], "venue": "Technical Report CS-95-11,", "citeRegEx": "Sharkey and Sharkey.,? \\Q1995\\E", "shortCiteRegEx": "Sharkey and Sharkey.", "year": 1995}, {"title": "Ensembles of one class support vector machines", "author": ["A.D. Shieh", "D.F. Kamm"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Shieh and Kamm.,? \\Q2009\\E", "shortCiteRegEx": "Shieh and Kamm.", "year": 2009}, {"title": "One-class support vector machines: an application in machine fault detection and classification", "author": ["H.J. Shin", "D.W. Eom", "S.S. Kim"], "venue": "Computers and Industrial Engineering,", "citeRegEx": "Shin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2005}, {"title": "Hypergraph-based anomaly detection of high-dimensional co-occurrences", "author": ["J. Silva", "R. Willett"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Silva and Willett.,? \\Q2009\\E", "shortCiteRegEx": "Silva and Willett.", "year": 2009}, {"title": "Single-class classifier learning using neural networks: An application to the prediction of mineral deposits", "author": ["A. Skabar"], "venue": "In Proc. of the Second International Conference on Machine Learning and Cybernetics,", "citeRegEx": "Skabar.,? \\Q2003\\E", "shortCiteRegEx": "Skabar.", "year": 2003}, {"title": "SVMs for novel class detection in bioinformatics", "author": ["E.J. Spinosa", "A.C.P.L. Ferreira de Carvalho"], "venue": "In Brazilian Workshop on Bioinformatics,", "citeRegEx": "Spinosa and Carvalho.,? \\Q2004\\E", "shortCiteRegEx": "Spinosa and Carvalho.", "year": 2004}, {"title": "Weighted low-rank approximations", "author": ["N. Srebro", "T. Jaakkola"], "venue": "In Proc. of the 20th International Conference on Machine Learning,", "citeRegEx": "Srebro and Jaakkola.,? \\Q2003\\E", "shortCiteRegEx": "Srebro and Jaakkola.", "year": 2003}, {"title": "A novel method for chinese spam detection based on one-class support vector machine", "author": ["D. Sun", "Q.A. Tran", "H. Duan", "G. Zhang"], "venue": "Journal of Information and Computational Science,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "One-class classifier for HFGWR ship detection using similarity-dissimilarity representation", "author": ["Y. Tang", "Z. Yang"], "venue": "In Proc. of the 18th international conference on Innovations in Applied Artificial Intelligence,", "citeRegEx": "Tang and Yang.,? \\Q2005\\E", "shortCiteRegEx": "Tang and Yang.", "year": 2005}, {"title": "Averaging regularized estimators", "author": ["M. Tanigushi", "V. Tresp"], "venue": "Neural Computation,", "citeRegEx": "Tanigushi and Tresp.,? \\Q1997\\E", "shortCiteRegEx": "Tanigushi and Tresp.", "year": 1997}, {"title": "Support vector data description applied to machine vibration analysis", "author": ["D.M . J. Tax", "A. Ypma", "R.P.W. Duin"], "venue": "In Proc. of the 5th Annual Conference of the ASCI,", "citeRegEx": "Tax et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tax et al\\.", "year": 1999}, {"title": "One-class Classification", "author": ["D.M.J. Tax"], "venue": "PhD thesis, Delft University of Technology,", "citeRegEx": "Tax.,? \\Q2001\\E", "shortCiteRegEx": "Tax.", "year": 2001}, {"title": "Data domain description using support vectors", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "In Proc. of European Sysmposium on Artificial Neural Networks,", "citeRegEx": "Tax and Duin.,? \\Q1999\\E", "shortCiteRegEx": "Tax and Duin.", "year": 1999}, {"title": "Support vector domain description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Tax and Duin.,? \\Q1999\\E", "shortCiteRegEx": "Tax and Duin.", "year": 1999}, {"title": "Data description in subspaces", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "In Proc. of 15th Int. Conference on Pattern Recognition,", "citeRegEx": "Tax and Duin.,? \\Q2000\\E", "shortCiteRegEx": "Tax and Duin.", "year": 2000}, {"title": "Combining one class classifiers", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "In Proc. of the 2nd International Workshop on Multiple Classifier Systems,", "citeRegEx": "Tax and Duin.,? \\Q2001\\E", "shortCiteRegEx": "Tax and Duin.", "year": 2001}, {"title": "Uniform object generation for optimizing one-class classifiers", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tax and Duin.,? \\Q2001\\E", "shortCiteRegEx": "Tax and Duin.", "year": 2001}, {"title": "Support vector data description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Machine Learning,", "citeRegEx": "Tax and Duin.,? \\Q2004\\E", "shortCiteRegEx": "Tax and Duin.", "year": 2004}, {"title": "Anomaly detection combining one-class SVMs and particle swarm optimization algorithms", "author": ["J. Tian", "H. Gu"], "venue": "Nonlinear Dynamics,", "citeRegEx": "Tian and Gu.,? \\Q2010\\E", "shortCiteRegEx": "Tian and Gu.", "year": 2010}, {"title": "One-class support vector machine for anomaly network traffic detection", "author": ["Q.A. Tran", "H. Duan", "X. Li"], "venue": "In The 2nd Network Research Workshop of the 18th APAN, Cairns, Australia,", "citeRegEx": "Tran et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2004}, {"title": "Integrating local one-class classifiers for image retrieval", "author": ["Y. Tu", "G. Li", "H. Dai"], "venue": "Advanced Data Mining and Applications,", "citeRegEx": "Tu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2006}, {"title": "A theory of learnable", "author": ["L.G. Valiant"], "venue": "Coomunications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "An evaluation of dimension reduction techniques for one-class classification", "author": ["S.D. Villalba", "P. Cunningham"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Villalba and Cunningham.,? \\Q2007\\E", "shortCiteRegEx": "Villalba and Cunningham.", "year": 2007}, {"title": "PSoL: a positive sample only learning algorithm for finding non-coding", "author": ["C. Wang", "C. Ding", "R.F. Meraz", "S.R. Holbrook"], "venue": "RNA genes. BioInformatics,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "One class training for masquerade detection", "author": ["K. Wang", "S.J. Stolfo"], "venue": "In ICDM Workshop on Data Mining for Computer Security,", "citeRegEx": "Wang and Stolfo.,? \\Q2003\\E", "shortCiteRegEx": "Wang and Stolfo.", "year": 2003}, {"title": "Visual object recognition through one-class learning", "author": ["Q.H. Wang", "L.S. Lopes", "D.M.J. Tax"], "venue": "Image Analysis and Recognition,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "Wolpert.,? \\Q1992\\E", "shortCiteRegEx": "Wolpert.", "year": 1992}, {"title": "Using visual features for anti spam filtering", "author": ["C.T. Wu", "K.T. Cheng", "Q. Zhu", "Y.L. Wu"], "venue": "In IEEE International Conference on Image Processing,", "citeRegEx": "Wu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2005}, {"title": "SVM-Based data editing for enhanced one-class classification of remotely sensed imagery", "author": ["S. Xiaomu", "F. Guoliang", "M. Rao"], "venue": "IEEE Geoscience and Remote Sensing Letters,", "citeRegEx": "Xiaomu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xiaomu et al\\.", "year": 2008}, {"title": "Automated single-nucleotide polymorphism analysis using fluorescence excitation\u2013emission spectroscopy and one-class classifiers", "author": ["Y. Xu", "R.G. Brereton"], "venue": "Analytical and Bioanalytical Chemistry,", "citeRegEx": "Xu and Brereton.,? \\Q2007\\E", "shortCiteRegEx": "Xu and Brereton.", "year": 2007}, {"title": "Wearable accelerometer based extendable activity recognition system", "author": ["J. Yang", "S. Wang", "N. Chen", "X. Chen", "P. Shi"], "venue": "In 2010 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Brain activation detection by neighborhood one-class SVM", "author": ["J. Yang", "N. Zhong", "P. Liang", "J. Wang", "Y. Yao", "S. Lu"], "venue": "Cognitive Systems Research :,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "One-class support vector machine calibration using particle swarm optimisation", "author": ["L. Yang", "M.G. Madden"], "venue": "In AICS 2007,", "citeRegEx": "Yang and Madden.,? \\Q2007\\E", "shortCiteRegEx": "Yang and Madden.", "year": 2007}, {"title": "One-class support vector machines for recommendation tasks", "author": ["Y. Yasutoshi"], "venue": "In PKDD,", "citeRegEx": "Yasutoshi.,? \\Q2006\\E", "shortCiteRegEx": "Yasutoshi.", "year": 2006}, {"title": "Leveraging one-class SVM and semantic analysis to detect anomalous content", "author": ["O. Yilmazel", "S. Symonenko", "N. Balasubramanian", "E.D. Liddy"], "venue": "In IEEE International Conference on Intelligence and Security Informatics,", "citeRegEx": "Yilmazel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yilmazel et al\\.", "year": 2005}, {"title": "Learning from positives examples when the negative class is undermined \u2013 microRNA gene identification", "author": ["M. Yousef", "S. Jung", "L.C. Showe", "M.K. Showe"], "venue": "Algorithms for Molecular Biology,", "citeRegEx": "Yousef et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yousef et al\\.", "year": 2008}, {"title": "Support objects for domain approximation", "author": ["A. Ypma", "R.P.W. Duin"], "venue": "In Proc. of the 8th International Conference on Artificial Neural Networks,", "citeRegEx": "Ypma and Duin.,? \\Q1998\\E", "shortCiteRegEx": "Ypma and Duin.", "year": 1998}, {"title": "SVMC: single-class classification with support vector machines", "author": ["H. Yu"], "venue": "In Proc. of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Yu.,? \\Q2003\\E", "shortCiteRegEx": "Yu.", "year": 2003}, {"title": "Single-class classification with mapping convergence", "author": ["H. Yu"], "venue": "Machine Learning,", "citeRegEx": "Yu.,? \\Q2005\\E", "shortCiteRegEx": "Yu.", "year": 2005}, {"title": "PEBL: positive-example based learning for web page classification using SVM", "author": ["H. Yu", "J. Han", "K.C.C. Chang"], "venue": "In Eighth International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Yu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2002}, {"title": "Text classification from positive and unlabeled documents", "author": ["H. Yu", "C.X. Zhai", "J. Han"], "venue": "In Proc. of the 12th international conference on Information and knowledge management,", "citeRegEx": "Yu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2003}, {"title": "PEBL: web page classification without negative examples", "author": ["H. Yu", "J. Han", "K.C.C. Chang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Yu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2004}, {"title": "One-class classification for spontaneous facial expression analysis", "author": ["Z. Zeng", "Y. Fu", "G.I. Roisman", "Z. Wen", "Y. Hu", "T.S. Huang"], "venue": "In Proc. of the 7th International Conference on Automatic Face and Gesture Recognition,", "citeRegEx": "Zeng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2006}, {"title": "Learning from positive and unlabeled examples: A survey", "author": ["B. Zhang", "W. Zuo"], "venue": "In 2008 International Symposiums on Information Processing ISIP,", "citeRegEx": "Zhang and Zuo.,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Zuo.", "year": 2008}, {"title": "A learning algorithm for one-class data stream classification based on ensemble classifier", "author": ["D. Zhang", "L. Cai", "Y. Wang", "L. Zhang"], "venue": "In 2010 International Conference on Computer Application and System Modeling (ICCASM),", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Combining one class classification models for avian influenza outbreaks", "author": ["J. Zhang", "J. Lu", "G. Zhang"], "venue": "In 2011 IEEE Symposium on Computational Intelligence in Multicriteria DecisionMaking (MDCM),", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "One class support vector machine for anomaly detection in the communication network performance data", "author": ["R. Zhang", "S. Zhang", "S. Muthuraman", "J. Jiang"], "venue": "In Proc. of the 5th conference on Applied electromagnetics, wireless and optical communications,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "One-class classification of text streams with concept drift", "author": ["Y. Zhang", "X. Li", "M. Orlowska"], "venue": "In IEEE International Conference on Data Mining Workshops,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Customer churn prediction using improved one-class support vector machine", "author": ["Y. Zhao", "B. Li", "X. Li", "W. Liu", "S. Ren"], "venue": "In Advanced Data Mining and Applications,", "citeRegEx": "Zhao et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2005}, {"title": "Extraction of brain tumor from MR images using one-class support vector machine", "author": ["J. Zhou", "K.L. Chan", "V.F.H. Chong", "S.M. Krishnan"], "venue": "In Proc. of the 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "Zhu.,? \\Q2005\\E", "shortCiteRegEx": "Zhu.", "year": 2005}], "referenceMentions": [{"referenceID": 143, "context": "In one-class classification (OCC) (Tax, 2001; Tax and Duin, 2001b), one of the classes (which we will arbitrarily refer to as the positive or target class) is well characterized by instances in the training data, while the other class (negative or outlier) has either no instances or very few of them, or they do not form a statistically-representative sample of the negative concept.", "startOffset": 34, "endOffset": 66}, {"referenceID": 171, "context": "For example, in order to construct a homepage classifier (Yu et al., 2002), sample of homepages (positive training examples) and a sample of non-homepages (negative training examples) need to be gleaned.", "startOffset": 57, "endOffset": 74}, {"referenceID": 117, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 70, "endOffset": 97}, {"referenceID": 7, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 117, "endOffset": 131}, {"referenceID": 58, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 150, "endOffset": 167}, {"referenceID": 98, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 199, "endOffset": 263}, {"referenceID": 170, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 199, "endOffset": 263}, {"referenceID": 37, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007).", "startOffset": 199, "endOffset": 263}, {"referenceID": 91, "context": "It appears that Minter (1975) was the first to use the term \u2018single-class classification\u2019 four decades ago, in the context of learning Bayes classifier that requires only labelled data from the \u201cclass of interest\u201d.", "startOffset": 16, "endOffset": 30}, {"referenceID": 91, "context": "It appears that Minter (1975) was the first to use the term \u2018single-class classification\u2019 four decades ago, in the context of learning Bayes classifier that requires only labelled data from the \u201cclass of interest\u201d. Much later, Moya et al. (1993) originate the term One-Class Classification in their research work.", "startOffset": 16, "endOffset": 246}, {"referenceID": 7, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007). These terms originate as a result of different applications to which one-class classification has been applied. Juszczak (2006) defines One-Class Classifiers as class descriptors that are able to learn restricted domains in a multi-dimensional pattern space using primarily just a positive set of examples.", "startOffset": 118, "endOffset": 393}, {"referenceID": 7, "context": "Different researchers have used other terms such as Outlier Detection (Ritter and Gallegos, 1997), Novelty Detection (Bishop, 1994), Concept Learning (Japkowicz, 1999) or Single Class Classification (Munroe and Madden, 2005; Yu, 2005; El-Yaniv and Nisenson, 2007). These terms originate as a result of different applications to which one-class classification has been applied. Juszczak (2006) defines One-Class Classifiers as class descriptors that are able to learn restricted domains in a multi-dimensional pattern space using primarily just a positive set of examples. As observed by Tax (2001), the problems that are encountered in the conventional classification problems, such as the estimation of the classification error, measuring the complexity of a solution, the curse of dimensionality, the generalization of the classification method also appear in OCC and sometimes become even more prominent.", "startOffset": 118, "endOffset": 598}, {"referenceID": 18, "context": "Readers are advised to refer to detailed literature survey on outlier detection by Chandola et al. (2009) Readers are advised to refer to detailed literature survey on novelty detection by Markou and Singh (2003a,b)", "startOffset": 83, "endOffset": 106}, {"referenceID": 102, "context": "In such imbalanced dataset scenarios, several other performance metrics can also be useful, such as f-score, geometric mean etc (Nguyen et al., 2009).", "startOffset": 128, "endOffset": 149}, {"referenceID": 138, "context": "As mentioned in the work of Tax (2001), a confusion matrix (see Table 1) can be constructed to compute the classification performance of one-class classifiers.", "startOffset": 28, "endOffset": 39}, {"referenceID": 47, "context": "However, during testing if the outlier class is not presented in a reasonable proportion, then the actual accuracy values of the one-class classifier could be manipulated and they may not be the true representative of the metric; this point is explored in detail by Glavin and Madden (2009). In such imbalanced dataset scenarios, several other performance metrics can also be useful, such as f-score, geometric mean etc (Nguyen et al.", "startOffset": 266, "endOffset": 291}, {"referenceID": 143, "context": "Source: Tax (2001).", "startOffset": 8, "endOffset": 19}, {"referenceID": 94, "context": "Mazhelis (2006) presents a review of OCC algorithms and analyzed its suitability in the context of mobile-masquerader detection.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "Brew et al. (2007) present a review of several OCC algorithms along with Gaussian Mixture models for the Speaker Verification problem.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "Brew et al. (2007) present a review of several OCC algorithms along with Gaussian Mixture models for the Speaker Verification problem. Their main work revolves around front-end processing and feature extraction from speech data, and speaker and imposter modelling and using them in OCC framework. Kennedy et al. (2009) discuss some issues related to OCC and", "startOffset": 0, "endOffset": 319}, {"referenceID": 3, "context": "Bergamini et al. (2009) present a brief overview of OCC algorithms for the biometric applications.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods.", "startOffset": 0, "endOffset": 212}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed.", "startOffset": 0, "endOffset": 518}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed. The survey in this publication identifies some important research areas, raises several open questions in the study of OCC and discusses significant contributions made by researchers (see Section 4). In this work, we neither restrict the review of literature pertaining to OCC to a particular application domain, nor to specific algorithms that are dependent on type of the data or model. Our aim is to cover as many algorithms, designs, contexts and applications where OCC has been applied in multiple ways (as shown by examples in Section 1). This publication does not intend to duplicate or re-state previous review work; little of the research work presented here may be found in the past surveys of Mazhelis (2006), Brew et al.", "startOffset": 0, "endOffset": 1289}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed. The survey in this publication identifies some important research areas, raises several open questions in the study of OCC and discusses significant contributions made by researchers (see Section 4). In this work, we neither restrict the review of literature pertaining to OCC to a particular application domain, nor to specific algorithms that are dependent on type of the data or model. Our aim is to cover as many algorithms, designs, contexts and applications where OCC has been applied in multiple ways (as shown by examples in Section 1). This publication does not intend to duplicate or re-state previous review work; little of the research work presented here may be found in the past surveys of Mazhelis (2006), Brew et al. (2007), Kennedy et al.", "startOffset": 0, "endOffset": 1309}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed. The survey in this publication identifies some important research areas, raises several open questions in the study of OCC and discusses significant contributions made by researchers (see Section 4). In this work, we neither restrict the review of literature pertaining to OCC to a particular application domain, nor to specific algorithms that are dependent on type of the data or model. Our aim is to cover as many algorithms, designs, contexts and applications where OCC has been applied in multiple ways (as shown by examples in Section 1). This publication does not intend to duplicate or re-state previous review work; little of the research work presented here may be found in the past surveys of Mazhelis (2006), Brew et al. (2007), Kennedy et al. (2009), Bergamini et al.", "startOffset": 0, "endOffset": 1332}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed. The survey in this publication identifies some important research areas, raises several open questions in the study of OCC and discusses significant contributions made by researchers (see Section 4). In this work, we neither restrict the review of literature pertaining to OCC to a particular application domain, nor to specific algorithms that are dependent on type of the data or model. Our aim is to cover as many algorithms, designs, contexts and applications where OCC has been applied in multiple ways (as shown by examples in Section 1). This publication does not intend to duplicate or re-state previous review work; little of the research work presented here may be found in the past surveys of Mazhelis (2006), Brew et al. (2007), Kennedy et al. (2009), Bergamini et al. (2009) or Bartkowiak (2011).", "startOffset": 0, "endOffset": 1357}, {"referenceID": 2, "context": "Bartkowiak (2011) presents survey of research on Anomaly, Outlier and OCC. The research survey is mostly focussed on research and their applications of OCC for detecting unknown behaviour. Khan and Madden (2009) present a short survey of the recent trends in the field of OCC, wherein they present a taxonomy for the study of OCC methods. Their taxonomy is based on availability of training data, methodology used and application domains applied. This publication is an extension of the work of Khan and Madden (2009), and is more comprehensive, in-depth and detailed. The survey in this publication identifies some important research areas, raises several open questions in the study of OCC and discusses significant contributions made by researchers (see Section 4). In this work, we neither restrict the review of literature pertaining to OCC to a particular application domain, nor to specific algorithms that are dependent on type of the data or model. Our aim is to cover as many algorithms, designs, contexts and applications where OCC has been applied in multiple ways (as shown by examples in Section 1). This publication does not intend to duplicate or re-state previous review work; little of the research work presented here may be found in the past surveys of Mazhelis (2006), Brew et al. (2007), Kennedy et al. (2009), Bergamini et al. (2009) or Bartkowiak (2011). Moreover, this publication encompasses a broader definition of OCC than many.", "startOffset": 0, "endOffset": 1378}, {"referenceID": 83, "context": "Category (c) has been a matter of much research interest among the text/document classification community (Liu et al., 2003; Li and Liu, 2003; Lee and Liu, 2003) that will be discussed in detail in Section 4.", "startOffset": 106, "endOffset": 161}, {"referenceID": 80, "context": "Category (c) has been a matter of much research interest among the text/document classification community (Liu et al., 2003; Li and Liu, 2003; Lee and Liu, 2003) that will be discussed in detail in Section 4.", "startOffset": 106, "endOffset": 161}, {"referenceID": 74, "context": "Category (c) has been a matter of much research interest among the text/document classification community (Liu et al., 2003; Li and Liu, 2003; Lee and Liu, 2003) that will be discussed in detail in Section 4.", "startOffset": 106, "endOffset": 161}, {"referenceID": 153, "context": "Under the assumptions that each set of the features is sufficient for classification, and the feature sets of each instance are conditionally independent given the class, they provide PAC (Probably Approximately Correct) learning (Valiant, 1984) guarantees on learning from labelled and unlabeled data and prove that unlabeled examples can boost accuracy.", "startOffset": 230, "endOffset": 245}, {"referenceID": 70, "context": ", 2003; Li and Liu, 2003; Lee and Liu, 2003) that will be discussed in detail in Section 4.3.1. Tax and Duin (1999a,b) and Sch\u00f6lkopf et al. (1999b) have developed various algorithms based on support vector machines to tackle the problem of OCC using positive examples only; for a detailed discussion on them, refer to Section 4.", "startOffset": 26, "endOffset": 148}, {"referenceID": 9, "context": "The problem of learning with the help of unlabeled data given a small set of labelled examples was studied by Blum and Mitchell (1998) by using the concept of cotraining.", "startOffset": 110, "endOffset": 135}, {"referenceID": 9, "context": "The problem of learning with the help of unlabeled data given a small set of labelled examples was studied by Blum and Mitchell (1998) by using the concept of cotraining. The co-training approach can be applied when a data set has natural separation of their features and classifiers are built incrementally on them. Blum and Mitchell demonstrate the use of co-training methods to train the classifiers in the application of text classification. Under the assumptions that each set of the features is sufficient for classification, and the feature sets of each instance are conditionally independent given the class, they provide PAC (Probably Approximately Correct) learning (Valiant, 1984) guarantees on learning from labelled and unlabeled data and prove that unlabeled examples can boost accuracy. Denis (1998) was the first to conduct a theoretical study of PAC learning from positive and unlabeled data.", "startOffset": 110, "endOffset": 815}, {"referenceID": 9, "context": "The problem of learning with the help of unlabeled data given a small set of labelled examples was studied by Blum and Mitchell (1998) by using the concept of cotraining. The co-training approach can be applied when a data set has natural separation of their features and classifiers are built incrementally on them. Blum and Mitchell demonstrate the use of co-training methods to train the classifiers in the application of text classification. Under the assumptions that each set of the features is sufficient for classification, and the feature sets of each instance are conditionally independent given the class, they provide PAC (Probably Approximately Correct) learning (Valiant, 1984) guarantees on learning from labelled and unlabeled data and prove that unlabeled examples can boost accuracy. Denis (1998) was the first to conduct a theoretical study of PAC learning from positive and unlabeled data. Denis proved that many concept classes, specifically those that are learnable from statistical queries, can be efficiently learned in a PAC framework using positive and unlabeled data. However, the trade-off is a considerable increase in the number of examples needed to achieve learning, although it remains polynomial in size. De Comit\u00e9 et al. (1999) give evidence with both theoretical and empirical arguments that positive examples and unlabeled examples can boost accuracy of many", "startOffset": 110, "endOffset": 1263}, {"referenceID": 94, "context": "Muggleton (2001) presents a theoretical study in the Bayesian framework where the distribution of functions and examples are assumed to be known.", "startOffset": 0, "endOffset": 17}, {"referenceID": 82, "context": "Liu et al. (2002) extend Muggleton\u2019s result to the noisy case; they present sample complexity results for learning by maximizing the number of unlabeled examples labelled as negative while constraining the classifier to label all the positive examples correctly.", "startOffset": 0, "endOffset": 18}, {"referenceID": 55, "context": "However, SVDD can reject some fraction of positively labelled data when the volume of the hyper-sphere decreases. The hyper-sphere model of the SVDD can be made more flexible by introducing kernel functions. Tax (2001) considers Polynomial and a Gaussian kernel and found that the Gaussian kernel works better for most datasets considered (Figure 3).", "startOffset": 0, "endOffset": 219}, {"referenceID": 55, "context": "However, SVDD can reject some fraction of positively labelled data when the volume of the hyper-sphere decreases. The hyper-sphere model of the SVDD can be made more flexible by introducing kernel functions. Tax (2001) considers Polynomial and a Gaussian kernel and found that the Gaussian kernel works better for most datasets considered (Figure 3). Tax uses different values for the width of the kernel. The larger the width of the kernel, the fewer support vectors are selected and the description becomes more spherical. Also, using the Gaussian kernel instead of the Polynomial kernel results in tighter descriptions, but it requires more data to support more flexible boundary. Tax\u2019s method becomes inefficient when the data set has high dimension. It also does not work well when large variations in density exist among the positive-class objects; in such case, it starts rejecting the low-density target points as outliers. Tax (2001) demonstrates the usefulness of the approach on machine fault diagnostic data and handwritten digit data.", "startOffset": 0, "endOffset": 943}, {"referenceID": 55, "context": "However, SVDD can reject some fraction of positively labelled data when the volume of the hyper-sphere decreases. The hyper-sphere model of the SVDD can be made more flexible by introducing kernel functions. Tax (2001) considers Polynomial and a Gaussian kernel and found that the Gaussian kernel works better for most datasets considered (Figure 3). Tax uses different values for the width of the kernel. The larger the width of the kernel, the fewer support vectors are selected and the description becomes more spherical. Also, using the Gaussian kernel instead of the Polynomial kernel results in tighter descriptions, but it requires more data to support more flexible boundary. Tax\u2019s method becomes inefficient when the data set has high dimension. It also does not work well when large variations in density exist among the positive-class objects; in such case, it starts rejecting the low-density target points as outliers. Tax (2001) demonstrates the usefulness of the approach on machine fault diagnostic data and handwritten digit data. Tax and Duin (2001b) propose a sophisticated method which uses artificially generated outliers to optimize the OSVM parameters in order to balance between over-fitting and under-fitting.", "startOffset": 0, "endOffset": 1069}, {"referenceID": 143, "context": "Source: Tax (2001).", "startOffset": 8, "endOffset": 19}, {"referenceID": 143, "context": "Source: Tax (2001).", "startOffset": 8, "endOffset": 19}, {"referenceID": 16, "context": "As mentioned by Campbell and Bennett Campbell and P. Bennett (2001), the origin plays a crucial role in the methods of both Sch\u00f6lkopf et al.", "startOffset": 16, "endOffset": 68}, {"referenceID": 16, "context": "As mentioned by Campbell and Bennett Campbell and P. Bennett (2001), the origin plays a crucial role in the methods of both Sch\u00f6lkopf et al. and Tax & Duin, which is a drawback since the origin effectively acts as a prior for where the abnormal class instances are assumed to lie; this is termed the problem of origin. Sch\u00f6lkopf et al. (1999a) have tested their method on both synthetic and real-world data, including the US Postal Services dataset of handwritten digits.", "startOffset": 16, "endOffset": 344}, {"referenceID": 180, "context": "(Zhao et al., 2005) use this method for customer churn prediction for the wireless industry data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 85, "context": "Manevitz and Yousef (2001) evaluate the results on the Reuters data set using the 10 most frequent categories.", "startOffset": 0, "endOffset": 27}, {"referenceID": 55, "context": "However they observe that when the number of categories are increased, their version of OSVM obtains better results. Li et al. (2003) present an improved version of the approach of Sch\u00f6lkopf et al.", "startOffset": 0, "endOffset": 134}, {"referenceID": 55, "context": "However they observe that when the number of categories are increased, their version of OSVM obtains better results. Li et al. (2003) present an improved version of the approach of Sch\u00f6lkopf et al. (1999a) for detecting anomaly in an intrusion detection system, with higher accuracy.", "startOffset": 0, "endOffset": 206}, {"referenceID": 89, "context": "Source: Manevitz and Yousef (2001).", "startOffset": 8, "endOffset": 35}, {"referenceID": 125, "context": "An extension to the work of Tax and Duin (1999a,b) and Sch\u00f6lkopf et al. (2000) is proposed by Campbell and P.", "startOffset": 55, "endOffset": 79}, {"referenceID": 125, "context": "An extension to the work of Tax and Duin (1999a,b) and Sch\u00f6lkopf et al. (2000) is proposed by Campbell and P. Bennett (2001). They present a kernel OCC algorithm that uses linear programming techniques instead of quadratic programming.", "startOffset": 55, "endOffset": 125}, {"referenceID": 77, "context": "Source: Li et al. (2003).", "startOffset": 8, "endOffset": 25}, {"referenceID": 129, "context": "Madden (2007) apply particle swarm optimization to calibrate the parameters of OSVM (Sch\u00f6lkopf et al., 2000) and find experimentally that their method either matches or surpass the performance of OSVM with parameters optimized using grid search method, while using lower CPU time.", "startOffset": 84, "endOffset": 108}, {"referenceID": 129, "context": "Tian and Gu (2010) propose a refinement to the Sch\u00f6lkopf\u2019s OSVM model (Sch\u00f6lkopf et al., 2000) by searching optimal parameters using particle swarm optimization algorithm (Kennedy and Eberhart, 1995) and improving the original decision function with a boundary movement.", "startOffset": 70, "endOffset": 94}, {"referenceID": 62, "context": ", 2000) by searching optimal parameters using particle swarm optimization algorithm (Kennedy and Eberhart, 1995) and improving the original decision function with a boundary movement.", "startOffset": 84, "endOffset": 112}, {"referenceID": 125, "context": "Madden (2007) apply particle swarm optimization to calibrate the parameters of OSVM (Sch\u00f6lkopf et al., 2000) and find experimentally that their method either matches or surpass the performance of OSVM with parameters optimized using grid search method, while using lower CPU time. Tian and Gu (2010) propose a refinement to the Sch\u00f6lkopf\u2019s OSVM model (Sch\u00f6lkopf et al.", "startOffset": 85, "endOffset": 300}, {"referenceID": 19, "context": "In their formulation of OSVM, they assume the neighbourhood consistency hypothesis used by Chen et al. (2002). By integrating it with OSVM, they compute primal values which denote distance between points and hyper-plane in kernel space.", "startOffset": 91, "endOffset": 110}, {"referenceID": 91, "context": "Yu (2005) proposes a one-class classification algorithm with SVMs using positive and unlabeled data and without labelled negative data and discuss some of the limitations of other OSVMbased OCC algorithms (Tax and Duin, 2001b; Manevitz and Yousef, 2001).", "startOffset": 205, "endOffset": 253}, {"referenceID": 169, "context": "Source: Yu (2005).", "startOffset": 8, "endOffset": 18}, {"referenceID": 167, "context": "Yu notes that when the numbers of support vectors in OSVM were increased, it overfits the data rather than being more accurate. Yu (2003) presents an OCC algorithm called Mapping Convergence (MC) to induce accurate class boundary around the positive data set in the presence of unlabeled data and without negative examples.", "startOffset": 0, "endOffset": 138}, {"referenceID": 117, "context": "Rocchio (1971)) is used to extract strong negatives (those that are far from the class boundary of the positive data) from the unlabeled data.", "startOffset": 0, "endOffset": 15}, {"referenceID": 117, "context": "Rocchio (1971)) is used to extract strong negatives (those that are far from the class boundary of the positive data) from the unlabeled data. In the second phase, a base classifier (e.g. SVM) is used iteratively to maximize the margin between positive and strong negatives for better approximation of the class boundary. Yu (2003) also presents another algorithm called Support Vector Mapping Convergence (SVMC) that works faster than the MC algorithm.", "startOffset": 0, "endOffset": 332}, {"referenceID": 23, "context": "Another fuzzy OSVM classifier is proposed by Choi and Kim (2004) for generating visually salient and semantically important video segments.", "startOffset": 45, "endOffset": 65}, {"referenceID": 158, "context": "However, using just the best classifier and discarding the classifiers with poorer performance might waste valuable information (Wolpert, 1992).", "startOffset": 128, "endOffset": 143}, {"referenceID": 132, "context": "This may serve to increase the performance and also the robustness of the classification (Sharkey and Sharkey, 1995).", "startOffset": 89, "endOffset": 116}, {"referenceID": 141, "context": "This simple algorithm is known to give good results for multi-class problems (Tanigushi and Tresp, 1997).", "startOffset": 77, "endOffset": 104}, {"referenceID": 7, "context": "They use a Normal density and a mixture of Gaussians and the Parzen density estimation (Bishop, 1994) as two types of one-class classifiers.", "startOffset": 87, "endOffset": 101}, {"referenceID": 168, "context": "They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999).", "startOffset": 90, "endOffset": 111}, {"referenceID": 58, "context": "They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999).", "startOffset": 147, "endOffset": 164}, {"referenceID": 126, "context": "Ban and Abe (2006) address the problem of building a multi-class classifier based on an ensemble of one-class classifiers by studying two kinds of one-class classifiers, namely, SVDD (Tax and Duin, 1999b) and Kernel Principal Component Analysis (Sch\u00f6lkopf et al., 1998).", "startOffset": 245, "endOffset": 269}, {"referenceID": 53, "context": "However, using just the best classifier and discarding the classifiers with poorer performance might waste valuable information (Wolpert, 1992). To improve the performance of different classifiers which may differ in complexity or in the underlying training algorithm used to construct them, an ensemble of classifiers is a viable solution. This may serve to increase the performance and also the robustness of the classification (Sharkey and Sharkey, 1995). Classifiers are commonly ensembled to provide a combined decision by averaging the estimated posterior probabilities. This simple algorithm is known to give good results for multi-class problems (Tanigushi and Tresp, 1997). In the case of one-class classifiers, the situation is different. One-class classifiers cannot directly provide posterior probabilities for target (positive class) objects, because accurate information on the distribution of the outlier data is not available, as has been discussed earlier in this paper. In most cases, by assuming that the outliers are uniformly distributed, the posterior probability can be estimated. Tax (2001) mentions that in some OCC methods, distance is estimated instead of probability, and if there exists a combination of distance and probability outputs, they should be standardized before they can be combined.", "startOffset": 0, "endOffset": 1115}, {"referenceID": 53, "context": "However, using just the best classifier and discarding the classifiers with poorer performance might waste valuable information (Wolpert, 1992). To improve the performance of different classifiers which may differ in complexity or in the underlying training algorithm used to construct them, an ensemble of classifiers is a viable solution. This may serve to increase the performance and also the robustness of the classification (Sharkey and Sharkey, 1995). Classifiers are commonly ensembled to provide a combined decision by averaging the estimated posterior probabilities. This simple algorithm is known to give good results for multi-class problems (Tanigushi and Tresp, 1997). In the case of one-class classifiers, the situation is different. One-class classifiers cannot directly provide posterior probabilities for target (positive class) objects, because accurate information on the distribution of the outlier data is not available, as has been discussed earlier in this paper. In most cases, by assuming that the outliers are uniformly distributed, the posterior probability can be estimated. Tax (2001) mentions that in some OCC methods, distance is estimated instead of probability, and if there exists a combination of distance and probability outputs, they should be standardized before they can be combined. Having done so, the same types of combining rules as in conventional classification ensembles can be used. Tax and Duin (2001a) investigate the influence of the feature sets, their inter-dependence and the type of one-class classifiers for the best choice of combination rules.", "startOffset": 0, "endOffset": 1452}, {"referenceID": 6, "context": "They use a Normal density and a mixture of Gaussians and the Parzen density estimation (Bishop, 1994) as two types of one-class classifiers. They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999). In their experiments, the Parzen density estimator emerges as the best individual one-class classifier on the handwritten digit pixel dataset . Tax (2001) show that combining classifiers trained on different feature spaces is useful.", "startOffset": 88, "endOffset": 462}, {"referenceID": 6, "context": "They use a Normal density and a mixture of Gaussians and the Parzen density estimation (Bishop, 1994) as two types of one-class classifiers. They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999). In their experiments, the Parzen density estimator emerges as the best individual one-class classifier on the handwritten digit pixel dataset . Tax (2001) show that combining classifiers trained on different feature spaces is useful. In their experiments, the product combination rule gives the best results while the mean combination rule suffers from the fact that the area covered by the target set tends to be overestimated. Lai et al. (2002) study combining one-class classifier for image database retrieval and show that combining SVDD-based classifiers improve the retrieval precision.", "startOffset": 88, "endOffset": 754}, {"referenceID": 6, "context": "They use a Normal density and a mixture of Gaussians and the Parzen density estimation (Bishop, 1994) as two types of one-class classifiers. They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999). In their experiments, the Parzen density estimator emerges as the best individual one-class classifier on the handwritten digit pixel dataset . Tax (2001) show that combining classifiers trained on different feature spaces is useful. In their experiments, the product combination rule gives the best results while the mean combination rule suffers from the fact that the area covered by the target set tends to be overestimated. Lai et al. (2002) study combining one-class classifier for image database retrieval and show that combining SVDD-based classifiers improve the retrieval precision. Juszczak and Duin (2004) extend combining one-class classifierd for classifying missing data.", "startOffset": 88, "endOffset": 925}, {"referenceID": 6, "context": "They use a Normal density and a mixture of Gaussians and the Parzen density estimation (Bishop, 1994) as two types of one-class classifiers. They use four models, the SVDD (Tax and Duin, 1999b), K-means clustering, K-center method (Ypma and Duin, 1998) and an auto-encoder neural network (Japkowicz, 1999). In their experiments, the Parzen density estimator emerges as the best individual one-class classifier on the handwritten digit pixel dataset . Tax (2001) show that combining classifiers trained on different feature spaces is useful. In their experiments, the product combination rule gives the best results while the mean combination rule suffers from the fact that the area covered by the target set tends to be overestimated. Lai et al. (2002) study combining one-class classifier for image database retrieval and show that combining SVDD-based classifiers improve the retrieval precision. Juszczak and Duin (2004) extend combining one-class classifierd for classifying missing data. Their idea is to form an ensemble of one-class classifiers trained on each feature or each pre-selected group of features, or to compute a dissimilarity representation from features. The ensemble is able to predict missing feature values based on the remaining classifiers. As compared to standard methods, their method is more flexible, since it requires significantly fewer classifiers and does not require re-training of the system whenever missing feature values occur. Juszczak and Duin (2004) also show that their method is robust to small sample size problems due to splitting the classification problem into several smaller ones.", "startOffset": 88, "endOffset": 1493}, {"referenceID": 1, "context": "Ban and Abe (2006) address the problem of building a multi-class classifier based on an ensemble of one-class classifiers by studying two kinds of one-class classifiers, namely, SVDD (Tax and Duin, 1999b) and Kernel Principal Component Analysis (Sch\u00f6lkopf et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 55, "context": "Nanni (2006) studies combining several one-class classifiers using the random subspace method (Ho, 1998) for the problem of online signature verification.", "startOffset": 94, "endOffset": 104}, {"referenceID": 95, "context": "Nanni (2006) studies combining several one-class classifiers using the random subspace method (Ho, 1998) for the problem of online signature verification.", "startOffset": 0, "endOffset": 13}, {"referenceID": 20, "context": "Cheplygina and Tax (2011) propose to apply pruning to random sub-spaces of one-class classifiers.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Bergamini et al. (2008) present the use of OSVM for biometric fusion where very low false acceptance rates are required.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": "Bergamini et al. (2008) present the use of OSVM for biometric fusion where very low false acceptance rates are required. They suggest a fusion system that may be employed at the level of feature selection, score-matching or decision-making. A normalization step is used before combining scores from different classifiers. Bergamini et al. use z-score normalization, min-max normalization, and column norm normalization, and classifiers are combined using various ensemble rules. They find that min-max normalization with a weighted sum gives the best results on NIST Biometric Scores Set. Ges\u00f9 and Bosco (2007) present an ensemble method of combining one-class fuzzy KNN classifiers.", "startOffset": 0, "endOffset": 611}, {"referenceID": 3, "context": "However, bagging is useful when the classifiers are unstable and small changes in the training data can cause large changes in the classifier outputs (Bauer and Kohavi, 1999).", "startOffset": 150, "endOffset": 174}, {"referenceID": 142, "context": "Tu et al. (2006) extends the one-class information bottleneck method for information retrieval by introducing bagging ensemble learning.", "startOffset": 0, "endOffset": 17}, {"referenceID": 131, "context": "Shieh and Kamm (2009) propose an ensemble method for combining OSVM (Tax and Duin, 1999a) using bagging.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "However, bagging is useful when the classifiers are unstable and small changes in the training data can cause large changes in the classifier outputs (Bauer and Kohavi, 1999). OSVM is not an unstable classifier as its estimated boundary always encloses the positive class, therefore directly applying bagging on OSVM is not useful. Shieh and Kamm (2009) propose a kernel density estimation method to give weights to the training data objects, such that the outliers get the least weights and the positive class members get higher weights for creating bootstrap samples.", "startOffset": 151, "endOffset": 354}, {"referenceID": 35, "context": "Boosting methods are widely used in traditional classification problems (Dietterich, 2000) for their high accuracy and ease of implementation.", "startOffset": 72, "endOffset": 90}, {"referenceID": 85, "context": "(2002) propose a boosting-like one-class classification algorithm based on a technique called barrier optimization (Luenberger, 1984).", "startOffset": 115, "endOffset": 133}, {"referenceID": 35, "context": "Boosting methods are widely used in traditional classification problems (Dietterich, 2000) for their high accuracy and ease of implementation. R\u00e4tsch et al. (2002) propose a boosting-like one-class classification algorithm based on a technique called barrier optimization (Luenberger, 1984).", "startOffset": 73, "endOffset": 164}, {"referenceID": 35, "context": "Boosting methods are widely used in traditional classification problems (Dietterich, 2000) for their high accuracy and ease of implementation. R\u00e4tsch et al. (2002) propose a boosting-like one-class classification algorithm based on a technique called barrier optimization (Luenberger, 1984). They also show, through an equivalence of mathematical programs, that a support vector algorithm can be translated into an equivalent boosting-like algorithm and vice versa. It has been pointed out by Schapire et al. (1998) that boosting and SVMs are \u2018essentially the same\u2019 except for the way they measure the margin or the way they optimize their weight vector: SVMs use the l2-norm to implicitly compute scalar products in feature space with the help of kernel trick, whereas boosting employs the l1-norm to perform computation explicitly in the feature space.", "startOffset": 73, "endOffset": 516}, {"referenceID": 35, "context": "Boosting methods are widely used in traditional classification problems (Dietterich, 2000) for their high accuracy and ease of implementation. R\u00e4tsch et al. (2002) propose a boosting-like one-class classification algorithm based on a technique called barrier optimization (Luenberger, 1984). They also show, through an equivalence of mathematical programs, that a support vector algorithm can be translated into an equivalent boosting-like algorithm and vice versa. It has been pointed out by Schapire et al. (1998) that boosting and SVMs are \u2018essentially the same\u2019 except for the way they measure the margin or the way they optimize their weight vector: SVMs use the l2-norm to implicitly compute scalar products in feature space with the help of kernel trick, whereas boosting employs the l1-norm to perform computation explicitly in the feature space. Schapire et al. comment that SVMs can be thought of as a \u2018boosting\u2019 approach in high dimensional feature space spanned by the base hypotheses. R\u00e4tsch et al. (2002) exemplify this translation procedure for a new algorithm called the one-class leveraging.", "startOffset": 73, "endOffset": 1019}, {"referenceID": 26, "context": "They compare a number of unsupervised methods from classical pattern recognition to several variations of a standard shared weight supervised neural network (Cun et al., 1989) and show that adding a hidden layer with radial basis function improves performance.", "startOffset": 157, "endOffset": 175}, {"referenceID": 121, "context": "The network is trained using a standard backpropagation algorithm (Rumelhart and McClelland, 1986) to learn the identity function on the positive examples.", "startOffset": 66, "endOffset": 98}, {"referenceID": 29, "context": "de Ridder et al. (1998) conduct an experimental comparison of various OCC algorithms.", "startOffset": 3, "endOffset": 24}, {"referenceID": 26, "context": "They compare a number of unsupervised methods from classical pattern recognition to several variations of a standard shared weight supervised neural network (Cun et al., 1989) and show that adding a hidden layer with radial basis function improves performance. Manevitz and Yousef (2000a) show that a simple neural network can be trained to filter documents when only positive information is available.", "startOffset": 158, "endOffset": 289}, {"referenceID": 26, "context": "They compare a number of unsupervised methods from classical pattern recognition to several variations of a standard shared weight supervised neural network (Cun et al., 1989) and show that adding a hidden layer with radial basis function improves performance. Manevitz and Yousef (2000a) show that a simple neural network can be trained to filter documents when only positive information is available. They design a bottleneck filter that uses a basic feed-forward neural network that can incorporate the restriction of availability of only positive examples. They chose three level network with m input neurons, m output neurons and k hidden neurons, where k <m. The network is trained using a standard backpropagation algorithm (Rumelhart and McClelland, 1986) to learn the identity function on the positive examples. The idea is that while the bottleneck prevents learning the full identity function on m-space, the identity on the small set of examples is in fact learnable. The set of vectors for which the network acts as the identity function is more like a sub-space which is similar to the trained set. For testing a given vector, it is shown to the network and if the result is the identity, the vector is deemed interesting (i.e. positive class) otherwise it is deemed an outlier. Manevitz and Yousef (2000b) apply the auto-associator neural network to document classification problem.", "startOffset": 158, "endOffset": 1321}, {"referenceID": 113, "context": "5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets.", "startOffset": 12, "endOffset": 27}, {"referenceID": 36, "context": "Based on very fast decision trees (VFDT) (Domingos and Hulten, 2000) and POSC4.", "startOffset": 41, "endOffset": 68}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction.", "startOffset": 3, "endOffset": 24}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction. They modify standard C4.5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets. Letouzey et al. (2000) design an algorithm which is based on positive statistical queries (estimates for probabilities over the set of positive instances) and instance statistical queries (estimates for probabilities over the instance space).", "startOffset": 3, "endOffset": 496}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction. They modify standard C4.5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets. Letouzey et al. (2000) design an algorithm which is based on positive statistical queries (estimates for probabilities over the set of positive instances) and instance statistical queries (estimates for probabilities over the instance space). The algorithm guesses the weight of the target concept i.e. the ratio of positive instances in the instance space and then uses a hypothesis testing algorithm. They show that the algorithm can be estimated in polynomial time and is learnable from positive statistical queries and instance statistical queries only. Then, they design a decision tree induction algorithm, called POSC4.5, using only positive and unlabeled data and present experimental results on UCI datasets that are comparable to the C4.5 algorithm. Yu (2005) comments that such rule learning methods are simple and efficient for learning nominal features but are tricky to use for problems of continuous features, high dimensions, or sparse instance spaces.", "startOffset": 3, "endOffset": 1243}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction. They modify standard C4.5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets. Letouzey et al. (2000) design an algorithm which is based on positive statistical queries (estimates for probabilities over the set of positive instances) and instance statistical queries (estimates for probabilities over the instance space). The algorithm guesses the weight of the target concept i.e. the ratio of positive instances in the instance space and then uses a hypothesis testing algorithm. They show that the algorithm can be estimated in polynomial time and is learnable from positive statistical queries and instance statistical queries only. Then, they design a decision tree induction algorithm, called POSC4.5, using only positive and unlabeled data and present experimental results on UCI datasets that are comparable to the C4.5 algorithm. Yu (2005) comments that such rule learning methods are simple and efficient for learning nominal features but are tricky to use for problems of continuous features, high dimensions, or sparse instance spaces. Li and Zhang (2008) perform bagging ensemble on POSC4.", "startOffset": 3, "endOffset": 1462}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction. They modify standard C4.5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets. Letouzey et al. (2000) design an algorithm which is based on positive statistical queries (estimates for probabilities over the set of positive instances) and instance statistical queries (estimates for probabilities over the instance space). The algorithm guesses the weight of the target concept i.e. the ratio of positive instances in the instance space and then uses a hypothesis testing algorithm. They show that the algorithm can be estimated in polynomial time and is learnable from positive statistical queries and instance statistical queries only. Then, they design a decision tree induction algorithm, called POSC4.5, using only positive and unlabeled data and present experimental results on UCI datasets that are comparable to the C4.5 algorithm. Yu (2005) comments that such rule learning methods are simple and efficient for learning nominal features but are tricky to use for problems of continuous features, high dimensions, or sparse instance spaces. Li and Zhang (2008) perform bagging ensemble on POSC4.5 and classify test samples using the majority voting rule. Their result on UCI datasets shows that the classification accuracy and robustness of POSC4.5 could be improved by applying their technique. Based on very fast decision trees (VFDT) (Domingos and Hulten, 2000) and POSC4.5, Li et al. (2009) propose a one-class VFDT for data streams with applications to credit fraud detection and intrusion detection.", "startOffset": 3, "endOffset": 1796}, {"referenceID": 28, "context": "De Comit\u00e9 et al. (1999) present experimental results showing that positive examples and unlabeled data can efficiently boost accuracy of the statistical query learning algorithms for monotone conjunctions in the presence of classification noise and present experimental results for decision tree induction. They modify standard C4.5 algorithm (Quinlan, 1993) to get an algorithm that uses unlabeled and positive data and show the relevance of their method on UCI datasets. Letouzey et al. (2000) design an algorithm which is based on positive statistical queries (estimates for probabilities over the set of positive instances) and instance statistical queries (estimates for probabilities over the instance space). The algorithm guesses the weight of the target concept i.e. the ratio of positive instances in the instance space and then uses a hypothesis testing algorithm. They show that the algorithm can be estimated in polynomial time and is learnable from positive statistical queries and instance statistical queries only. Then, they design a decision tree induction algorithm, called POSC4.5, using only positive and unlabeled data and present experimental results on UCI datasets that are comparable to the C4.5 algorithm. Yu (2005) comments that such rule learning methods are simple and efficient for learning nominal features but are tricky to use for problems of continuous features, high dimensions, or sparse instance spaces. Li and Zhang (2008) perform bagging ensemble on POSC4.5 and classify test samples using the majority voting rule. Their result on UCI datasets shows that the classification accuracy and robustness of POSC4.5 could be improved by applying their technique. Based on very fast decision trees (VFDT) (Domingos and Hulten, 2000) and POSC4.5, Li et al. (2009) propose a one-class VFDT for data streams with applications to credit fraud detection and intrusion detection. They state that by using the proposed algorithm, even if 80% of the data is unlabeled, the performance of one-class VFDT is very close to the standard VFDT algorithm. D\u00e9sir et al. (2012) propose a one-class Random Forest algorithm that internally conjoins bagging and random feature selection (RFS) for decision trees.", "startOffset": 3, "endOffset": 2094}, {"referenceID": 143, "context": "Tax (2001) presents a one-class Nearest Neighbour method, called Nearest Neighbour Description (NN -d), where a test object z is accepted as a member of target class provided that its local density is greater than or equal to the local density of its nearest neighbour in the training set.", "startOffset": 0, "endOffset": 11}, {"referenceID": 143, "context": "Source: Tax (2001).", "startOffset": 8, "endOffset": 19}, {"referenceID": 142, "context": "Tax and Duin (2000) proposes a nearest neighbour method capable of finding data boundaries when the sample size is very low.", "startOffset": 0, "endOffset": 20}, {"referenceID": 96, "context": "Munroe and Madden (2005) extend the idea of one-class KNN to tackle the recognition of vehicles using a set of features extracted from their frontal view, and present results showing high accuracy in classification.", "startOffset": 0, "endOffset": 25}, {"referenceID": 12, "context": "Cabral et al. (2007) propose a one-class nearest neighbour data description using the concept of structural risk minimization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Cabral et al. (2009) present another approach where not only 1-NN is considered but all of the k-nearest neighbours, to arrive at a decision based on majority voting.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Cabral et al. (2009) present another approach where not only 1-NN is considered but all of the k-nearest neighbours, to arrive at a decision based on majority voting. In their experiments on artificial data, biomedical data and data from the UCI repository, they observe that the k-NN version of their classifier outperforms the 1-NN and is better than NN -d algorithms. Ges\u00fa et al. (2008) present a one-class KNN and test it on synthetic data that simulates microarray data for the identification of nucleosomes and linker regions across DNA.", "startOffset": 0, "endOffset": 390}, {"referenceID": 56, "context": "Popular kernels like the polynomial kernel (of degree 1 and 2), RBF and spectroscopic kernels (Spectral Linear Kernel (Howley, 2007) and Weighted Spectral Linear Kernel (Madden and Howley, 2008)) are used in their experiments.", "startOffset": 118, "endOffset": 132}, {"referenceID": 88, "context": "Popular kernels like the polynomial kernel (of degree 1 and 2), RBF and spectroscopic kernels (Spectral Linear Kernel (Howley, 2007) and Weighted Spectral Linear Kernel (Madden and Howley, 2008)) are used in their experiments.", "startOffset": 169, "endOffset": 194}, {"referenceID": 29, "context": "de Haro-Garca et al. (2009) use one-class KNN along with other one-class classifiers for identifying plant/pathogen sequences, and present a comparison of results.", "startOffset": 3, "endOffset": 28}, {"referenceID": 29, "context": "de Haro-Garca et al. (2009) use one-class KNN along with other one-class classifiers for identifying plant/pathogen sequences, and present a comparison of results. They find that these methods are suitable owing to the fact that genomic sequences of plant are easy to obtain in comparison to pathogens, and they build one-class classifiers based only on information from the sequences of the plant. One-class kNN classifiers are used by Glavin and Madden (2009) to study the effect of unexpected outliers that might arise in classification (in their case, they considered the task of classifying spectroscopic data).", "startOffset": 3, "endOffset": 462}, {"referenceID": 29, "context": "de Haro-Garca et al. (2009) use one-class KNN along with other one-class classifiers for identifying plant/pathogen sequences, and present a comparison of results. They find that these methods are suitable owing to the fact that genomic sequences of plant are easy to obtain in comparison to pathogens, and they build one-class classifiers based only on information from the sequences of the plant. One-class kNN classifiers are used by Glavin and Madden (2009) to study the effect of unexpected outliers that might arise in classification (in their case, they considered the task of classifying spectroscopic data). According to the authors, unexpected outliers can be defined as those outliers that do not come from the same distribution of data as the postive cases or outlier cases in the training data set. Their experiments show that the one-class kNN method is more reliable for the task of detecting such outliers than a similar binary kNN classifier. The \u2018kernel approach\u2019 has been used by various researchers to implement different flavours of NN-based classifier for multi-class classification. Khan (2010) extends this idea and propose two variants of one-class nearest neighbour classifiers.", "startOffset": 3, "endOffset": 1118}, {"referenceID": 27, "context": "Datta (1997) suggests a method to learn a N\u00e4\u0131ve Bayes (NB) classifier from samples of positive class data only.", "startOffset": 0, "endOffset": 13}, {"referenceID": 27, "context": "Therefore, Datta (1997) modifies", "startOffset": 11, "endOffset": 24}, {"referenceID": 27, "context": "Datta tests the above positive class algorithms on various datasets taken from UCI repository and conclude that NNPC (discussed in previous section) and NBPC have classification accuracy (both precision and recall values) close to C4.5\u2019s value, although C4.5 decision trees are learned from all classes whereas each of the one-class classifiers is learned using only one class. Wang and Stolfo (2003) use a simple one-class NB method that uses only positive samples, for masquerade detection in a network.", "startOffset": 0, "endOffset": 401}, {"referenceID": 108, "context": "Some of the important non-standard methods used in their study are Gaussian Data Description, kMeans, Principal Component Analysis, and Linear Programming (P\u0229kalska et al., 2003), as well as the SVDD.", "startOffset": 155, "endOffset": 178}, {"referenceID": 45, "context": "They employed a Multi-Layer Model (Ges\u00f9 et al., 2009) as a data processing step and then use their proposed fuzzy one-class classifier for testing on microarray data.", "startOffset": 34, "endOffset": 53}, {"referenceID": 146, "context": "Wang et al. (2004) investigate several one-class classification methods in the context of Human-Robot interaction for image classification into faces and non-faces.", "startOffset": 0, "endOffset": 19}, {"referenceID": 38, "context": "Ercil and Buke (2002) report a different technique to tackle the OCC problem, based on fitting an implicit polynomial surface to the point cloud of features, to model the target class in order to separate it from the outliers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "A fuzzy one-class classifier is proposed by Bosco and Pinello (2009) for identifying patterns of signals embedded in a noisy background.", "startOffset": 44, "endOffset": 69}, {"referenceID": 10, "context": "A fuzzy one-class classifier is proposed by Bosco and Pinello (2009) for identifying patterns of signals embedded in a noisy background. They employed a Multi-Layer Model (Ges\u00f9 et al., 2009) as a data processing step and then use their proposed fuzzy one-class classifier for testing on microarray data. Their results show that integrating these two methods could improve the overall classification results. Juszczak et al. (2009) propose a one-class classifier that is built on the minimum spanning tree of only the target class.", "startOffset": 44, "endOffset": 431}, {"referenceID": 10, "context": "A fuzzy one-class classifier is proposed by Bosco and Pinello (2009) for identifying patterns of signals embedded in a noisy background. They employed a Multi-Layer Model (Ges\u00f9 et al., 2009) as a data processing step and then use their proposed fuzzy one-class classifier for testing on microarray data. Their results show that integrating these two methods could improve the overall classification results. Juszczak et al. (2009) propose a one-class classifier that is built on the minimum spanning tree of only the target class. The method is based on a graph representation of the target class to capture the underlying structure of the data. This method performs distance-based classification as it computes the distance from a test object to its closest edge. They show that their method performs well when the data size is small and has high dimensions. Seg\u00f9\u0131 et al. (2010) suggest that the performance of the method of Juszczak et al.", "startOffset": 44, "endOffset": 880}, {"referenceID": 10, "context": "A fuzzy one-class classifier is proposed by Bosco and Pinello (2009) for identifying patterns of signals embedded in a noisy background. They employed a Multi-Layer Model (Ges\u00f9 et al., 2009) as a data processing step and then use their proposed fuzzy one-class classifier for testing on microarray data. Their results show that integrating these two methods could improve the overall classification results. Juszczak et al. (2009) propose a one-class classifier that is built on the minimum spanning tree of only the target class. The method is based on a graph representation of the target class to capture the underlying structure of the data. This method performs distance-based classification as it computes the distance from a test object to its closest edge. They show that their method performs well when the data size is small and has high dimensions. Seg\u00f9\u0131 et al. (2010) suggest that the performance of the method of Juszczak et al. is reduced in the presence of outliers in the target class. To circumvent this problem, they present two bagging ensemble approaches that are aimed to reduce the influence of outliers in the target training data and show improved performance on both real and artificially contaminated data. Hempstalk et al. (2008) present a one-class classification method that combines a density estimator to draw a reference distribution", "startOffset": 44, "endOffset": 1257}, {"referenceID": 129, "context": "They use the reference distribution to generate artificial data for the second class and decompose this problem as a standard two-class learning problem and show that their results on UCI dataset and typist dataset are comparable with standard OSVM (Sch\u00f6lkopf et al., 2000), with the advantage of not having to specify a target rejection rate at the time of training.", "startOffset": 249, "endOffset": 273}, {"referenceID": 126, "context": "They use the reference distribution to generate artificial data for the second class and decompose this problem as a standard two-class learning problem and show that their results on UCI dataset and typist dataset are comparable with standard OSVM (Sch\u00f6lkopf et al., 2000), with the advantage of not having to specify a target rejection rate at the time of training. Silva and Willett (2009) present a high dimension anomaly detection method that uses limited amount of unlabeled data.", "startOffset": 250, "endOffset": 393}, {"referenceID": 104, "context": "It is a common practice to build text classifiers using positive and unlabeled examples (in semi-supervised setting) as collecting unlabeled samples is relatively easy and fast in many text or Web page domains (Nigam et al., 2000; Liu et al., 2002).", "startOffset": 210, "endOffset": 248}, {"referenceID": 82, "context": "It is a common practice to build text classifiers using positive and unlabeled examples (in semi-supervised setting) as collecting unlabeled samples is relatively easy and fast in many text or Web page domains (Nigam et al., 2000; Liu et al., 2002).", "startOffset": 210, "endOffset": 248}, {"referenceID": 55, "context": "However, the core problem remains, that it is difficult or impossible to compile a set of negative samples that provides a comprehensive characterization of everything that is \u2018not\u2019 the target concept, as is assumed by a conventional binary classifier. It is a common practice to build text classifiers using positive and unlabeled examples (in semi-supervised setting) as collecting unlabeled samples is relatively easy and fast in many text or Web page domains (Nigam et al., 2000; Liu et al., 2002). In this section, we will discuss some of the algorithms that exploit this methodology with application to text classification. The ability to build classifiers without negative training data is useful in a scenario if one needs to extract positive documents from many text collections or sources. Nigam et al. (2000) show that accuracy of text classifiers can be improved by adding small amount of labelled training data to a large available pool of unlabelled data.", "startOffset": 0, "endOffset": 820}, {"referenceID": 55, "context": "However, the core problem remains, that it is difficult or impossible to compile a set of negative samples that provides a comprehensive characterization of everything that is \u2018not\u2019 the target concept, as is assumed by a conventional binary classifier. It is a common practice to build text classifiers using positive and unlabeled examples (in semi-supervised setting) as collecting unlabeled samples is relatively easy and fast in many text or Web page domains (Nigam et al., 2000; Liu et al., 2002). In this section, we will discuss some of the algorithms that exploit this methodology with application to text classification. The ability to build classifiers without negative training data is useful in a scenario if one needs to extract positive documents from many text collections or sources. Nigam et al. (2000) show that accuracy of text classifiers can be improved by adding small amount of labelled training data to a large available pool of unlabelled data. They introduce an algorithm based on EM and NB. The central idea of their algorithm is to train the NB classifier based on the labelled documents and then probabilistically label the unlabeled documents and repeat this process till it converges. To improve the performance of the algorithm, they propose two variants that give weights to modulate the amount of unlabeled data, and use mixture components per class. They show that using this type of methodology can result in reduction of error by factor of up to 30%. Liu et al. (2003) study the problem of learning from positive and unlabeled data and suggest that many algorithms that build text classifiers are based on two steps:", "startOffset": 0, "endOffset": 1506}, {"referenceID": 82, "context": "In this step, Spy-EM (Liu et al., 2002) uses a Spy technique, PEBL (Yu et al.", "startOffset": 21, "endOffset": 39}, {"referenceID": 173, "context": ", 2002) uses a Spy technique, PEBL (Yu et al., 2004) uses a technique called 1-DNF (Yu et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 171, "context": ", 2004) uses a technique called 1-DNF (Yu et al., 2002), and Roc-SVM (Liu et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 83, "context": ", 2002), and Roc-SVM (Liu et al., 2003) uses the Rocchio algorithm (Rocchio, 1971).", "startOffset": 21, "endOffset": 39}, {"referenceID": 118, "context": ", 2003) uses the Rocchio algorithm (Rocchio, 1971).", "startOffset": 35, "endOffset": 50}, {"referenceID": 175, "context": "Readers are advised to refer to survey paper by Zhang and Zuo (2008) Readers are advised to refer to survey paper on semi-supervised learning by Zhu (2005)", "startOffset": 48, "endOffset": 69}, {"referenceID": 175, "context": "Readers are advised to refer to survey paper by Zhang and Zuo (2008) Readers are advised to refer to survey paper on semi-supervised learning by Zhu (2005)", "startOffset": 48, "endOffset": 156}, {"referenceID": 166, "context": "It was shown theoretically by Yu et al. (2002) that if the sample size is large enough, maximizing the number of unlabeled examples classified as negative while constraining the positive examples to be correctly classified will give a good classifier.", "startOffset": 30, "endOffset": 47}, {"referenceID": 82, "context": "Liu et al. (2003) introduce two new methods, one for Step 1 (i.", "startOffset": 0, "endOffset": 18}, {"referenceID": 169, "context": "(2003) explore SVMC (Yu, 2003) (for detail on this technique refer to Section 4.", "startOffset": 20, "endOffset": 30}, {"referenceID": 110, "context": "Peng et al. (2006) present a text classifier from positive and unlabeled documents based on Genetic Algorithms (GA) by adopting a two stage strategy (as discussed above).", "startOffset": 0, "endOffset": 19}, {"referenceID": 105, "context": "Source: Onoda et al. (2005).", "startOffset": 8, "endOffset": 28}, {"referenceID": 138, "context": "Their first approach is based on weighted low rank approximation (Srebro and Jaakkola, 2003) that works on the idea of providing different weights to error terms of both positive and negative examples in the objective function.", "startOffset": 65, "endOffset": 92}, {"referenceID": 104, "context": "To solve this problem, Onoda et al. propose a feedback method using information from non-relevant documents only, called non-relevance feedback document retrieval. The design of non-relevance feedback document retrieval is based on OSVM (Sch\u00f6lkopf et al., 1999b). Their proposed method selects documents that are discriminated as not non-relevant and that are near the discriminant hyper-plane between non-relevant document and relevant documents. They compare the proposed approach with conventional relevance feedback methods and vector space model without feedback and show that it consistently gives better performance as compared to other methods. Pan et al. (2008) extend the concept of classifying positive examples with unlabeled samples in the Collaborative Filtering (CF) application.", "startOffset": 23, "endOffset": 671}, {"referenceID": 55, "context": "However, due to ambiguous interpretations, limited knowledge or lack of interest of users, the collection of valid negative data may be hampered. Sometime negative and unlabeled positive data are severely mixed up and it becomes difficult to discern them. Manually labelling negative data is not only intractable considering the size of the web but also will be poorly sampled. Traditional CF algorithms either label negative data, or assume missing data are negative. Both of these approaches have an inherent problem of being expensive and biased to the recommendation results. Pan et al. (2008) propose two approaches to one-class CF to handle the negative sparse data to balance the extent to which to treat missing values as negative examples.", "startOffset": 0, "endOffset": 598}, {"referenceID": 9, "context": "However, learning in this scenario may still be possible using the co-training framework (Blum and Mitchell, 1998) that looks for two feature views over the data.", "startOffset": 89, "endOffset": 114}, {"referenceID": 28, "context": "Denis et al. (2003) consider situations where only a small set of positive data is available together with unlabeled data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "However, learning in this scenario may still be possible using the co-training framework (Blum and Mitchell, 1998) that looks for two feature views over the data. They propose a Positive N\u00e4\u0131ve Co-Training algorithm, PNCT , that takes a small pool of positive documents as its seed. PNCT first incrementally builds NB classifiers from positive and unlabeled documents over each of the two views by using PNB. Along the co-training steps, self-labelled positive examples and self-labelled negative examples are added to the training sets. A base algorithm is also proposed which is a variant of PNB that is able to use these self-labelled examples. The experiments on the WebKB dataset show that co-training algorithms lead to significant improvement of classifiers, even when the initial seed is only composed of positive documents. Calvo et al. (2007) extend the PNB classification idea to build more complex Bayesian classifiers in the absence of negative samples when only positive and unlabeled data are present.", "startOffset": 90, "endOffset": 852}, {"referenceID": 9, "context": "However, learning in this scenario may still be possible using the co-training framework (Blum and Mitchell, 1998) that looks for two feature views over the data. They propose a Positive N\u00e4\u0131ve Co-Training algorithm, PNCT , that takes a small pool of positive documents as its seed. PNCT first incrementally builds NB classifiers from positive and unlabeled documents over each of the two views by using PNB. Along the co-training steps, self-labelled positive examples and self-labelled negative examples are added to the training sets. A base algorithm is also proposed which is a variant of PNB that is able to use these self-labelled examples. The experiments on the WebKB dataset show that co-training algorithms lead to significant improvement of classifiers, even when the initial seed is only composed of positive documents. Calvo et al. (2007) extend the PNB classification idea to build more complex Bayesian classifiers in the absence of negative samples when only positive and unlabeled data are present. A positive tree augmented NB (PTAN) in a positive-unlabeled scenario is proposed along with the use of a Beta distribution to model the apriori probability of the positive class, and it is applied to PNB and PTAN . The experiments suggest that when the predicting attributes are not conditionally independent, PTAN performs better than PNB. The proposed Bayesian approach to estimating apriori probability of the positive class also improves the performance of PNB and PTAN . He et al. (2010) improves the PNB by selecting the value of the prior probability of the positive class on the validation set using a performance measure that can be estimated from positive and unlabeled examples.", "startOffset": 90, "endOffset": 1509}, {"referenceID": 146, "context": "The method outperforms the NN -d (Tax and Duin, 2000) and OCC method by Hempstalk et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 83, "context": "They test their method on theapplication of identifying protein records and show it performs better in comparison to the standard biased SVM method (Liu et al., 2003).", "startOffset": 148, "endOffset": 166}, {"referenceID": 25, "context": "Learning from positive and unlabeled data has been studied in other domains as well apart from text classification such as facial expression recognition (Cohen et al., 2003), gene regulation networks (Cerulo et al.", "startOffset": 153, "endOffset": 173}, {"referenceID": 17, "context": ", 2003), gene regulation networks (Cerulo et al., 2010) etc.", "startOffset": 34, "endOffset": 55}, {"referenceID": 49, "context": "The method outperforms the NN -d (Tax and Duin, 2000) and OCC method by Hempstalk et al. (2008). Elkan and Noto (2008) show that if a classifier is trained using labelled and unlabeled data then its predicted probabilities differ from true probabilities only by a constant factor.", "startOffset": 72, "endOffset": 96}, {"referenceID": 34, "context": "Elkan and Noto (2008) show that if a classifier is trained using labelled and unlabeled data then its predicted probabilities differ from true probabilities only by a constant factor.", "startOffset": 0, "endOffset": 22}, {"referenceID": 34, "context": "Elkan and Noto (2008) show that if a classifier is trained using labelled and unlabeled data then its predicted probabilities differ from true probabilities only by a constant factor. They test their method on theapplication of identifying protein records and show it performs better in comparison to the standard biased SVM method (Liu et al., 2003). Zhang et al. (2008) propose a one-class classification method for the classification of text streams with concept drift.", "startOffset": 0, "endOffset": 372}, {"referenceID": 8, "context": "Blanchard et al. (2010) present a different outlook on learning with positive and unlabeled data that develops general solution to this problem by a surrogate problem related to NeymanPearson classification, which is a binary classification problem subject to a constraint on falsepositive rate while minimizing the false-negative rate.", "startOffset": 0, "endOffset": 24}, {"referenceID": 143, "context": "Some of these areas are: Handwriting Detection (Tax and Duin, 2001b; Tax, 2001; Sch\u00f6lkopf et al., 2000; Hempstalk et al., 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al.", "startOffset": 47, "endOffset": 127}, {"referenceID": 129, "context": "Some of these areas are: Handwriting Detection (Tax and Duin, 2001b; Tax, 2001; Sch\u00f6lkopf et al., 2000; Hempstalk et al., 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al.", "startOffset": 47, "endOffset": 127}, {"referenceID": 54, "context": "Some of these areas are: Handwriting Detection (Tax and Duin, 2001b; Tax, 2001; Sch\u00f6lkopf et al., 2000; Hempstalk et al., 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al.", "startOffset": 47, "endOffset": 127}, {"referenceID": 91, "context": ", 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al.", "startOffset": 31, "endOffset": 58}, {"referenceID": 60, "context": ", 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al., 2008); Image Database Retrieval (Tax and Duin, 2001b; Chen et al.", "startOffset": 89, "endOffset": 135}, {"referenceID": 160, "context": ", 2008); Information Retrieval (Manevitz and Yousef, 2001); Missing Data/Data Correction (Juszczak and Duin, 2004; Xiaomu et al., 2008); Image Database Retrieval (Tax and Duin, 2001b; Chen et al.", "startOffset": 89, "endOffset": 135}, {"referenceID": 21, "context": ", 2008); Image Database Retrieval (Tax and Duin, 2001b; Chen et al., 2001; Gondra et al., 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al.", "startOffset": 34, "endOffset": 106}, {"referenceID": 48, "context": ", 2008); Image Database Retrieval (Tax and Duin, 2001b; Chen et al., 2001; Gondra et al., 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al.", "startOffset": 34, "endOffset": 106}, {"referenceID": 131, "context": ", 2008); Image Database Retrieval (Tax and Duin, 2001b; Chen et al., 2001; Gondra et al., 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al.", "startOffset": 34, "endOffset": 106}, {"referenceID": 157, "context": ", 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al., 2004; Zeng et al., 2006; Bicego et al., 2005), Remote Sensing (Li et al.", "startOffset": 57, "endOffset": 116}, {"referenceID": 174, "context": ", 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al., 2004; Zeng et al., 2006; Bicego et al., 2005), Remote Sensing (Li et al.", "startOffset": 57, "endOffset": 116}, {"referenceID": 6, "context": ", 2004; Seo, 2007); Face/Object Recognition Applications (Wang et al., 2004; Zeng et al., 2006; Bicego et al., 2005), Remote Sensing (Li et al.", "startOffset": 57, "endOffset": 116}, {"referenceID": 79, "context": ", 2005), Remote Sensing (Li et al., 2011); Stream Mining (Zhang et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 84, "context": ", 2011); Stream Mining (Zhang et al., 2008, 2010; Liu et al., 2011); Chemometrics and Spectroscopy (Kittiwachana et al.", "startOffset": 23, "endOffset": 67}, {"referenceID": 68, "context": ", 2011); Chemometrics and Spectroscopy (Kittiwachana et al., 2010; Glavin and Madden, 2009; Khan, 2010; Xu and G. Brereton, 2007; Hao et al., 2010); Biometrics (Bergamini et al.", "startOffset": 39, "endOffset": 147}, {"referenceID": 47, "context": ", 2011); Chemometrics and Spectroscopy (Kittiwachana et al., 2010; Glavin and Madden, 2009; Khan, 2010; Xu and G. Brereton, 2007; Hao et al., 2010); Biometrics (Bergamini et al.", "startOffset": 39, "endOffset": 147}, {"referenceID": 64, "context": ", 2011); Chemometrics and Spectroscopy (Kittiwachana et al., 2010; Glavin and Madden, 2009; Khan, 2010; Xu and G. Brereton, 2007; Hao et al., 2010); Biometrics (Bergamini et al.", "startOffset": 39, "endOffset": 147}, {"referenceID": 49, "context": ", 2011); Chemometrics and Spectroscopy (Kittiwachana et al., 2010; Glavin and Madden, 2009; Khan, 2010; Xu and G. Brereton, 2007; Hao et al., 2010); Biometrics (Bergamini et al.", "startOffset": 39, "endOffset": 147}, {"referenceID": 122, "context": ", 2012b); Time Series Analysis (Sachs et al., 2006; Nguyen et al., 2011); Disease Detection (Cohen et al.", "startOffset": 31, "endOffset": 72}, {"referenceID": 103, "context": ", 2012b); Time Series Analysis (Sachs et al., 2006; Nguyen et al., 2011); Disease Detection (Cohen et al.", "startOffset": 31, "endOffset": 72}, {"referenceID": 24, "context": ", 2011); Disease Detection (Cohen et al., 2004; Zhang et al., 2011); Medical Analysis (Gardner et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 177, "context": ", 2011); Disease Detection (Cohen et al., 2004; Zhang et al., 2011); Medical Analysis (Gardner et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 42, "context": ", 2011); Medical Analysis (Gardner et al., 2006; Zhou et al., 2005); Bioinformatics (Spinosa and Ferreira de Carvalho, 2004; Ferreira de Carvalho, A.", "startOffset": 26, "endOffset": 67}, {"referenceID": 181, "context": ", 2011); Medical Analysis (Gardner et al., 2006; Zhou et al., 2005); Bioinformatics (Spinosa and Ferreira de Carvalho, 2004; Ferreira de Carvalho, A.", "startOffset": 26, "endOffset": 67}, {"referenceID": 0, "context": ", 2005); Bioinformatics (Spinosa and Ferreira de Carvalho, 2004; Ferreira de Carvalho, A. C. P. L., 2005; Alashwal et al., 2006; Wang et al., 2006; Yousef et al., 2008); Steganalysis (Lyu and Farid, 2004; Rodriguez et al.", "startOffset": 24, "endOffset": 168}, {"referenceID": 155, "context": ", 2005); Bioinformatics (Spinosa and Ferreira de Carvalho, 2004; Ferreira de Carvalho, A. C. P. L., 2005; Alashwal et al., 2006; Wang et al., 2006; Yousef et al., 2008); Steganalysis (Lyu and Farid, 2004; Rodriguez et al.", "startOffset": 24, "endOffset": 168}, {"referenceID": 167, "context": ", 2005); Bioinformatics (Spinosa and Ferreira de Carvalho, 2004; Ferreira de Carvalho, A. C. P. L., 2005; Alashwal et al., 2006; Wang et al., 2006; Yousef et al., 2008); Steganalysis (Lyu and Farid, 2004; Rodriguez et al.", "startOffset": 24, "endOffset": 168}, {"referenceID": 87, "context": ", 2008); Steganalysis (Lyu and Farid, 2004; Rodriguez et al., 2007); Spam Detection (Sun et al.", "startOffset": 22, "endOffset": 67}, {"referenceID": 119, "context": ", 2008); Steganalysis (Lyu and Farid, 2004; Rodriguez et al., 2007); Spam Detection (Sun et al.", "startOffset": 22, "endOffset": 67}, {"referenceID": 139, "context": ", 2007); Spam Detection (Sun et al., 2005; Wu et al., 2005; Schneider, 2004); Audio Surveillance, Sound & Speaker Classification (Brew et al.", "startOffset": 24, "endOffset": 76}, {"referenceID": 159, "context": ", 2007); Spam Detection (Sun et al., 2005; Wu et al., 2005; Schneider, 2004); Audio Surveillance, Sound & Speaker Classification (Brew et al.", "startOffset": 24, "endOffset": 76}, {"referenceID": 125, "context": ", 2007); Spam Detection (Sun et al., 2005; Wu et al., 2005; Schneider, 2004); Audio Surveillance, Sound & Speaker Classification (Brew et al.", "startOffset": 24, "endOffset": 76}, {"referenceID": 11, "context": ", 2005; Schneider, 2004); Audio Surveillance, Sound & Speaker Classification (Brew et al., 2007; Rabaoui et al., 2007, 2008); Ship Detection (Tang and Yang, 2005); Vehicle Recognition (Munroe and Madden, 2005); Collision Detection (Quinlan et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 140, "context": ", 2007, 2008); Ship Detection (Tang and Yang, 2005); Vehicle Recognition (Munroe and Madden, 2005); Collision Detection (Quinlan et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 98, "context": ", 2007, 2008); Ship Detection (Tang and Yang, 2005); Vehicle Recognition (Munroe and Madden, 2005); Collision Detection (Quinlan et al.", "startOffset": 73, "endOffset": 98}, {"referenceID": 112, "context": ", 2007, 2008); Ship Detection (Tang and Yang, 2005); Vehicle Recognition (Munroe and Madden, 2005); Collision Detection (Quinlan et al., 2003); Anomaly Detection (Li et al.", "startOffset": 120, "endOffset": 142}, {"referenceID": 78, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 151, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 178, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 111, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 166, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 101, "context": ", 2003); Anomaly Detection (Li et al., 2003; Tran et al., 2004; Zhang et al., 2007; Perdisci et al., 2006; Yilmazel et al., 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al.", "startOffset": 27, "endOffset": 143}, {"referenceID": 46, "context": ", 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al., 2005; Evangelista et al., 2005; Luo et al., 2007); Credit Scoring (Kennedy et al.", "startOffset": 43, "endOffset": 110}, {"referenceID": 40, "context": ", 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al., 2005; Evangelista et al., 2005; Luo et al., 2007); Credit Scoring (Kennedy et al.", "startOffset": 43, "endOffset": 110}, {"referenceID": 86, "context": ", 2005; Nguyen, 2002); Intrusion Detection (Giacinto et al., 2005; Evangelista et al., 2005; Luo et al., 2007); Credit Scoring (Kennedy et al.", "startOffset": 43, "endOffset": 110}, {"referenceID": 63, "context": ", 2007); Credit Scoring (Kennedy et al., 2009); Yeast Regulation Prediction (Kowalczyk and Raskutti, 2002); Customer Churn Detection (Zhao et al.", "startOffset": 24, "endOffset": 46}, {"referenceID": 70, "context": ", 2009); Yeast Regulation Prediction (Kowalczyk and Raskutti, 2002); Customer Churn Detection (Zhao et al.", "startOffset": 37, "endOffset": 67}, {"referenceID": 180, "context": ", 2009); Yeast Regulation Prediction (Kowalczyk and Raskutti, 2002); Customer Churn Detection (Zhao et al., 2005); Relevant Sentence Extraction (Kruengkrai and Jaruskulchai, 2003); Machine Vibration Analysis (Tax et al.", "startOffset": 94, "endOffset": 113}, {"referenceID": 71, "context": ", 2005); Relevant Sentence Extraction (Kruengkrai and Jaruskulchai, 2003); Machine Vibration Analysis (Tax et al.", "startOffset": 38, "endOffset": 73}, {"referenceID": 142, "context": ", 2005); Relevant Sentence Extraction (Kruengkrai and Jaruskulchai, 2003); Machine Vibration Analysis (Tax et al., 1999); Machine Fault Detection (Ercil and Buke, 2002; Tax and Duin, 2004; Shin et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 39, "context": ", 1999); Machine Fault Detection (Ercil and Buke, 2002; Tax and Duin, 2004; Shin et al., 2005; Sarmiento et al., 2005); and Recommendation Tasks (Yasutoshi, 2006).", "startOffset": 33, "endOffset": 118}, {"referenceID": 149, "context": ", 1999); Machine Fault Detection (Ercil and Buke, 2002; Tax and Duin, 2004; Shin et al., 2005; Sarmiento et al., 2005); and Recommendation Tasks (Yasutoshi, 2006).", "startOffset": 33, "endOffset": 118}, {"referenceID": 134, "context": ", 1999); Machine Fault Detection (Ercil and Buke, 2002; Tax and Duin, 2004; Shin et al., 2005; Sarmiento et al., 2005); and Recommendation Tasks (Yasutoshi, 2006).", "startOffset": 33, "endOffset": 118}, {"referenceID": 123, "context": ", 1999); Machine Fault Detection (Ercil and Buke, 2002; Tax and Duin, 2004; Shin et al., 2005; Sarmiento et al., 2005); and Recommendation Tasks (Yasutoshi, 2006).", "startOffset": 33, "endOffset": 118}, {"referenceID": 165, "context": ", 2005); and Recommendation Tasks (Yasutoshi, 2006).", "startOffset": 34, "endOffset": 51}, {"referenceID": 136, "context": "Compression neural networks for one-class classification have been used to detect mineral deposits (Skabar, 2003) and for fMRI Analysis (Hardoon and Manevitz, 2005b,a).", "startOffset": 99, "endOffset": 113}, {"referenceID": 99, "context": "One-class Fuzzy ART networks have been explored to classify cancerous cells (Murshed et al., 1996).", "startOffset": 76, "endOffset": 98}, {"referenceID": 57, "context": "We suggest it would be fruitful to investigate some more innovative forms of kernels, for example Genetic Kernels (Howley and Madden, 2006) or domain specific kernels, such as Weighted Linear Spectral Kernel (Howley, 2007), that have shown greater potential in standard SVM classification.", "startOffset": 114, "endOffset": 139}, {"referenceID": 56, "context": "We suggest it would be fruitful to investigate some more innovative forms of kernels, for example Genetic Kernels (Howley and Madden, 2006) or domain specific kernels, such as Weighted Linear Spectral Kernel (Howley, 2007), that have shown greater potential in standard SVM classification.", "startOffset": 208, "endOffset": 222}, {"referenceID": 120, "context": ", 2012a) address the issue of handling missing data in target class by using Bayesian Multiple Imputations (Rubin, 1987) and EM and propose several variants of one-class classifier ensemble that can perform better than traditional method of mean imputation.", "startOffset": 107, "endOffset": 120}, {"referenceID": 154, "context": "There are some studies in this direction (Villalba and Cunningham, 2007); however, a lack of advanced methods and techniques to handle high dimensional positive class data can still be a bottleneck in learning one-class classifiers.", "startOffset": 41, "endOffset": 72}, {"referenceID": 14, "context": "A possible direction in this attempt is to explore mixtures of beta distribution for inferring prior class probability (Calvo, 2008).", "startOffset": 119, "endOffset": 132}, {"referenceID": 81, "context": "However, traditional costsensitive learning methods (Ling and Sheng, 2010) may not fit here because neither the prior probability of the outlier class nor the associated cost of errors are known.", "startOffset": 52, "endOffset": 74}, {"referenceID": 86, "context": "In the papers we reviewed, only one research paper discusses the cost-sensitive aspect of OCC (Luo et al., 2007) and it shows that this area of research is largely unexplored.", "startOffset": 94, "endOffset": 112}, {"referenceID": 20, "context": "We believe that approaches based on careful application of Preference Elicitation techniques (Chen and Pu, 2004) can be useful to deduce cost of errors.", "startOffset": 93, "endOffset": 112}], "year": 2013, "abstractText": "One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research.", "creator": "LaTeX with hyperref package"}}}