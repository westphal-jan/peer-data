{"id": "1312.0412", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "Practical Collapsed Stochastic Variational Inference for the HDP", "abstract": "Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance.", "histories": [["v1", "Mon, 2 Dec 2013 10:58:01 GMT  (125kb,D)", "http://arxiv.org/abs/1312.0412v1", "NIPS Workshop; Topic Models: Computation, Application, and Evaluation"]], "COMMENTS": "NIPS Workshop; Topic Models: Computation, Application, and Evaluation", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["arnim bleier"], "accepted": false, "id": "1312.0412"}, "pdf": {"name": "1312.0412.pdf", "metadata": {"source": "CRF", "title": "Practical Collapsed Stochastic Variational Inference for the HDP", "authors": ["Arnim Bleier"], "emails": ["arnim.bleier@gesis.org"], "sections": [{"heading": "1 Background", "text": "We begin by considering a model in which each document d is a mixture of K-discrete distributions of topics spanning a vocabulary of V-terms. Let's let zdi-discretely specify the topic of the ith-word wdi-discrete distributions of topics in the document d-discrete (1,.., D) and set the Dirichlet priority parameters to the parameters \u03b8d, \u03c6k. We havezdi-discretely specify the top level of distribution over the topics and \u03b1 and \u03b2 are concentration parameters. While the dimensionality of K is fixed in latent dirichlet allocation (LDA), we want the model to break up the number of required topics. Consequently, we follow the assumptions of the Hierarchical Dirichlet Process (HDP)."}, {"heading": "2 Practical Collapsed Variational Inference", "text": "In this section, we review the variable Bayes conclusion (PCVB0) proposed by Sato and al. [3], which will later form the basis of our stochastic conclusion. HDP's collapsed representation is achieved by marginalizing information of different order. If only information of different order is used, the variable distributions zdi are updated about possible topic assignments for each word in Byq (zdi = k). \u2212 kwdi is the number of times the term wdi has been assigned to the topic k, in both cases without the current word di. + V \u03b2. (2) Where n \u00ac didk is the number of times a word in document d has been assigned to the topic k, and n \u00ac di kwdi the number of times the term wdi has been assigned to the topic k, in both cases without the current word di."}, {"heading": "3 Proposed Updates", "text": "One of the main drawbacks of the collapsed variation inference for the HDP is its high memory requirements for q q terms. We propose to work around this. Following the ideas behind the stochastic collapsed variants Bayesian inference (SCVB0), provided by Foulds et al. [4] we potentially allow data to arrive in a stream but maintain the simplicity of the original PCVB0 scheme. Suppose we have a guess of the current PCVB0 statistics. \u2212 Next, we draw the ith word wdi of document d and calculate its corresponding zdi via equation (2). The expected number of times k appears in document ndk, relative to the current word, is ndq (zdi = k)."}, {"heading": "4 Evaluation", "text": "In this section we describe an initial experimental analysis of the proposed PCSVB0 conclusion. We examined the predictive performance of the algorithm using data from The Associated Press (TREC-1). The dataset contains 398k tokens over 2250 documents with a vocabulary size of 10932 unique terms. For evaluation, we compared the perplexity with the number of documents seen for PCSVB0, SCVB0 and PCVB0. We trained the model on 80% of the documents. All held documents were split; 70% of the tokens in each held document were used to estimate the document parameters, the remaining 30% were used to calculate the perplexity. We used the increment plan \u0421t = s (\u0432 + t) 0.9."}, {"heading": "5 Discussion", "text": "Our algorithm is based on the application of a practical approximation of the shortened stick-breaking process to a collapsed stochastic inference scheme, and is easier to implement than other non-collapsed online variation inference algorithms for the HDP [5]. Initial small experiments show promising improvements in the predictive power of PCSVB0 over existing algorithms, both in terms of convergence rate and observed optimum. Guidelines for future work are the use of so-called \"clumping\" to perform the update for each individual term per document and then scale the update by the number of copies. Another direction is the use of mini-batches. Such optimizations would improve the wall clock time per iteration and allow a fair comparison with other online variation inference algorithms for the HDP."}], "references": [{"title": "Hierarchical Dirichlet Processes", "author": ["Y.W. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Collapsed variational inference for HDP", "author": ["Y.W. Teh", "K. Kurihara", "M. Welling"], "venue": "Advances in Neural Information Processing Systems. pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Practical Collapsed Variational Bayes Inference for Hierarchical Dirichlet Process", "author": ["I. Sato", "K. Kurihara", "H. Nakagawa"], "venue": "Proceedings of the 18 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation", "author": ["J. Foulds", "L. Boyles", "C. Dubois", "P. Smyth", "M. Welling"], "venue": "Proceedings of the 19 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Online Variational Inference for the Hierarchical Dirichlet Processes", "author": ["C. Wang", "J. Paisley", "D. Blei"], "venue": "Proceedings of the 14 International Conference on Artificial Intelligence and Statistic", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Our prior \u03c0 is constructed by a truncated sick-breaking process [2],", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "[3] which later will be the fundament of our stochastic inference.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Teh\u2019s [2] original variational Bayes inference required maintaining variance counts for \u03b1\u03c0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "[3] showed the usefulness of a lower-bound approximation for the number of tables in the Dirichlet process Chinese Restaurant representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] we potentially allow for data to arrive in a stream, but maintain the simplicity of the original PCVB0 schema.", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance. 1 Background We begin by considering a model where each document d is a mixture \u03b8d of K discrete topicdistributions \u03c6k over a vocabulary of V terms. Let zdi \u2208 {1, ..,K} denote the topic of the i word wdi \u2208 {1, .., V } in document d \u2208 {1, .., D} and place Dirichlet priors on the parameters \u03b8d, \u03c6k. We have zdi | \u03b8d \u223c Discrete(\u03b8d) , \u03b8d \u223c Dirichlet(\u03b1\u03c0) , wdi | zdi, {\u03c6k} \u223c Discrete(\u03c6zdi) , \u03c6k \u223c Dirichlet(\u03b2) , where \u03c0 is the top-level distribution over topics, and \u03b1 and \u03b2 are concentration parameters. While the dimensionality of K is fixed in latent Dirichlet allocation (LDA), we want the model to determine the number of topics needed. Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior. Our prior \u03c0 is constructed by a truncated sick-breaking process [2],", "creator": "LaTeX with hyperref package"}}}