{"id": "1312.0482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2013", "title": "Learning Semantic Representations for the Phrase Translation Model", "abstract": "This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results. Experimental evaluation has been performed on two Europarl translation tasks, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation sys-tem, leading to a gain of 0.7-1.0 BLEU points.", "histories": [["v1", "Thu, 28 Nov 2013 04:58:59 GMT  (521kb)", "http://arxiv.org/abs/1312.0482v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jianfeng gao", "xiaodong he", "wen-tau yih", "li deng"], "accepted": false, "id": "1312.0482"}, "pdf": {"name": "1312.0482.pdf", "metadata": {"source": "CRF", "title": "Learning Semantic Representations for the Phrase Translation Model", "authors": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Related Work", "text": "Latent Semantic Analysis (LSA) [5], originally conceived for information retrieval (IR), is probably the earliest continuous semantic model. In contrast to LSA, which is a linear projection model, generative topic models such as Probabilistic LSA [13] and Latent Dirichlet Allocation (LDA) [4] give a clear probabilistic interpretation of semantic representation. In contrast, recent work on models of continuous spatial language, such as the feed-forward neural network language model (NLM) [3] and the recursive neural network language model (RNLM) [19, 2] offer a different type of latent semantic representation. Since these latent semantic models are developed for monolingual settings, they are not directly applicable to translation."}, {"heading": "3. The Log-Linear Model for SMT", "text": "Phrase-based SMT is based on a log-linear model that requires learning a mapping between inputs and outputs. We receive training samples () for = 1..., in which each source sentence is paired with a reference translation in the target language; a procedure in which a list of the N-best candidates GEN () is created for an input, with GEN in this study being the baseline-phrase-based SMT system, i.e. a reimplementation of the mossing system that does not use the SPTM, and each GEN () being characterized by the BLEU score [10], which is denoted by sBleu."}, {"heading": "4. A Semantic-Based Phrase Translation Model (SPTM)", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "5. Training SPTM", "text": "This section describes the type of loss function that we use with the SPTM and the algorithm to train neural network weights on the basis of the loss function as an optimization target. We define the loss function () as a negative of the N leaderboard based on the expected BLEU designated by xBleu ().As part of the reassessment of SMT outlined in Section 3, xBleu (,) is defined by a training sample (,) asxBleu (,) sBleu (,) GEN (,) (,), where sBleu (,) is the sentence-level BLEU score and (,) is a normalized translation probability from (,) to (,) sBleu (,) (,) (,) (,) (, \",\", \",\", \",\" (, \",\", \",\", \",\", \",\", \"(,\"), \""}, {"heading": "5.1 Computing the Gradient", "text": "Since the loss does not explicitly depend on (,), we use the chain rule to differentiate: () () ()"}, {"heading": "5.1.2 Computing \ud835\udf39(\ud835\udc87,\ud835\udc86)", "text": "To simplify the notation, we write our loss function of (5) and (6) over a training sample () () = \u2212 xBleu () = \u2212 G () Z () () (11) () = sBleu (,) (,) (,) Z () (,) (8) and (11) together we have (,) = xBleu () () (,)"}, {"heading": "5.2 The Training Algorithm", "text": "In our experiments, we train the SPTM parameters using the L-BFGS optimizer described in [1] together with the loss function described in (5). The gradient is calculated as described in Section 5.1. Although the loss function is not convex, we found that the L-BFGS iterations over the entire training data (batch mode) minimize the loss in practice in a desirable way; for example, it was found that the convergence of the algorithm runs smoothly."}, {"heading": "6. Experiments", "text": "We conducted our experiments with two Europarl translation tasks, English-French (EN-FR) and German-English (DE-EN). Data sets are published for the joint task in the NAACL 2006 Workshop on Statistical Machine Translation (WMT06) [16]. For EN-FR, the training set contains 688K pairs of sentences with an average of 21 words per sentence. The training set contains 2000 sentences from the joint task WMT05 (TEST1) and 2000 sentences from the joint task WMT06 (TEST2). For DE-EN, the training set contains 751K pairs of sentences with an average of 21 words per sentence. The official development set for the joint task contains 2000 sentences. We used 2000 sentences from the joint task WMT05 as TEST1 and the 2000 sentences from the joint task WMT06 as TEST2. We used the Moses system [15] as the basis for the phrase T-System."}, {"heading": "6.1 Results", "text": "Table 2 shows the most important results measured in BLEU, measured against TEST1 and TEST2, with \u03b2 \u03b2 1 being the base system. Lines 2 to 5 are the systems that are improved by integrating different versions of the SPTM. SPTM in row 2 is the model described in sections 4. As shown in Figure 2, the number of nodes in the input layer is the vocabulary size. Both the hidden layer and the base layer have 100 nodes. That is, 1 is a matrix of 100 x and 2 is a 100 x 100 matrix. Table 2 shows that SPTM leads to a significant improvement in the base model across all test sets, with a statistically significant margin of 0.7 to 1.0 BLEU points. We have developed a number of variants of SPTM words as shown in lines 3 to 5 to examine two design decisions we made in the development of the SPTM: (1) whether a linear projection or a multilayered projection should be used (2) and whether we should propose a similarity (2)."}, {"heading": "6.2 Comparing with Previous Latent Semantic Models", "text": "This year, it is that it is able to retaliate, to retaliate, \"he says.\" It is as it is, \"he says.\" It is as it is, \"he says.\" It is as it is, \"he says.\" It is as it is, \"he says.\""}, {"heading": "6.3 Discussion", "text": "Although SGD is advocated for training neural networks to a local minimum due to its simplicity and robustness, we found that batch training based on L-BFGS performs well in our task despite the non-convexity of our loss. Another advantage of batch training is that the course can be efficiently calculated across all training data. As shown in Section 5, calculating simulators (x, x) / \u03b8 requires large matrix multiplications and is expensive for multi-layered neural networks. Equation (7) suggests that simulators (x, x) / (,) and (,) can be calculated separately, so that the calculation cost of the previous term only depends on the number of phrase pairs in the phrase table, but not on the size of the training data. Therefore, the training method described in Section 5 can be used for large amounts of training data without great difficulty."}, {"heading": "7. Conclusions", "text": "The work presented in this paper makes two important contributions. Firstly, we are developing a novel phrase translation model for SMT, in which the translation value of a pair of source-target phrases is represented as the distance between their characteristic vectors in a low-dimensional, continuously evaluated semantic space. Semantic space is derived from the representations generated by means of a multilayered neural network. Secondly, we are presenting a new learning method for directly training weights in the multilayered neural network for the end-to-end BLEU metric. The training method is based on LBFGS. We describe in detail how the gradient is derived in a closed form, as is required for efficient optimization. The objective function, which calculates the shape of the expected BLEU network from Nbest lists, is very different from the usual objective functions used in most existing neural networks as a closed form, as it is required for efficient optimization. Therefore, the objective function, which calculates the shape of the expected BLEU network from Nbest lists, is very different from the usual objective functions used in most of the existing neural networks as a closed form, as it is required for efficient optimization."}], "references": [{"title": "Scalable training of L1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In ICML", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Duharme", "P. Vincent", "C. Janvin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Why generative phrase models underperform surface heuristics", "author": ["J. DeNero", "D. Gillick", "J. Zhang", "D. Klein"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Automatic cross-language retrieval using latent semantic indexing", "author": ["S. Dumais", "T. Letsche", "M. Littman", "T. Landauer"], "venue": "Spring Symposium Series: Cross-Language Text and Speech Retrieval", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Training MRF-based translation models using gradient ascent", "author": ["J. Gao", "X. He"], "venue": "In NAACL-HLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Clickthrough-based latent semantic models for web search", "author": ["J. Gao", "K. Toutanova", "Yih", "W-T"], "venue": "In SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Maximum expected bleu training of phrase and lexicon translation models", "author": ["X. He", "L. Deng"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Discovering Binary Codes for Documents by Learning Deep Generative Models", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Topics in Cognitive Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang", "P-S", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In CIKM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Manual and automatic evaluation of machine translation between European languages", "author": ["P. Koehn", "C. Monz"], "venue": "In Workshop on Statistical Machine Translation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F. Och", "D. Marcu"], "venue": "In HLT-NAACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A phrase-based, joint probability model for statistical machine translation", "author": ["D. Marcu", "W. Wong"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F. Och"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu W-J"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Translingual Document Representations from Discriminative Projections", "author": ["J. Platt", "K. Toutanova", "W. Yih"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Smooth bilingual n-gram translation", "author": ["H. Schwenk", "M.R. Costa-Jussa", "J.A.R. Fonollosa"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Continuous space translation models with neural networks", "author": ["L.H. Son", "A. Allauzen", "F. Yvon"], "venue": "In NAACL-HLT,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Training phrase translation models with leavingone-out", "author": ["J. Wuebker", "A. Mauser", "H. Ney"], "venue": "In ACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Learning discriminative projections for text similarity measures", "author": ["W. Yih", "K. Toutanova", "J. Platt", "C. Meek"], "venue": "In CoNLL", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "The most common method of constructing the phrase table takes a two-phase approach [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "Motivated by recent studies on continuous-space language models (LM) [3, 19], we use a neural network to project a word vector to a feature vector.", "startOffset": 69, "endOffset": 76}, {"referenceID": 17, "context": "Motivated by recent studies on continuous-space language models (LM) [3, 19], we use a neural network to project a word vector to a feature vector.", "startOffset": 69, "endOffset": 76}, {"referenceID": 3, "context": "Latent Semantic Analysis (LSA) [5], originally designed for information retrieval (IR), is arguably the earliest continuous semantic model.", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "Unlike LSA which is a linear projection model, generative topic models, such as Probabilistic LSA [13] and Latent Dirichlet Allocation (LDA) [4] give a clear probabilistic interpretation of the semantic representation.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.", "startOffset": 56, "endOffset": 59}, {"referenceID": 17, "context": ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.", "startOffset": 116, "endOffset": 123}, {"referenceID": 1, "context": ", the feed-forward neural network language model (NNLM) [3] and the recurrent neural network language model (RNNLM) [19, 2], provide a different kind of latent semantic representation.", "startOffset": 116, "endOffset": 123}, {"referenceID": 5, "context": "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.", "startOffset": 83, "endOffset": 97}, {"referenceID": 7, "context": "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.", "startOffset": 83, "endOffset": 97}, {"referenceID": 20, "context": "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.", "startOffset": 83, "endOffset": 97}, {"referenceID": 24, "context": "As a result, variants of such models for crosslingual scenarios have been proposed [7, 9, 22, 26] where documents in different languages are projected into the shared latent concept space.", "startOffset": 83, "endOffset": 97}, {"referenceID": 21, "context": "The only exception we are aware of is the work of continuous space n-gram translation models [23, 24], where the feed-forward NNLM is extended to represent translation probabilities.", "startOffset": 93, "endOffset": 101}, {"referenceID": 22, "context": "The only exception we are aware of is the work of continuous space n-gram translation models [23, 24], where the feed-forward NNLM is extended to represent translation probabilities.", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].", "startOffset": 66, "endOffset": 84}, {"referenceID": 6, "context": "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].", "startOffset": 66, "endOffset": 84}, {"referenceID": 8, "context": "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].", "startOffset": 66, "endOffset": 84}, {"referenceID": 16, "context": "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].", "startOffset": 66, "endOffset": 84}, {"referenceID": 23, "context": "There has been much recent research on improving the phrase table [6, 8, 10, 18, 25].", "startOffset": 66, "endOffset": 84}, {"referenceID": 6, "context": "Among them, [8] is most relevant to the work described in this paper.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": ", a reimplementation of the Moses system [15] that does not use the SPTM, and each E \u2208 GEN(Fi) is labeled by the sentencelevel BLEU score [10], denoted by sBleu(Ei , E), which meaures the quality of E with respect to its reference translation Ei; \uf0b7 A vector of features h \u2208 R that maps each (Fi , E) to a vector of feature values; and \uf0b7 A parameter vector \u03bb \u2208 R, which assigns a real-valued weight to each feature.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": ", a reimplementation of the Moses system [15] that does not use the SPTM, and each E \u2208 GEN(Fi) is labeled by the sentencelevel BLEU score [10], denoted by sBleu(Ei , E), which meaures the quality of E with respect to its reference translation Ei; \uf0b7 A vector of features h \u2208 R that maps each (Fi , E) to a vector of feature values; and \uf0b7 A parameter vector \u03bb \u2208 R, which assigns a real-valued weight to each feature.", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "We fix \u03b8, and optimize \u03bb using MERT [20] to maximize the BLEU score on development data.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "In our experiments we train the parameters of the SPTM \u03b8 using the L-BFGS optimizer described in [1], together with the loss function described in (5).", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "We used the Moses system [15] as our baseline phrase-based SMT system.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "The metric used for evaluation is case insensitive BLEU score [21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": ", the lexical weighting model [17].", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model [17] and the word factored n-gram translation model [24].", "startOffset": 154, "endOffset": 158}, {"referenceID": 22, "context": "This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model [17] and the word factored n-gram translation model [24].", "startOffset": 206, "endOffset": 210}, {"referenceID": 7, "context": ", clickthrough data extracted from search logs) for query-document matching [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "To adopt these models for SMT, we view source-target sentence pairs as clicked query-document pairs, and trained both models using the same methods as in [9] on the parallel bilingual training data described earlier.", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "BTLMPR (Row 7) is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 7, "context": "DPM (Row 8) is the Discriminative Projection Model described in [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 24, "context": "W is trained on parallel training data using a Siamese neural network approach, S2Net [26], as follows.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": ", cross entropy or mean square error [11, 12].", "startOffset": 37, "endOffset": 45}, {"referenceID": 10, "context": ", cross entropy or mean square error [11, 12].", "startOffset": 37, "endOffset": 45}], "year": 2013, "abstractText": "This paper presents a novel semantic-based phrase translation model. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent semantic space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a multi-layer neural network whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results. Experimental evaluation has been performed on two Europarl translation tasks, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation system, leading to a gain of 0.7-1.0 BLEU points.", "creator": "Microsoft\u00ae Word 2013"}}}