{"id": "1312.0512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "Sensing-Aware Kernel SVM", "abstract": "We propose a novel approach for designing kernels for support vector machines (SVMs) when the class label is linked to the observation through a latent state and the likelihood function of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision boundary is a hyperplane under a mapping defined by the likelihood function. Combining this with the maximum margin principle yields kernels for SVMs that leverage knowledge of the sensing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the performance of rather sophisticated state-of-the-art approaches.", "histories": [["v1", "Mon, 2 Dec 2013 16:47:10 GMT  (15kb)", "https://arxiv.org/abs/1312.0512v1", null], ["v2", "Thu, 13 Mar 2014 12:02:10 GMT  (15kb)", "http://arxiv.org/abs/1312.0512v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weicong ding", "prakash ishwar", "venkatesh saligrama", "w clem karl"], "accepted": false, "id": "1312.0512"}, "pdf": {"name": "1312.0512.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dingwc@bu.edu", "pi@bu.edu", "srv@bu.edu", "wckarl@bu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.05 12v2 [cs.LG] 1 3M ar2 01Index Terms - Sensing model, Kernel method, SVM, Bag of Words, Supervised Classification"}, {"heading": "1. INTRODUCTION", "text": "This paper represents a new, demonstrably optimal method for designing nuclei for SVM that explicitly contains information about the structure of the underlying data generation process. We look at the typical classification task, where the observed data x x and the label y follow a common distribution. However, in many real problems, observations x are only indirectly associated with latent variables z through a generative process, where the diagnosis is based on reconstructed cross-sections of the body z and the latent variables have different distributions based on the underlying label y. An example of such a situation is medical tomography, where the acquired data x consists of X-ray projections, but the diagnosis is made on reconstructed cross-sections of the body z to determine the presence or absence of disease y. Another example is the classic \"Bag-of-Word\" (BoW) modeling of paradigms that are widely used."}, {"heading": "2. OPTIMUM SENSING-AWARE KERNEL", "text": "The following technical assumptions about the common distribution fulfilled in many application areas: (Assumption 1: (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (Square Integratability): (): (Integratability:: (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): (): ():: (): (): ()::: ()::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"}, {"heading": "3. KERNEL FOR BAG OF WORDS MODELS", "text": "In the \"Bag of words\" (BoW), or \"Bag of Features\" (BoF), there is a collection of words from a vocabulary of size W. Each document is modeled as it is generated by N i.i.d. (xW) T Therefore, p (x | z) is a multinomial distribution vector (x | z) = (Nx1,.., xW) W = 1zxwwFrom this it can be shown that the sensing kernel for two documents xi, xj defined in 5 is the following form, K (xi, xj) = W (xj) = 1zw."}, {"heading": "4. DOCUMENT CLASSIFICATION", "text": "In this context, the terms \"word,\" \"vocabulary\" and \"document\" have their natural meaning. We selected the 20 Newsgourps dataset, which contains 18,774 news items from 20 newsgroups (classes), to test performance; the average number of words per document in this dataset is 117. Following common practice [9], we have removed a standard list of keywords from the vocabulary."}, {"heading": "4.1. Binary Classification", "text": "This is a difficult task due to the similarity of the contents in these two groups [3, 10]. The training set contains 856 documents and the test set contains 569 documents. The average number of words per document is 132. We used LIBSVM [11] to train our kernel SVMs. We report only for the sensing-1 and sensing-2 approximations of our sensing-aware kernel, as in this dataset the number of words Ni varies significantly between the documents. Table 1 compares the correct classification rate (CCR%) of the proposed sensing-aware cores with two model-based kernel, PPK and Diff, and the baseline RBF kernel. Table 1 also shows results for a discriminatory latent-dirichlet allocation method (referred to as DiscLDA)."}, {"heading": "4.2. Multi-class Classification", "text": "Next, we examined the multi-class classification in the 20 newsgroup dataset with all 20 classes. We chose a widely used training / test split, where the training set consists of 11,269 documents, and the test set consists of 7,505 documents. We used the \"one-versus-all\" strategy according to [10] to perform the multi-class classification using binary classifiers. We followed the same settings as in the binary case. For sensing 1 and 2 cores, we used n = N = 150. Table 2 shows the CCRs for the proposed sensing-aware cores, the RBF baseline, the two model-based cores PPK and Diff, and the G-MedLDA [10] algorithm. Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12], which exceeds the standard RBF SVM. Deep Learning has proven to be a powerful approach similar to the one that has recently achieved in most applications, and the results are similar to those of MedDA."}, {"heading": "5. IMAGE CLASSIFICATION", "text": "In this section we will look at the problem of image category recognition. We will use the Natural Scene category dataset, first introduced in [2, 13]. This dataset consists of 15 image categories, e.g. office, street view, forests, etc., with 200-400 grayscale images in each category (4485 images in total) and an average image size of 300 x 250 pixels."}, {"heading": "5.1. Modeling Images as Bags of Features", "text": "There is extensive literature on BOF models for images. A typical model consists of 1) local feature extraction, 2) visual vocabulary construction and 3) image representation as BoFs. To emphasize the effect of cores, we follow the approach proposed in [2, 13, 14]. 1) Local feature extraction. For each image, the SIFT descriptors of all 16 x 16 image fields centered at grid points with a distance of 8 pixels are calculated. 2) Visual vocabulary construction. D fields are randomly selected from training images. W -meaning clusters are performed via the corresponding D-SIFT descriptors. W-mean vectors form a visual vocabulary that is called 1,.., W. 3) BoF representation. For each image, the SIFT descriptors of each field x-field are used."}, {"heading": "5.2. Experimental Results", "text": "We follow the settings in [13, 2]. We set the vocabulary size to = 400. We select 100 images per class and leave the rest for the test."}, {"heading": "6. CONCLUSION", "text": "In this paper, we proposed a novel kernel design principle to integrate partial model information. Using two different types of data, we demonstrated that the sensor-aware kernel improves with minimal modeling assumptions compared to other standard cores and handmade cores for specific domains. We also observed that the sensor-aware kernel can compete with the performance of modern approaches."}, {"heading": "7. REFERENCES", "text": "[1] D. Blei, \"Probabilistic topic models,\" Commun. of the ACM, vol. 55, no. 4, pp. 77-84, 2012. [2] Fei-Fei Li and P. Perona, \"A bayesian hierarchical model for learning natural scene categories,\" in Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR '05), San Diego, CA, Jun. 2005, pp. 524-531. [3] S. Lacoste-Julien, F. Sha, and M. Jordan, \"Disclda: Discriminative learning for Disdimensionality reduction and classification,\" in Advances in Neural Information Processing Systems 21 (NIPS' 09), pp. 897-904. 2009. [4] V. N. Vapnik, The nature of statistical learning theory, Springer Publishing House New York, Inc., Inc., 1995. [5] T. Hastie R. Tiziani, York, J. York, New York."}], "references": [{"title": "Probabilistic topic models", "author": ["D. Blei"], "venue": "Commun. of the ACM, vol. 55, no. 4, pp. 77\u201384, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["Fei-Fei Li", "P. Perona"], "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), San Diego, CA, Jun. 2005, pp. 524\u2013531.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M. Jordan"], "venue": "Advances in Neural Information Processing Systems 21(NIPS\u201909), pp. 897\u2013904. 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "The nature of statistical learning theory, Springer-Verlag", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res., vol. 5, pp. 819\u2013844, Dec. 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion kernels on statistical manifolds", "author": ["J. Lafferty", "G. Lebanon"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 129\u2013163, Dec. 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "A kullback-leibler divergence based kernel for svm classification in multimedia applications", "author": ["Pedro J. Moreno", "Purdy P. Ho", "Nuno Vasconcelos"], "venue": "Advances in Neural Information Processing Systems 16 (NIPS\u201903). 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "J. Mach. Learn. Res., vol. 5, pp. 361\u2013397, Dec. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Gibbs maxmargin topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "the 30th Int. Conf. on Machine Learning (ICML\u201913), Atlanta, GA, Jun. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 643\u2013669, Mar. 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), New York, NY, Jun. 2006, pp. 2169\u20132178.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Proc. the 10th IEEE International Conference on Computer Vision(ICCV\u201905), Beijing, China, Oct. 2005, pp. 1458\u20131465.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Modeling natural images using gated mrfs", "author": ["M. Ranzato", "V. Mnih", "J.M. Susskind", "G.E. Hinton"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2206\u20132222, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Reconfigurable models for scene recognition", "author": ["Sobhan Naderi Parizi", "John Oberlin", "Pedro F. Felzenszwalb"], "venue": "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2775\u20132782.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "For supervised classification, a generative approach would make further assumptions on p(z|y) which may either not hold or lead to an intractable posterior inference problem [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "On the other hand, the classical distribution-free discriminative paradigm would ignore knowledge of the sensing model p(x|z) and build classifiers using labeled training data [4, 5].", "startOffset": 176, "endOffset": 182}, {"referenceID": 4, "context": "The Probability Product Kernel (PPK) proposed in [6] first estimates latent variable \u1e91 using observation x and defines the kernel in terms of the estimate as K(x,x) := \u222b", "startOffset": 49, "endOffset": 52}, {"referenceID": 5, "context": "Heat or Diffusion kernels (Diff) that exploit the Fisher information metric on the probability manifold were proposed in [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "Kernels based on the KL divergence were proposed in [8].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Following standard practice [9], we removed a standard list of stop words from the vocabulary.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "This is a difficult task due to the similarity of content in these two groups [3, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 8, "context": "This is a difficult task due to the similarity of content in these two groups [3, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 2, "context": "35 DiscLDA [3] 83.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "00 PPK SVM[6] 81.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "02 G-MedLDA [10] 83.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "57 Diff SVM [7] 79.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "We used LIBSVM [11] to train our kernel SVMs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 8, "context": "The CCR for the Gibbs MedLDA method is for the best number of topics k as in [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "We used the \u201cone-versus-all\u201d strategy following [10] to do multiclass classification with binary classifiers.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "Table 2 shows the CCRs for the proposed sensing-aware kernels, the RBF baseline, the two model-based kernels PPK and Diff, and the G-MedLDA [10] algorithm.", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12] which outperforms the standard RBF SVM.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "Since [12] uses the same training settings, we simply quote the results reported in that paper.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "5 RBM [12] 76.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "2 PPK SVM[6] 78.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "2 G-MedLDA [10] 80.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "9 Diff SVM [7] 78.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "We use the Natural Scene category dataset first introduced in [2, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "We use the Natural Scene category dataset first introduced in [2, 13].", "startOffset": 62, "endOffset": 69}, {"referenceID": 1, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 11, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 12, "context": "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].", "startOffset": 89, "endOffset": 100}, {"referenceID": 11, "context": "To incorporate spatial structure, we use the pyramid matching scheme proposed in [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "We use the same weighting factors \u03b1 as in [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "We follow the settings in [13, 2].", "startOffset": 26, "endOffset": 33}, {"referenceID": 1, "context": "We follow the settings in [13, 2].", "startOffset": 26, "endOffset": 33}, {"referenceID": 11, "context": "Following [13], we consider three setups.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We also compare with the classic Spatial Pyramid (SP) kernel of [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Finally, we consider a recently developed deep learning algorithm [15] (denoted by MRF) which achieves state-of-the-art performance on the same task.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "3 PPK [6] 73.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "6 Diff [7] 74.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "5 SP [13] 74.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "4 RBOW [16] N/A 78.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "MRF [15] 81.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].", "startOffset": 142, "endOffset": 146}], "year": 2014, "abstractText": "We propose a novel approach for designing kernels for support vector machines (SVMs) when the class label is linked to the observation through a latent state and the likelihood function of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision boundary is a hyperplane under a mapping defined by the likelihood function. Combining this with the maximum margin principle yields kernels for SVMs that leverage knowledge of the sensing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the performance of rather sophisticated state-of-the-art approaches.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}