{"id": "1312.0579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "SpeedMachines: Anytime Structured Prediction", "abstract": "Structured prediction plays a central role in machine learning applications from computational biology to computer vision. These models require significantly more computation than unstructured models, and, in many applications, algorithms may need to make predictions within a computational budget or in an anytime fashion. In this work we propose an anytime technique for learning structured prediction that, at training time, incorporates both structural elements and feature computation trade-offs that affect test-time inference. We apply our technique to the challenging problem of scene understanding in computer vision and demonstrate efficient and anytime predictions that gradually improve towards state-of-the-art classification performance as the allotted time increases.", "histories": [["v1", "Mon, 2 Dec 2013 20:26:41 GMT  (849kb,D)", "http://arxiv.org/abs/1312.0579v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander grubb", "daniel munoz", "j", "rew bagnell", "martial hebert"], "accepted": false, "id": "1312.0579"}, "pdf": {"name": "1312.0579.pdf", "metadata": {"source": "CRF", "title": "SpeedMachines: Anytime Structured Prediction", "authors": ["Alexander Grubb", "Daniel Munoz", "J. Andrew Bagnell"], "emails": ["agrubb@cmu.edu", "dmunoz@cs.cmu.edu", "dbagnell@cs.cmu.edu", "hebert@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Structured prediction algorithms are needed for many problems to obtain accurate predictions, but this class of technology is typically more sophisticated in mathematics than simpler, locally independent predictions. Furthermore, with limited computing resources, we may be forced to make a prediction after a limited and unknown period of time. Therefore, we need an approach that is both efficient and able to provide a reasonable prediction at all times when it is requested. Furthermore, as the inference process is given more time, we should expect predictive performance to increase as well. Therefore, we need an approach that is both efficient and able to provide a reasonable prediction when it is requested."}, {"heading": "1.1 Related Work", "text": "One disadvantage of the cascade approach is that the method is designed for a specific sequence length and is not suitable for interruption. For example, predictions made after a module in a long cascade are generally much worse than those made by a predictor trained in isolation. Recent work [9, 12, 29, 14] has examined techniques for learning locally independent predictors that balance computing time and inference time during learning; however, they are not immediately available for the structured prediction scenario. In the structured environment, Jiang et al. [13] have proposed a technique for amplification learning that balances user-specified speed / accuracy compensation during learning, and Weiss and Taskar [27] have proposed a cascaded analogy for structured prediction where the solution is precise and not focused on structural properties that we have both in terms of time and in terms of structural procedures."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Structured Prediction", "text": "The goal is to learn a function f: X \u2192 Y that minimizes a certain risk R [f], which is typically evaluated pointedly via the inputs: R [f] = EX [l (f (x)]]]]. (1) We will further assume that each input and output pair has an underlying structure, like the graphical structure of graphic models that can be used to predict portions of the output locally. Let j index these structural elements. We then assume that a final structured output y can be represented as a variable length vector (y1,.yJ), where each element yj lies in some vector space yj \u00b2 Y \u00b2. For example, these outputs could be the probability distribution over class labels for each pixel in an image or distributions of language labels in a sentence."}, {"heading": "2.2 Anytime Prediction", "text": "In traditional boosting, the goal is to learn a function f (x) = p = t \u03b1tht (x), (3), which is additionally constructed from a series of weaker predictors h \u2212 H, which minimizes a certain risk. Minimizing this function can be regarded as a drop in the functional space [18, 8]. Assuming that the loss function proceeds from the form specified in (Eq. 1), the functional gradient, i.e. a function of the form (x) = drop in the functional space [18, 8] (4) The weak predictor h, which best minimizes the prediction error of the functional gradient, is selected for each iteration: ht = arg max h, h < R [ft \u2212 1], h >, h >, (5)."}, {"heading": "3 Anytime Structured Prediction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Weak Structured Predictors", "text": "We adjust the SpeedBoost framework to the structured prediction setting by learning an additive structured predictor = weak prediction function. To achieve this, we will adjust the policy-based iterative decoding approach to use an additive structured predictor function instead of one that replaces previous prediction functions. In the iterative decoding we described earlier, we can remember that we have two components, one for selecting the elements to be updated and another for updating the prediction function of the given elements. Let St be the set of components selected for updating the iteration t. (For the current prediction function yt) we can rewrite the policy for calculating the prediction function (for the next iterative decoding iterative decoding). (9) The additive version of this policy instead uses weak prediction factors that produced both the prestructured data and the prestructured data."}, {"heading": "3.2 Selecting Weak Predictors", "text": "In order to use the strategy of rapid improvement per unit of time used by SpeedBoost in (Eq. 8) (Eq. 8), we must be able to complete the prediction operation via the H. We assume that we create a fixed set of possible selection functions, HS and a set of L learning algorithms, {Al} Ll = 1, where A: D \u2192 HP yields a predictive value. In practice, these algorithms are generated by varying the complexity of the algorithm, for example, predicting depth in a decision tree. Given a fixed selection function hS and current prediction function y, we can build a dataset suitable for the formation of weak predictors hP as follows. To predict the error in (Eq. 7) for a predictor h of the form in (Eq. 12), it can be shown that this is used to determine the prediction function of the function hP, which performs mizarg."}, {"heading": "3.3 Handling Limited Training Data", "text": "In practice, this can be alleviated by adapting the concept of stacking [28], which has proven useful in other structured prediction work [3, 19]. Conceptually, the idea is that we do not want to use the same if we are currently learning to generate y for use in the next stronger iteration. Specifically, we can instead divide our entire training set into two disjunct subgroups, namely B, B, B, B. In training, we learn three separate structured prediction devices f, fA, fB, fB across datasets, A, B. Let us leave hP, P, P, P, h, B P the prediction functions for the structured prediction devices f, fA, fB, fB, fB, fB, b. In training hP over x, the prediction results y are generated from held-up prediction forecasts: for x, A, fY = (B, B, B, fB) for training, A for training, B for training, x for the training, Y x for the training."}, {"heading": "4 Anytime Scene Understanding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Background", "text": "Besides partially marking language in the processing of natural language, scene understanding in computer vision is another important and challenging structured prediction problem. While random fields are de facto a clean interface between modelling and inference, recent work [25, 19, 21, 6] has shown alternative approaches that achieve equivalent or improved performance with the added benefit of a simple, efficient and modular inference procedure. [19] Inspired by the hierarchical representation used in the state-of-the-art technique of scene understanding by Munoz et al. [25], we apply StructuredSpeedBoost to the scene understanding problem by thinking about different regions in the scene. Below, we briefly review the hierarchical inference machine approach (HIM) from [19] and then describe how we can perform a similar prediction in spirit at any time."}, {"heading": "4.2 Hierarchical Inference Machines", "text": "HIM analyzes the scene using a hierarchy of segmentations as shown in Fig. 1. By including several different segmentations, this representation solves the problem of scale ambiguity in images. Instead of drawing (approximate) conclusions on a large random field defined by regions, the conclusion is divided into a sequence of predictions. As shown in Fig. 1, each level in the hierarchy that predicts the probability distribution of classes / objects within each region is assigned a predictor f, which is then used by the subsequent predictor at the next level (in addition to the characteristics derived from image statistics) to make refined predictions about the finer regions; and the process iterates. By passing class distributions between predictors, contextual information is modeled, although segmentation at a certain level may be flawed."}, {"heading": "4.3 Speedy Inference Machines", "text": "While HIM breaks down the structured prediction problem into an efficient sequence of predictions, it is not readily suitable for predicting at any time. Firstly, the final predictions are made when the procedure at the leaf nodes ends in the hierarchy. Therefore, interrupting the procedure before this point in time would lead to final predictions about rough regions that could severely subsegment the scene. Secondly, the amount of computing time at each step of the procedure is invariant to current performance. Since the structure of the sequence is predefined, the inference procedure on a region will predict several times as it traverses the hierarchy, although there may be no room for improvement. Thirdly, inputting each predictor in sequence is a fixed feature descriptor for the region. Since these input descriptors must be pre-calculated for all regions in the hierarchy before the inference process begins, there is a fixed initial computational effort."}, {"heading": "4.3.1 Interruptible Prediction", "text": "To solve the first problem, we learn an additive predictor f, which predicts a classification per pixel for the entire image at once. In contrast to IT, whose losses are measured on multiple predictors across regions, we form a single predictor, whose loss is measured by pixels. Specifically, given the truth distributions per pixel in the ground pj-RK, we want to optimize the pixel, cross entropy risk for all pixels in the imageR [f] = EX \u2212 \u2211 j pjk log q (f (x)) jk, (18) whereq (y) jk = exp (yjk) \u0445k-exp (yjk), (19) i.e. the probability of the k'th class for the j'th pixel. Using (Equation. 12), the probability distribution associated with each pixel is then determined by 1) the pixels to be updated, selected by hS, and 2) the value of the predictor hP for the respective pixel functions."}, {"heading": "4.3.2 Structure Selection and Prediction", "text": "To take into account the scale ambiguity and structure in the scene, we can similarly integrate several regions q into our predictor. By using a hierarchical segmentation of the scene that produces many segments / regions, we can consider each resulting region or pixel segment S in the hierarchy as a possible set of results that need to be updated. Intuitively, there is no need to update regions of the image where the predictions in the current inference step are correct. Therefore, we want to update the part of the scene where the predictions are uncertain, i.e., have a high entropy H. To achieve this, we use a selector function that selects regions with high average per-pixel entropy in the current predictions, hS (x, y) = S region where we select a large region."}, {"heading": "4.3.3 Dynamic Feature Computation", "text": "To this end, we use the cost model SpeedBoost (Eq. 13) to automatically select the most mathematically efficient features. In many applications, it is useful to quantify these basic feature descriptors and to build them together to form a series of derived features [11, 16]. We follow the soft vector quantization approach in [2] to form a quantified code vector by calculating distances to multiple cluster centers in a dictionary. This calculation incurs fixed costs for 1) each group of features with a common basic function and 2) additional, smaller fixed costs for each feature actually used. To calculate these costs, we use an additive model similar to fixed costs."}, {"heading": "5 Experimental Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "We evaluate the performance indicators between SIM and HIM using the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes, following the same training / testing procedures originally described in the respective papers. As shown in Table 1, HIM achieves state-of-the-art performance and these data sets and analyses the computerized trade-offs compared to SIM. Since both methods operate across a regional hierarchy of the scene, we use the same segmentations, characteristics and regression trees (weak predictors) for a fair comparison."}, {"heading": "5.1.1 Segmentations", "text": "We construct a 7-step segmentation hierarchy by recursively executing the graph-based segmentation algorithm (FH) [7] with the parameters \u03c3 = 0.25, c = 102 \u00b7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300]. These values were qualitatively selected to generate regions with different resolutions."}, {"heading": "5.1.2 Features", "text": "The feature descriptor of a region consists of 5 feature groups: 1) Regional Boundary Form / Geometry / Location (SHAPE) [11], 2) Texture (TXT), 3) Local Binary Patterns (LBP), 4) SIFT via Intensity (I-SIFT), 5) SIFT separately via Colors R, G, and B (C-SIFT). The last 4 are derived from Pro-Pixel descriptors, for which we use the publicly available implementation from [16]. Calculations for segmentation and features are presented in Table 2; all times were calculated on an Intel i7-2960XM processor. SHAPE descriptor is calculated exclusively from the segmentation boundaries and is efficiently calculated."}, {"heading": "5.2 Analysis", "text": "We find that more efficient SHAPE descriptor is selected at the first iteration, followed by the next cheapest de-scriptors TXT and I-SIFT. Although LBP is cheaper than C-SIFT, the algorithm ignored LBP because it did not cost-efficiently improve the prediction. In Figure 2, we compare the classification performance of SIM and several other algorithms in terms of inference time. We also look at HIM as well as two variants that use a limited set of 4 sets of characteristics (only TXT and TXT & I-SIFT); these SIM and HIM models were executed on the same computer. We also compare the reported performance of other techniques and stress that these timings are reported from different computational configurations. The individual prediction approach generated at any time by our structured prediction approach is competitive with all specially trained, standardized models, without the need for manual analysis of the SIM."}, {"heading": "6 Conclusion", "text": "We proposed a methodology for structured prediction with always available characteristics. Our approach is based on the boosting framework, which automatically integrates new learners into our predictor, which best improves performance in terms of both trait and sequence calculation times. We demonstrated the effectiveness of our approach in the challenging task of scene understanding in computer vision, achieving state-of-the-art performance classifications with improved efficiency compared to previous work."}], "references": [{"title": "Segmentation and recognition using structure from motion point clouds", "author": ["Gabriel J. Brostow", "Jamie Shotton", "Julien Fauqueur", "Roberto Cipolla"], "venue": "In ECCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Stacked sequential learning", "author": ["William W. Cohen", "Vitor Carvalho"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Search-based structured prediction. MLJ", "author": ["Hal Daume III", "John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "On-line semantic perception using uncertainty", "author": ["Roderick de Nijs", "Sebastian Ramos", "Gemma Roig", "Xavier Boix", "Luc Van Gool", "Kolja Kuhnlenz"], "venue": "In IROS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["Clement Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "In T-PAMI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Efficient graph-based image segmentation", "author": ["Pedro F. Felzenszwalb", "Daniel P. Huttenlocher"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Active classification based on value of classifier", "author": ["Tianshi Gao", "Daphne Koller"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["Stephen Gould", "Richard Fulton", "Daphne Koller"], "venue": "In ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Multi-class segmentation with relative location", "author": ["Stephen Gould", "Jim Rodgers", "David Cohen", "Gal Elidan", "Daphne Koller"], "venue": "prior. IJCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Alexander Grubb", "J. Andrew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learned prioritization for trading off accuracy and speed", "author": ["Jiarong Jiang", "Adam Teichert", "Hal Daume III", "Jason Eisner"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Timely object recognition", "author": ["Sergey Karayev", "Tobias Baumgartner", "Mario Fritz", "Trevor Darrell"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Discriminative random fields", "author": ["Sanjiv Kumar", "Martial Hebert"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Global Structured Models towards Scene Understanding", "author": ["Lubor Ladicky"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "What, where & how many? combining object detectors and crfs", "author": ["Lubor Ladicky", "Paul Sturgess", "Karteek Alahari", "Chris Russell", "Philip H.S. Torr"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classifiers", "author": ["L. Mason", "J. Baxter", "P.L. Bartlett", "M. Frean"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Stacked hierarchical labeling", "author": ["Daniel Munoz", "J. Andrew Bagnell", "Martial Hebert"], "venue": "In ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["S. Ross", "D. Munoz", "M. Hebert", "J.A. Bagnell"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Combining appearance and structure from motion features for road scene understanding", "author": ["Paul Sturgess", "Karteek Alahari", "Lubor Ladicky", "Philip H.S. Torr"], "venue": "In BMVC,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Scalable cascade inference for semantic image segmentation", "author": ["Paul Sturgess", "Lubor Ladicky", "Nigel Crook", "Philip H.S. Torr"], "venue": "In BMVC,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Superparsing: Scalable nonparametric image parsing with superpixels", "author": ["Joseph Tighe", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Auto-context and its application to high-level vision tasks and 3d brain image segmentation", "author": ["Zhuowen Tu", "Xiang Bai"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Robust real-time face detection", "author": ["Paul A. Viola", "Michael J. Jones"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Structured prediction cascades", "author": ["David Weiss", "Ben Taskar"], "venue": "In AISTATS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Stacked generalization", "author": ["David H. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1992}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Zhixiang Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "1 Related Work A canonical approach for incorporating computation time during learning is a cascade of feed-forward modules, where each module becomes more sophisticated but also more computationally expensive the further it is down the cascade [26].", "startOffset": 245, "endOffset": 249}, {"referenceID": 8, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 11, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 28, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 13, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 12, "context": "[13] proposed a technique for reinforcement learning that incorporates a user specified speed/accuracy trade-off distribution, and Weiss and Taskar [27] proposed a cascaded analog for structured prediction where the solution space is is iteratively refined/pruned over time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[13] proposed a technique for reinforcement learning that incorporates a user specified speed/accuracy trade-off distribution, and Weiss and Taskar [27] proposed a cascaded analog for structured prediction where the solution space is is iteratively refined/pruned over time.", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "Recent work in computer vision and robotics [23, 5] has similarly investigated techniques for making approximate inference in graphical models more efficient via a cascaded procedure that iteratively prunes subregions in the scene to analyze.", "startOffset": 44, "endOffset": 51}, {"referenceID": 4, "context": "Recent work in computer vision and robotics [23, 5] has similarly investigated techniques for making approximate inference in graphical models more efficient via a cascaded procedure that iteratively prunes subregions in the scene to analyze.", "startOffset": 44, "endOffset": 51}, {"referenceID": 2, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 3, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 24, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 20, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 19, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 19, "context": "[20], this iterative decoding approach can is equivalent to message passing approaches used to solve graphical models, where each update encodes a single set of messages passed to one node in the graphical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Minimizing this functional can be viewed as performing gradient descent in function space [18, 8].", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "Minimizing this functional can be viewed as performing gradient descent in function space [18, 8].", "startOffset": 90, "endOffset": 97}, {"referenceID": 11, "context": "Extending this framework, Grubb and Bagnell [12] introduce an anytime prediction method that modifies the standard boosting criterion to automatically trade-off the loss of a weak predictor, h, with its cost c(h) \u2208 R.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Grubb and Bagnell prove that this SpeedBoost algorithm updates the resulting predictions at an increasing sequence of budgets that is competitive with any other sequence which uses the same weak predictors for a wide range of budgets [12].", "startOffset": 234, "endOffset": 238}, {"referenceID": 27, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 18, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 14, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 10, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 16, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 18, "context": "Figure 1: Hierarchical Inference Machines [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 18, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 20, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 5, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 18, "context": "[19], we apply StructuredSpeedBoost to the scene understanding problem by reasoning over differently sized regions in the scene.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In the following, we briefly review the hierarchical inference machine (HIM) approach from [19] and then describe how we can perform an anytime prediction whose structure is similar in spirit.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "8 HIM [19] 92.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "1 [6] 95.", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": "4 [21] - - - - - - - - - 78.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "5 HIM [19] 83.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "9 [5] 59 75 93 84 45 90 53 27 0 55 21 54.", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "0 [17]\u2020 81.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "The features used in this application, drawn from previous work [11, 16] and detailed in the following section, are computed as follows.", "startOffset": 64, "endOffset": 72}, {"referenceID": 15, "context": "The features used in this application, drawn from previous work [11, 16] and detailed in the following section, are computed as follows.", "startOffset": 64, "endOffset": 72}, {"referenceID": 1, "context": "In many applications it is useful to quantize these base feature descriptors and pool them together to form a set of derived features [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "We follow the soft vector quantization approach in [2] to form a quantized code vector by computing distances to multiple cluster centers in a dictionary.", "startOffset": 51, "endOffset": 54}, {"referenceID": 28, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In order to generate hP with a variety of costs, we use a modified regression tree that penalizes each split based on its potential cost, as in [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "1 Setup We evaluate performance metrics between SIM and HIM on the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes; we follow the same training/testing evaluation procedures as originally described in the respective papers.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "1 Setup We evaluate performance metrics between SIM and HIM on the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes; we follow the same training/testing evaluation procedures as originally described in the respective papers.", "startOffset": 176, "endOffset": 179}, {"referenceID": 6, "context": "1 Segmentations We construct a 7-level segmentation hierarchy by recursively executing the graph-based segmentation algorithm (FH) [7] with parameters \u03c3 = 0.", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 1, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 4, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 9, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 10, "context": "2 Features A region\u2019s feature descriptor is composed of 5 feature groups (\u0393): 1) region boundary shape/geometry/location (SHAPE) [11], 2) texture (TXT), 3) local binary patterns (LBP), 4) SIFT over intensity (I-SIFT), 5) SIFT separately over colors R, G, and B (C-SIFT).", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "The last 4 are derived from per-pixel descriptors for which we use the publicly available implementation from [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Using the soft code assignment from [2], the code is defined", "startOffset": 36, "endOffset": 39}], "year": 2013, "abstractText": "Structured prediction plays a central role in machine learning applications from computational biology to computer vision. These models require significantly more computation than unstructured models, and, in many applications, algorithms may need to make predictions within a computational budget or in an anytime fashion. In this work we propose an anytime technique for learning structured prediction that, at training time, incorporates both structural elements and feature computation trade-offs that affect test-time inference. We apply our technique to the challenging problem of scene understanding in computer vision and demonstrate efficient and anytime predictions that gradually improve towards state-of-the-art classification performance as the allotted time increases.", "creator": "LaTeX with hyperref package"}}}