{"id": "1312.0786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2013", "title": "Image Representation Learning Using Graph Regularized Auto-Encoders", "abstract": "We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering.", "histories": [["v1", "Tue, 3 Dec 2013 11:59:57 GMT  (17kb)", "https://arxiv.org/abs/1312.0786v1", "9pages"], ["v2", "Wed, 19 Feb 2014 11:13:57 GMT  (18kb)", "http://arxiv.org/abs/1312.0786v2", "9pages"]], "COMMENTS": "9pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yiyi liao", "yue wang", "yong liu"], "accepted": false, "id": "1312.0786"}, "pdf": {"name": "1312.0786.pdf", "metadata": {"source": "CRF", "title": "Image Representation Learning Using Graph Regularized Auto-Encoders", "authors": ["Yiyi Liao", "Yue Wang", "Yong Liu"], "emails": ["yongliu@iipc.zju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.07 86v2 [cs.LG] 1 9Fe b"}, {"heading": "1 Introduction", "text": "Although the dense original image can provide an intuitive visual representation, it is known that this representation can cover the hidden semantic patterns that need to be recognized by these image-based learning tasks. On the other hand, the performance of machine learning methods is also highly dependent on the corresponding data representation on which they are applied, making image representation a fundamental problem in visual analysis. In the face of an image data matrix X, each column of X corresponds to an image, the image representation should find a representation function H = f (H) that can extract useful information from X. And each column vector of H is the representation of an image in this concept space. In the perspective of dimension reduction, the learned representation should have a lower dimension than the original one."}, {"heading": "2 Background", "text": "It is not only the way in which the data of the original dataset is used by the original dataset X = 1 (X = 1,...), but also the way in which the data of the original dataset X = 2 (X = 2) is represented. (It is another function called a decoder, which is called a decoder from the attribute space back into the input space, which creates a reconstruction Q = qp (H). (X, Q), which is also called a loss function, is defined, and the set of parameters of the encoder and the decoder are simultaneously assigned to the task of reconstructing the original input, i.e."}, {"heading": "3 Graph Regularized Auto-Encoder", "text": "The problem is to design the second term in (1) to limit the representation space. In this section, we present our geometric regularization, which implements local invariance in unsupervised and semi-supervised learning."}, {"heading": "3.1 Single Layer Auto-Encoder Regularized with Graph", "text": "In our graph regulated auto-encoder (GAE), both the decoder and encoder play significant sigmoid as their activation functions. Denote sigmoid function as S (x) = 1 / (1 + e \u2212 x). Then, the encoder and decoder can be represented as follows: H = f\u03b8 (X) = S (WHX + bH) (5) Q = q3 (H) = S (WQH + bQ) (6) Since the representation should discover the latent structure of the original data, the geometric structure of the data will be the ideal latent structure in the representation of learning, especially in unattended or semi-monitored learning. A reasonable assumption is that if two data points xi, xj are close to the intrinsic geometry of the data distribution, then their corresponding representations, hi, hj, should also be close to each other."}, {"heading": "3.2 Multiple-layer Auto-Encoders Regularized with Graph", "text": "The auto encoder was proposed in the context of the neural network, which is later applied to train the deep structure of networks to achieve better expressiveness. In data representation, the idea of multi-layer representation still works. Thus, the representation of the original data can be represented with both a layer of mapping and multi-layer mapping. We also implement the locally invariant constraint into the multi-layer auto encoder by adding regularized terms to the graph. Since all layers can be stacked in multi-layer auto encoders at the same time, we train the multi-layer GAE constraint layer by layer in our multi-layer GAE. We use Hi \u2212 \u2212 \u2212 to denote the data representation of the ith layer, and the associated decoder is referred to as Qi. The input data of the ith layer is the data representation of the i \u2212 1st layer of the Hi \u2212 bi function of the ith-Qi (Hi-Qi-Qi) (Qi \u2212 Qi)."}, {"heading": "3.3 Graph Regularizer Design", "text": "As mentioned above, the performance of the date representation regulated by graphs is mainly based on the design of the weight matrix V, as it encodes the local inventory information of the data space. In this section, we will focus on the design of the weight matrix of supervised learning and unsupervised learning in the context of data representation using auto encoders."}, {"heading": "3.3.1 Unsupervised Learning", "text": "In case of unattended learning, the name of the data is not available. We can only derive the structure of the data from the local property of the data samples. In our GAE, three types of weights are used, which are introduced as follows: \u2022 KNN graph: First, for each data sample, the adjacent k records are constructed. \u2022 KNN graph: If xi is in the adjacent k sample, the weights vij are specified as the distance between these two data samples, i.e. exp (\u2212 xj), otherwise vij is set to zero. \u2022 KNN graph: First, the K neighbor records are constructed for each data sample, where the data sample xi in the adjacent sample contains all data samples whose distances to xi are smaller than xi. If xi is in the adjacent xj sample, the weights vij are specified as the distance between these two data samples, i.e. exp (xi \u2212 j) is set to vioj."}, {"heading": "3.3.2 Semi-supervised Learning", "text": "In our GAE, the design of the graph regularizer for the semi-supervised learning task is similar to the unsupervised learning task. First, we construct the k-nearest neighbor sets for the entire dataset and set the weight vij to zero if xi and xj are not neighbors. For the condition that xi and xj are neighbors, the weights are calculated as follows: vij = {exp (\u2212 xi \u2212 xj) either xi or xj is unlabeled 1 xi, xj have the same labeling 0 xi, xj have different designations (12) As already mentioned, the weights in the graph are calculated by exp (\u2212 xi \u2212 xj \u0441), which is a value smaller than 1 but greater than 0. Since the labeled data provide basic truth information, the weights of two samples with the same labeling are calculated by exp (\u2212 xi \u2212 xj \u0441.), with the weights in the graph lying directly between the two sizes."}, {"heading": "4 Experimental Results", "text": "In this section, comparative experiments are performed to demonstrate the performance of our proposed method in tasks of both unsupervised learning and semi-supervised learning. To evaluate image representations acquired quantitatively using different methods, the k-mean cluster is applied to representations learned through different methods. Two metrics, Normalized Mutual Information (MI) and Accuracy (AC), are used to measure cluster performance. To provide a fair comparison, the dimension of learned representation is adjusted by all algorithms to be equal. Normalized Mutual Information (MI) is specified in [4], which is a standardized measure to evaluate how similar two groups of clusters are. Accuracy (AC) is used to measure the percentage of correct designations provided by the data set. Specifically, a data sample xi with cluster marking and soil truth marking c is specified."}, {"heading": "4.1 Variants Comparison", "text": "The fine-tuning [7] of a pre-trained deep network can sometimes improve performance in many supervised learning tasks. However, in unsupervised learning, the weight matrix built at Euclidean distance may contain some incorrect information, i.e. samples with different labels can be connected to each other. We can calculate the error rate as the ratio of false connections and total connections. We construct a 2-layer auto-encoder and implement a layer-by-layer pre-training based on the method in Section 3.2. Then we refine the deep auto-encoder with the single chart regulation, i.e. only the second term of (11). The input data is selected from COIL20. You have 20 classes. In this experiment, we select 8 classes at random for comparison. Thus, the unattended weight matrix is also a kind of random principle based on the data set. The test result in Table 2 shows that if the weight matrix is stimulated by fine-tuning without performance, the fine-tuning can be achieved."}, {"heading": "4.2 Experiments in Unsupervised Learning", "text": "The methods used for comparison include: \u2022 k means: This is the basic method that simply clusters in the original attribute space. \u2022 PCA: Principal Components Analysis. This is the classic linear technique for dimension reduction. \u2022 GAE: Graph Auto-Encoder (2 layers). It is a contribution of this paper that introduces graph constraint to the auto-encoder. \u2022 In the experiments, our GAE uses the KNN graph. There are two coefficients, k for the number of neighbors in the KNN graph and \u03bb for the intensity of the graph regulator. They are all selected by the grid-based search. \u2022 SAE: Sparse + Auto-Encoder (2 layers)."}, {"heading": "4.3 Experiments in Semi-supervised Learning", "text": "In this experiment, these labeled samples are randomly selected; for COIL20, 10% of the samples in each class are labeled, so there are 7 labeled samples in each class; for ORL and Yale, 20% are labeled, then 2 samples in each class are labeled; the reference to the unattended learning experiments is the dimension of the learned representation equal to the number of classes in the dataset; the comparison methods used in this experiment include: \u2022 CNMF: limited NMF. It is proposed in [12]. Within its framework, the samples are labeled with the same label to have the same representation in reduced space; and there are also no user-defined parameters. \u2022 SGAE: Semi-Graph Regulates Auto-Encoder (2 layers)."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel graph-regulated auto-encoder that can learn a locally invariant representation of images for both unattended and semi-monitored learning tasks. In unattended learning, our approach trains image representation through a multi-layer auto-encoder that is regulated with the graph and encodes the local neighborhood relations of the original data. In semi-monitored learning, the graph regulator used in our auto-encoders is extended to the semi-graph regulator that adds the punishment and reward obtained from the marked data to the locally adjacent weight matrix. Experimental results in image clustering show that our method offers better performance compared to the state-of-the-art approaches. Further work could focus on studying the effects of parameter settings in GAE and the effects of deep structure are also possible future work."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS, pages 585\u2013591,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, 7:2399\u20132434,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1548\u20131560,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 22, pages 646\u2013654.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR \u201906, pages 1735\u20131742, Washington, DC, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, 313(5786):504 \u2013 507,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": "NIPS, pages 3\u201310,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106\u20131114.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, pages 536\u2013543,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Constrained nonnegative matrix factorization for image representation", "author": ["H. Liu", "Z. Wu", "D. Cai", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1299\u20131311,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep supervised t-distributed embedding", "author": ["M.R. Min", "L. Maaten", "Z. Yuan", "A.J. Bonner", "Z. Zhang"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 791\u2013798,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 737\u2013744. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["C. Poultney", "S. Chopra", "Y.L. Cun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y.-L. Boureau", "Y. LeCun"], "venue": "J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1185\u20131192. MIT Press, Cambridge, MA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C.S. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS, pages 1137\u20131144,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part II, ECML PKDD\u201911, pages 645\u2013660, Berlin, Heidelberg,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Contractive Auto-Encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Applications of Computer Vision, 1994., Proceedings of the Second IEEE Workshop on, pages 138\u2013142. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Convolutional-recursive deep learning for 3d object classification", "author": ["R. Socher", "B. Huval", "B.P. Bath", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, pages 665\u2013673,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Comput., 23(7):1661\u20131674, July", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, ICML \u201908, pages 1096\u20131103, New York, NY, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.", "startOffset": 101, "endOffset": 108}, {"referenceID": 21, "context": "It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.", "startOffset": 101, "endOffset": 108}, {"referenceID": 6, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 7, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 2, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 6, "context": "g in Hinton\u2019s work [7], which train the encoder network (from X to Hi, i is the number of the layer) one-by-one using the Restricted Boltzamann Machines and the decoder layers of the network are formed by the inverse of the trained encoder layers, such as WH = (WQ) in one layer auto-encoder.", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 10, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 4, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 9, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 23, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 25, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 24, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 18, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "It is pointed out that the sparse penalty used in sparse auto-encoder will tend to make only few input configurations can have a low reconstruction error [16], which may hurt the numerical optimization of parameters.", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "Previous studies have also shown that the locally invariant idea [6] will play an important role in the image representation, especially for those tasks of unsupervised learning and semi-supervised learning.", "startOffset": 65, "endOffset": 68}, {"referenceID": 19, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "[14] proposed a graph regularizer that constrains the similarity between consecutive frames, which shows the human knowledge can be applied in this term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In [13], a graph constrains the data points belonging to the", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Another work similar to us is [6], in which a convolutional neural network is applied to minimize a graph regularizer.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 19, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 1, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 3, "context": "The normalized mutual information(MI) is given in [4], which is a normalized measure to evaluate how similar two sets of clusters are.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "This function is implemented using the code published by [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 20, "context": "The data sets employed for the experiments including: ORL[21], Yale and COIL20, whose statistics are shown in table 1.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "Fine-tuning[7] of a pre-trained deep network can sometimes improve the performance in many supervised learning tasks.", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 SAE: Sparse+Auto-encoder(2 layers)[15].", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "It is proposed in [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 11, "context": "It is proposed in [12].", "startOffset": 18, "endOffset": 22}], "year": 2014, "abstractText": "It is an important task to learn a representation for images which has low dimension and preserve the valuable information in original space. At the perspective of manifold, this is conduct by using a series of local invariant mapping. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). The local invariant is achieved using a graph regularizer, which preserves the local Euclidean property from original space to the representation space, while the deep nonlinear mapping is based on an unsupervised trained deep auto-encoder. This provides an alternative option to current deep representation learning techniques with its competitive performance compared to these methods, as well as existing local invariant methods.", "creator": "LaTeX with hyperref package"}}}