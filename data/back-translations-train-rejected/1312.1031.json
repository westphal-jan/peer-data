{"id": "1312.1031", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Analysis of Distributed Stochastic Dual Coordinate Ascent", "abstract": "In \\citep{Yangnips13}, the author presented distributed stochastic dual coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss minimization. Extraordinary performances have been observed and reported for the well-motivated updates, as referred to the practical updates, compared to the naive updates. However, no serious analysis has been provided to understand the updates and therefore the convergence rates. In the paper, we bridge the gap by providing a theoretical analysis of the convergence rates of the practical DisDCA algorithm. Our analysis helped by empirical studies has shown that it could yield an exponential speed-up in the convergence by increasing the number of dual updates at each iteration. This result justifies the superior performances of the practical DisDCA as compared to the naive variant. As a byproduct, our analysis also reveals the convergence behavior of the one-communication DisDCA.", "histories": [["v1", "Wed, 4 Dec 2013 05:48:30 GMT  (311kb)", "https://arxiv.org/abs/1312.1031v1", null], ["v2", "Sun, 23 Mar 2014 22:13:17 GMT  (311kb)", "http://arxiv.org/abs/1312.1031v2", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["tianbao yang", "shenghuo zhu", "rong jin", "yuanqing lin"], "accepted": false, "id": "1312.1031"}, "pdf": {"name": "1312.1031.pdf", "metadata": {"source": "CRF", "title": "Analysis of Distributed Stochastic Dual Coordinate Ascent", "authors": ["Tianbao Yang", "Shenghuo Zhu", "Rong Jin", "Yuanqing Lin"], "emails": ["ylin}@nec-labs.com,", "rongjin@cse.msu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.10 31v2 [cs.DC] 2 3M ar"}, {"heading": "1 Introduction", "text": "With the exponential growth of data, it has become an urgent task to design distributed (or parallel) optimization for big data analytics.The rise of a large cluster of machines has enabled distributed optimization.The goal of distributed optimization is to optimize a specific goal defined by millions of billions of data distributed across many machines by focusing on a particular distributed optimization algorithm, i.e., coordinating distributed stochastic dual coordinate ascents (DisDCA).The idea behind the stochastic dual coordinate ascent is to maximize convergence performance while maximizing convergence performance. In this work, we focus on a particular distributed stochastic dual coordinate ascent (DisDCA)."}, {"heading": "2 DisDCA: the practical updates versus the naive updates", "text": "We start with a description of the practical problems and the problems associated with them. (...) We assume that the training data are evenly distributed, and leave Xk = 1,. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...).). (...). (...).).). (...).). (...).). (...). (...).). (...).). (...).). (...).). (...). (...).).). (...).). (...).). (...).). (...). (...). (...).).). (...).). (...).). (...). (...).). (...).). (...).). (...). (...).).). (...). (...). (...). (...). (...).). (...).). (...).).). (...). (...). (...).). (...)"}, {"heading": "3 Analysis of DisDCA for Orthogonal Data", "text": "In this section, we present our first theoretical result in relation to the convergence rate of the practical DisDCA for orthogonal data on different machines. In fact, in this case, the practical DisDCA can be slightly modified to achieve better convergence. We can change the scale rate = 1 in Inc-Dual, i.e. the update of data will depend on each individual type. (In this case, we have to change the updates of the primary variables ut, jk tout = u t, j \u2212 1 k \u2212 12 (usually) 2 x 2 (9) accordingly. We can change the updates of the primary variables ut, jk tout = u t, j \u2212 n machines ut, ixk, i (10) As we can see, the scale factor is reduced to 1 compared to K in the practical variant."}, {"heading": "3.1 Proof", "text": "To facilitate the proof, we present some notations to simplify our analysis."}, {"heading": "4 Analysis of the Practical DisDCA in General Case", "text": "The challenge for the analysis of the general case arises from the fact that we cannot resolve the quadratic norm of wt, j as the sum of the quadratic norm of each wt, jk without loss. Our strategy for the analysis is to derive a similar inequality as in Lemma 2. However, as we will soon see, there is an additional term that takes into account the difference between the use of the global primary solution wt, j \u2212 1 and the local primary solution ut, j \u2212 1k. According to the updates, we want to define: wtk = Kjxk, i, ixk, i, w t = 1KK, k = 1wtk, wt, jk, j \u2212 1 k + K, n, jt, jt, ijxk, ij, ij, w t, j, ixk, ixk, k, k, k, wk, wk, wk, wk, wk, wk, wk t, wk, wk, wk, wk, t, wk, t, wk, wk, wk, j, wk, wk, j, t, jk, jk, jk."}, {"heading": "5 Conclusions and Open Problems", "text": "In this manuscript, we have made progress in the theoretical analysis of the practical DisDCA. Supported by empirical studies, we show that the practical DisDCA is able to achieve an exponential acceleration of convergence by increasing the number of dual updates with each iteration. This result justifies the superiority of the practical DisDCA over the naive variant, which has only a partial linear velocity. There are still outstanding problems for future research. First, what is the analytical form of \u03b5 (t, m), which exceeds the upper limits S (t, m) by a certain velocity (t, m). Second, how can communication be designed asynchronously and convergence proven?"}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 1", "text": "Ak = \u2212 j \u2212 j \u2212 j \u2212 j \u2212 jj \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 j \u2212 t, ij \u2212 t, ij \u2212 t, ij \u2212 t, ij \u2212 t, j \u2212 t, j \u2212 t \u2212 t, ij \u2212 t, ij \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, k, k k k, k k k, k k, j, t, t, t, t, t t \u2212 t \u2212 t, t \u2212 t \u2212 t, k \u2212 t, t \u2212 t, t \u2212 t \u2212 t, k \u2212 t, k \u2212 t, k, k, k k, k, k, k, t, t, j, t, t, t, t, t t, t, t t, t, t \u2212 t \u2212 t \u2212 t \u2212 t, t \u2212 t \u2212 t, \u2212 t \u2212 t \u2212 t, \u2212 t \u2212 t, k \u2212 t, k \u2212 t \u2212 t, k \u2212 t, k \u2212 t \u2212 t \u2212 t, k \u2212 t \u2212 t \u2212 t, k \u2212 t \u2212 t, k \u2212 t \u2212 t, k \u2212 t \u2212 t \u2212 t, k \u2212 t \u2212 t \u2212 t, k \u2212 t, k \u2212 t \u2212 t \u2212 t, k, k \u2212 t \u2212 t \u2212 t, k, k, k, k, k k, k k k k k k, k k k, k, k k k, k k, k k, k, k \u2212 t, k, k \u2212 t, k \u2212 t, k \u2212 t, k, k \u2212 t, k \u2212 t, k \u2212 t, k \u2212 t, k \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212 t, j \u2212"}, {"heading": "Proof of Lemma 3", "text": "After the definition of K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1), K (1, K (1), K (1), K (1), K (1), K (1), K (1), K (1), K ("}], "references": [{"title": "Efficient Large-Scale distributed training of conditional maximum entropy models", "author": ["Mann", "Gideon", "McDonald", "Ryan", "Mohri", "Mehryar", "Silberman", "Nathan", "Walker", "Dan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Yang", "Tianbao"], "venue": null, "citeRegEx": "Yang and Tianbao.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Tianbao.", "year": 2013}, {"title": "Communication-Efficient algorithms for statistical optimization", "author": ["Zhang", "Yuchen", "Duchi", "John", "Wainwright", "Martin"], "venue": "In dvances in Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Parallelized stochastic gradient descent", "author": ["Zinkevich", "Martin", "Weimer", "Markus", "Smola", "Alexander J", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "It was advocated by practitioners and has been analyzed in (Zhang et al., 2012; Mann et al., 2009; Zinkevich et al., 2010) for least square regression, conditional maximum entropy models and stochastic gradient descent.", "startOffset": 59, "endOffset": 122}, {"referenceID": 0, "context": "It was advocated by practitioners and has been analyzed in (Zhang et al., 2012; Mann et al., 2009; Zinkevich et al., 2010) for least square regression, conditional maximum entropy models and stochastic gradient descent.", "startOffset": 59, "endOffset": 122}, {"referenceID": 3, "context": "It was advocated by practitioners and has been analyzed in (Zhang et al., 2012; Mann et al., 2009; Zinkevich et al., 2010) for least square regression, conditional maximum entropy models and stochastic gradient descent.", "startOffset": 59, "endOffset": 122}, {"referenceID": 2, "context": "The data is generated similarly as in (Zhang et al., 2012) with extra concern.", "startOffset": 38, "endOffset": 58}, {"referenceID": 2, "context": "Similar results have been reported in previous works (Zhang et al., 2012; Mann et al., 2009), where they have established the statistical convergence of the final averaged solution as well.", "startOffset": 53, "endOffset": 92}, {"referenceID": 0, "context": "Similar results have been reported in previous works (Zhang et al., 2012; Mann et al., 2009), where they have established the statistical convergence of the final averaged solution as well.", "startOffset": 53, "endOffset": 92}], "year": 2014, "abstractText": "In (Yang, 2013), the author presented distributed stochastic dual coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss minimization. Extraordinary performances have been observed and reported for the well-motivated updates, as referred to the practical updates, compared to the naive updates. However, no serious analysis has been provided to understand the updates and therefore the convergence rates. In the paper, we bridge the gap by providing a theoretical analysis of the convergence rates of the practical DisDCA algorithm. Our analysis helped by empirical studies has shown that it could yield an exponential speed-up in the convergence by increasing the number of dual updates at each iteration. This result justifies the superior performances of the practical DisDCA as compared to the naive variant. As a byproduct, our analysis also reveals the convergence behavior of the one-communication DisDCA.", "creator": "LaTeX with hyperref package"}}}