{"id": "1312.1121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Interpreting random forest classification models using a feature contribution method", "abstract": "Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For \"black box\" models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution \"patterns\", are discovered. These patterns represent a standard behaviour of the model and allow for an additional assessment of the model reliability for a new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models.", "histories": [["v1", "Wed, 4 Dec 2013 11:57:53 GMT  (218kb,D)", "http://arxiv.org/abs/1312.1121v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna palczewska", "jan palczewski", "richard marchese robinson", "daniel neagu"], "accepted": false, "id": "1312.1121"}, "pdf": {"name": "1312.1121.pdf", "metadata": {"source": "CRF", "title": "Interpreting random forest classification models using a feature contribution method", "authors": ["Anna Palczewska", "Jan Palczewski", "Richard Marchese Robinson", "Daniel Neagu", "John Moores"], "emails": ["a.m.wojak@bradford.ac.uk", "j.palczewski@leeds.ac.uk", "r.l.marcheserobinson@ljmu.ac.uk", "d.neagu@bradford.ac.uk"], "sections": [{"heading": null, "text": "@ bradford.ac.uk \u2020 j.palczewski @ leeds.ac.uk \u0445 r.l.marcheserobinson @ ljmu.ac.uk \u00a7 d.neagu @ bradford.ac.ukar Xiv: 131 2.11 21v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2 Random Forest", "text": "A random forest model (RF) that Breiman has introduced is a collection of tree predictors. Each tree is grown according to the following procedure [6]: 1. The bootstrap phase: select randomly a subset of the training dataset - a local training for the growth of the tree. The remaining samples in the training dataset form a so-called out-of-bag group (OOB) and are used to determine the quality-of-fit.2. The growth phase: grow the tree by selecting the local training at each node according to the value of a variable from a randomly selected subset of variables (the best division) using the tree classification and regression (CART) method [7].3. Each tree is grown to the maximum extent possible. There is no pruning. The bootstrap and growth phases require input of random quantities. It is assumed that these quantities are independently distributed and identically."}, {"heading": "3 Feature Contributions for Binary Classifiers", "text": "nI \"s, so nohc nvo nvo nlrhee\u00fccnlhSe in the eeircnh-eaeaJnlhsrcnlhsAeaeaeaeaeoiSrlhc ni in the eeirlrrcehnlhSe nvo of eeirsn-eaJnlhdcnlhSrlcnlhsrteaeaeaeaeaeaeaeaiSrmnlhc ni of eeirlrlrrlrcnlhSn in the eeirmde nlteeaeaeGt, nn\" n \"n\" n \"n\" n \"nov os os.\" nI \"nI\" s \"D nknan ide evo evo neirmde eeirmde eeirrrmde eeeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeeeeeeeeeeaeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "3.1 Example", "text": "The mentionlcnheeSi rf\u00fc ide nlrF\u00fc nvo nlrf\u00fc ide nlrF\u00fc nvo nlrf\u00fc ide nlrF\u00fc nvo nvo nlrf\u00fc ide nlrF\u00fc nvo nlrf\u00fc ide nlrF\u00fc nvo nlrf\u00fc nvo nlrf\u00fc the nlrf\u00fc-eaeaeFnlrFnlrFnlrFngne\u00fceaeF\u00fc nlrf\u00fc the nlrf\u00fc-eaeFnlrFnlrFnlrFnlrf\u00fc-eaeaeFnlrf\u00fc-eaeeeoiugnngn nlrf\u00fc the nlrf\u00fc the nlrf\u00fc the nlrrf\u00fc-eaeaeFrf\u00fc-eaeFrf\u00fc-eaeFnlrf\u00fc-eFnlrf\u00fc nlrnlrf\u00fc the nlrnlrf\u00fc nlrnlrf\u00fc nlrfu nlrfu nlrfu nlrfu nlrfu nlrfu nlrfu nlrfu nlrfu nlrfu nlrf\u00fc nlrfu nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlrf\u00fc nlf\u00fc nlrf\u00fc nlf\u00fc nlrf\u00fc nlrfu nlf\u00fc nlrf\u00fc nlrfu nlf\u00fc nlrf\u00fc nlrfu nlf\u00fc nlrfu nlrfu nlfu nlrfu nlrfu nlrfu nllrfu nlrfu nl"}, {"heading": "4 Feature Contributions for General Classifiers", "text": "If K > 2, the sentence \u2206 K = Y does not mean that the division of the parents into the parents is described above. Therefore, we generalize the quantities introduced in the previous section to a multidimensional case. Y nmean in a node n is an element of \u2206 K, whose K-th threshold, k = 1, 2,.., K, defines the number of elements of a set, k = 2, when selecting an instance from a local education, the probability that this instance is given in the class Ck is equal to the coordinate of the vector Y nmean., that the number of elements of a set is defined. Hence, when selecting an instance from a local education, the probability that this instance is given in the class Ck is equal to the coordinate of the vector Y nmean. Local increase LI c f is generalized analogously to a multidimensional case: LIcf."}, {"heading": "5 Analysis of Feature Contributions", "text": "Feature contributions provide the opportunity to understand mechanisms that lead the model to certain predictions. This is important for chemical or biological applications where additional knowledge of forest decision-making can influence the development of new chemical compounds or explain their interactions with living organisms. Feature contributions can also be useful for assessing the reliability of model predictions for invisible cases and provide complementary information on forest tuning outcomes. This section proposes three techniques to find patterns of how a random forest uses available features and links these patterns to forest predictions."}, {"heading": "5.1 Median", "text": "The median of a sequence of numbers is such a value that the number of elements is greater than it and the number of elements smaller than it is identical. If the number of elements in the sequence is odd, this is the central element of the sequence. Otherwise, it is customary to take the middle between the two most central elements. In statistics, the median is an estimate of expectation that is less influenced by outliers than the mean of the sample. We will use this property of the median to find a \"standard level\" of feature contributions for representatives of a particular class. This standard level will facilitate an understanding of which characteristics are decisive for the classification. It can also be used to assess the reliability of the forest prediction for invisible instability.For a given random forest model, we select those instances from the training dataset that are correctly classified. We calculate the median contributions of each characteristic separately for each class."}, {"heading": "5.2 Cluster Analysis", "text": "In this context, it should be noted that the individual groups are groups that are able to confine themselves to one group that is able to confine itself to one group, as well as groups that are able to confine themselves to one group and groups that are able to confine themselves to one group, and these groups are able to confine themselves to a group that is able to confine itself to one group, namely to a group of groups that are able to confine themselves to one group."}, {"heading": "5.3 Log-likelihood", "text": "Our method has probability roots and we assume that we will first provide a different type of contributions. Using a popular k-mean cluster method for each class, vectors corresponding to the contributions of instances in the training dataset - in groups that minimize the Euclidean distance from the center in each group. Figure 3 shows a box plot of contributions to characteristics in a core cluster in a hypothetical random forest model. Note that some characteristics within a cluster are stable - the height of the box is small. Others (F1 and F4) exhibit higher variability. Therefore, one would expect the same divergence of contributions to characteristics F3 and F4 to be treated differently from their mean value. It is more significant for characteristic F3 than for characteristic F4. Unfortunately, this is not taken into account when considering euclidean distance. Here, we propose an alternative method of assessing the distance from the cluster center that takes into account the variation of characteristics within a cluster."}, {"heading": "6 Applications", "text": "In this section, we will show how the techniques from the previous section can be applied to improve understanding of a random forest model. We will consider an example of a binary classifier using the UCI Breast Cancer Wisconsin Dataset [1] (BCW Dataset) and an example of a generic classifier for the UCI Iris Dataset [3]. We will supplement our studies with a robustness analysis. 2A covariance matrix of the feature contribution provides F (F + 1) / 2 unique entries, with F being the number of features. This value is usually larger than the size of a cluster, making it impossible to retrieve useful information about the dependency structure of feature contributions. Applying more advanced methods, such as main component analysis, is left to future research."}, {"heading": "6.1 Breast Cancer Wisconsin Dataset", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6.2 Cluster Analysis and Log-likelihood", "text": "This led to the determination of three clusters for class 0 and three clusters for class 1. However, the structure and size of the clusters is presented in Table 4. Each class has a large cluster: Cluster 3 for class 0 and Cluster 2 for class 1. Both have a greater concentration of points around the cluster center (small mean distance) than the remaining clusters. This suggests that there is exactly one core cluster that corresponds to a class. This explains the success of the median-based analysis, since the vectors of the medians near the centers are unique core clusters. Figure 7 provides support for our interpretation of the core clusters. The left panel shows the box action of the trees belonging to the class 0 among the training instances, a value close to a prediction for which the forest is almost the case."}, {"heading": "6.3 Iris Dataset", "text": "In this area, we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves, that we are able to accomplish our goals, and that we are able to accomplish our goals, \"he said.\" We will be able to accomplish the goals that we have set ourselves, \"he said.\" We will be able to accomplish the goals that we have set ourselves in order to accomplish the goals that we have set ourselves. \""}, {"heading": "6.4 Robustness Analysis", "text": "For the validity of the study of feature entries, it is critical that the results are not artifacts of a particular realization of a random forest model, but that they convey actual information held by the data. We therefore propose a method for the robustness analysis of feature entries. In fact, we use the UCI Breast Cancer Wisconsin Dataset, which was examined in Subsection 6.1, as an example. We removed instance number 3 from the original dataset to perform tests with an invisible instance. We generated 100 random forest models with 500 trees, with each model using an independent randomly generated training set of 379 \u2248 2 \u00b7 568 instances. The remainder of the dataset for each model was used for its validation. The average model accuracy was 0.963. For each model generated, we collected medians of feature entries separately for training and test datasets and each class. The variation of these quantities over the castes for a class and training dataset is presented using the models for the 1 and training dataset."}, {"heading": "7 Conclusions", "text": "Feature contributions offer a novel approach to black box model interpretation. They measure the impact of variables / features on prediction outcomes and explain why a model makes a particular decision. In this work, we expanded the feature contribution method from [12] to random forest classification models and proposed three techniques (median, cluster analysis and log likelihood) to find patterns in the use of available features in the random forest. Using UCI benchmark data sets, we demonstrated the robustness of the proposed methodology. We also demonstrated how feature contributions can be applied to understand the dependence between instance characteristics and their predicted classification and to evaluate the reliability of the prediction. We also examined the relationship between feature contributions and standard metrics of variable importance. The software used in empirical analysis was supported in R as an add-on for the Random Forest package (RFC 854 is currently partially implemented under the RFC / BB 30r1)."}], "references": [{"title": "How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert Muller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Interpretation of nonlinear qsar models applied to ames mutagenicity data", "author": ["Lars Carlsson", "Ernst Ahlberg Helgee", "Scott Boyer"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Clifford Stein", "Ronald L. Rivest", "Charles E. Leiserson"], "venue": "McGraw-Hill Higher Education,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Principles of data mining", "author": ["David J. Hand", "Padhraic Smyth", "Heikki Mannila"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Visual interpretation of kernel-based prediction models", "author": ["Katja Hansen", "David Baehrens", "Timon Schroeter", "Matthias Rupp", "Klaus- Robert Muller"], "venue": "Molecular Informatics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Interpretation of qsar models based on random forest methods", "author": ["Victor E. Kuz\u2019min", "Pavel G. Polishchuk", "Anatoly G. Artemenko", "Sergey A. Andronati"], "venue": "Molecular Informatics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Classification and regression by randomforest", "author": ["Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Mining of Massive Datasets", "author": ["Anand Rajaraman", "Jeffrey D. Ullman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Interpreting linear support vector machine models with heat map molecule coloring", "author": ["Lars Rosenbaum", "Georg Hinselmann", "Andreas Jahn", "Andreas Zell"], "venue": "Journal of Cheminformatics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Conditional variable importance for random forests", "author": ["Carolin Strobl", "Anne-Laure Boulesteix", "Thomas Kneib", "Thomas Augustin", "Achim Zeileis"], "venue": "BMC Bioinformatics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Best practices for qsar model development, validation, and exploitation", "author": ["Alexander Tropsha"], "venue": "Molecular Informatics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "Good practice of model development [17] involves: 1) data analysis 2) feature selection, 3) model building and 4) model evaluation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "But, how to interpret an existing model? How to analyse the relation between predicted values and the training dataset? Or which features contribute the most to classify a specific instance? Answers to these questions are considered particularly valuable in such domains as chemoinformatics, bioinformatics or predictive toxicology [15].", "startOffset": 332, "endOffset": 336}, {"referenceID": 2, "context": "In [8], the authors present a method for a local interpretation of Support Vector Machine (SVM) and Random Forest models by retrieving the variable corresponding to the largest component of the decision-function gradient at any point in the model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Interpretation of classification models using local gradients is discussed in [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "A method for visual interpretation of kernel-based prediction models is described in [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Another approach, which is presented in detail later, was proposed in [12] and aims at shedding light at decision-making process of regression random forests.", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "However, in [16], the authors showed that the above measures are biased in favor of continuous variables and variables with many categories.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "propose in [12] a new technique to calculate a feature contribution, i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "This new approach was positively tested in [12] on a Quantitative Structure-Activity (QSAR) model for chemical compounds.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "The procedure from [12] for the computation of feature contributions applies to random forest models predicting numerical observed values.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "The difficulty of achieving this aim lies in the fact that a discrete set of classes does not have the algebraic structure of real numbers which the approach presented in [12] relies on.", "startOffset": 171, "endOffset": 175}, {"referenceID": 1, "context": "node according to the value of one variable from a randomly selected subset of variables (the best split) using classification and regression tree (CART) method [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "An element p \u2208 \u2206K is uniquely represented by its first coordinate p1 (p2 = 1 \u2212 1The distribution \u0176i is calculated by the function predict in the R package randomForest [13] when the type of prediction is set to prob.", "startOffset": 168, "endOffset": 172}, {"referenceID": 3, "context": "We propose to use DFS (depth first search [9]) to traverse the tree and compute the vector Y n mean once a training set for a node n is determined.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Clustering is an approach for grouping elements/objects according to their similarity [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": ", the Akaike information criterion (AIC), the Bayesian information criterion (BIC) or the Elbow method [10, 14].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": ", the Akaike information criterion (AIC), the Bayesian information criterion (BIC) or the Elbow method [10, 14].", "startOffset": 103, "endOffset": 111}, {"referenceID": 6, "context": "In this work, we extended the feature contribution method of [12] to random forest classification models and we proposed three techniques (median, cluster analysis and log-likelihood) for finding patterns in the random forest\u2019s use of available features.", "startOffset": 61, "endOffset": 65}], "year": 2013, "abstractText": "Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For \u201cblack box\u201d models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution \u201dpatterns\u201d, are discovered. These patterns represent a standard behaviour of the model and allow for an additional assessment of the model reliability for a new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models. \u2217a.m.wojak@bradford.ac.uk \u2020j.palczewski@leeds.ac.uk \u2021r.l.marcheserobinson@ljmu.ac.uk \u00a7d.neagu@bradford.ac.uk 1 ar X iv :1 31 2. 11 21 v1 [ cs .L G ] 4 D ec 2 01 3", "creator": "TeX"}}}