{"id": "1312.1277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Bandits and Experts in Metric Spaces", "abstract": "In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions.", "histories": [["v1", "Wed, 4 Dec 2013 18:48:00 GMT  (101kb)", "https://arxiv.org/abs/1312.1277v1", "This manuscript is a merged and definitive version of (R. Kleinberg, Slivkins, Upfal: STOC 2008) and (R. Kleinberg, Slivkins: SODA 2010), with a significantly revised presentation"], ["v2", "Thu, 19 Nov 2015 14:26:27 GMT  (149kb)", "http://arxiv.org/abs/1312.1277v2", "This manuscript is a merged and definitive version of (R. Kleinberg, Slivkins, Upfal: STOC 2008) and (R. Kleinberg, Slivkins: SODA 2010), with a significantly revised presentation"]], "COMMENTS": "This manuscript is a merged and definitive version of (R. Kleinberg, Slivkins, Upfal: STOC 2008) and (R. Kleinberg, Slivkins: SODA 2010), with a significantly revised presentation", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["robert kleinberg", "aleksandrs slivkins", "eli upfal"], "accepted": false, "id": "1312.1277"}, "pdf": {"name": "1312.1277.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we study a very general setting for the multi-arm bandit problem, in which strategies form a metric space, and the payout function fulfills a Lipschitz state in terms of metrics. We refer to this problem as the Lipschitz-MAB problem. We present a solution to the multi-arm bandit problem in this setting. That is, for each metric space we define an isometric invariant that is limited from below the performance of the Lipschitz-MAB algorithms for this metric space, and we present an algorithm that comes arbitrarily close to this limit. In addition, our technique provides even better results for benign payout functions. We also address the full feedback (\"best expert\") version of the problem, where after each round the payouts from all arms are evident."}, {"heading": "1 Introduction", "text": "In a multi-armed bandit problem, an online algorithm must be selected from a range of possible strategies (also referred to as \"weapons\") in a sequence of studies to maximize the overall impact of the chosen strategies, which are the most important theoretical tool for modeling the exploration and exploitation of goals inherent in sequential decision-making. (Kleinmann and Cesa-Bianchi, 2012) Bandit problems have been studied intensively for decades (Thompson, 1933; Robbins, 1952; Berry and Fristedt, 1985; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012), bandit problems are increasingly having visible effects on computer science due to their diverse applications, including theory of learning in games. The performance of a multi-armed bandit algorithm is often evaluated in terms of its regret, defined as the gap between the expected triggering of the algorithm and the optimal strategy."}, {"heading": "1.1 Prior work", "text": "While our work addresses the Lipschitz-MAB problem in general metric spaces, specific cases of the problem are implicit in earlier work on the continuum-armed bandit problem (Agrawal, 1995; Auer et al., 2007; Cope, 2009; Kleinberg, 2004) - which corresponds to the space [0, 1] under metric 1, d \u2265 1 - and the experimental work on \"Bandits for Taxonomies\" (Pandey et al., 2007a), which corresponds to the case in which (X, D) is a tree metric (3), in which Hazan and Megiddo (2007) consider a contextual bandit conversion using a metric space instead of weapons. Before describing our results in more detail, it is helpful to put them into context by presenting the near optimal boundaries for the one-dimensional continuum-armed bandit problem, a first problem formulated in Agrawal."}, {"heading": "1.2 Initial result", "text": "We make an initial observation that the analysis of the UniformMesh algorithm in Theorem 1.1 is based only on the congruence properties of metric space and cannot be extended to its real structure and (with minor changes) to any metric space of constant congruence dimension. Theorem 1.2. Let us consider the Lipschitz-MAB problem to a metric space of congruence dimension d \u2265 0. There is an algorithm whose regret satisfies in any case p (x, y) = 0. If metric space (X, D) is understood from the context in which \u03b3 = d + 1d + 2.1Specifically, it is a pseudometric problem because some pairs of different points x, y \u00b2 X satisfy coverage D (x, y) = 0. If metric space (X, D) is understood from the context, we can also refer to \u00b5 as an example."}, {"heading": "1.3 Present scope", "text": "This work is a comprehensive study of the Lipschitz MAB problem in the arbitrary metric of regret. While regret in Theorem 1.1 is essentially optimal when the metric space (0, 1), that is, it is remarkably strange that it is reached by such a simple algorithm as UniformMesh. Specifically, the algorithm ignores this information after choosing its initial mesh strategy. Is this really the best algorithm theory that raises further grounds for suspicion: It is based on a constructed, highly singular payout function that scales between constant at some distance scales and very steep at other (much smaller) distances to create a multi-scalable phenomenon that almost exceeds the usability of the information contained?"}, {"heading": "1.4 Our contributions: Lipschitz MAB problem", "text": "We admit a complete solution (Q1) by describing a much better benign problem for each metric problem (X, D). In addition, we also give a satisfactory answer (Q2); our solution is arbitrarily close to the optimum in terms of the observed payouts. It combines the technique used in earlier bandit algorithms such as UCB1 (Auer et al., 2002a) with a novel adaptive development step that uses the past to reform mesh (\"zoom in\") in regions with high observed campaigns."}, {"heading": "1.5 Accessing the metric space", "text": "The simplest way to interpret our theorems is to ignore conversion details and interpret an \"algorithm\" to mean an abstract rule of decision, i.e. a (possibly randomized) Borel measurable function that maps the history of past observations to an arm x x x x X played in the current period. All our theorems are valid under this interpretation, but they can also lead to precise algorithmic results, provided the algorithm gets adequate oracle access to metric space. The zooming algorithm requires only a covering oracle that requires a finite collection of open spheres and either declares that they cover X or output an exposed point."}, {"heading": "1.6 Map of the paper", "text": "We discuss related work in Section 2. In particular, there is a considerable amount of follow-up work that we will examine in Section 2.1. Preparatory work will be presented in Section 3, including a sufficient background on metric topology and dimensionality concepts and proof of original observation (Theorem 1.2). In the rest of the paper, we will present our technical results. Section 4 deals with benevolent payout functions: the zoom algorithm and its extensions. Section 5 deals with the per-metric optimal algorithms with polynomial regret. Section 6 defines the full feedback version (Lipschitz experts) and reviews our contributions. Section 7 deals with the dichotomy between (sub) logarithmic and \u221a t regret. Section 8 studies for which metric spaces the Lipschitz bandits / expert problem o (t) -traceable. Section 9 deals with the appendix regret algorithms for Lipschitz experts."}, {"heading": "2 Related work and follow-up work", "text": "\"I think it's a good thing that we're going to be able to have some of the best players in the world here, and I think we're going to be able to have some of the best players in the world here,\" he said, adding that he'd like to see more players coming through the ranks."}, {"heading": "2.1 Follow-up work", "text": "In fact, it is the case that you are able to play by the rules."}, {"heading": "3 Preliminaries", "text": "This section contains various definitions that essentially make the work autonomous (the only exception being ordinals), and in particular the work uses terms from general topology that are typically included in any introductory text or course on the subject."}, {"heading": "3.1 Problem formulation and notation", "text": "The Lipschitz-MAB problem is a triple problem (X, D, \u00b5), where (X, D) is a metric space and (X, D) a Lipschitz function on (X, D) with Lipschitz constant 1. (In other words: \u00b5 fulfils Lipschitz condition (1). (X, D) is disclosed to an algorithm, whereas (X, D) in each round receives at most one strategy x, (X, D) and (X) a reward f (x), which is chosen with expectation (x) independently of any distribution Px. Without loss of universality, the diameter of (X, D) is at most 1.Through the paper (X, D) and \u00b5 each denotes a metric space of diameter \u2264 1 and a Lipschitz function as above."}, {"heading": "3.2 Metric topology and set theory", "text": "Let X be a set and let (X, D) be a metric space. An open sphere with radius r around the point x-X is B (x, r) = {y-X: D (x, y) < r}. The diameter of a set is the maximum distance between two points in this set.A cauchy sequence in (X, D) is a sequence, so for each \u03b4 > 0, there is an open ball with radius \u03b4 containing all but an infinite number of points in the sequence.We say X is complete when each cauchy sequence has a boundary point in X. For two cauchy sequences x = (x1, x2,.) and y = (y1, y2,.) the distance d (x, y) = limi \u2192 d (xi, yi) is well defined. Two cauchy sequences are declared equal if their distance is 0. The equivalence classes of the cauchy sequences form a metric space (X), D."}, {"heading": "3.3 Dimensionality notions", "text": "The basic concepts are the coverage dimension, which is a version of the fractal dimension based on the coverage of numbers. We will also use several refinements of the coverage dimension of Y that are matched to the coverage dimension of Y, each covering less than r, this coverage Y. The minimum number of subsets in an r coverage is referred to as the r coverage number of Y and denoted Nr coverage dimensions of Y. The coverage dimension of Y is a collection of subsets of Y that are strictly less than r, this coverage Y. The minimum number of subsets in an r coverage is referred to as the r coverage number of Y and denoted Nr coverage dimensions of Y. The coverage dimension of Y with multiple c, denoted COVc (Y), is the infimum of all d covers 0, so that Nr (Y) coverage size for each > 0."}, {"heading": "3.4 Concentration inequalities", "text": "We use an elementary concentration inequality known as Chernoff boundaries. In the literature, there are several formulations; the one we use comes from Mitzenmacher and Upfal (2005).Theorem 3.3 (Chernoff boundaries Mitzenmacher and Upfal (2005).Let us i.i.d. consider random variables Z1,.., Zn with values in [0, 1].Let Z = 1 n \u0445 n i = 1 Zi be their average, and let us leave E = E [Z]. Then: (a) Pr [| Z \u2212 icha] < 2 exp (\u2212 ichn\u04412 / 3) for any \u0421\u0430\u043d\u0430\u043d\u043d\u043d\u043d\u0438\u0439 (0, 1). (b) Pr [Z > a] < 2 \u2212 an for any a > 6\u0439."}, {"heading": "3.5 Initial observation: proof of Theorem 1.2", "text": "We extend the algorithm UniformMesh from metric space ([0, 1], [d1]) to any metric space of the coverage dimension d. The algorithm is parameterized by metric space and divides time into phases of exponentially increasing length. In fact, during each phase i it executes a | Si | armed bandit algorithm on the arms in Si. To specify this, let us say that we use UCB1 (any other bandit algorithm with the same guarantee of remorse) and each phase i takes 2i rounds. The parameter i is optimal for d and the phase duration T; the optimal value turns out to be D = O (T \u2212 1 / d + 2). The algorithm can be analyzed using Kleinberg 3.4 technology."}, {"heading": "4 The zooming algorithm for Lipschitz MAB", "text": "This section is on the zooming algorithm, which uses adaptive refinement to take advantage of the benign input inputs. We specify and analyze the algorithm and derive a number of extensions from it. The zooming algorithm proceeds in phases i = 1, 2, 3,.. Each phase takes 2i rounds. Let's define the algorithm for a single phase iph of the algorithm. For each arm x (x) = 0. Note that both quantities are known to the algorithm. Define the confidence radius of arm x at time t, and let \u00b5t (x) be the corresponding average reward. Define \u00b5t (x) = 0 if nt (x) = 0. Define the confidence radius of arm x at time. Define the confidence radius of arm x at time t asrt (x): = 8 iph 1 + nt (x)."}, {"heading": "4.1 Analysis of the zooming algorithm", "text": "The only difficulty is to determine a suitable application of Chernoff boundaries along the connection boundary. Let T = 2iph iph.Fix the duration of a given phase with a probability of at least 1 \u2212 4 \u2212 iph.Proof. Remember that every time an algorithm plays arm x, the payout is i.e. by some distribution variables Px. Definitative variables Zx, s for 1 \u2264 s \u2264 s the duration of a given phase iph.Fix any arm x. Remember that every time an algorithm plays arm x, the payout is i.e. by some distribution variables Px. Definitative variables Zx, s for 1 \u2264 s \u2264 T is as follows: for s \u2264 n (x), Zx, s is the payout of the s-th ph, and for the s-th time x is played, and for s > n (x) it is an independent sample of Px."}, {"heading": "4.2 Extension: maximal expected payoff close to 1", "text": "We get a sharper limit of regret that (8) matches and becomes much smaller when the optimal reward is (3) x > confidence. < < (3) The key ingredient here is a more sophisticated confidence radius. (13) The confidence radius in (13) behaves as well as rt (up to constant factors) in the worst case: r (x). < (3) O (iph) nt (x), and gets much better when it (x) is close to 1: r (x). (iph) nt (x). Note: The right side of (13) can be calculated from the observable data; in particular, it does not require knowledge of the magnitude of theorem 4.10. Consider an instance of the lip-tension situation of (3) MAB relaxing."}, {"heading": "4.3 Application: Lipschitz MAB with a \u201ctarget set\u201d", "text": "We are looking at a version of the Lipschitz-MAB problem where the expected reward of each arm x = = is fixed by the distance between that arm and a fixed target rate S-X, which is not transferred to the algorithm. Here, the distance is defined as D (x, S), with each arm x (x, y). The motivating example is \u00b5 (x) = max (0, \u00b5) \u2212 D (x, S). Generally, we assume that \u00b5 (x, S))) is for each arm x (x, y)), for a known, non-increasing function f: [0, 1] \u2192 [0, 1]. We call this version the target MAB problem with objective S and the form f.1717171717 note that the payout function \u00b5 does not necessarily meet the Lipschitz condition with respect to D. If f (z) = high."}, {"heading": "4.4 Application: mean-zero noise with known shape", "text": "form of P. In these examples, we cannot limit the payout distributions to having a limited supply. 18Normal distributions may start with the most natural example, when the noise distribution P is the 0-mean normal distribution. Then, we can use the trust radius r (\u00b7) (\u00b7) = the standard deviation of P. (8) multiply by the real distribution (8).In fact, this result can be generalized. (\u00b7) Where we multiply the standard deviation of P. (8) by the pure distribution."}, {"heading": "5 Optimal per-metric performance", "text": "This section deals with the question (Q1) raised in Section 1.3: What is the best possible algorithm for the Lipschitz MAB problem on a given metric space (X, D). We look at the worst-case regret of a given algorithm over all possible problem cases (X, D).20 We focus on minimizing regret in such a way that for each payout function \u00b5 the regret of the algorithm is so great that the smallest dimension of the algorithm 0 is so large that one can reach the exponential sections \u2265 t0 (\u00b5). With regard to Theorem 1.2, we will use a more focused notation: we define the regret dimension of an algorithm on (X, D) as essentially the smallest dimension d-0 is so large that one can reach the exponential quantity + 1d + 2. Definition 5.1. Consider the Lipschitz MAB problem on a given metric space (X) D."}, {"heading": "5.1 Lower bound on regret dimension", "text": "It is well known (Auer et al., 2002b) that a worst-case instance of the K-armed bandit problem consists of K-1 arms with identical payoff distributions, and one that is slightly better. We refer to this as a \"needle-in-a-haystack\" instance. Our lower limit is based on a multi-scale needle-in-a-haystack instance, in which there are K-splintered open sentences, and K-1 of them consists of arms with identical payoff distributions, but in the remaining open instance there are arms whose payoff is slightly better. Furthermore, this special open group contains K-K fragmented subsets, of which only one contains arms that are superior to the others, and so on down through infinite levels of recursion. In more precise terms, we need the existence of a certain structure: an infinitely deep-rooted tree whose nodes x-shape correspond to balls in metric space, so that all the metric balls are the children."}, {"heading": "It follows that the regret dimension of any algorithm is at least d.", "text": "For our purposes, a weaker version of (21) is sufficient: for each algorithm A there is a payout function p = 13, so that the event holds in (21) (meaning that DIM (A) \u2265 d). In Section 7, we will also use this lower limit for d = 0. In the rest of this subsection, we will prove that Lemma 5.4.2 is the problem in (0, 1) surrounded by a metric space (X) with a ball tree, we construct a distribution P via payout functions as follows: (x0, r0) define the bump Fw: X \u2192 [0, 1] byFw (x) = {min {r0 \u2212 D (x, x0), r0} if x B (x0, r0), 0 otherwise. (22) This function represents a \"bump\" supported on B (x0, r0)."}, {"heading": "5.1.1 Lipschitz-continuity of the lower-bounding construction", "text": "In this subsection, we prove that the function of Lipschitz is specified for (X, D), for each end w of the ball tree (1). In fact, we specify and prove a more general problem, in which the bump functions are summed up over all tree nodes with arbitrary weights in [\u2212 1, 1]. Let V be the set of all tree nodes. For each given weight vector: V \u2192 [\u2212 1, 1] and an absolute constant c0 (0, 12] define the payout functions of D = c0 + 13 (w) \u00b7 Fw, where Fw is the bump function of (22). Then, we are the bump function of Lipschitz on (X, D). In the rest of this subsection, we prove Lemma 5.8. (The proof for the special case is Fw), where Fw is the bump function of (22)."}, {"heading": "5.2 The max-min-covering dimension", "text": "We need a more nuanced conception, which we call the maximum coverage dimension, to ensure that each of the open sets resulting from the construction of the ball tree has enough subsets to continue the next level of recursion. (D) Furthermore, this new conception is an intermediary that connects our lower limit with the upper limit that we will develop in the upcoming subsections. (D) For a metric space (X, D) and subsets Y (X) that we define, we will define. (Y) = inf {COV (U): The lower limit is not empty and open in the coming subsections. (X)}, MaxMinCOV (X) = sup {MinCOV (Y).We call it, or the minimum coverage dimension of X.The infimum via the open U in the definition of Y."}, {"heading": "5.3 Special case: metric space with a \u201cfat subset\u201d", "text": "The two examples are as follows: \u2022 a high-grade node on each level; the high-grade nodes form a path called a bold end. \u2022 2i high-grade nodes on each level of the tree have the same degree, and this degree is such that the tree contains 4i nodes on each level i: \u2022 a high-grade node on each level; the high-grade nodes form a path called a bold end. \u2022 2i high-grade nodes on each level. \u2022 The high-grade nodes form a path called a fat end. \u2022 2i high-grade nodes on each level i."}, {"heading": "5.4 Warm-up: taking advantage of fat subsets", "text": "So we are not so far away that we are engaging in a concealment of weapons. (...) We are not so far away that we are engaging in a concealment of weapons. (...) We are not so far away that we are engaging in a concealment of weapons. (...) We are not so far away that we are maintaining a concealment of weapons. (...) We are not so far away that we can maintain a concealment of weapons. (...) We are so far away that it requires too many weapons. (...) We are so far away that we can maintain a concealment of weapons. (...) We are so far away that we can maintain a concealment of weapons. (...) We are so far away that we have to maintain a concealment of weapons. (...)"}, {"heading": "5.5 Transfinite fatness decomposition", "text": "The fact that d = MaxMinCOV \u03b2 (X) < COV (X) \u03b2 = \u03b2 \u03b2 (then) implies the existence of a d-fatness decomposition of any finite length. Instead, we prove the existence of a much more general structure, which we then use to design the per-metric optimal algorithm. However, this structure is a transfinite sequence of subsets of X, i.e. a transfinite sequence indexed by ordinal numbers and not by integers.27Definition 5.26. Fixing a metric space (X, D). A transfinite d-fatty decomposition of the length \u03b2, where \u03b2 is an ordinal sequence, is a transfinite sequence {S\u03bb}. 0 \u2264 d of the closed subsets of X, which read: (a) S0 = X, S\u03b2 + 1 = 1 =."}, {"heading": "6 The Lipschitz experts problem", "text": "We turn our attention to the problem of the Lipschitz experts: the complete feedback of the Lipschitz method to the Lipschitz method. We turn our attention to the problem of the Lipschitz method: the complete feedback of the Lipschitz method to the Lipschitz method. (We point to P as problem solution.) We point to P as problem solution if the metric structure of the algorithm is not known. (We point to P as problem instance if the metric structure of the algorithm is clear. (X, D). (X, D)] The metric structure of the algorithm is known. (We point to P as problem instance if the metric space (X, D) is clear. (X, D) The metric structure of the algorithm is known. (X, D) The metric structure of the algorithm is not known. (We point to P as problem instance if the metric space (X, D) is clear."}, {"heading": "In both cases, (sub)logarithmic tractability occurs if and only if X is countable.", "text": "We also show two auxiliary results: the (log t) recalcitrance for Lipschitz bandits in infinite metric spaces (theorem 1.7) and an algorithmic result using a more intuitive oracle access to metric space (for metric spaces of the finite Cantor-Bendixson scale, a classic term from point-set topology).The section is structured as follows: We offer a common analysis for Lipschitz bandits and Lipschitz experts: an overview in Section 7.1, the lower limit in Section 7.2, and the algorithmic result in Section 7.3. Both auxiliary results are in Section 7.4 and Section 7.5 respectively."}, {"heading": "7.1 Regret dichotomies: an overview of the proof", "text": "We identify a simple topological property (existence of a topological well-being order) that results in the algorithmic result and another topological property (existence of a perfect subspace) that contains the lower boundary. If such a problem exists, X is called perfect if it does not contain isolated points. A topological well-being of X is called well-ordered if and only if its metric topology is well-ordered. Perfect spaces are a classical term in point-set topology. Topological well-orders are implicit in the work of Cantor (1883), but the particular definition given here is new. The evidence for Theorem 7.1 consists of three parts: the algorithmic result for a compact space that connects space with two spatial boundaries."}, {"heading": "7.2 Lower bounds via a perfect subspace", "text": "In this section, we prove the following metric spaces with a perfect space, in which each space has exactly two spaces. (iii) In this section, we prove the following metric spaces with a perfect space. (ii) In this section, we prove the following metric spaces with a perfect space. (ii) In this section, we prove the following metric spaces with a perfect space. (ii) In this section, we prove ourselves to be perfect space. (ii) In each such case, there is a distribution P over a problem, so that for all experts there is an algorithm we call havePr \u00b5. (R, \u00b5) (t) = O\u00b5 (g) = 0. (34) We construct the desired distribution over problem substances. First, we use the existence of a perfect subspace to construct a ball tree (cf.).Lemma 7.5. For each metric space, there is a perfect subspace in which we have a perfect subspace."}, {"heading": "7.3 Tractability for compact well-orderable metric spaces", "text": "In this section we will prove the most important algorithmic results. Theorem 7.13. Consider a compact, well-ordered metric space (X, D). Then: (a) the Lipschitz MAB problem on (X, D) is f -tractable for each f + 0 (log t); (b) the Lipschitz MAB / expert problem on a compact metric space (X, D) is 1-tractable, even with a double feedback. We will present a common exposure for both the bandit and the expert version. Consider the Lipschitz MAB / expert problem on a compact space (X, D) with a topological wellording system and a payout function."}, {"heading": "7.5 Tractability via more intuitive oracle access", "text": "In this section, we show that for a broad family of metric spaces - including, for example, compact metric spaces with a finite number of boundary points - no such oracle is needed: we provide an algorithm that covers metric space via a finite series of oracles. We look at metric spaces with a finite number of boundary points - a classic notion of a point viewed from above. Fix a metric space (X, D) in which there is a sequence of oracles converting to x, then x is called a boundary point. For S (S) denote the limit is set: the set of all boundary points of S. Let lim (S, 0) = S, and lim (S, i)."}, {"heading": "8 Boundary of tractability: proof of Theorem 6.2", "text": "We will prove that a full metric space is compact if and only if it can be covered by a number located in a limited space. We will prove that a full metric space is compact if and only if it covers a compact metric surface.First, we will reduce the theorems to full metric spaces, see Appendix B. In what follows, we will use a basic fact that a full metric space is compact if and only if we reduce the theorems to full metric spaces, see Appendix B. In what follows, we will use a full metric space if and only if there is a full metric space."}, {"heading": "9 Lipschitz experts in a (very) high dimension", "text": "This section deals with polynomic repentance results for Lipschitz experts in metric spaces of (very) high dimension: theorem 6.3, theorem 6.4 and theorem 6.5, as described in section 6."}, {"heading": "9.1 The uniform mesh (proof of Theorem 6.3)", "text": "We begin with a version of the UniformMesh algorithm, which is discussed in the introduction.33 This algorithm, called UniformMeshExp (b), is parameterized by b > 0. It runs in phases. Each phase i lasts for T = 2i rounds and gives its best guess. (At the end of the phase, x * X (which is played throughout phase i + 1.) During phase i, the algorithm selects an x-beating set 34 for X of size in most places (X), for T \u2212 1 / (b + 2). At the end of the phase, x * i is selected as the point in S with the highest sample average. This completes the description of the algorithm. It is easy to see that the regret of UniformMeshExp is naturally described in relation to the log covering dimension (see (32). The proof is based on the Kleinberg argument (2004)."}, {"heading": "9.2 Uniformly Lipschitz experts (proof of Theorem 6.4)", "text": "We now turn our attention to the uniformed Lipschitz expert problem, a limited version of the Lipschitz expert problem in which a problem (X, D, P) fulfills another property that defines each function f (P) itself as a Lipschitz function on (X, D). We show that for this version UniformMeshExp gets a much better guarantee, through a more comprehensive analysis. As we will see in the next section, there is a suitable upper limit for each class of metric spaces. Theorem 9.2. Consider the uniform Lipschitz problem with full feedback. Fix a metric space (X, D) is for each b > LCD so that b \u2212 2, UniformhExp (b \u2212 2) reaches Regret R (t) = O (t1 \u2212 b).Proof."}, {"heading": "9.3 Regret characterization (proof of Theorem 6.5)", "text": "As it turns out, logbook coverage is not the right approach to characterize the optimal regret for the spread of obesity. < B > B > B > B > B > B (X, D) -B (X, D) -B (X, D) -D (X, D) -D (X, D) -D (X) -D (X, D) -D (X, D) -D (X, D) -D (X, D) -D (X, D) -D (X, D) -D) -D (X) -D) -D (X) -D) -D (X) -D (X) -D) -D (X) -D (X) -D) -D (X) -D (D) -D (X) -D) -D (X) -D) -D (X) -D (D) -D (X) -D) -D (D) -D (D) -D (D) -D (D) -D)."}, {"heading": "10 Directions for further work", "text": "Kleinberg et al. (2008c) (i.e. sections 4 and 5 of this paper) introduced the Lipschitz-MAB problem and motivated a number of open questions. Many of these questions have been addressed in subsequent work, including Kleinberg and Slivkins (2010) (i.e. the rest of this paper) and the work described in Section 2.1. In the following, we describe the current state of play of the open questions. First, the adaptive refinement technique from Section 4 can potentially be used in other contexts of explorer exploit learning, where ancillary information on the similarity between the weapons is available. Specific potential applications include Adversarial MAB, Gaussian process bandits, and dynamic pricing. A stronger analysis of this technique also appears possible in the context of ranked bandits (see Slivkins et al. (2013) for details). Second, it is desirable to consider MAB with a more general structure via payout functions, and to consider a specific structure that has a specific leptic structure, for example, which would pose a specific structural problems."}, {"heading": "A KL-divergence techniques", "text": "All lower limits in this work are strong with the term Kullback-Leibler-Divergence (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL) (KL) (KL-Divergence) (KL) (KL-Divergence) (KL-KL) (KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL) (KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL-KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL-KL) (KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL-KL) (KL-Divergence) (KL-KL-KL) (KL-Divergence) (KL-KL) (KL-Divergence (KL-KL) (KL-KL-Divergence) (KL) (KL-KL-KL-Divergence) (KL-KL) (KL-Divergence) (KL-KL-KL) (KL-Divergence) (KL) (KL-"}, {"heading": "B Reduction to complete metric spaces", "text": "In this section, we reduce the Lipschitz-MAB problem to the full metric space.Lemma B.1. The Lipschitz-MAB problem in a metric space (X, d) is f (t) -tractable if and only if it is f (t) -tractable on the completion of X, d). Lipschitz experts will also consider the problem with the double feedback. Proof. Let (X, d) be a metric space with completion (Y, d). Since Y contains an isometric copy of X, we will misuse the notation and consider X as a subset of Y. We will present the evidence for the Lipschitz-MAB problem; for the experts, the evidence is similar. Given an algorithm AX that contains f (t) -tractable for (t) -tractable for (X, d), we may use it as a Lipschitz MAB algorithm for (Y, d) as well."}, {"heading": "C Topological equivalences: proof of Lemma 7.3", "text": "We must remember that there is an equivalent result for compact metric spaces, and there are two implications for any metric spaces: (i), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (i), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (ii), (), (ii), (ii), (), (ii), (), (ii), (ii), (), (ii), (ii), (), (ii), (ii, (), (ii), (), (ii), (ii), (), (ii), (), (ii), (ii), (), (ii), (), (ii, (), (), (), (ii), (ii), (ii), (ii), (, (, (), (), (), (ii), (ii), (ii), (, (, (, (), (), (), (, (), (, (, (), (), (), (ii), (ii), (, (), (), ("}, {"heading": "D Log-covering dimension: an example", "text": "It is an example from Section 6. Fix a metric space (X, D) of finite diameter and covering dimension (1).Leave PX the set of all probability dimensions above X. Let (PX, W1) is the space of all probability dimensions above (X, D) below the Waterstone W1 metric (see footnote 30 on page 47).Theorem D.1. The logical coverage dimension of (PX, W1) is for completeness reasons: for all \u00b5, \u00b5 \"PX,\" the Waterstone W1 metric, a.k.a. the earthmover distance is defined as W1 (1) = inf E (Y, Y)] where the infimum is taken over all common distributions (Y, Y)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in<lb>a sequence of trials so as to maximize the total payoff of the chosen strategies. While the<lb>performance of bandit algorithms with a small finite strategy set is quite well understood,<lb>bandit problems with large strategy sets are still a topic of very active investigation, motivated<lb>by practical applications such as online auctions and web advertisement. The goal of such<lb>research is to identify broad and natural classes of strategy sets and payoff functions which<lb>enable the design of efficient solutions.<lb>In this work we study a very general setting for the multi-armed bandit problem in which<lb>the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with<lb>respect to the metric. We refer to this problem as the Lipschitz MAB problem. We present<lb>a solution for the multi-armed bandit problem in this setting. That is, for every metric space<lb>we define an isometry invariant which bounds from below the performance of Lipschitz MAB<lb>algorithms for this metric space, and we present an algorithm which comes arbitrarily close<lb>to meeting this bound. Furthermore, our technique gives even better results for benign payoff<lb>functions. We also address the full-feedback (\u201cbest expert\u201d) version of the problem, where after<lb>every round the payoffs from all arms are revealed. ACM Categories: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical<lb>Algorithms and Problems; F.1.2 [Computation by Abstract Devices]: Modes of Computation\u2014<lb>Online computation<lb>", "creator": "LaTeX with hyperref package"}}}