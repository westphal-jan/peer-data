{"id": "1312.1737", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2013", "title": "Curriculum Learning for Handwritten Text Line Recognition", "abstract": "Recurrent Neural Networks (RNN) have recently achieved the best performance in off-line Handwriting Text Recognition. At the same time, learning RNN by gradient descent leads to slow convergence, and training times are particularly long when the training database consists of full lines of text. In this paper, we propose an easy way to accelerate stochastic gradient descent in this set-up, and in the general context of learning to recognize sequences. The principle is called Curriculum Learning, or shaping. The idea is to first learn to recognize short sequences before training on all available training sequences. Experiments on three different handwritten text databases (Rimes, IAM, OpenHaRT) show that a simple implementation of this strategy can significantly speed up the training of RNN for Text Recognition, and even significantly improve performance in some cases.", "histories": [["v1", "Thu, 5 Dec 2013 23:53:45 GMT  (82kb,D)", "http://arxiv.org/abs/1312.1737v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j\\'er\\^ome louradour", "christopher kermorvant"], "accepted": false, "id": "1312.1737"}, "pdf": {"name": "1312.1737.pdf", "metadata": {"source": "CRF", "title": "Curriculum Learning for Handwritten Text Line Recognition", "authors": ["J\u00e9r\u00f4me Louradour", "Christopher Kermorvant"], "emails": ["jl@a2ia.com", "ck@a2ia.com"], "sections": [{"heading": "1 Introduction", "text": "The application of interest in these papers is indeed a challenge for all concerned."}, {"heading": "2 A curriculum for Text Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Two tasks when learning to recognize text: Localization and Classification", "text": "In text recognition, it is necessary to locate the characters in order to recognize them. However, in many public databases for handwriting recognition, the positions and text content are given for each page or paragraph, not for the characters. Line localization is relatively easy to achieve by means of automatic line segmentation. But character localization is a more difficult problem, especially in the case of handwritten text, where even people disagree about how to segment the characters. For this reason, a Connectivist Time Classification (NLL) approach, as proposed by [7], is a very practical method to train RNN models effortlessly without intensive labeling. In its CTC approach, [7] efficiently calculates and derives a cost function that represents the negative LogLikelihood (NLL), assuming that all frame probabilities are independent and that all possible alignments are reconcilable. \"In addition, an additional character will not be considered in the blank box,\" but also the blank box for two. \""}, {"heading": "2.2 Building a suitable Curriculum", "text": "One of the reasons for this difficulty in starting to learn is that when initializing with quasirandom model parameters, the RNN has little chance of generating reasonable segmentation. Furthermore, it is clear that the longer the sequences are, the more serious the problem is. In short, it is difficult and inefficient to learn long sequences from the outset. Let's think in the same spirit as [2], let's draw a parallel on how to teach children to read and write. A natural way to do this is to do it step by step (e.g. RNN using CTC): First teach them to recognize characters by showing them isolated symbols, then teach them short words before introducing longer words and phrases.A similar curriculum learning process can be performed when neural networks are optimized by gradient pedigree (e.g. RNN using CTC): Optimize first on a database of isolated characters (then on a database of isolated characters, then on a database of isolated words), and finally on a database of isolated words."}, {"heading": "2.3 Proposal: continuous curriculum", "text": "In practice, it is cumbersome to establish a step-by-step schedule by dividing a database in terms of sequence lengths. Instead, we prefer to draw an example line with the probability of drawing from the training database. We suggest that the idea of defining such a probability in order to examine the training database has already been successfully applied in Active Learning [19, 4]. | If (Xt, Yt) denotes a training sample (a picture together with the corresponding target sequence of labels), we suggest avoiding this sample with the following probability parameter: P\u03bb (pull up (Xt, Yt)) = 1N\u03bb (short-life (Xt, Yt)) = Shortlife (short-life) = Shortlife (Xt, Yt) = Shortlife (short-life) is a normalization constant so that (1) a probability is defined over all available training samples. \u2022 Shortness [0, is a minimum value, like a]."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Databases", "text": "Three public data sets are used to evaluate our system: \u2022 the IAM database, which contains pages of handwritten text [16], \u2022 the Rimeteria database, which is used in two NIST letters, and the translation of the letters (most recently the Rimeteria database)."}, {"heading": "3.3 Performance assessment", "text": "Since we are interested in the convergence rate, we draw convergence curves that represent the evolution of some costs in relation to a progression unit of the training algorithm. In our case, we use Stochastic Gradient Descent [14] and the progression unit could be, for example, the number of training samples searched. This unit is more representative of the computing time than the number of updates, since the inputs are variable length sequences, and we recall that the curriculum strategies tend to process shorter sequences at the beginning of the learning process. We recall that the cost-optimized use of CTC [7] is the minimum computation time than the number of updates, since the inputs are variable length sequences, and we recall that the curriculum strategies tend to process shorter sequences at the beginning of the learning process."}, {"heading": "3.4 Results and analysis", "text": "The convergence curves for learning handwritten text recognition in the three languages, respectively IAM (English), Rimes (French) and OpenHaRT (Arabic), are shown in Figures 2, 3 and 4. They show the progression of the costs on the validation data set, and the vertical lines indicate the best cost values achieved throughout the learning process. All systems use exactly the same RNN topology and optimization process, the only difference being the way in which the training samples are drawn. The basic system, represented by the black solid curves, consists in distributing the data sets differently in each epoch, i.e. drawing the training samples with flat priests and without substitution."}, {"heading": "4 Conclusion", "text": "This paper describes an easy-to-implement strategy for accelerating the learning process, which can ultimately lead to better performance. The basic principle is the creation of a curriculum based on the length of the target sequences. Experimental results show that in the case of the Recurrent Neural Network for text line recognition, which is optimized by stochastic gradient descent, the first learning phase can be drastically shortened and the generalization performance can be improved, especially if the training set is limited. At the same time, the slowness of the last learning phase remains a problem that needs to be investigated in the future. Further research also includes testing our Curriculum Learning method in combination with more sophisticated optimization methods [15, 23]."}, {"heading": "Acknowledgments", "text": "This work was supported by the French Research Agency under the Cognilego ANR 2010 - CORD-013 contract."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Active batch learning with stochastic query-by-forest", "author": ["Alexander Borisov", "Eugene Tuv", "George Runger"], "venue": "JMLR: Workshop and Conference Proceedings,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L. Elman"], "venue": "Cognition, 48:781\u2013799,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent"], "venue": "Journal of Machine Learning Research, JMLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fernandez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fernandez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Icdar 2011: French handwriting recognition competition", "author": ["Emmanuele Grosicki", "Haikal El Abed"], "venue": "In Proc. of the Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["Faisal Khan", "Xiaojin (Jerry) Zhu", "Bilge Mutlu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Flexible shaping: how learning in small steps helps", "author": ["Kai A. Krueger", "Peter Dayan"], "venue": "Cognition, 110:380\u2013394,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "Proc. of the International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "The iam-database: an english sentence database for offline handwriting recognition", "author": ["Urs-Viktor Marti", "Horst Bunke"], "venue": "International Journal on Document Analysis and Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "The A2iA French handwriting recognition system at the Rimes-ICDAR2011 competition", "author": ["Far\u00e8s Menasi", "J\u00e9r\u00f4me Louradour", "Anne-laure Bianne-bernard", "Christopher Kermorvant"], "venue": "In Document Recognition and Retrieval Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research, JMLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Active sampling for class probability estimation and ranking", "author": ["Maytal Saar-tsechansky", "Foster Provost"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Neural network learning control of robot manipulators using gradually increasing task difficulty", "author": ["T.D. Sanger"], "venue": "IEEE Trans. on Robotics and Automation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Stabilize sequence learning with recurrent neural networks by forced alignment", "author": ["Marc-Peter Schambach", "Sheikh Faisal Rashid"], "venue": "In Proc. of the Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "From baby steps to leapfrog: How \u201cless is more\u201d in unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky"], "venue": "In IN NAACL-HLT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "On the utility of curricula in unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar"], "venue": "In Proc. of the Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "At the time being, the most powerful models for this task are Recurrent Neural Networks (RNN) with several layers of multi-directional Long-Short Term Memory (LSTM) units [9, 17].", "startOffset": 171, "endOffset": 178}, {"referenceID": 16, "context": "At the time being, the most powerful models for this task are Recurrent Neural Networks (RNN) with several layers of multi-directional Long-Short Term Memory (LSTM) units [9, 17].", "startOffset": 171, "endOffset": 178}, {"referenceID": 2, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 10, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 17, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 14, "context": "There are other ways to efficiently learn RNN, namely enhanced optimization approaches such as second-order methods [15] or good initialization with momentum [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "There are other ways to efficiently learn RNN, namely enhanced optimization approaches such as second-order methods [15] or good initialization with momentum [23].", "startOffset": 158, "endOffset": 162}, {"referenceID": 6, "context": "Secondly, RNN are used here for an unconstrained \u201cTemporal Classification\u201d task [7, 8], where the length of the sequence to recognize is in general different from the length of the input sequence.", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Secondly, RNN are used here for an unconstrained \u201cTemporal Classification\u201d task [7, 8], where the length of the sequence to recognize is in general different from the length of the input sequence.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "Here we propose to make the training process more effective by using the concept of Curriculum Learning, that has already been successfully applied in the context of deep models and Stochastic Gradient Descent [2].", "startOffset": 210, "endOffset": 213}, {"referenceID": 1, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 21, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 23, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 19, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "This is why a Connectionist Temporal Classification (CTC) approach as proposed by [7] is a very practical way to train RNN models without intensive labeling effort.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "In their CTC approach, [7] efficiently compute and derive a cost function that is the Negative LogLikelihood (NLL), with the assumptions that all the frame probabilities are independent and that all possible alignments are equiprobable.", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "But it also unveils a vague localization of the characters, especially at the beginning of the training process, when the RNN gives quasi-random guesses for the posteriors of the labels (see the CTC Error Signal of Figure 4 in [7]).", "startOffset": 227, "endOffset": 230}, {"referenceID": 20, "context": "Several studies about Text Recognition have revealed that the training process of RNN is particularly long [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Thinking in the same spirit as [2], let us make a parallel with how to teach kids to read and write.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "RNN cannot decode paragraphs, just single lines: the common RNN architectures collapse the 2D input image into a 1D signal just before aligning using CTC [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 18, "context": "The idea of defining such a probability for probing the training database has already been successfully applied in Active Learning [19, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 3, "context": "The idea of defining such a probability for probing the training database has already been successfully applied in Active Learning [19, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 0, "context": "\u2022 shortness \u2208 [0, 1] is a bounded value to represent how easy is a training sample.", "startOffset": 14, "endOffset": 20}, {"referenceID": 15, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 16, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 8, "context": "It is the same as described in [9], except that the sizes of the filters have been adapted to images in 300 dpi: we used 2x2 input tiling, and 2x4 filters in the two sub-sampling hidden layers (which are convolutional layers without overlap between the filters).", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "All the models were optimized using Stochastic Gradient Descent [14]: a model update happens after each training sample (i.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "In our case, we use Stochastic Gradient Descent [14] and the unit of progression could be for instance the number of updates, that is the number of training samples that have been browsed.", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "We remind that the cost optimized using CTC [7] is the Negative Log-Likelihood (NLL), which can be averaged over the number of training sequences.", "startOffset": 44, "endOffset": 47}, {"referenceID": 20, "context": "For instance, techniques to compute a forced alignment [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "The fact that Curriculum Learning can improve generalization performance supports one point mentioned by [6], namely that the networks optimized by stochastic gradient descent are greatly influenced by early training samples.", "startOffset": 105, "endOffset": 108}, {"referenceID": 22, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 214, "endOffset": 220}, {"referenceID": 5, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 214, "endOffset": 220}, {"referenceID": 14, "context": "Further research also includes to experiment our Curriculum Learning procedure in combination with more elaborated optimization methods [15, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 22, "context": "Further research also includes to experiment our Curriculum Learning procedure in combination with more elaborated optimization methods [15, 23].", "startOffset": 136, "endOffset": 144}], "year": 2013, "abstractText": "Recurrent Neural Networks (RNN) have recently achieved the best performance in off-line Handwriting Text Recognition. At the same time, learning RNN by gradient descent leads to slow convergence, and training times are particularly long when the training database consists of full lines of text. In this paper, we propose an easy way to accelerate stochastic gradient descent in this set-up, and in the general context of learning to recognize sequences. The principle is called Curriculum Learning, or shaping. The idea is to first learn to recognize short sequences before training on all available training sequences. Experiments on three different handwritten text databases (Rimes, IAM, OpenHaRT) show that a simple implementation of this strategy can significantly speed up the training of RNN for Text Recognition, and even significantly improve performance in some cases.", "creator": "LaTeX with hyperref package"}}}