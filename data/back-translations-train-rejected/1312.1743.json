{"id": "1312.1743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2013", "title": "Dual coordinate solvers for large-scale structural SVMs", "abstract": "This manuscript describes a method for training linear SVMs (including binary SVMs, SVM regression, and structural SVMs) from large, out-of-core training datasets. Current strategies for large-scale learning fall into one of two camps; batch algorithms which solve the learning problem given a finite datasets, and online algorithms which can process out-of-core datasets. The former typically requires datasets small enough to fit in memory. The latter is often phrased as a stochastic optimization problem; such algorithms enjoy strong theoretical properties but often require manual tuned annealing schedules, and may converge slowly for problems with large output spaces (e.g., structural SVMs). We discuss an algorithm for an \"intermediate\" regime in which the data is too large to fit in memory, but the active constraints (support vectors) are small enough to remain in memory. In this case, one can design rather efficient learning algorithms that are as stable as batch algorithms, but capable of processing out-of-core datasets. We have developed such a MATLAB-based solver and used it to train a collection of recognition systems for articulated pose estimation, facial analysis, 3D object recognition, and action classification, all with publicly-available code. This writeup describes the solver in detail.", "histories": [["v1", "Fri, 6 Dec 2013 00:55:51 GMT  (18kb)", "https://arxiv.org/abs/1312.1743v1", null], ["v2", "Fri, 13 Jun 2014 04:10:06 GMT  (71kb,D)", "http://arxiv.org/abs/1312.1743v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["deva ramanan"], "accepted": false, "id": "1312.1743"}, "pdf": {"name": "1312.1743.pdf", "metadata": {"source": "CRF", "title": "Dual coordinate solvers for large-scale structural SVMs", "authors": ["Deva Ramanan"], "emails": [], "sections": [{"heading": null, "text": "Dual coordinate solvers for large structural SVMs"}, {"heading": "Deva Ramanan", "text": "In this case, a solution can be found to the problem that has arisen in the past."}, {"heading": "1 Generalized SVMs", "text": "We first describe a general formulation of an SVM that includes several common problems such as binary classification, regression, and structured prediction. (Suppose we are able to solve the following optimization problem: argmin wL (w) = 12 + 0 max. (0, lij \u2212 wTxij) (1) ar Xiv: 131 2.17 43v2 [cs.LG] 1 3Ju n20 14We can write the above optimizations as the following square program (QP): argmin w, 2 + N (2).i (2) s.t."}, {"heading": "2 Batch optimization", "text": "We start by describing a solution approach that works in batch processing and requires access to all training examples and all sets of constants for each example. A naive implementation of a dual solution approach would require maintaining an N-N kernel matrix. Liblinear's innovation is the realization that you can implicitly represent the kernel matrix for linear SVMs by maintaining the primary weight vector w, which is much smaller. We show a similar insight to design efficient dual solvers for generalized SVMs of the form (36). Shared Slacks complicate the coordinate-wise updates, but it is important that we describe an optimization plan that is as quick as possible."}, {"heading": "2.1 Tracking w and \u03b1i", "text": "To allow an efficient calculation of the gradient for the next coordinate step, we track the change in w using the CCT condition (5): w: = w + a \u0445 xij for a single dual update; e.g., conditions 1-3 (17) w: = w + a \u0445 (xij \u2212 xik) consider a pair dual update; e.g. condition 4 applies to (18) Similarly, we can track the change in \u03b1i of (10): \u03b1i: = \u03b1i + a \u0445 (19) that only needs to be updated for a single dual update."}, {"heading": "2.2 Dual coordinate-descent", "text": "We offer pseudo code for our general batch optimization algorithm below: {xij, lij} Output: w1, \u03b1ij: = 0, \u03b1i: = 0, w: = 0; / Initialize variables (if not used as arguments) 2 Repeat 3 Randomly pick a dual variable \u03b1ij; 4 Compute Gradient gij from (11); 5 if gij > and \u03b1i = 1 then / / Find another variable when linear constraint is active 6 Random pick another dual variable with same i (\u03b1ik for k 6 = j); 7 Compute a Gradient gij from (15); 8 Update \u03b1ij, w with (16), 9 else if | > then / * Else update single dual variable * / 10 Compute a."}, {"heading": "3 Online learning", "text": "In this section, we assume that the user provides a function for online optimization, since large streaming datasets and at worst. \"Importantly, our online algorithm caches a small number of constraints and is therefore used for problems with a large number of examples i and / or problems with a large number of shared constraints per example j-Ni. The latter is important to solve structured prediction problems where Ni is exponentially large and practically difficult to enumerate explicitly. If applied cyclically to finite datasets, our online algorithm is guaranteed to solve the optimal prediction problems. Loss-augmented inference: To deal with exponentially large Ni algorithms, our online algorithm does not require access to the entire set of constraints {xij, j, j} associated with example i, but rather assumes that this set can be searched efficiently."}, {"heading": "10 end", "text": "11 if UB - LB > tol then / / Optimize over cache if duality gap is violated 12 (w, \u03b1A, LB, UB) = Optimize ({xA, lA, \u03b1A}, tol); 13 A: = A\\ {ij: \u03b1ij = 0}; / / (Optional) Remove non-support vectors from cache 14 end"}, {"heading": "15 end", "text": "Algorithm 2: The above online algorithm performs a learning process within a dataset by maintaining a cache of sample indices A = {(ij)}. At any time, the associated dual variables {\u03b1ij} can define the optimal model for QP defined by the current examples, up to the duality gap tol.Convergence: To investigate the convergence of the online algorithm, let us distinguish between two QP problems. Cached QP is defined by the current examples in cache A. FullQP is the convergence defined by the possibly infinite number of examples in the full dataset, with the online convergence between UB and LB always defining upper and lower limits for the cache QP. FullQP is also a lower limit for the full QP value defined by the convergence."}, {"heading": "4 Theoretical guarantees", "text": "In this section, we briefly refer to some theoretical analyses that are necessary to ensure that the pair and the cyclic-online version of our algorithm approach the global optimum. Specifically, this analysis shows that the convergence of coordinate-by-coordinate updates does not necessarily produce the globally optimal solution, thereby eliminating the need to optimize pairs of dual variables in Alg. 1. Global optimality: It is easier to define the optimality conditions of differentiable, unrestricted convex functions - the gradient of the function must be zero. Our dual objective function is differentiated but limited (7). Boyd shows that optimality conditions are more subtle in such cases [5]. Specifically, \u03b1 is optimal when and only when there is no practicable downward direction. In other words, it is sufficient to show that every small step along any direction that is feasible (in the constant form defined by our limitations)."}, {"heading": "5 Non-negativity constraints", "text": "We will now describe a simple modification of our proposed algorithms, the non-negative constraints of the parameters w: L (w,) = 12 | | w | | 2 + \u2211 i: i (26) s.t. wTxij > lij \u2212 i: 0 wk \u2265 0 wk, 0, k: N (27) The above corresponds to our original formulation of (2) with an additional set of non-negative constraints for a subset of parameters wk (27). To simplify the notation, we have assumed that all parameters are limited to be non-negative in the above Lagrangian criteria, but our final algorithm allows an abitary optimization of 0 \u2212 \u03b2 \u00b7 w (28).To simplify the notation, we have assumed that all parameters are limited to be non-negative in the above Lagrangian criteria."}, {"heading": "6 Flexible regularization", "text": "This section describes a method of using the above solvent to solve a more general SVM problem with a Gaussian regularization or \"before\" given to w by (\u00b5, \u03a3): Argmin w, (w \u2212 w0) R | 2 + (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) R: argmin w = (w \u2212 w0) W: argmin w = (w \u2212 w0) W: argmin w = (w \u2212 w0) W: argmin w = (w \u2212 w0)."}, {"heading": "7 Conclusion", "text": "The ideas described here are implemented in publicly available solutions published in [25, 8, 28, 14, 20]. Acknowledgements: The funding of this research was provided by the NSF grant 0954083 and the ONR-MURI grant N00014-10-1-0933. We would also like to thank the co-authors for numerous discussions and considerable debugging efforts."}, {"heading": "A Generalized SVMs", "text": "In this section, we show that various SVM problems can be written as general cases of our underlying QP (2), which allows our optimization algorithms and solvers to be applicable to a wide range of problems, including binary prediction, regression, structured prediction and latent estimation. To cut a long story short, we write the QP of (2) here: argmin w, \u03b6 \u2265 01 2 | | | w | | 2 + N \u2211 i \u0109i (36) s.t."}, {"heading": "A.1 Binary classification", "text": "A linearly parameterized binary classifier predicts a binary name for an input x: Label (x) = {wTx > 0} (37) The associated learning problem is defined by a set of designated examples {xi, yi} where xi-RN, yi margin, yi margin (1, 1): argmin \u03b21 | \u03b2 | 2 + C \u2211 i (38) s.t. yi (\u03b2 Txi + b) \u2265 1 \u2212 You can apply the above problems in (2) with the following: first apply a constant value v to each attribute to model the bias term with x \u2032 i = (xi, v), where v = 1 is the typical choice. This allows us to write \u03b2 Txi + b = w Tx \u2032 i where w = (\u03b2, b). We then multiply in the class name yi and slack C into each attribute \u2032, x, from which Cyxi = ixi."}, {"heading": "A.2 Multiclass SVMs", "text": "A linearly parameterized multi-class predictor generates a class label for x with the following result: Label (x) = argmax j (1... K) wTj xThe associated learning problem is defined by a dataset {xi, yi} in which xi-RN and yi {1, 2,.. K}. There are many approaches to multi-class prediction that reduce the problem to a series of binary prediction problems (say, by training K 1-vs-all predictors or K2 pair-by-pair predictors). Each of the basic binary prediction problems can be described as (38), and thus it can be directly mapped to (2). Here we describe the special multi-class formulation from [6] that requires explicit mapping: argmin w, 0, 012, jp, jp, jp, jp, the difference between classes i (39) s.t."}, {"heading": "A.3 Structural SVMs", "text": "A linearly parameterized structural predictor generates a label of the formula Label (x) = argmax y-YwT\u03c6 (xi, y), where Y represents a structured production space (possibly exponentially large).The associated learning problem is given by a data set {xi, yi} in which xi-RN and yi-Y: argmin w, 01 2 | | | 2 + \u2211 i-i (41) s.t.: i, h-Y wT\u03c6 (xi, yi) \u2212 wT\u03c6 (xi, h) \u2265 loss (yi, h) \u2212 \u0394i (42) One can define Ni = | Y |, xij = \u03c6 (xi, yi) \u2212 \u03c6 (xi, j) and lij = loss (yi, j), where j = h is interpreted as an index in the production space Y."}, {"heading": "A.4 Latent SVMs", "text": "A latent SVM generates a binary prediction by looking for a latent variable label (x) = {max z-Z-W \u00b7 (x, z) > 0} (43), whereby Z represents a (possibly exponentially large) latent space. In the latent SVM learning, especially in the convex optimization stage of coordinate descent [10], each training example is specified by {xi, zi, yi}, where yi, 1, 1} and zi latent variables are specified for positive examples: argmin w, 1, 1, 2, 2, 2, 2, 2, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5."}, {"heading": "A.5 Latent structural SVMs", "text": "The above model can be extended to the latent structural case in which the predictor behaves as follows: Label (x) = argmax y-Y [max z-Z wT\u03c6 (x, y, z)] The associated learning problem is defined by a dataset {xi, zi, yi} in which yi-Y is a structured label and not a binary one. In this scenario, the analog convex step of the \"coordinate descend\" corresponds to the optimization of the following optimization: argmin w, 01 2 | | | 2 + [2 +] i-i (47) s.t."}, {"heading": "A.6 Regression", "text": "A linear regressor makes the following predictions Label (x) = wTxThe associated SVM regression problem is specified by a dataset {xi, yi} in which xi-RN and yi-R: argmin w, \u0430 \u2265 01 2 | | w | | 2 + \u2211 i (\u043fi + \u0441\u0438\u0445 i) (48) s.t. [i], wTxi \u2265 yi \u2212 \u043fi wTxi \u2264 yi + + \u043a \u043a. [iThe above constraints can be converted to that of (2) by doubling the number of constraints by (x \u2032 i, y \u2032 i) = (xi, yi \u2212) and (x \u2032 2i, y \u2032 2i) = (\u2212 xi, \u2212 yi \u2212) and Ni = N2i = 1.Summary: In this section we have shown that many previously proposed SVM problems can be written as instances of the generic problem in (1), which can be written as a square program (in P)."}], "references": [{"title": "Applying support vector machines to imbalanced datasets", "author": ["Rehan Akbani", "Stephen Kwek", "Nathalie Japkowicz"], "venue": "In Machine Learning: ECML", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Solving multiclass support vector machines with LaRank", "author": ["A. Bordes", "L. Bottou", "P. Gallinari", "J. Weston"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Fast kernel classifiers with online and active learning", "author": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Detecting actions, poses, and objects with relational phraselets", "author": ["Chaitanya Desai", "Deva Ramanan"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Object detection with discriminatively trained part based models", "author": ["Pedro F. Felzenszwalb", "Ross B. Girshick", "David McAllester", "Deva Ramanan"], "venue": "IEEE TPAMI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Optimized cutting plane algorithm for support vector machines", "author": ["V. Franc", "S. Sonnenburg"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Parsing occluded people", "author": ["D. Ramanan. C. Fowlkes G. Ghiasi", "Y. Yang"], "venue": "In Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Analyzing 3d objects in cluttered images", "author": ["Mohsen Hejrati", "Deva Ramanan"], "venue": "In Advances in Neural Information Processing Systems, pages 602\u2013610,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Analysis by synthesis: 3d object recognition by object reconstruction", "author": ["Mohsen Hejrati", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Efficient backprop", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Steerable part models", "author": ["Hamed Pirsiavash", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Histograms of sparse codes for object detection", "author": ["Xiaofeng Ren", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "A tutorial on support vector regression", "author": ["Alex J Smola", "Bernhard Sch\u00f6lkopf"], "venue": "Statistics and computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML. ACM New York, NY,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Articulated human detection with flexible mixtures of parts", "author": ["Yi Yang", "Deva Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Learning structural svms with latent variables", "author": ["C.N.J. Yu", "T. Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["Xiangxin Zhu", "D Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "The latter is often phrased as a stochastic optimization problem [4, 21]; such algorithms enjoy strong theoretical properties but often require manual tuned annealing schedules, and may converge slowly for problems with large output spaces (e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 18, "context": "The latter is often phrased as a stochastic optimization problem [4, 21]; such algorithms enjoy strong theoretical properties but often require manual tuned annealing schedules, and may converge slowly for problems with large output spaces (e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 6, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 24, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 12, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 16, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 17, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 22, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 13, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 11, "context": "We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code.", "startOffset": 98, "endOffset": 133}, {"referenceID": 8, "context": "Approach: Our approach is closely based on data-subsampling algorithms for collecting hard examples [10, 11, 7], combined with the dual coordinate quadratic programming (QP) solver described in liblinear [9].", "startOffset": 100, "endOffset": 111}, {"referenceID": 9, "context": "Approach: Our approach is closely based on data-subsampling algorithms for collecting hard examples [10, 11, 7], combined with the dual coordinate quadratic programming (QP) solver described in liblinear [9].", "startOffset": 100, "endOffset": 111}, {"referenceID": 5, "context": "Approach: Our approach is closely based on data-subsampling algorithms for collecting hard examples [10, 11, 7], combined with the dual coordinate quadratic programming (QP) solver described in liblinear [9].", "startOffset": 100, "endOffset": 111}, {"referenceID": 7, "context": "Approach: Our approach is closely based on data-subsampling algorithms for collecting hard examples [10, 11, 7], combined with the dual coordinate quadratic programming (QP) solver described in liblinear [9].", "startOffset": 204, "endOffset": 207}, {"referenceID": 4, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 90, "endOffset": 93}, {"referenceID": 19, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 21, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 8, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 175, "endOffset": 183}, {"referenceID": 9, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 175, "endOffset": 183}, {"referenceID": 23, "context": "(1) and its QP variant (2) is a general form that encompasses binary SVMs, multiclass SVMs[6], SVM regression [22], structural SVMs [23, 24] the convex variant of latent SVMs [10, 11] and the convex variant of latent structural SVMs [27].", "startOffset": 233, "endOffset": 237}, {"referenceID": 7, "context": "The fastest current batch solver for linear SVMs appears to be liblinear [9], which is a dual coordinate descent method.", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "Lecun et al make similar observations to motivate random perturbations of data for stochastic gradient descent [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "To define our stopping criteria, we closely follow the duality-based stopping criteria described in [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "The LaRank algorithm [2] makes the observation that it is beneficial to frequently revisit past examples with a non-zero alpha, since they are more likely to trigger an update to their dual value.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "This can be implemented by maintaining a cache of \u201chard examples\u201d [11], or support vectors and routinely optimizing over them while exploring new examples.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "This is basis for much of the literature on both batch and online SVMs, through the use of heuristics such as active-set selection [9, 24] and new-process/optimization steps [2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 21, "context": "This is basis for much of the literature on both batch and online SVMs, through the use of heuristics such as active-set selection [9, 24] and new-process/optimization steps [2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "This is basis for much of the literature on both batch and online SVMs, through the use of heuristics such as active-set selection [9, 24] and new-process/optimization steps [2].", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "The hard-negative mining solver of [11] does exactly this.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "We propose here a novel on-line cutting plane algorithm [16, 17] that maintains running estimates of lower and upper bounds of the primal objective that naturally trade off the exploration of new data points versus the optimization of existing support vectors.", "startOffset": 56, "endOffset": 64}, {"referenceID": 2, "context": "[3] prove that is suffices to show that no improvement can be made along a set of \u201cwitness\u201d directions that form a basis for the feasible set of directions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Cyclic optimization: Consider an algorithm that randomly samples update directions with any distribution such that all feasible directions can be drawn with non-zero probability; [3] show that such an algorithm probably convergences to the optimum, within some specified tolerance, in finite time.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "The ideas described here are implemented in publicly available solvers released in [25, 8, 28, 14, 20].", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "The ideas described here are implemented in publicly available solvers released in [25, 8, 28, 14, 20].", "startOffset": 83, "endOffset": 102}, {"referenceID": 12, "context": "The ideas described here are implemented in publicly available solvers released in [25, 8, 28, 14, 20].", "startOffset": 83, "endOffset": 102}, {"referenceID": 17, "context": "The ideas described here are implemented in publicly available solvers released in [25, 8, 28, 14, 20].", "startOffset": 83, "endOffset": 102}, {"referenceID": 0, "context": "Such class-specific costs have been shown be useful for learning classifiers from imbalanced datasets [1].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Here, we describe the particular multiclass formulation from [6] which requires an explicit mapping:", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "from [6] defined a 0-1 loss where loss(yi, j) = 0 for j = yi and loss(yi, j) = 1 for j 6= yi.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "In latent-SVM learning, and in particular, the convex optimization stage of coordinate descent [10], each training example is given by {xi, zi, yi} where yi \u2208 {\u22121, 1}, and zi are latent variables specified for positive examples:", "startOffset": 95, "endOffset": 99}], "year": 2014, "abstractText": "This manuscript describes a method for training linear SVMs (including binary SVMs, SVM regression, and structural SVMs) from large, out-of-core training datasets. Current strategies for large-scale learning fall into one of two camps; batch algorithms which solve the learning problem given a finite datasets, and online algorithms which can process out-of-core datasets. The former typically requires datasets small enough to fit in memory. The latter is often phrased as a stochastic optimization problem [4, 21]; such algorithms enjoy strong theoretical properties but often require manual tuned annealing schedules, and may converge slowly for problems with large output spaces (e.g., structural SVMs). We discuss an algorithm for an \u201cintermediate\u201d regime in which the data is too large to fit in memory, but the active constraints (support vectors) are small enough to remain in memory. In this case, one can design rather efficient learning algorithms that are as stable as batch algorithms, but capable of processing out-of-core datasets. We have developed such a MATLAB-based solver and used it to train a series of recognition systems [25, 8, 28, 14, 19, 20, 26, 15, 13] for articulated pose estimation, facial analysis, 3D object recognition, and action classification, most of which has publicly-available code. This writeup describes the solver in detail. Approach: Our approach is closely based on data-subsampling algorithms for collecting hard examples [10, 11, 7], combined with the dual coordinate quadratic programming (QP) solver described in liblinear [9]. The latter appears to be current fastest method for learning linear SVMs. With regard to liblinear, we make two extensions (1) We generalize the solver to other types of SVM problems such as (latent) structural SVMs (2) We modify it to behave as a partially-online algorithm, which only requires access to small amounts of in-memory data at a time. Data-subsampling algorithms typically operate by iterating between searching for hard examples and optimization over a batch of hard-examples in memory. With regard to these approaches, our approach differs in that (1) we use previously-computed solutions to\u201chot-start\u201d optimizations, making frequent calls to a batch solver considerable cheaper and (2) we track upper and lower bounds to derive an \u201coptimal\u201d schedule for exploring new data versus optimizing over the existing batch. Overview: Sec. A describes a general formulation of an SVM problem that encompasses many standard tasks such as multi-class classification and (latent) structural prediction. Sec. 1 derives its dual QP, and Sec. 2 describes a dual coordinate descent optimization algorithm. Sec. 3 describes modifications for optimizing in an online fashion, allowing one to learn near-optimal models with a single pass over large, out-of-core datasets. Sec. 4 briefly touches on some theoretical issues that are necessary to ensure convergence. Finally, Sec. 5 and Sec. 6 describe modifications to our basic formulation to accommodate non-negativity constraints and flexible regularization schemes during learning.", "creator": "LaTeX with hyperref package"}}}