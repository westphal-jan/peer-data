{"id": "1312.2244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2013", "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "abstract": "Timeline Generation aims at summarizing news from different epochs and telling readers how an event evolves. It is a new challenge that combines salience ranking with novelty detection. For long-term public events, the main topic usually includes various aspects across different epochs and each aspect has its own evolving pattern. Existing approaches neglect such hierarchical topic structure involved in the news corpus in timeline generation. In this paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our model can aptly detect different levels of topic information across corpus and such structure is further used for sentence selection. Based on the topic mined fro HDM, sentences are selected by considering different aspects such as relevance, coherence and coverage. We develop experimental systems to evaluate 8 long-term events that public concern. Performance comparison between different systems demonstrates the effectiveness of our model in terms of ROUGE metrics.", "histories": [["v1", "Sun, 8 Dec 2013 19:15:15 GMT  (635kb,D)", "http://arxiv.org/abs/1312.2244v1", null], ["v2", "Mon, 21 Apr 2014 11:44:47 GMT  (1146kb,D)", "http://arxiv.org/abs/1312.2244v2", null], ["v3", "Tue, 14 Mar 2017 01:34:50 GMT  (1598kb,D)", "http://arxiv.org/abs/1312.2244v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["rumeng li", "tao wang", "xun wang"], "accepted": false, "id": "1312.2244"}, "pdf": {"name": "1312.2244.pdf", "metadata": {"source": "CRF", "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "authors": ["Tao Wang"], "emails": ["pwangtao@whu.edu.cn"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions I.7.5 [Document Entry]: [Document Analysis] General Terms Design, Algorithms, Theory, ExperimentationKeywords Timeline Generation, Dirichlet Process, Theme Model"}, {"heading": "1. INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2. RELATED WORK", "text": "The task of the Generation timeline is first proposed by Swan and Allan [27] by using designated units to select sentences in different time periods. Since then, researchers have attempted to create timelines by considering different aspects such as usefulness and novelty [3] or interest and fragility [10]. Yan et al. [39] extended the sentence-based algorithm in the Summary of Multiple Documents (MDS) to the Generation timeline by projecting sentences from different temporal corpus into a plane. Another work by Yan et al. [38] transforms this task into an optimization problem by considering the relevance, scope, and coherence of sentences. Existing approaches rarely examine the information contained in the corpus or the structure of news information. Li et al [17] used the theme model to capture the dynamic evolution of topics in the Generation.TopModels timeline."}, {"heading": "3. DP AND HDP", "text": "A DP indicated by DP (\u03b1, G0) is parameterized by a basic measurement G0 and a concentration parameter. We write G \u0445 DP (\u03b1, G0) for a draw of the DP distribution from the Dirichlet process. G itself is a distribution over a given parameter space, so we can write parameters 1: N of G after a Polya urn distribution [4] also known as a Chinese restaurant process [29] or by a stick breaking construction [26]. Within the Gibbes sampling method we have: p \u2212 1, p \u2212 1, p \u2212 k mk i \u2212 1 + \u03b1 urn distribution (4] also known as a Chinese restaurant process [29] or by a stick breaking construction [26]. Within the Gibbes sampling method we have: p \u2212 1 \u2212 1, p \u2212 kp \u2212 k \u2212 1 + urn distribution (1 \u2212 1 + \u03b1k) + G0 + G0 + Gamek \u2212 1 (1), where the Gk + 1 and Gk (1) values are clearly connected respectively."}, {"heading": "4. HDM FOR TIMELINE GENERATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Problem Formulation", "text": "Here we first give a standard formulation of the task. With a query Q = {wqi} i = ni = 1, where wqi is the word in the query, we get a series of query-related documents from the Internet. The corpus is divided into a series of document collections according to the published time as C = {Ct} t = Tt = 1, where Ct = {Dti} i = N ti = 1, corresponding to the document collection published at the time of publication. Dti stands for document i at time t and Nt for the number of words published at the time. Document Dti is called a word collection Dti = {wtin} n = Nti n = 1. V stands for the vocabulary size. The output of the algorithm consists of a series of timelines I = {It} t = Tt = 1 and It is called a collection of words Dti = {wtin} n = Nti n = 1."}, {"heading": "4.2 HDM", "text": "HDM is seen as an extension of the hierarchical LDA model, which represents a hierarchical structure for the structure of the menu. \u2192 Meat is a common usage pattern for us (or themes) that includes a collection of documents and attempts to integrate these themes into a hierarchy. However, HDM differs from hLDA for its non-parametric menu and ability to model time-correlated data. HDM can be better explained from a Chinese restaurant perspective. There are a number of restaurants in the city and each restaurant is associated with a restaurant-specific menu. Such a menu is characterized by a global menu that is shared in all restaurants throughout the city. In each restaurant there is an infinite number of tables. When a customer enters the restaurant, he would select a table and a dish according to the local restaurant menu. A demonstration of the restaurant menu is illustrated in Figure 3, we observe that the Chinese restaurant differentiates interpretation for HDM from other themes."}, {"heading": "4.3 Inference", "text": "Since we follow the strict assumption of Grubber et al. [16] that words in the same sentence are all generated from the same topic, we describe this part only briefly and skip the details found in the work of Teh et al. [29]. Sample table B for current customers: P (Bs = B | w, B \u2212 bs) when B is used: P (s | w, znew) when B (s | w, znew) when B (s | w, znew) when B (s | w, znew) is used, P (s | w, znew) when B (s | w, znew) is used, when B (s | s, znew), when B (s | s, znew), when B (s | s, znew), the probability of sentence s (s) generated by topic eg. New topic zz when mz) is described in Appendix A \u2212 example dish zzt eg for the new table."}, {"heading": "4.4 Tree-based Sentence Selection", "text": "We assume that sentences (or words) that follow similar paths should be more similar, because the common themes (w1) = KL (w1 + w22) + KL (w1 + w2) the similarity between two words (w1 + w2) and KL (w2) the similarity between two words (w1 + w2).KL (w1) The equality between two words (w1 + w2) is achieved by the first calculation of the Jensen-Shannon divergence (w1). (w1) The equality between the two words (w1 + w2) is the same (w1).KL (w2) The equality between the sexes (w1) and the sexes (w1).K (w1).K (w1) The equality between the sexes (w1) and the sexes (w2) is the equality between the sexes (w2).K (w2) The equality between the sexes (w2)."}, {"heading": "5. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets and Experiments Setup", "text": "In this essay, we build 8 sets of data that talk about the real long-term events that attract public attention and use the gold standards to evaluate the performance of various models, including \"Iraq War,\" \"Apple Inc.,\" \"Financial Crisis,\" \"Greek Debt Crisis,\" \"War in Afghanistan,\" \"American Presidential Campaign 2012,\" and \"North Korea Nuclear Crisis.\" We download 9935 news articles from various sources. Details of the data sets are presented in Tables1 and Table2. Record 1 \"Iraq War,\" 2 \"Apple Inc.,\" 6 \"Afghanistan War,\" and 8 \"North Korea Nuclear Crisis\" are used as parameter setting training kits and the rest is used for testing. Pre-processing: We first remove stop words from documents that use a stop-word list of 598 words, and the remaining words are taken from Porter Stemmer3. In addition to reading and stop-word removal, we will also provide the snippets with 39 each."}, {"heading": "5.2 Evaluation Metrics", "text": "We use the ROUGE toolkit (version 1.5,5) for performance evaluation. Timeline quality is measured by counting the number of intersecting units such as N grams, word strings and pairs between candidate timeline CT and soil-truth timelines GT. Several automatic evaluation methods are implemented in ROUGE and each of 3 http: / / tartarus.org /.martin / PorterStemmer /.the methods can generate values of recall, precision and fmeasure. Let's take ROUGE-N as an example: ROUGE \u2212 N \u2212 R = \"I\" GT \"gramN\" ICntmatch (gramN), \"I\" GT \"gramN\" I Cnt (gramN) ROUGE \u2212 N \"TurkP =\" I \"CT\" gramN \"ICntmatch (gramN),\" I \"CT\" gramN \"I\" (gramN), \"ROUGE\" I \"ROUGE\" (N) ROUGE \u2212 UF \u2212 2GE \u2212 N \u2212 N \u2212 N."}, {"heading": "5.3 Parameter Tuning", "text": "The first group of key parameters is w1, w2 and w3, where w4 = 1 \u2212 w1 \u2212 w2 \u2212 w3. We first change w1 gradually from 0 to 1 at an interval of 0.1 and set w2, w3 and w3 to the same value of (1 \u2212 w1) / 3. Similarly, we change w2 and w3 training. Experimental results are shown in Figure 7 and we note that coherence and non-redundancy facilitate performance, while relevance has a relatively weaker influence. We use w1 = 0.1, w2 = 0.3, w3 = 0.3 and therefore w4 = 0.3 according to the ROUGE points. \u03bb controls the rate of exponential decay and others controls the trade between influences from the base menu H and yesterday's menu list Gt \u2212 1. We conducted experiments in which different values are set from 1 to 11.5, with the rate of exponential decay and the further rate from 1 to grob."}, {"heading": "5.4 Performance Comparison with Baselines", "text": "We implement the following algorithms as base systems. In fairness, we perform the same pre-processing and post-processing for all algorithms. Random: The methods that randomly select sentences for the timeline generation.Centroid: The method that applies the MEAD algorithm [23], widely used in MDS for sentence selection by centrifugal value, position value, and first sentence, overlaps.GMDS: The graph-based MDS sentence selection strategy proposed by Wan et al. [30], which constructs a sentence connectivity graph based on cosmic similarity and then selects important sentences based on centrality. Chieu: A timeline system proposed by Chieu et al. [10] taking into account interest and stiness.ETTS: A timeline system proposed by Yan et al. [40] by projecting sentences from different temporal corpus into a plane, the development and ranking of the algorithm."}, {"heading": "5.5 Comparison with Other Topic Models", "text": "To illustrate the effectiveness of our topic model, we provide six additional base systems that use different aspect modeling techniques.tHDP: HDP is a time-dependent HDP model without consideration of the hierarchical structure of topics. It is a simple version HDM.Stand-HDP: Stand-HDP is a standard HDP model that treats different eras as a set of independent HDP, without consideration of background or time information.The overall results are presented in Figure 9 and details are in Table 4. As we can see, HDM is better than tHDP and HDP, confirming HDM's ability to model hierarchical topic structures in long-term news corpus."}, {"heading": "5.6 Qualitative Evaluation of Topic Modeling", "text": "Figure 10 (a) presents first-level topics that have been extracted from the Greek debt crisis dataset, and corresponding top words for each topic. Intensity corresponds to the number of sentences that have been assigned to each topic at a given time. As we see, each topic describes the real aspect within the event. According to the top word, we find that Topic 1 talks about a general concept of the Greek debt crisis, Topic 2 talks about the response or rescue from the international community, Topic 3 discusses protests from the Greek public and Topic 4 discusses actions that the Greek government is taking. We also find an interesting phenomenon that protests (Topic 3) usually culminate immediately after national actions (Topic 4) or international responses (Topic 2), which makes sense in real cases where people take to the streets for government policies. Figure 10 (b) presents sub-topics (subtopics) for Topic 3, which present as Topic 3-1, 3-3-3 and 3-3-3 times the protest, while 2-3 times present the differences between the world and 3-3-3."}, {"heading": "6. CONCLUSION", "text": "In this thesis, we develop a novel theme model, called the Time-Dependent Hierarchical Dirichlet Model (HDM), to explore the hierarchical theme structure for timeline creation. Our model combines Dirichlet tree with Dirichlet processes and can automatically learn the structure of trees across corpus. Different levels of Markovian time dependence and background information are taken into account for the construction of tree structures. We build an experimental system on 8 real long-term events affecting the public. Experimental results illustrate the effectiveness of our proposed model."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed and E. P. Xing 1973 and J. D. Thematics."}], "references": [{"title": "Dynamic non-parametric mixture models and the recurrent chinese restaurant process", "author": ["A. Ahmed", "E.P. Xing"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Timeline: A dynamic hierarchical dirichlet process model for recovering birth/death and evolution of topics in text stream", "author": ["A. Ahmed", "E.P. Xing"], "venue": "arXiv preprint arXiv:1203.3463,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Temporal summaries of new topics", "author": ["J. Allan", "R. Gupta", "V. Khandelwal"], "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Ferguson distributions via p\u00f3lya urn schemes", "author": ["D. Blackwell", "J.B. MacQueen"], "venue": "The annals of statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1973}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Generalized polya urn for time-varying dirichlet process mixtures", "author": ["F. Caron", "M. Davy", "A. Doucet"], "venue": "arXiv preprint arXiv:1206.5254,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Evolutionary clustering", "author": ["D. Chakrabarti", "R. Kumar", "A. Tomkins"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Evolutionary spectral clustering by incorporating temporal smoothness", "author": ["Y. Chi", "X. Song", "D. Zhou", "K. Hino", "B.L. Tseng"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Query based event extraction along a timeline", "author": ["H.L. Chieu", "Y.K. Lee"], "venue": "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Finding and linking incidents in news", "author": ["A. Feng", "J. Allan"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1973}, {"title": "Time-dependent event hierarchy construction", "author": ["G.P.C. Fung", "J.X. Yu", "H. Liu", "P.S. Yu"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A scalable global model for summarization", "author": ["D. Gillick", "B. Favre"], "venue": "In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D.M.B.T.L. Griffiths", "M.I.J.J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "Y. Weiss", "M. Rosen-Zvi"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Enhancing diversity, coverage and balance for summarization through structure learning", "author": ["L. Li", "K. Zhou", "G.-R. Xue", "H. Zha", "Y. Yu"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Generating aspect-oriented multi-document summarization with event-aspect model", "author": ["P. Li", "Y. Wang", "W. Gao", "J. Jiang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Hierarchical bayesian modeling of topics in time-stamped documents", "author": ["I. Pruteanu-Malinici", "L. Ren", "J. Paisley", "E. Wang", "L. Carin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Centroid-based summarization of multiple documents", "author": ["D.R. Radev", "H. Jing", "M. Sty\u015b", "D. Tam"], "venue": "Information Processing & Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "The dynamic hierarchical dirichlet process", "author": ["L. Ren", "D.B. Dunson", "L. Carin"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Multi-field correlated topic modeling", "author": ["K. Salomatin", "Y. Yang", "A. Lad"], "venue": "In SDM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A constructive definition of dirichlet priors", "author": ["J. Sethuraman"], "venue": "Technical report, DTIC Document,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Automatic generation of overview timelines", "author": ["R. Swan", "J. Allan"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Hierarchical dirichlet trees for information", "author": ["Y.W. Teh", "G. Haffari"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["X. Wan", "J. Yang"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Continuous time dynamic topic models", "author": ["C. Wang", "D. Blei", "D. Heckerman"], "venue": "arXiv preprint arXiv:1206.3298,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Mining correlated bursty topic patterns from coordinated text streams", "author": ["X. Wang", "C. Zhai", "X. Hu", "R. Sproat"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Dirichlet process based evolutionary clustering", "author": ["T. Xu", "Z. Zhang", "P.S. Yu", "B. Long"], "venue": "In Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Evolutionary clustering by hierarchical dirichlet process with hidden markov state", "author": ["T. Xu", "Z. Zhang", "P.S. Yu", "B. Long"], "venue": "In Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Timeline generation through evolutionary trans-temporal summarization", "author": ["R. Yan", "L. Kong", "C. Huang", "X. Wan", "X. Li", "Y. Zhang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Event recognition from news webpages through latent ingredients extraction", "author": ["R. Yan", "Y. Li", "Y. Zhang", "X. Li"], "venue": "Information Retrieval Technology,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution", "author": ["R. Yan", "X. Wan", "J. Otterbacher", "L. Kong", "X. Li", "Y. Zhang"], "venue": "In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Multi-candidate reduction: Sentence compression as a tool for document summarization tasks", "author": ["D. Zajic", "B.J. Dorr", "J. Lin", "R. Schwartz"], "venue": "Information Processing & Management,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora", "author": ["J. Zhang", "Y. Song", "C. Zhang", "S. Liu"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 9, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 32, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 34, "context": "Timeline generation [3, 10, 38, 40] gives an ideal solution to this problem by providing readers with a faster and better access to understand news.", "startOffset": 20, "endOffset": 35}, {"referenceID": 23, "context": "[27] by extracting clusters of noun phases and name entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Later they built up a system to provide timelines which consist of one sentence per date by considering usefulness and novelty[3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "[10] built a similar system in unit of sentences with interest and burstiness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[38] extended the graph based sentence ranking algorithm used in traditional multi-document summarization (MDS) to timeline generation by projecting sentences from different times into one plane.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "They further explored the timeline task from the optimization of a function by considering the combination of different respects such as relevance, coverage, coherence and diversity[40].", "startOffset": 181, "endOffset": 185}, {"referenceID": 32, "context": "Time dependency is considered in Yan et al\u2019s work[38, 40].", "startOffset": 49, "endOffset": 57}, {"referenceID": 34, "context": "Time dependency is considered in Yan et al\u2019s work[38, 40].", "startOffset": 49, "endOffset": 57}, {"referenceID": 10, "context": "Topic detection or mining is not a new task and many algorithms have been proposed in TDT (Topic Detection and Tracking) task[11, 13], or summarization task[32, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 12, "context": "Topic detection or mining is not a new task and many algorithms have been proposed in TDT (Topic Detection and Tracking) task[11, 13], or summarization task[32, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 5, "context": "Among existing approaches, topic models such as Latent Dirichlet Allocation (LDA)[6] or Hierarchical Dirichlet Proar X iv :1 31 2.", "startOffset": 81, "endOffset": 84}, {"referenceID": 25, "context": "cesses (HDP) have their advantages [29] in capturing latent topics within document collection due its clear probabilistic interpretions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "[15] introduced a hLDA model which organizes topics into a tree structure with depth L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 1, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 4, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 8, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 20, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 21, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 24, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 28, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 36, "context": "Second, to deal with time-correlated data, many approaches have been proposed based on topic approaches, aiming to discover the evolving patterns in the corpus as well as the snapshot topics at each time epoch [1, 2, 5, 9, 24, 25, 28, 33, 42].", "startOffset": 210, "endOffset": 242}, {"referenceID": 23, "context": "The task of timeline generation is firstly proposed by Swan and Allan[27] by using named entities for sentence selection in different time epochs.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Since then, researchers tried to generate timeline by considering different respects such as usefulness and novelty[3] or interest and burstiness[10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 33, "context": "[39] extended the graph based sentence ranking algorithm in multi-document summarization (MDS) to timeline generation by projecting sentences from different temporal corpus into one plane.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[38] transforms this task to an optimization problem by considering the relevance, coverage and coherence of the sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 25, "context": "Recently, bayesian topic models such as LDA[6] or HDP[29] have shown the power text mining for its clear probabilistic interpretation.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "hLDA [15] extends LDA model to a multi-level tree structure where each node is associated with a topic distribution over words.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 7, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 8, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 30, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 31, "context": "Learning evolutionary topics from a time-correlated corpus aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch [1, 8, 9, 19, 36, 37].", "startOffset": 161, "endOffset": 182}, {"referenceID": 0, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 31, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 30, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 39, "endOffset": 50}, {"referenceID": 4, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 27, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 28, "context": "Among exsting works, the approaches in [1, 37, 36] utilized time-dependent DP for topic modeling, while others focused on extending LDA to dynamic topic models [5, 31, 33] .", "startOffset": 160, "endOffset": 171}, {"referenceID": 6, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 20, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 29, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 36, "context": "In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [7, 22, 24, 34, 42].", "startOffset": 121, "endOffset": 140}, {"referenceID": 29, "context": "[34] focused on detecting the simultaneous busting of some topics in multiple text streams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[35] further extended [34] where they adjusted the timestamps of all documents to synchronize multiple streams and then learned a common topic model.", "startOffset": 22, "endOffset": 26}, {"referenceID": 36, "context": "[42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[42] introduced an approach based on Hierarchical Dirichlet Process [29] to discover interesting cluster evolutionary pattern from correlated time-varying text corpora.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "Dirichlet Process(DP) can be considered as a distribution over distributions[12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 139, "endOffset": 142}, {"referenceID": 25, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "G itself is a distribution over a given parameter space \u03b8, therefore we can draw parameters \u03b81:N from G following a Polya urn distribution [4] also known as a Chinese Restaurant process [29] or through a stick breaking construction [26].", "startOffset": 232, "endOffset": 236}, {"referenceID": 14, "context": "HDM can be viewed as an extension of Hierarchical LDA model [15] where we wish to discover common usage patterns (or topics) given a collection of documents and try to organize these topics into a hierarchy.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 1, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 20, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 36, "context": "Such theme has been introduced in a couple of previous works which try to address time dependency in timecorrelated data [1, 2, 24, 42].", "startOffset": 121, "endOffset": 135}, {"referenceID": 15, "context": "[16] that words in the same sentence are all generated from the same topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "space limit, we just briefly describe this part and skip the details that can be found in Teh et al\u2019s work [29].", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "A good timeline should properly consider the following key requirements[20]: (1)Focus: The timeline for epoch t should be related to the given query.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "\u2018s work[40].", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "We applied the sentence selection strategy in Yan et al\u2019s work [40] that literately generate I i to approximate Ii by maximizing function S based on the timeline generation in the last iteration.", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 17, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 35, "context": "Sentence compression techniques have been well explored in many existing works[14, 21, 41].", "startOffset": 78, "endOffset": 90}, {"referenceID": 17, "context": "We adopt the strategy introduced by [21] as shown in Figure 7.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 9, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 0, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 9, "context": "We performed experiments setting different values of \u03bb to [ 1 100 , 1 50 , 1 10 , 1, 10, 50, 100].", "startOffset": 58, "endOffset": 97}, {"referenceID": 19, "context": "Centroid: The method that applies MEAD algorithm [23], which has been widely used in MDS for sentence selection according to centroid value, positional value, and firstsentence overlap.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "[30] that constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on centrality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] by considering interest and burstiness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[40] by projecting sentences from different temporal corpus into one plane and developing the graph based ranking algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Dyn-LDA: A dynamic LDA[5] where topic-word distribution and popularity are linked across epochs by including Markovian assumption.", "startOffset": 22, "endOffset": 25}], "year": 2017, "abstractText": "Timeline Generation aims at summarizing news from different epochs and telling readers how an event evolves. It is a new challenge that combines salience ranking with novelty detection. For long-term public events, the main topic usually includes various aspects across different epochs and each aspect has its own evolving pattern. Existing approaches neglect such hierarchical topic structure involved in the news corpus in timeline generation. In this paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for timeline generation. Our model can aptly detect different levels of topic information across corpus and such structure is further used for sentence selection. Based on the topic mined fro HDM, sentences are selected by considering different aspects such as relevance, coherence and coverage. We develop experimental systems to evaluate 8 long-term events that public concern. Performance comparison between different systems demonstrates the effectiveness of our model in terms of ROUGE metrics.", "creator": "LaTeX with hyperref package"}}}