{"id": "1312.2578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2013", "title": "Kernel-based Distance Metric Learning in the Output Space", "abstract": "In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of a low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2- or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches.", "histories": [["v1", "Mon, 9 Dec 2013 20:58:16 GMT  (245kb)", "http://arxiv.org/abs/1312.2578v1", "11 pages, 7 figures, accepted by 2013 International Joint Conference on Neural Networks (IJCNN)"], ["v2", "Mon, 28 Apr 2014 20:08:47 GMT  (245kb)", "http://arxiv.org/abs/1312.2578v2", "11 pages, 7 figures, appeared in the Proceedings of 2013 International Joint Conference on Neural Networks (IJCNN)"]], "COMMENTS": "11 pages, 7 figures, accepted by 2013 International Joint Conference on Neural Networks (IJCNN)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cong li", "michael georgiopoulos", "georgios c anagnostopoulos"], "accepted": false, "id": "1312.2578"}, "pdf": {"name": "1312.2578.pdf", "metadata": {"source": "META", "title": "Output-Space Distance Metric Learning for k-NN Classification", "authors": ["Cong Li", "Michael Georgiopoulos", "Georgios C. Anagnostopoulos"], "emails": ["congli@eecs.ucf.edu,", "michaelg@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.25 78v1 [cs.LG] Dec 9 2Keywords: distance metric learning, kernel methods, reproduction of the kernel Hilbert space for vector weighted functions"}, {"heading": "1 Introduction", "text": "Most of the DML research to date focuses specifically on learning a weighted Euclidean metric, also known as the Mahalanobis distance (see [13]), or generalizing it, where the weights are derived from the data."}, {"heading": "2 RKHS for Vector-Valued Functions", "text": "Before introducing our methods, in this section we will briefly discuss the concept of reproducing the Hilbert Space Kernel (RKHS) for vector-weighted functions as in [14]. Let X be any set that we will call an input space, although it may not be a vector space per se. A matrix function K: X \u00b7 X \u2192 Rm \u00b7 m is called a positively-defined matrix core or simply a matrix core if it meets the following conditions: K (x, x) = KT (x, \"x,\" x \"X\" K \"(x) 0\" X \"X\" X \"X\" X \"(3), if X = {xi} n i\" n i \"and K\" (X) = R & M \"R & m,\" mn \"X,\" x \"mn,\" x, \"x,\" x, \"x,\" x. \""}, {"heading": "3 Fixed Matrix Kernel DML Formulation", "text": "In this section, we propose our first core-based DML method, which is based on an RKHS method based on RKHS functionality. (1) Suppose we are faced with a RKS-2 RM-2-2-RM-2-RM-2-2-RM-2-2-RM-2-2-2-RM-2-2-RM-2-2-2-RM-2-2-RM-2-2-2-RM-2-2-RM-2-2-2-RM-2-2-2-2-2-RM-2-2-2-2-2-RM-2-2-2-2-2-2-RM-2-2-2-2-2-RM-2-2-2-2-2-2-2-RM-2-2-2-2-2-2-2-2-2-RM-2-2-2-2-2-2-2-2-2-RM-2-2-2-2-2-2-2-2-2-2-2-RM-3-3-3-2-2-RM-2-2-2-2-2-2-2-2-2-RM-4-4-4-4-4-4-3-3-RM-3-2-2-2-2-2-RM-2-2-2-2-2-2-2-RM-2-2-2-2-4-4-4-4-4-4-RM-4-4-4-4-4-4-4-4-RM-4-4-4-4-4-2-2-RM-2-2-2-2-RM-2-2-2-2-2-2-RM-2-2-2-2-2-2-2-RM"}, {"heading": "4 Parameterized Matrix Kernel DML Formulation", "text": "Our next formula shares all the assumptions with the previous one, except that the matrix kernel function K is now parameterized. We will show that although the matrix kernel function is somewhat limited, it has the property of implicitly determining the output space predetermined by the user and B-Rm, we assume a matrix kernel of the form: K (x, x), B (14), where k is a scalar kernel function predetermined by the user and B-Rm, is a symmetrical, positive semi-defined matrix that will be learned from T. Based on these facts, K fulfills the equation (1) and is therefore a legitimate matrix kernel function."}, {"heading": "5 Experiments", "text": "In this section, we evaluate the performance of our two kernel-based DML methods based on other methods that we compare with other methods."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed two new kernel-based Distance Metric Learning (DML) methods based on the Reproduction Kernel Hilbert Spaces (RKHSs) of vector-weighted functions. Using a mapping f, the two methods map data from their original space to an output space whose dimension can be directly controlled. Subsequent distance measurements are performed in the output space using a Mahalanobis metric. The first proposed model uses a general matrix core function, providing considerable flexibility in modelling the input-to-output space mapping. Unlike previous kernel-based approaches, the relevant f-mappings are explicit for both of our methods. Coupled with the fact that the matrix core function can be learned directly from data, the relevant f-mappings explicitly show for the two methods."}, {"heading": "Acknowledgements", "text": "C. Li acknowledges partial support from the National Science Foundation (NSF) grant number 0806931. Likewise, M. Georgiopoulos acknowledges partial support from NSF grant number 0525429, no 0963146, no 1200566 and no 1161228. All opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF."}], "references": [{"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["Mikhail Bilenko", "Sugato Basu", "Raymond J. Mooney"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "A new kernelization framework for mahalanobis distance learning", "author": ["Ratthachat Chatpatanasiri", "Teesid Korsrilabutr", "Pasakorn Tangchanachaianan", "Boonserm Kijsirikul"], "venue": "algorithms. Neurocomputing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Inderjit S. Dhillon"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A metric learning perspective of svm: On the relation of LMNN and SVM", "author": ["Huyen Do", "Alexandros Kalousis", "Jun Wang", "Adam Woznica"], "venue": "In JMLR W&CP 5: Proceedings of Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Sam Roweis", "Geoff Hinton", "Ruslan Salakhutdinov"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Gsml: A unified framework for sparse metric learning", "author": ["Kaizhu Huang", "Yiming Ying", "Colin Campbell"], "venue": "In Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Inductive regularized learning of kernel functions", "author": ["Prateek Jain", "Brian Kulis"], "venue": "Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Metric and kernel learning using a linear transformation", "author": ["Prateek Jain", "Brian Kulis", "Jason V. Davis", "Inderjit S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Convex perturbations for scalable semidefinite programming", "author": ["Brian Kulis", "Suvrit Sra", "Dhillon", "Inderjit"], "venue": "In JMLR W&CP 5: Proceedings of Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Explicit solutions of linear matrix equations", "author": ["Peter Lancaster"], "venue": "SIAM Review, 12:pp", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "Geometry-aware metric learning", "author": ["Zhengdong Lu", "Prateek Jain", "Inderjit S. Dhillon"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "The mahalanobis distance", "author": ["R. De Maesschalck", "D. Jouan-Rimbaud", "D.L. Massart"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "On learning vector-valued functions", "author": ["Charles A. Micchelli", "Massimiliano Pontil"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Metric learning: A support vector approach", "author": ["Nam Nguyen", "Yunsong Guo"], "venue": "In Proceedings of the European conference on Machine Learning and Knowledge Discovery in Databases - Part II, ECML PKDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Large margin multi-task metric learning", "author": ["Shibin Parameswaran", "Kilian Q. Weinberger"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Learning a distance metric from relative comparisons", "author": ["Matthew Schultz", "Thorsten Joachims"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Large margin component analysis", "author": ["Lorenzo Torresani", "Kuang-chih Lee"], "venue": "Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Distance metric learning with kernels", "author": ["Ivor W. Tsang", "James T. Kwok"], "venue": "In Proceedings of International Conference on Artificial Neural Networks (ICANN),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Metric learning with multiple kernels", "author": ["Jun Wang", "Huyen Do", "Adam Woznica", "Alexandros Kalousis"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Laurence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Yiming Ying", "Peng Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Transfer metric learning by learning task relationships", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "see [13]), or generalizations of it, where the weights are inferred from the data.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "For example, the authors in [8] pointed out an equivalence between kernel learning and metric learning in the feature space.", "startOffset": 28, "endOffset": 31}, {"referenceID": 13, "context": "Leveraged by the Representer Theorem proposed in [14], all computations of both methods involve only kernel calculations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Thus, we can access the transformed data in the output space, and this feature can be even used to visualize the data [20], when the output space is 2- or 3-dimensional.", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 245, "endOffset": 248}, {"referenceID": 2, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 277, "endOffset": 280}, {"referenceID": 22, "context": "[23] proposed an early DML method, which minimizes the distance between similar points, while enlarging the distance between dissimilar points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], relative comparison constraints that involve three points at a time are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Neighborhood Components Analysis (NCA) [6] is proposed to learn a Mahalanobis distance for the k-NN classifier by maximizing the leave-one-out k-NN performance.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "[1] proposed a DML method for clustering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Large Margin Nearest Neighbor (LMNN) DML model [22] aims to produce a mapping, so that the k-nearest neighbors of any given sample belong to the same class, while samples from different classes are separated by large margins.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "Similarly, a Support Vector-based method is proposed in [15].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Also, LMNN is further extended to a Multi-Task Learning variation [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Another multi-task DML model is proposed in [25] that searches for task relationships.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "In [7], the authors proposed a general framework for sparse DML, such that several previous works are subsumed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "Recently, an eigenvalue optimization framework for DML was developed an presented in [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Moreover, the connection between LMNN and Support Vector Machines (SVMs) was discussed in [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 18, "context": "In the early work of [19], the Lagrange dual problem of the proposed DML formulation is derived, and the DML method is kernelized in the dual domain.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "Information-Theoretic Metric Learning (ITML) [4] is another kernelized method, which is based on minimizing the Kullback-Leibler divergence between two distributions.", "startOffset": 45, "endOffset": 48}, {"referenceID": 17, "context": "The kernelization of LMNN is discussed in [18] and [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The kernelization of LMNN is discussed in [18] and [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Moreover, a Kernel Principal Component Analysis (KPCA)-based kernelized algorithm is developed in [3], such that many DML methods, such as LMNN, can be kernelized.", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "In [12], the Mahalanobis matrix and kernel matrix are learned simultaneously.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8] and its extended work [9], the authors proposed a framework that builds connections between kernel learning and DML in the kernel-induced feature space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [8] and its extended work [9], the authors proposed a framework that builds connections between kernel learning and DML in the kernel-induced feature space.", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "is discussed in [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Before introducing our methods, in this section we will briefly review the concept of Reproducing Kernel Hilbert Space (RKHS) for vector-valued functions as presented in [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "The functional of Problem (6) satisfies the conditions stipulated by the Representer Theorem for Hilbert spaces of vector-valued elements (Theorem 5 in [14]) and, therefore, for a fixed value of L, the unique minimizer f\u0302 is of the form:", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "As noted in [11], this matrix equation can be solved for C as follows:", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "The second one relies on a popular DML method, namely the Large Margin Nearest Neighbor (LMNN) DML method [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "We also selected two kernelized approaches for comparison, namely, Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "We also selected two kernelized approaches for comparison, namely, Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "Similar to [8], we compare the produced mappings of our methods to Kernel Principal Component Analysis (KPCA).", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of a low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches.", "creator": "LaTeX with hyperref package"}}}