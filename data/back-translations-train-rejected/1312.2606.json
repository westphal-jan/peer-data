{"id": "1312.2606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2013", "title": "Multi-Task Classification Hypothesis Space with Improved Generalization Bounds", "abstract": "This paper presents a RKHS, in general, of vector-valued functions intended to be used as hypothesis space for multi-task classification. It extends similar hypothesis spaces that have previously considered in the literature. Assuming this space, an improved Empirical Rademacher Complexity-based generalization bound is derived. The analysis is itself extended to an MKL setting. The connection between the proposed hypothesis space and a Group-Lasso type regularizer is discussed. Finally, experimental results, with some SVM-based Multi-Task Learning problems, underline the quality of the derived bounds and validate the paper's analysis.", "histories": [["v1", "Mon, 9 Dec 2013 21:27:23 GMT  (46kb)", "http://arxiv.org/abs/1312.2606v1", "18 pages, 4 figures, submitted to IEEE Transactions on Neural Networks and Learning Systems"]], "COMMENTS": "18 pages, 4 figures, submitted to IEEE Transactions on Neural Networks and Learning Systems", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cong li", "michael georgiopoulos", "georgios c anagnostopoulos"], "accepted": false, "id": "1312.2606"}, "pdf": {"name": "1312.2606.pdf", "metadata": {"source": "META", "title": "Multi-Task Classification Hypothesis Space with Improved Generalization Bounds", "authors": ["Cong Li", "Michael Georgiopoulos", "Georgios C. Anagnostopoulos"], "emails": ["congli@eecs.ucf.edu,", "michaelg@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.26 06v1 [cs.LG] Dec 9 2Keywords: Multi-task learning, Kernel Methods, Generalization Bound, Support Vector Machines"}, {"heading": "1 Introduction", "text": "The basic philosophy of MTL is to learn several interconnected tasks with shared information at the same time, so the hope is to improve the generalization performance of each task by supporting other tasks. Formally, in a typical MTL setting with T tasks, we want to select T functions f = (f1) whose performance is optimized for each task based on a problem-specific criterion. Here [f1), \u00b7 \u00b7 \u00b7 \u00b7 fT (x), where x is an instance of some input set X, so that the performance of each task is optimized. [f1), \u00b7 \u00b7 \u00b7 fT (x), denotes the transposition of the row vector [f1), \u00b7 \u00b7 \u00b7 \u00b7 fT (x). \""}, {"heading": "2 Fixed Feature Mapping", "text": "Leave {xit, y i t} \u0441X \u00b7 {\u2212 1, 1}, i = 1, \u00b7 \u00b7, N, t = 1, \u00b7 \u00b7 \u00b7, T be i.i.d. training samples from any common distribution. Without loss of universality and for reasons of convenience, we assume an equal number of training samples for each task. Let H have an RKHS with reproducing core function k (\u00b7, \u00b7): X \u00d7 X 7 \u2192 R and associated attribute assignment \u03c6: X 7 \u2192 H. In the following, we give the theoretical analysis of our HS Fs when attribute assignment \u03c6 is specified."}, {"heading": "2.1 Theoretical Results", "text": "Considering our tasks, our goal is T linear functionalities ft (): H 7 \u2192 R, so that ft (x) = < wt, p (x) >, p (x) >, p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p), p (x), p (x), p (x), p (x), p (x), p), p (x), p (x), p), p (x), p (x), p (x), p), p (x), p (x), p (x), p), p (x), p (x), p), p (x), p (x), p (x), p (x), p), p (x), p (x), p), p (x, p), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x), p (x, p (x), p (x), p (x), p (x), p (x), p (x, p (x), p (x), p (x), p (x), p (x (x), p (x), p (x), p (x), p (x), p (x, p (x), p (x), p (x, p (x), p (x), p (x), p (x), p (x (x), p (x), p (x), p (x (x), p (x, p (x), p (x), p (x), p (x), p), p (x, p ("}, {"heading": "2.2 Analysis", "text": "It is worth mentioning some observations concerning the result of theorem 3. \u2022 It is not difficult to see that the limit of the ERC in (11) increases monotonously in s, just as in R (Fs). \u2022 As s \u2192 + \u221e, Fs is degraded to F. In this case, it should be noted that this limit coincides with the one given in [20]. This is due to the following relation between F \u00b2 and the HS of [20], F introduced in section 1: First, the operator A in F should be the identity operator, and then x in F should be an element of H, i.e., x in F (x) in F. Then F. \u2022 Obviously, if s is finite, the limit for Fs, which is of the order O (1 T 1 s \u221a 1 N), is preferred over the order of F given above."}, {"heading": "3 Learning the Feature Mapping", "text": "In this section we will consider the selection of the feature mapping \u03c6 during training using an MKL approach. In particular, we assume that \u03c6 = (\u221a \u03b81\u03c61, \u00b7 \u00b7 \u00b7, \u221a \u03b8M\u03c6M) \u0445 H1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 HM, with each \u03c6m: X 7 \u2192 Hm being selected before training."}, {"heading": "3.1 Theoretical Results", "text": "Consider the following HSFs, r, {x 7 \u2192 [< w1, \u03c6 (x) > \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Situations (x), < wT, \u03c6 (x) >] \":\" wt, \"\" p, \"\" p, \"\" p, \"\" p, \"\" p, \"\" p, \"\" p, \"\" p, \"\" \"p,\" \"\", \"p,\" p, \"p,\" p. \"If we follow the same derivative procedure of Lemma 1, we can verify that (7) also applies to Fs, r.\" Therefore, we only have to estimate the ERC. Similar to the previous section, we first give the results with respect to the monotonicity of R (Fs, r).Lemma 3. Let's leave \"p,\" [p], \"p\" p, \"\" p, \"\" p, \"\" \"\" m. \""}, {"heading": "3.2 Analysis", "text": "Again, it is worth commenting on the results of Theorem 6 and Korollary 1: \u2022 In general, there is a limit of order O (1T 1s). Obviously, s \u2192 + \u221e is least preferred, since its limit does not decrease with increasing number of tasks. Furthermore, the limit of the individual taskMKL scenario examined in [15] is okay O (M 1 r). Compared to O (M 1 r) min (M 1 r)), limited by individual taskMKL scenarios examined in [15], our limit for MT-MKL is narrower, for almost all M if r is small, which is usually a preferred setting. \u2022 If r \u00b2 logMT, the limit in (15) is given, the limit increases in relation to s."}, {"heading": "4 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Relation to Group-Lasso type regularizer", "text": "Theorem 7: The HS Fs is equivalent to FGLs, {x 7 \u2192 [< w1, \u03c6 (x) >, \u00b7 \u00b7, < wT, \u03c6 (x) >] \": (T = 1, < wt = 2 s \u2264 R} (17) Likewise, Fs, r is equivalent to FGLs, r, {x 7] [< w1, \u03c6 (x) >, < wT, \u03c6 (x) < (x) >: (T = 1, wt) >]\" (18): (T = 1, p = 2, p = 2, p = 2, p = 2, p = 2, p = 1, p = 2, p = 1, p = 1, p = 2, p = 2, p = 4, p = 4, p = 18): (T = 1, p = 2, p = 2, p = 2, p = 2, p = 2, p = 2, p = 1, p = 2, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p, p, p = 1 p, p, p, p, p =, p, p, p, p, p, p =, p, p, p, p, p, p, p =, p, p, p, p, p =, p, p, p, p =, p, p, p, p, p, p =, p, p, p =, p, p, p, p, p =, p, p, p =, p, p, p, p =, p, p, p, p, p =, p, p, p, p, p, p =, p, p, p, p, p, p =, p, p, p =, p, p, p, p, p, p =, p, p, p, p, p, p =, p =,"}, {"heading": "4.2 Other related works", "text": "It is a question of whether it is a question of a way in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question is about the question, to what is about the question, to what is about the question to what extent it is about the question, to what is about the question, to what is about the question is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question, to what is to what extent it is about the question, to what is about the question to what is about the question, to what extent it is about the question to what is about the question, to what extent it is about the question to what is about the question, to what extent it is about the question, to what is about the question is about the question, to what is about the question, to what is to what is to what is about the question, to what is to what is to what is to what is to what is the question, to what is to what"}, {"heading": "5 Experiments", "text": "In this section, we will experimentally examine the generalization limits of our HSs. First, we will evaluate the discrepancy between the ERC of Fs, Fs, r and their boundaries. We will experimentally show that the boundary gives a good estimate of the relevant ERC. Then, we will consider a new SVM-based MTL model that uses Fs as HS. The model will then be expanded to enable MT-MKL by using Fs, r as HS."}, {"heading": "5.1 ERC Bound Evaluation", "text": "Considering a data set and a pre-selected core function, however, we can calculate the core matrices Kt, t = 1, \u00b7 \u00b7 \u00b7, T. Then, the ERC is given by equation (10). To approximate the expectation, we use the Monte Carlo simulation by taking a large number, D, of i.i.d samples for the \u03c3t's from a uniform distribution on the hyper cube {\u2212 1, 1} N. Then, for each sample, we evaluate the argument of expectation and the average of the results. For Fs, r, the ERC value is calculated as in the first equation of (13). For each of the D samples of the cube, we can calculate the corresponding ut. Then, we solve the maximization problem by using CVX [11, 12]. Finally, we calculate the average of the D values to approximate the ERC value."}, {"heading": "5.2 SVM-based Model", "text": "In this subsection, we present a new SVM-based model that reflects our proposed problem."}, {"heading": "5.3 Experimental Results on the SVM-based model", "text": "We conducted our experiments on two well-known and commonly used multi-task datasets, Letter and Landmine, and two handwritten datasets, MNIST and USPS. Therefore, the letter dataset was described in the previous subsection. However, due to the large size of the original dataset, we randomly selected 200 dots for each letter to construct a single region of landmine fields, with the exception of the letter j, as it contains only 189 samples in total; the other 14 tasks correspond to regions that are bare earth or desert; the tasks include various 9-dimensional features extracted from radar images that capture a single region of landmine fields; the goal is to detect landmines in certain regions that are relatively highly foliated; the tasks correspond to regions that are bare earth or desert; the tasks include different sets of data that vary from 30 to 96 samples."}, {"heading": "6 Conclusions", "text": "In this thesis, we proposed a Multi-Task Learning (MTL) hypothesis Space (HS) Fs, which includes discriminatory T functions parameterized by weights wt. Weights are controlled by standard ball constraints, whose radii are variable and estimated during the training phase. It extends to a HS F limit previously studied in the literature where the radii are predetermined. It turns out that the latter space is a special case of Fs when s \u2192 + \u221e. We have derived and analyzed the generalization limit of Fs and shown that the limit in terms of s monotonous increases. Even in the optimal case (s = 1) a limit of the order O (\u221a log T) is reached. We extended the HS to Fs, r, which is suitable for Multi-Task Multiple Kernel Learning (MT-MKL)."}, {"heading": "Acknowledgments", "text": "C. Li acknowledges partial support from the National Science Foundation (NSF) grant number 0806931. In addition, M. Georgiopoulos acknowledges partial support from the NSF grant number 0525429, no 0963146, no 1200566 and no 1161228. Finally, G. C. Anagnostopoulos acknowledges partial support from the grant numbers 0717674 and no 0647018. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Preliminaries", "text": "In this subsection, we provide two results that are used in the following sections: Lemma 4. Leave p \u2265 1, x, a, a, so that a 0 and a 6 = 0. Then, max x, a, x =, a, p (31), where a (x), {x: x, p, p, \u2264 1}. This problem can be proved simply by applying Lagrange's multiplier method with respect to the maximization problem. Lemma 5. Leave x1, \u00b7 \u00b7, xn, H, then we have that E\u03c3, n, p, p, p, p, p, p, p, p, (p, i = 1, xi, 2, p, 2 (32) for each p where \u0441i are the radiator-distributed random variables. For 1, p < 2, the above result can simply be proved by Lyapunov's inequality."}, {"heading": "7.2 Proof to Lemma 2", "text": "The first proof: Fs is equivalent to the following HS: Fs, {x 7 \u2192 (\u03bb1 < w1, \u03c6 (x) >, \u00b7 \u00b7, \u2211 T < wT, \u03c6 (x) >) \"(33) According to the same reasoning of the equations (1) and (2) in [6] we know that wt = \u2211 N i = 1 \u03b1 i t\u03c6 (x i t), and the condition: wt \u00b2 2 \u2264 R is equivalent to \u03b1 tKt\u03b1t \u2264 R. Therefore, based on the definition of ERC given in Equation (8), we have thatR (Fs) = 2TN E\u043c {sup\u03b1t \u0445 FsT \u0445 t = 1\u041at\u043c R = maxi \u00b2 (34), where Fs = \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2."}, {"heading": "7.3 Proof to Theorem 1", "text": "Proof. First, it should be noted that in Equation 1 > s2 \u2265 1 we have this 1 \u2264 s * 1 < s * 2, which means that in Equation (37) we immediately have R-1 (Fs1) \u2265 R-2 (Fs2), which gives the monotony of R-2 (Fs) in relation to s."}, {"heading": "7.4 Proof to Theorem 2", "text": "Proof. Similar to the proof for Lemma 2, we write the ERC of F: R: A (F: A) = 2 TNE\u03c3 {sup: A (F: T) = 1\u03c3 \"tKt\u03b1t\" (38) Optimize with respect to \u03b1t gives R: A (F: A) = 2 \u221a RTN E\u03c3 {T: T: A (F: A) = 1 \u043c \"tKt\u03c3t\" (39) Based on Lemma 2, we immediately get R: A (F: A) = R: A (F: A)."}, {"heading": "7.5 Proof to Theorem 3", "text": "Proof: As sqq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q c q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "7.6 Proof to Lemma 3", "text": "If we define Kt M = 1 KtK, then we can write writeR (Fs) = 2TN E\u03c3 {sup\u03b1t-Fs, rT-T = 1\u03bbt\u03c3 \"tKt\u03b1t\" (45), where Fs = {\u03b1t | \u03b1-tKt\u03b1t \u2264 \u04212tR, \u0441stststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststtistststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "7.7 Proof to Theorem 4", "text": "Consider Equation (13) and (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9) (9), (9) (9), (9), (9), (9) (9), (9), (9), (9) (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9) (9), (9), (9) (9), (9), (9), (9),"}, {"heading": "7.8 Proof to Theorem 5", "text": "Proof. Define Kt, \u0445 M m = 1 \u03b8mK m t, \u0394t, thenR (F-r) = 2TN E\u03c3 {sup\u03b1t-F-T-T-T = 1\u03c3 \"tKt\u03b1t\" (48) Fix \u03b8 and optimize with respect to \u03b1t gives R-R-R (F-r) = 2TN-RE\u03c3 {sup\u03b8-r-T-T-T-T-T-T-T-T-T-T-T-T (49) Based on Equation (13) we immediately get R-R (F-r) = R-R (F + \u221e, r)."}, {"heading": "7.9 Proof to Theorem 6", "text": "s inequality, we let c, max {0, 1 r \u00b2 -2 s \u00b2 have that R \u00b2 (Fs, r) \u2264 2TN \u00b2 RE\u03c3 {sup\u03b8) T \u00b2 t = 1 (1) T \u00b2 t = 1 (1) T \u00b2 t = 1 s \u00b2 2) 1s \u00b2 (50) Applied to Jensen's inequality, we have that R \u00b2 (Fs, r) \u2264 2TN \u00b2 s \u00b2 s \u00b2 s \u00b2 (T, M \u00b2 t = 1Ectum (T) s \u00b2 2 s \u00b2 2 s \u00b2 2 s \u00b2 (1) 1s \u00b2 (50) Applied to Jensen's inequality, we have that R \u00b2 (Fs, r) \u2264 2TN \u00b2 s \u00b2 s \u00b2 s \u00b2 (T, M \u00b2 t = 1Ectum (T) s \u00b2 s \u00b2 s \u00b2 (2), 1 s \u00b2 (T \u00b2), 2 (T), T (T), T (T), T (T), T (T), 2 (T), T (T), T (T), T (T), T (T), T (2), T (T (T), T (2), T (T (T)."}, {"heading": "7.10 Proof to Corollary 1", "text": "s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Fs, r) \u2264 T 1 s + (Fs, r) \u2264 T 1 s + (Fs, r) \u2265 s = 1. If r + (Fs, r) = 2T1 s \u00b2 (Fs, r) = 2T1 s \u00b2 (Ne, 2eRM 1 r \u00b2 logT = 2T (RT) 2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s) \u2264 T 1 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (RT) 2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (Fs, r) \u2264 T 1 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (RT)) s (RT)) s + (RT) s + (RT) s + (RT) s +, r) \u2265 s + (RT) \u2265 s (RT) \u2265 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s) \u2265 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (RT) s (RT)) s (RT)) s (RT)) s (RT) s + (RT) s +) s + (RT)"}, {"heading": "7.11 Proof to Theorem 7", "text": "Evidence. We have already shown that the ERC of Fs isR (Fs) = 2TN E\u03c3 {sup\u03b1, \u03bbT \u2211 t = 1\u03c3 \"tKt\u03b1t} (56) It is not difficult to see that the optimization of the following problem sup \u03b1, \u03bbT \u0445 t = 1\u03c3\" tKt\u03b1ts.t. \u03b1 \"tKt\u03b1t \u2264 \u03bb2tR\" s \u2264 1 (57) with respect to \u03b1t must reach its optimum at the limit, i.e., the optimum must satisfy \"tKt.\" tK\u03b1t = \u03bb2tR. Therefore, problem (57) can be rewritten: Assumption \u03b1, \"T \u0445 t = 1\u03c3\" tKt\u03b1ts.t. \"tKt\u03b1t\" = \u03bb 2 tR \"s \u2264 1 (58) Replacing the first constraint in the second leads directly to the result. The proof with respect to Fs, r is similar, and therefore we omit it."}], "references": [{"title": "Variable sparsity kernel learning", "author": ["Jonathan Aflalo", "Aharon Ben-Tal", "Chiranjib Bhattacharyya", "Jagarlapudi Saketha Nath", "Sankaran Raman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Universal multi-task kernels", "author": ["Andrea Caponnetto", "Charles A. Micchelli", "Massimiliano Pontil", "Yiming Ying"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["Jianhui Chen", "Jiayu Zhou", "Jieping Ye"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Generalization bounds for learning kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Learning multiple tasks with kernel methods", "author": ["Theodoros Evgeniou", "Charles A. Micchelli", "Massimiliano Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Structured feature selection and task relationship inference for multi-task learning", "author": ["Hongliang Fei", "Jun Huan"], "venue": "In ICDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Multiple kernel learning algorithms", "author": ["Mehmet Gonen", "Ethem Alpaydin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Multi-stage multi-task feature learning", "author": ["Pinghua Gong", "Jieping Ye", "Changshui Zhang"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["M. Grant", "S. Boyd"], "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Regularization techniques for learning with matrices", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "lp-norm multiple kernel learning", "author": ["Marius Kloft", "Ulf Brefeld", "Soren Sonnenburg", "Alexander Zien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Marginal regression for multitask learning", "author": ["Mladen Kolar", "Han Liu"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Random Series and Stochastic Integrals: Single and Multiple", "author": ["Stanislaw Kwapien", "Wojbor A. Woyczynski"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert R.G. Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Multi-level lasso for sparse multi-task regression", "author": ["Aurelie C. Lozano", "Grzegorz Swirszcz"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Bounds for linear multi-task learning", "author": ["Andreas Maurer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "The rademacher complexity of linear transformation classes", "author": ["Andreas Maurer"], "venue": "In Gbor Lugosi and HansUlrich Simon, editors, Learning Theory, volume 4005 of Lecture Notes in Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Structured sparsity and generalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Large margin multi-task metric learning", "author": ["Shibin Parameswaran", "Kilian Q. Weinberger"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "lp\u2212lq penalty for sparse linear and sparse multiple kernel multitask learning", "author": ["Alain Rakotomamonjy", "Remi Flamary", "Gilles Gasso", "Stephane Canu"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "On general minimax theorems", "author": ["M. Sion"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1958}, {"title": "On multiple kernel learning with multiple labels", "author": ["Lei Tang", "Jianhui Chen", "Jieping Ye"], "venue": "In IJCAI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Globally convergent methods for semi-infinite programming", "author": ["G.A. Watson"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1981}, {"title": "Transfer metric learning by learning task relationships", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "In KDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Convex multitask learning with flexible task clusters", "author": ["Leon wenliang Zhong", "James T. Kwok"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "1 Introduction Multi-Task Learning (MTL) has been an active research field for over a decade [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 55, "endOffset": 65}, {"referenceID": 7, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 55, "endOffset": 65}, {"referenceID": 9, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 55, "endOffset": 65}, {"referenceID": 18, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 78, "endOffset": 86}, {"referenceID": 15, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 78, "endOffset": 86}, {"referenceID": 27, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 108, "endOffset": 112}, {"referenceID": 6, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 135, "endOffset": 148}, {"referenceID": 2, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 135, "endOffset": 148}, {"referenceID": 23, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 135, "endOffset": 148}, {"referenceID": 0, "context": "MTL has been successfully applied in feature selection [2, 8, 10], regression [19, 16], metric learning [28][23], and kernel-based MTL [7, 3, 24, 1] among other applications.", "startOffset": 135, "endOffset": 148}, {"referenceID": 20, "context": "A theoretically well-studied MTL framework is regularized linear MTL model, whose generalization bound is studied in [21, 13, 22].", "startOffset": 117, "endOffset": 129}, {"referenceID": 12, "context": "A theoretically well-studied MTL framework is regularized linear MTL model, whose generalization bound is studied in [21, 13, 22].", "startOffset": 117, "endOffset": 129}, {"referenceID": 21, "context": "A theoretically well-studied MTL framework is regularized linear MTL model, whose generalization bound is studied in [21, 13, 22].", "startOffset": 117, "endOffset": 129}, {"referenceID": 4, "context": "For example, [5] looks for group sparsity of w, [29] discovers group structure of multiple tasks, and [8, 10] select features in a MTL context.", "startOffset": 13, "endOffset": 16}, {"referenceID": 28, "context": "For example, [5] looks for group sparsity of w, [29] discovers group structure of multiple tasks, and [8, 10] select features in a MTL context.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "For example, [5] looks for group sparsity of w, [29] discovers group structure of multiple tasks, and [8, 10] select features in a MTL context.", "startOffset": 102, "endOffset": 109}, {"referenceID": 9, "context": "For example, [5] looks for group sparsity of w, [29] discovers group structure of multiple tasks, and [8, 10] select features in a MTL context.", "startOffset": 102, "endOffset": 109}, {"referenceID": 1, "context": "Such an approach is followed in [2, 14].", "startOffset": 32, "endOffset": 39}, {"referenceID": 13, "context": "Such an approach is followed in [2, 14].", "startOffset": 32, "endOffset": 39}, {"referenceID": 25, "context": "One example of this technique is given in [26].", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "One previous work which discussed the generalization bound of this method in a classification context is [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Obviously, the HS that is considered in [20] does not cover this scenario, since it only allows the operator A to be linear operator, which is not the case when A = \u03c6.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "Obviously, the HS in [20] fails to cover this HS, due to the lack of the constraint \u03c6 \u2208 \u03a9(\u03c6), which indicates that the feature mapping \u03c6 (hence, its corresponding kernel function) is learned during the training phase instead of of being selected beforehand.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "We refer readers to [18] and a survey paper [9] for details on MKL.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "We refer readers to [18] and a survey paper [9] for details on MKL.", "startOffset": 44, "endOffset": 47}, {"referenceID": 20, "context": "By considering the generalization bound of Fs based on the Empirical Rademacher Complexity (ERC) [21], we first demonstrate that the ERC is monotonically increasing with s, which implies that the tightest bound is achieved, when s = 1.", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "Additionally, if M kernel functions are involved, the bound is of order O( \u221a logM), which has been proved to be the best bound that can be obtained in single-task multi-kernel classification [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 0, "context": "The empirical error based on a surrogate loss function L\u0304 : R 7\u2192 [0, 1], which is a Lipschitz-continuous function that upper-bounds the 0/1 loss function, is defined as", "startOffset": 65, "endOffset": 71}, {"referenceID": 19, "context": "For the constraints on the wt\u2019s, instead of pre-defining a common radius R for all tasks as discussed in [20], we let \u2016wt\u2016 \u2264 \u03bbtR, where \u03bbt is learned during the training phase.", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "Let L\u0304 : R 7\u2192 [0, 1] be a Lipschitz-continuous loss function with Lipschitz constant \u03b3 and upper-bound the 0/1 loss function 1(\u2212\u221e,0](\u00b7).", "startOffset": 14, "endOffset": 20}, {"referenceID": 20, "context": "where R\u0302(Fs) is the ERC for MTL problems defined in [21]:", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "This lemma can be simply proved by utilizing Theorem 16 and 17 in [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "Define F\u0303 , {x 7\u2192 [\u3008w1, \u03c6(x)\u3009, \u00b7 \u00b7 \u00b7 , \u3008wT , \u03c6(x)\u3009]\u2032 : \u2016wt\u2016 \u2264 R}, which is the HS that is given in [20] under kernelized MTL setting, then F\u0303 is the HS with equal radius for each \u2016wt\u2016.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Note that this bound matches the one that is given in [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "This is because of the following relation between F\u0303 and the HS of [20], F , that is introduced in Section 1: First, let the operator A in F be the identity operator, and then let x in F be an element of H, i.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Compared to the O( \u221a M 1 r min(\u2308logM\u2309, \u2308r\u2217\u2309)) bound of single-task MKL scenario, which is examined in [15], our bound for MT-MKL is tighter, for almost all M , when r is small, which is usually a preferred setting.", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Note that it is proved that the best bound that can be obtained in single-task multiple kernel classification is of order O( \u221a logM) [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "First, we consider [1] and [24].", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "First, we consider [1] and [24].", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Specifically, [1] utilized the regularizer", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "In [24], the authors considered", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In the following, we discuss the difference between our work and the two theoretical works, [22] and [13], which derived generalization bound of the HSs that are similar to ours.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "In the following, we discuss the difference between our work and the two theoretical works, [22] and [13], which derived generalization bound of the HSs that are similar to ours.", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "In [22], the authors consider the regularizer \u2016w\u2016M , inf{ \u2211", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [13], the authors derived generalization bound for regularization-based MTL models, with regularizer \u2016W \u2016r,p , \u2016(\u2016w\u2016r, \u00b7 \u00b7 \u00b7 , \u2016w\u2016r)\u2016p.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Then, we solve the maximization problem by using CVX [11, 12].", "startOffset": 53, "endOffset": 61}, {"referenceID": 11, "context": "Then, we solve the maximization problem by using CVX [11, 12].", "startOffset": 53, "endOffset": 61}, {"referenceID": 24, "context": "Since it is a convex-concave min-max problem with compact feasible region, the order of min and max can be interchanged [25], which gives the objective function", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "We omit the details of this method and refer the readers to [27], since it is not the focus of this paper.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "1 of [15], which we summarize here: Obviously, for different s, the optimal solution (ft\u2019s) may be different.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "1 in [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "1 in [15].", "startOffset": 5, "endOffset": 9}], "year": 2013, "abstractText": "This paper presents a RKHS, in general, of vector-valued functions intended to be used as hypothesis space for multi-task classification. It extends similar hypothesis spaces that have previously considered in the literature. Assuming this space, an improved Empirical Rademacher Complexity-based generalization bound is derived. The analysis is itself extended to an MKL setting. The connection between the proposed hypothesis space and a Group-Lasso type regularizer is discussed. Finally, experimental results, with some SVM-based Multi-Task Learning problems, underline the quality of the derived bounds and validate the paper\u2019s analysis.", "creator": "LaTeX with hyperref package"}}}