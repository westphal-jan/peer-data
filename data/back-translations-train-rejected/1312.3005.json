{"id": "1312.3005", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2013", "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 74.4. A combination of techniques leads to 37% reduction in perplexity, or 11% reduction in cross-entropy (bits), over that baseline.", "histories": [["v1", "Wed, 11 Dec 2013 00:25:57 GMT  (13kb)", "https://arxiv.org/abs/1312.3005v1", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"], ["v2", "Fri, 28 Feb 2014 22:26:59 GMT  (13kb)", "http://arxiv.org/abs/1312.3005v2", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"], ["v3", "Tue, 4 Mar 2014 18:30:26 GMT  (13kb)", "http://arxiv.org/abs/1312.3005v3", "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper"]], "COMMENTS": "Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "tomas mikolov", "mike schuster", "qi ge", "thorsten brants", "phillipp koehn", "tony robinson"], "accepted": false, "id": "1312.3005"}, "pdf": {"name": "1312.3005.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 2,30 05v3 [cs.CL] 4 Mar 2The benchmark is available as a code.google.com project; in addition to the scripts required to rebuild the training / hold-out data, it also provides log probability values for each word in each of the ten hold-out datasets for each of the baseline n-gram models."}, {"heading": "1 Introduction", "text": "To name just a few: automatic speech recognition, machine translation, spell-checking, \"soft\" touch-screen keyboards and many applications for processing natural language depend on the quality of language models (LMs).LMs \"performance is largely determined by several factors: the amount of training data, the quality and consistency of training data with test data, and the choice of modeling technology to estimate from the data. It is widely accepted that the amount of data and the ability of a given estimation algorithm to accommodate large amounts of training are very important for providing a solution that successfully competes with the established n-gram LMs. At the same time, scaling a novel algorithm to a large amount of data requires a large amount of work, and represents a significant barrier to entering new modeling techniques. By selecting a billion words as the amount of training data, we hope to find a balance between the relevance of the large amount of data benchmarks in the benchmark."}, {"heading": "2 Description of the Benchmark Data", "text": "In the following experiments, we used text data obtained from the WMT11 website1. \u2022 The data preparation process was performed as follows: \u2022 All monolingual / English corpora were selected \u2022 Normalization and tokenization was performed using scripts distributed from the WMT11 website, slightly expanded to normalize various UTF-8 variants for common punctuation, e.g. \"\u2022 Duplicate sentences were removed, with the number of words falling from about 2.9 billion to about 0.8 billion (829250940, or more accurately, the counting of sentence boundary markers < S >, < S >) \u2022 Vocabulary (793471 words including sentence markers < S >, < S >) was constructed by dividing all words with a count of less than 3 \u2022 words outside the vocabulary."}, {"heading": "3 Baseline Language Models", "text": "As a starting point, we chose (Katz, 1995) and Interpolated (Kneser and Ney, 1995) (KN) 5-gram LMs, as they are the most common. Since these models are pruned in practice, often quite aggressively, we also illustrate the negative effects of entropy (Stolcke, 1998) on both models, similar to (Chelba et al., 2010)."}, {"heading": "4 Advanced Language Modeling Techniques", "text": "The number of advanced techniques for statistical language modeling is very large. It is not part of this paper to provide its detailed description, but we mention some of the most popular: \u2022 N-gram with modified Kneser-Ney smoothing (Chen and Goodman, 1996) \u2022 Cache (Jelinek et al., 1991) \u2022 Class-based (Brown et al., 1992) \u2022 Maximum entropy (Rosenfeld, 1994) \u2022 Structured (Chelba and Jelinek, 2000) \u2022 Neural networks based (Bengio et al., 2003) \u2022 Discriminative (Roark et al., 2004) \u2022 Random forest (Xu, 2005) \u2022 Bayesian (Teh, 2006) Below is a brief description of the models we used in our comparison using the benchmark data."}, {"heading": "4.1 Normalized Stupid Backoff", "text": "The Stupid Backoff LM was proposed in (Brants et al., 2007) as a simplified version of the backoff LM that is suitable for client-server architectures in a distributed computing environment. It does not apply discounting to relative frequencies and uses a single backoff weight instead of context-dependent backoff weights. As a result, the Stupid Backoff model does not generate normalized probabilities. For purposes of computational confusion, as reported in Table 1, the values returned by the model have been normalized across the entire LM vocabulary."}, {"heading": "4.2 Binary Maximum Entropy Language Model", "text": "The MaxEnt binary model was proposed in (Xu et al., 2011) and aims to avoid costly probability normalization during training by using independent binary predictors. Each predictor is trained on all positive examples, but the negative examples are drastically sampled down. This type of model is attractive for parallel training, so we have continued to investigate its effectiveness. We trained two models with a sampling rate of 0.001 for negative examples, with one using only n-gram characteristics and the other n-gram and 1-gram characteristics. We separated the phases of generating negative examples and tuning model parameters so that the output of the first phase can be divided by two models. Generating the negative examples took 7.25 hours with 500 machines, while fine-tuning the parameters with 5000 machines took 50 minutes and 70 minutes for the two models, respectively."}, {"heading": "4.3 Maximum Entropy Language Model with Hierarchical Softmax", "text": "Another way to reduce the training complexity of MaxEnt models is to use a hierarchical Softmax (Goodman, 2001b; Morin and Bengio, 2005). The idea is to estimate the probability of word groups, as in a class-based model - only the classes that contain the positive examples need to be evaluated. In our case, we examined a binary Huffman-tree representation of vocabulary, so that the evaluation of more frequent words takes less time. The idea of using word frequencies for a hierarchical Softmax was already presented in (Mikolov et al., 2011a)."}, {"heading": "4.4 Recurrent Neural Network Language Model", "text": "It has been shown that RNN LM significantly outperforms many other speech modeling techniques based on the Penn Treebank dataset (Mikolov et al., 2011b); it has also been shown that RNN models can be very well scaled to datasets with hundreds of millions of words (Mikolov et al., 2011c), even though the reported training times for the largest models were in the order of weeks. We shortened training times by a factor of 20-50 for major problems with a number of techniques that allow RNN training in typically 1-10 days with billions of words, > 1M vocabularies and up to 20B parameters on a single standard machine without GPU. These techniques were in order of importance: a) parallelization of training using available CPU threads, b) use of SIMD instructions, c) reduction of output parameters, without PU."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Performance of Individual Models", "text": "We focused on minimizing confusion in the selection of hyperparameters, but we also report on the time it takes to train the models. Training times are not necessarily comparable as they depend on the underlying implementation. Mapreduces can potentially handle larger data sets than stand-alone implementations, but require a great deal of communication and File E / O. Discussion of implementation details is outside the scope of this paper."}, {"heading": "5.2 Model Combination", "text": "The best results in terms of perplexity were obtained by linearly interpolating probabilities of all models. However, only some models had a significant weight in the combination; the weights were matched to the data collected. As can be seen in Table 2, the best perplexity is about 35% lower than the baseline - the modified Kneser-Ney model smoothed 5-gram model without counting limits. This corresponds to a reduction in cross-entropy (bits) of about 10%. Slightly surprisingly, the SBO model in the linear combination receives a relatively high weight, despite its poor performance in perplexity, while the KN baseline receives a relatively low weight compared to the other models in the combination."}, {"heading": "6 Conclusion", "text": "We have introduced a new dataset to measure research progress in statistical language modeling; the benchmark dataset is based on resources freely available on the Web, so a fair comparison of different techniques is easily possible; the importance of such efforts is undeniable: it has often been seen in the history of research that significant progress can be made when different approaches are measurable, reproducible, and the entry barrier is low; the choice of approximately one billion words may seem somewhat restrictive; in fact, it is unlikely that new techniques will be immediately competitive on a large dataset; however, mathematically expensive techniques can still be compared, for example, by using only the first or the first 10 partitions of this new dataset, which is equivalent to approximately 10 million and 100 million words; however, in order to achieve effective results in areas such as speech recognition and machine translation, the language modeling of techniques that need to be scaled to large amounts of data is very promising."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Large language models in machine translation", "author": ["Brants et al.2007] T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "ClassBased n-gram Models of Natural Language", "author": ["Brown et al.1992] P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J. Della Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Finding Structure in Time", "author": ["J. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A bit of progress in language modeling, extended version", "author": ["J.T. Goodman"], "venue": "Technical report MSR-TR-2001-72", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Classes for fast maximum entropy training", "author": ["J.T. Goodman"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "A Dynamic Language Model for Speech Recognition", "author": ["Jelinek et al.1991] F. Jelinek", "B. Merialdo", "S. Roukos", "M. Strauss"], "venue": "In Proceedings of the DARPA Workshop on Speech and Natural Language", "citeRegEx": "Jelinek et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1991}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] C. Chelba", "F. Jelinek"], "venue": "Computer Speech & Language", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1996] S.F. Chen", "J.T. Goodman"], "venue": "In Proceedings of ACL", "citeRegEx": "Chen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1996}, {"title": "Shrinking exponential language models", "author": ["S.F. Chen"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Chen.,? \\Q2009\\E", "shortCiteRegEx": "Chen.", "year": 2009}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "In IEEE Transactions on Acoustics, Speech and Signal Processing", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Improved Backing-Off For M-Gram Language Modeling", "author": ["Kneser", "Ney1995] R. Kneser", "H. Ney"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al.2010] T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions ofl Recurrent Neural Network Language Model", "author": ["Mikolov et al.2011a] T. Mikolov", "S. Kombrink", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Statistical Language Models based on", "author": ["T. Mikolov"], "venue": "Neural Networks. Ph.D. thesis, Brno University of Technology", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] A. Mnih", "G. Hinton"], "venue": "In Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] F. Morin", "Y. Bengio"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Discriminative language modeling with conditional random fields and the perceptron algorithm", "author": ["Roark et al.2004] B. Roark", "M. Saralar", "M. Collins", "M. Johnson"], "venue": "Proceedings of ACL", "citeRegEx": "Roark et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2004}, {"title": "Adaptive Statistical Language Modeling: A Maximum Entropy Approach", "author": ["R. Rosenfeld"], "venue": "Ph.D. thesis,", "citeRegEx": "Rosenfeld.,? \\Q1994\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1994}, {"title": "Learning internal representations by back-propagating errors", "author": ["G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Entropy-based Pruning of Back-off Language Models", "author": ["A. Stolcke"], "venue": "In Proceedings of News Transcription and Understanding Workshop", "citeRegEx": "Stolcke.,? \\Q1998\\E", "shortCiteRegEx": "Stolcke.", "year": 1998}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["R. Schluter", "H. Ney"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "A hierarchical Bayesian language model based on PitmanYor processes", "author": ["Y.W. Teh"], "venue": "In Proceedings of Coling/ACL", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Random forests and the data sparseness problem in language modeling", "author": ["Peng Xu"], "venue": null, "citeRegEx": "Xu.,? \\Q2005\\E", "shortCiteRegEx": "Xu.", "year": 2005}, {"title": "Efficient Subsampling for Training Complex Language Models", "author": ["Xu et al.2011] Puyang Xu", "A. Gunawardana", "S. Khudanpur"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Xu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Speed Regularization and Optimality in Word Classing", "author": ["Zweig", "Makarychev2013] G. Zweig", "K. Makarychev"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Zweig et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "This follows the work of Goodman (2001a), who explored performance of various language modeling techniques when applied to large data sets.", "startOffset": 25, "endOffset": 41}, {"referenceID": 0, "context": "Another contribution is that we provide strong baseline results with the currently very popular neural network LM (Bengio et al., 2003).", "startOffset": 114, "endOffset": 135}, {"referenceID": 21, "context": "Since in practice these models are pruned, often quite aggressivley, we also illustrate the negative effect of (Stolcke, 1998) entropy pruning on both models, similar to (Chelba et al.", "startOffset": 111, "endOffset": 126}, {"referenceID": 6, "context": "\u2022 Cache (Jelinek et al., 1991)", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "\u2022 Class-based (Brown et al., 1992)", "startOffset": 14, "endOffset": 34}, {"referenceID": 18, "context": "\u2022 Maximum entropy (Rosenfeld, 1994)", "startOffset": 18, "endOffset": 35}, {"referenceID": 0, "context": "\u2022 Neural net based (Bengio et al., 2003)", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": "\u2022 Discriminative (Roark et al., 2004)", "startOffset": 17, "endOffset": 37}, {"referenceID": 24, "context": "\u2022 Random forest (Xu, 2005)", "startOffset": 16, "endOffset": 26}, {"referenceID": 23, "context": "\u2022 Bayesian (Teh, 2006)", "startOffset": 11, "endOffset": 22}, {"referenceID": 1, "context": "The Stupid Backoff LM was proposed in (Brants et al., 2007) as a simplified version of backoff LM, suited to client-server architectures in a distributed computing environment.", "startOffset": 38, "endOffset": 59}, {"referenceID": 25, "context": "The Binary MaxEnt model was proposed in (Xu et al., 2011) and aims to avoid the expensive probability normalization during training by using independent binary predictors.", "startOffset": 40, "endOffset": 57}, {"referenceID": 14, "context": "The Recurrent Neural Network (RNN) based LM have recently achieved outstanding performance on a number of tasks (Mikolov, 2012).", "startOffset": 112, "endOffset": 127}], "year": 2014, "abstractText": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.", "creator": "LaTeX with hyperref package"}}}