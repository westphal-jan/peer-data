{"id": "1312.3020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2013", "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", "abstract": "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, click through data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to partition. As a consequence most distributed algorithms on these graphs are communication intensive. Many algorithms on natural graphs involve an Allreduce: a sum or average of partitioned data which is then shared back to the cluster nodes. Examples include PageRank, spectral partitioning, and many machine learning algorithms including regression, factor (topic) models, and clustering. In this paper we describe an efficient and scalable Allreduce primitive for power-law data. We point out scaling problems with existing butterfly and round-robin networks for Sparse Allreduce, and show that a hybrid approach improves on both. Furthermore, we show that Sparse Allreduce stages should be nested instead of cascaded (as in the dense case). And that the optimum throughput Allreduce network should be a butterfly of heterogeneous degree where degree decreases with depth into the network. Finally, a simple replication scheme is introduced to deal with node failures. We present experiments showing significant improvements over existing systems such as PowerGraph and Hadoop.", "histories": [["v1", "Wed, 11 Dec 2013 02:33:45 GMT  (1390kb,D)", "http://arxiv.org/abs/1312.3020v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.MS", "authors": ["huasha zhao", "john canny"], "accepted": false, "id": "1312.3020"}, "pdf": {"name": "1312.3020.pdf", "metadata": {"source": "CRF", "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", "authors": ["Huasha Zhao"], "emails": ["hzhao@cs.berkeley.edu", "jfc@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Keywords-Allreduce; butterfly network; fault tolerant; big data; I. INTRODUCTIONPower-law statistics are the norm for most behavioural datasets, i.e. data generated by people, including the web graph, social networks, text data, clickthrough data etc. by power-law, we mean that the probability distributions of elements in one or both (row and column) dimensions of these matrices follow a function of the formp d \u2212 \u03b1 (1) where d is the degree of that feature (the number of nonzeros in the corresponding row or column). These datasets are large: 40 billion etices for the web graph, terabytes for social media logs and news archives is."}, {"heading": "A. Applications", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves."}, {"heading": "B. Summary of Work and Contributions", "text": "In this paper, we describe Sparse Allreduce, a communication primitive that supports high-performance distributed machine learning on sparse data. Our Sparse Allreduce has the following properties: 1) Each network node specifies a sparse vector of initial values, and a vector of input indices whose values it wants to get from the protocol. 2) Index calculations (configuration) can be separated from value calculations and calculated only once for problems where the indices are fixed (e.g. pagerank iterations). 3) Sparse Allreduce Network is a nested, heterogeneous butterfly butterfly butterfly degree k that differs from one layer of the network to another."}, {"heading": "II. BACKGROUND: ALLREDUCE ON CLUSTERS", "text": "Allreduce is a common procedure for aggregating local model updates in distributed machine learning processes. This section discusses the practice of cross-processor data partitions and the popular Allreduce implementations and their limitations."}, {"heading": "A. AllReduce", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the ref\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "B. Partitions of Power-Law Data", "text": "As shown in [2], edge partitioning for large potential datasets is much more effective than vertex partitioning. [2] describes two edge partitioning schemes, one random and one greedy. Here, we will only use random edge partitioning - we believe this is more typical of data that \"sits on the network,\" although the results should be similar for other partitioning schemes."}, {"heading": "III. SPARSE ALLREDUCE", "text": "In this section we describe a typical workflow of distributed machine learning and introduce Sparse Allreduce. Examples of using Sparse Allreduce are discussed in Section III-B."}, {"heading": "A. Overview of Sparse AllReduce", "text": "In fact, most of us are able to play by the rules we have set ourselves, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules we have set ourselves. \""}, {"heading": "B. Use Case Examples", "text": "We offer two methods of configuration and reduction, for programmers. Configuration involves passing the outgoing indices (an array of vertex indices that need to be reduced (starting from outgoing) and incoming indices (an array of indices that need to be collected).After configuration, the reduce function is called to get the vertex values for the next iteration.The following code examples show how our primitive is used to execute the PageRank algorithm and mini-batch update algorithm (starting from outgoing vertibles), and returns the vertex values for the next update (starting from incoming vertices).The following code examples show how our primitive is used to execute the PageRank algorithm and mini-batch update algorithm.PageRank: var out = outgoing (G) var in = inbound (G) config (outgoing indices), in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in..in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in.in...in.......in.in.in........................."}, {"heading": "A. Layered Config and Reduce", "text": "Sparse Allreduce has two phases: config step and reduce step.In the configuration phase, each processor computes one map for each input vector. This map maps indices from the input vector to the sparse sum of all input vectors. The map facilitates adding values from above, and then the allgather phase upwards. In the reduction phase, the processors exchange the vertex values to be reduced. Only text values are communicated, since vertex indexes are already hard coded in the maps.We have described methods separately here as they simplify the explanation and the code includes these operations. We also offer a combined config-reduce method that performs both operations in a single round of communication in each layer, i.e. the indices and values during the downconfiguration phase are processed using the same messages.Code for config and reduce: / / D: Total levels from the metterfly / setup indices."}, {"heading": "B. Heterogeneous Butterfly Network", "text": "In a heterogeneous butterfly network, the degree of inputs and outputs varies from layer to layer. Figure 4 illustrates a 3 x 2 network in which each processor talks to 3 neighbors in layer 1 and 2 in layer 2. The advantage of using heterogeneous degrees is the balance of work. The heterogeneous butterfly network is a hybrid of pure round robin network and standard packet (grade 2) butterfly. Pure round robin, the packet size in each round robin communication is limited to C / M2, with C being the total size of the records, and M the number of machines. This can be too small - less than the packet effort. For example, the packet size in the Twitter torture curve is about 0.5 MB in a 64-node round robin network. Our tests suggested that the effective packet bottom for these EC2 nodes is the effective packet size 2-4 MB. On the other hand, the total number becomes the metallic number of a lot of the butterflies."}, {"heading": "C. Multi-Threading and Latency Hiding", "text": "Scientific computing systems typically maintain a high level of synchronicity between nodes performing a calculation. In cluster environments, we need to consider many sources of variability in the timing of nodes, latency, and throughput. While our network conceptually uses synchronized messages to different destinations to avoid congestion, this does not provide optimal performance in practice. Instead, we use multithreading and opportunistic communication, i.e. we start threads to send all messages simultaneously, and create a thread to process each message received. In the case of replicated messages, as soon as the first message from an eplicate group is received, the other threads waiting for duplicates are terminated and these copies are discarded. However, the network interface itself is a shared resource, so we must be careful that excessive threading does not impair performance by switching the active message threads."}, {"heading": "D. Language and Networking Libraries", "text": "Unfortunately, MPI implementations lacked the key functionality we needed to support multi-threading, asynchronous messaging, cancellation, etc., or these functionalities did not work over the Java interface. Java NIO was simply more complex to use, with no performance benefit, and all the features needed were easily implemented with sockets, and ultimately more suited to the level of abstraction and configurability we needed. We acknowledge that the network interface could be greatly improved, and the ideal choice would be RDMA over Ethernet (RoCE) and even better RoCE directly from the GPUs. This feature actually already exists (as GPUdirect for NVIDIA CUDA GPUs), but it is currently only available for Infiniband networks. Other benchmarks of this technology suggest that a four- to five-fold improvement should be possible."}, {"heading": "V. FAULT TOLERANCE", "text": "Machine failures are a reality of large clusters. In this section, we present a simple but relatively efficient fault tolerance mechanism to deal with multiple node failures."}, {"heading": "A. Data Duplication", "text": "Our approach is to replicate by one replication factor r, the data for each node and all messages. Thus, data via machine i also appears on replicas M + i to i + (r \u2212 1) \u0445 M. Similarly, any configuration and reduction message aimed at node j is also sent to replicas M + j to j + (r \u2212 1) \u043a M. When receiving a message expected by node j, the other replicas are also listened to. The first message received is used and the other listeners are cancelled.This protocol is completed unless all replicas in a group are dead. For example, if the replication factor is two, the probability is that this will happen in the event of random failures in an M node network, such as \u221a M (birthday paradox)."}, {"heading": "B. Packets Racing", "text": "Replication by r increases communication per node by r in the worst-case scenario (cancellations reduce it somewhat), resulting in some performance loss, as shown in the next section. On the other hand, replication offers potential benefits in networks with high latency or throughput variance, as it generates a race for the fastest response (as opposed to the unreplicated network, which is driven by the slowest path on the network instead)."}, {"heading": "VI. EXPERIMENTS", "text": "In this section, we evaluate the performance and scalability of Sparse Allreduce compared to other popular systems such as Hadoop, Spark and PowerGraph. In this section, we primarily examine three sets of data. \u2022 Twitter Follower's Graph. The graph consists of 60 million vertices and 1.5 billion edges. Figure 1 in [2] shows the legal ownership of this graph. \u2022 Yahoo! Altavista Web Graphic. This is one of the largest publicly available web graphics with 1.4 billion vertices and 6 billion edges. \u2022 Twitter Document-Term Graphic. The data set consists of 1 billion unique tweets, each with 40 million uni-gram words. The data is collected using the Twitter API in March 2013, which provides 10% of the sampled \"garden hose\" Twitterstream in XML format. All experiments are conducted on the Amazon EC2 Cluster, consisting of cluster Compute Nodes (1.4xlarge). Each node is connected to 108 and each other via virtualized cores."}, {"heading": "A. Sparsity of the Datasets", "text": "Table I illustrates the smallness of the partitioned data sets. The Twitter follower graph and the Yahoo web graph are distributed across 64 processors using a random edge partition, and the model size corresponds to the total number of vertices in the diagram. While the Twitter document term graph is partitioned by hours and tweets, each partition is a \"minibatch\" that can be fed into a subgradient / online method. The model dimension is the number of unigram features. As shown in Table I, all data sets show a strong smallness after the partition. The Yahoo web graph is the largest in terms of model size, and it is also the most economical of the three, with each partition containing only 3 percent of all vertices."}, {"heading": "B. Optimal System Parameters", "text": "In this section, we will empirically determine the optimal configuration of the butterfly degree to achieve the best sparse allreduce performance.Figure 5 records packet sizes at different levels of the butterfly network for different configurations in a 64-node cluster containing the random (edge) subdivided Twitter follower graphic.As shown in the figure, the 64 round robin topology per round sends 0.5 MB packets per round, but these are unlikely to fully utilize bandwidth. Also, for the butterfly configuration, although the packet size decreases with the depth of the network, the full Grade 2 butterfly can send packets of 17 MB for each machine in the first round of communication. Figure 6 plots the average reduction in time per iteration and throughput for different configurations, for both Twitter followers and Yahoo followers, this problem is almost evenly distributed in terms of the total number of the 4 networks."}, {"heading": "C. Effect of Multi-Threading", "text": "We compare the Allreduce runtime for different thread levels in Figure 7. All results are executed under the 16 \u00d7 4 configuration. Significant performance improvement can be observed by increasing the number of threads from one thread to 4 threads, and the figure also shows that the benefit of AddingTable I: Sparsity of the Partited Data Set Twitter Follower's Graph Yahoo Web Graph Twitter Document Graph Partition # of corners 12.1M 48M 5,1MTotal # of corners 60M 1,6B 40M Percentage of total corners 0.21 0,03 0,12 (a) Twitter Follower's Graph (b) Yahoo Web Graph Figure 6: Alltime per iteration and Thread level is marginally beyond 8 threads (remember that we are running on 8-core machines)."}, {"heading": "D. Cost of Fault Tolerance", "text": "Table II shows the overhead of data replication in terms of runtime. We compare an 8 \u00d7 4 network with replication to a 16 \u00d7 4 network and an 8 \u00d7 4 network without replication. The 8 \u00d7 4 network with error tolerance uses the same amount of resources as the 16 \u00d7 4 network: both require 64 machines. The data is divided into 32 parts and each part is hosted by 2 machines. It doubles the resource requirement compared to an 8 \u00d7 4 network with no error tolerance. As shown in the table, the impact of data duplication on runtime is moderate. In the 8 \u00d7 4 network, the replication version is only 10-15% slower than the version without replication. 64 machines have a fault tolerance of 50-60% slower than without error tolerance.We also compare runtime for different number of node failures. Without replication, the system cannot calculate the correct results. The replicated version is able to calculate the correct error tolerance with no error tolerance."}, {"heading": "E. Performance and Scalability", "text": "This year, it is only a matter of time before an agreement is reached, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "VII. RELATED WORKS", "text": "Our work, however, is closest to the GraphLab project, which also focuses on PowerLaw graphs and matrices. [1] Improves the infrastructure of Hadoop MapReduce by expressing asynchronous iterative algorithms with sparse computational dependencies. PowerGraph is an improved version of GraphLab, where concerns about graph mining were first proposed. We have taken a slightly more modular approach, isolating the allreduce primitive from matrix and machine learning modules. Pegasos project proposed GIM-V, a primitive generalization to a variety of graph mining tasks. We further generalize the primitive to mini-batch update algorithms that cover regressions, factor models, topic models, etc. There are a variety of similar distributed data mining systems [10], [15], [21], [21] approaches to hadoop based on the shared philosophy, [21] and the hadoop]."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we describe Sparse Allreduce for efficient and scalable distributed machine learning. Primitive Allreduce is particularly well adapted to the usual energy-related data used in machine learning and graph analysis. We demonstrated that the best approach is a hybrid between butterfly and round robin topologies, using a nested communication pattern and inhomogeneous layer grades. We added a replication layer to the network that offers a high degree of error tolerance with modest overhead. Finally, we presented a series of experiments that primitively examine the performance of Sparse Allreduce. We showed that it is significantly faster than other primitives and is currently limited by the performance of the underlying technology, Java Sockets. In the future, we hope to achieve further gains by using more advanced network layers that use RDMA over Converged Ethernet (RoCE). Our code is open source and is currently available as part of the BIDat suite, but can be distributed without further functionality."}], "references": [{"title": "Graphlab: A new framework for parallel machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein"], "venue": "arXiv preprint arXiv:1006.4990, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Powergraph: Distributed graph-parallel computation on natural graphs", "author": ["J.E. Gonzalez", "Y. Low", "H. Gu", "D. Bickson", "C. Guestrin"], "venue": "Proc. of the 10th USENIX conference on Operating systems design and implementation, OSDI, vol. 12.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "Proceedings of the 2010 international conference on Management of data. ACM, 2010, pp. 135\u2013146.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Butterfly mixing: Accelerating incremental-update algorithms on clusters", "author": ["H. Zhao", "J. Canny"], "venue": "SIAM International Conference on Data Mining, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Pegasus: A peta-scale graph mining system implementation and observations", "author": ["U. Kang", "C.E. Tsourakakis", "C. Faloutsos"], "venue": "Data Mining, 2009. ICDM\u201909. Ninth IEEE International Conference on. IEEE, 2009, pp. 229\u2013238.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Dryad: distributed data-parallel programs from sequential building blocks", "author": ["M. Isard", "M. Budiu", "Y. Yu", "A. Birrell", "D. Fetterly"], "venue": "ACM SIGOPS Operating Systems Review, vol. 41, no. 3, pp. 59\u201372, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Graphsig: A scalable approach to mining significant subgraphs in large graph databases", "author": ["S. Ranu", "A.K. Singh"], "venue": "Data Engineering, 2009. ICDE\u201909. IEEE 25th International Conference on. IEEE, 2009, pp. 844\u2013855.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "gapprox: Mining frequent approximate patterns from a massive network", "author": ["C. Chent", "X. Yan", "F. Zhu", "J. Han"], "venue": "Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on. IEEE, 2007, pp. 445\u2013450.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "gspan: Graph-based substructure pattern mining", "author": ["X. Yan", "J. Han"], "venue": "Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002, pp. 721\u2013724.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Disco: Distributed co-clustering with map-reduce: A case study towards petabyte-scale end-toend mining", "author": ["S. Papadimitriou", "J. Sun"], "venue": "Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on. IEEE, 2008, pp. 512\u2013521.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Big data analytics with small footprint: Squaring the cloud", "author": ["J. Canny", "H. Zhao"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Large scale online learning", "author": ["L.B.Y. Le Cun", "L. Bottou"], "venue": "Advances in neural information processing systems, vol. 16, p. 217, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 999999, pp. 2121\u20132159, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "An architecture for parallel topic models", "author": ["A. Smola", "S. Narayanamurthy"], "venue": "Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 703\u2013710, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast diameter estimation and mining in massive graphs with hadoop", "author": ["U. Kang", "C. Tsourakakis", "A.P. Appel", "C. Faloutsos", "J. Leskovec", "Hadi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Bandwidth optimal all-reduce algorithms for clusters of workstations", "author": ["P. Patarasuk", "X. Yuan"], "venue": "Journal of Parallel and Distributed Computing, vol. 69, no. 2, pp. 117\u2013124, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Benchmarking amazon ec2 for high-performance scientific computing", "author": ["E. Walker"], "venue": "Usenix Login, vol. 33, no. 5, pp. 18\u201323, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A performance analysis of ec2 cloud computing services for scientific computing", "author": ["S. Ostermann", "A. Iosup", "N. Yigitbasi", "R. Prodan", "T. Fahringer", "D. Epema"], "venue": "Cloud Computing. Springer, 2010, pp. 115\u2013131.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Haloop: Efficient iterative data processing on large clusters", "author": ["Y. Bu", "B. Howe", "M. Balazinska", "M.D. Ernst"], "venue": "Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 285\u2013296, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Avoiding communication in sparse matrix computations", "author": ["J. Demmel", "M. Hoemmen", "M. Mohiyuddin", "K. Yelick"], "venue": "Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1\u201312.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Communication-avoiding krylov subspace methods", "author": ["M. Hoemmen"], "venue": "Ph.D. dissertation, University of California, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Communication-avoiding parallel and sequential qr factorizations", "author": ["J. Demmel", "L. Grigori", "M. Hoemmen", "J. Langou"], "venue": "CoRR abs/0806.2159, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Many groups are developing tools to analyze these datasets on clusters [1]\u2013[10].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "Many groups are developing tools to analyze these datasets on clusters [1]\u2013[10].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].", "startOffset": 76, "endOffset": 79}, {"referenceID": 10, "context": "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].", "startOffset": 237, "endOffset": 241}, {"referenceID": 11, "context": "1) MiniBatch Machine Learning Algortihms: Recently there has been considerable progress in sub-gradient algorithms [12], [13] which partition a large dataset into minibatches and update the model using sub-gradients, illustrated in Figure 1.", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "1) MiniBatch Machine Learning Algortihms: Recently there has been considerable progress in sub-gradient algorithms [12], [13] which partition a large dataset into minibatches and update the model using sub-gradients, illustrated in Figure 1.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "large datasets show convergence in a single pass [12].", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "In practice to reduce communication overhead, the sample updates are batched in very similar fashion to sub-gradient updates [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Diameter Estimation In the HADI [15] algorithm for diameter estimation, the number of neighbours within hop h is encoded in a probabilistic bit-string vector b.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Allreduce is commonly implemented with 1) tree structure [16], 2) simple round-robin in fullmesh networks or 3) butterfly topologies [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Allreduce is commonly implemented with 1) tree structure [16], 2) simple round-robin in fullmesh networks or 3) butterfly topologies [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "As shown in [2], edge partitioning is much more effective for large, power-law datasets than vertex partitioning.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "The paper [2] describes two edge partitioning schemes, one random and one greedy.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "IVec [] outputs = new IVec[2];", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "upi = outputs[1];", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "Figure 1 in [2] shows the power-law property of this graph.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Many other distributed learning systems are under active development at this time [1]\u2013[6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Many other distributed learning systems are under active development at this time [1]\u2013[6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "[1] improves upon the Hadoop MapReduce infrastructure by expressing asynchronous iterative algorithms with sparse computational dependencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Our work is also related with research in distributed SpMV (Sparse Matrix Vector multiplication) algorithms [22]\u2013[24] in the parallel/scientific computing community.", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "Our work is also related with research in distributed SpMV (Sparse Matrix Vector multiplication) algorithms [22]\u2013[24] in the parallel/scientific computing community.", "startOffset": 113, "endOffset": 117}], "year": 2013, "abstractText": "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, clickthrough data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to partition. As a consequence most distributed algorithms on these graphs are communicationintensive. Many algorithms on natural graphs involve an Allreduce: a sum or average of partitioned data which is then shared back to the cluster nodes. Examples include PageRank, spectral partitioning, and many machine learning algorithms including regression, factor (topic) models, and clustering. In this paper we describe an efficient and scalable Allreduce primitive for power-law data. We point out scaling problems with existing butterfly and round-robin networks for Sparse Allreduce, and show that a hybrid approach improves on both. Furthermore, we show that Sparse Allreduce stages should be nested instead of cascaded (as in the dense case). And that the optimum throughput Allreduce network should be a butterfly of heterogeneous degree where degree decreases with depth into the network. Finally, a simple replication scheme is introduced to deal with node failures. We present experiments showing significant improvements over existing systems such as PowerGraph and Hadoop. Keywords-Allreduce; butterfly network; fault tolerant; big data;", "creator": "LaTeX with hyperref package"}}}