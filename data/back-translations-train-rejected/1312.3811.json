{"id": "1312.3811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2013", "title": "Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE", "abstract": "Policy Gradient methods that explore directly in parameter space are among the most effective and robust direct policy search methods and have drawn a lot of attention lately. The basic method from this field, Policy Gradients with Parameter-based Exploration, uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in \\emph{asymmetrical} reward distributed problems gathered with the usual baseline approach. The exploration parameters are still updated by a baseline approach - leaving the exploration prone to asymmetric reward distributions. In this paper we will show how the exploration parameters can be sampled quasi symmetric despite having limited instead of free parameters for exploration. We give a transformation approximation to get quasi symmetric samples with respect to the exploration without changing the overall sampling distribution. Finally we will demonstrate that sampling symmetrically also for the exploration parameters is superior in needs of samples and robustness than the original sampling approach.", "histories": [["v1", "Fri, 13 Dec 2013 14:10:30 GMT  (792kb,D)", "http://arxiv.org/abs/1312.3811v1", "Artificial Neural Networks and Machine Learning - ICANN 2013 Springer Berlin Heidelberg 2013. 130-137"]], "COMMENTS": "Artificial Neural Networks and Machine Learning - ICANN 2013 Springer Berlin Heidelberg 2013. 130-137", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["frank sehnke"], "accepted": false, "id": "1312.3811"}, "pdf": {"name": "1312.3811.pdf", "metadata": {"source": "CRF", "title": "Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE", "authors": ["Frank Sehnke"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The basic method in the area of Exploring Policy Gradients (PEPG) [8], Policy Gradients with Parameter-based Exploration (PGPE) [1], uses two samples that are symmetrical around the current hypothesis to circumvent misleading rewards in asymmetrical reward distribution problems collected using the usual baseline approach. [4] shows that symmetrical samples (SyS) are superior to even the optimal baseline. However, the exploration parameters are still updated by a baseline approach - making exploration vulnerable to asymmetrical reward distributions that are susceptible to asymmetrical distributions. While the optimal baseline has significantly improved this problem, as the sample shows [4], it is likely that the approximation of the baseline as a whole is achieved through symmetricism."}, {"heading": "2 Method", "text": "In this section we derive the supersymmetric sampling method (SupSyS). We show how the method relates to SyS and sampling with baseline and thus summarize the derivative from [1] for SyS and Baseline Sampling PGPE."}, {"heading": "2.1 Parameter-Based Exploration", "text": "In order to remain consistent with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed episode, story, trajectory, or introduction. In this context, the goal of amplification learning is to find the optimal policy parameters that maximize the agent's expected reward. (3) The probability policy used in standard PG is replaced by a probability distribution over the parameters for PGPE. The advantage of this approach is that the measures are deterministic and an entire story can therefore be generated from a single parameter sample. (This reduction in the samples-per-history is what reduces the variance in gradient estimation (see [1] for details."}, {"heading": "2.2 Sampling with a Baseline", "text": "If there is a sufficient sample, Eq. (3) will determine the reward gradient with arbitrary accuracy. However, each sample requires a complete history, which is costly. Following [9], we obtain a cheaper gradient estimate by establishing a single sample and comparing its reward r with a base reward b, given, for example, by a moving average over previous samples. Intuitively, if we adjust r > b to increase the probability of \u03b8, and r < b the opposite. If, as in [9], we use a step variable \u03b1i = \u03b1\u03c3 2 i in the direction of positive gradients (where \u03b1 is a constant), we obtain the following parameter update equations: \u0435 \u00b5i = \u03b1 (r \u2212 \u00b5i) (\u0445i \u2212 \u00b5i) (\u0445i = \u03b1 (r \u2212 b) (r \u2212 b) (r \u2212 b) (\u0421i \u2212 \u00b5i) (\u0421i \u2212 \u00b5i) (\u0421i \u2212 \u0441\u0441\u0441\u0441i) (\u0441\u0441\u0441\u0441\u0441i) (\u0442\u0438\u0442\u0438\u0442i \u2212 \u0442i)."}, {"heading": "2.3 Symmetric Sampling", "text": "While the baseline sample is efficient and reasonably accurate for most scenarios, it has several drawbacks. In particular, if the reward distribution is heavily distorted, the comparison between the reward of the sample and the baseline reward is misleading. A more robust gradient approximation can be found by measuring the reward difference between two symmetrical samples on either side of the current mean. That is, we select an error from the distribution N (0, \u03c3) and then create symmetrical parameter samples. (4) To get a benefit from Eq. (4). (4) To get a benefit from the two samples, we must use p + (r + \u2212) 2\u03c32i, (8) as a reward corresponding to the reward, as a reward by inserting the two samples into Eq. (3) and using Eq. (4) to get a benefit from Eq."}, {"heading": "2.4 Super-Symmetric Sampling", "text": "While SyS eliminates the misleading baseline problem for estimating the \u00b5 gradient, the \u03c3 gradient still uses a reward = q reward and is susceptible to this problem. On the other hand, there is no correct symmetric sample with respect to the standard deviation \u2212 \u2212 \u2212 because the standard deviation is limited to 0 on the one hand and unlimited on the positive side. Another problem is that 23 of the samples are on one side of the standard deviation and only 13 on the other - mirroring this sample to the other side of the standard deviation would therefore deform the normal distribution so much that it would no longer be close enough to meet the assumptions that lead to the PGPE update. Therefore, we have chosen to define the normal distribution by the mean and the mean deviation."}, {"heading": "3 Experiments and Results", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "4 Conclusions and Future Work", "text": "We have introduced SupSyS-PGPE, a completely basal-line free PGPE that uses quasi-symmetrical samples that include the exploration parameters. We have shown that this novel method of rastrigin function, as an example of a test function with exponentially many local Optima, is clearly superior to the standard SyS-PGPE and that both methods will be equivalent in their performance if the search space does not exhibit any disturbing local optimism. For future work, we would like to emphasize that SupSyS-PGPE can easily be combined with other extensions of PGPE. Multimodal PGPE [10] can be directly equipped with SupSyS sampling. Also, the natural grave used for PGPE in [3] can be defined via the SupSyS-Gradient rather than via the vanilla gradient of PGPE. If the full 4 supersymmetrical sample set is only used when the first Sub-Sub Sub-Sub-Sub-Sub Sub-Sub-Sub-Sub-Sub can be described as a Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub (Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub), such as Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub (Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub-Sub)."}], "references": [{"title": "Parameter-exploring policy gradients", "author": ["F. Sehnke", "C. Osendorfer", "T. R\u00fcckstie\u00df", "A. Graves", "J. Peters", "J. Schmidhuber"], "venue": "Neural Networks 23(4)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring parameter space in reinforcement learning", "author": ["T. R\u00fcckstie\u00df", "F. Sehnke", "T. Schaul", "D. Wierstra", "Y. Sun", "J. Schmidhuber"], "venue": "Paladyn. Journal of Behavioral Robotics 1(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks", "author": ["A. Miyamae", "Y. Nagata", "I. Ono"], "venue": "NIPS.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis and improvement of policy gradient estimation", "author": ["T. Zhao", "H. Hachiya", "G. Niu", "M. Sugiyama"], "venue": "Neural networks : the official journal of the International Neural Network Society", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient sample reuse in policy gradients with parameter-based exploration", "author": ["T. Zhao", "H. Hachiya", "V. Tangkaratt", "J. Morimoto", "M. Sugiyama"], "venue": "arXiv preprint arXiv:1301.3966", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Path integral policy improvement with covariance matrix adaptation", "author": ["F. Stulp", "O. Sigaud"], "venue": "arXiv preprint arXiv:1206.4621", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural evolution strategies", "author": ["D. Wierstra", "T. Schaul", "J. Peters", "J. Schmidhuber"], "venue": "Evolutionary Computation, 2008. CEC 2008., IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning 8", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Multimodal parameterexploring policy gradients", "author": ["F. Sehnke", "A. Graves", "C. Osendorfer", "J. Schmidhuber"], "venue": "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 1, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 2, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 3, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 4, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 5, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 141, "endOffset": 154}, {"referenceID": 6, "context": "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "The basic method from the field of Parameter Exploring Policy Gradients (PEPG) [8], Policy Gradients with Parameter-based Exploration (PGPE) [1], uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in asymmetrical reward distributed problems, gathered with the usual baseline approach.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "[4] shows that Symmetric Sampling (SyS) is superior even to the optimal baseline.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "While the optimal baseline improved this issue substantially, like shown again by [4], it is likely that removing the baseline altogether by a SyS wrt.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "We show how the method relates to SyS and sampling with a baseline, thereby summarizing the derivation from [1] for SyS and baseline sampling PGPE.", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "To stay conform with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed length episode, history, trajectory or roll-out.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "To stay conform with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed length episode, history, trajectory or roll-out.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "This reduction in samples-per-history is what reduces the variance in the gradient estimate (see [1] for details).", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "We name the distribution over parameters in accordance with [1] \u03c1.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "Following [9], we obtain a cheaper gradient estimate by drawing a single sample \u03b8 and comparing its reward r to a baseline reward b given e.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "If, as in [9], we use a step size \u03b1i = \u03b1\u03c3 2 i in the direction of positive gradient (where \u03b1 is a constant) we get the following parameter update equations:", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "[4] showed recently that an optimal baseline can be achieved for PGPE and the algorithm converges significantly faster with an optimal baseline of the form:", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Even though symmetric sampling requires twice as many histories per update, [1] and [4] have shown that it gives a considerable improvement in convergence quality and time.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Even though symmetric sampling requires twice as many histories per update, [1] and [4] have shown that it gives a considerable improvement in convergence quality and time.", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Multi-modal PGPE [10] can be equipped straight forward with SupSyS sampling.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Also the natural gradient used for PGPE in [3] can be defined over the SupSyS gradient instead over the vanilla gradient.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "3) a combination with the optimal baseline (described for PGPE in [4]) can yield a superior method to both SupSyS-PGPE and optimal baseline PGPE.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "Also importance mixing introduced for PGPE by [5] is applicable to SupSyS-PGPE.", "startOffset": 46, "endOffset": 49}], "year": 2013, "abstractText": "Policy Gradient methods that explore directly in parameter space are among the most effective and robust direct policy search methods and have drawn a lot of attention lately. The basic method from this field, Policy Gradients with Parameter-based Exploration, uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in asymmetrical reward distributed problems gathered with the usual baseline approach. The exploration parameters are still updated by a baseline approach leaving the exploration prone to asymmetric reward distributions. In this paper we will show how the exploration parameters can be sampled quasi symmetric despite having limited instead of free parameters for exploration. We give a transformation approximation to get quasi symmetric samples with respect to the exploration without changing the overall sampling distribution. Finally we will demonstrate that sampling symmetrically also for the exploration parameters is superior in needs of samples and robustness than the original sampling approach.", "creator": "LaTeX with hyperref package"}}}