{"id": "1312.4314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Learning Factored Representations in a Deep Mixture of Experts", "abstract": "Mixtures of Experts combine the outputs of several \"expert\" networks, each of which specializes in a different part of the input space. This is achieved by training a \"gating\" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\"where\") experts at the first layer, and class-specific (\"what\") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "histories": [["v1", "Mon, 16 Dec 2013 11:15:10 GMT  (1495kb,D)", "http://arxiv.org/abs/1312.4314v1", null], ["v2", "Wed, 19 Feb 2014 17:57:53 GMT  (1496kb,D)", "http://arxiv.org/abs/1312.4314v2", null], ["v3", "Sun, 9 Mar 2014 20:15:03 GMT  (1496kb,D)", "http://arxiv.org/abs/1312.4314v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david eigen", "marc'aurelio ranzato", "ilya sutskever"], "accepted": false, "id": "1312.4314"}, "pdf": {"name": "1312.4314.pdf", "metadata": {"source": "CRF", "title": "Learning Factored Representations in a Deep Mixture of Experts", "authors": ["David Eigen", "Ilya Sutskever"], "emails": ["deigen@cs.nyu.edu", "ranzato@fb.com", "ilyasu@google.com"], "sections": [{"heading": null, "text": "Mixtures of experts combine the results of multiple \"expert networks,\" each specializing in a different part of the input space, through the formation of a \"gated\" network that maps each input to a distribution across the experts. Such models promise to build larger networks that can be calculated cheaply during the test period and more parallelized during the training period. In this work, we expand the mix of experts to a stacked model, the Deep Mixture of Experts, with multiple groups of gating and experts. This exponentially increases the number of effective experts by linking each input to a combination of experts at each level, while maintaining a modest model size. In a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\"where\") experts on the first shift and class-specific (\"what\") experts on the second shift."}, {"heading": "1 Introduction", "text": "A fundamental limitation of these architectures, however, is that the entire network must be run for all inputs, and this computational load sets limits on network size. One way to scale these networks while keeping computational costs low is to increase the total number of parameters and hidden units, but to use only a small portion of the network for each given input. Then, experience a computationally inexpensive mapping function from input to the corresponding parts of the network. The expert model [4] is a continuous version of this: A scholarly gating network mixes the outputs of N \"expert networks to achieve final output. In this work, we expand the mix of experts to use a different gating network at each level in a multi-layer network, forming a deep mix of experts (DMoE), increasing the number of effective experts by introducing an exponential number of paths."}, {"heading": "2 Related Work", "text": "A standard mix of experts (MoE) [4] learns a series of expert networks fi together with a gating network g. Each fi maps the input x to C output (one for each class c = 1,..., C), while g (x) is a distribution over experts i = 1,.., N, which becomes 1. The final output is then derived from Eqn. 1FMoE (x) = N \u2211 i = 1 gi (x) Softmax (fi (x)) (1) = N \u2211 i = 1 p (ei | x) p (c | ei, x) = p (c | x) (2) This can also be seen as a probability model in which the final probability over classes is marginalized compared to the selection of experts: p (ei | x) = gi (x) = gi (ei) and p (c | ei, x) p (c | ei, x) = softmax (fi (x))."}, {"heading": "3 Approach", "text": "To expand MoE into a DMoE, we present two expert groups with gating networks (g1, f1i) and (g2, f2j), together with a final linear layer f3 (see Figure 1). The final output is obtained by the composition of the mixtures on each layer: z1 = N \u2211 i = 1 g1i (x) f 1 i (x) z2 = M \u2211 j = 1 g2j (z 1) f2j (z 1) F (x) = z3 = softmax (f3 (z2)). We put each f li on a single linear map with rectification and each g l i on two layers of linear maps with rectification (but with few hidden units); f3 is a single linear layer. See Section 4 for details.We train the network using stochastic gradient descending (SGD) with an additional restriction to gating tasks described below."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Jittered MNIST", "text": "We trained and tested our model on MNIST with random uniform ratios of \u00b1 4 pixels, resulting in grayscale images of 36 x 36. As explained above, the model was trained to classify digits into ten classes. For this task, we put all linear models f1i and f 2 j on an equalization plane, f 1 i (x) = max (0, W 1i x + b 1 i), and similarly for f 2 y. We put f3 on a linear plane, f3 (z2) = W 3z2 + b3. We varied the number of hidden units of f1i and f 2 j between 20 and 100. The final output of f3 has 10 units (one for each class). The gating networks g1 and g2 consist of two linear + reflection layers, each with 50 or 20 hidden units, and 4 output units (one for each expert), i.e. g1 (soft layer = a second layer, we expect a max x and a max layer)."}, {"heading": "4.2 Monophone Speech", "text": "In addition, we performed our model on a dataset of monophonic voice samples, a random subset of about one million samples from a larger proprietary database containing several hundred hours of US English data collected using voice search, voice typing, and read data. [5] For our experiments, each sample was limited to 11 images at 10 ms intervals and had 40 frequency bins. Each input was fed to the network as a 440-dimensional vector. There were 40 possible output phonemes classes. We trained a model with 4 experts on the first layer and 16 on the second layer. Both layers had 128 hidden units. The gating networks consisted of two layers each with 64 units in the hidden layer. As before, we evaluated the effect of using a mixture on the second layer by comparing the use of only one expert on the second layer or the concatenation of the results of all experts."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Jittered MNIST", "text": "Table 1 shows the error on the training and test kits for each model size (the test set is the MNIST test set with a single random translation per frame). In most cases, the deep stacked experts perform as expected between the two baselines on the training set. However, the deep models often suffer from overadjustment: the blend error on the test set is worse for two of the four model sizes than that of the individual expert. In Figure 2, we show the mean assignment to each expert (i.e. the mean gating performance) both by input translation and by class. The first layer assigns experts according to the translation, while the assignment by class is uniform. Conversely, the second layer assigns experts by class but is uniform according to the translation. This shows that the two expert layers are actually used in a complementary way, so that all expert combinations are effective. The first layer experts will be selective where the digit appears, while the second layer for each layer is selective for each layer, while the second layer is selective for each layer is selective for each second layer."}, {"heading": "Training Set Error: Jittered MNIST Model Single Expert Mixed Experts Concatenated Experts", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Test Set Error: Jittered MNIST Model Single Expert Mixed Experts Concatenated Experts", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Jittered MNIST: Two-Layer Deep Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 Monophone Speech", "text": "Table 2 shows the error on the training and test sets. As with MNIST, the error of the mix is on the training set between the two foundations, but in this case the performance of the test set is roughly the same for both basic lines and the mixture. Figure 4 shows the 16 test examples with the highest gating value for each expert combination (we only show 4 experts on the second layer due to space constraints). As before, the assignment of the first layer runs across the rows while the second layer runs across columns. Although not as interpretable as with MNIST, each expert combination seems to handle a certain portion of the input, reinforced by Figure 5, where we record the average number of assignments to each expert combination. Here, the choice of the second layer expert does not depend much on the choice of the first layer expert."}, {"heading": "Training Set Error: Monophone Speech Model Single Expert Mixed Experts Concatenated Experts", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Test Set Error: Monophone Speech Model Single Expert Mixed Experts Concatenated Experts", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Conclusion", "text": "The Deep Mixture of Experts model we are examining is a promising step toward developing large, sparse models that calculate only a fraction of themselves for a given input. We see exactly the gating tasks required to effectively use all expert combinations: blurred MNIST, factorization in translation and class, and the distinctive use of each combination for monophonic voice data. However, we continue to use a continuous mix of expert combinations, not just the top few. Such an expansion is necessary to achieve our goal of using only a small fraction of the model for each input; we hope to take this into account in future work."}, {"heading": "Acknowledgements", "text": "The authors thank Matthiew Zeiler for his contributions to the enforcement of compensation restrictions during the training."}, {"heading": "Monophone Speech: Conditional Assignments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Flexible", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "high performance convolutional neural networks for image classification. In IJCAI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "ICASSP", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "ICANN, 1:1\u20136", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, 3:1\u201312", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Application of pretrained deep neural networks to large vocabulary speech recognition", "author": ["N. Jaitly", "P. Nguyen", "A. Senior", "V. Vanhoucke"], "venue": "Interspeech", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural Computation, 6:181\u2013214", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 1, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "The Mixture of Experts model [4] is a continuous version of this: A learned gating network mixes the outputs of N \u201cexpert\u201d networks to produce a final output.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "A standard Mixture of Experts (MoE) [4] learns a set of expert networks fi along with a gating network g.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "A product of experts (PoE) [3] is similar, but instead combines the log probabilities to form a product:", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Also closely related to our work is the Hierarchical Mixture of Experts [6], which learns a hierarchy of gating networks in a tree structure.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "This dataset is a random subset of approximately one million samples from a larger proprietary database of several hundred hours of US English data collected using Voice Search, Voice Typing and read data [5].", "startOffset": 205, "endOffset": 208}], "year": 2017, "abstractText": "Mixtures of Experts combine the outputs of several \u201cexpert\u201d networks, each of which specializes in a different part of the input space. This is achieved by training a \u201cgating\u201d network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\u201cwhere\u201d) experts at the first layer, and class-specific (\u201cwhat\u201d) experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "creator": "LaTeX with hyperref package"}}}