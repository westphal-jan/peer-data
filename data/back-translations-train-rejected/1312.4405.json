{"id": "1312.4405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Learning Deep Representations By Distributed Random Samplings", "abstract": "In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction -- deep distributed random samplings, which performs like a stack of unsupervised bootstrap aggregating. First, its network structure is novel: each layer of the network is a group of mutually independent $k$-centers clusterings. Second, its learning method is extremely simple: the $k$ centers of each clustering are only $k$ randomly selected examples from the training data; for small-scale data sets, the $k$ centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.", "histories": [["v1", "Mon, 16 Dec 2013 15:40:05 GMT  (2558kb,D)", "http://arxiv.org/abs/1312.4405v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiao-lei zhang"], "accepted": false, "id": "1312.4405"}, "pdf": {"name": "1312.4405.pdf", "metadata": {"source": "META", "title": "Learning Deep Representations By Distributed Random Samplings", "authors": ["Xiao-Lei Zhang"], "emails": ["HUOSHAN6@126.COM"], "sections": [{"heading": "1. Introduction", "text": "The most recent advanced work is by Hinton and Salakhutdinov (Hinton & Salakhutdinov, 2006).The method, called deep belief networks (DBN), comprises two phases - the bottom-up greedy layer-by-layer preparation phase and the top-down fine adjustment phase. In this paper, we propose a simple method of deep learning - deep distributed random samplings (DDRS), for the uncontrolled reduction of dimensionality. The time and storage complexity of the DDRS scale is linear according to the size of the dataset. In the rest of the paper, we will describe DDRS in Section 2 and the experimental results in Section 3. Finally, we conclude this work in Section 4. Theoretical justification, motivation, related work and several complementary experiments are contained in the complementary material.A very basic design used to explain the ownership of this algorithm."}, {"heading": "2. Algorithm description", "text": "Each layer consists of V independent k-centers clustering. Given a set of d-dimensional input data X = {x1,..., xn}, the training process of v-th clustering is as follows: 1. Random selection of Badc dimensions of X to form a subset of X (v), referred to as X (v) = {x (v) 1,..., x (v) n}, where a represents a fraction of the selected dimensions. 2. Random selection: Random selection k examples from X (v), which are referred to as Wv = [wv, 1,.., wv, k], since the k centers represent the v-th of the v-th clustering. 3. Random selection of brbadcc dimensions of Wv, and a single-stage cyclic shift to the selected layer i-centers in Figure (v-4), with the i-dimensions being evenly selected as the v-layer (v-4)."}, {"heading": "3. Empirical evaluation", "text": "In this section, we will focus on the problem of unattended dimensionality reduction. If we evaluate runtime, the experiments are performed with a single-core PC with 8 GB of memory. Experiments are performed on three sets of data, i.e. the handwritten digits of the MNIST or a small dataset. Analysis of how the parameters affect performance is included in the supplementary material.The bottom layer of the DDRS uses the linear core to calculate the similarities between the input data and the centers in all datasets.Since DDRS only learns a sparse high-dimensional representation, we project it onto a low-dimensional subspace using Principal Component Analysis (PCA). Only a few largest eigenvalues and the corresponding eigenvectors are preserved for the construction of the subspace."}, {"heading": "3.1. Results on the MNIST digits", "text": "MNIST handwritten digit dataset is a benchmark dataset containing 10 handwritten integer digits from 0 to 9. It consists of 60,000 training examples and 10,000 test examples. Each example has 784 dimensions. We normalize each example to [0, 1] by adding 255 to each entry in the example. DDRS parameter setting is as follows: The learned representations are projected onto {2, 3, 5, 10, 30} low-dimensional subspaces. DBN parameter settings are the same as in (Hinton & Salakhutdinov, 2006) except that the number of units in the linear output layer is set to {2, 3, 5, 10, 30} each. CPU time spent on pre-training and fine-tuning are the same as in (Hinton & Salakhutdinov, 2006), except that the number of units in the linear output layer is set to {2, 3, 20, 30}."}, {"heading": "3.2. Results on a small-scale data set", "text": "The data set consists of 38 training examples and 34 test examples. The parameter settings of DDRS are as follows: The learned representations are projected onto {2, 3, 5, 10} low-dimensional subspaces. We compare DDRS with PCA and some methodologies Table 1. CPU time comparison (in hours) on MNIST.DBNpretraining DBNfine _ tunning DDRSTime 2.94 74.54 5.16 based on diagrams. Competition methods are single-layer nonlinear dimension reduction methods that have an O (n2) complexity. We perform the experiment ten times and report the average performance. If k-mean clusters are used for evaluation, we introduce k averages for each individual experimental run on the entire data set and record the average NDRI performance that can best be achieved by using DISS indicators."}, {"heading": "4. Conclusions", "text": "DDRS is simple, fast and effective."}, {"heading": "Acknowledgement", "text": "The author thanks Prof. DeLiang Wang for providing the Ohio Supercomputing Center, Columbus, OH, USA, for the experimental operation."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["Strehl", "Alexander", "Ghosh", "Joydeep"], "venue": "JMLR, 3:583\u2013617,", "citeRegEx": "Strehl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction \u2013 deep distributed random samplings. First, its network structure is novel: each layer of the network is a group of mutually independent k-centers clusterings. Second, its learning method is extremely simple: the k centers of each clustering are only k randomly selected examples from the training data; for small-scale data sets, the k centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.", "creator": "LaTeX with hyperref package"}}}