{"id": "1312.4461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "histories": [["v1", "Mon, 16 Dec 2013 18:58:34 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v1", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v2", "Wed, 18 Dec 2013 18:11:21 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v2", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v3", "Sat, 21 Dec 2013 16:57:47 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v3", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v4", "Tue, 28 Jan 2014 22:29:55 GMT  (71kb)", "http://arxiv.org/abs/1312.4461v4", "10 pages, 5 figures. Submitted to ICLR 2014"]], "COMMENTS": "8 pages, 5 figures. Submitted to ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew davis", "itamar arel"], "accepted": false, "id": "1312.4461"}, "pdf": {"name": "1312.4461.pdf", "metadata": {"source": "CRF", "title": "Low-Rank Approximations for Conditional Feedforward Computation", "authors": ["Andrew S. Davis"], "emails": ["andrew.davis@utk.edu", "itamar@utk.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.44 61v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "In recent years, deep neural networks have redefined the state of the art in many areas of application, particularly in computer vision [11] and language processing [14], but in order to scale to more difficult problems, neural networks need to grow, implying an increase in computing resources. Converting computing to highly parallel platforms such as GPUs enabled the formation of massive neural networks that would otherwise be trained too slowly on conventional CPUs. While the extremely high computing power used for the experiment in [12] (16,000-nucleus training over many days) was greatly reduced in [4] (3-server training over many days), specialized high-performance platforms still require several machines and several days of processing time. However, there may be more fundamental changes to the algorithms involved that can significantly contribute to scaling neural networks (16,000-nucleus training over many days). Many of these state-of-the-art neural networks have several features in common: high-accuracy activation loss due to a high level of non-linear radio activation components."}, {"heading": "2 Conditional Computation in Deep Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Exploiting Redundancy in Deep Architectures", "text": "The authors take advantage of this redundancy to train only 5% of the weights in a neural network, while estimating the other 95% by using carefully constructed, low-threshold decompositions of the weight matrices. Such a reduction in the number of active training parameters can facilitate optimization by reducing the number of variables to be optimized, and it can also help solve the problem of scalability by greatly reducing the communication effort in a distributed system. Assuming that there is a significant degree of redundancy in weight parameterization, a similar degree of redundancy is likely to be found in the activation patterns of individual neurons. Therefore, in an input sample, the set of redundant activations in the network can be roughly calculated."}, {"heading": "2.2 Sparse Representations, Activation Functions, and Prediction", "text": "For many data sets considered in the literature, it has been shown that sparse representations are superior to dense representations, especially in the context of deep architectures [7]. However, sparse representations learned from neural networks with sigmoidal activations are not really \"sparse,\" since activations toward negative infinity only approach the zero limit. Therefore, a conditional calculation model that estimates the sparseness of a sigmoidal network would have to impose a threshold above which the neuron is considered inactive. So-called \"hard threshold\" activation functions such as rectified linear units, on the other hand, produce true zeros that can be used by conditional calculation models without imposing additional hyperparameters."}, {"heading": "3 Problem Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Estimating Activation Sign via Low-Rank Approximation", "text": "Given the activation of layer l of a neural network, the activation of layer l + 1 of layer l + 1 requires the assumption that the function of layer l (alWl) (1) is given, where \u03c3 (\u00b7) denotes the function that defines the nonlinearity of the neuron, al Rn \u00b7 h1, al + 1 Rn \u00b7 h2, Wl Rh1 \u00b7 h2. If the weight matrix is highly redundant, as in [5], it can be well approximated using a low-level representation and we can rewrite (1) asal + 1 = low (alUlVl) (2), where UlVl is the low-level approximation to Wl, Ul Rh1 \u00b7 k, Vl Rk \u00d7 h2, k min (h1, h2) is the true element of layer i as long as k < h1h2h1 h2 h2, the low multiplication Ulpall."}, {"heading": "3.2 SVD as a Low-Rank Approximation", "text": "The Singular Value Decomposition (SVD) is a common matrix decomposition technique which factorises a matrix A-Rm \u00b7 n in A-Rm \u00b7 m, N-Rm \u00b7 n, V-Rn \u00b7 n. Through [6], the matrix A can be calculated on the basis of a matrix with a low rank A-R, which corresponds to the solution of the limited optimization of min A-R \u00b7 n, V-Rn \u00b7 n, where the first r-columns of V are the Frobenius standard, and A-R must necessarily be of rank < rank (A). The minimizer A-R is given by taking the first r-columns of U, the first r-columns of V and the first r-columns of V. The matrices Ur-V V, r-V V V and Vr-V are multiplied and result in A-R = Urb-RV-Tr. The low R-character of U, the first r-columns of V and the first r-columns of V-V. The resulting matrices Ur-V-V-V-V are multiplied V V V-V-V and V-V-V-V-V-V-V-V-V-V-V is obtained A-V-V-V-V-V-V-V. The low R-character W-R-R = UV-V-V V V-V V V V V-V V V V-V V-V V-V V-V-V V-V-V-V-V-V-V-V-V-V-V is then defined as such (we-V-V-V-V V-V-V-V-V-V-V-V V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V"}, {"heading": "3.3 Encouraging Neural Network Sparsity", "text": "In order to overcome the additional overhead imposed by the conditional computation architecture, the neural network must have sparse activations. Without encouragement to agree on weights that lead to sparse activations, such as penalties for loss function, a neural network does not necessarily become sparse enough to be useful in the context of the conditional computation. Therefore, a 1% penalty for the activation vector of each layer is applied to the total loss function, such as that J (W, \u03bb) = L (W) + 5% l = 1% al 1 (6) Such a penalty is commonly used in sparse dictionary learning algorithms and tends to push elements from al to zero [13]. Failure regulation [9] is another technique known to save the hidden activations in a neural network."}, {"heading": "3.4 Implementation Details", "text": "The neural network is built using Rasmus Berg Palm's Deep Learning Toolbox [16]. All hidden units are corrected-linear, and the output units are trained softmax with a negative log-likelihood loss function. Weights w are set as w \u0445 N (0, \u03c32) and distortions b to 1 to encourage the neurons to work in their unsaturated region as soon as training begins. In all experiments, the failure probability p is set to 0.5. The learning rate is planned so that \u03b3n = 0\u03bbn is the learning rate for the ninth epoch, \u03b30 is the initial learning rate and \u03bb is a failure probability of slightly less than 1, e.g. 0.995. The impulse term \u03bd is designed so that \u03bdn = max (\u03bdmax, \u03bd0\u03b2n) is where Hign is the impulse for the ninth epoch, and \u04450 of the initial dynamics is slightly less than 0.995."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 SVHN", "text": "Street View House Numbers (SVHN) [15] is a large image dataset with over 600,000 marked examples of street sign digits. Each example is an RGB 32 x 32 (3072-dimensional) image. To pre-process the dataset, each image is transformed into the YUV color space. Next, local contrast normalization [10] is applied to the Y channel followed by histogram compensation. U and V channels are discarded, resulting in a 1024-dimensional vector per example. The dataset is then normalized for the neural network by subtracting the mean and dividing it by the square root of the variance for each variance. To evaluate the sensitivity of the activation estimator, several parameters are evaluated for the activation estimator. Each network is trained with the hyperparameters in Table 4.1 and the results of seven parameters in Figure 4.1 are described as approximation value from 75 to one, each of which is approximated from 40 to one."}, {"heading": "100-75-50-25 9.96%", "text": "Table 4.2 summarizes the test rate error for the control and activation estimation networks. W1 appears to be the most sensitive, quickly reducing the test rate error from 10.72% to 12.16% when the rank of W-1 is reduced from 75 to 50. The rank of W-4 appears to be the least sensitive, reducing the test rate error from 9.96% to 10.01% as the rank is lowered from 25 to 15."}, {"heading": "4.2 MNIST", "text": "MNIST is a well-known set of handwritten digits containing 70,000 28 x 28 labeled images and generally divided into 60,000 training and 10,000 test examples. To achieve good results, very little pre-processing is required - each feature is transformed by xt = x \u221a \u03c32 max \u2212 0.5, where x is the input feature, \u03c32max is the maximum variance of all features, and 0.5 is a constant term to roughly center each feature. Multiple parameters for the activation estimator are evaluated for a neural network trained with the hyperparameters listed in Table 4.1, using the same approach as the SVHN experiment above. Results for the validation set plotted against the number of epochs are shown in Figure 4.3, and the final accuracy of the test set is shown in Table 4.3.A neural network with a very low matrix in the activation assessment can be surprisingly well trained on NIST."}, {"heading": "25-25-25 1.60%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50-35-25 1.43%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Discussion and Further Work", "text": "In the context of corrected-linear hidden units, computing time can be greatly reduced if this estimate is reliable and the hidden activations are sufficiently sparse. This approach applies to all hard-threshold activation functions, such as those examined in [8], and can easily be extended to be used with Convolutionary Neural Networks. While the activation error does not tend to differ too much between the minibatches, as shown in Figure 5.1, this is not guaranteed. An online approach to the low-threshold approximation would therefore be preferable to a once-per-epoch calculation."}], "references": [{"title": "Adaptive dropout for training deep neural networks", "author": ["Jimmy Ba", "Brendan Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep learning of representations: Looking forward", "author": ["Yoshua Bengio"], "venue": "Statistical Language and Speech Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron C. Courville"], "venue": "CoRR, abs/1308.3432,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Deep learning with cots hpc systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1306.0543,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Carl Eckart", "Gale Young"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1936}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Saturating auto-encoder", "author": ["Rostislav Goroshin", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3577,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Yann LeCun"], "venue": "In Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A Mohamed", "Tara N Sainath", "George Dahl", "Bhuvana Ramabhadran", "Geoffrey E Hinton", "Michael A Picheny"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated.", "startOffset": 72, "endOffset": 75}, {"referenceID": 10, "context": "In recent years, deep neural networks have redefined state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "In recent years, deep neural networks have redefined state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), specialized high-performance platforms still require several machines and several days of processing time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), specialized high-performance platforms still require several machines and several days of processing time.", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "In [5], the authors made the observation that deep models tend to have a high degree of redundancy in their weight parameterization.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "For many datasets considered in the literature, sparse representations have been shown to be superior to dense representations, particularly in the context of deep architectures [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "If the weight matrix is highly redundant, as in [5], it can be well-approximated using a low-rank representation and we may rewrite (1) as", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "By [6], the matrix A can be approximated using a low rank matrix \u00c2r corresponding to the solution of the constrained optimization of min \u00c2r \u2016A\u2212 \u00c2r\u2016F (5)", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Such a penalty is commonly used in sparse dictionary learning algorithms and tends to push elements of al towards zero [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Dropout regularization [9] is another technique known to sparsify the hidden activations in a neural network.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "When the regularized network is running in the inference mode, dropout has been observed to have a sparsifying effect on the hidden activations [17].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "The neural network is built using Rasmus Berg Palm\u2019s Deep Learning Toolbox [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "Next, local contrast normalization [10] followed by a histogram equalization is applied to the Y channel.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "This approach is applicable to any hardthresholding activation function, such as the functions investigated in [8], and can be easily extended to be used with convolutional neural networks.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "In [1], the authors propose a method called \u201cadaptive dropout\u201d by which the dropout probabilities are chosen by a function optimized by gradient descent instead of fixed to some value.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "This approach bears some resemblance to this paper, but with the key difference that the approach in [1] is motivated by improved regularization and this paper\u2019s method is motivated by computational efficiency.", "startOffset": 101, "endOffset": 104}], "year": 2017, "abstractText": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "creator": "LaTeX with hyperref package"}}}