{"id": "1312.4706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "abstract": "Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information.", "histories": [["v1", "Tue, 17 Dec 2013 10:18:44 GMT  (851kb)", "http://arxiv.org/abs/1312.4706v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.CL", "authors": ["donna vakharia", "rachel gibbs"], "accepted": false, "id": "1312.4706"}, "pdf": {"name": "1312.4706.pdf", "metadata": {"source": "CRF", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "authors": ["Donna Vakharia", "Rachel Gibbs"], "emails": ["donna@cs.utexas.edu", "rachel.gibbs@utexas.edu"], "sections": [{"heading": null, "text": "Spontaneous language in the form of conversations, meetings, voicemails, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines that provide access to such language collections have the potential to better inform intelligence and provide users with relevant data on vast audio / video archives. This project presents a search interface design that supports search tasks via a language collection, consisting of a historical archive containing almost 52,000 audiovisual testimonies of Holocaust survivors and witnesses and other genocides.The design includes a multi-faceted search, along with other UI elements such as highlighted search terms, keywords, snippets, etc., to facilitate discovery and exploratory search.Two different designs have been developed to support both manual and automated transcripts."}, {"heading": "1. INTRODUCTION", "text": "The simplicity of the collection and encryption of audiovisual data has led to the creation of a massive language collection with limited resources for the effective analysis of the data. As the size and use of such content increases, efficient ways of automatically searching for relevant segments are needed. While there are efficient search engines for today, there are no satisfactory systems for performing audiovisual data because these systems do not perform detailed analysis for them. We know that the core messages of oral history are notoriously underused. Historical archives provide rich information about the vast amounts of data. Even for the properly catalogued audiovisual collection, any efforts to collect this data are of little value if they are not available to the target audience."}, {"heading": "1.1 Proposed Solution", "text": "Develop a search system that would allow users to perform a content-based search and retrieve the transcript of the interview relevant to the query. Support search tasks about the content versus the keywords, keywords associated with the file. Support the multi-faceted search to help users explore and make discoveries. Use elements such as keyword tagging, snippets, category search, keywords, etc., to improve the user experience."}, {"heading": "1.2 Intended users", "text": "The intended users of the system will be mainly historians, academics, researchers, genealogists, students and history buffs. This intended user group will include people from a variety of disciplines and skills, from experts to beginners. As the collection focuses on a topic of global interest, users will vary in terms of native language, terminology skills and education level."}, {"heading": "1.3 Supported search tasks", "text": "Exploratory and Iterative Search, Re-finding, and Known Item Search are supported, but Known Item Search is less emphasized as it will be a rare use case; the rest of the paper is organized as follows. Section 2 explains the motivation behind this study. Previous work on data evaluation of the design of search systems is briefly described in Section 3. Elements of interface design are mentioned in Section 4. Section 5 mentions our research hypothesis, which we test using predefined methodology, presented in Section 6. Evaluation is performed on human subjects and the resulting results are presented in terms of self-reported feedback, and calculated measures are discussed in Section 7. Finally, Section 8 discusses the research areas that require further investigation in the future, followed by conclusions in Section 9."}, {"heading": "2. MOTIVATION", "text": "Both researchers had previously studied the subject extensively and had personal connections to the subject either through cultural exposure or family relationships with survivors and victims. One researcher had already been involved in a research activity using the same dataset, and the interview of one relative of the other was included in this collection of contemporary witnesses and survivors conducted by the Institute of the USC Shoah Foundation before they were lost. Therefore, both researchers had an interest in extending access to the audiovisual interviews and transcripts. This collection is extensive and was collected as quickly as possible to preserve first-hand accounts of eyewitnesses and survivors before they were lost forever. The simplicity of collecting and coding audiovisual data has resulted in the production of an enormous amount of language collections with limited tools to effectively analyze them. While today there are efficient search engines for text documents, there are no satisfactory systems for conducting the audiovisual data collection and coding of playback interviews that would allow this project to design a search system that would allow users to encounter the relevant elements of the content."}, {"heading": "3. RELATED WORK", "text": "Jones et al. [2] conducted a psycholinguistic study measuring the readability of various types of speech transcripts, measuring readability by measuring the accuracy of answers to comprehension questions, response time for reading the passage, response time for answering questions, and a subjective assessment of the difficulty of the passage. However, the data source for their study included texts from telephone language and news language, and was entirely based on automatic transcripts. A study comparing the effects of manual and automatic transcripts remains unexplored.The Informedia Digital Video Library Project at Carnegie Mellon University is creating a digital library of texts, images, videos, and audio data available for the full restoration of content [4]. Hauptmann et al. built a system by integrating technologies suitable for creating a digital video library to search and retrieve complete content."}, {"heading": "4. INTERFACE DESIGN", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "5. RESEARCH HYPOTHESES", "text": "The study was conducted at the University of Texas at Austin, School of Information, with a sample of 8 users with mixed ethnographic backgrounds, varying expertise and familiarity with database / archive search interfaces. The aim of the study was to test three hypotheses regarding user interactions and perceptions when presenting information in the SUI and search result documents: Hypothesis 1 (H1): users will prefer human transcribed search results over automatically machine-generated search results. Hypothesis 2 (H2): As the degree of disfluidity is reduced and sentence markers are added, users will be able to assess the perceived relevance of the presented search results more quickly and with greater confidence.Hypothesis 3 (H3): Marking loudspeaker replacements in the text of the transcripts improves the understanding and search efficiency of users, resulting in users finding transcriptions that were shortened (as opposed to)."}, {"heading": "6. METHODOLOGY", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "7. RESULTS", "text": "This year, it was only half as much as it was half as much."}, {"heading": "8. FUTURE WORK", "text": "In the future, a more comprehensive representative sample would help in accurately evaluating the SUI design, supported search methods and presentation of transcription information. Future samples would include domain experts (who are familiar with both the topic and the use of advanced search systems), such as historians and professors; subject experts such as survivors, witnesses, relatives of alumni and teachers (who may have less experience with search systems); and domain newcomers with or without prior experience in conducting research with advanced search systems. In addition, researchers would use more traditional A / B tests to evaluate various search elements. In the current study, conducting separate studies on tasks 1 and 2 might better address the issue of efficiency. Although it would have required more users and a randomizing interface presentation, and since this study involved only 7 users, it would not have been possible to expand user behavior in relation to future studies of effectiveness."}, {"heading": "9. CONCLUSION", "text": "We were able to verify and test our research hypotheses in this study. In H1, the hypothesis was put forward that users would prefer humanly transcribed search results over automatically generated ones. From our A / B tests, we obtained 100% accuracy in Task 1 compared to 42.8% in Task 2, which helped us to conclude that H1 is true and readability is an important factor for users in decision making. By receiving feedback from users on various markers (end of sentence, change of language), we observed that disfluence markers were very helpful in the current evaluation. Users reported that they would prefer if these highlighted disfluences could be removed altogether in order to improve readability and not lead to loss of information. However, feedback on markers at the end of the sentence showed ambiguous results; therefore, H2 does not remain conclusive in the current evaluation. Contrary to our expectations and hypothesis H3, users reported that the markup of text tasks does not enhance the understanding of the speaker's performance and performance."}, {"heading": "10. REFERENCES", "text": "[1] Kelly, Diane. IEEE International Conference on Vol. 1. IEEE 1997, Samuel A.] Project 6, 2001. [3] \"Foundationsand Trends in Information Retrieval 3.1 - 2 (2009): 1- 224. [2] Jones, Douglas A., et al.\" Measuring the readability of automatic speech-to-text transcripts. \"INTERSPEECH. 2003. [3] Frisch, Michael.\" Interactions 1.4 (1994): 67-71. [5] Hauptmann, Alexander G. and Howard D. Wactlar. \"Indexing and search of multimodal information.\" Acoustics, Speech, and Signal Processing, 1997. ICASSP-97."}, {"heading": "11. APPENDIX", "text": "In this section you will find screenshots of our search engine showing various design elements."}], "references": [{"title": "Methods for evaluating interactive information retrieval systems with users.", "author": ["Kelly", "Diane"], "venue": "Foundations and Trends in Information Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Measuring the readability of automatic speech-to-text transcripts.\" INTERSPEECH", "author": ["Jones", "Douglas A"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Oral history and the digital revolution.", "author": ["Frisch", "Michael"], "venue": "The Oral History Reader,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Informedia: improving access to digital video.\" Interactions", "author": ["Stevens", "Scott", "Michael Christen", "Howard Wactlar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Indexing and search of multimodal information.", "author": ["Hauptmann", "Alexander G", "Howard D. Wactlar"], "venue": "Acoustics, Speech, and Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Text, Speech, and Vision for Video Segmentation: The Informedia TM Project.\" AAAI fall symposium, computational models for integrating language and vision", "author": ["A. Hauptmann", "M. Smith"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Speech transcript analysis for automatic search.", "author": ["Coden", "Anni R", "Eric W. Brown"], "venue": "System Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Automatic recognition of spontaneous speech for access to multilingual oral history archives.", "author": ["Byrne", "William"], "venue": "Speech and Audio Processing, IEEE Transactions on 12.4", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Supporting access to large digital oral history archives.", "author": ["Gustman", "Samuel"], "venue": "Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Speechbot: an experimental speech-based search engine for multimedia content on the web.\" Multimedia", "author": ["Van Thong", "J-M"], "venue": "IEEE Transactions", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Imperfect transcript driven speech recognition.\"InterSpeech", "author": ["Lecouteux", "Benjamin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "The Cambridge University spoken document retrieval system.\" Acoustics", "author": ["Johnson", "S. E"], "venue": "Speech, and Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Experiments in broadcast news transcription.\"Acoustics", "author": ["Woodland", "P. C"], "venue": "Speech and Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Designing the search experience: The information architecture of discovery", "author": ["Russell-Rose", "Tony", "Tyler Tate"], "venue": "Access Online via Elsevier,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Faceted metadata for image search and browsing.\"Proceedings of the SIGCHI conference on Human factors in computing systems", "author": ["Yee", "Ka-Ping"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "The anatomy of a large-scale hypertextual Web search engine.\" Computer networks and ISDN systems", "author": ["Brin", "Sergey", "Lawrence Page"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Faceted Search (Synthesis Lectures on Information Concepts, Retrieval, and Services).", "author": ["Tunkelang", "Daniel"], "venue": "Morgan and Claypool Publishers", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Exploratory search: Beyond the query-response paradigm.\" Synthesis Lectures on Information Concepts, Retrieval, and Services", "author": ["White", "Ryen W", "Resa A. Roth"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Evaluating advanced search interfaces using established information\u2010seeking models.\" Journal of the American Society for Information", "author": ["Wilson", "Max L", "Ryen W. White"], "venue": "Science and Technology", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "While there are efficient search engines for text documents present today, there are no satisfactory systems for performing search over audiovisual data as these systems do not perform any detailed analysis for them [11].", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "We know that core audio-video dimension of oral history is notoriously underutilized [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Because of which the considerable potential of audio and video documents to support high-impact, vivid, thematic, and analytic engagement with meaningful issues, personalities, and contexts, is largely untapped [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 1, "context": "[2] performed a psycholinguistic study measuring readability of several types of speech transcripts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The Informedia Digital Video Library Project at Carnegie Mellon University is creating a digital library of text, images, videos and audio data available for full content retrieval [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "built a system by integrating technologies involved in creating a digital video library suitable for fullcontent search and retrieval [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Doing so, they try to overcome limitations of each technology [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "also try to address the problem of finding pertinent information for visual data [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "An experimental speech-based search engine, SpeechBot, uses automatic transcripts for indexing and in turn uses searchable index to provide an ability to play the segments of interest within the audio file [11].", "startOffset": 206, "endOffset": 210}], "year": 2013, "abstractText": "Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding userperspective on the design elements, and ease of parsing information.", "creator": "Microsoft\u00ae Office Word 2007"}}}