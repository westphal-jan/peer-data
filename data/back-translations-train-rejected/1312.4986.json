{"id": "1312.4986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "A Comparative Evaluation of Curriculum Learning with Filtering and Boosting", "abstract": "Not all instances in a data set are equally beneficial for inferring a model of the data. Some instances (such as outliers) are detrimental to inferring a model of the data. Several machine learning techniques treat instances in a data set differently during training such as curriculum learning, filtering, and boosting. However, an automated method for determining how beneficial an instance is for inferring a model of the data does not exist. In this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of being misclassified (instance hardness). The underlying assumption of this method is that instances with a high likelihood of being misclassified represent more complex concepts in a data set. Ordering the instances in a data set allows a learning algorithm to focus on the most beneficial instances and ignore the detrimental ones. We compare ordering the instances in a data set in curriculum learning, filtering and boosting. We find that ordering the instances significantly increases classification accuracy and that filtering has the largest impact on classification accuracy. On a set of 52 data sets, ordering the instances increases the average accuracy from 81% to 84%.", "histories": [["v1", "Tue, 17 Dec 2013 22:12:52 GMT  (23kb)", "http://arxiv.org/abs/1312.4986v1", "19 pages, 2 figures, 6 tables"]], "COMMENTS": "19 pages, 2 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael r smith", "tony martinez"], "accepted": false, "id": "1312.4986"}, "pdf": {"name": "1312.4986.pdf", "metadata": {"source": "CRF", "title": "An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks", "authors": ["Michael R. Smith", "Tony Martinez"], "emails": ["msmith@axon.cs.byu.edu,", "martinez@cs.byu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.49 86v1 [cs.LG] 1 7Keywords: curriculum learning, instance harness, outlier filtering, boosting"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Works", "text": "This year it is more than ever before."}, {"heading": "3 Ordering the Instances", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "4 Empirical Evaluation", "text": "With the order of instances provided by the instance of hardness, we examine how to apply a hardness order in the learning process. We examine the use of a hardness order based on a set of 52 UCI datasets (Frank and Asuncion, 2010) presented in Table 2. We use a hardness order in the curriculum of learning, filtering and enhancement. Since curricula are less fully researched, we focus on the development of curricula. We then compare curricula with filters and enhancers. We implement the methods using multi-layered perceptrons (MLPs) trained with back propagation and decision trees (DTs) trained on the basis of C4.5 (Quinlan, 1993). We have opted for an MLP because the curriculum set can be expanded during training, which is a natural fit for curricula curriculum. Other incremental learning algorithms that are propagated with backup and decision trees (DTs) can also be used where continuous."}, {"heading": "4.1 Curriculum Learning", "text": "Curriculum Learning trains a learning algorithm on the simplest concepts before training on the more complex instances, analogous to teaching a child in a subject such as mathematics: addition and subtraction are taught before algebra is taught before arithmetic, etc. One of the disadvantages of curriculum learning is that there is no automated way to create curricula for curriculum learning. Instance hardness is a natural fit for curriculum learning and provides a sequence of instances from the simplest to the toughest."}, {"heading": "4.1.1 Implementation Details", "text": "In fact, the fact is that most of them are able to hide without being able to achieve their goals."}, {"heading": "4.1.2 Results", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "4.2 Comparison with Filtering and Boosting", "text": "In this section, we compare curriculum learning with filters and topping-ups. The filtering method used for each instance with an instance hardness value greater than or equal to a threshold (we use 0.75) (Smith and Martinez, 2012). We refer to the filtering method as IH.75. However, the evaluation for filtering is based on unfiltered test data, we compare the method of learning with two different learning methods: AdaBoost and Schapire, 1996) and MultiBoost (Webb, 2000) (MB). For curriculum learning with MLPs, we use the method of adding more complex instances to education after 100 epochs. For DTs, pruning is done before adding more complex instances to education as it achieves higher accuracy. A statistical comparison of curriculum learning, filtering and topping-up for MLPs is given in Tables 5 and 6 Respects.The first method for average accuracy is achieved."}, {"heading": "5 Conclusions", "text": "This year it is more than ever before."}], "references": [{"title": "Outliers in statistical data", "author": ["V. Barnett", "T. Lewis"], "venue": "John Wiley & Sons Ltd.,", "citeRegEx": "Barnett and Lewis,? \\Q1978\\E", "shortCiteRegEx": "Barnett and Lewis", "year": 1978}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Lof: identifying density-based local outliers", "author": ["M.M. Breunig", "Kriegel", "H.-P", "R.T. Ng", "J. Sander"], "venue": null, "citeRegEx": "Breunig et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Breunig et al\\.", "year": 2000}, {"title": "Identifying mislabeled training data", "author": ["C.E. Brodley", "M.A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brodley and Friedl,? \\Q1999\\E", "shortCiteRegEx": "Brodley and Friedl", "year": 1999}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J.L. Elman"], "venue": null, "citeRegEx": "Elman,? \\Q1993\\E", "shortCiteRegEx": "Elman", "year": 1993}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "In Proceedings of the Third Annual Workshop on Computational Learning Theory,", "citeRegEx": "Freund,? \\Q1990\\E", "shortCiteRegEx": "Freund", "year": 1990}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of the 13th International Conference on Machine Learning,", "citeRegEx": "Freund and Schapire,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1996}, {"title": "Noise detection and elimination in data preprocessing: Experiments in medical domains", "author": ["D. Gamberger", "N. Lavra\u010d", "S. D\u017eeroski"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "Gamberger et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gamberger et al\\.", "year": 2000}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Robust decision trees: Removing outliers from databases", "author": ["G.H. John"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "John,? \\Q1995\\E", "shortCiteRegEx": "John", "year": 1995}, {"title": "Leveraging the margin more carefully", "author": ["N. Krause", "Y. Singer"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Krause and Singer,? \\Q2004\\E", "shortCiteRegEx": "Krause and Singer", "year": 2004}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "A metric for unsupervised metalearning", "author": ["J. Lee", "C. Giraud-Carrier"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Lee and Giraud.Carrier,? \\Q2011\\E", "shortCiteRegEx": "Lee and Giraud.Carrier", "year": 2011}, {"title": "On-line outlier detection and data cleaning", "author": ["H. Liu", "S. Shah", "W. Jiang"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Learning complex motions by sequencing simpler motion templates", "author": ["G. Neumann", "W. Maass", "J. Peters"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Neumann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2009}, {"title": "Estimating the potential for combining learning models", "author": ["A.H. Peterson", "T.R. Martinez"], "venue": "In Proceedings of the ICML Workshop on Meta-Learning,", "citeRegEx": "Peterson and Martinez,? \\Q2005\\E", "shortCiteRegEx": "Peterson and Martinez", "year": 2005}, {"title": "Neural network learning control of robot manipulators using gradually increasing task difficulty", "author": ["T.D. Sanger"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Sanger,? \\Q1994\\E", "shortCiteRegEx": "Sanger", "year": 1994}, {"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Schapire,? \\Q1990\\E", "shortCiteRegEx": "Schapire", "year": 1990}, {"title": "Improving classification accuracy by identifying and removing instances that should be misclassified", "author": ["M.R. Smith", "T. Martinez"], "venue": "In Proceedings of the IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Smith and Martinez,? \\Q2011\\E", "shortCiteRegEx": "Smith and Martinez", "year": 2011}, {"title": "Increasing task accuracy with adaptive filter sets", "author": ["M.R. Smith", "T. Martinez"], "venue": null, "citeRegEx": "Smith and Martinez,? \\Q2012\\E", "shortCiteRegEx": "Smith and Martinez", "year": 2012}, {"title": "An instance level analysis of data complexity", "author": ["M.R. Smith", "T. Martinez", "C. Giraud-Carrier"], "venue": "Machine Learning,", "citeRegEx": "Smith et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "From baby steps to leapfrog: \u201chow less is more\u201d in unsupervised dependency parsing", "author": ["V.I. Spitkovsky", "H. Alshawi", "D. Jurafsky"], "venue": "In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "On the utility of curricula in unsupervised learning of probabilistic grammars", "author": ["K. Tu", "V. Honavar"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence", "citeRegEx": "Tu and Honavar,? \\Q2011\\E", "shortCiteRegEx": "Tu and Honavar", "year": 2011}, {"title": "Multiboosting: A technique for combining boosting and wagging", "author": ["G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Webb,? \\Q2000\\E", "shortCiteRegEx": "Webb", "year": 2000}, {"title": "The lack of a priori distinctions between learning algorithms", "author": ["D.H. Wolpert"], "venue": "Neural Computation,", "citeRegEx": "Wolpert,? \\Q1996\\E", "shortCiteRegEx": "Wolpert", "year": 1996}], "referenceMentions": [{"referenceID": 3, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 7, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 18, "context": "Filtering identifies and removes noisy instances and outliers from a data set prior to training and generally results in an increase in classification accuracy on non-filtered test data (Brodley and Friedl, 1999; Gamberger et al., 2000; Smith and Martinez, 2011).", "startOffset": 186, "endOffset": 262}, {"referenceID": 17, "context": "Boosting also treats instances differently during training by incrementally adjusting the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 131, "endOffset": 161}, {"referenceID": 5, "context": "Boosting also treats instances differently during training by incrementally adjusting the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 131, "endOffset": 161}, {"referenceID": 1, "context": "Curriculum learning was recently formalized by Bengio et al. (2009) as a means of using an ordering of the training data from simplest to most complex to train a learning algorithm.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "In this paper, we present an automated method for ordering the instances in a data set based on their likelihood of being misclassified (instance hardness (Smith et al., 2013)) which we call a hardness ordering.", "startOffset": 155, "endOffset": 175}, {"referenceID": 10, "context": "As filtering and boosting have received considerable attention (Krause and Singer, 2004; Liu et al., 2004), we focus primarily on developing curriculum learning and comparing curriculum learning with filtering and boosting.", "startOffset": 63, "endOffset": 106}, {"referenceID": 13, "context": "As filtering and boosting have received considerable attention (Krause and Singer, 2004; Liu et al., 2004), we focus primarily on developing curriculum learning and comparing curriculum learning with filtering and boosting.", "startOffset": 63, "endOffset": 106}, {"referenceID": 16, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 14, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 21, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 22, "context": "Other work has demonstrated the utility of starting small in specific application areas (Sanger, 1994; Neumann et al., 2009; Spitkovsky et al., 2010; Tu and Honavar, 2011).", "startOffset": 88, "endOffset": 171}, {"referenceID": 3, "context": "Elman (1993) realized that the early stages of training dictate what solutions are possible in multilayer perceptrons.", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "Bengio et al. (2009) formalized these ideas in curriculum learning.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2009) formalized these ideas in curriculum learning. The idea behind curriculum learning is to first optimize a smoothed objective function and gradually reduce the degree of smoothing during the training process. At a more concrete level, curriculum learning is a weighting scheme for training. Each training instance is assigned a weight which controls how the instance is used in training. Initially, the weights on the training instances favor the \u201ceasier\u201d instances or those that represent simpler concepts. As training proceeds, the weights on the training instances are updated such that \u201charder\u201d instances and more complex concepts are introduced into the training set. This continues until all of the instances in the target training set are uniformly weighted. Kumar et al. (2010) recently presented self-paced learning for latent variable models, building on the idea of curriculum learning.", "startOffset": 0, "endOffset": 806}, {"referenceID": 17, "context": "Boosting is another approach that incrementally adjusts the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 101, "endOffset": 131}, {"referenceID": 5, "context": "Boosting is another approach that incrementally adjusts the weights of the instances during training (Schapire, 1990; Freund, 1990).", "startOffset": 101, "endOffset": 131}, {"referenceID": 10, "context": "However, this can lead to overfitting noise and new methods were proposed to ignore suspected outliers and weight the more informative or boundary instances higher (Krause and Singer, 2004).", "startOffset": 164, "endOffset": 189}, {"referenceID": 18, "context": "Outliers and noisy instances have been observed to adversely affect an induced model (Smith and Martinez, 2011).", "startOffset": 85, "endOffset": 111}, {"referenceID": 0, "context": "As such, a variety of different noise and outlier detection methods exist, such as statistical methods (Barnett and Lewis, 1978), densitybased clustering (Breunig et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 2, "context": "As such, a variety of different noise and outlier detection methods exist, such as statistical methods (Barnett and Lewis, 1978), densitybased clustering (Breunig et al., 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": ", 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": ", 2000), and classification-based methods (John, 1995; Brodley and Friedl, 1999).", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 7, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 13, "context": "These methods have been used to identify and remove outliers prior to training, resulting in higher classification accuracy (Brodley and Friedl, 1999; Gamberger et al., 2000; Liu et al., 2004).", "startOffset": 124, "endOffset": 192}, {"referenceID": 20, "context": "In this paper, we use instance hardness (Smith et al., 2013) to order the instances by complexity.", "startOffset": 40, "endOffset": 60}, {"referenceID": 24, "context": "If all learning algorithms were equally likely, then all instances would have the same instance hardness value under the no free lunch theorem (Wolpert, 1996).", "startOffset": 143, "endOffset": 158}, {"referenceID": 12, "context": "For instance hardness, the diversity of the learning algorithms is determined using unsupervised meta-learning (Lee and Giraud-Carrier, 2011).", "startOffset": 111, "endOffset": 141}, {"referenceID": 15, "context": "Unsupervised meta-learning uses Classifier Output Difference (COD) (Peterson and Martinez, 2005) to measure the diversity between learning algorithms.", "startOffset": 67, "endOffset": 96}, {"referenceID": 8, "context": "The learning algorithms are used as implemented in Weka with their default parameters (Hall et al., 2009).", "startOffset": 86, "endOffset": 105}, {"referenceID": 19, "context": "75) (Smith and Martinez, 2012).", "startOffset": 4, "endOffset": 30}, {"referenceID": 6, "context": "We compare curriculum learning with two boosting techniques: AdaBoost (Freund and Schapire, 1996) (AB) and MultiBoost (Webb, 2000) (MB).", "startOffset": 70, "endOffset": 97}, {"referenceID": 23, "context": "We compare curriculum learning with two boosting techniques: AdaBoost (Freund and Schapire, 1996) (AB) and MultiBoost (Webb, 2000) (MB).", "startOffset": 118, "endOffset": 130}, {"referenceID": 4, "context": "Elman (1993) pointed out that one of the reasons starting small is so important is due to backpropagation\u2019s inflexibility of learning late in the learning process.", "startOffset": 0, "endOffset": 13}], "year": 2013, "abstractText": "Not all instances in a data set are equally beneficial for inferring a model of the data. Some instances (such as outliers) are detrimental to inferring a model of the data. Several machine learning techniques treat instances in a data set differently during training such as curriculum learning, filtering, and boosting. However, an automated method for determining how beneficial an instance is for inferring a model of the data does not exist. In this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of being misclassified (instance hardness). The underlying assumption of this method is that instances with a high likelihood of being misclassified represent more complex concepts in a data set. Ordering the instances in a data set allows a learning algorithm to focus on the most beneficial instances and ignore the detrimental ones. We compare ordering the instances in a data set in curriculum learning, filtering and boosting. We find that ordering the instances significantly increases classification accuracy and that filtering has the largest impact on classification accuracy. On a set of 52 data sets, ordering the instances increases the average accuracy from 81% to 84%.", "creator": "LaTeX with hyperref package"}}}