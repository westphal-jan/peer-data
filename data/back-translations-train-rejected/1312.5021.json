{"id": "1312.5021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Efficient Online Bootstrapping for Large Scale Learning", "abstract": "Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging.", "histories": [["v1", "Wed, 18 Dec 2013 02:10:21 GMT  (87kb,D)", "http://arxiv.org/abs/1312.5021v1", "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013"]], "COMMENTS": "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhen qin", "vaclav petricek", "nikos karampatziakis", "lihong li", "john langford"], "accepted": false, "id": "1312.5021"}, "pdf": {"name": "1312.5021.pdf", "metadata": {"source": "CRF", "title": "Efficient Online Bootstrapping for Large Scale Learning", "authors": ["Zhen Qin", "Vaclav Petricek", "Nikos Karampatziakis", "Lihong Li"], "emails": ["zqin001@cs.ucr.edu", "vpetricek@eharmony.com", "nikosk@microsoft.com", "lihongli@microsoft.com", "jcl@microsoft.com"], "sections": [{"heading": null, "text": "Bootstrapping is a useful way to estimate the uncertainty of a predictor, such as prediction confidence intervals. Due to its high computational cost, it is typically used for small to medium-sized datasets. This work describes a highly scalable online bootstrapping strategy that is implemented within Vowpal Wabbit and is many times faster than traditional strategies. Our experiments suggest that our implementation of online bootstrapping not only provides a black box-like method of estimating uncertainty, but can also help build models with better predictive performance based on model averaging."}, {"heading": "1 Introduction", "text": "Bootstrapping is a very common method of estimating sample statistics. It generates N unique datasets from the original training data by analyzing sample replacement examples; each resampled set is used to train a separate model. However, instantiating the individual bootstrapped samples is costly, both in terms of storage and processing. Naive implementation would require only N times the initial runtime for the training plus the resampling overhead, which makes bootstrapping fit for large-scale learning tasks. Vowpal Wabbit1 (VW) [4] is a very fast open source implementation of an online out-of-core learner. Among the many efficient tricks within this sampling, it assigns a fixed (user-specifiable) memory size for displaying the learners, implements a hash trick that performs the names to numerical indices, hash, and parallel learning."}, {"heading": "2 Background on Online Bootstrapping", "text": "This is a very effective online approach to batch bootstrapping, using the following argument: bootstrapping a dataset D with n examples means sampling n examples from D with a substitute. Each example i will occur Zi times in the bootstrapping sample, where Zi is a random variable. In the case of all unit weight examples, Zi is distributed as a binome (n, 1 / n), because when resampling the i example n chances are to be selected, each with the probability of 1 / n. This binom (n, 1 / n) distribution converts to a Poisson distribution with the rate 1, even for modest n (see Figure 1). The Poisson distribution is much easier to extract, making it particularly suitable for large-scale learning processes."}, {"heading": "3 Efficient Online Bootstrapping", "text": "Fig. 2 shows the basic algorithm. Input: Example E with weight W, custom number of bootstrapping rounds. NWe implemented online bootstrapping as a top-level reduction in VW (see Fig. 2). In this way, bootstrapping can be combined with any other algorithm that implements Learning (). Parameter i is passed to the base learner to specify an offset for storing the feature weight of the current bootstrapping submodel. This architecture has three advantages: i) It keeps bootstrapping code separate from the learning code and uses all improvements in the base learning algorithm, ii) weights for the same feature within different bootstraps can be colocated in memory, which keeps memory access locally and maximizes cache hits, and iiii) Each example only needs to be analyzed once for all bound submodels."}, {"heading": "4 Experiments", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "5 Conclusions", "text": "In this paper, we demonstrate a highly effective and efficient online bootstrapping strategy, which we implemented as part of the open source online learning package Vowpal Wabbit. It is fast, practicable for large data sets, improves the predictions of the resulting model, and provides a black box-like ability to obtain uncertainty estimates that work with a variety of existing learning algorithms."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u0131\u0301k", "John Langford"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Online importance weight aware updates", "author": ["Nikos Karampatziakis", "John Langford"], "venue": "In UAI, pages 392\u2013399,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The Big Data Bootstrap", "author": ["Ariel Kleiner", "Ameet Talwalkar", "Purnamrita Sarkar", "Michael I. Jordan"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Vowpal wabbit open source project", "author": ["John Langford", "Lihong Li", "Alexander Strehl"], "venue": "In Technical Report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Online bagging and boosting", "author": ["Nikunj C. Oza", "Stuart Russell"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Sampling streaming data with replacement", "author": ["Byung-Hoon Park", "George Ostrouchov", "Nagiza F. Samatova"], "venue": "Comput. Stat. Data Anal.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Vowpal Wabbit1 (VW) [4] is a very fast open-source implementation of an online out-of-core learner.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "VW is able to learn a tera-feature (10) dataset on 1000 nodes in one hour [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "In this work, we extend an online version of bootstrapping for examples with unit weights proposed by Oza and Russell [6] to arbitrary positive real-valued weights, taking advantage of the good support of handling varying weights in VW [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "In this work, we extend an online version of bootstrapping for examples with unit weights proposed by Oza and Russell [6] to arbitrary positive real-valued weights, taking advantage of the good support of handling varying weights in VW [2].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "All of our code is a part of the open-source Vowpal Wabbit project [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Online bootstrapping via sampling from Poisson distribution was first proposed in [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Park et al [7] proposed reservoir sampling with replacement for sampling streaming data - a technique that could be used to implement bootstrapping without approximation.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Kleiner et al [3] propose a different bootstrap approximation which divides the large dataset into many little and possibly non-overlapping subsamples, but each set of subsamples are still processed in a batch manner, thus it is not applicable for typical online settings.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "The RCV1 [5] training dataset contains 781265 examples and 80 features per example on average.", "startOffset": 9, "endOffset": 12}], "year": 2013, "abstractText": "Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging.", "creator": "LaTeX with hyperref package"}}}