{"id": "1312.5129", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "histories": [["v1", "Wed, 18 Dec 2013 13:34:16 GMT  (16kb)", "https://arxiv.org/abs/1312.5129v1", null], ["v2", "Thu, 19 Dec 2013 11:01:02 GMT  (15kb)", "http://arxiv.org/abs/1312.5129v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": false, "id": "1312.5129"}, "pdf": {"name": "1312.5129.pdf", "metadata": {"source": "CRF", "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.lmu.de"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.51 29v2 [cs.CL] 1 9Deep learning embeddings have been successfully used for many problems of natural language processing. Embeddings are mostly calculated for word forms, although a number of recent work has extended this to other linguistic units such as morphemes and phrases. In this paper we argue that learning embeddings for discontinuous language units should also be considered. In an experimental evaluation of correlation resolution we show that such embeddings work better than word form embeddings."}, {"heading": "1 Motivation", "text": "One advantage of recent work on natural language processing (NLP) is that the linguistic units are represented by rich and informative embeddings, which support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarities and other linguistic properties. Embeddings are mostly derived for word forms, although a number of recent papers have extended these to other linguistic units (Luong et al., 2013) and sentences (Mikolov et al., an important question is: What are the basic linguistic units represented by embeddings in a deep learning system?"}, {"heading": "2 Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Embedding learning", "text": "With the English Gigaword Corpus, we use the Skip-gram model implemented in word2vec1 (Mikolov et al., 2013) to induce embeddings. To be able to use word2vec directly without changing the code, we present the corpus as a sequence of sentences, each consisting of two tokens: an MC (written as the two enclosing words separated by a star) and a word that occurs between the two enclosing words. The distance k between the two enclosing words can be varied. In our experiments, we use either distance k = 2 or distance 2 \u2264 k 3. For example, the trigram wi \u2212 1 wi + 1 generates the single sentence \"wi \u2212 1 wi + 1 wi\"; and for 2 \u2264 k \u2264 3, the fourgram wi \u2212 2 wi \u2212 1 wi + 1 the four sentences \"wi \u2212 2 wi \u2212 1,\" wi \u2212 pute, \"wi \u2212 2 \u2212 wi + 1 wi + 1.\""}, {"heading": "2.2 Markable classification task", "text": "A markable expression is a linguistic term that refers to an entity in the real world or another linguistic expression. Examples of markers are noun phrases (\"the man\"), designated entities (\"Peter\") and nested noun phrases (\"their\"). We deal with the task of animating classification of markers: the classification as animated / inanimate. \"This characteristic is useful for systems of correlation resolution, because only animated markers can be referred to the use of masculine and feminine pronouns in English like\" he \"and\" they. \"This is therefore an important indication for automatically classifying the markers of a document into correct chains of correction. In order to create training and test sentences, we extract all 39,689 correlation chains from the CoNL2012 OntoNotes Corpus. 2 We designate chains that contain one of the markers, his or her, his or her, his or his or her 342."}, {"heading": "3 Experimental results", "text": "We compare the following representations for the animation classification of markers. (i) MC: minimal context embedding with k = 2 and 2 \u2264 k \u2264 3; (ii) concatenation: concatenation of the embedding of the two enclosing words where the embedding is either standard word 2vec embedding (see Section 2.1) or the embedding published by Collobert et al. (2011); 4 (iii) the word bag (BOW) 1https: / / code.google.com / p / word2vec / 2http: / conll.cemantix.org / 2012 / data.html 3https: / github.com / bwaldvogel / liblinear-java 4http: / metaoptimize.com / projects / word2vec / representation / 2http: / conll.cemantix.org / data.html 3https: / bwaldlinegel / libjava of two word optimizers: http: / conll.cemantix.org / data.html 3https: / / bwaldlinegel / libjava of two word optimizers: the size of two Dimenheiser 4V: http: / conll.cemantix.org / data.html"}, {"heading": "4 Related work", "text": "Most work on embedding focuses, with a few exceptions, on word forms, especially embedding for strains and morphemes (Luong et al., 2013) and for phrases (Mikolov et al., 2013). To our knowledge, our work is the first to learn embedding for discontinuous linguistic units. An alternative to learning embedding for a linguistic unit is to calculate its distributed representation from the distributed representations of its parts; the most well-known work in this direction is (Socher et al., 2012, 2010, 2011). This approach is superior to units that are compositional, i.e. whose properties are systematically predictable from their parts. Our approach (as well as similar work on continuous phrases) only makes sense for non-compositional units."}, {"heading": "5 Conclusion and Future Work", "text": "We have argued that discontinuous linguistic units are part of the inventory of linguistic units for which we should calculate embeddings, and we have shown that such embeddings are better than word form embeddings in a correlation resolution task. It is obvious that we cannot and do not want to calculate embeddings for all possible discontinuous linguistic units. Likewise, the subset of phrases for which embeddings are calculated should be carefully selected. In future work, we plan to address the question of how to select a subset of linguistic units - e.g. those that are least composed - when making embeddings."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["M.T. Luong", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of the Conference on Computational Natural Language Learning,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "in: Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic representations that do not directly represent information about similarity and other linguistic properties.", "startOffset": 70, "endOffset": 94}, {"referenceID": 1, "context": "Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013) and phrases (Mikolov et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 2, "context": ", 2013) and phrases (Mikolov et al., 2013).", "startOffset": 20, "endOffset": 42}, {"referenceID": 2, "context": "With English Gigaword Corpus, we use the skip-gram model as implemented in word2vec1 (Mikolov et al., 2013) to induce embeddings.", "startOffset": 85, "endOffset": 107}, {"referenceID": 0, "context": "1) or the embeddings published by Collobert et al. (2011);4 (iii) the bag-of-words (BOW) https://code.", "startOffset": 34, "endOffset": 58}, {"referenceID": 1, "context": "Most work on embeddings has focused on word forms with a few exceptions, notably embeddings for stems and morphemes (Luong et al., 2013) and for phrases (Mikolov et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 2, "context": ", 2013) and for phrases (Mikolov et al., 2013).", "startOffset": 24, "endOffset": 46}], "year": 2013, "abstractText": "Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "creator": "LaTeX with hyperref package"}}}