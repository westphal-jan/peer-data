{"id": "1312.5242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Unsupervised feature learning by augmenting single images", "abstract": "When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).", "histories": [["v1", "Wed, 18 Dec 2013 17:44:17 GMT  (124kb,D)", "https://arxiv.org/abs/1312.5242v1", "6 pages, 4 figures, 1 table"], ["v2", "Fri, 24 Jan 2014 18:02:09 GMT  (124kb,D)", "http://arxiv.org/abs/1312.5242v2", "ICLR 2014 workshop track submission (6 pages, 4 figures, 1 table)"], ["v3", "Sun, 16 Feb 2014 13:07:23 GMT  (134kb,D)", "http://arxiv.org/abs/1312.5242v3", "ICLR 2014 workshop track submission (7 pages, 4 figures, 1 table)"]], "COMMENTS": "6 pages, 4 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["alexey dosovitskiy", "jost tobias springenberg", "thomas brox"], "accepted": false, "id": "1312.5242"}, "pdf": {"name": "1312.5242.pdf", "metadata": {"source": "CRF", "title": "Unsupervised feature learning by augmenting single images", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg"], "emails": ["dosovits@cs.uni-freiburg.de", "springj@cs.uni-freiburg.de", "brox@cs.uni-freiburg.de"], "sections": [{"heading": null, "text": "When applying deep learning to visual object detection, data enhancement is often used to generate additional training data without additional labeling costs. It helps to reduce over-matching and increase the performance of the algorithm. In this work, we investigate whether it is possible to use data enhancement as the main component of an unattended learning architecture of features. To this end, we randomly sample a number of random image fields and declare each of them a separate one-image surrogate class. We then expand these trivial one-element classes by applying a variety of transformations to the initial \"seed fields.\" Finally, we train a Convolutionary Neural Network to differentiate between these surrogate classes. The feature representation learned through the network can then be used in various visual tasks. We find that this simple feature-learning algorithm is surprisingly successful and achieves competitive classification results on several popular Vision 10 (Caltech-STAR) data sets (Caltech-101)."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.1 Related work", "text": "In contrast to our method, most uncontrolled learning approaches, e.g. [13, 14, 23, 6, 25], are explicitly based on modelling of input distribution - often through a reconstruction error - rather than training a discriminatory model and therefore cannot be used to train multiple layers of a deep neural network in a simple way.Among these uncontrolled methods, most are similar to our approach, namely several studies of learning-invariant representations from transformed input samples, e.g. [22, 25, 15].Our proposed method can be related to work on metric learning processes [10, 12]. Instead of directly enforcing a metric on feature representation, we force the representation of transforming images only implicitly by introducing substitution labels."}, {"heading": "2 Learning algorithm", "text": "Here we describe in detail our feature learning pipeline. The two main phases of our approach are the generation of surrogate training data and the training of a revolutionary neural network based on this data."}, {"heading": "2.1 Data acquisition", "text": "The input to our algorithm is a set of blank images coming from approximately the same distribution as the images we want to classify later. We sample N [50, 32000] random fields of size 32 x 32 pixels from different images, at different positions and scales. We sample only from regions with considerable gradient energy to avoid uniform colored fields. Then, we apply K [1, 100] random fields vertically and horizontally. \u2022 Scale: Multiply the scale of the field by a factor between 0.7 and 1.4. \u2022 Color: Multiply the fields by a distance within 0.25 of the fields size vertically and horizontally. \u2022 Color: Multiply the fields of data fields by a factor between 0.5 and 2. (Factors are independent of the main components of pixel formation and all components of pixel formation)."}, {"heading": "2.2 Training", "text": "As a result of the procedure described above, we apply to each patch xi-X from the set of initially sampled patches X = {x1,.. xN} a series of transformations Ti = {T 1i,..., TKi} and obtain a set of its transformed versions Sxi = {T j i xi | T j i, Ti}. Then we declare each of these sentences a class by assigning the label i to the class Sxi and forming a revolutionary neural network to distinguish between these replacement classes. Formally, we minimize the following loss function: L (X) = [X] x x x [T ji] l (i, T ji xi), (1) where l (i, T ji xi) is the loss on the sample T j i xi with (surrogate) true label i."}, {"heading": "2.2.1 Pre-training", "text": "In some of our experiments, where the number of surrogate classes is large relative to the number of training samples per surrogate class, we observed that the training error during the training process does not significantly decrease compared to the initial opportunity level. To alleviate this problem, we train the network before training on the entire surrogate dataset on a subset with fewer surrogate classes, typically 100. We stop training as soon as the training error begins to fall, indicating that the optimization has found a direction toward a good local minimum. We then use the weights learned during this pre-training phase as the initialization for training on the entire surrogate dataset."}, {"heading": "2.3 Testing", "text": "Upon completion of the training process, we apply the learned feature representation to classification tasks on \"real\" data sets, consisting of images that may differ in size from the surrogate training images. To extract features from these new images, we compute the reactions of all network layers except the topmost Softmax layer in a revolutionary way, and use this to form a three-layer spatial pyramid. We then train a linear support vector machine (SVM) and select the hyperparameters of the SVM by cross-validation."}, {"heading": "3 Experiments", "text": "We report our classification results for the STL-10, CIFAR-10 and Caltech-101 datasets and approximate or exceed the state of the art for unattended algorithms on each of these datasets. We also evaluate the impact of the number of surrogate classes and the number of training samples per surrogate class in the training data. For training the network in all our experiments, we generate a surrogate dataset with patches extracted from the STL-10 dataset without labels. For STL-10, we use the usual test protocol for averaging the results over 10 predefined folds of training data and report the mean and standard deviation. For CIFAR-10, we provide two results: \"CIFAR-10\" means training on the entire CIFAR-10 training set and \"CIFAR-10-reduced\" means the average of over 10 random selections per 400 training samples per training class with 30 samples per training class, rather than following the usual Caltech-101 sample selection."}, {"heading": "3.1 Classification results", "text": "In Table 1, we compare our classification results with other recent work. Our network is built on a surrogate dataset of 8000 surrogate classes of 150 samples each. We remind you that we use the first 3 layers of the network of 64, 64 and 128 filters to extract features during the test period. Therefore, the representation of features is much more compact than in most competing approaches. We do not list the results of the monitored methods on CIFAR-10 (the best of which currently exceed 90% accuracy) because they are not directly comparable to our unattended learning methodology. As shown in the table, our results are comparable to the state of the art on CIFAR-10 and exceed the performance of many unguarded algorithms on Caltech-101. On STL-10, for which Figure 1As mentioned, we do not compare with the methods that calculate monitored information for learning functions on the full CIFAR-10 dataset, there are two possibilities to calculate the 101 accuracy in each case."}, {"heading": "3.2 Influence of the data acquisition on classification performance", "text": "Our pipeline allows us to easily vary the number of surrogate classes in the training data and the number of training samples per surrogate class. We use this to measure the impact of these factors on the quality of the resulting characteristics. We vary the number of surrogate classes between 50 and 32000 and the number of training samples per surrogate class between 1 and 100. Results are shown in Fig. 3 and 4. In Fig. 4 we also show as a baseline the classification performance of random filters (all weights are sampled from a normal distribution with a standard deviation of 0.001, all distortions are set to zero).The initialization of the random filters does not require any training data and can therefore be considered the use of 0 samples per surrogate class. Error bars in Fig. 3 show the standard deviations calculated in tests on 10 folds of the STL-10 datasets that exceed the classification. An obvious trend in Fig. 3 is that increasing the number of the surrogate classes leads to an increase in the number of the optimum classes by 8000."}, {"heading": "4 Discussion", "text": "While our approach sets the state of the art at STL-10, it remains to be seen whether this success can be translated into consistently better performance on other datasets, and the performance of our method is saturated as the number of replacement classes increases. A likely reason for this is that the replacement task we use is relatively simple and does not allow the network to learn complex invariances such as 3D viewing invariance or inter-instance invariance. We believe that our unattended feature learning method could learn more powerful higher-level features if the surrogate data were real like real marked datasets, which could be achieved by additional weak monitoring provided by, for example, video data or a small number of tagged samples. Another possible way to obtain richer surrogate attraction data would be the (unattended) merging of similar surrogate classes."}, {"heading": "Acknowledgements", "text": "We acknowledge the support of the ERC Starting Grant VideoLearn (279401)."}], "references": [{"title": "Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks", "author": ["A. Ahmed", "K. Yu", "W. Xu", "Y. Gong", "E. Xing"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Semi supervised logistic regression", "author": ["M.-R. Amini", "P. Gallinari"], "venue": "In ECAI, pages 390\u2013394,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Unsupervised Feature Learning for RGB-D Based Object Recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In ISER,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Multipath sparse coding using hierarchical matching pursuit", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Ask the locals: multi-way local pooling for image recognition", "author": ["Y. Boureau", "N. Le Roux", "F. Bach", "J. Ponce", "Y. LeCun"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201911)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "In NIPS, pages 3248\u20133256,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S.T. Roweis", "G.E. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Entropy regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. Lecun"], "venue": "In In Proc. Computer Vision and Pattern Recognition Conference", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Direct modeling of complex invariances for visual object features", "author": ["K.Y. Hui"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML- 13),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Y. Jia", "C. Huang", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks", "author": ["D.-H. Lee"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Tangent prop - a formalism for specifying selected invariances in an adaptive network", "author": ["P. Simard", "B. Victorri", "Y. LeCun", "J.S. Denker"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W.Y. Zou", "A.Y. Ng", "S. Zhu", "K. Yu"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Deep convolutional neural networks trained via backpropagation have recently been shown to perform well on image classification tasks containing millions of images and thousands of categories [17, 24].", "startOffset": 192, "endOffset": 200}, {"referenceID": 23, "context": "Deep convolutional neural networks trained via backpropagation have recently been shown to perform well on image classification tasks containing millions of images and thousands of categories [17, 24].", "startOffset": 192, "endOffset": 200}, {"referenceID": 17, "context": "While deep convolutional neural networks have been known to yield good results on supervised image classification tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efficient model averaging and data augmentation techniques [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": "While deep convolutional neural networks have been known to yield good results on supervised image classification tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efficient model averaging and data augmentation techniques [17].", "startOffset": 289, "endOffset": 293}, {"referenceID": 23, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 247, "endOffset": 254}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 247, "endOffset": 254}, {"referenceID": 23, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 268, "endOffset": 272}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 301, "endOffset": 304}, {"referenceID": 6, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 341, "endOffset": 344}, {"referenceID": 8, "context": "The feature representation learned by these networks achieves state of the art performance not only on the classification task the network is trained for, but also on various other computer vision tasks, for example: classification on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9].", "startOffset": 378, "endOffset": 381}, {"referenceID": 12, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "[13, 14, 23, 6, 25], rely on modeling the input distribution explicitly \u2013 often via a reconstruction error term \u2013 rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 24, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 14, "context": "Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15].", "startOffset": 165, "endOffset": 177}, {"referenceID": 9, "context": "Our proposed method can be related to work on metric learning, for example [10, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Our proposed method can be related to work on metric learning, for example [10, 12].", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "However, instead of enforcing a metric on the feature representation directly, as in [12], we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Learning invariant features with a discriminative objective was previously considered in early work on tangent propagation [21], which aims to learn features invariant to small predefined transformations by directly penalizing the derivative of the network output with respect to the parameters of the transformation.", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "Tangent propagation has been successfully combined with an unsupervised feature learning algorithm in [20] to build a classifier exploiting information about the manifold structure of the learned representation.", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 160, "endOffset": 168}, {"referenceID": 18, "context": "Loosely related to our work is research on using unlabeled data for regularizing supervised algorithms, for example self-training [2] or entropy regularization [11, 19].", "startOffset": 160, "endOffset": 168}, {"referenceID": 0, "context": "Finally, the idea of creating a pseudo-task to improve the performance of a supervised algorithm is used in [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Then we apply K \u2208 [1, 100] random transformations to each of the sampled patches.", "startOffset": 18, "endOffset": 26}, {"referenceID": 16, "context": "For training the network we use an implementation based on the fast convolutional neural network code from [17], modified to support dropout.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "STL-10 CIFAR-10-reduced CIFAR-10 Caltech-101 K-means [6] 60.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "0 \u2014 Multi-way local pooling [5] \u2014 \u2014 \u2014 77.", "startOffset": 28, "endOffset": 31}, {"referenceID": 24, "context": "6 Slowness on videos [25] 61.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Receptive field learning [16] \u2014 \u2014 [83.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "7 Hierarchical Matching Pursuit (HMP) [3] 64.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Multipath HMP [4] \u2014 \u2014 \u2014 82.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "5 Sum-Product Networks [8] 62.", "startOffset": 23, "endOffset": 26}, {"referenceID": 14, "context": "View-Invariant K-means [15] 63.", "startOffset": 23, "endOffset": 27}], "year": 2014, "abstractText": "When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial \u2019seed\u2019 patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).", "creator": "LaTeX with hyperref package"}}}