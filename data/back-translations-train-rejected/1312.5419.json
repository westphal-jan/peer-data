{"id": "1312.5419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as activation layer, optimization, and generalization algorithms performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "histories": [["v1", "Thu, 19 Dec 2013 06:53:24 GMT  (161kb,D)", "https://arxiv.org/abs/1312.5419v1", "12 pages, 3 figures, prepared for ICLR 2014"], ["v2", "Mon, 30 Dec 2013 05:41:15 GMT  (158kb,D)", "http://arxiv.org/abs/1312.5419v2", "12 pages, 3 figures, submitted to ICLR 2014"], ["v3", "Thu, 15 May 2014 11:32:03 GMT  (497kb,D)", "http://arxiv.org/abs/1312.5419v3", "16 pages, 4 figures, submitted to ECML 2014"]], "COMMENTS": "12 pages, 3 figures, prepared for ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jinseok nam", "jungi kim", "eneldo loza menc\\'ia", "iryna gurevych", "johannes f\\\"urnkranz"], "accepted": false, "id": "1312.5419"}, "pdf": {"name": "1312.5419.pdf", "metadata": {"source": "CRF", "title": "Large-scale Multi-label Text Classification \u2014 Revisiting Neural Networks", "authors": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "As the volume of text files on the Web and in digital libraries increases rapidly, the need to add metadata to unstructured data is also growing."}, {"heading": "2 Multi-label Classification", "text": "Formally, the classification of multiple labels can be defined as: X-RD is a set of M instances, each of which is a D-dimensional characteristic vector, and L is a set of labels. Each instance x is associated with a subset of L labels, the so-called relevant labels; all other labels are irrelevant to this example. The student's task is to learn a mapping function f: RD \u2192 2L that assigns a subset of labels to a given instance. An alternative view is that we have to predict an L-dimensional target vector y (0, 1} L, where yi = 1 indicates that the i-th label is relevant, whereas yi = 0 indicates that it is irrelevant to the given instance. Many algorithms have been developed to address this type of problem. The easiest way is to construct the binary relevance (BR) of the label of the learner; the label to construct the L-label."}, {"heading": "2.1 State-of-the-art multi-label classifiers and limitations", "text": "The most well-known learning method for multi-label text classification is the use of a BR approach with strong binary classifiers such as SVM [23, 29] despite its simplicity. It is well known that the properties of high-dimensional and sparse data, such as text data, make decision problems linearly separable [14], and this property fits well with the strengths of SVM classifiers. Unlike benchmark data sets, real-world text collections consist of a large number of training examples displayed in a high-dimensional space with a large number of labels. In order to deal with such data sets, researchers have derived efficient linear SVMs [15, 7] that can handle large problems. The training time of these solvers scales linearly with the number of instances, so that they show good performance on standard benchmarks. However, their performance decreases as the number of labels grows and the label frequency distribution is undistorted [18]."}, {"heading": "3 Neural Networks for Multi-label Classification", "text": "In this section, we propose a neural network-based multi-label classification system that consists of a single hidden layer and works with recent developments in neural network and optimization techniques that allow the model to converge in a few steps from parameter updates into good regions of the error interface. Our approach consists of two modules (Figure 1): a neural network that generates label values (Sections 3.2-3.5), and a label predictor that converts label values into binary values using a threshold method (Section 3.3).4"}, {"heading": "3.1 Rank Loss", "text": "The most intuitive goal for multi-label learning is to minimize the number of misalignments between a pair of relevant and irrelevant labels called rank loss: L (y, f (x)) = w (y) \u2211 yi < yj I (fi (x) > fj (x) + 1 2 I (fi = fj) (1), where w (y) is a normalization factor, I (\u00b7) is the indicator function and fi (\u00b7) is a predictive value for a label size. Unfortunately, it is difficult to minimize these losses due to the non-convex property of the loss function, so convex replacement losses have been suggested as alternatives to rank loss [25, 6, 31]."}, {"heading": "3.2 Pairwise Ranking Loss Minimization in Neural Networks", "text": "Suppose we want to make a prediction about L-labels of D-dimensional input functions. Consider the neural network model with a single hidden layer in which F-hidden units are defined and input units x-RD \u00b7 1 with hidden units h-RF \u00b7 1 with weights W (1), RF \u00b7 D and distortions b (1), RF \u00b7 1. The hidden units are with output units o-RL \u00b7 1 by weights W (2), RL \u00b7 F and distortions b (2), RL \u00b7 1. The network can then be written in a matrix vector form, and we can construct a feed-forward network f-labels: x-o as a composite of non-linear functions in the range [0, 1]: filled (x) = optimization (2), fh (W (1) x + b (2), in the miniature."}, {"heading": "3.3 Thresholding", "text": "Once the formation of the neural network is complete, its output can be interpreted as a probability distribution p (o | x) across the labels of a particular document x. The probability distribution can be used to classify labels, but additional measures are required to divide the ranking into relevant and irrelevant labels. To transform the ranking of labels into a set of binary predictions, we train a multi-level threshold predictor from training data. This type of threshold method is also used in [6, 31] For each document xm, labels are sorted according to probabilities in descending order. Ideally, if NNs successfully learn a mapping function, all correct (positive) labels will be placed at the top of the sorted list and there should be a large distance between the set of positive labels and the set of negative labels. Using F1 values as a reference measure, we calculate classification performance (the best performing on each pair of consecutive labels and the best performing one)."}, {"heading": "3.4 Ranking Loss vs. Cross Entropy", "text": "However, it is not to be expected that there will be a loss if there is a loss that is not convex and has a discontinuity [2]. Furthermore, univariate loss functions such as log losses are rather consistent with rank losses [4]."}, {"heading": "3.5 Recent Advances in Deep Learning", "text": "In recent years, a number of techniques have been proposed to efficiently overcome the difficulty of learning neural networks (degrees). In particular, we use ReLUs, Ada\u0445 degrees and dropout training, which are briefly discussed hereinafter. 3 The shape of the functions is not changed even if we put c on the hidden level, since it is suggested and shown to achieve better generalization performance by functional values in the z-axis in relation to only W (1) and W (2) 1.8 rectified linear units (ReLUs) as activation units on the hidden level [21, 12, 30]. A ReLU deactivates negative activation (ReLU) = max (0, x), so that the number of parameters to be learned during the training decreases. This frugality makes ReLUs advantageous over traditional activation units such as sigmoid and tanh in relation to generalization performance."}, {"heading": "4 Experimental Setup", "text": "We have shown why the structure of the NNs in the previous ranking needs to be looked at anew. - In this section, we describe the rating yardsticks to show how effectively the NNs can be rated in two groups of metrics: in two groups of metrics and rankings that relate to the classification of ratings, i.e. a set of labels that are assigned by classifiers to each document, while ranking metrics are operated on the ranking of labels. - To evaluate the quality of a ranking, we look at several rankings that relate to a document x and associated label information, we look at a multi-label learner (x) that is able to produce scores for each label. - These scores can then be sorted in the ranking."}, {"heading": "5 Results", "text": "In fact, most of them are able to survive on their own."}, {"heading": "6 Conclusion", "text": "We found that our approach outperforms BPMLL in both predictive power and computational complexity and convergence speed. We investigated why BP-MLL does not perform well as a multi-label text classifier; our experimental results showed that the proposed framework is an effective method for the task of multi-label text classification; we also conducted extensive analyses to characterize the effectiveness of combining ReLUs with AdaGrad for rapid convergence rates; and the use of dropout to prevent revision leading to better generalization. [9] For PWE, we use tanh in the output layer, but sigmoid is used for CE because predictions o must lie for the calculation of CE with targets y between 0 and 1.1415 occlusal dgments. This work was supported by the Information Center for Education of the German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientific (DSL) program."}], "references": [{"title": "Multi-label classification on tree-and dag-structured hierarchies", "author": ["W. Bi", "J.T. Kwok"], "venue": "Proceedings of the 28th International Conference on Machine Learning. pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On the (non-)existence of convex, calibrated surrogate losses for ranking", "author": ["C. Calauz\u00e8nes", "N. Usunier", "P. Gallinari"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K. Dembczy\u0144ski", "W. Cheng", "E. H\u00fcllermeier"], "venue": "Proceedings of the 27th International Conference on Machine Learning. pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["K. Dembczy\u0144ski", "W. Kot\u0142owski", "E. H\u00fcllermeier"], "venue": "Proceedings of the 29th International Conference on Machine Learning. pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E. Loza Menc\u0131\u0301a", "K. Brinker"], "venue": "Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "On the consistency of multi-label learning", "author": ["W. Gao", "Z.H. Zhou"], "venue": "Artificial Intelligence 199\u2013200,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Collective multi-label classification", "author": ["N. Ghamrawi", "A. McCallum"], "venue": "Proceedings of the 14th ACM International Conference on Information and Knowledge Management. pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP. pp", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP. pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proceedings of the 10th European Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Efficient backprop. In: Neural Networks: tricks of the trade", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.R. M\u00fcller"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Support vector machines classification with a very large-scale taxonomy", "author": ["T.Y. Liu", "Y. Yang", "H. Wan", "H.J. Zeng", "Z. Chen", "W.Y. Ma"], "venue": "SIGKDD Explorations 7(1),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Efficient voting prediction for pairwise multilabel classification", "author": ["E. Loza Menc\u0131\u0301a", "S.H. Park", "J. F\u00fcrnkranz"], "venue": "Neurocomputing 73(7-9),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning. pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "Machine Learning 85(3),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine Learning 88(1-2),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature 323(6088),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning 39(2/3),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Accelerated learning in layered neural networks", "author": ["S.A. Solla", "E. Levin", "M. Fleisher"], "venue": "Complex Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1988}, {"title": "Multi-label classification of music into emotions", "author": ["K. Trohidis", "G. Tsoumakas", "G. Kalliris", "I. Vlahavas"], "venue": "Proceedings of the 9th International Conference on Music Information Retrieval. pp", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Random k-labelsets for multilabel classification", "author": ["G. Tsoumakas", "I. Katakis", "I.P. Vlahavas"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Multilabel classification with meta-level features in a learning-to-rank framework", "author": ["Y. Yang", "S. Gopal"], "venue": "Machine Learning 88(1-2),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M.Z. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "In: Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 26, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 21, "context": "However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classification concentrates on the question of how such dependencies can be exploited [22, 3].", "startOffset": 217, "endOffset": 224}, {"referenceID": 2, "context": "However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classification concentrates on the question of how such dependencies can be exploited [22, 3].", "startOffset": 217, "endOffset": 224}, {"referenceID": 30, "context": "One such approach is BP-MLL [31], which formulates multi-label classification problems as a neural network with multiple output nodes, one for each label.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Second, as it has been shown in the literature [14], popular feature representation schemes for textual data such as variants of tf-idf term weighting already incorporate a certain degree of higher dimensional features, and we speculate that even a single-layer NN model can work well with text data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 193, "endOffset": 200}, {"referenceID": 18, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 193, "endOffset": 200}, {"referenceID": 27, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 248, "endOffset": 252}, {"referenceID": 21, "context": "Classifier chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "Classifier chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.", "startOffset": 18, "endOffset": 25}, {"referenceID": 5, "context": "[6] present a large-margin classifier, RankSVM, that minimizes a ranking loss by penalizing incorrectly ordered pairs of labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In order to make a prediction, the ranking has to be calibrated [8], i.", "startOffset": 64, "endOffset": 67}, {"referenceID": 30, "context": "Similarly, Zhang and Zhou [31] introduced a framework that learns ranking errors in neural networks via backpropagation (BP-MLL).", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "The most prominent learning method for multi-label text classification is to use a BR approach with strong binary classifiers such as SVMs [23, 29] despite its simplicity.", "startOffset": 139, "endOffset": 147}, {"referenceID": 28, "context": "The most prominent learning method for multi-label text classification is to use a BR approach with strong binary classifiers such as SVMs [23, 29] despite its simplicity.", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "It is well known that characteristics of high-dimensional and sparse data, such as text data, make decision problems linearly separable [14], and this characteristic suits the strengths of SVM classifiers well.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "To handle such datasets, researchers have derived efficient linear SVMs [15, 7] that can handle large-scale problems.", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "To handle such datasets, researchers have derived efficient linear SVMs [15, 7] that can handle large-scale problems.", "startOffset": 72, "endOffset": 79}, {"referenceID": 17, "context": "However, their performance decreases as the number of labels grows and the label frequency distribution becomes skewed [18, 23].", "startOffset": 119, "endOffset": 127}, {"referenceID": 22, "context": "However, their performance decreases as the number of labels grows and the label frequency distribution becomes skewed [18, 23].", "startOffset": 119, "endOffset": 127}, {"referenceID": 5, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 30, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 162, "endOffset": 169}, {"referenceID": 2, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 162, "endOffset": 169}, {"referenceID": 24, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 5, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 30, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 0, "context": "The network, then, can be written in a matrix-vector form, and we can construct a feed-forward network f\u0398 : x\u2192 o as a composite of non-linear functions in the range [0, 1]:", "startOffset": 165, "endOffset": 171}, {"referenceID": 30, "context": "BP-MLL [31] minimizes errors induced by incorrectly ordered pairs of labels, in order to exploit dependencies among labels.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "This sort of thresholding methods are also used in [6, 31] For each document xm, labels are sorted by the probabilities in decreasing order.", "startOffset": 51, "endOffset": 58}, {"referenceID": 30, "context": "This sort of thresholding methods are also used in [6, 31] For each document xm, labels are sorted by the probabilities in decreasing order.", "startOffset": 51, "endOffset": 58}, {"referenceID": 1, "context": "t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL\u2019s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL\u2019s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9].", "startOffset": 205, "endOffset": 211}, {"referenceID": 3, "context": "Furthermore, univariate surrogate loss functions such as log loss are rather consistent with rank loss [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 25, "context": "parameters plays an important role in learning parameters of neural networks [26, 11] which we follow.", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "parameters plays an important role in learning parameters of neural networks [26, 11] which we follow.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 11, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 29, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 15, "context": "A common approach is to estimate the learning rate which gives lower training errors on subsamples of training examples [16] and then decrease it over time.", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Furthermore, to accelerate learning speed of SGD, one can utilize momentum [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "Instead of a fixed or scheduled learning rate, an adaptive learning rate method, namely AdaGrad, was proposed [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 12, "context": "Dropout training [13] is a technique for preventing overfitting in a huge parameter space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "In order to evaluate the quality of a ranked list, we consider several ranking measures [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "There are two ways of computing such performance measures: Micro-averaged measures and Macro-averaged measures4[20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "For Reuters21578 we used the same training/test split as previous works [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "Training and test data were switched for RCV1-v2 [17] which originally consists of 23,149 train and 781,265 test documents.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "That NNs with ReLUs at the hidden layer converge faster into a better weight space has been previously observed for the speech domain [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "8 However, unlike the results of [30], in our preliminary experiments adding more hidden layers did not further improve generalization performance.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "Limiting Small Learning Rates in BP-MLL The learning rate strongly influences convergence and learning speed [16].", "startOffset": 109, "endOffset": 113}], "year": 2014, "abstractText": "Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL\u2019s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.", "creator": "LaTeX with hyperref package"}}}