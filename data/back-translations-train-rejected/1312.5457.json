{"id": "1312.5457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Codebook based Audio Feature Representation for Music Information Retrieval", "abstract": "Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on \\emph{content based} methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the \\emph{audio content representation}.", "histories": [["v1", "Thu, 19 Dec 2013 09:40:03 GMT  (211kb,D)", "http://arxiv.org/abs/1312.5457v1", "Journal paper. Submitted to IEEE transactions on Audio, Speech and Language Processing. Submitted on Dec 18th, 2013"]], "COMMENTS": "Journal paper. Submitted to IEEE transactions on Audio, Speech and Language Processing. Submitted on Dec 18th, 2013", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.MM", "authors": ["yonatan vaizman", "brian mcfee", "gert lanckriet"], "accepted": false, "id": "1312.5457"}, "pdf": {"name": "1312.5457.pdf", "metadata": {"source": "CRF", "title": "Codebook based Audio Feature Representation for Music Information Retrieval", "authors": ["Yonatan Vaizman", "Brian McFee", "Gert Lanckriet"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. Related work", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "B. Our contribution", "text": "In this paper, we are looking for compact audio content representations that will perform for two different MIR applications: query-by-tag and query-by-example. We will perform a large-scale evaluation using the CAL10k and Last.FM datasets. We will evaluate the impact of different design decisions in the \"low-level feature, encoding, pooling\" scheme, and finally recommend a representation \"recipe\" (based on vector quantification) that is efficient to calculate and performs consistently well in both MIR applications. The rest of the paper is organized as follows: In?? we will describe the audio representations we compare, including the low-level audio features, the coding methods and the pooling. In?? we will describe the MIR tasks we evaluate, query-by-tag and query-by-sample retrieval."}, {"heading": "II. SONG REPRESENTATION", "text": "The persons mentioned above are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "A. Low-level audio features", "text": "Since we are not interested in melodic or harmonic information, but rather in general sonic similarity or semantic representation, we assume that timbral characteristics are appropriate here (an assumption worth investigating).Our low-level characteristics are based on mel frequency spectra (MFS), which are calculated by calculating the short-term Fourier Transformation (STFT), summarizing the energy distribution along mel-scaled frequency bins, and compressing the values with logarithm. Mel frequency receiver coefficients (MFCCs [30]) are the result of the further processing of MFS using discrete cosine transformation (DCT) to both generate uncorrelated characteristics from the correlated frequency bins and reduce the feature dimension. In addition to traditional DCT, we edit the MFS alternatively using a method of decoration based on the main component analysis (details are specified in the PCT)."}, {"heading": "B. Encoding with the LASSO", "text": "The least absolute shrink and selection operator (LASSO) has been proposed as the optimization criterion for linear regression, selecting only a few of the regression coefficients with effective magnitude, while the rest of the coefficients are either shrunk or even cancelled [44]. Using the LASSO regression coefficients as representation of input is often referred to as \"sparse encoding.\" In our formulation, the encoding of a feature vector xt using the LASSO criterion is: ct = argmin c-Rk1-2-xt-Dc-22 + sparse encoding."}, {"heading": "C. Encoding with vector quantization (VQ)", "text": "In vector quantization (VQ), a continuous multi-dimensional vector space is quantified to a discrete finite set of containers, each of which has its own representative vector. Formation of a vector code book is essentially a cluster formation that describes the distribution of vectors in space. During encoding, the characteristic vector of each frame is quantified to the nearest code word in the code book, which means that it is encoded as ct, a sparse binary vector with only a single \"on\" value in the index of the code word that has the smallest distance to it (we use the Euclidean distance). It is also possible to use a softer version of VQ by selecting a vector vector for each feature vector vector vector xxts the nearest neighbors among the k code words by creating a code vector vector that has the smallest distance to it (we use the euclidean distance), using a vector vector vector vector vector to create a shortest vector for each vector vector word, using a vector vector vector vector vector next version of each word."}, {"heading": "D. Encoding with cosine similarity (CS)", "text": "VQ encoding is easy and fast to calculate (unlike LASSO, whose solution algorithms, such as ADMM, are iterative and slow). However, it includes a hard threshold (even if \u03c4 > 1) that may distort the data and miss important information. If VQ is used for communication and reconstruction of signals, it is necessary to use this threshold to have a low bit rate (where each dictionary codeword is used as a linear filter over the feature vectors).However, in our case of coding songs for recovery, we have different requirements. As an alternative to VQ, we are experimenting with a different form of encoding in which each dictionary codeword is used as a linear filter over the feature vectors \u2212 nothing.Instead of calculating the distance between each feature vector and each code word (as in VQ), we calculate a similarity between them - the standardized product-product-product-product-product-product-product-product-product-product-product-product-product-product-sample-product-product-product-product-v2; <"}, {"heading": "E. Dictionary training", "text": "The formation of the dictionaries (coding books) takes place using the online learning algorithm for sparse coding presented by Mairal et al. ([46]). As an initialization phase, we apply online k-means to a stream of training vectors of d-dimensional characteristics to group them into an initial coding book of k-coding words. This initial dictionary is then given to the online algorithm, which alternates between coding a small group of new instances using the current dictionary and updating the dictionary using the newly coded instances. In each iteration, the updated coding words are normalized to unit L2 standard."}, {"heading": "III. MIR TASKS", "text": "We study the use of different song representations for two basic MIR applications in the hope of finding stable representations that are consistently successful in both tasks. We use simple, linear methods of machine learning, as our goal here is to find meaningful song representations rather than finding sophisticated new learning algorithms."}, {"heading": "A. Query-by-tag (QbT)", "text": "We use an L2-regulated logistic regression as a tag model. For each semantic tag, we use the positively and negatively designated training instances (k-dimensional song vectors) to train a tag model. We then use the trained tag model for each song in the test record and for each day to estimate the likelihood of the tag's relevance to the song (the likelihood of the song vector in relation to the tag model). For each song, the tag vector is then normalized as a categorical probability above the tag, also known as a semantic multinomic (SMN) representation of a song. Retrieval: For each day, the songs in the test record are classified according to their SMN value relevant to the tag. Per-day range below the curve (AUC), precision at top-10 (P @ 10) and average precision (AP) are calculated as in [15], [20]."}, {"heading": "B. Query-by-example (QbE)", "text": "Considering a query song whose audio content is represented as a vector q-Rk, our query system calculates its distance dist (q, r) from each query song r-Rk and the query result of the recommendation are the songs of the query song, which are ranked in increasing order of distance from the query song. Euclidean distance is a possible simple distance measure between the representations of the song. However, it gives equal weight to each of the vectors, and it is possible that there are dimensions that carry most of the relevant information, while other dimensions only cause noise. Therefore, we use a more general measurement as a distance measure, the mahalan anobis distance: dist (q, r) = (q \u2212 r) TW (q \u2212 r), if W-Rk \u00d7 k is the parameter matrix for the metric (W must be a positive semidefinitive matrix for a valid measurement metric) using the McR-47 algorithm for the various frames."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data", "text": "In this work, we use the CAL10k dataset [48]. This dataset contains 10,865 complete songs from over 4,500 different artists across 18 music genres. Everywhere on paper, we use the practical term \"song\" to refer to a piece of music (although many of the elements in CAL10k are pieces of classical music and would not commonly be called songs). It also includes semantic tags that come from the Pandora website, including 475 acoustic tags and 153 genre (and sub-genre) tags. These tag comments were made by people, music experts. The songs in CAL10k are weakly labeled in the sense that if a song does not have a specific tag, it does not necessarily mean that the tag is not relevant to the song, but for the evaluation we assume that missing song-tag associations are negative labels."}, {"heading": "B. Processing", "text": "The next step is to standardize the functions so that each dimension would have zero mean and unit variance. To have comparable audio features, we reduce the audio features of the 39 audio features. The audio files are averaged to individual channels (if they are given in stereo) and re-sampled at 22, 050Hz. The audio extraction is performed over semi-overlapping short frames of 2, 048 samples (a feature vector is stored once per 1, 024 samples, each of which are evaluated once per square kilometer 46msec). The size spectrum (magnitude DFT) of each individual frame is summarized in 34 Mel-scaled FCbins, and log value is stored to produce initial MFS characteristics. To standardize the MFCC characteristics, another step of standardize the discrete cosmic transform (DCT) is performed and 13 coefficients are saved to standardize the MFS (the 1. and 102). To standardize the features is to standardize the next step is to standardize the (DCT)."}, {"heading": "C. Experiments", "text": "Each experiment refers to a different way of presenting audio content. We experiment with different combinations of the following parameters: \u2022 Low-level features: MFCC \u2206 or MFS \u2206 PC, \u2022 Codebook size k, {128, 256, 512, 1024}, \u2022 Encoding method: the LASSO, VQ or CS, \u2022 Encoding parameters: - the LASSO: \u043c {0.01, 0.1, 0.5, 1, 2, 10, 100}, - VQ: \u03c4 {1, 2, 4, 8, 16, 32}, - CS: \u03b8 {0, 0.1, 0.2.., 0.9}, \u2022 Pooling function: either mean or max-abs, \u2022 VQ: either by PPK transformation or not."}, {"heading": "V. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. query-by-tag results", "text": "First, to compare the results of our current 5x partition: random levels are the result of cramping the sequence of song labels and performing the query-by-tag task while encoding the representations with MFS \u2206 PC, k = 1024 and VQ encoding with \u03c4 = 8. Then, to control the encryption methods in our scheme, we conduct the experiments without the encryption level PC (instead of encoding the feature vectors with a codebook, which leaves them as low-level features and bundles them). Finally, as an alternative to codebook-based systems, we evaluate the HEM-GMM system, which is the appropriate candidate from the generative model, which is efficient and assumes a bag of features (like our current codebook systems). We process the data as in [20] for HEM-GMM, using our current 5x partition."}, {"heading": "B. Query-by-example results", "text": "Next, we examine the performance of the query-by-example task (AUC) for the different song representations.???????? show the query-by-example results for the three encryption methods. The PCA dimension selected for each k is written in parentheses. See supplementary material. As expected, all encryption methods show improvements with increasing codebook size k and similar results (the performance values were slightly higher, but the comparisons between encoders or encryption parameters were the same.) As expected, all encryption methods show an improvement with increasing codebook size k. For the LASSO (?), we again see the sensitivity to the performance of encoders or encryption parameters, which is also affected by too low profile numbers. Unlike query-by-tag, there is no strong advantage of max-abs pooling over mean value pooling.For the VQ (?) we get a partial reproduction of the McFeet trends."}, {"heading": "C. Encoding runtime", "text": "Since we are looking for practical features and representations for large systems, computation resources should also be taken into account when selecting a preferred representation method. We compare the runtime complexity of the three encryption methods, from the feature vector xt-Rd to the code vector ct-Rk: \u2022 CS includes the multiplication of xt by matrix D (O (dk)), the purchase of xt vector multiplication and the application of shrinkage to cosmic similarities (O (k)), resulting in an overall complexity of TCS = O (dk). \u2022 VQ includes the same matrix vector multiplication and standards calculation to calculate the Euclidean similarities (O (k), resulting in an overall complexity of TCS = O (dk)."}, {"heading": "VI. CONCLUSION", "text": "The difference is statistically significant, but small, which shows that even the data agnostic DCT can compress music data well. Increasing the code book size (up to 1024) results in improved performance for all encoding methods. The degree of scarcity of the code has (possibly indirectly) an impact on the performance of all encoding methods that achieve optimum performance with codes that are not too sparse and not too dense. We find that a simple, efficient encoding method (VQ) can successfully compete with the more sophisticated method (LASSO) by achieving comparable and even better performance by making gentle and controlled changes in the performance of its density parameters."}], "references": [{"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on speech and audio processing, vol. 10, no. 5, pp. 293\u2013302, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "An investigation of feature models for music genre classification using the support vector classifier", "author": ["A. Meng", "J. Shawe-Taylor"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2005, pp. 604\u2013609.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A study on music genre classification based on universal acoustic models", "author": ["J. Reed", "C. Lee"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2006, pp. 89\u201394.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Classifying music audio with timbral and chroma features", "author": ["D.P. Ellis"], "venue": "ISMIR 2007: Proceedings of the 8th International Conference on Music Information Retrieval: September 23-27, 2007, Vienna, Austria. Austrian Computer Society, 2007, pp. 339\u2013340.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Shift-invariant sparse coding for audio classification.", "author": ["R. Grosse", "R. Raina", "H. Kwong", "A.Y. Ng"], "venue": "Conference on Uncertainty in AI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "on the use of sparse time-relative auditory codes for music.", "author": ["P.A. Manzagol", "T. Bertin-Mahieux", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M. Mandel", "D. Ellis"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 577\u2013582.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Temporal integration for audio classification with application to musical instrument classification", "author": ["C.J.S. Essid", "G. Richard"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 1, pp. 174\u2013186, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks.", "author": ["P. Hamel", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Unsupervised learning of sparse features for scalable audio classification", "author": ["M. Henaff", "K. Jarrett", "K. Kavukcuoglu", "Y. LeCun"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2011, pp. 681\u2013686.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of local features for music classification", "author": ["J. Wulfing", "M. Riedmiller"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2012, pp. 139\u2013144.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised dictionary learning for music genre classification", "author": ["C.C.M. Yeh", "Y.H. Yang"], "venue": "ICMR, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Dual-layer bag-of-frames model for music genre classification", "author": ["C.-C.M. Yeh", "L. Su", "Y.-H. Yang"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Support vector machine active learning for music retrieval", "author": ["M. Mandel", "G. Poliner", "D. Ellis"], "venue": "Multimedia systems, vol. 12, no. 1, pp. 3\u201313, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic annotation and retrieval of music and sound effects", "author": ["D. Turnbull", "L. Barrington", "D. Torres", "Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic generation of social tags for music recommendation", "author": ["D. Eck", "P. Lamere", "T. Bertin-Mahieux", "S. Green"], "venue": "Advances in Neural Information Processing Systems, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Autotagger: a model for predicting social tags from acoustic features on large music databases", "author": ["T. Bertin-Mahieux", "D. Eck", "F. Maillet", "P. Lamere"], "venue": "Journal of New Music Research, vol. 37, no. 2, pp. 115\u2013135, June 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining feature kernels for semantic music retrieval", "author": ["L. Barrington", "M. Yazdani", "D. Turnbull", "G. Lanckriet"], "venue": "2008, pp. 723\u2013728.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Using regression to combine data sources for semantic music discovery", "author": ["B. Tomasik", "J. Kim", "M. Ladlow", "M. Augat", "D. Tingle", "R. Wicentowski", "D. Turnbull"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2009, pp. 405\u2013410.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Time Series Models for Semantic Music Annotation", "author": ["E. Coviello", "A. Chan", "G. Lanckriet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1343\u20131359, July 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning sparse feature representations for music annotation and retrieval", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J. Smith"], "venue": "International Society for Music Information Retrieval conference (ISMIR), 2012, pp. 565\u2013570.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A bag of systems representation for music auto-tagging", "author": ["K. Ellis", "E. Coviello", "A. Chan", "G. Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Content-based retrieval of music and audio", "author": ["J.T. Foote"], "venue": "Voice, Video, and Data Communications. International Society for Optics and Photonics, 1997, pp. 138\u2013147.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "A music similarity function based on signal analysis", "author": ["B. Logan", "A. Salomon"], "venue": "IEEE International Conference on Multimedia and Expo, 2001, pp. 745\u2013748.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Music similarity measures: What\u2019s the use?", "author": ["J. Aucouturier", "F. Pachet"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Learning a metric for music similarity", "author": ["M. Slaney", "K. Weinberger", "W. White"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 313\u2013318.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Content-based musical similarity computation using the hierarchical Dirichlet process", "author": ["M. Hoffman", "D. Blei", "P. Cook"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2008, pp. 349\u2013354.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "An efficient hybrid music recommender system using an incrementally trainable probabilistic generative model", "author": ["K. Yoshii", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 16, no. 2, pp. 435\u2013447, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning content similarity for music recommendation", "author": ["B. McFee", "L. Barrington", "Lanckriet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 8, pp. 2207\u20132218, October 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["B. Logan"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), vol. 28, 2000.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Large-scale cover song recognition using the 2d fourier transform magnitude", "author": ["T. Bertin-Mahieux", "D.P. Ellis"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR 2012), 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio.", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "International Society for Music Information Retrieval conference (ISMIR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Features for audio and music classification", "author": ["M. McKinney", "J. Breebaart"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2003, pp. 151 \u2013158.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic combination of features for music classification", "author": ["A. Flexer", "F. Gouyon", "S. Dixon", "G. Widmer"], "venue": "Proc. International Society for Music Information Retrieval conference (ISMIR), 2006, pp. 111\u2013114.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "A largescale evaluation of acoustic and subjective music-similarity measures", "author": ["A. Berenzweig", "B. Logan", "D.P.W. Ellis", "B. Whitman"], "venue": "Computer Music Journal, vol. 28, no. 2, pp. 63\u201376, 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Multivariate Autoregressive Mixture Models for Music Autotagging", "author": ["E. Coviello", "Y. Vaizman", "A.B. Chan", "G. Lanckriet"], "venue": "13th International Society for Music Information Retrieval Conference (ISMIR 2012), 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "The variational hierarchical EM algorithm for clustering hidden Markov models", "author": ["E. Coviello", "A.B. Chan", "G. Lanckriet"], "venue": "Neural Information Processing Systems (NIPS 2012), 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "The Journal of Machine Learning Research, vol. 5, pp. 819\u2013844, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Sound retrieval and ranking using sparse auditory representations", "author": ["R. Lyon", "M. Rehn", "S. Bengio", "T.C. Walters", "G. Chechik"], "venue": "Neural Computation, vol. 22, no. 9.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient auditory coding", "author": ["E.C. Smith", "M.S. Lewicki"], "venue": "Nature, vol. 439, pp. 978\u2013982, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Complex events detection using data-driven concepts", "author": ["Y. Yang", "M. Shah"], "venue": "ECCV, 2012, pp. 722\u2013735.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML), 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Journal of Machine Learning (JMLR), vol. 15, p. 48109, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 19\u201360, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Metric learning to rank", "author": ["B. McFee", "G. Lanckriet"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML\u201910), June 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring automatic music annotation with \u201cacoustically-objectiv\u201d tags", "author": ["D. Tingle", "Y.E. Kim", "D. Turnbull"], "venue": "Proc. MIR, New York, NY, USA, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Etude comparative de la distribution florale dans une portion des alpes et des jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Societe Vaudoise des Sciences Naturelles, vol. 37, pp. 547\u2013579, 1901.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1901}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "arXiv preprint arXiv:1208.3922, 2012. PLACE PHOTO HERE  Yonatan Vaizman Biography text here.  PLACE PHOTO HERE Brian McFee Biography text here. PLACE PHOTO HERE  Gert Lanckriet Biography text here.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "[1]\u2013[13]), semantic annotation (auto-tagging) and retrieval (QbT [14]\u2013 [22]) and music similarity for song-to-song recommendation (QbE [23]\u2013[29]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 11, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 26, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "Many MIR research works used mel frequency cepstral coefficients (MFCC) as audio features ([1]\u2013[4], [8], [12], [14], [16], [17], [19], [23]\u2013[25], [27]\u2013[30]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 363, "endOffset": 366}, {"referenceID": 17, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 368, "endOffset": 372}, {"referenceID": 30, "context": "Other types of popular low-level audio features, based on short time Fourier transform are the constant-Q transform (CQT), describing a short time spectrum with logarithmically scaled frequency bins ([10]\u2013[12], [16], [17]), and chroma features, which summarize energy from all octaves to a single 12-dimensional (per frame) representation of the chromatic scale ([4], [18], [31]).", "startOffset": 374, "endOffset": 378}, {"referenceID": 31, "context": "suggested using principal component analysis (PCA) whitening of mel scaled spectral features as alternative to MFCC [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "([1], [8], [26], [33], [34]).", "startOffset": 1, "endOffset": 4}, {"referenceID": 7, "context": "([1], [8], [26], [33], [34]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "([1], [8], [26], [33], [34]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "([1], [8], [26], [33], [34]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "([1], [8], [26], [33], [34]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "[7], [16]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[7], [16]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 130, "endOffset": 133}, {"referenceID": 35, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 2, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 158, "endOffset": 161}, {"referenceID": 36, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "Various generative models were used: GMM ([1], [8], [14], [15], [18], [19], [25], [27], [28], [34], [35]) , DTM ([20]), MAR ([2], [8]), ARM ([36]), HMM ([3], [8], [37]), HDP ([27]).", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 169, "endOffset": 173}, {"referenceID": 35, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 175, "endOffset": 179}, {"referenceID": 37, "context": "Computing similarity between two songs is not straight forward using a generative model (although there are some ways to handle it, like the probability product kernel ([18], [36], [38])), whereas for vectorial representation there are many efficient generic ways to compute similarity between two vectors of the same dimension.", "startOffset": 181, "endOffset": 185}, {"referenceID": 35, "context": "In [36] the song level generative model (multivariate autoregressive mixture) was actually used to create a kind of vectorial representation for a song by describing the frequency response of the generative model\u2019s dynamic system, but still, being a mixture model, the resulted representation was a bag of four vectors, and not a single vectorial representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 52, "endOffset": 55}, {"referenceID": 28, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 57, "endOffset": 61}, {"referenceID": 38, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Quantization tree ([23]), vector quantization (VQ) ([3], [29], [39]), sparse coding with the LASSO ([5]) and other variations ([10], [11]) were used to represent the features at a higher level.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ([6], [40]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 39, "context": "Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ([6], [40]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 161, "endOffset": 164}, {"referenceID": 38, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 39, "context": "As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ([6], [39], [40]).", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "The bag of systems approach combined various generative models as codewords ([22]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "Multi-modal signals (audio and image) were combined in a single framework ([41]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ([12], [41]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 40, "context": "Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ([12], [41]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "Deep belief networks were used in [9], also combining unsupervised network weights training with supervised fine tuning.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "In [13] audio features were processed in two layers of encoding with codebooks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "examined different variations of low-level audio processing, and compared different encoding methods (VQ, the LASSO and sparse restricted Boltzman machine) for music annotation and retrieval with the CAL500 dataset [21].", "startOffset": 215, "endOffset": 219}, {"referenceID": 41, "context": "In [42] Coates and Ng examined the usage of different combinations of dictionary training algorithms and encoding algorithms to better explain the successful performance of sparse coding in previous works.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "In [43] Coates et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "In our experiments we used three encoding systems, the LASSO ([44]), vector quantization (VQ), and cosine similarity (CS) (all explained later), and applied both mean and max-abs pooling functions to the coded vectors.", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "Mel frequency cepstral coefficients (MFCCs [30]) are the result of further processing MFS, using discrete cosine transform (DCT), in order to both create uncorrelated features from the correlated frequency bins, and reduce the feature dimension.", "startOffset": 43, "endOffset": 47}, {"referenceID": 43, "context": "The least absolute shrinkage and selection operator (the LASSO) was suggested as an optimization criterion for linear regression that selects only few of the regression coefficients to have effective magnitude, while the rest of the coefficients are either shrunk or even nullified [44].", "startOffset": 282, "endOffset": 286}, {"referenceID": 44, "context": "The general algorithm, and a specific version for the LASSO are detailed in [45].", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "In [29] it was shown that for codeword histogram representations (VQ encoding and mean pooling), it was beneficial to take the square root of every entry, consequently transforming the song representation vectors from points on a simplex", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "5 on the original codeword histograms [38].", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": "LASSO) in a fast feed-forward way ([42]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 45, "context": "([46]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 14, "context": "Per-tag area under curve (AUC), precision at top-10 (P@10) and average precision (AP) are calculated as done in [15], [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "Per-tag area under curve (AUC), precision at top-10 (P@10) and average precision (AP) are calculated as done in [15], [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 46, "context": "In [47] McFee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [29] the authors further demonstrated the usage of MLR for music recommendation, and the usage of collaborative filtering data to train the metric, and to test the ranking quality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "In this work we use the CAL10k dataset [48].", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "As done in [29] we calculate the artist-artist similarity based on Jaccard index ([50]) and the binary song-song relevance metric, which is used as the target metric to be emulated by MLR.", "startOffset": 11, "endOffset": 15}, {"referenceID": 48, "context": "As done in [29] we calculate the artist-artist similarity based on Jaccard index ([50]) and the binary song-song relevance metric, which is used as the target metric to be emulated by MLR.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "This is unlike the sampling from within the experimental set, as was done in [21], which might cause over-fitting.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "The query-by-example evaluation is done with 10 splits of the data in the same manner as done in [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "We use the AUC rank measure to define the MLR loss between two rankings (marked as \u2206(y\u2217, y) in [47]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "In [29] the heuristic was to reduce to the estimated effective dimensionality \u2014 meaning to project to the first PCs covering 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "We process the data as was done in [20] for HEM-GMM, using our current 5-fold partition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "in [29]: improved performance with increasing codebook size and significant improvement when adding the", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "However, since in [29] the representations were reduced to the estimated effective dimensionality, which was a decreasing function of \u03c4 , there was a different effect of \u03c4 than what we find here (where we fix the reduced dimension for a given k): where in [29], for k = 512, 1024 with PPK increasing \u03c4 seemed to hurt the performance, we show that when PCA is done to a fixed dimension, increasing \u03c4 can maintain a stable performance, and even slightly improve the performance (for both with/without PPK), peaking at around \u03c4 = 8.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "However, since in [29] the representations were reduced to the estimated effective dimensionality, which was a decreasing function of \u03c4 , there was a different effect of \u03c4 than what we find here (where we fix the reduced dimension for a given k): where in [29], for k = 512, 1024 with PPK increasing \u03c4 seemed to hurt the performance, we show that when PCA is done to a fixed dimension, increasing \u03c4 can maintain a stable performance, and even slightly improve the performance (for both with/without PPK), peaking at around \u03c4 = 8.", "startOffset": 256, "endOffset": 260}, {"referenceID": 49, "context": "Recently linear convergence rate was shown for solving the LASSO with ADMM [51], implying that M = O(log 1 ), but even with fast convergence ADMM is still heavier than VQ.", "startOffset": 75, "endOffset": 79}], "year": 2013, "abstractText": "Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on content based methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the audio content representation. Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed codebook and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely the LASSO, vector quantization (VQ) and cosine similarity (CS). We evaluate the representations\u2019 quality in two music information retrieval applications: queryby-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top-\u03c4 VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO.", "creator": "LaTeX with hyperref package"}}}