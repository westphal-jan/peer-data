{"id": "1312.5542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Word Emdeddings through Hellinger PCA", "abstract": "Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.", "histories": [["v1", "Thu, 19 Dec 2013 13:31:11 GMT  (215kb,D)", "http://arxiv.org/abs/1312.5542v1", "9 pages, 5 tables. arXiv admin note: text overlap witharXiv:1103.0398by other authors"], ["v2", "Tue, 18 Mar 2014 10:23:35 GMT  (219kb,D)", "http://arxiv.org/abs/1312.5542v2", "9 pages, 5 tables"], ["v3", "Wed, 4 Jan 2017 17:01:11 GMT  (31kb)", "http://arxiv.org/abs/1312.5542v3", "9 pages, 5 tables"]], "COMMENTS": "9 pages, 5 tables. arXiv admin note: text overlap witharXiv:1103.0398by other authors", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": false, "id": "1312.5542"}, "pdf": {"name": "1312.5542.pdf", "metadata": {"source": "CRF", "title": "Word Embeddings through Hellinger PCA", "authors": ["R\u00e9mi Lebret", "Ronan Collobert"], "emails": ["rlebret@idiap.ch", "ronan@collobert.com"], "sections": [{"heading": null, "text": "Word embedding resulting from neural language models has proven successful for a variety of NLP tasks. However, such an architecture may be difficult to learn and time consuming. Instead, we propose to dramatically simplify the calculation of word embedding using a Hellinger PCA of the word co-occurence matrix. We compare these new word embedding with some known embedding in NER and film review tasks and show that we can achieve similar or even better results. Although deep learning is not really necessary to generate good word embedding, we show that it can provide an easy way to adapt embedding to specific tasks."}, {"heading": "1 Introduction", "text": "Popular approaches such as Brown Clustering Algorithm [1] have been used successfully in a variety of NLP tasks [2, 3, 4]. These word embedding are often seen as a low-dimensional vector aspect in which dimensions can be seen as properties that potentially describe syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of word embedding [5, 6, 7, 8, 9]. However, a neural network architecture can be difficult to train. Finding the right architectural parameters to fit the model is often a challenging task and the training phase is generally computationally expensive.This paper aims to show that such good word embedding can be achieved using simple linear operations. We show that similar word embedding can be calculated using the word LVasl."}, {"heading": "2 Related Work", "text": "Since 80% of the meaning of the English text comes from the word choice and the remaining 20% from the word order [13], it seems quite important to use the word order to capture all semantic information. Connectionist approaches were therefore proposed to develop distributed representations that encode the structural relationships between words [14, 15, 16]. However, a neural network language model was recently proposed in Bengio et al. [17] where word vector representations are learned simultaneously with a statistical language model. This architecture inspired other authors: Collobert and Weston [5] designed a neural language model that eliminates linear dependence on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al. [18] investigated a recursive neural network architecture for speech modeling. Such architectures, which are trained by large companies with unlabeled texts, aimed at predicting correct results."}, {"heading": "3 Spectral Method for Word Embeddings", "text": "An NNLM learns which words within the vocabulary appear more likely after a given word sequence. Formally, it learns the next word probability distribution. Instead, word counting can simply be performed on a large corpus of unlabeled text to determine these word distributions and display words [20]."}, {"heading": "3.1 Word co-occurence statistics", "text": "It is a natural choice to use the word coevent statistics to obtain representations of word meanings; the frequency of coevents of raw words is calculated by counting the number of times each word w, D occurs after a context sequence of words T: p (w, T) = p (w, T) p (T) = n (w, T) \u2211 w n (w, T), (1) where n (w, T) is the number of times each context word occurs after the context T. The next word probability distribution p for each word or word sequence is obtained like this. It is a multinomic distribution of | D | Classes (Words). A coevent matrix of size N \u00d7 | D | is obtained by calculating these frequencies over all N possible word sequences."}, {"heading": "3.2 Hellinger distance", "text": "Similarities between words can be derived by calculating a distance between their corresponding word distributions. There are several distances (or metrics) via discrete distributions, such as the Bhattacharyya distance, the Hellinger distance or the Kullback-Leibler divergence. We chose the Hellinger distance here because of its simplicity and symmetry property (since it is a true distance). Taking into account two discrete probability distributions P = (p1,..., pk) and Q = (q1, qk), the Hellinger distance is formally defined as: H (P, Q) = \u2212 1 \u221a 2 ----k \u0445 i = 1 (qi) 2, which is directly related to the euclimatic norm of the square root vectors: H (Q) = 1 \u221a 2 \u0432\u0430\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0435\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0435\u0441\u0441\u0438\u043d\u0435\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0435\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0435\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0435\u043d\u0435\u043d"}, {"heading": "3.3 Dimensionality Reduction", "text": "Since discrete distributions depend on the size of the vocabulary, the direct use of the distribution as word embedding for large vocabulary is not really comprehensible. We propose to perform a principal component analysis (PCA) of the probability matrix of the word co-occurence in order to represent words in a lower dimensional space and at the same time minimize the reconstruction error corresponding to the Hellinger distance."}, {"heading": "4 Architectures for NLP tasks", "text": "The selection of attributes is a task-specific empirical process. In contrast, we want to pre-process our attributes as little as possible. In this respect, a multi-layered neural network architecture seems appropriate, as it can be trained continuously on the interesting task."}, {"heading": "4.1 Sentence-level Approach", "text": "The approach of the sentence level is to highlight each word in a particular sentence. [Embedding each word in a sentence is fed into linear and nonlinear classification models followed by a particular sentence] We have chosen neural networks as classification criteria that are crucial to characterize word meanings. Thus, we look at n context words around each word that lead to a window of N = (2n + 1) words that aim to characterize the middle word in that window.,., xt + n) Since each word is embedded in a dwrd-dimensional vector, this results in a dwrd-dimensional vector that represents a window of N-words. In the face of a complete set of T-words, we can obtain a context-dependent representation for each word by sliding over all possible windows in the sentence."}, {"heading": "4.2 Document-level Approach", "text": "The document-level approach is a binary classifier for documents. For each document, a set of (trained) filters is applied to the sliding window described in Section 4.1. The maximum value obtained by the ith filter over the entire document is: max t [x] t + bi] i, t 1 \u2264 i \u2264 nfilter. (10) It can be considered a way to measure whether the information represented by the filter was captured in the document or not. We feed all of these intermediate values to a linear classifier that leads to the following simple model: f\u03b8 (x) = \u03b1max t [W [x] t + b]. (11) In the case of film reviews, the ith filter could detect positive or negative feelings depending on the sign of \u03b1i. As in Section 4.1, we will also consider a non-linear classifier in the experiments. Training The neural network is trained using stochastic gradient ascents."}, {"heading": "4.3 Embedding Fine-Tuning", "text": "These embeddings can then be used as features for monitored NLP systems and help improve overall performance [7, 8, 9]. However, most of these systems cannot optimize these embeddings because they are structurally unable to do so. By using the deep architecture of our system, we can define a reference table layer initialized with existing embeddings as the first layer of the network. Reference table level we consider a fixed-size dictionary D. Considering a sequence of N-words w1, w2,.... wN, each word beginning with \"W\" is first embedded in a three-dimensional vector space using a reference table operation: LTW (wn) = W (0,.., 1,., 0at index wn) = < W > < (13), with the matrix word \"W\" and the word \"D\" being the embed table. \""}, {"heading": "5 Experimental Setup", "text": "We evaluate the quality of our embedding in large corpora of unlabeled texts by comparing their performance with the CW [5], Turian [7], HLBL [10] and LR-MVL [11] embedding in NER and film review tasks. We also show that overall performance for these tasks can be improved by fine-tuning the word embedding."}, {"heading": "5.1 Building Word Representation over Large Corpora", "text": "Our English corpus consists of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus, and the Wall Street Journal (WSJ) corpus. We chose lowercase letters to limit the number of words in the vocabulary. Additionally, all occurrences of strings within a word are replaced by the string \"NUMBER.\" The resulting text was symbolized with the Stanford Tokenizer2. The record contains approximately 1652 million words. As a vocabulary, we considered all words within a vocabulary that occur at least one hundred times, resulting in a vocabulary of 178,080 words. To form the coexistence matrix, we used only the 10,000 most common words within our vocabulary as context words. Each word is represented in a 50-dimensional vector as the other embedding in the literature. The resulting embedding matrix is composed by the following sections of the A-H, which we also refer to as the PCA-PCx."}, {"heading": "5.2 Existing Available Word Embeddings", "text": "We decided to compare the embedding of our H-PCA with the following publicly available embedding: 1Available at http: / / download.wikimedia.org. We took the May 2012 version. 2Available at http: / / nlp.stanford.edu / software / tokenizer.shtml \u2022 LR-MVL3: It includes 300,000 words with 50 dimensions for each word. They were trained on the RCV1 corpus using the Low Rank Multi-View Learning method. \u2022 Turian5: It covers 268,810 words with 25, 50, 100 or 200 dimensions for each word."}, {"heading": "5.3 Supervised Evaluation Tasks", "text": "Using our word embeddings, we trained the sentence-level architecture described in Section 4.1 on an NER task. Named Entity Recognition (NER) It identifies atomic elements in the sentence in categories such as \"PERSON\" or \"LOCATION.\" The CoNLL 2003 setup6 is an NER benchmark data set based on Reuters data. The competition provides training, validation and test sets. The networks are fed with two raw functions: word embeddings and a uppercase feature. The \"Caps\" function says whether each word was written in lowercase letters had at least one non-initial uppercase letter. No other feature was used to tune the models. This is a significant difference from other systems that typically use more features than POS tags, prefixes or Gazetteers. Hyper parameters were selected to validate the context = We have tuned the system."}, {"heading": "5.4 Results", "text": "H-PCA embedding results summarized in Table 1a show that performance on NER task may be as good with word embedding from a word co-occurence matrix decomposition as with a neural network language model for weeks. Indeed, the best F1 results are obtained with the H-PCA embedding. Results for the movie evaluation task in Table 1b show that H-PCA embedding is 3Available at http: / / www.cis.upenn.edu / hungar / eigenwords / 4From SENNA: http: / ml.nec-labs.com / senna / 5Available by http: / metaoptimize.com / projects / wordreprs / www.cnts.ua.be / conll2003 / ner / 7Available at at at at at at at at http: / andrew-maas.net / data / sentiment."}, {"heading": "6 Conclusion", "text": "We have shown that appealing word embedding can be achieved by calculating a Hellinger PCA of the word cocidence matrix, which results in similar results for NLP tasks, even with a N \u00d7 10,000 word cocidence matrix. It turns out that the presence of a significant but not too large set of common words is sufficient to capture most of the syntactic and semantic properties of words. As PCA of the aN \u00d7 10,000 matrix is really fast and does not require memory, our method offers an interesting and practical alternative to neural language models for word embedding. However, we have shown that deep learning is an interesting framework for fine-embedding certain NLP tasks. The embedding of our H-PCA is available online here: http: / / www.lebret.ch / words /."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the HASLER Foundation with the support of \"Information and Communication Technology for a Better World 2020\" (SmartWorld)."}], "references": [{"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Distributional part-of-speech tagging", "author": ["H. Sch\u00fctze"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 141\u2013148. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 595\u2013603,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147\u2013155. Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributional representations for handling sparsity in supervised sequence-labeling", "author": ["F. Huang", "A. Yates"], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 495\u2013503. Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "ACL,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u2013 2537,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The expressive power of word embeddings", "author": ["Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "CoRR, abs/1301.3226,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["A. Mnih", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, volume 21,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-view learning of word embeddings via cca", "author": ["P.S. Dhillon", "D. Foster", "L. Ungar"], "venue": "Advances in Neural Information Processing Systems (NIPS), volume 24,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Re-embedding words", "author": ["I. Labutov", "H. Lipson"], "venue": "ACL,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "On the computational basis of learning and cognition: Arguments from lsa", "author": ["T.K. Landauer"], "venue": "N. Ross, editor, The psychology of learning and motivation, volume 41, pages 43\u201384. Academic Press, San Francisco, CA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning distributed representations of concepts", "author": ["G.E. Hinton"], "venue": "Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1\u201312. Hillsdale, NJ: Erlbaum,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "Recursive distributed representations", "author": ["J.B. Pollack"], "venue": "Artificial Intelligence, 46:77\u2013105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["J.L. Elman"], "venue": "Machine Learning, 7:195\u2013225,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. ernock", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Philosophical Investigations", "author": ["L. Wittgenstein"], "venue": "Blackwell,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1953}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "CoRR, abs/1003.1141,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards a theory of semantic space, pages 576\u2013581", "author": ["W. Lowe"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "A solution to Plato\u2019s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review, 104:211\u2013240,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Word category maps based on emergent features created by ICA", "author": ["J.J. V\u00e4yrynen", "T. Honkela"], "venue": "Heikki Hy\u00f6tyniemi, Pekka Ala-Siuru, and Jouko Sepp\u00e4nen, editors, Proceedings of the STeP\u20192004 Cognition + Cybernetics Symposium, number 19 in Publications of the Finnish Artificial Intelligence Society, pages 173\u2013185. Finnish Artificial Intelligence Society,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL, pages 142\u2013150,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": "1952-59:1\u201332,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1957}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, pages 257\u2013286,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1989}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang", "P. Bartlett"], "venue": "Journal of Machine Learning Research, 6:1817\u20131853,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].", "startOffset": 117, "endOffset": 126}, {"referenceID": 2, "context": "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].", "startOffset": 117, "endOffset": 126}, {"referenceID": 3, "context": "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].", "startOffset": 117, "endOffset": 126}, {"referenceID": 4, "context": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].", "startOffset": 131, "endOffset": 146}, {"referenceID": 5, "context": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].", "startOffset": 131, "endOffset": 146}, {"referenceID": 6, "context": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].", "startOffset": 131, "endOffset": 146}, {"referenceID": 7, "context": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].", "startOffset": 131, "endOffset": 146}, {"referenceID": 8, "context": "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].", "startOffset": 131, "endOffset": 146}, {"referenceID": 4, "context": "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].", "startOffset": 153, "endOffset": 163}, {"referenceID": 7, "context": "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].", "startOffset": 153, "endOffset": 163}, {"referenceID": 10, "context": "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].", "startOffset": 153, "endOffset": 163}, {"referenceID": 11, "context": "For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order [13], it seems quite important to leverage word order to capture all the semantic information.", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].", "startOffset": 149, "endOffset": 161}, {"referenceID": 14, "context": "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].", "startOffset": 149, "endOffset": 161}, {"referenceID": 15, "context": "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].", "startOffset": 149, "endOffset": 161}, {"referenceID": 16, "context": "[17] where word vector representations are simultaneously learned along with a statistical language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "This architecture inspired other authors: Collobert and Weston [5] designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al.", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "This architecture inspired other authors: Collobert and Weston [5] designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al.", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "[18] investigated a recurrent neural network architecture for language modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Using the word co-occurrence statistics is thus a natural choice to embed similar words into a common vector space [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities [21].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "LSA [22], ICA [23]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "LSA [22], ICA [23]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank Multi-View Learning (LRMVL) method which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings [11].", "startOffset": 240, "endOffset": 244}, {"referenceID": 6, "context": "It has been proved that using word embeddings as features helps to improve general performance on many NLP tasks [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "[24] proposed a model for jointly capturing semantic and sentiment components of words into vector spaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "More recently, Labutov and Lipson [12] presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Instead, simply counting words on a large corpus of unlabeled text can be performed to retrieve those word distributions and to represent words [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 24, "context": "\u201dYou shall know a word by the company it keeps\u201d [25].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "It can be computed in linear time with the Forward algorithm, which derives a recursion similar to the Viterbi algorithm (see [26]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 7, "context": "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 8, "context": "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].", "startOffset": 117, "endOffset": 126}, {"referenceID": 4, "context": "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.", "startOffset": 146, "endOffset": 149}, {"referenceID": 9, "context": "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 7, "context": "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 8, "context": "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 26, "context": "[27] which reached 89.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] which reached 88.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some wellknown embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.", "creator": "LaTeX with hyperref package"}}}