{"id": "1312.5559", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "abstract": "There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "histories": [["v1", "Thu, 19 Dec 2013 14:18:14 GMT  (15kb)", "https://arxiv.org/abs/1312.5559v1", "4 pages, 1 table, ICLR Workshop"], ["v2", "Tue, 14 Jan 2014 17:33:49 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v2", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results"], ["v3", "Tue, 18 Feb 2014 14:17:46 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v3", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results; related word added"]], "COMMENTS": "4 pages, 1 table, ICLR Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["irina sergienya", "hinrich sch\\\"utze"], "accepted": false, "id": "1312.5559"}, "pdf": {"name": "1312.5559.pdf", "metadata": {"source": "CRF", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "authors": ["Irina Sergienya"], "emails": ["irina@cis.lmu.de"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.55 59v3 [cs.CL] 1 8There are two main approaches to the distributed representation of words: low-dimensional embedding in deep learning and high-dimensional distribution models in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embedding based on distribution models - as opposed to one-dimensional vectors used by default in deep learning. We show that the combined approach performs better in assessing word relationships."}, {"heading": "1 Introduction", "text": "The standard approach to achieving deep learning embedding is to present each word of the input vocabulary as a single hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013a). There is no usable information in this initial presentation; in this sense, the standard type of embedding will be a form of learning from the ground up. In this paper, we examine whether it may be beneficial to use a more informative initial representation for embedding. Specifically, we will test distribution models for this purpose - where we invent a distribution model or a distribution representation of a target word as a vector of dimension."}, {"heading": "2 Method", "text": "As we have just discussed, we would expect distribution initialization to be mainly beneficial for rare words. Conversely, it is likely that distribution initialization will actually affect the quality of embedding learned for common words, because distribution initialization restricts the relationship between different embeddings, which is good for rare words because it forces the similarity of embedding of similarly rare words, but it can be harmful for common words, which often have idiosyncratic properties. For common words, it is better to use a one-dimensional embedding that basically imposes no restrictions on the type of embedding that can be learned. Based on this motivation, we propose a hybrid distribution scheme: all words with a frequency f >."}, {"heading": "3 Experimental setup", "text": "As a training set for word embedding, we use parts 02 to 21 of the Wall Street Journal (Marcus et al., 1993), a corpus of about one million tokens and approximately 35,000 word types. We used two sets of data to evaluate word affinities: MEN1 (Bruni et al., 2012) and WordSim3532 (Finkelstein et al., 2001). The two sets of data contain word pairs with humanly assigned similarity values. We evaluate only the 2186 MEN pairs (out of a total of 3000) and 303 WordSim353 pairs (out of a total of 353) covered by our data set, i.e. both words entered into WSJ.We added to the continuous word gram model (Mikolov et al., 2013a) of word2vec3 both uniform and distribution-related models for initialization."}, {"heading": "4 Results and discussion", "text": "Table 1 shows the average Spearman correlation coefficients between human and embedded similarity for very hot initialization values for MEN and WordSim. Embedding is generated by skipped gram models with uniform, hybrid (mixed or separated) and distributed initialization effect, the threshold being different for the two hybrid models (column \"\u03b8\"), the correlation coefficients being given in the last two columns.1http: / / clic.cimec.unitn.it / elia.bruni / MEN 2http: / / www.cs.technion.ac.il / gabr / resources / data / wordsim353 / wordsim353.html 3https: / / code.google.com / p / word2vec / The main result is that the two hybrid initializations following the hot initialization effect are significantly better than the hot initialization for both Sord4 and WordS4."}, {"heading": "5 Related work", "text": "The problem of word embedding has also been addressed by Le et al. (2010), who propose three initialization programs, two of which, re-initialization and iterative re-initialization, use vectors from the prediction area to initialize the context space during the training. This approach is both more complex and less efficient than ours. The third initialization scheme, a vector initialization, initializes all word embedding with the same random vector: This helps to keep rare words close together, since vectors of rare words are rarely updated. However, this approach is also less efficient than ours, as the original embedding is much denser than in our approach."}, {"heading": "6 Conclusion", "text": "We have proposed to use a hybrid initialization for word embedding, an initialization that combines the standard one-hot initialization with a distributional initialization for rare words.4Studentens t-Test, two-tailed, p <.05Experimental results on a word kinship task provide preliminary evidence that hybrid initialization produces better embedding than a one-hot initialization. Our results are not directly comparable to previous research on word kinship judgment modeling, in part because the corpus we use has low coverage of the words in the weighting sets. We also use a simple binary distribution vector representation that is likely to have a negative effect on embedding performance. In future work, we will test our models on larger companies and look at a wider range of distribution models to get results that are directly comparable to other word kinship work."}], "references": [{"title": "Attribute-based and value-based clustering: An evaluation", "author": ["A. Almuhareb", "M. Poesio"], "venue": null, "citeRegEx": "Almuhareb and Poesio,? \\Q2004\\E", "shortCiteRegEx": "Almuhareb and Poesio", "year": 2004}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics", "citeRegEx": "Baroni and Lenci,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Lenci", "year": 2010}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.K. Tran"], "venue": null, "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": null, "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Training continuous space language models: Some practical issues", "author": ["H.S. Le", "A. Allauzen", "G. Wisniewski", "F. Yvon"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Le et al\\.", "year": 2010}, {"title": "Producing high-dimensional semantic spaces from lexical cooccurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers", "citeRegEx": "Lund and Burgess,? \\Q1996\\E", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space. In: Workshop at ICLR", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dimensions of meaning", "author": ["H. Sch\u00fctze"], "venue": "ACM/IEEE Conference on Supercomputing", "citeRegEx": "Sch\u00fctze,? \\Q1992\\E", "shortCiteRegEx": "Sch\u00fctze", "year": 1992}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "Ratinov", "L.-A", "Y. Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Measuring praise and criticism: Inference of semantic orientation from association", "author": ["P.D. Turney", "M.L. Littman"], "venue": "ACM TOIS", "citeRegEx": "Turney and Littman,? \\Q2003\\E", "shortCiteRegEx": "Turney and Littman", "year": 2003}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": ", Turian et al. (2010), Collobert et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "(2010), Collobert et al. (2011), Mikolov et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 3, "context": "(2010), Collobert et al. (2011), Mikolov et al. (2013a)).", "startOffset": 8, "endOffset": 56}, {"referenceID": 9, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 2, "endOffset": 17}, {"referenceID": 6, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 18, "endOffset": 42}, {"referenceID": 1, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 43, "endOffset": 67}, {"referenceID": 5, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al.", "startOffset": 142, "endOffset": 169}, {"referenceID": 0, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al.", "startOffset": 194, "endOffset": 222}, {"referenceID": 14, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al., 2011) and sentiment analysis (Turney and Littman, 2003).", "startOffset": 258, "endOffset": 279}, {"referenceID": 13, "context": ", 2011) and sentiment analysis (Turney and Littman, 2003).", "startOffset": 31, "endOffset": 57}, {"referenceID": 8, "context": "As training set for the word embeddings, we use parts 02 to 21 of the Wall Street Journal (Marcus et al., 1993), a corpus of about one million tokens and roughly 35,000 word types.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": "We used two word relatedness data sets for evaluation: MEN1 (Bruni et al., 2012) and WordSim3532 (Finkelstein et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 4, "context": ", 2012) and WordSim3532 (Finkelstein et al., 2001).", "startOffset": 24, "endOffset": 50}, {"referenceID": 6, "context": "The problem of word embedding initialization was also addressed by Le et al. (2010). They propose three initialization schemes.", "startOffset": 67, "endOffset": 84}], "year": 2014, "abstractText": "There are two main approaches to the distributed representation of words: lowdimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributionalmodel vectors \u2013 as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "creator": "LaTeX with hyperref package"}}}