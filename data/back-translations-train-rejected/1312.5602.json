{"id": "1312.5602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Playing Atari with Deep Reinforcement Learning", "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "histories": [["v1", "Thu, 19 Dec 2013 16:00:08 GMT  (221kb,D)", "http://arxiv.org/abs/1312.5602v1", "NIPS Deep Learning Workshop 2013"]], "COMMENTS": "NIPS Deep Learning Workshop 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["volodymyr mnih", "koray kavukcuoglu", "david silver", "alex graves", "ioannis antonoglou", "daan wierstra", "martin riedmiller"], "accepted": false, "id": "1312.5602"}, "pdf": {"name": "1312.5602.pdf", "metadata": {"source": "CRF", "title": "Playing Atari with Deep Reinforcement Learning", "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move around without being able to move."}, {"heading": "2 Background", "text": "We consider tasks in which an agent interacts with an environment E, in this case the Atari emulator, in a sequence of actions, observations and rewards. In each time step, the agent selects an action from the series of legal game actions, A = {1,., Q}. The action is passed on to the emulator and modifies its internal state and game results. In general, the internal state of the emulator cannot be observed by the agent; instead, he observes an image xt \"Rd\" from the emulator, which is a vector of raw pixel values representing the current screen. In addition, he receives a reward that represents the change in game results. In general, the game score may depend on the entire previous sequence of actions and observations; feedback about an action can only be received after many thousands of time steps representing elapsed.Since the agent observes images of the current screen and many emulators are completely discernible, the emulators situation is completely excluded."}, {"heading": "3 Related Work", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live than in another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "4 Deep Reinforcement Learning", "text": "The most successful approaches are trained directly from the raw data, using lightweight updates based on stochastic gradient parentage. Our goal is to connect a reinforcement learning algorithm to a deep neural network that works directly on RGB images and efficiently processes training data by providing stochastic gradient updates. Tesla's TD gammon architecture provides a starting point for such an approach. This architecture updates the parameters of a network that estimates the value function directly from empirical examples, st, at, st + 1, drawn from the algorithms \"interactions with the environment (or through self-play in the case of backgammon)."}, {"heading": "4.1 Preprocessing and Model Architecture", "text": "Working directly with raw Atari frames, which are 210 x 160 pixel images with a 128 color palette, can be mathematically demanding, so we apply a basic pre-processing step aimed at reducing the input dimensionality; the raw frames are pre-processed by first converting their RGB representation to grayscale and scanning them down to a 110 x 84 image; the final input representation is obtained by cropping an 84 x 84 region of the image that roughly covers the gameplay area; the last cutout stage is only necessary because we use the GPU implementation of 2D convolutions from [11] that expect square inputs; the experiments in this paper, the \u03c6 function from algorithm 1, apply this pre-processing to the last 4 frames of a story and stack them to input them to the Q function.There are several possible ways of parameterizing types of Q network operations based on their neural configuration using Q."}, {"heading": "5 Experiments", "text": "So far, we have conducted experiments on seven popular ATARI games - Beam Rider, Breakout, Enduro, Pong, Q * bert, Seaquest, Space Invaders. We use the same network architecture, learning algorithm, and hyperparameter settings in all seven games, which shows that our approach is robust enough to work on a variety of games without including game-specific information. While we evaluated our agents on the real and unmodified games, we only made a change to the reward structure of the games during training. As the scale of results varies greatly from game to game, we fixed all positive rewards to 1 and all negative rewards to \u2212 1, leaving 0 rewards unchanged. Scoring the rewards in this way limits the scale of error derivatives and makes it easier to use the same learning rate in multiple games. At the same time, it could affect the performance of our agent as it cannot distinguish between rewards of different sizes."}, {"heading": "5.1 Training and Stability", "text": "In the area of reinforcement learning, however, accurately assessing an agent's progress during training can be difficult. As our rating metric is the total reward that the agent collects on average over a number of games in an episode or game, as suggested by [3], we calculate it regularly during training. The average total reward metric tends to be very loud because small changes in the weights of a policy can cause major changes in the distribution of states that visit the policy. Figure 2 shows how the average total reward develops during training on the Seaquest and Breakout games. Both average reward charts are actually quite loud, giving the impression that the learning algorithm is not making steady progress. Another, more stable metric is the estimated action value function Q, which provides an estimate of how much reward the agent can achieve by following its policy."}, {"heading": "5.2 Visualizing the Value Function", "text": "Figure 3 shows a visualization of the learned value function in the game Seaquest. The figure shows that the predicted value jumps after an enemy appears on the left side of the screen (dot A.) The agent then fires a torpedo at the enemy and the predicted value reaches its peak when the torpedo is about to hit the enemy (dot B.) Finally, the value falls approximately to its original value after the enemy disappears (dot C. Figure 3 shows that our method is able to learn how the value function develops for a reasonably complex sequence of events."}, {"heading": "5.3 Main Evaluation", "text": "We compare our results with the best methods from the RL literature."}, {"heading": "6 Conclusion", "text": "This paper presented a new deep learning model for reinforcement learning and demonstrated its ability to master difficult control strategies for Atari 2600 computer games, using only raw pixels as input. We also introduced a variant of online Q-Learning, which combines stochastic mini-batch updates with experience replay memory to facilitate deep network training for RL. Our approach delivered up-to-date results in six of the seven games tested without architecture or hyperparameter adjustment."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In Proceedings of the 12th International Conference on Machine Learning (ICML", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Sketch-based linear value function approximation", "author": ["Marc Bellemare", "Joel Veness", "Michael Bowling"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Investigating contingency awareness using atari 2600 games", "author": ["Marc G Bellemare", "Joel Veness", "Michael Bowling"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Bayesian learning of recursively factored environments", "author": ["Marc G. Bellemare", "Joel Veness", "Michael Bowling"], "venue": "In Proceedings of the Thirtieth International Conference on Machine Learning (ICML", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": "In Proc. ICASSP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A neuro-evolution approach to general atari game playing", "author": ["Matthew Hausknecht", "Risto Miikkulainen", "Peter Stone"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Actor-critic reinforcement learning with energy-based policies", "author": ["Nicolas Heess", "David Silver", "Yee Whye Teh"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "MarcAurelio Ranzato", "Yann LeCun"], "venue": "In Proc. International Conference on Computer Vision and Pattern Recognition (CVPR", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["Sascha Lange", "Martin Riedmiller"], "venue": "In Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Long-Ji Lin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation", "author": ["Hamid Maei", "Csaba Szepesvari", "Shalabh Bhatnagar", "Doina Precup", "David Silver", "Rich Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Toward off-policy learning control with function approximation", "author": ["Hamid Maei", "Csaba Szepesv\u00e1ri", "Shalabh Bhatnagar", "Richard S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Machine Learning for Aerial Image Labeling", "author": ["Volodymyr Mnih"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less real time", "author": ["Andrew Moore", "Chris Atkeson"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Why did td-gammon work", "author": ["Jordan B. Pollack", "Alan D. Blair"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Reinforcement learning with factored states and actions", "author": ["Brian Sallans", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["Pierre Sermanet", "Koray Kavukcuoglu", "Soumith Chintala", "Yann LeCun"], "venue": "In Proc. International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}], "referenceMentions": [{"referenceID": 10, "context": "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].", "startOffset": 153, "endOffset": 165}, {"referenceID": 21, "context": "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].", "startOffset": 153, "endOffset": 165}, {"referenceID": 15, "context": "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].", "startOffset": 153, "endOffset": 165}, {"referenceID": 5, "context": "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].", "startOffset": 189, "endOffset": 195}, {"referenceID": 6, "context": "Recent advances in deep learning have made it possible to extract high-level features from raw sensory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].", "startOffset": 189, "endOffset": 195}, {"referenceID": 12, "context": "an experience replay mechanism [13] which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "We apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Environment (ALE) [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 22, "context": "Such value iteration algorithms converge to the optimal actionvalue function, Qi \u2192 Q\u2217 as i \u2192 \u221e [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "Perhaps the best-known success story of reinforcement learning is TD-gammon, a backgammonplaying program which learnt entirely by reinforcement learning and self-play, and achieved a superhuman level of play [24].", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "This led to a widespread belief that the TD-gammon approach was a special case that only worked in backgammon, perhaps because the stochasticity in the dice rolls helps explore the state space and also makes the value function particularly smooth [19].", "startOffset": 247, "endOffset": 251}, {"referenceID": 24, "context": "Furthermore, it was shown that combining model-free reinforcement learning algorithms such as Qlearning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause the Q-network to diverge.", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "Furthermore, it was shown that combining model-free reinforcement learning algorithms such as Qlearning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause the Q-network to diverge.", "startOffset": 184, "endOffset": 187}, {"referenceID": 24, "context": "Subsequently, the majority of work in reinforcement learning focused on linear function approximators with better convergence guarantees [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "Deep neural networks have been used to estimate the environment E ; restricted Boltzmann machines have been used to estimate the value function [21]; or the policy [9].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "Deep neural networks have been used to estimate the environment E ; restricted Boltzmann machines have been used to estimate the value function [21]; or the policy [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 13, "context": "These methods are proven to converge when evaluating a fixed policy with a nonlinear function approximator [14]; or when learning a control policy with linear function approximation using a restricted variant of Q-learning [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "These methods are proven to converge when evaluating a fixed policy with a nonlinear function approximator [14]; or when learning a control policy with linear function approximation using a restricted variant of Q-learning [15].", "startOffset": 223, "endOffset": 227}, {"referenceID": 19, "context": "Perhaps the most similar prior work to our own approach is neural fitted Q-learning (NFQ) [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "NFQ has also been successfully applied to simple real-world control tasks using purely visual input, by first using deep autoencoders to learn a low dimensional representation of the task, and then applying NFQ to this representation [12].", "startOffset": 234, "endOffset": 238}, {"referenceID": 12, "context": "Q-learning has also previously been combined with experience replay and a simple neural network [13], but again starting with a low-dimensional state rather than raw visual inputs.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "The use of the Atari 2600 emulator as a reinforcement learning platform was introduced by [3], who applied standard reinforcement learning algorithms with linear function approximation and generic visual features.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Subsequently, results were improved by using a larger number of features, and using tug-of-war hashing to randomly project the features into a lower-dimensional space [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "The HyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where it was used to evolve (separately, for each distinct game) a neural network representing a strategy for that game.", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "By feeding sufficient data into deep neural networks, it is often possible to learn better representations than handcrafted features [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "In contrast to TD-Gammon and similar online approaches, we utilize a technique known as experience replay [13] where we store the agent\u2019s experiences at each time-step, et = (st, at, rt, st+1) in a data-set D = e1, .", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "This approach has several advantages over standard online Q-learning [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "It is easy to see how unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or even diverge catastrophically [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "A more sophisticated sampling strategy might emphasize transitions from which we can learn the most, similar to prioritized sweeping [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "The final cropping stage is only required because we use the GPU implementation of 2D convolutions from [11], which expects square inputs.", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Since Q maps historyaction pairs to scalar estimates of their Q-value, the history and the action have been used as inputs to the neural network by some previous approaches [20, 12].", "startOffset": 173, "endOffset": 181}, {"referenceID": 11, "context": "Since Q maps historyaction pairs to scalar estimates of their Q-value, the history and the action have been used as inputs to the neural network by some previous approaches [20, 12].", "startOffset": 173, "endOffset": 181}, {"referenceID": 9, "context": "The first hidden layer convolves 16 8\u00d7 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18].", "startOffset": 121, "endOffset": 129}, {"referenceID": 17, "context": "The first hidden layer convolves 16 8\u00d7 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18].", "startOffset": 121, "endOffset": 129}, {"referenceID": 2, "context": "Following previous approaches to playing Atari games, we also use a simple frame-skipping technique [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Since our evaluation metric, as suggested by [3], is the total reward the agent collects in an episode or game averaged over a number of games, we periodically compute it during training.", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "We compare our results with the best performing methods from the RL literature [3, 4].", "startOffset": 79, "endOffset": 85}, {"referenceID": 3, "context": "We compare our results with the best performing methods from the RL literature [3, 4].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "The method labeled Sarsa used the Sarsa algorithm to learn linear policies on several different feature sets handengineered for the Atari task and we report the score for the best performing feature set [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 3, "context": "Contingency used the same basic approach as Sarsa but augmented the feature sets with a learned representation of the parts of the screen that are under the agent\u2019s control [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3, 5] and report the average score obtained by running an -greedy policy with = 0.", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[3, 5] and report the average score obtained by running an -greedy policy with = 0.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "We also include a comparison to the evolutionary policy search approach from [8] in the last three rows of table 1.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "4 157 110 179 Sarsa [3] 996 5.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "2 129 \u221219 614 665 271 Contingency [4] 1743 6 159 \u221217 960 723 268 DQN 4092 168 470 20 1952 1705 581 Human 7456 31 368 \u22123 18900 28010 3690 HNeat Best [8] 3616 52 106 19 1800 920 1720 HNeat Pixel [8] 1332 4 91 \u221216 1325 800 1145 DQN Best 5184 225 661 21 4500 1740 1075", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "2 129 \u221219 614 665 271 Contingency [4] 1743 6 159 \u221217 960 723 268 DQN 4092 168 470 20 1952 1705 581 Human 7456 31 368 \u22123 18900 28010 3690 HNeat Best [8] 3616 52 106 19 1800 920 1720 HNeat Pixel [8] 1332 4 91 \u221216 1325 800 1145 DQN Best 5184 225 661 21 4500 1740 1075", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "2 129 \u221219 614 665 271 Contingency [4] 1743 6 159 \u221217 960 723 268 DQN 4092 168 470 20 1952 1705 581 Human 7456 31 368 \u22123 18900 28010 3690 HNeat Best [8] 3616 52 106 19 1800 920 1720 HNeat Pixel [8] 1332 4 91 \u221216 1325 800 1145 DQN Best 5184 225 661 21 4500 1740 1075", "startOffset": 193, "endOffset": 196}], "year": 2013, "abstractText": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "creator": "LaTeX with hyperref package"}}}