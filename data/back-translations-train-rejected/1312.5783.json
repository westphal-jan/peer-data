{"id": "1312.5783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "abstract": "In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.", "histories": [["v1", "Fri, 20 Dec 2013 00:21:36 GMT  (306kb,D)", "http://arxiv.org/abs/1312.5783v1", "9 pages, submitted to ICLR"]], "COMMENTS": "9 pages, submitted to ICLR", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["yunlong he", "koray kavukcuoglu", "yun wang", "arthur szlam", "yanjun qi"], "accepted": false, "id": "1312.5783"}, "pdf": {"name": "1312.5783.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "authors": ["Yunlong He", "Koray Kavukcuoglu", "Yun Wang", "Arthur Szlam", "Yanjun Qi"], "emails": ["heyunlong@gatech.edu,", "koray@deepmind.com,", "yunwang@princeton.edu,", "aszlam@ccny.cuny.edu", "yanjun@virginia.edu,"], "sections": [{"heading": "1 Introduction", "text": "In the last decade, people have realized that the central problem of object recognition is learning meaningful representations (characteristics) of the image / video. Much emphasis has been placed on building an effective learning architecture that combines modern methods of machine learning and, in the meantime, takes into account the characteristics of image data and visual problems. In this thesis, we combine the power of the deep learning architecture and the pipeline of \"bag-of-visual-words\" (BoV) to construct a new, unattended feature-learning architecture for image representation. Compared to the single-layer \"spare coding\" (SC) framework, our method can extract feature hierarchies at the different levels of abstraction. The sparse codes at the same level hold spatial smoothness across image fields and different SC hierarchies for the representation of abstraction. As a result, the method of visual recognition has a greater reach for object recognition tasks, and therefore has better performance in the presentation of the FT."}, {"heading": "1.1 Bag-of-visual-words pipeline for object recognition", "text": "We check the picture word pipeline, consisting of handmade descriptors, which are each derived from the individual picture components. (...) We check the picture word pipeline, consisting of handmade descriptors, which are each derived from the individual picture components. (...) We check the picture word pipeline, consisting of handmade descriptors, picture components, picture components, picture components, picture components, picture components and picture components (...) are all picture components, picture components, picture components, picture components and picture components. (...) The picture components, picture components, picture components and picture components, picture components, picture components and picture components (...) are all picture components, picture components and picture components. (The picture components, picture components and...)."}, {"heading": "1.2 Dimensionality reduction by learning an invariant mapping", "text": "We are now testing a method called Dimensionality Reduction by learning an invariant mapping (DRLIM, see [12]), which is the basic model for our new method in Section 2.3. Unlike traditional, unattended dimensionality reduction methods, DRLIM relies not only on a set of training examples y1, y2, \u00b7 \u00b7, yn, RK, but also on a set of binary markers {lij: (i, j), where I am the set of index pairs, so that (i, j) \"I\" when the label is available for the corresponding instance pair (yi, yj). Binary markup lij = 0, if the pair of training examples are yi and yj are similar instances, and lij = 1, if yi and yj are known to be dissimilar. Note that the similarity indicated by lij is usually from the additional resource, rather than from \u00b7 j, the knowledge \u00b7 yj, from the data example."}, {"heading": "2 Deep sparse learning framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "This year is the highest in the history of the country."}, {"heading": "2.2 Learning the pooling function", "text": "In this subsection, we introduce the details of designing the local spatial pooling, which is executed as the first part of the pooling function. First, we define the pooling function as a map of a set of sparse codes on a sampling grid on a set of dense codes on a new sampling grid. Suppose that G is the sampling grid that includes M sampling points on an image, with the two adjacent sampling points having a fixed distance (number of pixels) between them. As introduced in Section 1.1, each sampling point corresponds to the center of an image patch. Let Y = [y1, \u00b7, yM] have the sparse codes on the sampling grid G, where each yi is associated with a sampling point on G, and each sampling point on which is mapped."}, {"heading": "2.3 Dimensionality reduction with spatial information", "text": "In this subsection, we present the details of combining the DRLIM method [12] with the spatial information of the image fields to learn low-dimensional embedding. (5) Since the feature vector of A is transformed into a low-dimensional space, some of its information is discarded while some are preserved. (As introduced in subsection 1.2, DRLIM is trained on a collection of data pairs (y-i, y-j), each of which is associated with a binary label indicating their relationship. Therefore, it provides the opportunity to include earlier knowledge in the dimensionality reduction process by specifying the binary labels of the image pairs based on previous knowledge. In the case of object recognition, the prior knowledge we want to impose on the system is that an image patch is moved between a few pixels, and still contains the same object."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the performance of the DeepSC Framework for image classification using three sets of data: Caltech-101 [7], Caltech-256 [11], and 15-Scene. Caltech-101 dataset contains 9144 images belonging to 101 classes, with approximately 40 to 800 images per class. Most images from Caltech-101 are medium resolution, i.e. approximately 300 x 300. Caltech-256 dataset contains 29, 780 images from 256 categories. The collection has a higher variability in class and object location than Caltech-101. The images are of similar size to Caltech-101. The 15-Scene dataset is compiled by several researchers [8, 13, 15], and contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400. Categories include living room, bedroom, kitchen, highway, mountain, street, and etal.For each dataset, the average accuracy per class is given."}, {"heading": "3.1 Effects of Number of DeepSC Layers", "text": "As shown in Figure 2, the DeepSC framework uses multiple layers of feature abstraction to get a better representation for images. Here, we first examine the effects of variation in the number of layers used in our framework. Table 1 shows the average accuracy of detection per class on three sets of data when all use 1024 as the dictionary size. The number of training images per class for the three sets is 30 for Caltech-101, 60 for Caltech-256, and 100 for 15 scenes, respectively. The second line shows the results when we have only one layer of sparse encoding, while the third and fourth line describe the results when we have two layers in DeepSC or three layers in DeepSC. The multi-layer DeepSC framework clearly performs better on all three sets of data compared to the single-layer SPM-SC system. In addition, classification accuracy improves with increasing number of layers."}, {"heading": "3.2 Effects of SC Dictionary Size", "text": "We examine how the performance of the proposed DeepSC framework changes when the dictionary size of the sparse encoding is varied. In each of the three datasets, we look at three settings where the dimension of the sparse encoding K is 1024, 2048, and 4096. The number of training frames per class for these experiments is set at 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-scene. We can observe that the performance of DeepSC is improving with more and more levels, while in the case of K = 4096 the performance increase in terms of the accuracy of the sparse encoding K from 1024 to 4096 is not as significant. This is probably due to the fact that the parameter space in this case is already very large for the limited training data size. Another observation we have made from Table 2 is that Ep3 and 4-SC are getting better and better (SP2-SC = De48-SC = De48-SC)."}, {"heading": "3.3 Effects of Varying Training Set Size", "text": "In addition, we check the change in performance by varying the number of training frames per class to two Caltech datasets. At this point, we set the dimension of the sparse K codes to 2048. On Caltech-101, we compare two cases: randomly select 15 or 30 frames per category as training frames and test the rest. On Caltech-256, we randomly select 60, 30 or 15 frames per category as training frames and test the rest. Table 5 and Table 6 show that the DeepSC framework with the smaller set of training frames further improves accuracy with further levels."}, {"heading": "3.4 Effects of varying parameters of DRLIM", "text": "In Table 7 we report on the performance fluctuations in the adjustment of DRLIM parameters. The parameter \u03c3 is the threshold for the selection of positive and negative training pairs (see (6)) and the parameter \u03b2 for the loss of hinges (see (7)) of the DRLIM model serves to control the punishment of negative training pairs. We recognize that it is important to select the correct threshold parameter \u03c3 in such a way that the transformation learned by DRLIM can distinguish mostly overlapping image pairs and partly overlapping image pairs."}, {"heading": "3.5 Comparison with other methods", "text": "We then compare our results with other algorithms in Table 8. The most direct baselines 2 for DeepSC for comparison are the sparse encoding plus SPM Framework (ScSPM) [17], LLC [16] and SSC [1]. Table 8 shows the comparison of our DeepSC with the ScSPM and SSC. We can see that our results are comparable to SSC, with lower accuracy in 15-scene data (the default value of SSC is much higher than ours). For the LLC method proposed by [16], it reported 73.44% for Caltech-101 when using K = 2048 and 47.68% when using K = 4096. Our DeepSC-3 reached 78.43% for Caltech-101 when using K = 2048 and 49.91% when using K = 4096. Overall, our system achieves state-of-the-art performance in all three data sectors. 2We are also aware that some works achieve very high accuracy when using K = 2048 and 491% when using K = 4091."}], "references": [{"title": "Smooth sparse coding via marginal regression for learning sparse representations", "author": ["K. Balasubramanian", "K. Yu", "G. Lebanon"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv preprint arXiv:1206.5538,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multipath sparse coding using hierarchical matching pursuit", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "In CVPR, Workshop on Generative-Model Based Vision.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Geometric p-norm feature pooling for image classification", "author": ["J. Feng", "B. Ni", "Q. Tian", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Local features are not lonely\u2013laplacian sparse coding for image classification", "author": ["S. Gao", "I.W. Tsang", "L.-T. Chia", "P. Zhao"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelop", "author": ["A. Oliva", "A. Torraba"], "venue": "In IJCV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "We should note, however, other sparse encoding methods such as vector quantization and LLC could be used to learn the sparse representations (see [6] for review and comparisons).", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "Moreover, the dictionary learning process of finding V in (1) is often conducted in an online style [14] and then the feature descriptors of the i-th image stored in X are encoded as the bag-of-visual-words representations stored in Y (i) = [y 1 , \u00b7 \u00b7 \u00b7 , y (i) Mi ] in the K-dimensional space (K >> D).", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "To achieve this, each image is divided into three levels of pooling regions as suggested by the spatial pyramid matching (SPM) technique [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "We now review a method called dimensionality reduction by learning an invariant mapping (DRLIM, see [12]), which is the base model for our new method in Subsection 2.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "Recent progress in deep learning [2] has shown that the multi-layer architecture of deep learning system, such as that of deep belief networks, is helpful for learning feature hierarchies from data, where different layers of feature extractors are able to learn feature representations of different scopes.", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "\u2022 Second, it is well-known (see [16, 10]) that sparse coding is not \u201csmooth\u201d, which means a small variation in the original space might lead to a huge difference in the code space.", "startOffset": 32, "endOffset": 40}, {"referenceID": 9, "context": "\u2022 Second, it is well-known (see [16, 10]) that sparse coding is not \u201csmooth\u201d, which means a small variation in the original space might lead to a huge difference in the code space.", "startOffset": 32, "endOffset": 40}, {"referenceID": 11, "context": "In this subsection, we introduce the details of combining the DRLIM method [12] with the spatial information of image patches to learn a low-dimensional embedding A such that", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "In this section, we evaluate the performance of DeepSC framework for image classification on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene.", "startOffset": 122, "endOffset": 125}, {"referenceID": 10, "context": "In this section, we evaluate the performance of DeepSC framework for image classification on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene.", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 12, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 14, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 3, "context": "For each image, following [4], we sample 16 \u00d7 16 image patches with 4-pixel spacing and use 128 dimensional SIFT feature as the basic dense feature descriptors.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "The final step of classification is performed using one-vs-all SVM through LibSVM toolkit [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "For the LLC method proposed from [16], it reported to achieve 73.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or multiple-path system that utilizes image patches of multiple sizes [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or multiple-path system that utilizes image patches of multiple sizes [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 16, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 99, "endOffset": 102}], "year": 2013, "abstractText": "In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.", "creator": "LaTeX with hyperref package"}}}