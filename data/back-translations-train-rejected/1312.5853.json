{"id": "1312.5853", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Multi-GPU Training of ConvNets", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs within the same server.", "histories": [["v1", "Fri, 20 Dec 2013 08:45:07 GMT  (162kb,D)", "https://arxiv.org/abs/1312.5853v1", "Machine Learning, Deep Learning, Computer Vision"], ["v2", "Mon, 23 Dec 2013 15:50:49 GMT  (162kb,D)", "http://arxiv.org/abs/1312.5853v2", "Machine Learning, Deep Learning, Convolutional Networks, Computer Vision, GPU, CUDA"], ["v3", "Sat, 28 Dec 2013 08:31:12 GMT  (162kb,D)", "http://arxiv.org/abs/1312.5853v3", "Machine Learning, Deep Learning, Convolutional Networks, Computer Vision, GPU, CUDA"], ["v4", "Tue, 18 Feb 2014 21:35:13 GMT  (163kb,D)", "http://arxiv.org/abs/1312.5853v4", "Machine Learning, Deep Learning, Convolutional Networks, Computer Vision, GPU, CUDA"]], "COMMENTS": "Machine Learning, Deep Learning, Computer Vision", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["omry yadan", "keith adams", "yaniv taigman", "marc'aurelio ranzato"], "accepted": false, "id": "1312.5853"}, "pdf": {"name": "1312.5853.pdf", "metadata": {"source": "CRF", "title": "Multi-GPU Training of ConvNets", "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman"], "emails": ["ranzato}@fb.com"], "sections": [{"heading": "Multi-GPU Training of ConvNets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Omry Yadan Keith Adams Yaniv Taigman Marc\u2019Aurelio Ranzato", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Facebook AI Group", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}], "references": [{"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Imagenet: a large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "L. Couprie", "C. amd Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A.R. Mohamed", "H. Jiang", "G. Penn"], "venue": "In ICASSP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Improvements to deep convolutional neural networks for lvcsr", "author": ["T. Sainath", "A.R. Kingsbury", "Mohamed", "G. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A. Aravkin", "B. Ramabhadran"], "venue": "In ASRU,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["S. Zhang", "C. Zhang", "Z. You", "R. Zheng", "B. Xu"], "venue": "In ICASSP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Pipelined back-propagation for context-dependent deep neural networks", "author": ["X. Chen", "A. Eversole", "G. Li", "D. Yu", "F. Seide"], "venue": "In Interspeech,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Deep learning with cots hpc", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 96, "endOffset": 105}, {"referenceID": 3, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 96, "endOffset": 105}, {"referenceID": 5, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 124, "endOffset": 130}, {"referenceID": 6, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 124, "endOffset": 130}, {"referenceID": 7, "context": "Convolutional neural networks [3] have proven useful in many domains, including computer vision [1, 4, 5], audio processing [6, 7] and natural language processing [8].", "startOffset": 163, "endOffset": 166}, {"referenceID": 0, "context": "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs.", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.", "startOffset": 21, "endOffset": 32}, {"referenceID": 9, "context": "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.", "startOffset": 21, "endOffset": 32}, {"referenceID": 10, "context": "Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm.", "startOffset": 21, "endOffset": 32}, {"referenceID": 8, "context": "We consider two basic approaches: data and model parallelism [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Model parallelism, on the other hand, consists of splitting an individual network\u2019s computation across multiple GPUs [9, 12].", "startOffset": 117, "endOffset": 124}, {"referenceID": 11, "context": "Model parallelism, on the other hand, consists of splitting an individual network\u2019s computation across multiple GPUs [9, 12].", "startOffset": 117, "endOffset": 124}, {"referenceID": 0, "context": "[1] further customized the architecture of the network to better leverage model parallelism: the architecture consists of two \u201ccolumns\u201d each allocated on one GPU.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] (with the only exception of the mini-batch size).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The task is classification on the ImageNet 2012 datasset [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "[1], achieved a speed-up of 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classification and investigate methods to speed convergence by parallelizing training across multiple GPUs. In this work, we used up to 4 NVIDIA TITAN GPUs with 6GB of RAM. While our experiments are performed on a single server, our GPUs have disjoint memory spaces, and just as in the distributed setting, communication overheads are an important consideration. Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm. Instead, we isolate the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochastic gradient descent.", "creator": "LaTeX with hyperref package"}}}