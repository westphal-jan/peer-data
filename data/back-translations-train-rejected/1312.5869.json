{"id": "1312.5869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Principled Non-Linear Feature Selection", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "histories": [["v1", "Fri, 20 Dec 2013 10:16:13 GMT  (232kb,D)", "http://arxiv.org/abs/1312.5869v1", null], ["v2", "Tue, 18 Feb 2014 17:25:43 GMT  (277kb,D)", "http://arxiv.org/abs/1312.5869v2", "arXiv admin note: substantial text overlap witharXiv:1311.5636"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dimitrios athanasakis", "john shawe-taylor", "delmiro fernandez-reyes"], "accepted": false, "id": "1312.5869"}, "pdf": {"name": "1312.5869.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Linear Feature Maps With An Application To Representation Learning", "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "emails": ["dathanasakis@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Character selection is an important aspect of implementing machine learning methodologies, and the selection of informative features can reduce generalization errors and memory and processing requirements for large datasets. In addition, economical models provide valuable insights into the relationships underlying the process studied. There is a wealth of literature on feature selection when the relationship between variables is linear. Unfortunately, the relationship between nonlinear feature selection [1] becomes much more nuanced. Core methods are characterized by modeling nonlinear relationships. Unsurprisingly, a number of kernel-based feature selection algorithms have been proposed. Early approaches, such as recursive feature selection (RFE) [1], can be computer prohibitive, while attempts to learn a convex combination of low-level cores fail due to nonlinearity in the underlying relationship. Newer approaches that address core adaptations, but can increase core adaptations, and non-expose relationships."}, {"heading": "1.1 Related Work", "text": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as an empirical estimate for the Hilbert Schmidt Criterion of Independence (HSIC). The work on HSIC [4] provides the basis for using the alignment of centered kernel matrices as a basis for measuring statistical dependence. The Hilbert Schmidt criterion of independence is the basis for further work in [5], where the Department of Computer Science, University College London, UK \u2020 National Institute for Medical Research, London, UKar Xiv: 131 2,58 69v1 [cs."}, {"heading": "2 Preliminaries", "text": "We consider the supervised learning problem of modeling the relationship between an m \u00b7 n input matrix X and a corresponding m \u00b7 n \u00b2 output matrix Y. The simplest instance of such a problem is the binary classification, where the learning problem consists in learning a function f: x \u2192 y that maps input vectors x to the desired outputs y. In the binary case, we are confronted with a m \u00b7 n matrix X and a vector of outputs y, limiting the class of discrimination functions to linear classifiers, which we want to classify as f (x) = \u2211 i wixi = < w, x > The linear learning formulation can be generalized to the nonlinear setting by using a nonlinear characteristic card \u03c6 (x), which leads to the kernel formula: f (x) = < w, \u03c6 (x) < < short-term (xi), short-term (K), short-term (K) equals (K) and constant (K):"}, {"heading": "3 Development of key ideas", "text": "The approach we will take will be based on the following known observation, which links the orientation of the nucleus to the degree to which an input space contains a linear projection that correlates with the target."}, {"heading": "4 Properties of the algorithm", "text": "We now define our randomized selection algorithm (randSel). Given a m \u00b7 n input matrix X and the corresponding output matrix Y, randSel assumes that the individual contribution of the features is estimated by estimating the alignment of a number of random subsamples comprising n2 and n 2 + 1 randomly selected features, resulting in an estimate of the expected alignment contribution including one trait. The algorithm is parameterized by the number of bootstraps N, a bootstraps sizenb, and a percentage of z% of the features falling after N bootstraps. The algorithm proceeds iteratively until only two traits remain. There are a number of advantages of this approach, apart from the tangible probability guarantees. RandSel gracefully scales the composition of a kernel k (x, x \u2032) for the composition of a kernel k (x), x \u00b2 for the atomic complication, and the number of the kernel item item."}, {"heading": "5 Feature Selection for learned representations", "text": "The depth of a learning architecture refers to the composition of different levels of nonlinear operations in the learned function. This suggests that the use of function selection to refine a set of learned representations would greatly benefit from capturing nonlinear interactions between the learned features. RandSel for function selection in this setting relies on a number of properties. RandSel is easily applicable to a large sample size, a key characteristic for the large sample sizes that are typically associated with imaging learning. In addition, the algorithm is easily applicable to domains that have a certain structure. The multi-class structure of the black box challenge is an example in which this property is important. This is a common property of all HSIC variants. Finally, randSel is granular."}, {"heading": "5.1 Prediction", "text": "RandSel progressively produces fine-grained combinations of characteristics. A prediction mechanism that effectively utilizes the increasingly granular combinations of characteristics includes the last step of our approach, in which we take a reinforced approach based on LPBOOST-MKL [10]. Architecture progresses by building a number of Gaussian cores parameterized by the different sets of characteristics on which they are defined, and a kernel bandwidth parameter that contains only variables contained in the set si. For simplicity's sake, we assume that there are nK unique combinations of sets si and corresponding bandwidths. We define a kernel on each such combination of characteristics and bandwidth as vectors that contain only variables contained in the set si. For simplicity's sake, let's assume that there are nK unique combinations of sets si and corresponding bandwidths."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Results on the ICML Black Box Learning Challenge", "text": "We used our proposal in the recent ICML 2013 Challenges in Representation Learning Black Box Learning challenge [8] [12]. The data set used in the challenge was a blurred subset of the Street View House Numbers dataset [15]. The original data was projected down to 1875 dimensions by random matrix multiplication, and the organizers did not reveal the source of the dataset until after the competition was over. The training set consisted of only 1,000 marked samples, while a further 130,000 samples were provided for unattended lecture training purposes. Cross-validation was used for our submissions to select the number of features to be learned with economical filtering, with our best solution using a set of 625 learned features. Randsel was then used to select combinations of the 625 learned features that dropped 12.5% of the least contributing features at the end of each iteration."}, {"heading": "6.2 Application to cleavage site prediction", "text": "It is not only the way in which the people in the USA, but also the way in which the people in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "6.3 Experimental Results", "text": "Table 1 summarizes the results for the various approaches. Using the original feature representation with a nonlinear SVM results in a generalization accuracy of 67.2%, which is substantially lower than all approaches based on the learned feature representation, suggesting that performance increases occur when using a learned representation. In terms of using the feature selection on the learned representation, the results suggest that randSel has an advantage over the other two methods, with RFE also performing slightly better than the l1-regulated logistic regression-based stability selection. The learned representation provides a case where it is reasonable to suspect benign nonlinear matches between features intended to exploit both RFE and randSel, and the large sample size allows increased confidence in concluding such relationships. The fact that randSel exceeds the RFE indicates that dependence on the support factors may be negatively affected."}, {"heading": "7 Conclusions", "text": "In this paper, we propose randSel, a new algorithm for nonlinear feature selection based on randomized estimates of HSIC. RandSel stochastically estimates the expected importance of features in each iteration and continues to cull uninformative features at the end of each iteration. Our theoretical analysis provides strong guarantees for the expected performance of this process, which are further demonstrated by testing a number of real and artificial data sets. In addition, we presented a simple system that produces a classification rule based on non-linear learned feature combinations of increasing granularity. The architecture of the system includes a fast, unmonitored feature learning mechanism, randomized nonlinear feature selection, and a multi-kernel based classifier. The guiding principle of this approach is to use simple components that require minimal parameter adjustment, with components further down the pipeline balancing the potential imbalances upstream."}, {"heading": "PREDICTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "OUTPUT LAYER MACHINERY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "KERNEL ON FEATURESET ...", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RANDSEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "INCOMING DATA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "OVER-COMPLETE LEARNED", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "SPARSE FILTERING", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning 46,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "On kernel target alignment. Advances in neural information processing systems", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Feature selection via dependence maximization", "author": ["Song", "Le", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Feature selection for SVMs. Advances in neural information processing systems: 668-674", "author": ["Weston", "Jason", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Multiple Kernel Learning on the Limit Order Book", "author": ["Tristan Fletcher", "Zakria Hussain", "John Shawe-Taylor"], "venue": "Proceedings of the First Workshop on Applications of Pattern Analysis", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Fast dependency-aware feature selection in very-highdimensional pattern recognition", "author": ["Somol", "Petr", "Jiri Grim", "Pavel Pudil"], "venue": "In Systems, Man, and Cybernetics (SMC) 2011 IEEE International Conference on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Challenges in Representation Learning: A report on three machine learning contests", "author": ["I.J. Goodfellow", "D. Erhan", "P.L. Carrier", "A. Courville", "M. Mirza", "B. Hamner", "Y. Bengio"], "venue": "Proceedings of the 20th International Conference on Neural Information Processing", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "PrediSi: prediction of signal peptides and their cleavage positions.", "author": ["Hiller", "Karsten", "Andreas Grote", "Maurice Scheer", "Richard Mnch", "Dieter Jahn"], "venue": "Nucleic acids research 32, no. suppl", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "SignalP 4.0: discriminating signal peptides from transmembrane regions.", "author": ["Petersen", "Thomas Nordahl", "Sren Brunak", "Gunnar von Heijne", "Henrik Nielsen"], "venue": "Nature methods 8, no", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical estimator for the Hilbert-Schmidt Independence Criterion(HSIC).", "startOffset": 66, "endOffset": 71}, {"referenceID": 2, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical estimator for the Hilbert-Schmidt Independence Criterion(HSIC).", "startOffset": 66, "endOffset": 71}, {"referenceID": 3, "context": "The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where \u2217Department of Computer Science, University College London, London, UK \u2020National Institute For Medical Research, London, UK", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "The dependence estimation of random subsets of variables is similar to the approach of [11], which is extended through bootstrapping and carefully controlled feature set sizes.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "This approach is a straightforward extension of the idea of BaHsic [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "A prediction mechanism effectively utilising the increasingly granular combinations of features comprises the last step of our approach, where we take a boosting approach based on LPBOOST-MKL [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "We used our proposal in the recent ICML 2013 Challenges in Representation Learning Black Box Learning challenge [8][12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "The Predisi dataset [13] of eukaryotic signal sequences was used for experimentation.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "However, our proposed approach appears to outperform SignalP\u2019s [14] reported accuracy of 72.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides probabilistic guarantees for correct identification of relevant features under reasonable assumptions. RandSel\u2019s characteristics make it an ideal candidate for identifying informative learned representations. We\u2019ve conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "creator": "LaTeX with hyperref package"}}}