{"id": "1312.5946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Adaptive Seeding for Gaussian Mixture Models", "abstract": "In this paper, we consider simple and fast approaches to initialize the Expectation-Maximization algorithm (EM) for multivariate Gaussian mixture models. We present new initialization methods based on the well-known $K$-means++ algorithm and the Gonzalez algorithm. These methods close the gap between simple uniform initialization techniques and complex methods, that have been specifically designed for Gaussian mixture models and depend on the right choice of hyperparameters. In our evaluation we compare our methods with a commonly used random initialization method, an approach based on agglomerative hierarchical clustering, and a known, plain adaption of the Gonzalez algorithm. Our results indicate that algorithms based on $K$-means++ outperform the other methods.", "histories": [["v1", "Fri, 20 Dec 2013 14:08:48 GMT  (202kb,D)", "http://arxiv.org/abs/1312.5946v1", null], ["v2", "Mon, 1 Aug 2016 08:33:13 GMT  (203kb,D)", "http://arxiv.org/abs/1312.5946v2", "An improved version of this paper (Adaptive Seeding for Gaussian Mixture Models) has been accepted for publication in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2016 and is available via the DOI 10.1007/978-3-319-31750-2_24"], ["v3", "Tue, 30 May 2017 07:44:37 GMT  (652kb,D)", "http://arxiv.org/abs/1312.5946v3", "This is a preprint of a paper that has been accepted for publication in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2016. The final publication is available at link.springer.com (this http URL24)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["johannes bl\\\"omer", "kathrin bujna"], "accepted": false, "id": "1312.5946"}, "pdf": {"name": "1312.5946.pdf", "metadata": {"source": "CRF", "title": "Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models", "authors": ["Johannes Bl\u00f6mer", "Kathrin Bujna"], "emails": ["bloemer@uni-paderborn.de", "kathrin.bujna@uni-paderborn.de"], "sections": [{"heading": null, "text": "I. INTRODUCTIONGaussian mixing modeling is an important task in the field of clustering and pattern recognition. There are various approaches to estimating the parameters of a mixture such as graphical methods, moment matching, Bayean approaches and the method of maximum probability estimation (MLE). A widely used approach to the latter problem is the ExpectationMaximization (EM) algorithm [1]. Starting from a certain initial mixing model, the EM algorithm alternates between calculating a lower limit of log probability and improving the current model in relation to this lower limit. The algorithm converges to a certain stationary point of probability function. Unfortunately, the probability function is generally non-convex, has many stationary points, including small local maxima, and even worse, local minima and saddle points (cf. [2], [3]). Furthermore, the convergence of the EM algorithm to both types depends on the degree of points chosen."}, {"heading": "B. Related Work", "text": "The simplest and most widespread random initialization schemes are the so-called rndEM and emEM methods (cf. [5] - [8]), which we also call \"random initializations,\" which generate multiple candidates by drawing evenly random means from the input set and then using some (data-dependent) approximations of covariances and weights. In the case of emEM, the candidates are improved by a few iterations of the EM algorithm. Finally, these methods select the candidate with the highest probability. Other popular approaches are based on hierarchical agglomerative clustering (HAC). For example, in [6] - [8] HAC (with Ward's criterion, average linkage, and model-based distance measurement) is used to obtain the means of the initial model."}, {"heading": "C. Our Contribution", "text": "Of course, there is no way to determine \"the\" best initialization algorithm that outperforms all algorithms on all instances. Initialization performance depends on the data and the permitted computational costs. Nevertheless, the initializations presented so far (with the exception of simple random initializations) face two main problems: First, they are quite complex and time consuming; second, the choice of hyperparameters is crucial to the outcome of the respective algorithm; first, we consider methods that apply known initialization methods directly to different K clustering problems; this closes the gap between the simple uniform methods and those developed specifically for Gaussian mixing models; the initialization methods presented in this paper can be divided into two groups; first, we consider methods that directly apply known initialization methods to different K clustering problems; namely, we use the K-Means + + algorithm [13], the Gonz\u00e1lez algorithm, and a uniform initialization."}, {"heading": "A. Finding Initial Means", "text": "We will consider the following three known initialization methods for K clustering algorithms. The first method is the so-called K mean + + initialization [13], which is to be used for the K mean clustering algorithm. In each round, K mean + + extracts a new mean from the given data setMeans2GMM (X, {\u00b51,.., \u00b5k}) algorithm 1 1: Derive partition {C1,..., CK} from X by assigning each point xi-X to its nearest mean. 2: Set-K = 1 / | Ck-x-Ck-3: Set-k = 1 / | Ck-x-Ck-Ck-Ck (x-\u00b5k) T. 4: Set the weights to the fraction of the points associated with the therespeective cluster, i.e. wk = | Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck-Ck (x-k (x-k) Ck (x-T. \u00b5k)"}, {"heading": "B. Directly Using Initial Means", "text": "The following three methods directly use the above-mentioned initializations: Gonzalez executes the Gonzales algorithm, Kmeans + + executes the K-means + + algorithm [13], and Uniform draws K-means evenly randomly from the given dataset. Subsequently, they all modify and expand the resulting K-means set to a complete Gaussian mixing model using a method we call Means2GMM (cf. Fig. 1). For K-means, Means2GMM reduces the MLE problem to K-independent 1-MLE problems. That is, it partitions the data according to the K-means clustering induced by the means, and then solves the 1-MLE problems in relation to each of the clusters (cf. [4]). However, a cluster may be too small. In such cases, Means2GMM determines an appropriate approximation (i.e. a spherical codiance matrix)."}, {"heading": "C. Transferring the Ideas to Gaussian Mixture Models", "text": "The implementation of the ideas behind the Gonzalez and Kmeans + + algorithm for the Gaussian mixture raises two problems. First, we need to choose a suitable density function, according to which a point is selected from the dataset (either by drawing according to the density or by determining the point with the highest density). Second, we also need to determine how the current Gaussian mixture model is updated, while the latter can assume negative values. Thus, we use an approximation of the negative log probability. To define this, we consider the minimum negative log probability of a point x recorded by each component."}, {"heading": "III. EXPERIMENTS", "text": "In addition to the presented algorithms, we evaluated a method that uses HAC with average link costs, i.e. a method that HAC executes on a uniform sample of the size s \u00b7 | X | of the input set X and then uses Means2GMM. For this approach and for the algorithms presented in paragraph II-C. For this approach, we tested only a few reasonable values for hyperparameters, i.e. s = 0.1 and \u03b1 = 1, 0.5}. In order to obtain comparable results, all methods were implemented in C + + (available at [18]). Since all the presented methods are randomized, we performed each initialization with 30 different values 2 in order to obtain reasonable results. As a stop criterion for the EM algorithm, we simply used a fixed number of rounds. Usually, a stop criterion is used based on the relative change in probability. However, this requires the choice of a reasonable threshold, which is crucial not only for the resulting probability, but also for the running time of the first 50 rounds."}, {"heading": "A. Data", "text": "The first dataset is provided by the ELKI database. [19] The second dataset is based on the property based on the color palette based on various characteristics in the color space (cf. [21]). We have created the second dataset, called the Cities dataset, from the data provided by the geographic database GeoNames. (http: / / www.geonames.org /), which contains several characteristics of 135 082 cities around the world with a population of at least 1000. Our dataset is a projection of the geographic coordinates of cities on one level. The third dataset is the Forest Covertype dataset, which contains 581 012 data points and is available from the UCI Machine Learning Library. [22] To get reasonable data, we ignore class 44 and the remaining attributes."}, {"heading": "B. Results", "text": "In our evaluation, we focus on the quality of the initial and final solution. Full results can also be found at [18]. [3Thus, while maintaining the remaining parameters fixed, we essentially obtain (except for the scaled means) the same mixing models and can assess the effects of the different degrees of separation.1) Generated Data Sets: Note that we have generated an enormous amount of data. For each data set, we calculate the average value and variance of the negative log probability of the solutions obtained through an initialization methodology. To summarize this information in relation to a fixed test set, we proceed in the following two steps.4: First, we create rankings of the initialization methods that we obtain for each data set four different rankings based on the original and final solution and measured by the average and variance of the negative log probability. Second, we count the respective times a certain rank has been reached."}, {"heading": "C. Conclusion", "text": "If you need a quick and easy initialization method that does not require training in hyperparameters, we recommend using one of the following. Given a dataset that presumably contains noise, the adaptive initialization method with \u03b1 = 0.5 is a good choice. If there is no noise, just use the simple K-mean + + initialization followed by Means2GMM."}], "references": [{"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B: Statistical Methodology, vol. 39, no. 1, pp. 1\u201338, 1977.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1977}, {"title": "On the convergence properties of the EM algorithm", "author": ["C. Wu"], "venue": "The Annals of Statistics, vol. 11, no. 1, pp. 95\u2013103, 1983.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "The EM Algorithm and Extensions (Wiley Series in Probability and Statistics), 2nd ed", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Choosing starting values for the EM algorithm for getting the highest likelihood in multivariate Gaussian mixture models", "author": ["C. Biernacki", "G. Celeux", "G. Govaert"], "venue": "Comput. Stat. Data Anal., vol. 41, no. 3-4, pp. 561\u2013575, Jan. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Initializing the EM algorithm in Gaussian mixture models with an unknown number of components", "author": ["V. Melnykov", "I. Melnykov"], "venue": "Computational Statistics & Data Analysis, Nov. 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Initializing Partition-Optimization Algorithms", "author": ["R. Maitra"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 6, no. 1, pp. 144\u2013157, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An Experimental Comparison of Several Clustering and Initialization Methods", "author": ["M. Meil\u0103", "D. Heckerman"], "venue": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, Inc., San Francisco, CA, 1998, pp. 386\u2013395.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient greedy learning of Gaussian mixture models", "author": ["J.J. Verbeek", "N. Vlassis", "B. Kr\u00f6se"], "venue": "Neural computation, vol. 15, no. 2, pp. 469\u2013 485, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Accelerated quantification of Bayesian networks with incomplete data", "author": ["B. Thiesson"], "venue": "University of Aalborg, Institute for Electronic Systems, Department of Mathematics and Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Initialization of Iterative Refinement Clustering Algorithms.", "author": ["U.M. Fayyad", "C. Reina", "P.S. Bradley"], "venue": "in KDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Initializing EM using the properties of its trajectories in Gaussian mixtures.", "author": ["C. Biernacki"], "venue": "Statistics and Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA, N. Bansal, K. Pruhs, and C. Stein, Eds. SIAM, 2007, pp. 1027\u20131035.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "On the initialization of dynamic models for speech features", "author": ["A. Krueger", "V. Leutnant", "R. Haeb-Umbach", "M. Ackermann", "J. Bloemer"], "venue": "ITG- Fachbericht-Sprachkommunikation 2010, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A New Method for Random Initialization of the EM Algorithm for Multivariate Gaussian Mixture Learning", "author": ["W. Kwedlo"], "venue": "Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013. Springer International Publishing, 2013, pp. 81\u201390.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science, vol. 38, pp. 293\u2013306, 1985.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1985}, {"title": "Learning Mixtures of Gaussians", "author": ["S. Dasgupta"], "venue": "FOCS. IEEE Computer Society, 1999, pp. 634\u2013644.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Evaluation of Clusterings \u2013 Metrics and Visual Support", "author": ["E. Achtert", "S. Goldhofer", "H.-P. Kriegel", "E. Schubert", "A. Zimek"], "venue": "Data Engineering, International Conference on, vol. 0, pp. 1285\u20131288, 2012. [Online]. Available: http://elki.dbs.ifi.lmu.de/wiki/DataSets/MultiView", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "The Amsterdam Library of Object Images", "author": ["J.M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders"], "venue": "International Journal of Computer Vision, vol. 6, no. 1, pp. 103\u2013112, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Evaluation of Multiple Clustering Solutions", "author": ["H.-P. Kriegel", "E. Schubert", "A. Zimek"], "venue": "Proc. ECML PKDD Workshop MultiClust, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository", "author": ["D.N.A. Asuncion"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/\u223cmlearn/MLRepository.html", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "A widely used approach for the latter problem is the ExpectationMaximization (EM) algorithm [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "[2], [3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2], [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]\u2013 [8]), which we will also refer to as \u201crandom initializations\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5]\u2013 [8]), which we will also refer to as \u201crandom initializations\u201d.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "For instance, in [6]\u2013[8] HAC (with Ward\u2019s criterion, average linkage, and a model-based distance measure, re-", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "For instance, in [6]\u2013[8] HAC (with Ward\u2019s criterion, average linkage, and a model-based distance measure, re-", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Except for [7], [6] and [8] report that this", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Except for [7], [6] and [8] report that this", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Except for [7], [6] and [8] report that this", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Another approach that makes use of HAC is the multistage approach proposed in [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "[6] a density based approach is proposed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In [9] a greedy algorithm ar X iv :1 31 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": ", in [7], [10]\u2013[12].", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": ", in [7], [10]\u2013[12].", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": ", in [7], [10]\u2013[12].", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Namely, we use the K-means++ algorithm [13], the Gonzalez algorithm, and a simple uniform initialization.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": ", in [14] these GMMs are used for speech recognition).", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Some initial research in this area has been done in [15], where a modification of the Gonzalez algorithm is considered.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "presented in [15] and an algorithm based on HAC.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "called K-means++ initialization [13], which is intended to be used for the K-means clustering algorithm.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "The second method is the Gonzalez algorithm [16], which is a 2-approximation algorithm for the discrete radius K-clustering problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Gonzalez executes the Gonzales algorithm, Kmeans++ executes the K-means++ algorithm [13], and Uniform draws K means uniformly at random from the given data set.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "That is, for some fixed S \u2286 X , we define for all x \u2208 S and \u03b1 \u2208 [0, 1]", "startOffset": 64, "endOffset": 70}, {"referenceID": 14, "context": "In our experiments, we compare them with another adaption of the Gonzalez algorithm presented in [15], which we refer to as", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "1 Our first attempts consisted in just adding a new component to the old mixture model, for instance, by choosing the covariances as in [17] (i.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "Adaptive ( X \u2282 R,K \u2208 N, \u03b1 \u2208 [0, 1] )", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": ",K uniformly at random from [0, 1] and normalize them by \u2211K k=1 wk.", "startOffset": 28, "endOffset": 34}, {"referenceID": 14, "context": "Adaption of the Gonzalez algorithm according to [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Generation of a random covariance matrix according to [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The first data set is provided by the ELKI project [19] and based on the Amsterdam Library of Object Images (ALOI) [20], which consists of 110 250 pictures of 1 000 small objects (taken under various conditions).", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The first data set is provided by the ELKI project [19] and based on the Amsterdam Library of Object Images (ALOI) [20], which consists of 110 250 pictures of 1 000 small objects (taken under various conditions).", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "[21]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The third one is the Forest Covertype data set which contains 581 012 data points and is available from the UCI Machine Learning Library [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "The fourth data set is the Spambase data set which is also provided by the UCI Machine Learning Library [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Following [17], we define the separation c\u03b8 of a Gaussian mixture model \u03b8 by", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "5, 1, 2 uniform different [1, 10]", "startOffset": 26, "endOffset": 33}, {"referenceID": 9, "context": "5, 1, 2 uniform different [1, 10]", "startOffset": 26, "endOffset": 33}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "In this paper, we consider simple and fast approaches to initialize the Expectation-Maximization algorithm (EM) for multivariate Gaussian mixture models. We present new initialization methods based on the well-known K-means++ algorithm and the Gonzalez algorithm. These methods close the gap between simple uniform initialization techniques and complex methods, that have been specifically designed for Gaussian mixture models and depend on the right choice of hyperparameters. In our evaluation we compare our methods with a commonly used random initialization method, an approach based on agglomerative hierarchical clustering, and a known, plain adaption of the Gonzalez algorithm. Our results indicate that algorithms based on K-means++ outperform the other methods.", "creator": "LaTeX with hyperref package"}}}