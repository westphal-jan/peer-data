{"id": "1312.5985", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "histories": [["v1", "Fri, 20 Dec 2013 15:21:15 GMT  (57kb,D)", "https://arxiv.org/abs/1312.5985v1", "Submitted as part of the open review process for ICLR'14. The paper contains 9 pages, 3 figures, 4 tables"], ["v2", "Tue, 18 Feb 2014 15:27:24 GMT  (55kb,D)", "http://arxiv.org/abs/1312.5985v2", "Submitted as part of the open review process for ICLR'14. The paper contains 10 pages, 3 figures, 4 tables"]], "COMMENTS": "Submitted as part of the open review process for ICLR'14. The paper contains 9 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["tamara polajnar", "luana fagarasan", "stephen clark"], "accepted": false, "id": "1312.5985"}, "pdf": {"name": "1312.5985.pdf", "metadata": {"source": "CRF", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "authors": ["Tamara Polajnar"], "emails": ["first.last@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to help themselves, to help themselves, to manoeuvre themselves into a situation in which they are able, in which they are able, in which they are able to manoeuvre themselves into a situation in which they are not able, in which they are able, in which they are able to unfold, in which they are able, and in which they are able to put themselves in a situation in which they are able, in which they are able to act themselves."}, {"heading": "2 Syntactic Types to Tensors", "text": "The syntactic type of a transitive verb in English is (S\\ NP) / NP (using Steedman [29] notation), which means that a transitive verb is a function that takes a NP argument to the right, a NP argument to the left, and leads to a sentence S. Such slash categories are complex categories; S and NP are basic or atomic categories. The interpretation of such categories under Coecke et al. is simple. Firstly, there is a corresponding vector space for each atomic category; in this case, the slash S and the noun space N.2 Hence are the meanings of a noun or noun sentence, for example, people who have a vector in noun space. \u2212 To get the meaning of a transitive verb, each slash is replaced by a tensitive product."}, {"heading": "3 Verb and Sentence Representation", "text": "As described above, in this paper we have decided to focus on a two-dimensional \"plausibility space\" for the meanings of the sentences. One way to think of this space is to simply extend the truth values from the traditional truth theory account to a real environment. We also focus on the plausibility of transitive verbal sentences with the simple subject verb object (SVO) grammatical structure, e.g. humans eat fish (as in Figure 1). These sentences were generated automatically by finding certain transitive verbs in a dependent corpus and extract the headwords from the subject and the object (see Section 4). The nouns have atomic syntactic types and are represented by distributional semantic vectors built using standard techniques."}, {"heading": "3.1 Tensor learning", "text": "Following [20], we learn the tensor values as parameters (V) of a regression algorithm. To represent this space as a distribution over two classes, we apply a sigmoid (\u03c3) to limit the output to the [0,1] range and the Softmax activation function (g) to balance the class probabilities.The full 3We do not concern ourselves too much with the philosophical interpretation of these plausibility values; rather, we consider the plausibility set space as a useful clinical test bed for the tensor-based semantic framework. The parameter set for which we need to optimize is B = {V, \u0432}, where we are the Softmax parameters for the two classes. For each verb, we optimize the KL divergence L between the training particles ti and classifier prediction using: O (B) = N \u00b2 i (ti, g) (V) (n), the object (T) (II)."}, {"heading": "3.2 Baseline", "text": "The baseline is a simple corpus-based approach to generating a matrix from an average of the subject's Kronecker products and object vectors from the positively designated subset of training data [18] for each verb. Intuition is that the matrix for each verb represents an average of the paired contextual features of a typical subject and object (as extracted from instances of the verb).To determine the plausibility of a new subject-object pair for a given verb, we calculate the subject's Kronecker product and object vectors for that pair (is human, is food) and compare the resulting matrix with the average verb matrix using cosine similarity. Intuitively, the average verb prediction can be considered as what the verb expects."}, {"heading": "4 Data", "text": "In order to form a classifier for each verb, a dataset with positive and negative examples is required. While we can consider subject-verb-object triples, which occur naturally in corpus data, as positive, a technique for generating pseudo-negative examples is required, which is described below."}, {"heading": "4.1 Training examples", "text": "To generate training data, we used two large corpora: the Google Syntactic N-gram (GSN) [16] and the Wikipedia October 2013 Dump. Wikipedia Corpus consists of the textual content symbolized by Stanford NLP Tools5 and analyzed and lemmatized with the C & C parser and the Morpha Lemmatiser [8, 22]. We first chose transitive verbs with different concreteness values [5] and frequencies to obtain a variety of verb types. Then, the positive SVO examples were extracted from the GSN Corpus. Specifically, we extracted all unique syntactic trigrams of the form nsubj ROOT dobj, with the root of the phrase being one of our target verbs. We lemmatized the words using the NLTK6 Lemmatizer and filtered these examples to keep only those that also occur in Wikipedia."}, {"heading": "4.2 Noun representation", "text": "In this paper we use sentence paths to define context vector vectors that we appropriate in the context vector that we capture in the context vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "5 Experiments", "text": "We performed three experiments: the first used all available training examples in 5 repetitions of a double cross-validation experiment (5x2cv) to evaluate the peak performance for each of the verbs (Table 3); the performance of verbs with many subject-object pairs was limited to 4,000 instances (the 2,000 most common positive pairs and 2,000 pseudo-negative); we compared the performance of the learning methods baseline and tensor at 20- and 40-dimensional vectors; 8The performance was evaluated by the range below the ROC (AUC) and the measure F1 (based on precision and recall over the plausible class); the AUC evaluated whether a method ranks positive examples over negative, regardless of the class intersection value; F1 shows how accurately a method assigns the correct class designation; since the baseline ad hoc class designation is used, AUC is the fairer measurement (AUC); in the second experiment, we repeated the points for each 5x2v data set of each training session with four small sets of AUC)."}, {"heading": "5.1 Analysis", "text": "In general, the tenor learning algorithm learns more effectively and with less variance than the baselines, especially with the smaller dimensions of the vectors. the FS scores indicate that learning is necessary for an accurate classification, while the Baseline AUC results show only positive examples. Analysis of the errors shows that the baseline vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vrr vector vector vector vector vector vrr vector vector vector vector vector vector vrr vector vector vector vector vector vrr"}, {"heading": "6 Conclusion", "text": "In this paper, we have examined the learning of 3rd order tensors to illustrate the semantics of transitive verbs with a two-dimensional \"plausibility\" placement space. There are obvious links to the large literature on selective preference learning (see e.g. [27] for a recent paper); however, our goal is not to contribute to this literature, but to use a selective preference task as the first corpus-driven test of the type-driven tensor-based semantic framework of Coecke et al., and to introduce this framework into the machine learning community. We have shown that standard techniques from neural network literature can be effectively applied to learning 3rd order tensor from corpus data, with our results showing positive trends compared to a competitive corpus-based baseline. There is much to be done to expand the techniques in this paper, both in terms of a higher-dimensional, potentially structured order, as well as a third-order."}, {"heading": "Acknowledgments", "text": "Tamara Polajnar is supported by the ERC Starting Grant DisCoTex (306920), Stephen Clark is supported by the ERC Starting Grant DisCoTex and the EPSRC Grant EP / I037512 / 1. Luana Fa-ga-ras-an is supported by the EPSRC Doctoral Training Partnership Award. Thanks to Laura Rimell and Jean Maillard for helpful discussions."}], "references": [{"title": "Matlab tensor toolbox version 2.5", "author": ["Brett W. Bader", "Tamara G. Kolda"], "venue": "Available online,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Frege in space: A program for compositional distributional semantics (to appear)", "author": ["M. Baroni", "R. Bernardi", "R. Zamparelli"], "venue": "Linguistic Issues in Language Technologies,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N. Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Improving the use of pseudo-words for evaluating selectional preferences", "author": ["Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the 48th Meeting of the ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Type-driven syntax and semantics for composing meaning vectors", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Wide-coverage efficient statistical parsing with CCG and log-linear models", "author": ["Stephen Clark", "James R. Curran"], "venue": "Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis (Lambek Festschrift),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Multi-way tensor factorization for unsupervised lexical acquisition", "author": ["Tim Van de Cruys", "Laura Rimell", "Thierry Poibeau", "Anna Korhonen"], "venue": "In Proceedings of COLING", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "A dataset of syntactic-ngrams over time from a very large corpus of English books", "author": ["Yoav Goldberg", "Jon Orwant"], "venue": "In Second Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "author": ["Edward Grefenstette"], "venue": "PhD thesis, University of Oxford,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank", "author": ["Julia Hockenmaier", "Mark Steedman"], "venue": "Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Vector space semantic parsing: A framework for compositional vector space models", "author": ["Jayant Krishnamurthy", "Tom M Mitchell"], "venue": "In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["Wee Sun Lee", "Bing Liu"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Applied morphological processing of English", "author": ["Guido Minnen", "John Carroll", "Darren Pearce"], "venue": "Natural Language Engineering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark"], "venue": "In 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Latent variable models of selectional preference", "author": ["Diarmuid O Seaghdha"], "venue": "In Proceedings of ACL 2010, Uppsala,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "The Syntactic Process", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Cost-conscious comparison of supervised learning algorithms over multiple data sets", "author": ["Ayd\u0131n Ula\u015f", "Olcay Taner Y\u0131ld\u0131z", "Ethem Alpayd\u0131n"], "venue": "Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 2, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 9, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 16, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 8, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 26, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 6, "context": "An emerging subfield of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7].", "startOffset": 165, "endOffset": 190}, {"referenceID": 24, "context": "The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13].", "startOffset": 130, "endOffset": 138}, {"referenceID": 29, "context": "The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13].", "startOffset": 130, "endOffset": 138}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2].", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 6, "context": "Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10].", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10].", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "This same insight lies behind the work of Baroni and Zamparelli [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "In this paper we consider a simple sentence space: the \u201cplausibility space\u201d described by Clark [7], represented here as a probability distribution (and hence having only 2 dimensions).", "startOffset": 95, "endOffset": 98}, {"referenceID": 26, "context": "Current methods, for example the work of Socher [28], typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 28, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 10, "context": "[25, 30, 11]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "The syntactic type of a transitive verb in English is (S\\NP)/NP (using Steedman [29] notation), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S .", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Meanings combine using tensor contraction, which can be thought of as a multi-linear generalisation of matrix multiplication [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "In practice, using for example the wide-coverage grammar from CCGbank [19], there will be many types with more than 3 slashes, with corresponding higher-order tensors.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "For example, a common In practice, for example using the CCG parser of Clark and Curran [8], there will be additional atomic categories, such as PP , but not many more.", "startOffset": 88, "endOffset": 91}, {"referenceID": 29, "context": "The nouns have atomic syntactic types and are represented by distributional semantic vectors, built using standard techniques [31], while the verb is a multi-linear map that takes in two nouns and outputs values in the plausibility space.", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "We add some additional processing to the tensor network, following standard practice in neural networks and following [20], by passing the output values though a non-linear sigmoid function, and then creating a probability distribution over over two classes, plausible (>) and implausible (\u22a5), using a softmax function.", "startOffset": 118, "endOffset": 122}, {"referenceID": 18, "context": "This method was introduced in [20], but only implemented as a proof-of-concept with vectors of length 2 and small, manually created datasets based on propositional logic examples.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "2) that improves low-dimensional singular value decomposition (SVD) [12], and thus enables us to effectively limit the number of parameters while learning from corpus-sized data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "As a baseline we adapted a method from [18], where the verb is represented as the average of the Kronecker products of the subject and object vectors from the positive training data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "Following [20], we learn the tensor values as parameters (V) of a regression algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "To represent this space as a distribution over two classes (>,\u22a5) we apply a sigmoid (\u03c3) to restrict the output to the [0,1] range and the softmax activation function (g) to balance the class probabilities.", "startOffset": 118, "endOffset": 123}, {"referenceID": 12, "context": "The derivatives are obtained via the chain rule with respect to each set of parameters and gradient descent is performed using the Adagrad algorithm [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Tensor contraction is implemented using the Matlab Tensor Toolbox [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "The baseline is a simple corpus-driven approach of generating a matrix from an average of Kronecker products of the subject and object vectors from the positively labelled subset of the training data [18], for each verb.", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "As well as being an intuitive corpus-based method for representing the meaning of a transitive verb, this method has also performed well experimentally [18], and hence we consider it to be a competitive baseline.", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "In order to generate training data we made use of two large corpora: the Google Syntactic N-grams (GSN) [16] and the Wikipedia October 2013 dump.", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22].", "startOffset": 166, "endOffset": 173}, {"referenceID": 20, "context": "The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22].", "startOffset": 166, "endOffset": 173}, {"referenceID": 4, "context": "We first chose transitive verbs with different concreteness scores [5] and frequencies, in order to obtain a variety of verb types.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "selection preferences literature [6].", "startOffset": 33, "endOffset": 36}, {"referenceID": 29, "context": "Distributional semantic models [31] encode word meaning in a vector format by counting cooccurrences with other words within a specified context, which can be defined in many ways, for example as a whole document, an N-word window, or a grammatical relation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "The value N is chosen by testing on the development subset of the MEN dataset (MENdev), a standard dataset for evaluating the quality of semantic vectors [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 29, "context": "Applying SVD to such a matrix is a standard technique for removing noise and uncovering the latent semantic dimensions in the data, and has been reported to improve performance on a number of semantic similarity tasks [31].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "Together these two simple techniques [24] markedly improve the performance of SVD on smaller dimensions (K) on the MENdev set (see Figure 2), and enable us to train the verb tensors using 20-dimensional noun vectors.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "On an older, highly reported dataset of 353 word pairs [15] our vectors achieve the Spearman correlation of 0.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Bold indicates that the method performs better, and \u2020 that the result is significant according to the 5x2cv F-test [32].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "TL also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or treating negative data as possibly positive [21].", "startOffset": 262, "endOffset": 266}, {"referenceID": 25, "context": "[27] for a recent paper); however, our goal is not to contribute to that literature, but rather to use a selectional preference task as a first corpus-driven test of the type-driven tensor-based semantic framework of Coecke et al.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "creator": "LaTeX with hyperref package"}}}