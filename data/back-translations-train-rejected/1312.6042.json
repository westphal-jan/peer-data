{"id": "1312.6042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Learning States Representations in POMDP", "abstract": "We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned.", "histories": [["v1", "Fri, 20 Dec 2013 17:03:50 GMT  (61kb,D)", "https://arxiv.org/abs/1312.6042v1", null], ["v2", "Sat, 8 Feb 2014 10:10:53 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v2", "4 pages; added references in section 4"], ["v3", "Tue, 11 Mar 2014 14:12:36 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v3", "4 pages"], ["v4", "Tue, 17 Jun 2014 10:24:51 GMT  (62kb,D)", "http://arxiv.org/abs/1312.6042v4", "4 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gabriella contardo", "ludovic denoyer", "thierry artieres", "patrick gallinari"], "accepted": false, "id": "1312.6042"}, "pdf": {"name": "1312.6042.pdf", "metadata": {"source": "META", "title": "Learning States Representations in POMDP", "authors": ["Gabriella Contardo", "Ludovic Denoyer"], "emails": ["gabriella.contardo@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lip6.fr", "patrick.gallinari@lip6.fr"], "sections": [{"heading": "1. Introduction", "text": "We consider a Markov Decision Process (MDP), defined by possible states, as a problem that the system cannot move in a split second. Although possible actions constitute an \"A,\" transition between states \"P\" (s \"| s,\" a) and reward function \"r\" (s, \"a). Standard reinforcement learning approaches are based on the assumption that the input provided to the model (i.e. the state of the system) contains enough information to learn an optimal policy (i.e. a function\" \u03c0 \"(s). A, which selects which action to take in a state to maximize the expected discounted reward). In the face of assumed amplification problems, the input consists of a feature vector - called\" observation \"- which assumes to fully characterize the current state of the process, thereby enabling optimal choice of action. However, in real life applications this assumption is unrealistic, where the view of the observation is limited only by the observation of the state."}, {"heading": "2. Model", "text": "The latent representation of a state is referred to as zt-Rn. Finding an optimal policy at the level of representation, \u03c0 (zt), can be done using standard reinforcement learning techniques. Our model is based on two ideas: (i) The latent representation of a state Xiv: 131 2,60 42v4 [cs.LG] 1 7Ju n20 14sentence zt of a state should contain enough information to calculate the corresponding observation ot. Therefore, we consider a decoder function d\u043a: Rn \u2192 Rm, which aims to calculate the observation in the light of the representation of the current state. (ii) The representation of a state st should contain information about the dynamics of the system enabling a dynamic: Rn \u2192 Rm, which aims to calculate the dynamics of the future state."}, {"heading": "2.1. Unsupervised Learning", "text": "Considering a sequence of observations and actions (o1, a1,..., ot, at), we define the following loss function: L (z, \u03b8, \u03b3) = \u2211 t-dec (d\u03b8 (zt), ot) + \u2211 t-dyn (m\u03b3 (zt, at), zt + 1) (1), where \"dec\" measures the quality of the decoder, \"dyn\" measures the quality of the dynamic model and \"z\" is the sequence of latent representations, the value of which directly reflects the ability of z, \u03b8 and \u03b3 to explain the observations. Considering a number of Q trajectories, a learning conclusion emerges: z *, \u03b8 *, \u03b3 = argmin \u00b2 q [1; Q] L (zq, \u03b8, \u03b3) (2), where \"zq\" is the sequence of representations calculated for the path number q.Learning, both the optimal decoder function habi and the dynamic function associated with the tractories Q *."}, {"heading": "2.2. Inferring new representations", "text": "The next question is the calculation of plots for new trajectories. Consider that at the time t, due to a sequence of observations (o1, a1,..., ot, at), the first t plots z1 to zt have already been calculated. We propose two methods to calculate the plots zt + 1:"}, {"heading": "PO FLat 2 0.86", "text": "Exact Conclusion In view of a new observation ot + 1, the first method consists in solving the following processes: z * = argminz1,.., zt + 1L (z, \u03b8 *) (3) This property of our model can be interpreted as a thought process, since each new information collected by the system causes it to revise the entire representation sequence. The disadvantage of such an inference scheme is its high complexity: finding a new representation can be slow and the optimization must be performed at each step. Fast Conclusion The second method is to use the dynamic function to directly calculate the next representation by zt + 1 = m\u03b3 (zt, at). In this case, the new representation can be generated directly from zt without the need for observation ot + 1. The advantages are twofold: First, the calculation of mhoch (zt, at), which adapts the dynamics of this procedure for particularly slow acquisition processes to the Monte simulation."}, {"heading": "3. Experiments", "text": "iDe rf\u00fc ide rf\u00fc die f \u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc"}, {"heading": "4. Related work", "text": "Efficient approaches have been proposed to extract high-level representations using deep learning (Bengio, 2009), but few studies have suggested an extension to address sequential processes; a formal analysis has been proposed in (Ryabko, 2013). Models that concern partially observable sequential processes have been proposed in the context of task control problems, for example (Schmidhuber, 1990) and (Cuccu et al., 2011) present models that use relapsing neural networks (RNN) to learn a controller for a given task. In these approaches, informative representations are constructed by the RNN, but these representations are driven by the task to be solved. Some uncontrolled approaches have recently been proposed, in which case a model of representational learning is learned through the observations, without the need to define a reward function. Politics is then learned from these representations using classical RL algorithms."}, {"heading": "5. Conclusion", "text": "We proposed a novel approach to learning representations of sequential processes when only partial observations are made. It is unsupervised and transductive, and can be used both to infer new representations and as a simulator to predict what may happen in the future. Experiments in more realistic areas are currently being investigated."}, {"heading": "Acknowledgements", "text": "This work was carried out within the framework of the Labex SMART, supported by the French public funds managed by the ANR under the Investissements d'Avenir programme under reference ANR-11-LABX-65 and the Lampada project ANR-09-EMER-007."}], "references": [{"title": "Intrinsically motivated neuroevolution for vision-based reinforcement learning", "author": ["Cuccu", "Giuseppe", "Luciw", "Matthew", "Schmidhuber", "J\u00fcrgen", "Gomez", "Faustino"], "venue": "In Development and Learning (ICDL),", "citeRegEx": "Cuccu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cuccu et al\\.", "year": 2011}, {"title": "Rollout sampling approximate policy iteration", "author": ["Dimitrakakis", "Christos", "Lagoudakis", "Michail G"], "venue": "Machine Learning,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2008}, {"title": "Solving partially observable reinforcement learning problems with recurrent neural networks", "author": ["Duell", "Siegmund", "Udluft", "Steffen", "Sterzing", "Volkmar"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Duell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duell et al\\.", "year": 2012}, {"title": "Sequential constant size compressors for reinforcement learning", "author": ["Gissl\u00e9n", "Linus", "Luciw", "Matt", "Graziano", "Vincent", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Gissl\u00e9n et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gissl\u00e9n et al\\.", "year": 2011}, {"title": "Unsupervised model-free representation learning", "author": ["Ryabko", "Daniil"], "venue": "arXiv preprint arXiv:1304.4806,", "citeRegEx": "Ryabko and Daniil.,? \\Q2013\\E", "shortCiteRegEx": "Ryabko and Daniil.", "year": 2013}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "For example, (Schmidhuber, 1990) and (Cuccu et al., 2011) present models using recurrent neural networks (RNN) to learn a controller for a given task.", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "For instance, (Gissl\u00e9n et al., 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 2, "context": ", 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al., 2012) present an extension of RNN for unsupervised learning.", "startOffset": 108, "endOffset": 128}], "year": 2014, "abstractText": "We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned.", "creator": "LaTeX with hyperref package"}}}