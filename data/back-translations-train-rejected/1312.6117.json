{"id": "1312.6117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Comparison three methods of clustering: k-means, spectral clustering and hierarchical clustering", "abstract": "Comparison of three kind of the clustering and find cost function and loss function and calculate them. Error rate of the clustering methods and how to calculate the error percentage always be one on the important factor for evaluating the clustering methods, so this paper introduce one way to calculate the error rate of clustering methods. Clustering algorithms can be divided into several categories including partitioning clustering algorithms, hierarchical algorithms and density based algorithms. Generally speaking we should compare clustering algorithms by Scalability, Ability to work with different attribute, Clusters formed by conventional, Having minimal knowledge of the computer to recognize the input parameters, Classes for dealing with noise and extra deposition that same error rate for clustering a new data, Thus, there is no effect on the input data, different dimensions of high levels, K-means is one of the simplest approach to clustering that clustering is an unsupervised problem.", "histories": [["v1", "Thu, 19 Dec 2013 21:45:10 GMT  (896kb)", "http://arxiv.org/abs/1312.6117v1", "all of figure were create by myself and i want to expand this idea"], ["v2", "Thu, 13 Nov 2014 05:52:05 GMT  (0kb,I)", "http://arxiv.org/abs/1312.6117v2", "This paper has been withdrawn by the author due to improve add more results"]], "COMMENTS": "all of figure were create by myself and i want to expand this idea", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kamran kowsari"], "accepted": false, "id": "1312.6117"}, "pdf": {"name": "1312.6117.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kamran Kowsari"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able, that we are able, and that we are able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able."}, {"heading": "K-Means", "text": "K Mean Clustering is one of the simplest methods of clustering, where the data D = (1.........) is given in d-dimensional vectors. The method is to identify groups of data points and assign each point to one of the groups. What is a good clustering? There is no universal approach, but we will try the K Mean Approach. But there are many problems we cannot calculate, although the loss function is available. A measure of how good a cluster is would be the sum of distances to the center. Therefore, K Means wants to minimize this \u00a3(amount) by selecting \u00b5's and a, but it is difficult to do analytically because a's binary tasks. So K Mean Algorithm tries to solve minimization (a kind of greedy algorithm) iteratively."}, {"heading": "Minimize \u00a3 with respect to a\u2019s and \u00b5\u2019s by:", "text": "1- Initialize the cluster as you like. \u2212 2- Select the optimal mapping \"a\" = = for all available centerings. \u2212 3- Select the optimal \"a\" for fixed \"a.\" \u2212 4- Repeat (iterative) 2 & 3 to convergence."}, {"heading": "Spectral Clustering", "text": "Although K means are useful and several projects have been carried out using this method, K means actually has a lot of problems like this algorithm can cluster very well for all datasets, so a novel cluster method was needed to find a better cluster with a lower error rate \"In this essay we talk about how to find the error rate with K value.\" Spectral clustering solves this problem by using affinity matrix, which is a very large matrix, and this matrix finds the best relationship between the two different data points. The time complexity of the spectral cluster is (3), but in Kmeans the time complexity is not fixable, which is on (2) to (2), which is one of the most important problems of the K mean; however, sometimes K means for a simpler K means project implementation is simpler and also more sensitive. If you look at Figure 1: you can see that it has a lot of error rate in the evaluation part, which we consider to be the most common one of our data sets, even if we consider the result of the 12."}, {"heading": "Hierarchical Clustering", "text": "The other problem with K-mean and spectral cluster algorithms is that they cannot figure out how many clusters we have, and another thing is that this method cannot cluster with different K-values. It means that the result cannot be clustered by two different K-values; for example, in many real projects the K-value is not available, so if we need to cluster by more than two K-values, the result comes in two or more numbers, so we do not have the result on a specific number. The time complexity of hierarchical clustering for two methods is that for agglomerative clustering (3) and for divisive clustering with an exhaustive search (2) [17] The same data in Figure 2The left image is original data coming from the George Washington University School of Medicine and Health Sciences, the MGPC project (McCormick Genomic and ProteomicCenter), and the right image is post-cluster of sample data and optimized data. \""}, {"heading": "Evaluation", "text": "One of the most sophisticated factors for clustering algorithms and evaluated is the average error rate for each clustering method. But one of the important factors for calculating the error rate is that the result of labeling each clustering method may be completely different from the result of our test labels; for example, K value is equal to 2 and clustering method can find two different labels, so the error rate of this algorithm can be more than 50%. In the result of one data set, the result of the record was 99.98% for which K is equal to 2. In clustering in two clusters, the error rate cannot be more than 50%, so the real record of this data set is equal to 0.02%, because all clustering algorithms guess the labels, and those that cannot find exact labels, so the real error rate is equal to the minimum error rate by different labels of K.For K = 2: Error Rate = 2: Error Rate {> Test- Labels are available for only two, for K | - Labels."}], "references": [{"title": "kmeans++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACMSIAM symposium on Discrete algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Spectral clustering with perturbed data", "author": ["L. Huang", "D. Yan", "M.I. Jordan", "N. Taft"], "venue": "In Advances in Neural Information Processing Systems (NIPS), December", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Minkowski metric, feature weighting and anomalous cluster initializing in K-Means clustering", "author": ["R.C. Amorim", "B Mirkin"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning spectral clustering", "author": ["F. Bach", "M. Jordan"], "venue": "In Proc. of NIPS-16. MIT Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering with Bregman divergence", "author": ["Banerjee", "S. Merugu", "I. Dhillon", "J. Ghosh"], "venue": "Proceeding of SIAM Data Mining conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Clustering Large Datasets in Arbitrary Metric Spaces,", "author": ["V. Ganti"], "venue": "Proc. 15th Int\u2019l Conf. Data Eng., IEEE CS Press, Los Alamitos, Calif.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Efficient and Effective Clustering Method for Spatial Data Mining,", "author": ["R. Ng", "J. Han"], "venue": "Proc. 20th Int\u2019l Conf. Very Large Data Bases,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Fast approximate spectral clustering. InProceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09)", "author": ["Donghui Yan", "Ling Huang", "Michael I. Jordan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Spectral grouping using the Nystr \u0308om method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Quantization", "author": ["R.M. Gray", "D.L. Neuhoff"], "venue": "IEEE Transactions of Information Theory, 44(6):2325\u20132383,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "and A", "author": ["S. Gunter", "N.N. Schraudolph"], "venue": "V. N. Vishwanathan.Fast iterative kernel principal component analysis. Journal of Machine Learning Research, 8:1893\u20131918,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "SLINK: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "The Computer Journal (British Computer Society)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}], "referenceMentions": [{"referenceID": 7, "context": "\u201d[12].", "startOffset": 1, "endOffset": 5}, {"referenceID": 11, "context": "Time complexity of Hierarchical Clustering for two method is that for agglomerative clustering is O(n) and for Divisive clustering with an exhaustive search is O(2) [17] Figure 4 : Simplest approaches to Spectral Clustering", "startOffset": 165, "endOffset": 169}], "year": 2013, "abstractText": "Comparison of three kind of the clustering and find cost function and loss function and calculate them. Error rate of the clustering methods and how to calculate the error percentage always be one on the important factor for evaluating the clustering methods, so this paper introduce one way to calculate the error rate of clustering methods. Clustering algorithms can be divided into several categories including partitioning clustering algorithms, hierarchical algorithms and density based algorithms. Generally speaking we should compare clustering algorithms by: 1. Scalability 2. Ability to work with different attribute 3. Clusters formed by conventional 4. Having minimal knowledge of the computer to recognize the input parameters. 5. Classes for dealing with noise and extra deposition(same error rate for clustering a new data) 6. Thus, there is no effect on the input data. 7. different dimensions of high levels K-means is one of the simplest approach to clustering (clustering is an unsupervised problem). \u201cThe difference between k-means and k-means++ on the real-world datasets was also substantial.\u201d[3]. In kmeans++ achievement is bigger than 10% accuracy improvement over k-means, and it often performed much better, on the other hand, k-means++ achieved potentials 20 to 1000 times smaller than those achieved by standard k-means. Hierarchical clustering: as we know every time we have to know the value of the k for k-means, but for Hierarchical clustering we don\u2019t need the value of k that is the number of the data cluster. In this project we calculate Hierarchical clustering with the k-center cost function and compare this kind of the clustering with K-means. This kind of clustering is really useful for data that K value is not available and result of output is many cluster in one figure like DNA sequences, RNA sequences etc. which the number of the cluster is not available for specialist and case by case is different \u201cFirst, there was apparent genecentered clustering of RNA-exclusive events, and second, high proportion of the putative RNA editing sites seemed to be predominant, often to monozygotic level, of the variant (vs. wt.) nucleotide harboring reads.\u201d[1] Spectral clustering: \u201cthis method is a more powerful and specialized algorithm (compared to K-means), derives its name from spectral analysis of a graph, which is how the data are represented. Each object to be clustered can initially be represented as an ndimensional numeric vector, but the difference with this algorithm is that there must also be some method for performing a comparison between each object and expressing this comparison as a scalar. \u201d [2] Generally speaking, Spectral clustering is more powerful than k-means because it can cluster data point which are not in very close to each other\u2019s. If look at figure 1 it compares this two method. In this project we compare three different algorithms by details. Many clustering algorithms on data sets with low volume) Less than a few percent of big data may well work, but millions have done. Clustering on a sample big data set may therefore be inaccurate. Finally we talk about these three algorithms.", "creator": "Microsoft\u00ae Word 2013"}}}