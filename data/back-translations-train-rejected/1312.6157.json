{"id": "1312.6157", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Distinction between features extracted using deep belief networks", "abstract": "Data representation is an important pre-processing step in many machine learning algorithms. There are a number of methods used for this task such as Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some of the features extracted using automated feature extraction methods may not always be related to a specific machine learning task, in this paper we propose two methods in order to make a distinction between extracted features based on their relevancy to the task. We applied these two methods to a Deep Belief Network trained for a face recognition task.", "histories": [["v1", "Fri, 20 Dec 2013 21:52:08 GMT  (3173kb,D)", "https://arxiv.org/abs/1312.6157v1", "4 pages, 4 figures, ICLR 2014 workshop track"], ["v2", "Thu, 2 Jan 2014 17:06:25 GMT  (3173kb,D)", "http://arxiv.org/abs/1312.6157v2", "4 pages, 4 figures, ICLR 2014 workshop track"]], "COMMENTS": "4 pages, 4 figures, ICLR 2014 workshop track", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["mohammad pezeshki", "sajjad gholami", "ahmad nickabadi"], "accepted": false, "id": "1312.6157"}, "pdf": {"name": "1312.6157.pdf", "metadata": {"source": "CRF", "title": "Distinction between features extracted using Deep Belief Networks", "authors": ["Mohammad Pezeshki", "Sajjad Gholami"], "emails": ["m.pezeshki@aut.ac.ir", "s.gholami@aut.ac.ir", "nickabadi@aut.ac.ir"], "sections": [{"heading": "1 Introduction", "text": "The efficiency of many machine learning algorithms depends on the quality of the characteristics used for the training [1]. There are some automated methods of trait extraction such as Principal Component Analysis and Deep Belief Networks. The outcome of these methods is potentially useful, but there is a problem with these characteristics. It is not always transparent what characteristics will be relevant to a given machine learning task. Consequently, it would be a great task to separate extracted characteristics based on their relevance to the task. One of the most advanced feature extraction tools is Deep Belief Network (DBN). It would be useful if we were able to distinguish between nodes that have different characteristics. For example, in a face detection task, the main object of face and side information is such as noise. If we use a DBN for trait extraction, some nodes in the last layer of the DN are expected to find the face and therefore we present the other side information distinctly in the two."}, {"heading": "2 Deep Belief Networks", "text": "Deep Belief Networks (DBNs) are probable graphical models with multiple hidden layers. DBN is a mixed-directional, undirected model, so all layers are connected to Directedar Xiv: 131 2,61 57v2 [cs.Llinks except the top layer, which forms an undirected two-sided graph. [Figure 1 (a)] Hinton et al. introduced a fast, greedy, layer-by-layer algorithm that can be used to learn DBNs. [2] DBNs can be constructed by setting up several two-sided, undirected graphical models called Restricted Boltzmann Machines (RBMs). RBM is a Boltzmann machine that has only one hidden layer and one visible layer, and also has no visible and hidden connections [3]. A graphical representation of an RBM is shown in Figure 2 (b)."}, {"heading": "3 Proposed Methods", "text": "In this section we will discuss proposed methods for distinguishing between the last layer nodes in a DBN."}, {"heading": "3.1 Method of Variances", "text": "This method is based on the fact that inputs with different aspects (attribute sets) activate different nodes. Trying to perform this process on some inputs with the same aspects should force some nodes to have a significant variation over others. If we feed the network with a group of inputs that consist of only one aspect, the values of some particular nodes would change significantly in the last layer. Consequently, these nodes would have higher variations. Therefore, a statistical criterion such as variance could be a good tool to distinguish between different types of nodes."}, {"heading": "3.2 Method of Relative Activities", "text": "The second method is based on the concept of relative activity. Relative activity is an indicator for detecting the dependence of the nodes of the last layer of a network on the characteristics of the given input. In this technique, the relative activity of the nodes can be calculated by subtracting the values of the nodes of the top layer for two types of input."}, {"heading": "4 Experimental results", "text": "To evaluate the above methods, we train a DBN with 4 hidden layers: 2000-1000-500-100. Training and tests are performed using the following data set, which consists of three parts: 1. Facial images from the CMU PIE Face Database [5], Size: 10,000 2. Handwritten digits from the MNIST dataset [6], Size: 5000 3. Facial images corrupted by numeric images, Size: 5000 Some sample inputs are shown in Figure 2. To discover the nodes presenting the face images, we applied our two suggested methods in the following way:"}, {"heading": "4.1 Using method of Variance", "text": "Now the variance of the nodes is calculated and nodes with a variance greater than 0.1 are considered nodes representing the face images. Again, the DBN is fed another input consisting of numeric images. Likewise, nodes with a variance greater than 0.1 have higher activity compared to other nodes, as shown in Figure 2-a."}, {"heading": "4.2 Using method of Relative activities", "text": "According to Method 2, each mixed image and its clear digit image are assigned to the network. Node difference between the last layer nodes for these two images shows relative activity. Finally, the average relative activity is calculated for all images, and nodes with an average relative activity of more than 0.7 are considered nodes that exhibit facial characteristics. This process is illustrated in Figure 2-b. By applying the methods mentioned in the preceding paragraphs, we discovered face nodes (the nodes that present face images). Now, when a mixed image is fed to the DBN, all nodes are active. To reconstruct the entire face image previously corrupted by a digit is necessary to make number nodes (the nodes that present face images) inactive. Digit nodes would be inactive if a neutral value is entered instead of their current value. These neutral values can only be calculated by averaging the network when these nodes are inserted into the node."}, {"heading": "5 Conclusion", "text": "In this work, we focused on the characteristics of the characteristics extracted with the help of Deep Belief Networks. Obviously, it would be very useful if we were able to distinguish between the characteristics extracted with the help of a DBN. We proposed two new methods to understand which nodes have which characteristics. In our methods, variance and relative activity are two criteria for distinguishing between nodes."}], "references": [{"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A practical guide to training restricted Boltzmann machines. Momentum", "author": ["G. Hinton"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Learning deep generative models (Doctoral dissertation, University of Toronto", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "S. Baker", "M. Bsat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Efficiency of many machine learning algorithms depends on the quality of features used for training [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "[2] DBNs can be constructed by staking multiple bipartite undirected graphical models called Restricted Boltzmann Machines (RBMs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "RBM is a Boltzmann Machine which is restricted to have only one hidden layer and one visible layer and also have no visible-visible and hidden-hidden connections [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "[4]", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Face images from CMU PIE face database [5], size: 10,000 2.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Handwritten digits from MNIST dataset [6], size: 5000 3.", "startOffset": 38, "endOffset": 41}], "year": 2014, "abstractText": "Data representation is an important pre-processing step in many machine learning algorithms. There are a number of methods used for this task such as Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some of the features extracted using automated feature extraction methods may not always be related to a specific machine learning task, in this paper we propose two methods in order to make a distinction between extracted features based on their relevancy to the task. We applied these two methods to a Deep Belief Network trained for a face recognition task.", "creator": "LaTeX with hyperref package"}}}