{"id": "1312.6158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Deep Belief Networks for Image Denoising", "abstract": "Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs' ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.", "histories": [["v1", "Fri, 20 Dec 2013 21:56:38 GMT  (604kb,D)", "https://arxiv.org/abs/1312.6158v1", "ICLR 2014 Conference track"], ["v2", "Thu, 2 Jan 2014 17:04:35 GMT  (604kb,D)", "http://arxiv.org/abs/1312.6158v2", "ICLR 2014 Conference track"]], "COMMENTS": "ICLR 2014 Conference track", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["mohammad ali keyvanrad", "mohammad pezeshki", "mohammad ali homayounpour"], "accepted": false, "id": "1312.6158"}, "pdf": {"name": "1312.6158.pdf", "metadata": {"source": "CRF", "title": "Deep Belief Networks for Image Denoising", "authors": ["Mohammad Ali Keyvanrad", "Mohammad Pezeshki", "Mohammad Mehdi Homayounpour"], "emails": ["keyvanrad@aut.ac.ir", "m.pezeshki@aut.ac.ir", "homayoun@aut.ac.ir"], "sections": [{"heading": "1 Introduction", "text": "Removing noise from the image is an important issue in computer vision, as this step could be the pre-processing step of many other applications. So far, various methods have been proposed to remove noise (denoise) from visual data, many of which focus on Fourier analysis [1], spatial filtering [2], and Wavelet Transform [3], as well as some other methods based on Spare Coding and Dictionary Learning [4]. On the other hand, machine learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN), which have been used in several papers to address this problem [5], describe one of the biggest difficulties in forming such deep networks is that the cost function of such deep architectures gets stuck in poor local optimizations caused by random initialization of weights."}, {"heading": "2 Deep Learning", "text": "Restricted Boltzmann Machines (RBMs) are the building blocks of DBNs. Therefore, in this section we will first briefly describe RBMs and then examine DBNs."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "Boltzmann Machinery (BMs) and Restricted Boltzmann Machinery (RBMs) were introduced in the 1980s < > Expectations are a type of Markov Random Fields (MRF) that have a two-layer structure, one layer is called visible and another is called hidden layer. They are limited to having no visible or hidden connection and connections are between layers. A graphic representation of RBM is shown in Figure 1.A, the common configuration, (v, h) of the visible and hidden units has an energy given by [9]: E (v, h) = \u2212 visible or hidden connections are between layers. A graphic representation of RBM is shown in Figure 1.A, a common configuration, (v, h) of the visible and hidden units has an energy given by [9]: E (v, h) = \u2212 visibleaivi \u2212 j \u2212 hidden year \u2212 hidden layers, which are a hidden layer."}, {"heading": "2.2 Deep Belief Networks", "text": "It is difficult to optimize weights in non-linear deep networks with multiple hidden layers. With good starting weights, gradient descent works well, but finding such starting weights requires an entirely different algorithm that learns a layer of features at once. Hinton et al. [1] introduced a new algorithm to solve the above problem based on training a sequence of RBMs. To construct a DBN, we train sequentially as many RBMs as the number of hidden layers in the DBN, i.e. for a DBN with h hidden layers we need to train h RBMs. These RBMs are placed on top of each other. Figure 2 gives an overview of the basic concept. For the first RBM, which consists of the DBNs input layer and the first hidden layer, the input of RBM is the training set. For the second RM, which uses the square layer, we use the BBN for the first and the BBN for the second layer."}, {"heading": "3 Learning", "text": "To train a DBN for image resolution, the normalized values of an image pixel are used. Unlike the first and last levels of DBN, other layers have binary nodes. The basic idea is to train a DBN so that it can map noisy images to images with less noise or even no noise. It can be implemented by learning the behavior of noise and image content and presenting these behaviors in some nodes on the last level of the network. The network is trained with a collection of noisy and noiseless images. We used a criterion called relative activity to detect nodes. Relative activity of each node is defined as the difference between two values of a particular node resulting from feeding the network using a noiseless image."}, {"heading": "4 Experimental Results", "text": "The MNIST dataset is a set of handwritten digits that consisted of 60,000 images for training and 10,000 images for the test. To model natural noise, we added an additional 0.20 variance of white Gaussian noise (AWGN) to the images. Therefore, our new dataset consisted of 120,000 noisy and clean images along with 10,000 noisy images for the test (the entire test set is noisy). We used a subset of training sessions, each containing 20,000 elements for the training phase and the entire test set for the test phase. According to empirical results, we created a DBN with 4 hidden layers: 784-1000-500-250-100. We trained this network with 200 data packets, each containing 100 frames. After previous discussions, we used relative activity to find nodes in the last layer of the DBN: For all images in the data node, we gave a clean image and then input its associated network as a node."}, {"heading": "5 Conclusion and Future Works", "text": "The proposed method provides a model for learning noise behavior using a Deep Belief Network (DBN) and attempts to divide the behavior of noise and image content into two different groups of nodes. By omitting noise nodes, the network will then be able to reconstruct the input image to produce a noise-free (clean) image. In our thesis, thresholds for noise detection were set manually. Future work will include: 1. Using an automatic technique to determine threshold values for noise detection in the mitigation technique presented in this paper. 2. Applying our mitigation approach to address some other problems in computer vision, speech recognition, etc. These areas will be addressed in future phases of this project."}, {"heading": "6 References", "text": "[1] Brigham, E. O., & Morrow, R. E. (1967). The fast Fourier transform. Spectrum, IEEE, 4 (12), 63-70. [2] Kervrann, C., & Boulanger, J. (2006). Optimal spatial adaptation for patch-based image denoising. Image Processing, IEEE Transactions on, 15 (10), 2866-2878. [3] Pan, Q., Zhang, L., Dai, & Zhang, H. (1999). Two denoising methods by wavelet transform. Signal Processing, IEEE Transactions on, 47 (12), 3401-3406. [4] Elad, M., & Ahargio, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries."}], "references": [{"title": "Optimal spatial adaptation for patch-based image denoising", "author": ["C. Kervrann", "J. Boulanger"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Two denoising methods by wavelet transform", "author": ["Q. Pan", "L. Zhang", "G. Dai", "H. Zhang"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "Farabet", "May"], "venue": "In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on (pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Image denoising and inpainting with deep neural networks.", "author": ["Xie", "Junyuan", "Linli Xu", "Enhong Chen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A practical guide to training restricted Boltzmann machines. Momentum", "author": ["G. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Learning deep generative models (Doctoral dissertation, University of Toronto)", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Also there are some other methods based on Spare Coding and Dictionary Learning [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "[7] proposed a new greedy layer-wise algorithm to tackle this issue and introduced Deep Belief Networks (DBNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "paper [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "A joint configuration, (v, h) of visible and hidden units has an energy given by [9]:", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "The network assigns a probability to every possible pair of a visible and a hidden vector via this energy function [10]:", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Hence, in practice, learning is done by following an approximation to the gradient of a different objective function, called the Contrastive Divergence (CD) [11].", "startOffset": 157, "endOffset": 161}], "year": 2014, "abstractText": "Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs\u2019 ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.", "creator": "LaTeX with hyperref package"}}}