{"id": "1312.6168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "histories": [["v1", "Fri, 20 Dec 2013 22:44:26 GMT  (43kb)", "https://arxiv.org/abs/1312.6168v1", "11 pages, 2 tabels, ICLR-2014"], ["v2", "Wed, 15 Jan 2014 04:49:47 GMT  (43kb)", "http://arxiv.org/abs/1312.6168v2", "11 pages, 2 tables, ICLR-2014"], ["v3", "Tue, 18 Feb 2014 11:22:30 GMT  (45kb)", "http://arxiv.org/abs/1312.6168v3", "12 pages, 2 tables, ICLR-2014"]], "COMMENTS": "11 pages, 2 tabels, ICLR-2014", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["anjan nepal", "alexander yates"], "accepted": false, "id": "1312.6168"}, "pdf": {"name": "1312.6168.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anjan.nepal@temple.edu", "yates@temple.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.61 68v3 [cs.LG] 1 8"}, {"heading": "1 Introduction", "text": "Most existing representation algorithms project and transform the local context of the data to generate their representations, but in many applications, especially in natural language processing (NLP), collective prediction is critical to the success of the application, and this requires consideration of the global structure. We are investigating a representation approach based on factorial Hidden Markov models. Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, which means that they assign a multidimensional vector of the properties of each word, although in our models each latent dimension is discrete."}, {"heading": "2 Previous Work", "text": "There is a long tradition of NLP research on representations, which usually fall into one of four categories: 1) Vector-space models of meaning based on lexical random statistics [45, 50, 44]; 2) Dimensionality reduction techniques for vector-space models [16, 26, 34, 43, 5, 51]; 3) Use of clusters based on distributed similarity [9, 42, 40] as not sparse features [38, 10, 35, 53]; 4) and more recently language models [3, 41] as representations [12, 52, 11, 2], some of which have already provided the state of the art in adaptation tasks [27, 29, 30, 28] and information extraction tasks [1, 19]. Our work combines the power of distributed representations from neural network models with the common conclusions from the HMMMM-based approaches."}, {"heading": "3 The Factorial Hidden Markov Model for Learning Representations", "text": "The Factory Hidden Markov Model (FHMM) [23] is an approximate graphical level in which a sequence of observed variables is generated from a latent sequence of discretely evaluated vectors, as shown in Figure 1a., \"T,\" the latent state St is factorically incorporated into the M dimensions, where each latent factor Smt (m) is in time a vector of the Km Boolean variables with the restriction that exactly one of the Boolean variables has a value., we refer to the variable Smt, k, which has value 1 as the state of the layer m., in general, the model could allow a different number of states Km for each layer m, but in our experiments we use a single value Kfor all layers. The observed variables Yt are discrete variables, where V is the size of the vocabulary variables."}, {"heading": "4 Variational Methods for Learning and Inference", "text": "The goal of our unattended Q-Q process is to maximize the marginal log probability: L (\u03b8) = logP ({Yt}) = log \u2211 {St, Yt}) (5) We can rewrite the target by means of an arbitrary new distribution Q ({St}), which we call the variation distribution, and apply Jensen's inequality as follows: log \u2211 {St} P ({St, Yt}) = log \u2211 {St} Q ({St, Yt}) P ({St, Yt}) Q ({St, Yt}) Q (St, Yt) = P (St, Yt)."}, {"heading": "4.1 E-step", "text": "S S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "4.2 M-step", "text": "In the M step, the best parameter sets of the model are determined by maximizing the objective functionF = E [logP ({St, Yt}). (20) Initial and transition parameters are normalized, as in a standard HMM, on the basis of the expected sufficient statistics collected in the E step. For the observation parameters, there is no closed form solution and we resort to the gradient descent. We again use the limit on the objective function to shift the expectation within the parameter of the protocol. The gradient of the lower limit F of the object is F \u2202 F \u0445 Ymk = \u0445 tE [Smt, k] 1 [Y = Yt] \u2212 \u0441t \u0445 n6 = m (\u0445 k \u2032 E [Snt, k \u2032) exp \u0445Y nk \u2032) E [Smt, k] exp \u0445Ymk (21), whereby, \u0441\u0442T = (\u0445 Y \u0445 m \u0445 k \u2032 E [Smt \u2032 k \u2032 exk \u2032 exk 1.)"}, {"heading": "4.3 Inference", "text": "For the calculation of the posterior P ({St} | {Yt}) we find the variation distribution Q, which approaches the posterior P best with the same procedure as in the E step. Q is also used to find the maximum a posteriori state sequence with the help of the Viterbi algorithm."}, {"heading": "4.4 Implementation Details", "text": "Instead of naively using equations 19 and 21, we pre-calculated few values to reduce the complexity of the runtime. First, we calculated \u03c6t and stored for each Y the product for all m (where memory O (V) is required). To calculate the sum for all Y, we used these values, which vary only by a factor that includes the point product with layer m and Y. Subsequently, we used these calculations to finally update the parameter in time O (MKV T). In the M step, we used a similar calculation technique that makes the costly runtime of the gradient calculation O (MKV T). Note that after processing the result, we can delete the memory for a token that makes the memory usage independent of the number of tokens. However, the runtime depends on the number of tokens. We used the online learning technique [37, 8] to learn our model on large corpus, we used a mini-batch of 5 and a mini-batch of 1000 sets of GSets."}, {"heading": "5 Experiments", "text": "In both experiments, the source domain comes from the newswire text and the target domain from the biomedical text. However, the domain matching settings of the two experiments differ. In the PoS tagging experiment, we gain access to the text from the test domain and can use it to train the representative learning model. However, in the chunking experiment, we gain access to a domain related to the test domain, but not to the text from the test domain itself. In both experiments, latent vectors from the FHMMs are used as representations in the monitored experiments."}, {"heading": "5.1 Experimental Setup", "text": "This year, it is time for us to be able to find a solution that is capable, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "5.2 Results and Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Conclusion and Future Work", "text": "We have developed a learning mechanism for discretely rated FHMMs that includes a novel two-step variational approach that can be trained in a reasonable amount of time on datasets of nearly three million tokens. Using the latent vectors predicted by our best FHMM model as characteristics, we have shown that a standard classifier can perform two NLP tasks, chunking and tagging, and make significant improvements on new domains, especially on words that have never or rarely been observed in designated training data. Much remains to be done to make the FHMM more useful as a representation learning technique. Most importantly, it needs to be scaled to larger datasets by means of distributed implementation. More work is also required to fully compare the relative benefits of using the global context with the local context in a representation learning framework. The same idea of combining the global context for predicting characteristics with other tasks, including modelling tasks such as parsing or even using hierarchical structuring tasks, may remain open to use the advantages of combining the global context under different hierarchies."}, {"heading": "Acknowledgments", "text": "This research was partially supported by NSF funding IIS-1065397."}], "references": [{"title": "Improved extraction assessment through better language models", "author": ["Arun Ahuja", "Doug Downey"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Neural net language models", "author": ["Yoshua Bengio"], "venue": "Scholarpedia, 3(1):3881,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Correlated topic models", "author": ["David Blei", "John Lafferty"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Learning bounds for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jenn Wortman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "In IN: ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Improving generative statistical parsing with semi-supervised word clustering", "author": ["M. Candito", "B. Crabbe"], "venue": "In IWPT, pages 138\u2013141,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Frustratingly easy semi-supervised domain adaptation", "author": ["Hal Daum\u00e9 III", "Abhishek Kumar", "Avishek Saha"], "venue": "In Proceedings of the ACL Workshop on Domain Adaptation (DANLP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society of Information Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "SERIES B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1977}, {"title": "Two step cca: A new spectral method for estimating vector models of words", "author": ["Paramveer S. Dhillon", "Jordan Rodu", "Dean P. Foster", "Lyle H. Ungar"], "venue": "In Proceedings of the 29th International Conference on Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["Doug Downey", "Stefan Schoenmackers", "Oren Etzioni"], "venue": "In ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Online methods for multi-domain learning and adaptation", "author": ["Mark Dredze", "Koby Crammer"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Multi-domain learning by confidence weighted parameter combination", "author": ["Mark Dredze", "Alex Kulesza", "Koby Crammer"], "venue": "Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Hierarchical bayesian domain adaptation", "author": ["Jenny Rose Finkel", "Christopher D. Manning"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Factorial hidden markov models", "author": ["Zoubin Ghahramani", "Michael I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Hidden markov tree models for semantic class induction", "author": ["Edouard Grave", "Guillaume Obozinski", "Francis Bach"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["James Henderson", "Ivan Titov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Self-organizing maps of words for natural language processing applications", "author": ["T. Honkela"], "venue": "In Proceedings of the International ICSC Symposium on Soft Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "Learning Representations for Weakly Supervised Natural Language Processing Tasks", "author": ["Fei Huang", "Arun Ahuja", "Doug Downey", "Yi Yang", "Yuhong Guo", "Alexander Yates"], "venue": "Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Distributional representations for handling sparsity in supervised sequence labeling", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Exploring representation-learning approaches to domain adaptation", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Open-domain semantic role labeling by modeling word spans", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Factorial Hidden Markov Models and the Generalized Backfitting Algorithm", "author": ["Robert A. Jacobs", "Wenxin Jiang", "Martin A. Tanner"], "venue": "Neural Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["Jing Jiang", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Dimensionality reduction by random mapping: Fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "In IJCNN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Semi-supervised learning for natural language", "author": ["Percy Liang"], "venue": "In MASTERS THESIS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X Wu"], "venue": "In ACL-IJCNLP,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Domain adaptation with multiple sources", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["S. Martin", "J. Liermann", "H. Ney"], "venue": "Speech Communication,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1998}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1993}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}, {"title": "The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "author": ["M. Sahlgren"], "venue": "PhD thesis, Stockholm University,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2006}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1983}, {"title": "Introduction to the conll-2000 shared task: Chunking", "author": ["Erik F. Tjong Kim Sang", "Sabine Buchholz", "Kim Sang"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Association for Computational Linguistics ACL),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Ivan Titov", "James Henderson"], "venue": "In Association for Computational Linguistics ACL,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Towards explicit semantic features using independent component analysis", "author": ["J.J. V\u00e4yrynen", "T. Honkela", "L. Lindqvist"], "venue": "In Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Deep learning via semi-supervised embedding", "author": ["Jason Weston", "Frederic Ratle", "Ronan Collobert"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing", "author": ["Hai Zhao", "Wenliang Chen", "Chunyu Kit", "Guodong Zhou"], "venue": "In CoNLL 2009 Shared Task,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "We investigate a representation learning approach based on Factorial Hidden Markov Models [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 22, "endOffset": 26}, {"referenceID": 51, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 54, "endOffset": 62}, {"referenceID": 26, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 180, "endOffset": 188}, {"referenceID": 8, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 207, "endOffset": 214}, {"referenceID": 48, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 207, "endOffset": 214}, {"referenceID": 0, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 370, "endOffset": 373}, {"referenceID": 44, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 49, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 43, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 15, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 25, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 33, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 42, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 4, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 50, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 8, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 41, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 39, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 37, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 9, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 34, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 52, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 2, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 466, "endOffset": 473}, {"referenceID": 40, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 466, "endOffset": 473}, {"referenceID": 11, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 51, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 10, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 1, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 26, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 28, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 29, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 48, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 27, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 0, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 655, "endOffset": 662}, {"referenceID": 18, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 655, "endOffset": 662}, {"referenceID": 30, "context": "[31] use a generalized backfitting approach for training a discrete-observation FHMM, but they have not run experiments on a naturally-occurring dataset, and they focus on language modeling rather than representation learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We use a different training procedure based on variational EM [33], and provide empirical results for a representation learning task on a standard dataset.", "startOffset": 62, "endOffset": 66}, {"referenceID": 46, "context": "[47] has annotated parse trees with latent vectors that compose hierarchically.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": "Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "[24] use mainly two types of latent variable models: one essentially an HMM and the other has the latent variables associated in a syntactic dependency tree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 31, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 14, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 21, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 20, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 19, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 5, "context": "Learning bounds are known [6, 39].", "startOffset": 26, "endOffset": 33}, {"referenceID": 38, "context": "Learning bounds are known [6, 39].", "startOffset": 26, "endOffset": 33}, {"referenceID": 13, "context": "[14] use semi-supervised learning to incorporate labeled and unlabeled data from the target domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "3 The Factorial Hidden Markov Model for Learning Representations The Factorial Hidden Markov Model (FHMM) [23] is a bayesian graphical model in which a sequence of observed variables is generated from a latent sequence of discrete-valued vectors, as shown in Figure 1a.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "While we borrow the model structure from Ghahramani and Jordan [23], the main between their model and our model is in the observation distribution.", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "As a result of the change to the observation distribution, we need to change the inference and learning procedures, although we continue to follow [23] in using variational approximations, which we explain below.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "{St} Q({St}) log P ({St}|{Yt}) Q({St}) = L(\u03b8)\u2212KL(Q({St})||P ({St}|{Yt})) (8) and F (Q, \u03b8) = E{St}\u223cQ[logP ({St, Yt})]\u2212 E{St}\u223cQ[logQ({St})] (9) We use the Expectation Maximization (EM) algorithm [17], a block coordinate-ascent algorithm, to learn the parameters \u03b8.", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "We borrow the structured variational approximation model from Ghahramani and Jordan [23] which is shown in 1b.", "startOffset": 84, "endOffset": 88}, {"referenceID": 32, "context": "To handle this term, we make use of a second variational bound, log x \u2264 \u03c6x \u2212 log\u03c6 \u2212 1 [33, 4], where \u03c6 is a new variational parameter which can be varied to make the bound tight.", "startOffset": 86, "endOffset": 93}, {"referenceID": 3, "context": "To handle this term, we make use of a second variational bound, log x \u2264 \u03c6x \u2212 log\u03c6 \u2212 1 [33, 4], where \u03c6 is a new variational parameter which can be varied to make the bound tight.", "startOffset": 86, "endOffset": 93}, {"referenceID": 36, "context": "We used online learning technique [37, 8] for learning our model on large corpus.", "startOffset": 34, "endOffset": 41}, {"referenceID": 7, "context": "We used online learning technique [37, 8] for learning our model on large corpus.", "startOffset": 34, "endOffset": 41}, {"referenceID": 27, "context": "[28] and Blitzer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] which contains 561 MEDLINE sentences with 14554 tokens from the Penn BioIE project.", "startOffset": 0, "endOffset": 3}, {"referenceID": 45, "context": "The dataset consist of sections 15-18 of the Penn Treebank containing 8932 sentences which are annotated with PoS tags and the chunk tags[46].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The test set consist of 198 manually annotated sentences [28] from the biomedical domain of Open American National Corpus (OANC).", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "Then we used a preprocessing step defined in [36] to select a subset of good sentences for training the FHMM, resulting in 112,824 sentences.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clusters as representations.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clusters as representations.", "startOffset": 118, "endOffset": 121}, {"referenceID": 48, "context": "[49], we used the prefixes of length 4, 6, 10 and 20 in addition to the whole path for the Brown representation as the features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We also trained a neural network model described in Collobert and Weston [11] and used previous 10 words to predict the next word in the sequence.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "[28], with addition of the context words and context representation dimension as features in a window of size 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In the chunking dataset, the representation learning text we used is different but the test set is the same as the previous work [28] we compare against, who also used the test sentences in the representation learning system.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "The 50-dimensional word embeddings learned using neural network model from Collobert and Weston [11] is represented as EMBEDDINGS-50.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "HUANGHMM-80 is a system defined in [28] and SCL is a system defined in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively).", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "HUANGHMM-80 is a system defined in [28] and SCL is a system defined in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively).", "startOffset": 71, "endOffset": 74}, {"referenceID": 27, "context": "Previous system HUANGHMM-80 [28] included the text from the test domain itself for representation learning.", "startOffset": 28, "endOffset": 32}], "year": 2014, "abstractText": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "creator": "LaTeX with hyperref package"}}}