{"id": "1312.6190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning", "abstract": "Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledge-based transfer.", "histories": [["v1", "Sat, 21 Dec 2013 01:50:08 GMT  (413kb,D)", "https://arxiv.org/abs/1312.6190v1", "9 pages 7 figures, summitting to 2nd International Conference on Learning Representations (ICLR2014)"], ["v2", "Wed, 28 May 2014 16:35:17 GMT  (529kb,D)", "http://arxiv.org/abs/1312.6190v2", "9 pages 7 figures, new experimental results on ranking and transfer have been added, typo fixed"]], "COMMENTS": "9 pages 7 figures, summitting to 2nd International Conference on Learning Representations (ICLR2014)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["son n tran", "artur d'avila garcez"], "accepted": false, "id": "1312.6190"}, "pdf": {"name": "1312.6190.pdf", "metadata": {"source": "CRF", "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning", "authors": ["Son N. Tran", "Artur d\u2019Avila Garcez"], "emails": ["Son.Tran.1@city.ac.uk", "aag@soi.city.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "2 Feature Selection by Ranking", "text": "In this section, we present a method for selecting representations from an RBM by evaluating their properties. We define a ranking function and show that they can capture the most important information in the RBM to obtain a measurement of information loss. In a trained RBM, we define a scoring for each unit j in the hidden layer. Scoring represents the weight of a sub-network that includes all connections between unit j and the visible layer. A score cj is a positive real number that can be considered a recording of uncertainty in the sub-network. We expect all weights of a network to be replaced by suitable cj or \u2212 cj of information loss."}, {"heading": "3 Adaptive Feature Learning", "text": "In this section, we present adaptive feature learning from the source domain. Given an RBM being trained on a target domain, we are interested in investigating whether the score ranking introduced in Section 2 can be useful in improving the predictive accuracy of another RBM being trained on a target domain. From a transfer learning perspective, we intend to produce a general transfer learning algorithm by transferring knowledge from a source domain to a target domain (ranking function for transferring it to a target domain) in order to steer learning to a target domain (ranking function for transferring it to a target domain, as follows: The selection of features in the source domain is general that it is independent of the data from the target domain. In the target domain, an RBM is assigned a number of transferred parameters (W (t): a fixed set of weights that are associated with high-level networks from the source domain."}, {"heading": "4 Experimental Results", "text": "Let's start by evaluating the approach on the MNIST handwritten dataset. First, we want to verify whether hidden units with low values are actually less significant in the case of image domains. Then, we show that knowledge from one image domain can be used to improve predictive accuracy in another image domain."}, {"heading": "4.1 Feature Selection", "text": "We trained an RBM with 500 hidden nodes on 20,000 samples from the MNIST dataset to visualize the filter bases of the 50 highest rated subnets and the 50 lowest rated subnets (each taking 10% of the network capacity). Figure 2 shows the result of using a standard RBM, and Figure 3 shows the result of using a sparse RBM [6]. As can be seen, high values are usually associated with more concrete visualizations of the expected MNIST patterns, while low values are usually associated with fading or noise patterns. Figure 3 associates high values with sparse representations, while low values generate less meaningful representations according to the way in which the RBM was trained. In sparse RBM, we use PCA to reduce the dimensionality of the images to 69 and train the network with visible units."}, {"heading": "4.2 Unsupervised Transfer Learning", "text": "In fact, it is true that it is a way in which most people are able to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented a method and an efficient algorithm for ranking and selecting representations of a Boltzmann Restricted Machine trained on a source domain to be transferred to a target domain. The method is general in that the knowledge chosen by the ranking function does not depend on its relationship with a particular target domain, and it works with unattended learning and knowledge-based transfer. Experiments conducted with the image data sets MNIST, ICDAR and TiCC show that the proposed adaptive feature ranking and transfer learning method provides statistically significant improvements in the training of RBMs. In this paper, we will focus on the selection of features from flat networks (RBM) for general transfer learning. In future work, we are interested in selectively learning and transferring high-level features from deep networks for a particular domain."}], "references": [{"title": "Multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "The connectionist inductive learning and logic programming system", "author": ["Artur S. Avila Garcez", "Gerson Zaverucha"], "venue": "Applied Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Deep transfer via second-order markov logic", "author": ["Jesse Davis", "Pedro Domingos"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Transfer learning by borrowing examples for multiclass object detection", "author": ["Joseph J. Lim", "Ruslan Salakhutdinov", "Antonio Torralba"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Unsupervised and transfer learning challenge: a deep learning approach", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Xavier Glorot", "Salah Rifai", "Yoshua Bengio", "Ian J. Goodfellow", "Erick Lavoie", "Xavier Muller", "Guillaume Desjardins", "David Warde-Farley", "Pascal Vincent", "Aaron C. Courville", "James Bergstra"], "venue": "In ICML Unsupervised and Transfer Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Mapping and revising markov logic networks for transfer learning", "author": ["Lilyana Mihalkova", "Tuyen Huynh", "Raymond J. Mooney"], "venue": "In Proceedings of the 22nd national conference on Artificial intelligence - Volume 1,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["Rajat Raina", "Alexis Battle", "Honglak Lee", "Benjamin Packer", "Andrew Y. Ng"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Transfer learning via advice taking", "author": ["Lisa Torrey", "Jude W. Shavlik", "Trevor Walker", "Richard Maclin"], "venue": "In Advances in Machine Learning I,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Knowledge-based artificial neural networks", "author": ["Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Heterogeneous transfer learning with rbms", "author": ["Bin Wei", "Christopher Pal"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 1, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 0, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 10, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 12, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 8, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 2, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 9, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 0, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 10, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 12, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 7, "context": "In connectionist transfer learning, many approaches transfer the knowledge selected specifically based on the target, and (in some cases) with the provision of labels in the source domain [8, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 14, "context": "In connectionist transfer learning, many approaches transfer the knowledge selected specifically based on the target, and (in some cases) with the provision of labels in the source domain [8, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 10, "context": "In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6].", "startOffset": 209, "endOffset": 216}, {"referenceID": 5, "context": "In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6].", "startOffset": 209, "endOffset": 216}, {"referenceID": 11, "context": "This paper introduces a method and efficient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain.", "startOffset": 148, "endOffset": 155}, {"referenceID": 3, "context": "This paper introduces a method and efficient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain.", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "For example, in [7], knowledge such as data samples in the source domain are transformed to a target domain to enrich supervised learning.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "In our approach, the transfer learning is unsupervised, as done in [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "For example, in [1] a common orthonormal matrix is learned to construct linear features among multiple tasks.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "Self-taught learning [11], on the other hand, applies sparse coding to transfer the representations learned from source data onto the target domain.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "The output of transferred hidden nodes in target domain is considered as self-taught features [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "How much the down-weights affect the learning in the target RBM depends on an influence factor \u03b8; in this paper \u03b8 \u2208 [0 1].", "startOffset": 116, "endOffset": 121}, {"referenceID": 3, "context": "using Contrastive Divergence [4].", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Figure 2 shows the result of using a standard RBM, and Figure 3 shows the result of using a sparse RBM [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "e TiCCc:TICCw B) To compare our transfer learning approach with self-taught learning [11], we train an RBM in the source domain and use it to extract common features in the target domain for classification.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledge-based transfer.", "creator": "LaTeX with hyperref package"}}}