{"id": "1312.6192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Can recursive neural tensor networks learn logical reasoning?", "abstract": "Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of \"some animal walks\" from \"some dog walks\" or \"some cat walks,\" given that dogs and cats are animals. The results are promising for the ability of these models to capture logical reasoning, but the model tested here appears to learn representations that are quite specific to the templatic structures of the problems seen in training, and that generalize beyond them only to a limited degree.", "histories": [["v1", "Sat, 21 Dec 2013 02:29:42 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v1", "Submitted for presentation at ICLR 2014"], ["v2", "Tue, 24 Dec 2013 01:42:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v2", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v3", "Tue, 4 Feb 2014 18:02:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v3", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v4", "Sat, 15 Feb 2014 20:59:04 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v4", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"]], "COMMENTS": "Submitted for presentation at ICLR 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samuel r bowman"], "accepted": false, "id": "1312.6192"}, "pdf": {"name": "1312.6192.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sbowman@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.61 92v1 [cs.CL] 2 1"}, {"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight"}, {"heading": "2 Reasoning with monotonicity", "text": "One can replace the first argument of the quantifier with another (the first of the two predicates follows the other), namely with a more general category in which the dogs bark. The second argument is less specific, but it can also be less specific. It can be said that some dogs make noises, but it cannot be said that they bark at cars. This type of inference is at least partially dependent on the way in which they bark."}, {"heading": "2.1 The task: natural language inference", "text": "The fact is that you have to be able to manoeuvre yourself into a situation in which you see yourself in a position, in which you are able to assert yourself, and in which you are able to assert yourself, in which you are able to assert yourself, and in which you are able to assert yourself."}, {"heading": "3 Methods", "text": "The model is based on a recursively applied functionality, each based on two reduced functions. [14] This means imitating the recursive construction of meanings in formal models of semantics. In this scheme, pairs of words are grouped into phrase representations, which are used from a function of length 2N to representations of length 2N. This mode of expression is then further combined with other words and phrases until the entire phrase or sentence being evaluated is represented in a single vector, which is then used as input to a classification and used in a higher-level learning process to develop a model directly from existing literature for this task is impossible, as no asymmetric relationship between phrases has been proposed (although it may be possible to easily adapt the paraphrase model by Socher et al. [14] To build on the task in future work, I build a combination model."}, {"heading": "4 Data", "text": "The data set is based on a small purpose-built vocabulary that is said to be large and varied enough to allow a variety of experiments on various semantic phenomena beyond those shown here, but this is still small enough to exhaustively manually label the relationships between the lexical elements themselves if necessary. The vocabulary contains 41 predicates (nouns, adjectives and intransitive verbs; see Appendix B for more on the design of the word list), six quantifiers, the logical conjunctions and or, and the naughty operator note.The data take the form of 12k pairs of short sentences, each with a relational label, which I divide into about 200 smaller datasets. These datasets contain narrowly limited approaches to a specific valid argumentation pattern that differ in most lexical elements, and associate all parts of the data set."}, {"heading": "5 Experiments and results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Discussion", "text": "The results of the experiments described above show that recursive neural tensor networks are capable of learning lexical representations that support logical thinking; the model can correctly identify invisible examples of inferences with perfect accuracy; however, the representations that the model learns are not really universal; English speakers who know the meaning of all the words in a sentence are able to think about whether that sentence involves any other sentence of English, even if they have never seen the particular pattern of inference before; this broad capacity for generalization does not seem to have been learned in these experiments; pessimistically, the model could primarily memorize the characteristics of the argument itself, with any generalization going beyond the placement of an erratic and case-specific word; and optimistically, the model could learn to handle logic in a way that is generalizing."}, {"heading": "Acknowledgments", "text": "I would like to thank Chris Manning and Chris Potts for their advice at every stage of this project, Richard Socher and Jeffrey Pennington for detailed and helpful discussions, and J.T. Chipman for his help in designing a pilot experiment."}, {"heading": "Appendix A: The syntactically untied model", "text": "I did some of the experiments described above with a potentially more powerful variant of the model, following Socher et al. [19], with three separately parameterized composition functions instead of a single universal function.All instances of the composition layer use the same basic RNTN layer structure, including sigmoid nonlinearity, but the parameters are chosen from one of three sentences as follows: \u2022 Negation composition parameters: Used in the composition not with a predicate, as in non + European or non + dog. \u2022 Quantifier first reasoning parameters: used in the composition of a quantifier with its first argument (not a single word or a two-word phrase with its first argument), as in all + dog or non-European. \u2022 Quantifier second reasoning parameters: used in the composition of a quantifier first argument parameters: in the composition of a quantifier with its first argument."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proc. ICML. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. EMNLP. ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP. EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ICLR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Frege in space: A program for compositional distributional semantics. Submitted, draft at http://clic", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "cimec. unitn. it/composes,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Natural language inference", "author": ["Bill MacCartney"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Monotonicity phenomena in natural language", "author": ["Jack Hoeksema"], "venue": "Linguistic Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Categorial grammar and natural reasoning. ILTI Publication Series for Logic, Semantics, and Philosophy of Language LP-91-08", "author": ["V\u0131\u0301ctor S\u00e1nchez-Valencia"], "venue": "University of Amsterdam,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. EMNLP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In Proc. EACL. ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A formal approach to linking logical form and vector-space lexical semantics", "author": ["Dan Garrette", "Katrin Erk", "Raymond Mooney"], "venue": "Computing Meaning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In Proc. ICML", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Proc. IEEE International Conference on Neural Networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["Jorge Nocedal"], "venue": "Mathematics of computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1980}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 1, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 2, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 3, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 4, "context": "Much of the work to date analyzing vector representations for words (see [5]) has focused on lexical semantic behaviors\u2014like the similarity between words like Paris and France.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Natural language inference (NLI), the ability to reason about the truth of a statement on the basis of some premise, is among the clearest examples of a task that requires comprehensive and accurate natural language understanding [6].", "startOffset": 230, "endOffset": 233}, {"referenceID": 5, "context": "I borrow the structure of the task from MacCartney [6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "framework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.", "startOffset": 36, "endOffset": 42}, {"referenceID": 7, "context": "framework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.", "startOffset": 36, "endOffset": 42}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Monotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from first order logic or a similar system.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "Monotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from first order logic or a similar system.", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "[9] shows exactly this sort of behavior in VSMs learned for language modeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In the standard formulation of the task (and the one used in the RTE datasets [10]), the goal is to determine whether a reasonable human would infer a hypothesis from a premise.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "[11] show in an aside that a Boolean logic with negation and conjunction can be learned in a minimal recursive neural network model with one-dimensional (scalar) representations for words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The natural logic engine at the core of MacCartney\u2019s [6] NLI system requires a complex set of linguistic knowledge, much of which takes the form of what he calls projectivity signatures.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "[12] have achieved limited success in building a classifier to judge entailments between one- and two-word phrases (including some with quantifiers), though their vector representations were crucially based on distributional statistics and were not learned for the task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] propose a way to improve standard discrete NLI with vector representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], which is meant to mimic the recursive construction of meanings in formal models of semantics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] to the task in future work).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] (itself adapted from Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]) and shown below.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "In particular, I use the leaky rectified linear function [15]: fb(~x) = max(~x, 0) + 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "The model is trained using backpropagation through structure [16], wherein the negative log of the probability assigned to the correct label is taken as a cost function, and the gradient of each parameter with respect to that cost function is computed at each node, with information passing down the tree", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "Optimization: I train the model with stochastic gradient descent (SGD), with gradients pooled from randomly chosen minibatches of 32 training examples (chosen empirically), and learning rates computed using AdaGrad [17].", "startOffset": 215, "endOffset": 219}, {"referenceID": 17, "context": "L-BFGS [18] was tried unsuccessfully as an alternative to SGD.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "I additionally reran the experiments with a variation of the network that uses different composition functions to compose different types of phrase, following [19], but saw no associated improvement in performance.", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "Running experiments like the ones presented above on models with matrices or higher order tensors as word representations [20, 21] might be a promising start.", "startOffset": 122, "endOffset": 130}, {"referenceID": 20, "context": "Running experiments like the ones presented above on models with matrices or higher order tensors as word representations [20, 21] might be a promising start.", "startOffset": 122, "endOffset": 130}], "year": 2017, "abstractText": "Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of some animal walks from some dog walks or some cat walks, given that dogs and cats are animals. The results are promising for the ability of these models to capture logical reasoning, but the model tested here appears to learn representations that are quite specific to the templatic structures of the problems seen in training, and that generalize beyond them only to a limited degree.", "creator": "LaTeX with hyperref package"}}}