{"id": "1312.6204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "abstract": "Dataset bias remains a significant barrier towards solving real world computer vision tasks. Though deep convolutional networks have proven to be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-of-the-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost.", "histories": [["v1", "Sat, 21 Dec 2013 04:32:51 GMT  (63kb,D)", "https://arxiv.org/abs/1312.6204v1", null], ["v2", "Tue, 18 Feb 2014 02:57:42 GMT  (63kb,D)", "http://arxiv.org/abs/1312.6204v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["judy hoffman", "eric tzeng", "jeff donahue", "yangqing jia", "kate saenko", "trevor darrell"], "accepted": false, "id": "1312.6204"}, "pdf": {"name": "1312.6204.pdf", "metadata": {"source": "CRF", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "authors": ["Judy Hoffman", "Eric Tzeng", "Jeff Donahue", "Yangqing Jia"], "emails": ["jhoffman@eecs.berkeley.edu", "etzeng@eecs.berkeley.edu", "jdonahue@eecs.berkeley.edu", "jiayq@google.com", "saenko@cs.uml.edu", "trevor@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "2 Background: Deep Domain Adaptation Approaches", "text": "For our task we consider the matching between a large source domain and a target domain with few or no marked examples. A typical approach to domain adaptation or transfer learning with deep architectures is to take the representation learned via back propagation on a large dataset, and then transfer the representation to a smaller dataset by fine tuning, i.e. the back propagation at a lower learning rate [11, 23]. However, fine tuning requires a large amount of labeled target data and should therefore not work well if we take into account the very sparse label state, which we evaluate below, where we have only one labeled example per category in the target domain. In fact, in our experiments under this premise the performance actually reduces to the Imageur label condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition-condition is-condition-condition-condition-condition."}, {"heading": "3 Adapting Deep CNNs with Few Labeled Target Examples", "text": "We propose a general framework for selectively adjusting the parameters of a Convolutionary Neural Network (CNN), whose representation and classification weights are trained on a large-scale source domain such as ImageNet. Our framework adds a final domain adaptive classification layer that uses the activation of one of the layers of the existing network as input characteristics. Note that the network cannot be refined effectively without access to better labeled target data. This adaptive layer is a linear classification that combines source and target training data using an adaptation method. To demonstrate the universality of our framework, we select a representative set of popular linear classification approaches, which we evaluate empirically in Section 4. We structure our discussion into the group of monitored and unattended adaptation settings."}, {"heading": "3.1 Unsupervised Adaptation", "text": "Many unattended adjustment techniques aim to minimize the gap between subdomains representing the source and target domains, which we refer to as U-, U- and U-domains, respectively. GRP [12] The Geodetic Flow Kernel (GRP) method is an unattended domain adjustment approach that looks for embeddings for the source and target points that minimize domain shift. Inputs into the method are U- and U-domains, low-dimensional embeddings of the source and target domains (e.g. from the main component analysis), and the method constructs the geodetic flow using a closed-form solution, and classification occurs by forming an SVM at the source dataX and transformed target data GX."}, {"heading": "3.2 Supervised Adaptation", "text": "This method requires the definition of a hyperparameter that determines the weights of the source and target classifiers. Late fusion has two important advantages: it is easy to implement the classification of the source and target classifiers and combine the results of the two to create a definitive scoring function. We call this approach Late Fusion. It has been researched by many for a simple adaptation approach. \u2022 Max: Produce the scores of the source and target classifiers as Vs and select the score of the two target classifiers as the endpoint for each example. Therefore, we are researching two methods of combining these scores, which are described below. \u2022 Produce the scores of the source and target classifiers and simply select the maximum score of the two as the endpoint for each example. SVA score of the two as the endpoint. This method requires the definition of a hyperparameter that sets the weights of the source and target classifiers."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "The Office [20] dataset is a collection of images from three different areas: Amazon, DSLR, and webcam. The 31 categories of the dataset consist of objects commonly found in office environments, such as keyboards, filing cabinets, and laptops. Of these 31 categories, 16 overlap with the categories contained in the 1000-category ImageNet classification task 1. Therefore, we limit our experiments to these 16 classes. In our experiments, which use Amazon as source domain, we follow the standard training protocol for this dataset by using 20 source examples per category [20, 12], for a total of 320 images. The 16 overlapping categories are backpack, bicycle helmet, bottle, desk lamp, desktop computer, filing cabinet, keyboard, laptop computer, cell phone, mouse, printer, projector, ring binder, speaker, and trash. ImageNet [3] is the largest available dataset of 2K images that overlap each other in the 1000 categories."}, {"heading": "4.2 Experimental Setup & Baselines", "text": "For our experiments, we use the fully trained deep CNN model we described in Section 2, to give just one example of individual categories (each level of CNN), then train a source classifier that applies these features to one of two source domains, and adjust to the target domain we are looking at is either the Amazon domain or the corresponding 16-category ImageNet subset, in which each category has many more examples. We focus on the webcam domain as our target (test) domain, as Amazon-to-webcam has shown that the only challenging shift in [9] (the DSLR domain is much more similar to webcam and does not require customization when using deep mid-level features). This combination illustrates the shift from online web images to real images taken in typical office / home environments."}, {"heading": "4.3 Effect of Source Domain Size", "text": "Previous studies have looked at source domains from the Office dataset. In this section, we ask what happens when an order of magnitude larger source dataset is used. For completeness, we begin by evaluating Amazon as the source domain. Preliminary results on this setting are reported in [9], where we expand the comparison by presenting the results with more customization algorithms and a more complete evaluation of hyperparameter settings. Table 1 presents multi-class accuracies for each algorithm using either layer 6 or 7 from the deep network, corresponding to the results from each of the fully connected layers. An SVM trained only with Amazon data achieves an accuracy of 78.6% in the domain (tested on the same domain) when using the DeCAF6 feature and 80.2% in the domain accuracy when using the DeCAF7 feature. These numbers are significantly higher than the performance of the same dataset in the webcam test data collection, suggesting that AF itself is still possible with the feature."}, {"heading": "4.4 Adapting with a Large Scale Source Domain", "text": "To begin answering this question, we will follow the same experimental paradigm as the previous experiment, but use ImageNet as our source data set. The results will be shown in Table 2. Again, we will first verify that the source only performs better when tested for indomain data than for webcam data. In fact, the source SVM for the 16 overlapping labels has an accuracy of 62.50% for ImageNet data using DeCAF6 features and an accuracy of 74.50% when using DeCAF7 features. Compare this with the 54% and 59% for webcam evaluation and a dataset bias is still clearly evident. Note that the overall performance of all algorithms improves when using ImageNet as the source domain. Furthermore, uncontrolled adaptation approaches are more effective than with the smaller source domain experiment."}, {"heading": "4.5 Adapting a Pre-trained Classifier to a New Label Set", "text": "DeCAF8 differs from the other DeCAF functions in that it represents the 1000 activations that correspond to the 1000 labels in the ImageNet classification task. In the CNN proposed by [17], these activations are fed into a Softmax unit to calculate the label probabilities. Instead, we experiment with using the DeCAF8 activations directly as a feature representation that resembles the training of another classifier that uses the output of the 1000-way CNN classification. Table 3 shows results for various adaptation techniques that both ImageNet and Amazon use as source domains. We use the same setup as before, but instead use DeCAF8 as a feature representation. The ImageNet results are consistently better with DeCAF8 than with DeCAF6 or DeCAF7, probably due to the fact that DeCAF8 was explicitly trained on ImageNet data to effectively differentiate between ImageNet categories."}, {"heading": "4.6 Analysis and Practical Considerations", "text": "Our customization experiments show that, despite its large size, even ImageNet is not large enough to cover all areas, and that traditional domain customization methods go a long way to increasing performance and mitigating the effects of this shift. Depending on the characteristics of the problem at hand, our results suggest that different methods may be most suitable. If there are no labels in the target domain, then there are unattended customization algorithms that are easy to use and quickly calculated to achieve the customization time but still increased performance via pure source methods.For this scenario, we experimented with two subspace alignment methods, both requiring the definition of a parameter that specifies the dimensionality of the input subspaces. Figure 1 (b) shows the effect that changing the subspace dimensionality has an impact on overall methodological performance. In general, we noticed that these methods were not particularly sensitive to these parameters as long as the dimensionality group remains larger than the number of categories in our domain."}, {"heading": "5 Conclusion", "text": "We have shown that although ImageNet as a source domain is more generalized than other smaller source domains, there is still a domain shift when adapting to other visual domains. Our experimental results show that deep adaptation methods can do much to mitigate the effects of this domain shift. Based on our results, we have also made a number of practical recommendations for selecting a feature representation and adaptation method that take into account limitations of runtime and accuracy. There are a number of interesting directions that we can take given our results. First, we note that while DeCAF8 is the strongest feature that can be used to learn a classifier on ImageNet data, DeCAF7 is actually a better feature that can be used with the Amazon source domain and the webcam target domain. First, we note that while DeCAF8 may be the most powerful feature that can be used to learn a classifier on ImageNet data, and a combined model that generates source information that can be used effectively before applying customized architecture, which could result in a hybrid differentiated characteristic representation for the different domains and a combined model that generates source domain that is based on the specific adaptation architecture."}], "references": [{"title": "Tabula rasa: Model transfer for object category detection", "author": ["Y. Aytar", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "Proc. NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["A. Berg", "J. Deng", "L. Fei-Fei"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning bounds for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman"], "venue": "In Proc. NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "DLID: Deep learning for domain adaptation by interpolating between domains", "author": ["S. Chopra", "S. Balakrishnan", "R. Gopalan"], "venue": "In ICML Workshop on Challenges in Representation Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "In Proc. ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In Proc. ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Discovering latent domains for multisource domain adaptation", "author": ["J. Hoffman", "B. Kulis", "T. Darrell", "K. Saenko"], "venue": "In Proc. ECCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Efficient learning of domain-invariant image representations", "author": ["J. Hoffman", "E. Rodner", "J. Donahue", "K. Saenko", "T. Darrell"], "venue": "In Proc. ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A. Efros", "A. Torralba"], "venue": "In Proc. ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In Proc. CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Towards adapting imagenet to reality: Scalable domain adaptation with implicit low-rank transformations", "author": ["Erik Rodner", "Judy Hoffman", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "CoRR, abs/1308.4200,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Adapting SVM classifiers to data with shifted distributions", "author": ["J. Yang", "R. Yan", "A. Hauptmann"], "venue": "In ICDM Workshops,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ArXiv e-prints,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Supervised deep convolutional neural networks (CNNs) trained on large-scale classification tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classification challenges [3, 23].", "startOffset": 234, "endOffset": 241}, {"referenceID": 22, "context": "Supervised deep convolutional neural networks (CNNs) trained on large-scale classification tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classification challenges [3, 23].", "startOffset": 234, "endOffset": 241}, {"referenceID": 2, "context": "While the continuing expansion of web-based datasets like ImageNet [3] promises to produce labeled data for almost any desired category, such large-scale supervised datasets may not include images of the category across all domains of practical interest.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8].", "startOffset": 163, "endOffset": 169}, {"referenceID": 7, "context": "Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8].", "startOffset": 163, "endOffset": 169}, {"referenceID": 16, "context": "appealing, they have heretofore been unable to match the level of performance of supervised models, and unsupervised training of networks with the same level of depth as [17] remains a challenge.", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "Unfortunately, image datasets are inherently biased [21].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 12, "endOffset": 18}, {"referenceID": 3, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 12, "endOffset": 18}, {"referenceID": 19, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 46, "endOffset": 54}, {"referenceID": 20, "context": "Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods\u2019 test error increases in proportion to the difference between the test and training input distribution.", "startOffset": 46, "endOffset": 54}, {"referenceID": 6, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 21, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 0, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 19, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 17, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 15, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 12, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 11, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 13, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 14, "context": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models.", "startOffset": 89, "endOffset": 127}, {"referenceID": 19, "context": "Evaluation for image category classification across visually distinct domains has focused on the Office dataset, which contains 31 image categories and 3 domains [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 8, "context": "Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [20].", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [20].", "startOffset": 230, "endOffset": 234}, {"referenceID": 8, "context": "However, [9] limited their experiments to small-scale source domains found only in Office, and evaluated on only a subset of relevant layers.", "startOffset": 9, "endOffset": 12}, {"referenceID": 18, "context": "[19] attempted to adapt from ImageNet to the SUN dataset, but did not take advantage of deep convolutional features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "2 million labeled images available in the 2012 ImageNet 1000-way classification dataset [3] to train the model in [17] and evaluate its generalization to the Office dataset.", "startOffset": 88, "endOffset": 91}, {"referenceID": 16, "context": "2 million labeled images available in the 2012 ImageNet 1000-way classification dataset [3] to train the model in [17] and evaluate its generalization to the Office dataset.", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "backpropagation at a lower learning rate [11, 23].", "startOffset": 41, "endOffset": 49}, {"referenceID": 22, "context": "backpropagation at a lower learning rate [11, 23].", "startOffset": 41, "endOffset": 49}, {"referenceID": 4, "context": "A separate method that was recently proposed for deep adaptation is called Deep Learning for domain adaptation by Interpolating between Domains (DLID) [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "[9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "GFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "GFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "SA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional embeddings of the source and target domains U and \u0168 , respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "SA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional embeddings of the source and target domains U and \u0168 , respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Daum\u00e9 III [7] This simple feature replication method was proposed for domain adaptation by [7].", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Daum\u00e9 III [7] This simple feature replication method was proposed for domain adaptation by [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "PMT [1] This classifier adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "PMT [1] This classifier adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM.", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "MMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classifier parameters \u03b8 in the source feature space.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "MMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classifier parameters \u03b8 in the source feature space.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The Office [20] dataset is a collection of images from three distinct domains: Amazon, DSLR, and Webcam.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.", "startOffset": 151, "endOffset": 159}, {"referenceID": 11, "context": "In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.", "startOffset": 151, "endOffset": 159}, {"referenceID": 2, "context": "ImageNet [3] is the largest available dataset of image category labels.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam was shown to be the only challenging shift in [9] (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features).", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "GFK [12] Amazon 53.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "1 SA [10] Amazon 51.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "3 Daum\u00e9 III [7] Amazon+Webcam 68.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "4 PMT [1] Amazon+Webcam 64.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "8 MMDT [15] Amazon+Webcam 65.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "We show here multiclass accuracy on the target domain test set for both supervised and unsupervised adaptation experiments across the two fully connected layer features (similar to [9], but with one labeled target example).", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "Preliminary results on this setting are reported in [9], here we extend the comparison here by presenting the results with more adaptation algorithms and more complete evaluation of hyperparameter settings.", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "GFK [12] ImageNet 65.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "4 SA [10] ImageNet 59.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "1 Daum\u00e9 III [7] ImageNet+Webcam 59.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "5 PMT [1] ImageNet+Webcam 66.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "8 MMDT [15] ImageNet+Webcam 59.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "GFK [12] Source 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "2 SA [10] Source 66.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "3 Daum\u00e9 III [7] Source+Webcam 77.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "7 PMT [1] Source+Webcam 70.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "1 MMDT [15] Source+Webcam 73.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "In the CNN proposed by [17], these activations are fed into a softmax unit to compute the label probabilities.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "In each table we report both the performance of linear interpolation both averaged across hyper parameter settings \u03b1 \u2208 [0, 1] as well as the performance of linear interpolation with the best possible setting of \u03b1 per experiment \u2013 this is denoted as \u201cOracle\u201d performance.", "startOffset": 119, "endOffset": 125}, {"referenceID": 6, "context": "If there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the \u201cfrustratingly easy\u201d approach originally proposed by Daum\u00e9 III [7] and applied again for deep models in [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "If there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the \u201cfrustratingly easy\u201d approach originally proposed by Daum\u00e9 III [7] and applied again for deep models in [5].", "startOffset": 241, "endOffset": 244}], "year": 2014, "abstractText": "Dataset bias remains a significant barrier towards solving real world computer vision tasks. Though deep convolutional networks have proven to be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-ofthe-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost.", "creator": "LaTeX with hyperref package"}}}