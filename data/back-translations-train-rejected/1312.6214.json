{"id": "1312.6214", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Volumetric Spanners: an Efficient Exploration Basis for Learning", "abstract": "Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance, called volumetric spanners, and give efficient algorithms to construct such a basis.", "histories": [["v1", "Sat, 21 Dec 2013 06:51:50 GMT  (27kb,D)", "http://arxiv.org/abs/1312.6214v1", null], ["v2", "Sun, 26 Jan 2014 12:16:59 GMT  (27kb,D)", "http://arxiv.org/abs/1312.6214v2", null], ["v3", "Sun, 25 May 2014 11:57:08 GMT  (36kb,D)", "http://arxiv.org/abs/1312.6214v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DS", "authors": ["elad hazan", "zohar karnin", "raghu mehka"], "accepted": false, "id": "1312.6214"}, "pdf": {"name": "1312.6214.pdf", "metadata": {"source": "CRF", "title": "Volumetric Spanners and their Applications to Machine Learning", "authors": ["Elad Hazan", "Zohar Karnin", "Raghu Meka"], "emails": ["ehazan@ie.technion.ac.il", "zkarnin@ymail.com", "meka@microsoft.com"], "sections": [{"heading": null, "text": "We define a novel geometric concept of the low-variance exploration base, the volumetric spreaders, and provide efficient algorithms for constructing such bases. We show how efficient volumetric spreaders elicit the first efficient and optimal remorse algorithm for bandit linear optimization compared to general convex sets. Previously, such results were known only for certain convex sets or under special conditions such as the presence of an efficient, self-conforming barrier for the underlying set."}, {"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "1.1 Introduction to Exploration: Barycentric Spanners and John\u2019s Ellipsoid", "text": "In this section we will consider two geometric constructions used in earlier machine learning work to design an exploratory basis for a body K-Rd. The first is the ellipsoid corresponding to the barycentric key of K, which is defined as the ellipsoid of maximum volume supported by exactly d-points of K. The second is the minimum volume enclosing an ellipsoid (MVEE), also known as John's ellipsoid. As we will show later, our definition of a volumetric key enjoys the properties of both objects. Similar to barycentric keys, it is supported by a small (quasi-linear) set of points of K. at the same time, and unlike the barycentric counterpart, the volumetric ellipsoid contains the body K, a property shared with John's ellipsoid."}, {"heading": "1.2 Barycentric Spanners", "text": "The notion of barycentric tensioners was introduced in the work of [3] to define an exploration basis for an online problem with the shortest path. Barycentric tensioners have since been used as an exploration basis in several works: in [10] for linear online bandit optimization, in [5] for a highly probable equivalent to linear online bandit optimization, in [15] for repeated decisions about approximate functions, and in [9] for a stochastic version of linear bandit optimization. Definition 1.1. A barycentric tensioner of K is a series of d-points S = {u1,.., ud} K, so that any point in K can be expressed as a linear combination of elements of S by using coefficients in [\u2212 1, 1]. For C > 1, S is a C approximate barycentric tensioner of K, if any point in K is represented as a linear combination of elements of the coefficient of S in \u2212 1."}, {"heading": "1.3 The Fritz John Ellipsoid", "text": "The John ellipsoid is the only ellipsoid of the smallest volume that contains a particular convex body in Euclidean space. Its properties are the subject of the study in convex geometric terms that are far more abundant in John's position.) Then, John's theorem asserts the surprising fact that we have made a linear rethink in the way in which Bn denotes the unit in Rn. Furthermore, for symmetric convex bodies (which are far more abundant in machine learning) the factor 1d above can be replaced. John's ellipsoid, and in particular its points of contact with the convex body it uses for appealing exploration, and indeed [7] we have used precisely this machinery to achieve optimal optimization."}, {"heading": "1.4 Structure of the paper", "text": "In the next section, we delve into convex geometry and define the key terms behind our constructions. Then, in section 3, we list preliminary and known results from measurement concentration, convex geometry and online learning. In sections 4 and 5, we specify the construction of an efficient volumetric key for continuous or discrete sentences. Then, in section 6, we describe an application for bandit linear optimization."}, {"heading": "2 Volumetric Ellipsoids and Spanners", "text": "There are a number of factors that we will use in this section to establish ourselves in this area, and we will use it in this section to construct a particular decision before we define another form that characterizes a more discrete nature that defines a series of vectors S = {v1}, we denote of E (S) the ellipsoid definition. (S) The ellipsoid definition of A (S) the ellipsoid definition of E (S) the ellipsoid definition of E (S) the ellipsoid definition of E (S) the ellipsoid definition of A. (S) The ellipsoid definition of A (S) the definition of A (S). (S) The ellipsoid definition of A (S) the definition of A (S). (S) the ellipsoid definition of A (S) is the ellipsoid definition of A (S) the definition of A (S) the ellipsoid definition of A (S). (S) the ellipsoid definition of A (S) is the definition of S."}, {"heading": "2.1 Existence and Construction of Volumetric Ellipsoids", "text": "In this area, we prove to be the most important structural results, which we in the enderegulating, enrpsoid i.D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D D.\" D. \"D.\" D \"D.\" D. \"D.\" D D. \"D.\" D. \"D.\" D D. \"D.\" D D. \"D D.\" D. \"D.\" D. \"D D.\" D. \"D.\" D. \"D.\" D D. \"D.\" D. \"D D.\" D D. \"D.\" D D. \"D.\" D. \"D.\" D D"}, {"heading": "2.2 Approximate Volumetric Spanners", "text": "In this section, we present two different types of approximations for a volumetric extender. In both types, we need a small support for the extender of roughly linear size. In the first case, we allow the ellipsoid to contain the body only after it has been expanded by some product. In the second approximation, we allow a small fraction of the points of the body to lie outside the ellipsoid. Definition 2.3. Definition-ratio-volumetric extender S of K is a subset S K that is expanded for all x values. In the second approximation, we allow a small fraction of the points of the body outside the ellipsoid. Definition 2.3."}, {"heading": "3 Preliminaries", "text": "In the following sections we will use a result that shows that a log concave distribution efficiently sampled.Lemma 3.1 ([17], Theorems 2.1 and 2.2)."}, {"heading": "4 Algorithmic Construction for Convex Sets", "text": "In this section, we provide a construction for (p, \u03b5) -exp-volumetric (as in Definition 2.4): (p,) -exp-volumetric (as in Definition 2.4). (2) We start by providing a more comprehensive technical definition of a spreader. (2) In contrast to previous definitions, the following definition is not impenetrable for linear operators and is only used to support our construction. (2) Definition 4.1. (2) A-relative spreader is a discrete subset of S'K that applies to all x-K's. (S) 2E (S) \u2264 \u00df \u03b2 x 2.A first step is a spectral characterization of relative spreaders: Lemma 4.1. Let S = {v1, vT} span K and be so thatW = T = 1 viv > i1 \u03b2 IdThen S is a \u03b2-relational spreader. Let S = LT \u00b7 d Split by the matrix x."}, {"heading": "5 Algorithmic Construction for Discrete Sets", "text": "In this section we describe an algorithm based on distribution. (...) The order of the data we collect is suboptimal, and it is not as good as we expect it to be. (...) The algorithm is particularly easy and efficient to implement without answering questions. (...) The algorithm is particularly simple and efficient to implement, and it is suboptimal. (...) The algorithm is suboptimal. (...) The algorithm is very likely. (...) The algorithm is very simple. (...) The algorithm is very simple and efficient. (...) The Cd log d is then suboptimal. (...) The algorithm is suboptimal. (...) The algorithm is very likely. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good. (...) The algorithm is very good."}, {"heading": "6 Bandit Linear Optimization", "text": "Recall the problem of \"Bandit Linear Optimization\" (BLO): \"This is a fundamental problem we have in terms of the basic insights into the previous sections.\" (The environment chooses a loss vector Lt, which is not disclosed to the player.) The goal is to minimize the loss and to minimize the regret, which is defined as the loss of the strategy, minus the loss of the best fixed strategy of choice of x-K for all. From now on, and this is w.l.o.g. via standard scaling techniques, we assume that the loss vectors Lt's are chosen from the polar of K, meaning that of {L > x | 1 x = K}. The problem of the BLO is a natural generalization of the classic multi-armed bandit problem and extremely useful for efficient modeling of decision making among partial feedback for structured problems."}, {"heading": "6.1 Proof of Theorem 6.1", "text": "We continue the analysis of the geometric hedge algorithm, similar to [10, 7], under certain assumptions about the exploration strategy. For convenience, we assume that the set of possible arms K is finite. \u2212 \u2212 \u2212 \u2212 This assumption applies to all other areas where K is infinite, can be considered unlimited (this does not affect the computational complexity of our algorithm, but a mere technical convenience in the proof below). Before we prove the theory, we need three technical lemmas. In the first, we show that L't is an unbiased estimator of Lt. in the second, we have bound its variance. In the second, we have the expected value of its exponent.Lemma 6.3. In each t, L't't 'is an unbiased estimator of LtProof."}, {"heading": "A Concentration bounds for non centered isotropic log con-", "text": "We begin by proving an auxiliary problem used in the chain of proof of Corollary 3.1.Lemma A.1. (...) Let us be a positive integer and let us consider n = Ct 4d log2 (t / \u03b4) \u03b42 for a sufficiently large universal constant C. Let us exclude y1,. (...) Let us consider for convenience the d-dimensional vectors from an isotropic log-concave distribution. (...) Let us exclude the d-dimensional vectors from an isotropic log-concave distribution. (...) Since the y-concave distribution is independent, Sn is also distributed log-concave. Notice that E [Sn] = 0 and E [SnS T n] = 1 n n n-dimensional vectors from convenience. (...) Let us leave the d-dimensional vectors from an isotropic log-concave distribution."}], "references": [{"title": "Interior-point methods for full-information and bandit online learning", "author": ["J.D. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles", "author": ["Radoslaw Adamczak", "Alexander E. Litvak", "Alain Pajor", "Nicole Tomczak-Jaegermann"], "venue": "Journal of American Mathematical Society,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Online linear optimization and adaptive routing", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "An elementary introduction to modern convex geometry. In Flavors of Geometry, pages 1\u201358", "author": ["Keith Ball"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["Peter L. Bartlett", "Varsha Dani", "Thomas P. Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems, volume 5 of Foundations and Trends in Machine Learning", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Linear convergence of a modified frankwolfe algorithm for computing minimum-volume enclosing ellipsoids", "author": ["S. Damla Ahipasaoglu", "Peng Sun", "Michael J. Todd"], "venue": "Optimization Methods and Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In COLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Sham M Kakade", "Thomas P Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "An arithmetic proof of john\u2019s ellipsoid theorem", "author": ["Peter M. Gruber", "Franz E. Schuster"], "venue": "In arXiv:1207.7246,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Interpolating thin-shell and sharp large-deviation estimates for lsotropic log-concave measures", "author": ["Olivier Gudon", "Emanuel Milman"], "venue": "Geometric and Functional Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "L\u00f6wner-John ellipsoids", "author": ["Martin Henk"], "venue": "Documenta Mathematica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Extremum Problems with Inequalities as Subsidiary Conditions", "author": ["F. John"], "venue": "Studies and Essays: Courant Anniversary Volume,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1948}, {"title": "Playing games with approximation algorithms", "author": ["Sham M Kakade", "Adam Tauman Kalai", "Katrina Ligett"], "venue": "SIAM Journal on Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Rounding of polytopes in the real number model of computation", "author": ["Leonid G Khachiyan"], "venue": "Mathematics of Operations Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "The geometry of logconcave functions and sampling algorithms", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Santosh Vempala"], "venue": "Random Structures & Algorithms,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Random vectors in the isotropic position", "author": ["M. Rudelson"], "venue": "Journal of Functional Analysis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "To mention a few: Awerbuch and Kleinberg [3] devise the notion of barycentric spanners, and use this construction to give the first low-regret algorithms for complex decision problems such as online routing.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Abernethy, Hazan and Rakhlin [1] use self-concordant barriers to build an efficient exploration strategy for convex sets in Euclidean space.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "Bubeck, Cesa-Bianchi and Kakade [7] apply tools from convex geometry, namely John\u2019s ellipsoid to construct optimal regret algorithms for bandit linear optimization (albeit not always efficiently).", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "is that of online routing in graphs: a decision maker iteratively chooses a path in a given graph from source to destination, the adversary chooses lengths of the edges of the graph, and the decision maker receives as feedback the length of the path she chose but no other information (see [3]).", "startOffset": 290, "endOffset": 293}, {"referenceID": 0, "context": "learning permutations, rankings and other examples (see [1]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "The reader is referred to the recent survey of Bubeck and Cesa-Bianchi [6] for more details on algorithmic results for BLO.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "2 Barycentric Spanners The notion of Barycentric spanners was introduced in the work of [3], in order to define an exploration basis for an online shortest path problem.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 14, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 285, "endOffset": 288}, {"referenceID": 2, "context": "In [3] it is shown that any compact set has a barycentric spanner.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "5 in [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 93, "endOffset": 96}, {"referenceID": 12, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "John\u2019s ellipsoid and in particular its contact points with the convex body it encapsulates makes for an appealing exploration basis, and indeed [7] have used exactly this machinery to attain an optimal-regret bandit linear optimization algorithm.", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "[4] Let K \u2208 R be a symmetric convex set and assume that the unit sphere is its minimal enclosing ellipsoid.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16, 8]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[16, 8]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "If the vertices are [0, 1], [\u2212 \u221a 3 2 ,\u2212 1 2 ], [ \u221a 3 2 ,\u2212 1 2 ], then the eigenpoles of the ellipsoid of the bottom two vertices are [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "23 ], [2, 0].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "A distribution over R is log-concave when for its probability distribution function (pdf) p it holds that for any x, y \u2208 R, \u03bb \u2208 [0, 1], p(\u03bbx+ (1\u2212 \u03bb)y) \u2265 p(x)\u03bbp(y)1\u2212\u03bb", "startOffset": 128, "endOffset": 134}, {"referenceID": 16, "context": "1 ([17], Theorems 2.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "1 in [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 11, "context": "1 in [12]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "3 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "This implies via [17] (Lemaa 3.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "Interestingly, the 2 \u221a d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of [10].", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Numerous machine learning problems require an exploration basis a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.", "creator": "LaTeX with hyperref package"}}}