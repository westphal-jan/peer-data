{"id": "1312.6461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2013", "title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "histories": [["v1", "Mon, 23 Dec 2013 03:23:04 GMT  (348kb,D)", "https://arxiv.org/abs/1312.6461v1", "For ICLR2014"], ["v2", "Tue, 24 Dec 2013 02:54:29 GMT  (349kb,D)", "http://arxiv.org/abs/1312.6461v2", "For ICLR2014, revised into 9 pages"], ["v3", "Wed, 19 Feb 2014 20:02:05 GMT  (822kb,D)", "http://arxiv.org/abs/1312.6461v3", "For ICLR2014, revised into 9 pages; revised into 12 pages (with supplements)"]], "COMMENTS": "For ICLR2014", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sho sonoda", "noboru murata"], "accepted": false, "id": "1312.6461"}, "pdf": {"name": "1312.6461.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "authors": ["Sho Sonoda", "Noboru Murata"], "emails": ["s.sonoda0110@toki.waseda.jp,", "noboru.murata@eb.waseda.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Since hidden parameters are set within non-linear activation functions, simultaneous learning of all parameters is accompanied by backward propagation by a non-convex optimization problem. If the machine starts from an initial point far away from the target, the learning curve easily gets stuck in local minimums or gets lost in plateaus, and the machine fails in the convergence of the following fine-tuning phase. In the pre-training phase, the weight parameters draw enormous attention to their overwhelming high performance for real problems [1, 2]. Deep learning steps consist of two steps: pre-training and fine-tuning. The pre-training phase plays an important role in the convergence of the subsequent fine-tuning. In the pre-training parameters, layer by layer are constructed using unsupervised learning machines such as the restricted Boltzmann machines [3] or the denoization of autocoders [4]."}, {"heading": "2 Back ground and related works", "text": "As a matter of fact, most of them are able to outdo themselves, and they are able to outdo themselves. Most of them are able to outdo themselves, but most of them are not able to outlive themselves. Most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, most of them are able to outlive themselves, and most of them are able to outlive themselves, and most of them are able to outlive themselves, and most of them are able to outlive themselves, and most of them are able to outlive themselves, and most of them are able to outlive themselves."}, {"heading": "3 Nonparametric weight initialization via integral transform", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Sampling based two-stage learning", "text": "Let us make g: Rm \u2192 R be a neural network with a single hidden layer, expressed asg (x) = J \u2211 j = 1 wj\u03c6 (aj \u00b7 x \u2212 bj) + w0, (1) where the map \u03c6 is called the activation function; aj and bj are called hidden parameters, andwj are output parameters. In an ordinary sigmoid function \u03c3 (z): = 11 + exp (\u2212 z), the activation function \u03c6 should be the sigmoid pair in the formatting (z): = 1H {\u03c3 (z + h) \u2212 \u03c3 (z \u2212 h)}, (h > 0), (2) where H: = \u03c3 (h) \u2212 \u03c3 (\u2212 h) normalizes the maximum value of Eins. Let us consider an oracle distribution p (a, b) of hidden parameters. If such a distribution exists, we can first select these hidden parameters after p (a, b), and then we could call the remaining output parameters an equilibrium of the ression by a regression."}, {"heading": "3.2 Integral representations of neural networks", "text": "Consider the approach to a map f: Rm \u2192 R with a neural network. Murata [6] defines an integral transformation T of f in relation to a corrosive constant. Murata also showed that in the face of the corrosive kernel [a, b] there exists a formula that comprises the compositional nucleus in such a way that for each f: L1 (Rm), Lp (Rm) (1), (1), p: the inverse formula (x), b: the inverse formula (x), b: a formula that exists for each f: Lp (Rm), (Rm) (1), p: 1, p: \u2264), p: [4), p: [4), p: [6], with the inverse formula (x:), x: b: lim = lim \u2192 Rm + 1, p: c: an x formula."}, {"heading": "3.3 Practical calculation of the integral transform", "text": "Now we try to use the integral transformation according to the law of large numbers only if the input factors are uniform. As an oracle distribution p (a, b) of the hidden parameters. Although the distribution is given in the explicit form, as we have seen in the previous section, further refinements are required for the practical calculation. Given is a set {(xn, yn) Nn = 1, (8) of the input and output pairs, T (a, b) is empirically approximate asT (a, b) \u2248 1 Z N = 1 (a, xn \u2212 b) Nn (n) n = 1, (8) with a constant Z > 0, which is difficult to calculate accurately. In fact, the sampling algorithms such as the acceptance-rejection method [27] and the Markov chain Monte Carlo method [27] work with any unnormalized distribution, because they only approximate the ratio between the probability values. Let us note that the approximation of T (the exact) is running in T."}, {"heading": "3.4 For more efficient sampling", "text": "In Figure 1, the same distribution is transformed into another (\u03b1, \u03b2) coordinate system (which is explained below). Support for the distribution is converted into a rectangular shape, which means that scanning | T (\u03b1, \u03b2) | is easier than scanning | T (a, b). This bad form is formulated as the following statement: The objective function f (x) has compact support, which means that scanning | T (\u03b1, \u03b2) | is easier than scanning | T (a, b). Scanning the objective function f (x) has compact support, then supporting its transformation T (a, b) in the region is adopted: = (a, b)."}, {"heading": "4 Experimental results", "text": "We performed three sets of experiments comparing three types of learning methods: BP Whole parameters are initiated by samples from a uniform distribution and BackPropagation.SBP Hidden parameters are initialized by sampling | T (a, b) |; the residual parameters are initiated by linear regression.To compare the ability of the three methods, we performed three experiments on different problems: One-dimensional curve regression, Multidimensional Boolean functions and real data sets are initiated by linear regression.To compare the ability of the three methods, we performed three experiments."}, {"heading": "4.2 Multidimensional Boolean functions approximation - Combined AND, OR and XOR", "text": "Second, we performed a two-dimensional problem with two-dimensional input and three-dimensional output. Output vectors are composed of three logical functions: F (x, y): = (xANDy, xORy, xXORy). Therefore, the total number of data is only four: (x, y), (0, 0), (0, 1), (1, 1), (1, 1). The number of hidden units was set to 10. Output function was set to sigmoid and the loss function was set to cross-entropy. Uniformly random initialization parameters for BP and SBP were pulled from the interval [\u2212 1, 1]. Sampling from | T was performed using the acceptance-rejection method. Fig.3 shows both cross-entropy curves and classification errors in thin and thick lines."}, {"heading": "5 Conclusion and future directions", "text": "In this paper, we introduced a two-step method of weight initialization for backpropagation: sampling hidden parameters from oracle distribution and adjusting output parameters by ordinary linear regression. Based on the integral representation of neural networks, we constructed our oracle distributions from given data in a non-parametric way. Since the shapes of these distributions are not simple in high-dimensional input cases, we also discussed some numerical techniques such as coordinate transformation and mixing approximation of oracle distributions. We conducted three numerical experiments: complicated curve regression, Boolean function approximation, and handwritten digitalization classification. These experiments show that our initialization method works well with backpropagation. In particular, for low-dimensional problems, well-sampled parameters alone achieve good accuracy without parameter updates through backpropagation."}, {"heading": "Acknowledgments", "text": "The authors thank Hideitsu Hino for his succinct comments on the essay and Mitsuhiro Seki for his constructive discussions with them."}, {"heading": "A Sampling recipes", "text": "We assume here that support for the distribution of applications (a, b) is as narrow as possible. (a, b) We assume that the procedure for distributing applications (a, b) is as narrow as possible. (b) We assume that support for distributing applications (a, b) is as narrow as possible. (a) We assume that the distribution of applications (a, b) is as narrow as possible. (b) We assume that support for distributing applications (a, b) is as narrow as possible. (a, b) We assume that the distribution of applications (a, b) is as narrow as possible. (b) We assume that support for distributing applications (a, b) is as narrow as possible. (b) We assume that the distribution of applications (a, b) is as narrow as possible. (b) We assume that support for distributing applications (a, b) is as narrow as possible."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "A new initialization method for hidden parameters in a neural network is pro-<lb>posed. Derived from the integral representation of neural networks, a nonparamet-<lb>ric probability distribution of hidden parameters is introduced. In this proposal,<lb>hidden parameters are initialized by samples drawn from this distribution, and<lb>output parameters are fitted by ordinary linear regression. Numerical experiments<lb>show that backpropagation with proposed initialization converges faster than uni-<lb>formly random initialization. Also it is shown that the proposed method achieves<lb>enough accuracy by itself without backpropagation in some cases.", "creator": "LaTeX with hyperref package"}}}