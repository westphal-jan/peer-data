{"id": "1312.6712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2013", "title": "Invariant Factorization Of Time-Series", "abstract": "Time-series classification is an important domain of machine learning and a plethora of methods have been developed for the task. In comparison to existing approaches, this study presents a novel method which decomposes a time-series dataset into latent patterns and membership weights of local segments to those patterns. The process is formalized as a constrained objective function and a tailored stochastic coordinate descent optimization is applied. The time-series are projected to a new feature representation consisting of the sums of the membership weights, which captures frequencies of local patterns. Features from various sliding window sizes are concatenated in order to encapsulate the interaction of patterns from different sizes. Finally, a large-scale experimental comparison against 6 state of the art baselines and 43 real life datasets is conducted. The proposed method outperforms all the baselines with statistically significant margins in terms of prediction accuracy.", "histories": [["v1", "Mon, 23 Dec 2013 22:15:59 GMT  (46kb,D)", "http://arxiv.org/abs/1312.6712v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["josif grabocka", "lars schmidt-thieme"], "accepted": false, "id": "1312.6712"}, "pdf": {"name": "1312.6712.pdf", "metadata": {"source": "CRF", "title": "Invariant Factorization of Time Series", "authors": ["Josif Grabocka", "Lars Schmidt-Thieme"], "emails": ["}@ismll.uni-hildesheim.de"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2 Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "3 Definitions and Notations", "text": "1. Time Series: A time series is an ordered sequence of point values. In a data set of N-series instances in which each series has Q-points, we refer to the series data set as T-RN-Q. 2. Moving window segments: A moving window content of size L-N is a row sub-sequence that begins at position j-1,...., Q-L} of a row i of record T and is designated as Si, j-RL, Si, j: = (Ti, j, Ti, j + 1,..., Ti, j + L \u2212 1).3. All data set segments: The starting position of each moving window segment is incremented by an offset Phillips = {1,., L}, so the maximum number of segments per series is defined as M: = Q-LLC. All segments of a time series data set are defined as S-RN-M-x-Lats."}, {"heading": "4 Invariant Factorization of Time Series", "text": "4.1 Segmentation of segments of a set of segments as product of defined patterns are segmented in a sliding window approach with the size L = 1. Segmentation of individual segments is described in Algorithm 1. (Segmentation of individual segments is described in Algorithm 1.) Segmentation of individual segments is, however, at 0 and deviation from 1.Algorithm 1 SegmentesRequire: T, RN, L, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E,"}, {"heading": "5 Experimental Results", "text": "We compared the accuracy of predicting our method, called Invariant Factorization (INFA), with the following six basics of art: \u2022 TSBF: The number of time series (TSBF) terms tested is relatively high. \u2022 BOW: The Bag of Words (BOW) method splits the series into local SAX words and uses a histogram representation of the words as the new property representation. \u2022 DTW: Dynamic Time Warping (DTW) compils the best orientation of the time indices resulting in the minimum distance [18, 27]."}, {"heading": "6 Conclusions", "text": "In this study, we introduced Invariant Factorization, a method that first divides the time series into a series of overlapping segments by learning a series of latent patterns and the degree of affiliation of each segment. We formalized factorization as an objective limiting function and proposed a stochastic method to solve this problem. Finally, we performed a thorough experimental comparison with 6 modern baselines in 43 real time series datasets. Our method exceeds all baselines by statistically significant margins and marks the best published results in the field of time series classification in terms of UCR collection of datasets."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Shift and 2d rotation invariant sparse coding for multivariate signals", "author": ["Q. Barthelemy", "A. Larue", "A. Mayoue", "D. Mercier", "J.I. Mars"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A complexity-invariant distance measure for time series", "author": ["Gustavo E.A.P.A. Batista", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": "In SDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Cid: an efficient  complexity-invariant distance for time series", "author": ["Gustavo E.A.P.A. Batista", "Eamonn J. Keogh", "Oben Moses Tataw", "Vinicius M.A. Souza"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A bag-of-features framework to classify time series", "author": ["M. Baydogan", "G. Runger", "E. Tuv"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Motif-based classification of time series with bayesian networks and svms", "author": ["Krisztian Buza", "Lars Schmidt-Thieme"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "On the marriage of lp-norms and edit distance", "author": ["Lei Chen", "Raymond Ng"], "venue": "In Proceedings of the Thirtieth international conference on Very large data bases - Volume 30,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Spade: On shape-based pattern detection in streaming time series", "author": ["Yueguo Chen", "M.A. Nascimento", "Beng-Chin Ooi", "A. Tung"], "venue": "In Data Engineering,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Fast global alignment kernels", "author": ["Marco Cuturi"], "venue": "In Getoor et al., editor, Proceedings of the ICML 2011,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Classification of sparse time series via supervised matrix factorization", "author": ["Josif Grabocka", "Alexandros Nanopoulos", "Lars Schmidt- Thieme"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Invariant time-series classification", "author": ["Josif Grabocka", "Alexandros Nanopoulos", "Lars Schmidt- Thieme"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Support vector machines and dynamic time warping for time series", "author": ["Steinn Gudmundsson", "Thomas Philip Runarsson", "Sven Sigurdsson"], "venue": "In IJCNN,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Classification of time series by shapelet transformation. Data Mining and Knowledge Discovery, accepted subject to minor corrections", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Pooling robust shiftinvariant sparse representations of acoustic signals", "author": ["Po-Sen Huang", "Jianchao Yang", "Mark Hasegawa-Johnson", "Feng Liang", "Thomas S. Huang"], "venue": "In IN- TERSPEECH. ISCA,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["Eamonn Keogh", "Kaushik Chakrabarti", "Michael Pazzani", "Sharad Mehrotra"], "venue": "Knowledge and Information Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Scaling up dynamic time warping for datamining applications", "author": ["Eamonn J. Keogh", "Michael J. Pazzani"], "venue": "In KDD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Spatial representation for efficient  sequence classification", "author": ["P.P. Kuksa", "V. Pavlovic"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Coding timevarying signals using sparse, shift-invariant representations", "author": ["Michael S. Lewicki", "Terrence J. Sejnowski"], "venue": "In Proceedings of NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Experiencing sax: a novel symbolic representation of time series", "author": ["Jessica Lin", "Eamonn Keogh", "Li Wei", "Stefano Lonardi"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["Jessica Lin", "Rohan Khade", "Yuan Li"], "venue": "J. Intell. Inf. Syst.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Finding structural similarity in time series data using bag-of-patterns representation", "author": ["Jessica Lin", "Yuan Li"], "venue": "In Proceedings of the 21st International Conference on Scientific and Statistical Database Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Logical-shapelets: an expressive primitive for time series classification", "author": ["Abdullah Mueen", "Eamonn J. Keogh", "Neal Young"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Advances in kernel methods. chapter Fast training of support vector machines using sequential minimal optimization, pages 185\u2013208", "author": ["John C. Platt"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Fast shapelets: A scalable algorithm for discovering time series shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "Proceedings of the 13th SIAM International Conference on Data Mining,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["Thanawin Rakthanmanon", "Bilson Campana", "Abdullah Mueen", "Gustavo Batista", "Brandon Westover", "Qiang Zhu", "Jesin Zakaria", "Eamonn Keogh"], "venue": "In Proceedings of the 18th ACM SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Discovering similar multidimensional trajectories", "author": ["M. Vlachos", "G. Kollios", "D. Gunopulos"], "venue": "In Data Engineering,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach", "author": ["Fei Wang", "Noah Lee", "Jianying Hu", "Jimeng Sun", "Shahram Ebadollahi"], "venue": "In Proceedings of ACM SIGKDD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Bag-of-words representation for biomedical time series classification", "author": ["Jin Wang", "Ping Liu", "Mary F.H. She", "Saeid Nahavandi", "Abbas Kouzani"], "venue": "Biomedical Signal Processing and Control,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["Xiaoyue Wang", "Abdullah Mueen", "Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Eamonn Keogh"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Time series classification using support vector machine with gaussian elastic metric kernel", "author": ["Dongyu Zhang", "Wangmeng Zuo", "David Zhang", "Hongzhi Zhang"], "venue": "In ICPR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Therefore traditionally strong classifiers, such as Support Vector Machines (SVM), fail to excel in terms of prediction accuracy [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "An early pioneer method called Dynamic Time Warping (DTW), (still considered competitive [11, 27]), computes the similarity among series by re-aligning the time indexes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 26, "context": "An early pioneer method called Dynamic Time Warping (DTW), (still considered competitive [11, 27]), computes the similarity among series by re-aligning the time indexes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 17, "context": "The algorithm explores all the possible relative alignments of time indexes of two series and picks the one yielding the minimum overall distance [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 6, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 7, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 3, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 31, "context": "For instance, the invariant kernel functions have been applied to measure instance similarities in the projected space of a non-linear SVM [32, 14].", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "For instance, the invariant kernel functions have been applied to measure instance similarities in the projected space of a non-linear SVM [32, 14].", "startOffset": 139, "endOffset": 147}, {"referenceID": 12, "context": "Another paper proposes to generate all pattern variations as new instances and inflate the training set [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Those local segments are converted into symbolic words and a histogram of the words\u2019 occurrences is built [22, 23].", "startOffset": 106, "endOffset": 114}, {"referenceID": 22, "context": "Those local segments are converted into symbolic words and a histogram of the words\u2019 occurrences is built [22, 23].", "startOffset": 106, "endOffset": 114}, {"referenceID": 4, "context": "Another study constructs a supervised codebook generated from local patterns, which is used to create features for a random forest classifiers [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 17, "context": "The most popular of those approaches is the Dynamic Time Warping (DTW) measure [18], which overcomes deficiencies of the L2 norm distance by aligning the time indexes of two series instances.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 47, "endOffset": 55}, {"referenceID": 30, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 47, "endOffset": 55}, {"referenceID": 26, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "Other similarity based distance metrics have extended the edit distance of strings into the time-series domain [7, 8].", "startOffset": 111, "endOffset": 117}, {"referenceID": 7, "context": "Other similarity based distance metrics have extended the edit distance of strings into the time-series domain [7, 8].", "startOffset": 111, "endOffset": 117}, {"referenceID": 27, "context": "Furthermore, the longest common subsequence of time series has also been used as an indication of similarity [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "Moreover, similarities of sequential data have been measured using sparse spatial sample kernels [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "CID significantly improves the accuracy of DTW [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 3, "context": "CID significantly improves the accuracy of DTW [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 13, "context": "For instance DTW has been used as a SVM kernel [14], even though the resulting kernel is not positive semi definite.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "Consecutively, another study has proposed a Gaussian elastic kernel [32].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "A method which produces a semi-definite kernel is called global alignment kernels and builds an average statistics from all possible warping paths of time indexes [9].", "startOffset": 163, "endOffset": 166}, {"referenceID": 12, "context": "In addition, another study has inflated the training set by adding new instances that represent variations of original training data [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Dimensionality reduction has been used to project the time series into a low-rank data space [17], while a recent method incorporates class segregation into the projection [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Dimensionality reduction has been used to project the time series into a low-rank data space [17], while a recent method incorporates class segregation into the projection [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "accuracy [26, 24].", "startOffset": 9, "endOffset": 17}, {"referenceID": 23, "context": "accuracy [26, 24].", "startOffset": 9, "endOffset": 17}, {"referenceID": 14, "context": "A related study detects a set of shapelets and transforms the series data into a new representation, defined by the distance to those shapelets [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 5, "context": "For instance frequencies of time-series motifs have been fed into standard classifiers [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "Another attempt has focused on building histograms of local patterns represented as symbolic words [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 177, "endOffset": 185}, {"referenceID": 22, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 177, "endOffset": 185}, {"referenceID": 29, "context": "One similar bag-of-words approach has also been applied to long biomedical data [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "Moreover, a bag-of-patterns study proposes to extract series segments of various lengths and positions and generate a supervised codebook of those patterns [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "That study demonstrates considerable improvements over baselines in terms of prediction accuracy [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 19, "context": "A shift-invariant sparse coding of signals has been proposed for reconstructing noisy or missing series segments [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data [2], and also invariant features of audio data [16].", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data [2], and also invariant features of audio data [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 28, "context": "Moreover, a temporal decomposition of multivariate streams has been used to discover patterns in patients\u2019 clinical events [29].", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "In order to avoid this bottleneck, we propose to update the memberships in pairs, inspired by a similar strategy known as the Sequential Minimal Optimization algorithm [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The answer is addressed via a technique utilized to find the initial centroids in a clustering setup [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "The patterns (analogy to centroids) are initialized to segments with a probability proportional to the distance to all the other segments [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "1 Baselines We compared the prediction accuracy of our method, denoted Invariant Factorization (INFA), against the following six state of the art baselines: \u2022 TSBF: The bag-of-features framework for time series (TSBF) uses a supervised codebook to extract features for a random forest classifier [5].", "startOffset": 296, "endOffset": 299}, {"referenceID": 18, "context": "\u2022 SSSK: Sparse Spatial Similarity Kernel (SSSK) measures sequence similarity through sampling sequence features at different resolutions [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "\u2022 BOW: The Bag of Words (BOW) method decomposes the series into local SAX words and uses a histogram representation of words as the new feature representation [22, 23].", "startOffset": 159, "endOffset": 167}, {"referenceID": 22, "context": "\u2022 BOW: The Bag of Words (BOW) method decomposes the series into local SAX words and uses a histogram representation of words as the new feature representation [22, 23].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "\u2022 DTW: Dynamic Time Warping (DTW) computes the best alignment of time indexes resulting in the mininal distance [18, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 26, "context": "\u2022 DTW: Dynamic Time Warping (DTW) computes the best alignment of time indexes resulting in the mininal distance [18, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 3, "context": "\u2022 CID: The complexity invariant distance (CID) adds a L2-based total variation regularization term into the DTW distance [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 25, "context": "\u2022 FSH: Fast shapelet (FSH) extracts the most discriminative segment of the series dataset, such that the distance from the dataset instances to the optimal shapelet can be used as a feature for classification [26].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "The applied classifier was a polynomial kernel SVM with a polynomial degree being 3 and the complexity parameter 1, which are competitive SVM settings for the UCR collection [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "In order to compare multiple classifiers across a large number of datasets we follow the established benchmarks of counting wins and Wilcoxon\u2019s Signed-Rank test for statistical significance [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}, {"referenceID": 3, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}, {"referenceID": 25, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}], "year": 2013, "abstractText": "Time-series classification is an important domain of machine learning and a plethora of methods have been developed for the task. In comparison to existing approaches, this study presents a novel method which decomposes a time-series dataset into latent patterns and membership weights of local segments to those patterns. The process is formalized as a constrained objective function and a tailored stochastic coordinate descent optimization is applied. The time-series are projected to a new feature representation consisting of the sums of the membership weights, which captures frequencies of local patterns. Features from various sliding window sizes are concatenated in order to encapsulate the interaction of patterns from different sizes. Finally, a large-scale experimental comparison against 6 state of the art baselines and 43 real life datasets is conducted. The proposed method outperforms all the baselines with statistically significant margins in terms of prediction accuracy.", "creator": "LaTeX with hyperref package"}}}