{"id": "1312.6807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "Iterative Nearest Neighborhood Oversampling in Semisupervised Learning from Imbalanced Data", "abstract": "Transductive graph-based semi-supervised learning methods usually build an undirected graph utilizing both labeled and unlabeled samples as vertices. Those methods propagate label information of labeled samples to neighbors through their edges in order to get the predicted labels of unlabeled samples. Most popular semi-supervised learning approaches are sensitive to initial label distribution happened in imbalanced labeled datasets. The class boundary will be severely skewed by the majority classes in an imbalanced classification. In this paper, we proposed a simple and effective approach to alleviate the unfavorable influence of imbalance problem by iteratively selecting a few unlabeled samples and adding them into the minority classes to form a balanced labeled dataset for the learning methods afterwards. The experiments on UCI datasets and MNIST handwritten digits dataset showed that the proposed approach outperforms other existing state-of-art methods.", "histories": [["v1", "Tue, 24 Dec 2013 12:24:30 GMT  (478kb)", "http://arxiv.org/abs/1312.6807v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fengqi li", "chuang yu", "nanhai yang", "feng xia", "guangming li", "fatemeh kaveh-yazdy"], "accepted": false, "id": "1312.6807"}, "pdf": {"name": "1312.6807.pdf", "metadata": {"source": "CRF", "title": "Iterative Nearest Neighborhood Oversampling in Semi-supervised Learning from Imbalanced Data", "authors": ["Fengqi Li", "Chuang Yu", "Nanhai Yang", "Feng Xia", "Guangming Li", "Fatemeh Kaveh-Yazdy"], "emails": ["f.xia@ieee.org"], "sections": [{"heading": null, "text": "In this context, it should be noted that this is an attempt to put into practice the knowledge acquired in the past, and that it is an attempt to put into practice the knowledge acquired in the past."}, {"heading": "2 Related Work", "text": "In recent years, however, graph-based SSL approaches have attracted increasing attention due to their good performance and ease of implementation. SSL graph-based approaches see both label-based and label-based samples as wells in a graph and build edges between the pair-by-pair wells. Transductive graph-based SSL methods predict that the label will be used for the unlabel-based partition or label propagation."}, {"heading": "3 Motivation", "text": "Transductive graph-based SSL methods propagate label information of labeled samples to their neighbors through edges to obtain the predicted labels of unlabeled samples. Once there is an unbalanced distribution of classes in labeled data sets, the class boundary is greatly shifted to the majority samples, which have a greater ability to affect the predicted labels of unlabeled samples. We draw the influence of the imbalance classification on three popular transductive GSSL methods on the two-dimensional toy samples in Figure 1. The symbols \"\" and \"stand for class\" + 1 \"and\" -1 \"in raw data, and we use solid symbols to represent labeled data. Originally, class\" + 1 \"contains a labeled sample, and class\" -1 \"contains ten labeled data. Figure 1 shows that the effects of the imbalance distribution of labels on the aforementioned algorithms are even transferred to well-separated data sets."}, {"heading": "4 Iterative Nearest Neighborhood Oversampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Graph-based SSL Formulation", "text": "Given a raw dataset containing n samples in which XL = (x1, y1), (x2, y2), (xl, yl), the labeled dataset is cardinality | XL | = l and XU = {xl + 1, xl + 2,..., the unlabeled dataset is cardinality | XU | = u, where l + u = n and typically l < < u. Define the labeling correlation to labeled records is YL = {y1, y2,..., yl}, where the labeling is graph {1,2,..., c} (i = 1... l) and c is the number of different classes. The goal of SSL is to YU = {yl + 1, yl + 2, yl + u} of the unlabeled samples by combining XL and XU."}, {"heading": "4.2 Methodology", "text": "In the real world, it is as if we describe the unbalanced learning scenarios in the balanced learning scenarios as follows: We assume that there are many undescribed patterns around a described sample collection that are located in an undescribed space. Therefore, we can select a few undescribed patterns for an undescribed class to form a balanced pattern. We describe our over-sample model as follows: If we look at the multi-level classification scenarios, we leave r = {r1, r2, rc} denotes the size of the described classes in which rj = 1... c) is the number of described samples in class j. We use standard variants var (r), we represent the quantity of described samples in each class, and the imbalance of the ratio var (r)."}, {"heading": "4.3 Complexity analysis", "text": "Our method queries k neighbors of each labeled sample in each iteration, the query time is (rsum + rsum \u00b7 k) \u00b7 k, and the time of iteration in the worst situation is rmax \u00b7 c \u2212 rmin \u00b7 (c-1), with rmax and rmin being the largest and smallest number of labels. Time complexity of the suggested algorithm is 2 3 3max maxmax max max max (1) () () 2r O c r k k k O ck r r r r r r r r r k r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r) r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r) r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r) r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r) r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r"}, {"heading": "5 Experiments", "text": "There are many accuracy yardsticks to evaluate the two-class classification problems, such as precision, recall, geometric mean (G mean) and F measure [12]. To evaluate classification performance, we calculate accuracy using a confusion matrix as shown in Table 1. According to Table 1, many performance yardsticks can be derived and domain classes are considered positive and negative classes. One of the most common criteria is the general accuracy used for two class classification problems in this document. It can be defined as TP TNaccuracyp n (4), which provides an easy way to describe the performance of a classifier using a given dataset. Meanwhile, we apply the RBF core function Wij = exp (\u0442d | xid - whered | 2 / \u03c32) to calculate the similarity between the samples, and the parameters \u03b1 = 0.99 in LGC [9] and KerimAM = 14 are an average during the [GT3AM and GT9]."}, {"heading": "5.1 UCI datasets", "text": "First, we evaluate the effectiveness of our proposed INNO algorithm combined with SSL methods on IRIS and IONOSPHER data sets from UCI repository. IRIS data set consists of three different categories of flowers, \"setosa,\" \"versicolor\" and \"virginica.\" Each category contains 50 samples, and the characteristic dimension of a sample is 4. We fix the number of labeled samples in the category \"setosa\" at 10, but range the number of labeled samples in the category \"versicolor\" from 1 to 10, and also range the number of labeled samples in the category \"virginica\" from 10 to 20. Stop parameter s is set to zero in this data set, namely the balance algorithm stops when the labeled data sets are fully balanced. Set \u03c3 = 0.26 in RBF kernel function and the neighbors k = 5 in k-NN. \"Figure 5 (a) shows the classification result on HERIS HEROSPISPI121B data set."}, {"heading": "5.2 Handwritten digit dataset", "text": "In this section, we perform two classification experiments using MNIST handwritten digit data set. MNIST handwritten data set has a training set of 60,000 samples and a test set of 10,000 samples, each sample has a pixel of 28 x 28, and each pixel is a grayscale range from 0 to 255. In these experiments, we combined training and test set, and the pixel values of the image were used directly as characteristics, i.e. each sample has a characteristic number of 784. We randomly selected 200 samples of the number \"0\" to \"9\" from the entire data set, so that the sample set has 2000 samples. We used the parameter \u03c3 = 380 that Zhu et al. [8] set in MNIST and the stop parameter s = 0 in INNO. We selected the digit \"5\" to \"9\" to perform a 5-class experiment, and the digit \"LGM\" experiment does not show that GT5 is the classification algorithm."}, {"heading": "5.3 Parameter discussion", "text": "Intuitively, the number of neighbors, k, will affect the result of the Indonesian origin category. To confirm this assumption, we are conducting an experiment with the aforementioned datasets."}, {"heading": "6 Conclusion", "text": "In classification scenarios, state-of-the-art semi-supervised learning methods estimate a classification function based on the assumption that there is a balanced distribution in marked and unmarked data sets. However, the class boundary will be greatly distorted by majority classes in an imbalance between classes, as demonstrated by experiments with UCI data sets and MNIST digit recognition. As the distortion caused by disproportionately unbalanced data sets will be adjusted by re-sampling or re-weighting, we proposed the INNO algorithm to solve this imbalance problem easily and effectively, eliminating the \"injustice\" that unbalanced marked data sets go hand in hand with popular graph-based SSL methods. Our method iteratively searches the neighbors of marked samples in minority classes to locate the closest neighbor for all marked minority class samples, and we can therefore try to locate the unmarked boundary for the classified sample or the unmarked boundary in the SSL."}], "references": [{"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6kopf", "A. Zien", "Eds"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Semi-supervised learning for imbalanced sentiment classification.", "author": ["S. Li", "Z. Wang"], "venue": "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Parameter Estimation of One-Class SVM on Imbalance Text Classification.", "author": ["L. Zhuang", "H. Dai"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A comprehensive survey of data mining-based fraud detection research.", "author": ["C. Phua", "V. Lee", "K. Smith", "R. Gayler"], "venue": "Artificial Intelligence Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Outside the Closed World: On Using Machine Learning for Network Intrusion Detection", "author": ["R. Summer", "V. Paxson"], "venue": "Proc. 2010 IEEE Symp. Security and Privacy, IEEE CS Press, 2010, pp. 305\u2013316.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Classification and knowledge discovery in protein databases.", "author": ["P. Radivojac", "N.V. Chawla", "A.K. Dunker", "Z. Obradovic"], "venue": "J. Biomedical Informatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Toward Scalable Learning with Non-Uniform Class and Cost Distributions", "author": ["P. Chan", "S. Stolfo"], "venue": "Proc. Int\u2019l Conf. Knowledge Discovery and Data Mining, pp. 164-168, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "A Multiple Resampling Method for Learning from Imbalanced Data  Sets", "author": ["A. Estabrooks", "T. Jo", "N. Japkowicz"], "venue": "Computational Intelligence, vol. 20, pp. 18-36, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Machine Learning Research., vol. 7, pp. 2399\u20132434, Nov. 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 912\u2013919, 2003", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems, vol. 16. Cambridge, MA: MIT Press, 2004, pp. 321\u2013328.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Y. Yang", "F. Nie", "D. Xu", "J. Luo", "Y. Zhuang", "Y. Pan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 723\u2013742, Apr. 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning from Imbalanced Data", "author": ["H. Haibo", "E.A. Garcia"], "venue": "IEEE Trans. on Knowl. Data Eng.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Exploratory under-sampling for class-imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "In Proc. of the International Conference on Data Mining (ICDM),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Graph transduction via alternating minimization", "author": ["J. Wang", "T. Jebara", "S.-F. Chang"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 1144\u20131151, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Label diagnosis through self tuning for web image search", "author": ["J. Wang", "Y.-G. Jiang", "S.-F. Chang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1390\u20131397, 2009", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "author": ["M. Plessis", "M. Sugiyama"], "venue": "Proc. Int. Conf. Mach. Learn., pp. 823-830, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble Manifold Regularization", "author": ["D. Tao", "C. Xu", "L. Yang", "X.S. Hua"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., pp 1227-1223, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "MWMOTE-Majority Weighted Minority Oversampling Technique for Imbalaced Data Set Learning", "author": ["S. Barua", "M. Isam", "X. Yao", "K. Murase"], "venue": "IEEE Trans. Knowl. Data Eng., vol ,2012", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Sample-based software defect prediction with active and semi-supervised learning", "author": ["M. Li", "H. Zhang", "R. Wu", "Z.H. Zhou"], "venue": "Autom Softw Eng, vol 19, pp. 201\u2013230, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "SMOTE: Synthetic Minority Over-Sampling Technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "J. Artificial Intelligence Research, vol. 16, pp. 321-357, 2002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust and Scalable Graph-Based Semisupervised Learning", "author": ["W. Liu", "J. Wang", "S.-F. Chang."], "venue": "Proceedings of the IEEE , vol.100, no.9, pp.2624-2638, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "An Instance-Weighting Method to Induce Cost-Sensitive Trees", "author": ["K.M. Ting"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 14, no. 3, pp. 659-665, 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A Kernel-Based Two-Class Classifier for Imbalanced Data Sets", "author": ["X. Hong", "S. Chen", "C.J. Harris"], "venue": "IEEE Trans. Neural Networks, vol. 18, no. 1, pp. 28-41, Jan. 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning on the Border: Active Learning in Imbalanced Data Classification", "author": ["S. Ertekin", "J. Huang", "L. Bottou", "L. Giles"], "venue": "Proc. ACM Conf. Information and Knowledge Management, pp. 127-136, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "In machine learning, semi-supervised learning (SSL) methods [1] train a classifier by combining labeled and unlabeled samples together, which has attracted attentions due to their advantage of reducing the need for labeled samples and improving accuracy in comparison with most of supervised learning methods.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "However, although most existing methods have shown encouraging success in many applications, they assume that the distribution between classes in both labeled and unlabeled datasets are balanced, which may not satisfy the reality [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "In many real-world applications such as text classification [3], credit card fraud detection [4], intrusion detection [5], and classification of protein databases [6], datasets are imbalanced and skewed.", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "The imbalance learning problem [7] puzzles many machine learning methods established on the assumption that every class has the same or approximate same quantity of samples in raw data.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 139, "endOffset": 147}, {"referenceID": 24, "context": "These methods can be classified into re-sampling [8], cost-sensitive learning [23], kernel-based learning [24] and active learning methods [20, 25].", "startOffset": 139, "endOffset": 147}, {"referenceID": 18, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 41, "endOffset": 49}, {"referenceID": 20, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "Re-sampling methods include oversampling [19, 21] and undersampling [14] approaches, in which the class distribution is balanced by adding a few of samples to minority class or removing a few of samples from majority class, respectively.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 13, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 18, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 22, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "Most existing studies on imbalanced classification focus on supervised imbalanced classification instances [8, 14, 19, 23], and there are few studies on semi-supervised methods for imbalanced classification [4].", "startOffset": 207, "endOffset": 210}, {"referenceID": 14, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 15, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 16, "context": "The bias caused by differing class balances can be systematically adjusted by re-weighting [15, 16] or re-sampling [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "combined active learning with SSL methods that sample a few of most helpful modules for learning a prediction model in [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "Transductive graph-based SSL methods predict the label for unlabeled samples via graph partition or label propagation using a small portion of seed labels provided by initial labeled dataset [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "Popular transductive algorithms include the Gaussian fields and harmonic function based method (GFHF) [10], the local and global consistency method (LGC) [11],the graph transduction via alternating minimization (GTAM) [15], popular inductive methods consist of transductive support vector machines (TSVM) and manifold regularization [9].", "startOffset": 333, "endOffset": 336}, {"referenceID": 17, "context": "Recent researches on graph-based SSL include ensemble manifold regularization [18] and relevance feedback [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Recent researches on graph-based SSL include ensemble manifold regularization [18] and relevance feedback [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "However, these graph-based SSL methods developed with smoothness, clustering assumption, and manifold assumption [1] frequently perform a bad classification if provided an imbalanced dataset.", "startOffset": 113, "endOffset": 116}, {"referenceID": 14, "context": "[15] proposed a node regularizer to balance the inequitable influence of labels from different classes, which can be regarded as a re-weighting method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Its modified algorithm LDST [16] revises the unilateral greedy search strategy into a bidirectional manner, which can drive wrong label correction in addition to eliminate imbalance problem.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "[2] proposed semi-supervised learning with dynamic subspace generation algorithm based on undersampling to handle imbalanced classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "The conventional transductive graph-based SSL algorithms, such as GFHF [10], LGC [11], and GTAM [15], fail to give the acceptable classification result.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "[19] reported some cases of insufficiencies and inappropriateness in existing methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Plessis and Sugiyama [17] proposed a semi-supervised learning method to estimate the class ratio for test dataset by combining train and test dataset in supervised learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "Therefore, classic oversampling methods [19, 23] is not capable in this situation, because they need to judge of the informative data close to class boundary, in order to synthetically generate new samples for minority class.", "startOffset": 40, "endOffset": 48}, {"referenceID": 22, "context": "Therefore, classic oversampling methods [19, 23] is not capable in this situation, because they need to judge of the informative data close to class boundary, in order to synthetically generate new samples for minority class.", "startOffset": 40, "endOffset": 48}, {"referenceID": 11, "context": "5 Experiments There are many accuracy measures for evaluating the two-class classification problems, such as precision, recall, geometric-mean (G-mean) and F-measure [12].", "startOffset": 166, "endOffset": 170}, {"referenceID": 8, "context": "99 in LGC [9] and \u03bc = 99 in GTAM [14] during the experiments and the results are the average results of 50 runs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "99 in LGC [9] and \u03bc = 99 in GTAM [14] during the experiments and the results are the average results of 50 runs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "proposed in CMN method in [10] to solve the negative influence to the classification result caused by imbalanced labeled dataset.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "[8] set in MNIST and set the stop parameter s = 0 in INNO.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "And as well taking into account of the randomness of labeled samples selected from raw data by algorithm, we cannot foresee the occurrence of such a situation [17], namely the imbalanced output may also occur even the stop condition s is 0, so the classification accuracy ranges in a certain scope.", "startOffset": 159, "endOffset": 163}], "year": 2013, "abstractText": "Transductive graph-based semi-supervised learning methods usually build an undirected graph utilizing both labeled and unlabeled samples as vertices. Those methods propagate label information of labeled samples to neighbors through their edges in order to get the predicted labels of unlabeled samples. Most popular semi-supervised learning approaches are sensitive to initial label distribution happened in imbalanced labeled datasets. The class boundary will be severely skewed by the majority classes in an imbalanced classification. In this paper, we proposed a simple and effective approach to alleviate the unfavorable influence of imbalance problem by iteratively selecting a few unlabeled samples and adding them into the minority classes to form a balanced labeled dataset for the learning methods afterwards. The experiments on UCI datasets and MNIST handwritten digits dataset showed that the proposed approach outperforms other existing state-of-art methods.", "creator": null}}}