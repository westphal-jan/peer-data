{"id": "1312.6838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "Greedy Column Subset Selection for Large-scale Data Sets", "abstract": "In today's information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively-distributed data, which is formally known as the Column Subset Selection (CSS) problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.", "histories": [["v1", "Tue, 24 Dec 2013 15:10:23 GMT  (131kb,D)", "http://arxiv.org/abs/1312.6838v1", "Under consideration for publication in Knowledge and Information Systems"]], "COMMENTS": "Under consideration for publication in Knowledge and Information Systems", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["ahmed k farahat", "ahmed elgohary", "ali ghodsi", "mohamed s kamel"], "accepted": false, "id": "1312.6838"}, "pdf": {"name": "1312.6838.pdf", "metadata": {"source": "CRF", "title": "Greedy Column Subset Selection for Large-scale Data Sets", "authors": ["Ahmed K. Farahat", "Ahmed Elgohary", "Ali Ghodsi", "Mohamed S. Kamel"], "emails": ["afarahat@uwaterloo.ca", "aelgohary@uwaterloo.ca", "aghodsib@uwaterloo.ca", "mkamel@uwaterloo.ca"], "sections": [{"heading": null, "text": "Ahmed K. Farahat Department of Electrical and Computer Engineering University of Waterloo, Ontario, Canada N2L 3G1 Tel.: + 1 519-888-4567 Email: afarahat @ uwaterloo.caAhmed Elgohary Department of Electrical and Computer Engineering University of Waterloo, Ontario, Canada N2L 3G1 Tel.: + 1 519-888-4567 Email: aelgohary @ uwaterloo.caAli Ghodsi Department of Statistics and Actuarial Science University of Waterloo, Ontario, Canada N2L 3G1 Tel.: + 1 519-888-4567 Email: aelgohary @ uwaterloo.caAli Ghodsi Department of Statistics and Actuarial Science University of Waterloo Waterloo, Ontario, Canada N2L 3G1 Tel.: + 1 519-888-4567 Email: aelgohary @ uwaterloloo.ca.cai @ waterca.ca.cai @ Waterloo, Ontarial Science University of Waterloo Waterloo Waterloo, Ontario, Ontario, Canada N2G1 Tel.: + 1 519-888-4567 Email: aelgohary @ uwaterloloo.ca.cai @ Waterloo, Ontarial Science University of Waterloo Waterloo Waterloo, Ontario, Ontario 3G2G1 Subscription 3G1 Tel."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2 Notations", "text": "Scalars are denoted by small letters (e.g. m, n), sets are denoted by characters (e.g. S, R), vectors by small, bold italics (e.g. f, g), and matrices by uppercase letters (e.g. A, B). The subscript (i) indicates that the variable corresponds to the i-th data block in the distributed environment. Furthermore, the following notations are used: For a set S: | S | the cardinality of the set; for a vector x: xi i-th element of x."}, {"heading": "3 Background", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "4 Greedy Column Subset Selection", "text": "\"I'm not going to tell you what to do, I'm not going to tell you what to do,\" he said, \"I'm not going to tell you what to do, I'm not going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you what to do, I'm going to tell you to tell you what to tell you to do, I'm going to tell you to tell you to tell you to tell you.\""}, {"heading": "5 Distributed Column Subset Selection on MapReduce", "text": "This section describes a MapReduce algorithm for the distributed column selection problem. Given a large data matrix A with columns distributed across different machines, the goal is to select a subset of columns S from A so that the CSS criterion F (S) is nearly error-free. Another approach to performing the distributed column selection is to select different subsets of column selection from the sub-matrices stored on different machines. The selected subsets are then sent to a central machine, where an additional selection step is optionally performed to filter irrelevant or redundant column selections. Let A (i) store the sub-matrix of those in the machine that optimizes the following function."}, {"heading": "6 Related Work", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "A = QW, A\u03a0 = QW\u03a0 = QR", "text": "The permutation of the columns of the embedding matrix W produces an upper triangular matrix. The greedy CSS algorithm differs from the greedy algorithm proposed by C-ivril and Magdon-Ismail [7] [8], since the latter depends on the initial calculation of the singular value decomposition of the data matrix, which is computationally complex, especially for large matrices. The proposed algorithm is also more efficient than the recently proposed volume sampling algorithms [14] [29]. Compared to other CSS methods, the distributed algorithm proposed in this paper is designed to be MapReduce efficient. At the selection stage, representative columns are selected on the basis of a common representation whose common representation is based on a random projection."}, {"heading": "7 Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "8 Conclusion", "text": "In this paper, a novel algorithm is proposed that greedily selects a subset of columns from a data matrix to minimize the reconstruction error of the data matrix. The algorithm is based on a novel recursive formula for the reconstruction error of the data matrix, which enables a greedy selection criterion to be efficiently calculated at each iteration. Furthermore, this paper presents a precise and efficient MapReduce algorithm for selecting a subset of columns from a massively distributed matrix. The algorithm begins by learning a precise representation of the data matrix by means of random projection, and then selects columns from each submatrix that best correspond to this precise approximation. A centralized selection step is then performed on the columns selected from different submatrices. To facilitate the implementation of the proposed method, a novel algorithm is proposed for generalized selection from different submatrices."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences 66(4), 671\u2013687", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Computing rank-revealing QR factorizations of dense matrices", "author": ["C. Bischof", "G. Quintana-Ort\u0301\u0131"], "venue": "ACM Transactions on Mathematical Software (TOMS) 24(2), 226\u2013253", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Near optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201911), pp. 305 \u2013314", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "CoRR abs/0812.4293", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the Twentieth Annual ACMSIAM Symposium on Discrete Algorithms (SODA\u201909), pp. 968\u2013977", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustered subset selection and its applications on it service metrics", "author": ["C. Boutsidis", "J. Sun", "N. Anerousis"], "venue": "Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management (CIKM\u201908), pp. 599\u2013608", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Deterministic sparse column based matrix reconstruction via greedy approximation of SVD", "author": ["A. \u00c7ivril", "M. Magdon-Ismail"], "venue": "Proceedings of the 19th International Symposium on Algorithms and Computation (ISAAC\u201908), pp. 414\u2013423. Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Column subset selection via sparse approximation of SVD", "author": ["A. \u00c7ivril", "M. Magdon-Ismail"], "venue": "Theoretical Computer Science 421(0), 1 \u2013 14", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Rank revealing QR factorizations", "author": ["T. Chan"], "venue": "Linear Algebra and Its Applications 88, 67\u201382", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.Y. Chen", "Y. Song", "H. Bai", "C.J. Lin", "E. Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(3), 568 \u2013586", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures and Algorithms 22(1), 60\u201365", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51(1), 107\u2013113", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science and Technology 41(6), 391\u2013407", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201910), pp. 329 \u2013338", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "Theory of Computing 2(1), 225\u2013247", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\u201906), pp. 1117\u20131126. ACM, New York, NY, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P. Drineas", "A. Frieze", "R. Kannan", "S. Vempala", "V. Vinay"], "venue": "Machine Learning 56(1-3), 9\u201333", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "author": ["P. Drineas", "R. Kannan", "M. Mahoney"], "venue": "SIAM Journal on Computing 36(1), 158\u2013183", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Subspace sampling and relative-error matrix approximation: Column-based methods", "author": ["P. Drineas", "M. Mahoney", "S. Muthukrishnan"], "venue": "Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 316\u2013326. Springer Berlin / Heidelberg", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Embed and conquer: Scalable embeddings for kernel k-means on mapreduce", "author": ["A. Elgohary", "A.K. Farahat", "M.S. Kamel", "F. Karray"], "venue": "CoRR abs/1311.2334", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Pairwise document similarity in large collections with MapReduce", "author": ["T. Elsayed", "J. Lin", "D.W. Oard"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers (HLT\u201908), pp. 265\u2013268", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast clustering using MapReduce", "author": ["A. Ene", "S. Im", "B. Moseley"], "venue": "Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201911), pp. 681\u2013689", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed column subset selection on mapreduce", "author": ["A.K. Farahat", "A. Elgohary", "A. Ghodsi", "M.S. Kamel"], "venue": "Proceedings of the Thirteenth IEEE International Conference on Data Mining (ICDM\u201913)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "An efficient greedy method for unsupervised feature selection", "author": ["A.K. Farahat", "A. Ghodsi", "M.S. Kamel"], "venue": "Proceedings of the Eleventh IEEE International Conference on Data Mining (ICDM\u201911), pp. 161 \u2013170", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient greedy feature selection for unsupervised learning", "author": ["A.K. Farahat", "A. Ghodsi", "M.S. Kamel"], "venue": "Knowledge and Information Systems 35(2), 285\u2013310", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast Monte-Carlo algorithms for finding low-rank approximations", "author": ["A. Frieze", "R. Kannan", "S. Vempala"], "venue": "Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201998), pp. 370 \u2013378", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Matrix Computations, 3rd edn", "author": ["G. Golub", "C. Van Loan"], "venue": "Johns Hopkins Univ Pr", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient algorithms for computing a strong rank-revealing QR factorization", "author": ["M. Gu", "S.C. Eisenstat"], "venue": "SIAM Journal on Scientific Computing 17(4), 848\u2013869", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1996}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["V. Guruswami", "A.K. Sinop"], "venue": "Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\u201912), pp. 1207\u20131214", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["N. Halko", "P.G. Martinsson", "Y. Shkolnisky", "M. Tygert"], "venue": "SIAM Journal on Scientific Computing 33(5), 2580\u20132594", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Face recognition using Laplacianfaces", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H. Zhang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 27(3), 328\u2013340", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1988}, {"title": "Hadi: Fast diameter estimation and mining in massive graphs with hadoop", "author": ["U. Kang", "C. Tsourakakis", "A. Appel", "C. Faloutsos", "J. Leskovec"], "venue": "CMU-ML-08-117", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "A model of computation for MapReduce", "author": ["H. Karloff", "S. Suri", "S. Vassilvitskii"], "venue": "Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\u201910), pp. 938\u2013948", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "CLUTO - a clustering toolkit", "author": ["G. Karypis"], "venue": "Tech. Rep. #02-017, University of Minnesota, Department of Computer Science", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering by means of medoids", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": "Tech. rep., Technische Hogeschool, Delft (Netherlands). Department of Mathematics and Informatics", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1987}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 27(5), 684\u2013698", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Reuters-21578 text categorization test collection distribution", "author": ["D. Lewis"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1999}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research 5, 361\u2013397", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Very sparse random projections", "author": ["P. Li", "T.J. Hastie", "K.W. Church"], "venue": "Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining (KDD\u201906), pp. 287\u2013296", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Handbook of Matrices", "author": ["H. L\u00fctkepohl"], "venue": "John Wiley & Sons Inc", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Robust regression on mapreduce", "author": ["X. Meng", "M. Mahoney"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 888\u2013896", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "On the existence and computation of rank-revealing LU factorizations", "author": ["C. Pan"], "venue": "Linear Algebra and its Applications 316(1), 199\u2013222", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "S. Baker", "M. Bsat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 25(12), 1615\u20131618", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2003}, {"title": "Parallel large scale feature selection for logistic regression", "author": ["S. Singh", "J. Kubica", "S. Larsen", "D. Sorokina"], "venue": "Proceedings of the SIAM International Conference on Data Mining pp. 1171\u20131182", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 30(11), 1958\u20131970", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Hadoop: The Definitive Guide, 1st edn", "author": ["T. White"], "venue": "O\u2019Reilly Media, Inc.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable maximum clique computation using mapreduce", "author": ["J. Xiang", "C. Guo", "A. Aboulnaga"], "venue": "Data Engineering (ICDE), 2013 IEEE 29th International Conference on, pp. 74\u201385", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 31, "context": "For instance, the traditional clustering algorithms such as k-means [32] tend to produce centroids which encode information about thousands of data instances.", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "Even clustering methods that use data instances as prototypes, such as k-medoid [36], learn only one representative for each cluster, which is usually not enough to capture the insights of the data instances in that cluster.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "On the other hand, traditional dimension reduction algorithms such as Latent Semantic Analysis (LSA) [13] tend to learn a few latent concepts in the feature space.", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "1 A preliminary version of this paper appeared as [23]", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem [26], [19] [6] [5] [3].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem [26], [19] [6] [5] [3].", "startOffset": 176, "endOffset": 180}, {"referenceID": 5, "context": "This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem [26], [19] [6] [5] [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem [26], [19] [6] [5] [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "This problem can be generally formulated as the selection of a subset of columns from a data matrix, which is formally known as the Column Subset Selection (CSS) problem [26], [19] [6] [5] [3].", "startOffset": 189, "endOffset": 192}, {"referenceID": 11, "context": "In order to alleviate these problems, MapReduce [12] was introduced to simplify large-scale data analytics over a distributed environment of commodity machines.", "startOffset": 48, "endOffset": 52}, {"referenceID": 46, "context": "Currently, MapReduce (and its open source implementation Hadoop [47]) is considered the most successful and widely-used framework for managing big data processing jobs.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "The Column Subset Selection (CSS) problem can be generally defined as the selection of the most representative columns of a data matrix [6] [5] [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "The Column Subset Selection (CSS) problem can be generally defined as the selection of the most representative columns of a data matrix [6] [5] [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "The Column Subset Selection (CSS) problem can be generally defined as the selection of the most representative columns of a data matrix [6] [5] [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 25, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 260, "endOffset": 264}, {"referenceID": 16, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 265, "endOffset": 269}, {"referenceID": 17, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 270, "endOffset": 274}, {"referenceID": 18, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 280, "endOffset": 284}, {"referenceID": 5, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 285, "endOffset": 288}, {"referenceID": 4, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 289, "endOffset": 292}, {"referenceID": 2, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 293, "endOffset": 296}, {"referenceID": 7, "context": "Although different criteria for column subset selection can be defined, a common criterion that has been used in much recent work measures the discrepancy between the original matrix and the approximate matrix reconstructed from the subset of selected columns [26] [17] [18] [19] [15] [6] [5] [3] [8].", "startOffset": 297, "endOffset": 300}, {"referenceID": 5, "context": "Some of the recent work on the CSS problem [6] [5] [3] derives theoretical bounds for both the Frobenius and spectral norms of the residual matrix.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "Some of the recent work on the CSS problem [6] [5] [3] derives theoretical bounds for both the Frobenius and spectral norms of the residual matrix.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Some of the recent work on the CSS problem [6] [5] [3] derives theoretical bounds for both the Frobenius and spectral norms of the residual matrix.", "startOffset": 51, "endOffset": 54}, {"referenceID": 26, "context": "The matrix Q can be obtained by applying an orthogonalization algorithm such as the Gram-Schmidt algorithm to the columns of A:S , or by calculating the Singular Value Decomposition (SVD) or the QR decomposition of A:S [27].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "The selected columns can also be used to calculate a column-based lowrank approximation of A [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "[3] can be used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This procedure results in a rank-k approximation of A within the column space of A:S that achieves the minimum reconstruction error in terms of Frobenius norm [3]: T \u2217 = arg min T, rank(T )=k \u2016A\u2212A:ST\u20162F .", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "MapReduce [12] was presented as a programming model to simplify large-scale data analytics over a distributed environment of commodity machines.", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "Currently, MapReduce has been successfully used for scaling various data analysis tasks such as regression [42], feature selection [45], graph mining [33,48], and most recently kernel k-means clustering [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 44, "context": "Currently, MapReduce has been successfully used for scaling various data analysis tasks such as regression [42], feature selection [45], graph mining [33,48], and most recently kernel k-means clustering [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "Currently, MapReduce has been successfully used for scaling various data analysis tasks such as regression [42], feature selection [45], graph mining [33,48], and most recently kernel k-means clustering [20].", "startOffset": 150, "endOffset": 157}, {"referenceID": 47, "context": "Currently, MapReduce has been successfully used for scaling various data analysis tasks such as regression [42], feature selection [45], graph mining [33,48], and most recently kernel k-means clustering [20].", "startOffset": 150, "endOffset": 157}, {"referenceID": 19, "context": "Currently, MapReduce has been successfully used for scaling various data analysis tasks such as regression [42], feature selection [45], graph mining [33,48], and most recently kernel k-means clustering [20].", "startOffset": 203, "endOffset": 207}, {"referenceID": 20, "context": "For complex analytical tasks, multiple jobs are typically chained together [21] and/or many rounds of the same job are executed on the input data set [22].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "For complex analytical tasks, multiple jobs are typically chained together [21] and/or many rounds of the same job are executed on the input data set [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 33, "context": "[34] defined a set of computational constraints that ensure the scalability and the efficiency of MapReduce-based analytical tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The minimization of this criterion is a combinatorial optimization problem whose optimal solution can be obtained inO ( nmnl ) [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 23, "context": "[24] [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24] [25].", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "Let S = BRR \u2212 B PRB \u22121 PPBPR be the Schur complement [41] of BPP in B.", "startOffset": 53, "endOffset": 57}, {"referenceID": 40, "context": "Using the block-wise inversion formula [41], B\u22121 can be calculated as:", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Random projection [11] [1] [40] is a well-known technique for dealing with the curse-of-the-dimensionality problem.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Random projection [11] [1] [40] is a well-known technique for dealing with the curse-of-the-dimensionality problem.", "startOffset": 23, "endOffset": 26}, {"referenceID": 39, "context": "Random projection [11] [1] [40] is a well-known technique for dealing with the curse-of-the-dimensionality problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "It has been shown that applying random projection \u03a9 to X preserves the pairwise distances between vectors in the row space of X with a high probability [11]:", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Examples of such matrices are Gaussian random matrices [11], uniform random sign (\u00b11) matrices [1], and sparse random sign matrices [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "Examples of such matrices are Gaussian random matrices [11], uniform random sign (\u00b11) matrices [1], and sparse random sign matrices [40].", "startOffset": 95, "endOffset": 98}, {"referenceID": 39, "context": "Examples of such matrices are Gaussian random matrices [11], uniform random sign (\u00b11) matrices [1], and sparse random sign matrices [40].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "3 The in-memory summation can also be replaced by a MapReduce combiner [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "[26] was the first to suggest the idea of randomly sampling l columns from a matrix and using these columns to calculate a rank-k approximation of the matrix (where l \u2265 k).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "was followed by different papers [17] [18] that enhanced the algorithm by proposing different sampling probabilities and deriving better error bounds for the reconstruction error.", "startOffset": 33, "endOffset": 37}, {"referenceID": 17, "context": "was followed by different papers [17] [18] that enhanced the algorithm by proposing different sampling probabilities and deriving better error bounds for the reconstruction error.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "[19] proposed a subspace sampling method which samples columns using probabilities proportional to the norms of the rows of the top k right singular vectors of A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] [15] proposed an adaptive sampling method which updates the sampling probabilities based on the columns selected so far.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] [15] proposed an adaptive sampling method which updates the sampling probabilities based on the columns selected so far.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "In the area of numerical linear algebra, the column pivoting method exploited by the QR decomposition [27] permutes the columns of the matrix based on their norms to enhance the numerical stability of the QR decomposition algorithm.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "The Rank-Revealing QR (RRQR) decomposition [9] [28] [2] [43] is a category of QR decomposition methods which permute columns of the data matrix while imposing additional constraints on the singular values of the two sub-matrices of the upper-triangular matrix R corresponding to the selected and non-selected columns.", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "The Rank-Revealing QR (RRQR) decomposition [9] [28] [2] [43] is a category of QR decomposition methods which permute columns of the data matrix while imposing additional constraints on the singular values of the two sub-matrices of the upper-triangular matrix R corresponding to the selected and non-selected columns.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "The Rank-Revealing QR (RRQR) decomposition [9] [28] [2] [43] is a category of QR decomposition methods which permute columns of the data matrix while imposing additional constraints on the singular values of the two sub-matrices of the upper-triangular matrix R corresponding to the selected and non-selected columns.", "startOffset": 52, "endOffset": 55}, {"referenceID": 42, "context": "The Rank-Revealing QR (RRQR) decomposition [9] [28] [2] [43] is a category of QR decomposition methods which permute columns of the data matrix while imposing additional constraints on the singular values of the two sub-matrices of the upper-triangular matrix R corresponding to the selected and non-selected columns.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "It has been shown that the constrains on the singular values can be used to derive an theoretical guarantee for the column-based reconstruction error according to spectral norm [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "[6] proposed a deterministic column subset selection method which first groups columns into clusters and then selects a subset of columns from each cluster.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "\u00c7ivril and Magdon-Ismail [7] [8]", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "\u00c7ivril and Magdon-Ismail [7] [8]", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "[3] presented a column subset selection algorithm which first calculates the top-k right singular values of the data matrix (where k is the target rank) and then uses deterministic sparsification methods to select l \u2265 k columns from the data matrix.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Deshpande and Rademacher [14] presented a polynomial-time deterministic algorithm for volume sampling with a theoretical guarantee for l = k.", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "Quite recently, Guruswami and Sinop [29] presented a deterministic algorithm for volume sampling with theoretical guarantee for l > k.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "[5] proposed a two-stage hybrid algorithm for column subset selection which runs in O ( min ( nm,nm )) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The greedy CSS algorithm differs from the greedy algorithm proposed by \u00c7ivril and Magdon-Ismail [7] [8] in that the latter depends on first calculating the Singular Value Decomposition of the data matrix, which is computationally complex, especially for large matrices.", "startOffset": 96, "endOffset": 99}, {"referenceID": 7, "context": "The greedy CSS algorithm differs from the greedy algorithm proposed by \u00c7ivril and Magdon-Ismail [7] [8] in that the latter depends on first calculating the Singular Value Decomposition of the data matrix, which is computationally complex, especially for large matrices.", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "The proposed algorithm is also more efficient than the recently proposed volume sampling algorithms [14] [29].", "startOffset": 100, "endOffset": 104}, {"referenceID": 28, "context": "The proposed algorithm is also more efficient than the recently proposed volume sampling algorithms [14] [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "This is more efficient than the work of \u00c7ivril and Magdon-Ismail [8] which selects columns based on the leading singular vectors.", "startOffset": 65, "endOffset": 68}, {"referenceID": 37, "context": "The Reuters-21578 is the training set of the Reuters-21578 collection [38].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "The pre-processed versions of Reviews and LA1 that are distributed with the CLUTO Toolkit [35] were used.", "startOffset": 90, "endOffset": 94}, {"referenceID": 43, "context": "The PIE-20 and YaleB-38 are pre-processed subsets of the CMU PIE [44] and Extended Yale Face [37] data sets respectively.", "startOffset": 65, "endOffset": 69}, {"referenceID": 36, "context": "The PIE-20 and YaleB-38 are pre-processed subsets of the CMU PIE [44] and Extended Yale Face [37] data sets respectively.", "startOffset": 93, "endOffset": 97}, {"referenceID": 30, "context": "[31] to evaluate different face recognition algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "The RCV1-200K is a subset of the RCV1 data set [39] which has been prepared and used by Chen et al.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "[10] to evaluate parallel spectral clustering algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The TinyImages-1M data set contains 1 million images that were sampled from the 80 million tiny images data set [46] and converted to grayscale.", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "\u2013 qr: is the QR decomposition with column pivoting [27] implemented by the MATLAB qr function.", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "\u2013 SRRQR: is the strong rank-revealing QR decomposition [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Algorithm 4 of [28] was implemented in MATLAB.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "In this implementation, the MATLAB qr function is first used to calculate the QR decomposition with column pivoting and then the columns are swapped using the criterion specified by Gu and Eisenstat [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "\u2013 ApproxSVD: is the sparse approximation of Singular Value Decomposition (SVD) [7] [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "\u2013 ApproxSVD: is the sparse approximation of Singular Value Decomposition (SVD) [7] [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "The use of the generalized CSS algorithm is equivalent to, but more efficient than, the algorithm proposed by \u00c7ivril and Magdon-Ismail [7] [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": "The use of the generalized CSS algorithm is equivalent to, but more efficient than, the algorithm proposed by \u00c7ivril and Magdon-Ismail [7] [8].", "startOffset": 139, "endOffset": 142}, {"referenceID": 29, "context": "Since the calculation of exact SVD is computationally complex, the Stochastic SVD algorithm [30] is used to approximate the leading singular values and vectors of the data matrix.", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] was not included in the comparison as its implementation is not available.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "7 9 In the implemented code, the efficient recursive formulas in Section 4 of [28] are used to implement the update of QR decomposition and the swapping criterion.", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "10 In [4] (a newer version of [5]), Boutsidis et al.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "10 In [4] (a newer version of [5]), Boutsidis et al.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Although the SRRQR algorithm achieves the theoretical guarantee presented in [5], the MATLAB qr function is used in the conducted experiments as it is much faster and it achieves comparable accuracy for the experimented data sets.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "\u2013 DistApproxSVD: is an extension of the centralized algorithm for sparse approximation of Singular Value Decomposition (SVD) [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "The use of the distributed CSS algorithm extends the original algorithm proposed by \u00c7ivril and Magdon-Ismail [8] to work on distributed matrices.", "startOffset": 109, "endOffset": 112}, {"referenceID": 29, "context": "For the methods that require the calculations of Singular Value Decomposition (SVD), the Stochastic SVD (SSVD) algorithm [30] is used to approximate the leading singular values and vectors of the data matrix.", "startOffset": 121, "endOffset": 125}], "year": 2013, "abstractText": "In today\u2019s information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively-distributed data, which is formally known as the Column Subset Selection (CSS) problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank Ahmed K. Farahat Department of Electrical and Computer Engineering University of Waterloo Waterloo, Ontario, Canada N2L 3G1 Tel.: +1 519-888-4567 E-mail: afarahat@uwaterloo.ca Ahmed Elgohary Department of Electrical and Computer Engineering University of Waterloo Waterloo, Ontario, Canada N2L 3G1 Tel.: +1 519-888-4567 E-mail: aelgohary@uwaterloo.ca Ali Ghodsi Department of Statistics and Actuarial Science University of Waterloo Waterloo, Ontario, Canada N2L 3G1 Tel.: +1 519-888-4567 x37316 E-mail: aghodsib@uwaterloo.ca Mohamed S. Kamel Department of Electrical and Computer Engineering University of Waterloo Waterloo, Ontario, Canada N2L 3G1 Tel.: +1 519-888-4567 x35761 E-mail: mkamel@uwaterloo.ca ar X iv :1 31 2. 68 38 v1 [ cs .D S] 2 4 D ec 2 01 3 2 Ahmed K. Farahat et al. approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.", "creator": "LaTeX with hyperref package"}}}