{"id": "1312.7292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Dec-2013", "title": "Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless Sensor Networks", "abstract": "The aim in this paper is to allocate the `sleep time' of the individual sensors in an intrusion detection application so that the energy consumption from the sensors is reduced, while keeping the tracking error to a minimum. We propose two novel reinforcement learning (RL) based algorithms - with both infinite horizon discounted and long-run average cost objectives - for solving this problem. All our algorithms incorporate feature-based representations to handle the curse of dimensionality associated with the underlying partially-observable Markov decision process (POMDP). Further, the feature selection scheme used in our algorithms intelligently manages the energy cost and tracking cost factors, which in turn assists the search for the optimal sleeping policy. The first algorithm in either (discounted or average) setting is based on Q-learning, while the second algorithm is a novel two-timescale algorithm that performs on-policy Q-learning. The latter possesses theoretical convergence guarantees, unlike the former Q-learning based algorithm. We also extend these algorithms to a setting where the intruder's mobility model is not known by incorporating a stochastic iterative scheme for estimating the mobility model. The simulation results on a synthetic 2-d network setting suggest that our proposed algorithms result in better tracking accuracy at the cost of a few additional sensors, in comparison to a recent prior work. We also observe empirically that the proposed model estimation scheme converges to the true model.", "histories": [["v1", "Fri, 27 Dec 2013 16:13:07 GMT  (561kb)", "http://arxiv.org/abs/1312.7292v1", null], ["v2", "Sun, 23 Mar 2014 13:44:48 GMT  (50kb)", "http://arxiv.org/abs/1312.7292v2", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["prashanth l a", "abhranil chatterjee", "shalabh bhatnagar"], "accepted": false, "id": "1312.7292"}, "pdf": {"name": "1312.7292.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Sleep\u2013Wake Scheduling in Sensor Networks", "authors": ["Prashanth L A", "Abhranil Chatterjee", "Shalabh Bhatnagar"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 2.72 92v1 [cs.SY] Keywords: sensor networks, sleep-wake scheduling, Reinforcement Learning, Q-learning, Simultaneous Disorder, Function Approximation, SPSA."}, {"heading": "1 Introduction", "text": "(.). (.). (.). (.). (.). (.). (.). (.). (.).). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.).). \"(.).).\" (. \"(.).\" (.).). \"(.).\" (.).). \"(.).\" (.).). \"(.).\" (.). \"(.).). (.).\" (.).). (.).). (.). \"(.).). (.). (.).). (.).). (.).). (.). (.). (.).). (.).). (.).). (.).). (.). (.).).).). (.).). (.)"}, {"heading": "2 Literature Review", "text": "In recent years, it has become clear that the people in the countries in which they live are not able to integrate, and that they are not able to integrate themselves, but that they are able to integrate themselves, and that they are able to integrate themselves, \"he said in an interview with the\" S\u00fcddeutsche Zeitung \"(Saturday)."}, {"heading": "3 Problem Formulation", "text": "We consider a central control setting for a sensor network with N sensors and, for simplicity, assume that the sensors cover the area of interest without overlapping. Each sensor can be either awake (i.e. active) or asleep. The control center gathers sensor information during each period and decides on the sleep guidelines for each sensor for the next period. At any time, the location of the intruder can be any of the N cells corresponding to the N sensors. The movement of the intruder is determined by a probability transition matrix P of size N \u00d7 N, with each input Pij being the probability of the intruder moving from location i to j."}, {"heading": "3.1 States, Actions and Observations", "text": "\"It's about being able to put ourselves at the top of the world to change the world,\" he says."}, {"heading": "3.2 Average Cost Objective", "text": "The long-term average cost J (\u03c0) for a given policy \u03c0 is defined as follows: J (\u03c0) = lim N \u2192 \u221e 1NN \u2212 1 \u2211 n = 0g (sn, an), (4) starting from each given state i (i.e. with s0 = i). In the above case, the policy \u03c0 = {\u03c01, \u03c02,..} with a practicable action at any time n.Let h (i) be the differential cost function corresponding to the state i, conceived under the policy. Then h (i) = definitive solution n = 1E [g (sn, an) \u2212 J (p) is the optimal action at any time p.Let h (i) is the differential cost function corresponding to the state i, conceived under the policy. Then h (i) = definitive solution n = 1E [g, an) \u2212 J (\u03c0) the system is equivalent."}, {"heading": "3.3 The Discounted Cost Objective", "text": "For a policy \u03c0, let us define the value function V \u03c0: S \u2192 R as follows: V \u03c0 (i) = E [\u221e \u2211 m = 0\u03b3mg (sm, am) | X0 = i], (10) for all i-S. In the example above, \u03b3 (0, 1) is a given discount factor. The goal is then to find an optimal value function V \u0445: S \u2192 R, i.e. V \u0445 (i) = min \u03c0 \u0445V \u03c0 (i) \u0445 (i) \u0445 = V (\u03c0), (11), where \u03c0 is the optimal policy, i.e. the one for which V \u0445 is the value function. It is well known, see Puterman [1994], that the optimal value function V \u0445 (\u00b7) fulfils the following Bellman equation of optimality in the case of discount costs: V \u0445 (i) = min a \u0445 A (i) g (i, a) + vice versa (j, a)."}, {"heading": "4 Algorithms for Average Cost Setting", "text": "Before describing our algorithms, we first discuss a well-known RL algorithm called Q-Learning, which uses Q-Learning, and then discuss the difficulty of using this algorithm on a high-dimensional state space (as is the case with the sleep-wake-control MDP).Q-Learning with full representationThis algorithm is based on the relative Q-value iteration algorithm. Let sn + 1 denotes the state of the system to Instant (n + 1) when the state to Instant n i and Action is selected. Let Qn (i, a) denotes the Q-value estimation to Instant n associated (i, a).The relative Q-value iteration (RQVI) scheme isQn + 1 (i) = g (i, a) + (i, a) + JP (i, a) min b) b) b-value estimation to Instant n associated (i, a)."}, {"heading": "4.1 Algorithm Structure", "text": "Both of our algorithms parameterise the Q function using a linear approximation architecture as follows: Q (s, a) \u2248 \u03b8T\u03c3s, a, \u0432 S, a (s). (15) In the figure above, a is a predetermined d-dimensional feature vector associated with the state stamp (s, a), where d < < | S \u00b7 A (S) | and \u03b8 is a tunable d-dimensional parameter. Our algorithms are online, incremental and obtain the sleep guideline by sampling from a system orbit. After considering a simulated sample of the single-stage costs, the parameter \u03b8 is updated in a negative downward direction in both of our algorithms as follows: \u03b8n + 1 = one (n) sleep procedure (sn, an)."}, {"heading": "4.2 Q-learning based Sleep\u2013wake Algorithm (QSA-A)", "text": "Let sn, sn + 1 use the state measured online in moments n, n + 1. Let us use any fixed state in S. The QSA-A algorithm uses the following update rule: \u03b8n + 1 = \u03b8n + a (n) \u03c3sn, an (g (sn, an) + min v \u00b2 A (sn + 1) \u03b8Tn\u03c3sn + 1, v \u2212 min r \u00b2 A (s) \u03b8Tn\u03c3s, r \u2212 \u03b8 T n\u03c3sn, an), (17) where \u03b80 is arbitrarily set. In (17) the act an in state sn is chosen according to a \"greed policy,\" i.e. with a probability of (1 \u2212 \u0432), a greedy act chosen by an = argminv \u00b2 A (sn), v and with the probability that an act in A (sn) is randomly selected."}, {"heading": "4.3 Feature selection", "text": "The idea behind the feature selection scheme is to select an energy-efficient sleep configuration, i.e. a configuration that keeps as many sensors awake as possible to track the intruder while at the same time having minimal energy costs. This is done by first truncating the actions in such a way that only those actions are selected that ensure that the energy costs are in the immediate vicinity of the tracking error, and then selecting from among the replaceable actions an action that minimizes the approximate Q-value. Cutting the actions is done as follows: Consider an action at (i) for the sensor i in due time. The sum of probabilities that the intruder is at location i over time moment 1,... one (i) is a measure of the tracking error (denotes T in fig. 2). On the other hand, the energy saved by having the sensor i-sleep for a (i) unit of time is proportional to can (i) (1) in Abb."}, {"heading": "4.4 Two-timescale Q-learning based sleep\u2013wake algorithm (TQSA-A)", "text": "It is theoretically difficult to prove that it converges to the optimal solution. Indeed, there have been cases where it has proved to be unstable (cf. Prashanth and Bhatnagar [2011b]). One possible reason for this problem is the off-policy characteristic of the QSA-A accompanied by the solution problem introduced by the feature-based Q-learning with functional approximation. Here, the off-policy problem results from the presence of the min operation in the Qlearning algorithm, which introduces non-linearity into the updating rule. Note: If instead of the min operation actions are selected according to a given policy, then the Q-learning update would resemble a temporal difference (TD) for the joint (state-action) Markov chain. It was shown in Tsitsiklis and Van Roy [1997] atTD with linear functional approximation."}, {"heading": "5 Algorithms for Discounted Cost Setting", "text": "In this section, we present two algorithms for sleep-wake planning with the aim of minimizing a discounted cost target described in Section 3.3. The overall structure of both algorithms follows the scheme provided for in Algorithm 1. Compared to the average cost algorithms described above, however, the parameter \u03b8 is updated here in a different way to meet the discounted cost target."}, {"heading": "5.1 Q-learning based Sleep\u2013wake Scheduling Algorithm (QSA-D)", "text": "As in the case of average cost determination, the Q-Learning algorithm cannot be used without functional approximation due to the size of the state sphere of action. Q-Learning's functional approximation variant in discounted cost determination parameterizes the Q-values in a manner similar to the average cost determination, i.e. according to (15). The algorithm works with a single online simulation curve of states and actions and updates the regulations according to phenomena + 1 = phenomena + a (n) phenomena, on (g (sn, an) + \u03b3 min b \u00b2 A (sn + 1) phenomena Tn\u03c3sn + 1, b \u2212 \u03b8 N, a), (26), where phenomena 0 is arbitrarily set. In the above case, sn and sn + 1 denote the state at the instants and n + 1 or events respectively the ninth update of the parameter."}, {"heading": "5.2 Two-timescale Q-learning based sleep\u2013wake scheduling algorithm (TQSA-D)", "text": "As with the average cost calculation, the Q-Learning algorithm with function approximation in the discounted setting does not guarantee convergence due to the non-political problem. A variant of Q-Learning Bhatnagar and Lakshmanan [2012] was recently proposed and has proven to be convergent, using a stochastic approximation (SPSA) with Hadamard matrix-based deterministic disturbance sequences Bhatnagar et al. [2003] The TQSA-D algorithm is a two-step stochastic approximation algorithm that uses a linear approximation architecture and parameterizes policy. \u2212 As in the case of TQSA-A, we assume that policy (s, a) is continuously differentiable in the parameter arrangement (Qs, a) and the function approximation parameters T (s, a)."}, {"heading": "6 Mobility Model Estimation", "text": "The algorithms described in the previous sections are based on knowledge of the transition dynamics (matrix P) of the Markov chain that controls the intruder movement. In this section, we present a method for estimating P and combining it with the sleep-wake scheduling algorithms described in the previous section. We assume that P is stationary, i.e. it does not change with time. The estimation procedure for P is online and convergent. Combined with the sleep-wake scheduling algorithms, it is done via the multi-time scale stochastic approach. Essentially, we perform the estimation procedure for P on the faster time scale, while the updates for the parameters of the sleep-wake scheduling algorithms are performed on the slower time span."}, {"heading": "7 Simulation Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Implementation", "text": "We are conducting our experiments on a 2-D network (see Figure 1) of 121 sensors (i.e., an 11-11 grid) where the sensor regions overlap, with each sensor region intersecting with that of its adjacent nodes. However, in particular, the sensor regions of the sensors inside the network overlap with eight adjacent nodes. We implemented our sleep-wake schema algorithms, which are proposed in Fuemmeler and Veeravalli. [2008] Note that for each of these algorithms, knowledge of the intruder's mobility model is assumed. Let's briefly recall these algorithms below: This algorithm approaches the state (3) according to PtP = ptP and then tries to find the time for each intruder."}, {"heading": "7.2 Results", "text": "We use the number of sensors awake and the number of detections per time step as performance indicators for the comparison of the different sleep / wake algorithms SA. While the former is the ratio of the total number of sensors in awake state to the number of actual time steps, the latter is the ratio of the number of successful detections of the intruder to the number of time steps. Fig. 4 presents the number of sensors in awake state and the number of detections per time step for each of the algorithms examined in the average cost setting, while Fig. 7 presents similar results for the algorithms in the discounted cost settings. We observe that compared to the QMDP algorithms, a slightly higher tracking accuracy is achieved compared to the cost of some additional sensors in awake state. On the other hand, our algorithms show a better trade between energy costs and tracking accuracy compared to the FCR algorithms."}, {"heading": "8 Conclusions", "text": "We investigated the problem of optimizing sleep times in a sensor network to detect intrusions. Following a POMDP formulation similar to that in Fuemmeler et al. [2011], our goal in this paper was to minimize certain long-term average and discounted cost targets, which in turn enabled us to study both transient and stationary system histories. We proposed two novel reinforcement algorithms for both settings. All of our algorithms are model-free, online and easy to implement. The first algorithm is based on the well-known Q-Learning algorithm with functional approximation; on the other hand, the second algorithm in both is a two-dimensional Q-Learning algorithm that has theoretical convergence guarantees, as opposed to the first algorithms that rely on the well-known Q-Learning algorithms."}, {"heading": "A Appendix", "text": "The approaches of the ODE approach are for the analysis of convergence of assumptions and recursions (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study (23) in the main study in the main study (23) in the main study (23) in the main study (23) in the main study in the main study (23) in the main study (23) in the main study (23) in the main study in the main study (23) in the main study."}], "references": [{"title": "Learning algorithms for Markov decision processes with average cost", "author": ["J. Abounadi", "D. Bertsekas", "V.S. Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Abounadi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Abounadi et al\\.", "year": 2002}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In ICML, pages 30\u201337,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "Multiple abstraction levels in performance analysis of wsn monitoring systems", "author": ["M. Beccuti", "D. Codetta-Raiteri", "G. Franceschinis"], "venue": "In International ICST Conference on Performance Evaluation Methodologies and Tools,", "citeRegEx": "Beccuti et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beccuti et al\\.", "year": 2009}, {"title": "Dynamic Programming and Optimal Control, vol. II, 3rd edition", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2007\\E", "shortCiteRegEx": "Bertsekas.", "year": 2007}, {"title": "Neuro-Dynamic Programming", "author": ["Dimitri P. Bertsekas", "John N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences", "author": ["S. Bhatnagar", "M.C. Fu", "S.I. Marcus", "I. Wang"], "venue": "ACM Transactions on Modeling and Computer Simulation (TOMACS),", "citeRegEx": "Bhatnagar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2003}, {"title": "Stochastic Approximation: A Dynamical Systems Viewpoint", "author": ["V.S. Borkar"], "venue": null, "citeRegEx": "Borkar.,? \\Q2008\\E", "shortCiteRegEx": "Borkar.", "year": 2008}, {"title": "Smart sleeping policies for energy efficient tracking in sensor networks", "author": ["J.A. Fuemmeler", "V.V. Veeravalli"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Fuemmeler and Veeravalli.,? \\Q2008\\E", "shortCiteRegEx": "Fuemmeler and Veeravalli.", "year": 2008}, {"title": "Sleep control for tracking in sensor networks", "author": ["J.A. Fuemmeler", "G.K. Atia", "V.V. Veeravalli"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Fuemmeler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fuemmeler et al\\.", "year": 2011}, {"title": "Power conservation and quality of surveillance in target tracking sensor networks", "author": ["C. Gui", "P. Mohapatra"], "venue": "In Proceedings of the international conference on mobile computing and networking,", "citeRegEx": "Gui and Mohapatra.,? \\Q2004\\E", "shortCiteRegEx": "Gui and Mohapatra.", "year": 2004}, {"title": "Energy efficient sleep scheduling based on moving directions in target tracking sensor network", "author": ["B. Jiang", "K. Han", "B. Ravindran", "H. Cho"], "venue": "In IEEE International Symposium on Parallel and Distributed Processing,", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Dynamic clustering for object tracking in wireless sensor networks", "author": ["G. Jin", "X. Lu", "M.S. Park"], "venue": "Ubiquitous Computing Systems,", "citeRegEx": "Jin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2006}, {"title": "Reinforcement learning with function approximation for traffic signal control", "author": ["L.A. Prashanth", "S. Bhatnagar"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "Prashanth and Bhatnagar.,? \\Q2011\\E", "shortCiteRegEx": "Prashanth and Bhatnagar.", "year": 2011}, {"title": "Reinforcement learning with average cost for adaptive control of traffic lights at intersections", "author": ["L.A. Prashanth", "S. Bhatnagar"], "venue": "In 14th International IEEE Conference on Intelligent Transportation Systems (ITSC),", "citeRegEx": "Prashanth and Bhatnagar.,? \\Q2011\\E", "shortCiteRegEx": "Prashanth and Bhatnagar.", "year": 2011}, {"title": "Adaptive sleep-wake control using reinforcement learning in sensor networks", "author": ["L.A. Prashanth", "Abhranil Chatterjee", "Shalabh Bhatnagar"], "venue": "In Sixth International Conference on Communication Systems and Networks (COMSNETS). IEEE,", "citeRegEx": "Prashanth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prashanth et al\\.", "year": 2014}, {"title": "Optimal sleep\u2013wake scheduling for quickest intrusion detection using sensor networks", "author": ["K. Premkumar", "A. Kumar"], "venue": "IEEE INFOCOM, Arizona,", "citeRegEx": "Premkumar and Kumar.,? \\Q2008\\E", "shortCiteRegEx": "Premkumar and Kumar.", "year": 2008}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "An Analysis of Temporal Difference Learning with Function Approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}], "referenceMentions": [{"referenceID": 7, "context": "We formulate this problem as a partially-observable Markov decision process (POMDP) in a manner similar to the one considered in Fuemmeler and Veeravalli [2008]. However, unlike their total cost objective, in this paper we consider infinite horizon average as well as discounted cost objectives.", "startOffset": 129, "endOffset": 161}, {"referenceID": 6, "context": "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation.", "startOffset": 137, "endOffset": 742}, {"referenceID": 6, "context": "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation. Finally, we also consider a setting where the mobility model of the intruder is not available and develop an online scheme that estimates the same. We combine this estimation procedure with the sleep-wake scheduling algorithms mentioned above using a multi-timescale scheme. We study our algorithms on a simple two-dimensional network setting (see Fig. 1) and compare their performance with the QMDP and FCR algorithms from Fuemmeler and Veeravalli [2008]. Our algorithms are seen to be easily implementable, converge rapidly with a short transient period and provide more consistent results than the QMDP and FCR algorithms.", "startOffset": 137, "endOffset": 1228}, {"referenceID": 6, "context": "The resulting algorithm turns out to be a stochastic approximation scheme on the faster timescale, but a stochastic recursive inclusion [Borkar, 2008, Chapter 5] scheme on the slower timescale. A detailed convergence analysis of the overall scheme is presented in the supplementary material. Next, in the discounted POMDP setting, we propose two algorithms. These algorithms can be seen to be the discounted-cost counterparts of the algorithms described above for the average cost setting. The first algorithm proposed is based on Q-learning with linear function approximation, while the second algorithm is an adaptation of the recently proposed two-timescale (convergent) variant of the Q-learning algorithm Bhatnagar and Lakshmanan [2012], with function approximation. Finally, we also consider a setting where the mobility model of the intruder is not available and develop an online scheme that estimates the same. We combine this estimation procedure with the sleep-wake scheduling algorithms mentioned above using a multi-timescale scheme. We study our algorithms on a simple two-dimensional network setting (see Fig. 1) and compare their performance with the QMDP and FCR algorithms from Fuemmeler and Veeravalli [2008]. Our algorithms are seen to be easily implementable, converge rapidly with a short transient period and provide more consistent results than the QMDP and FCR algorithms. Further, we observe that the procedure for estimating the mobility model of the intruder converges empirically to the true model. A short version of this paper with only the average cost criterion and without the convergence proofs is available in Prashanth et al. [2014].", "startOffset": 137, "endOffset": 1670}, {"referenceID": 6, "context": "2 Literature Review Sleep-wake scheduling: A Markov decision process (MDP) model for intrusion detection has also been formulated in Premkumar and Kumar [2008], where the authors present three sleep/wake scheduling algorithms to control the number of sensors in the wake state.", "startOffset": 133, "endOffset": 160}, {"referenceID": 3, "context": "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking.", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking. However, unlike us, their algorithms are under the waking channel assumption, i.e., a setting where the central controller can communicate with a sensor that is in the sleep state. In Jiang et al. [2008], a sleep/wake scheduling algorithm based on the target\u2019s moving direction has been proposed.", "startOffset": 3, "endOffset": 318}, {"referenceID": 3, "context": "In Gui and Mohapatra [2004], the authors present two sleep-wake scheduling algorithms for single object tracking. However, unlike us, their algorithms are under the waking channel assumption, i.e., a setting where the central controller can communicate with a sensor that is in the sleep state. In Jiang et al. [2008], a sleep/wake scheduling algorithm based on the target\u2019s moving direction has been proposed. In Jin et al. [2006], the authors present a heuristic algorithm that uses dynamic clustering of sensors to balance energy cost and tracking error.", "startOffset": 3, "endOffset": 432}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied.", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al.", "startOffset": 3, "endOffset": 208}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution.", "startOffset": 3, "endOffset": 233}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants.", "startOffset": 3, "endOffset": 700}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a \u2018good enough\u2019 policy that minimizes the long-run average sum of this cost. The term \u2018good enough\u2019 here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL.", "startOffset": 3, "endOffset": 1849}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a \u2018good enough\u2019 policy that minimizes the long-run average sum of this cost. The term \u2018good enough\u2019 here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL.", "startOffset": 3, "endOffset": 1874}, {"referenceID": 1, "context": "In Beccuti et al. [2009], the problem of finding an efficient sleep-wake policy for the sensors while maintaining good tracking accuracy by solving an MDP has been studied. In Fuemmeler and Veeravalli [2008], Fuemmeler et al. [2011], a partially observable Markov decision process (POMDP) has been used to model the problem of sleepwake scheduling for single object tracking when the sensing ranges of the sensors are non-overlapping and several sub-optimal algorithms are proposed for its solution. Here, the sub-optimal algorithms have been used as the traditional dynamic programming techniques cannot be used owing to the curse of dimensionality. In comparison to Fuemmeler and Veeravalli [2008], which is the closest related work, we would like to remark that the algorithms proposed there, for instance, QMDP , attempt to solve a balance equation for the total cost in an approximate fashion at each time instant and no information about the solution thus obtained is carried forward to the future instants. On the other hand, the algorithms that we propose are learning algorithms that observe the samples of a cost function from simulation and through incremental updates find a \u2018good enough\u2019 policy that minimizes the long-run average sum of this cost. The term \u2018good enough\u2019 here refers to the solution of a balance equation for the long term costs, where function approximations are employed to handle the curse of dimensionality. Reinforcement Learning: MDP presents a useful framework for modeling real-time control problems. However, in practice, the transition dynamics of the MDP is unavailable and reinforcement learning (RL) approaches which are essentially simulation-based sample-path techniques to obtain a good policy in the long run, provide an efficient alternative. The reader is referred to Bertsekas and Tsitsiklis [1996], Sutton and Barto [1998] for a comprehensive (text book) introduction to RL. Q-learning Watkins and Dayan [1992] is a well-known RL algorithm for finding the optimal policy.", "startOffset": 3, "endOffset": 1962}, {"referenceID": 1, "context": "However, for problems involving high-dimensional state spaces, the Q-learning algorithm with function approximation may diverge or may show large oscillations, Baird [1995]. This is primarily due to the inherent nonlinearity in the Q-learning update rule resulting from the explicit maximization/minimization in the update procedure.", "startOffset": 160, "endOffset": 173}, {"referenceID": 1, "context": "However, for problems involving high-dimensional state spaces, the Q-learning algorithm with function approximation may diverge or may show large oscillations, Baird [1995]. This is primarily due to the inherent nonlinearity in the Q-learning update rule resulting from the explicit maximization/minimization in the update procedure. A two-timescale variant of the Q-learning algorithm, proposed in Bhatnagar and Lakshmanan [2012], avoids this problem by using two timescales, where on the faster timescale the policy parameter is tuned in the negative gradient direction using SPSA estimates of the gradient and on the slower timescale, a TD-like update for the parameters is performed.", "startOffset": 160, "endOffset": 431}, {"referenceID": 7, "context": "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1.", "startOffset": 37, "endOffset": 69}, {"referenceID": 7, "context": "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value \u01eb otherwise.", "startOffset": 37, "endOffset": 661}, {"referenceID": 7, "context": "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value \u01eb otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak\u22121), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is \u015dk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above.", "startOffset": 37, "endOffset": 1164}, {"referenceID": 7, "context": "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value \u01eb otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak\u22121), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is \u015dk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above. Note that pk = (pk(1), ..., pk(N)) is the distribution at time step k of the object being in one of the locations 1, 2, ..., N and evolves according to pk+1 = elk+1I{rk+1(lk+1)=0} + pkPI{rk+1(lk+1)>0}, (3) where ei denotes an N -dimensional unit vector with 1 in the ith position and 0 elsewhere. The idea behind the evolution of pk is as follows: (i) the first term refers to the case when the location of the intruder is known, i.e., the sensor at lk+1 is in the wake state; (ii) the second term refers to the case when intruder\u2019s location is not known and hence, the intruder transitions to the next distribution pk+1 from the current pk via the transition probability matrix P . Note that the evolution of pk in our setting differs from Fuemmeler and Veeravalli [2008], as we do not have the termination state.", "startOffset": 37, "endOffset": 2082}, {"referenceID": 7, "context": "Note that, unlike the formulation in Fuemmeler and Veeravalli [2008], we do not consider a special termination state which indicates that the intruder has left the system1. The states, actions and single-stage cost function together constitute an MDP. However, since it is not possible to track the intruder at each time instant (i.e., lk is not known for all k) as the sensors at the location from where the intruder passes at a given time instant may be in the sleep state, the problem falls under the realm of MDPs with imperfect state information, or alternatively partially observed MDP (POMDP). Following the notation from Fuemmeler and Veeravalli [2008], the observation zk available to the control center is given by zk = (sk, ok), where sk is as before and ok = lk if the intruder location is known, or a special value \u01eb otherwise. Thus, the total information available to the control center at instant k is given by Ik = (z0, . . . , zk, a0, . . . , ak\u22121), where I0 denotes the initial state of the system. The action ak specifies the chosen sleep configuration of the n sensors and is a function of Ik. As pointed out in Fuemmeler and Veeravalli [2008], in the above POMDP setting, a sufficient statistic is \u015dk = (pk, rk), where pk = P ( lk| Ik) and rk is the remaining sleep time mentioned above. Note that pk = (pk(1), ..., pk(N)) is the distribution at time step k of the object being in one of the locations 1, 2, ..., N and evolves according to pk+1 = elk+1I{rk+1(lk+1)=0} + pkPI{rk+1(lk+1)>0}, (3) where ei denotes an N -dimensional unit vector with 1 in the ith position and 0 elsewhere. The idea behind the evolution of pk is as follows: (i) the first term refers to the case when the location of the intruder is known, i.e., the sensor at lk+1 is in the wake state; (ii) the second term refers to the case when intruder\u2019s location is not known and hence, the intruder transitions to the next distribution pk+1 from the current pk via the transition probability matrix P . Note that the evolution of pk in our setting differs from Fuemmeler and Veeravalli [2008], as we do not have the termination state. With an abuse of terminology, henceforth we shall refer to the sufficient statistic \u015dk as the state vector in the algorithms we propose next. Further, we would like to emphasize here that our algorithms do not require full observation of the state vector. Instead, by an intelligent choice of features that rely only on pk, the algorithms obtain a sleeping policy that works well. 1Since we study long-run average sum of (2) (see (4) below), we can consider the problem of tracking an intruder in an infinite horizon, whereas a termination state in Fuemmeler and Veeravalli [2008] was made necessary as they considered a total cost objective.", "startOffset": 37, "endOffset": 2705}, {"referenceID": 3, "context": "The following Bellman equation is satisfied (Chapter 4 of Bertsekas [2007]): J + h(i) = min a (g(i, a) + \u2211", "startOffset": 58, "endOffset": 75}, {"referenceID": 16, "context": "It is well known, see Puterman [1994], that the optimal value function V (\u00b7) satisfies the following Bellman equation of optimality in the discounted cost case: V (i) = min a\u2208A(i) \uf8eb", "startOffset": 22, "endOffset": 38}, {"referenceID": 0, "context": ", those with i \u2208 S and a \u2208 A(i) using the stochastic approximation version of (13) (see Abounadi et al. [2002]).", "startOffset": 88, "endOffset": 111}, {"referenceID": 12, "context": "we develop a feature based Q-learning algorithm as in Prashanth and Bhatnagar [2011a]. While the full state Qlearning algorithm in (14) cannot be used on even moderately sized sensing regions, its function approximation based variant can be used over larger network settings.", "startOffset": 54, "endOffset": 86}, {"referenceID": 0, "context": "We first present a sleep\u2013wake scheduling algorithm which is the function approximation analogue of the Qlearning with average cost algorithm proposed in Abounadi et al. [2002]. While this algorithm that we refer to as QSA-A is shown to work well in the numerical experiments, it does not possess theoretical convergence guarantees.", "startOffset": 153, "endOffset": 176}, {"referenceID": 12, "context": "Prashanth and Bhatnagar [2011b]).", "startOffset": 0, "endOffset": 32}, {"referenceID": 12, "context": "Prashanth and Bhatnagar [2011b]). A possible reason behind this problem is the off-policy characteristic of QSA-A accompanied by the resolution problem introduced by the feature based Q-learning with function approximation. The off-policy problem here arises because of the presence of the min operation in the Qlearning algorithm that introduces nonlinearity in the update rule. Note that if instead of the min operation, actions are selected according to a given policy, then the Q-learning update would resemble a temporal difference (TD) learning update for the joint (state-action) Markov chain. It has been shown in Tsitsiklis and Van Roy [1997] that", "startOffset": 0, "endOffset": 652}, {"referenceID": 5, "context": "3 of Bhatnagar et al. [2003] for details of the construction).", "startOffset": 5, "endOffset": 29}, {"referenceID": 5, "context": "3 of Bhatnagar et al. [2003] for details of the construction). Given the output from the perturbed simulation, the gradient of the approximate Q-value function Q(s, a) \u2248 \u03b8\u03c3s,a is estimated as: \u2207wQ(s, a) \u2248 \u03b8\u03c3s,a \u03b4 \u2206. (22) It has been shown in Bhatnagar et al. [2003] that an incremental stochastic recursive algorithm that incorporates the RHS of (22) as its update direction essentially performs a search in the gradient direction when \u03b4 is small.", "startOffset": 5, "endOffset": 266}, {"referenceID": 5, "context": "This algorithm uses two-timescale simultaneous perturbation stochastic approximation (SPSA) with Hadamard matrix based deterministic perturbation sequences Bhatnagar et al. [2003]. The TQSA-D algorithm is a two timescale stochastic approximation algorithm that employs a linear approximation architecture and parameterizes the policy.", "startOffset": 156, "endOffset": 180}, {"referenceID": 7, "context": "For the sake of comparison, we also implemented the FCR and QMDP algorithms proposed in Fuemmeler and Veeravalli [2008]. Note that for each of these algorithms, the knowledge of the mobility model of the intruder is assumed.", "startOffset": 88, "endOffset": 120}, {"referenceID": 3, "context": "The fact that a state satisfying such a criterion can be used as the fixed state has been established in Bertsekas [2007].", "startOffset": 105, "endOffset": 122}, {"referenceID": 8, "context": "Following a POMDP formulation similar to the one in Fuemmeler et al. [2011], our aim in this paper was to minimize certain long-run average and discounted cost objectives.", "startOffset": 52, "endOffset": 76}, {"referenceID": 6, "context": "75 of Borkar [2008]. Let Tw : R \u2192 R be the operator given by Tw(J)(i, a) = g(i, a)\u2212 J(\u03c0w)e+ \u2211", "startOffset": 6, "endOffset": 20}], "year": 2013, "abstractText": "The aim in this paper is to allocate the \u2018sleep time\u2019 of the individual sensors in an intrusion detection application so that the energy consumption from the sensors is reduced, while keeping the tracking error to a minimum. We propose two novel reinforcement learning (RL) based algorithms with both infinite horizon discounted and long-run average cost objectives for solving this problem. All our algorithms incorporate feature-based representations to handle the curse of dimensionality associated with the underlying partially-observable Markov decision process (POMDP). Further, the feature selection scheme used in our algorithms intelligently manages the energy cost and tracking cost factors, which in turn assists the search for the optimal sleeping policy. The first algorithm in either (discounted or average) setting is based on Q-learning, while the second algorithm is a novel two-timescale algorithm that performs on-policy Q-learning. The latter possesses theoretical convergence guarantees, unlike the former Q-learning based algorithm. We also extend these algorithms to a setting where the intruder\u2019s mobility model is not known by incorporating a stochastic iterative scheme for estimating the mobility model. The simulation results on a synthetic 2-d network setting suggest that our proposed algorithms result in better tracking accuracy at the cost of a few additional sensors, in comparison to a recent prior work. We also observe empirically that the proposed model estimation scheme converges to the true model.", "creator": "gnuplot 4.2 patchlevel 6 "}}}