{"id": "1312.7606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2013", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "abstract": "We apply diffusion strategies to develop fully-distributed cooperative reinforcement learning solutions, in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy sampling, meaning that the agents can learn to predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents could (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).", "histories": [["v1", "Mon, 30 Dec 2013 00:16:34 GMT  (1232kb)", "https://arxiv.org/abs/1312.7606v1", null], ["v2", "Wed, 5 Nov 2014 19:50:03 GMT  (2971kb)", "http://arxiv.org/abs/1312.7606v2", "36 pages, 4 figures, accepted for publication on IEEE Transactions on Automatic Control"]], "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.DC cs.LG", "authors": ["sergio valcarcel macua", "jianshu chen", "santiago zazo", "ali h sayed"], "accepted": false, "id": "1312.7606"}, "pdf": {"name": "1312.7606.pdf", "metadata": {"source": "CRF", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "authors": ["Sergio Valcarcel"], "emails": ["sergio@gaps.ssr.upm.es;", "santiago@gaps.ssr.upm.es).", "cjs09@ucla.edu;", "sayed@ee.ucla.edu)."], "sections": [{"heading": null, "text": "The results show a clear gain from the collaboration: if the individual agents can increase stability and reduce the variance of prediction errors; but, more importantly, the network is able to approach the optimal solution, even if none of the individual agents can develop the individual learning skills."}, {"heading": "A. Related works", "text": "There are several enlightening papers in the literature that deal with questions of distributed learning, albeit under scenarios and conditions different from those examined in this article. For example, the work in [24] suggests a useful algorithm called QD-Learning, which is a distributed implementation of Q-Learning using consensus-based stochastic approximation. Here, the diffusion strategy proposed here differs in several respects. QD-Learning asymptotically solves the optimal control problem by learning the policies that maximize the long-term reward of the agents. Here, we focus on predicting the long-term reward for a particular policy, which is an important part of the control problem. However, QD-Learning is developed on the assumption of perfect knowledge of the state. Here, we examine the case that the agents know only a functioning representation of the state that is used to build a parametric approximation of the value function that enables us to address major problems associated with the Learning-Q [25-Q]."}, {"heading": "B. Notation", "text": "Lowercase letters are used to denote both scalar values and vectors. Matrices are denoted by uppercase letters. Boldface notation denotes random variables (e.g. s is a realization for s). The state of the environment and the action of an agent are denoted by s or a. In case of slight misuse of the notation, s (i) and a (i) mean the state and action variables at the time i. In addition, if a variable is specific to an agent k, a subscript is added (e.g. sk (i) = s means that the environment seen by the agent k is at the time i).All vectors are column vectors. Superscript \u00b7 denotes the transposition. The identity matrix of size S is denoted by IS, the zero matrix of size M \u00d7 L is denoted by 0M matrix, and 1M and 0M represent vectors of length M."}, {"heading": "II. BELLMAN EQUATION AND VALUE FUNCTIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Markov decision processes (MDP)", "text": "We look at Markov decision-making processes (MDP) [6], [7] which are characterized by a finite series of states of magnitude S, | S |; a finite series of actions A; the core of transition probabilities P (s) which is associated with each transition, so that r (s, a) denotes the probability of transition from one state to another state s (s), given an act a; and the reward function r: S \u00b7 A \u00b7 S \u2192 R, which the agent associated with each transition wants to predict, so that r (s, a, s) denotes the reward that a generic agent receives for the transition from s to s (s). Agents want to predict the reaction of their environment when they follow a stationary policy, so that this approach represents the probability that an agent will choose an action when the environment is in state s. We assume that the finite state chain Markov resulting from the MDP is reducible and eroid."}, {"heading": "C. Approximate value function as a saddle-point problem", "text": "The algorithms save on calculations by relying on features that encompass a space of much lower dimensionality than the size of the original state space. Formally, however, we allow x: S \u2192 RM to show some maps of states to characteristics, so that xs is the feature vector of length M \u00b2 S that represents the state s. Now, it would be efficient to approach the original value function vp (s) as a parametric function of xs, for some parameter vectors w RM. When this is done, the problem of making a prediction (i.e., estimating the value vector vp s becomes the search for a parameter vector w that is optimal in a certain sense. Among many parameter variations, a linear approximation of the form vp s (s) is comprehensively investigated in the literature (see e.g. 26)."}, {"heading": "D. Primal-dual stochastic optimization", "text": "As already mentioned, since the agents have no prior knowledge of the environment, we must replace (24a) - (24b) with constructions that do not depend on quantities. In order to find the solution directly from the samples, we must convert these gradient iterations into stochastic approximations. (24b) The selection of an appropriate weighted standard becomes relevant now. (14) If we choose a weighting matrix D that represents the probability distribution of each state, then we can express the terms in (24a) - (24b) as expectations that we can replace with their sampling estimates. We continue to explain the detailed information. Let us use the weighting matrix in (14) equal to the probability caused by the behavioral policy (which we emphasize with the corresponding superscript), i.e."}, {"heading": "III. MULTI-AGENT LEARNING", "text": "We consider a network of interconnected agents operating in similar but independent MDPs. State-Space S, Action-Space A, and transition probabilities P are the same for each node, but their actions do not affect each other. Thus, the transition probabilities seen by each agent are determined only by his or her own actions, ak (i) This assumption is convenient because it makes the problem stationary without forcing each agent to know the actions and characteristics of each other agent in the network. Agents aim to predict the response of their environment to a common goal policy while following other behavioral policies determined by different behaviors. Motivated by the recent results on network behavior in [21], we find that each agent can contribute to the network by collaborating with his or her own experience."}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "In this section, we will analyze the existence and uniqueness of the optimal solution to the multi-agent learning problem (31). We will expand the energy-saving arguments of [16] - [19] to perform a Mean Square Error Analysis (MSE) of the Diffusion GTD Algorithm (35a) - (35d), and provide convergence guarantees for sufficiently small step variables. We will also obtain closed formulations of Mean Square Deviation (MSD) and analyze the bias of the algorithm. We will rely on some reasonable conditions on the data as explained below."}, {"heading": "A. Data model", "text": "First, we model the sizes in (35a) - (35d) as instantaneous realizations of random variables, which we denote by boldface notation. We aggregate the variables into vectors of length 2M each: \u03b1k, i, \u03b8k, iwk, i, \u0430k, i, \u0432 k, iw, i (38) gk, i + 1, \u2212 \u03b7xk, i \u00b7 \u0445k (i) \u00b7 rk (i + 1) 0M (39) where we are now write ul, i, \u00b5 and \u00b5ctuences, \u03b7\u00b5w, such that \u03b7 > 0 is the step-size ratio between the two adaptation steps. We continue with the following 2M \u00d7 2M coefficient matrix: Gk, i + 1, throuxk k, ix k (i)."}, {"heading": "B. Existence and uniqueness of solution", "text": "The solution of the aggregated dual problem (31) is equivalent to the determination of the saddle points (where, \u03b8o) of the global Lagrangian policy (33). A saddle point of the Lagrangian system must meet the following conditions [40]: L (\u03b8o, where) = min \u03b8 max w L (\u03b8, w) = max w min \u03b8 L (\u03b8, w) (42) These conditions are equivalent to the following system of linear equations: L (\u03b8, w) = X D\u03c6 (X\u03b8 \u2212 r\u03c0 + (IS \u2212 P \u03c0) Xw) = 0M (43)."}, {"heading": "C. Error recursion", "text": "We introduce the following error quantities, which measure the difference between the estimates {\u03b1k, i, \u0395k, i} at a given time i and the optimal solution \u03b1o = col {\u03b8o, where} for each agent k. Subtracting both sides from (41a) - (41b), we obtain the error recursion for the combination step becomes\u03b1 k, i + 1).k, i + \u00b5 (Gk, i + 1\u03b1o + gk, i + 1) (50) On the basis of the fact that clk = 0, if l / \u0441Nk, the error recursion for the combination step becomes\u03b1 k, i = l, Nk clk\u03b1, i + \u00b5 l (Gk, i)."}, {"heading": "D. Convergence in the mean", "text": "Introduce the following expected values for each active substance: Gk, EGk, i = \u03b7X DthroukX \u03b7X Dthrouk (IS \u2212 \u03b3P \u03c0) X \u2212 X (IS \u2212 \u03b3P \u03c0) D\u03c6kX 0M \u00b7 M (60) gk, Egk, i = \u2212 \u03b7X D\u03c6kr\u03c00M (61) Since assumption 1 implies that the variables Ri + 1 and \u03b1 i are independent of each other, we then obtain, by taking into account the expectations of both sides of (59), E\u03b1 i + 1 = C (I2MN \u2212 \u00b5R) E\u03b1 i + \u00b5C (G\u03b1o + g) (62) November 6, 2014 DRAFT20whereR, ERi = diag {G1,., GN} (63) G = EGi = EGi = col {G1,.,."}, {"heading": "E. Mean-square stability", "text": "Although the error vector converges on average, we must still ensure that it fluctuates around its fixed point (+ 71). To do this, we examine the evolution and equilibrium value of the variance E-2 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "F. Mean-square performance", "text": "If you take the boundary from both sides of (71), you get: lim i \u00b2 Networks \u00b2 E \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (80) Theorem 1 guarantees that limi \u00b2 \u00b2 E\u03b1 \u00b2 \u00b2 = \u00b2 \u00b2 \u00b2 \u00b2, i.e. the balance variance in (80) leads to lim i \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2. (81) The result is useful because it enables us to derive several performance measures by correctly selecting the free weight parameter vector in \u00b2 (or, accordingly, the parameter matrix \u00b2 \u00b2 (81) from November 6, 2014 DRAFT23, where q, h + 2U \u00b2 Networks \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 -deviation (MSD) as the mean value of the MSD of all actors in the network: MSD \u00b2 \u00b2 k \u00b2 = 1K = 1K (MSD \u00b2 = 1K) MK = 1K (MSD \u00b2 = 1K) MK = 1K (MSD \u00b2 = 1K) MSK = 1K = 1K (MSK = 1K)"}, {"heading": "G. Bias analysis", "text": "We have shown in (45) that under assumption 2 there is a unique solution \u03b1o to the global optimization problem (31). On the other hand, the error recursion (62) converges in the sense of the mean square to a certain biased value \u03b1. The main difference lies in the fact that we do not assume that the matrix R in (63) is symmetrical. Theorem 3 (bias at small step size) is similar to the investigation developed in [18, theorem 3] for multi-objective optimization. The main difference is that we do not consider the matrix R in (63) to be symmetrical. Theorem 3 (bias at small step size) is similar. Consider the data model of section IV-A, where the combination matrix C is primitively left-tacky (it satisfies (36) - (37)."}, {"heading": "V. SIMULATIONS", "text": "Consider a group of animals that locate themselves in the net by fixing a Gaussian radial function = south (see Figure 2). The group forms a network of N = 15 agents with arbitrarily linked topology and neighborhood size. [50], so that each member of the neighborhood, including himself, could receive the same weight (i.e., clk = 1 / | Nk), is obtained independently of each node following an average rule. [50], so that each member of the neighborhood can receive the same weight, including himself. (i.e., clk = 1 / Nk), this rule results in a left (instead of double) stochastic combination of agents that satisfies (36) - (37). We assume that the combination matrix C - and thus the network topology remains fixed. The world is a discrete, limited square with 20 rows and 20 columns, which amounts to S = 400 states."}, {"heading": "VI. CONCLUSION", "text": "Diffusion GTD maintains the efficiency of the Single Agent GTD2 [9], with linear complexity in both computation time and memory imprint. With Diffusion GTD, agents learn directly from samples (without prior knowledge of the environment) and cooperate to improve the stability and accuracy of their prediction. We note that collaboration is fully distributed through communication only within the vicinity of each agent; neither fusion center nor multi-hop communication is required. We have created conditions that guarantee convergence of the proposed diffusion GTD and derive performance limits for sufficiently small step sizes. Although our analysis assumes stationarity, constant step sizes are a desirable feature of an adaptive network, as they allow the network to continuously learn and trace concept drifts in the data."}, {"heading": "APPENDIX A", "text": "PROOF OF THEOREM 1Around the spectrum of C (I2MN) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M) (Y \u2212 1C) I2M) (Y \u2212 1C) I2M (88) I2M (I2M) I2M (I2M) I2M (JC) I2M (28M) I2M) (Y \u2212 1C) I2M (88), so that, by similarity, C (I2MN \u2212 3) I2M (I2M) I2M (I2M) I2M (I2M) I2M (I2M), I2M (I2M) (so) I2M (I2M) (I2M) I2M (I2M), I2M (I2M) (so) (I2M) I2M (I2M) (I2M), I2M (I2M), I2M (I2M) (I2M), I2M (I2M) (I2M), I2M (I2M), I2M (I2M) (I2M), I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M), I2M (I2M (I2M)"}, {"heading": "APPENDIX B", "text": "PROOF OF THEOREM 3We assume a similar argument as [18], [53]. It is enough to show that Limo (0), Limo (0), Limo (5), Limo (3), Limo (5), Limo (4), Limo (4), Limo (4), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5), Limo (5, 5, 5, 5 (5), 5 (5), 5 (5, 5), Limo (5, 5, 5 (5), 5 (5), 5 (5, 5, 5, 5, 5, 5 (5), 5 (5), 5 (5, 5, 5, 5, 5, 5, 5, 5 (5), 5 (5, 5), 5 (5, 5, 5, 5, 5, 5, 5, 5, 5, 5), 5 (5, 5, 5, 5, 5, 5, 5, 5 (5), 5 (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), 5 (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5),"}], "references": [{"title": "Cooperative off-policy prediction of Markov decision processes in adaptive networks", "author": ["S.V. Macua", "J. Chen", "S. Zazo", "A.H. Sayed"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, British Columbia, Canada, May 2013, pp. 4539\u20134543.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "Proc. Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, Taipei, Taiwan, 2011, pp. 761\u2013768. November 6, 2014  DRAFT  34", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling-up knowledge for a cognizant robot", "author": ["T. Degris", "J. Modayil"], "venue": "Notes AAAI Spring Symposium Series, Palo Alto, CA, USA, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior, vol. 22, no. 2, pp. 146\u2013160, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["L. M"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Dynamic Programming and Optimal Control, 4th ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesvari", "H.R. Maei"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 21, Vancouver, British Columbia, Canada, 2008, pp. 1609\u20131616.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvari", "E. Wiewiora"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Montreal, Quebec, Canada, 2009, pp. 993\u20131000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J. Tsitsiklis", "D. Bertsekas", "M. Athans"], "venue": "IEEE Transactions on Automatic Control, vol. 31, no. 9, pp. 803\u2013812, 1986.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48\u201361, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs", "author": ["S. Kar", "J. Moura"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674\u2013690, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Decentralized parameter estimation by consensus based stochastic approximation", "author": ["S. Stankovic", "M. Stankovic", "D. Stipanovic"], "venue": "IEEE Transactions on Automatic Control, vol. 56, no. 3, pp. 531\u2013543, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Consensus problems in networks of agents with switching topology and time-delays", "author": ["R. Olfati-Saber", "R. Murray"], "venue": "IEEE Transactions on Automatic Control, vol. 49, pp. 1520\u20131533, Sep 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122 \u20133136, July 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion LMS Strategies for Distributed Estimation", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035\u20131048, March 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Diffusion adaptation strategies for distributed optimization and learning over networks", "author": ["J. Chen", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4289\u20134305, Aug. 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed Pareto optimization via diffusion strategies", "author": ["\u2014\u2014"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 2, pp. 205\u2013220, Apr. 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Diffusion adaptation over networks", "author": ["A.H. Sayed"], "venue": "Academic Press Library in Signal Processing, R. Chellapa and S. Theodoridis, Eds. Elsevier, 2014, vol. 3, pp. 323\u2013454. Also available as arXiv:1205.4220v1, May 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion strategies for adaptation and learning over networks", "author": ["A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic"], "venue": "IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 155\u2013171, May 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive networks", "author": ["A.H. Sayed"], "venue": "Proceedings of the IEEE, vol. 102, no. 4, pp. 460\u2013497, April 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks", "author": ["S.-Y. Tu", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 12, pp. 6217\u20136234, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proc. Conference on Artificial General Intelligence (AGI), vol. 1, Lugano, Switzerland, 2010, pp. 91\u201396. November 6, 2014  DRAFT  35", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "QD-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations", "author": ["S. Kar", "J.M.F. Moura", "H.V. Poor"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1848\u20131862, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1848}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Tahoe City, CA, USA, 1995, pp. 30\u201337.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control, vol. 42, no. 5, pp. 674\u2013690, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "The Borkar-Meyn theorem for asynchronous stochastic approximations", "author": ["S. Bhatnagar"], "venue": "Systems and Control Letters, vol. 60, no. 7, pp. 472\u2013478, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed value functions", "author": ["J. Schneider", "W.-K. Wong", "A. Moore", "M. Riedmiller"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Bled, Slovenia, 1999, pp. 371\u2013378.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient distributed reinforcement learning through agreement", "author": ["P. Varshavskaya", "L. Kaelbling", "D. Rus"], "venue": "Distributed Autonomous Robotic Systems 8, H. Asama, H. Kurokawa, J. Ota, and K. Sekiyama, Eds. Springer Berlin Heidelberg, 2009, pp. 367\u2013378.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Haifa, Israel, 2010, pp. 959\u2013966.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Parametric value function approximation: A unified view", "author": ["M. Geist", "O. Pietquin"], "venue": "IEEE Symp. on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), Paris, France, 2011, pp. 9\u201316.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "Annals of Operations Research, vol. 134, pp. 215\u2013238, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M. Littman"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Helsinki, Finland, 2008, pp. 752\u2013759.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Basis function adaptation methods for cost approximation in MDP", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Proc. IEEE Symp. on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), Nashville, TN, USA, 2009, pp. 74\u201381.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning representation and control in Markov decision processes: New frontiers", "author": ["S. Mahadevan"], "venue": "Foundations and Trends in Machine Learning, vol. 1, no. 4, pp. 403\u2013565, Apr. 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive state temporal difference learning", "author": ["B. Boots", "G.J. Gordon"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 23, 2010, pp. 271\u2013279.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Sketch-based linear value function approximation", "author": ["M.G. Bellemare", "J. Veness", "M. Bowling"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 25, 2012, pp. 2222\u20132230.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive Filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Numerical solution of saddle point problems", "author": ["M. Benzi", "G.H. Golub", "J. Liesen"], "venue": "Acta Numerica, vol. 14, pp. 1\u2013137, 2005.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Introduction to Optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software Inc.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1987}, {"title": "Studies in Linear and Non-linear Programming", "author": ["K.J. Arrow", "L. Hurwicz", "H. Uzawa"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1958}, {"title": "On the limiting behavior of distributed optimization strategies", "author": ["J. Chen", "A.H. Sayed"], "venue": "Proc. Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, USA, October 2012, pp. 1535\u20131542.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-negative Matrices and Markov Chains", "author": ["E. Seneta"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "The O.D.E. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S. Meyn"], "venue": "SIAM Journal on Control and Optimization, vol. 38, pp. 447\u2013469, 1999.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1999}, {"title": "Transient analysis of data-normalized adaptive filters", "author": ["T.Y. Al-Naffouri", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 51, no. 3, pp. 639\u2013652, 2003.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "Convergence in multiagent coordination, consensus, and flocking", "author": ["V. Blondel", "J. Hendrickx", "A. Olshevsky", "J. Tsitsiklis"], "venue": "Proc. IEEE Conf. on Decision and Control, and European Control Conf. (CDC-ECC), Seville, Spain, 2005, pp. 2996\u2013 3000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Asynchronous adaptation and learning over networks \u2014 Part II: Performance analysis", "author": ["X. Zhao", "A.H. Sayed"], "venue": "submitted for publication. Also available as arXiv:1312.5438, Dec. 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "The learning behavior of adaptive networks \u2014 Part I: Transient analysis", "author": ["J. Chen", "A.H. Sayed"], "venue": "submitted for publication. Also available as arXiv:1312.7581, Dec. 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance limits for distributed estimation over LMS adaptive networks", "author": ["X. Zhao", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5107\u20135124, 2012. November 6, 2014  DRAFT", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A short preliminary version dealing with a special case of this work appears in the conference publication [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].", "startOffset": 210, "endOffset": 213}, {"referenceID": 4, "context": "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].", "startOffset": 214, "endOffset": 217}, {"referenceID": 1, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "There are several forms of diffusion; recent overviews appear in [19]\u2013[21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "There are several forms of diffusion; recent overviews appear in [19]\u2013[21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean-", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 188, "endOffset": 191}, {"referenceID": 8, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 193, "endOffset": 196}, {"referenceID": 22, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 198, "endOffset": 202}, {"referenceID": 23, "context": "For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].", "startOffset": 245, "endOffset": 249}, {"referenceID": 25, "context": "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].", "startOffset": 251, "endOffset": 255}, {"referenceID": 23, "context": "In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 305, "endOffset": 309}, {"referenceID": 25, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 311, "endOffset": 315}, {"referenceID": 26, "context": "In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Other related\u2014but more heuristic\u2014approaches include [28], [29].", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "Other related\u2014but more heuristic\u2014approaches include [28], [29].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 1, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.", "startOffset": 100, "endOffset": 103}, {"referenceID": 25, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [33]\u2013[38] ).", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": ", [33]\u2013[38] ).", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "To address this issue, one approach is to solve instead the projected Bellman equation [26]:", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "[9] considered the weighted least-squares problem:", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "Using (15), it can also be verified that the solution w that minimizes JPB(w) satisfies the following normal equations [39]: B\u22a4(X\u22a4DX)\u22121Bw\u22c6 = B\u22a4(X\u22a4DX)\u22121X\u22a4Dr\u03c0 (16) Since \u2016P \u2016\u221e = 1 and \u03b3 < 1, we can bound the spectral radius of \u03b3P \u03c0 by \u03c1(\u03b3P ) \u2264 \u2016\u03b3P \u2016\u221e = \u03b3 < 1 (17) Thus, the inverse (IS \u2212 \u03b3P \u03c0)\u22121 exists.", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument.", "startOffset": 120, "endOffset": 123}, {"referenceID": 38, "context": ", [41], [42, Ch.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "By using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows: X\u22a4D\u03c6 (X\u03b8i + (IS \u2212 \u03b3P )Xwi \u2212 r)", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "Recursions (29a)\u2013(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach.", "startOffset": 121, "endOffset": 124}, {"referenceID": 20, "context": "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.", "startOffset": 51, "endOffset": 55}, {"referenceID": 41, "context": "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 106, "endOffset": 110}, {"referenceID": 41, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 7, "endOffset": 10}, {"referenceID": 43, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": "Using the Kronecker product property vec(Y \u03a3Z) = (Z\u22a4 \u2297 Y )vec(\u03a3) [39], we can vectorize \u03a3\u2032 in (67) and find that its vector form is related to \u03a3 via the following linear relation: \u03c3\u2032 , vec(\u03a3\u2032) = F\u03c3, where the matrix F is given by F , (( I2MN \u2212 \u03bcR\u22a4 ) C ) \u2297 (( I2MN \u2212 \u03bcR\u22a4 ) C ) + \u03bcE [( (Ri+1 \u2212R\u22a4)C ) \u2297 ( (Ri+1 \u2212R\u22a4)C )]", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "To study the convergence of (71) we will expand it into a state-space model following [39], [49].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "To study the convergence of (71) we will expand it into a state-space model following [39], [49].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "By the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1].", "startOffset": 361, "endOffset": 364}, {"referenceID": 18, "context": ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 45, "context": ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "CONCLUSION Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint.", "startOffset": 75, "endOffset": 78}, {"referenceID": 46, "context": "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].", "startOffset": 69, "endOffset": 73}], "year": 2014, "abstractText": "We apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).", "creator": "LaTeX with hyperref package"}}}