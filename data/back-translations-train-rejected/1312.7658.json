{"id": "1312.7658", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2013", "title": "Response-Based Approachability and its Application to Generalized No-Regret Algorithms", "abstract": "Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup. Given a repeated game with vector payoffs, a target set $S$ is approachable by a certain player (the agent) if he can ensure that the average payoff vector converges to that set no matter what his adversary opponent does. Blackwell provided two equivalent sets of conditions for a convex set to be approachable. The first (primary) condition is a geometric separation condition, while the second (dual) condition requires that the set be {\\em non-excludable}, namely that for every mixed action of the opponent there exists a mixed action of the agent (a {\\em response}) such that the resulting payoff vector belongs to $S$. Existing approachability algorithms rely on the primal condition and essentially require to compute at each stage a projection direction from a given point to $S$. In this paper, we introduce an approachability algorithm that relies on Blackwell's {\\em dual} condition. Thus, rather than projection, the algorithm relies on computation of the response to a certain action of the opponent at each stage. The utility of the proposed algorithm is demonstrated by applying it to certain generalizations of the classical regret minimization problem, which include regret minimization with side constraints and regret minimization for global cost functions. In these problems, computation of the required projections is generally complex but a response is readily obtainable.", "histories": [["v1", "Mon, 30 Dec 2013 09:15:03 GMT  (37kb)", "http://arxiv.org/abs/1312.7658v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["andrey bernstein", "nahum shimkin"], "accepted": false, "id": "1312.7658"}, "pdf": {"name": "1312.7658.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["aberenstein@gmail.com", "shimkin@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 131 2.76 58v1 [cs.LG]"}, {"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Review of Approachability and Related No-Regret Algorithms", "text": "In this section, we present the accessibility problem and review Blackwell's accessibility conditions. We also discuss existing accessibility algorithms and illustrate the application of the accessibility problem to classic regret minimization problems."}, {"heading": "2.1 Approachability Theory", "text": "Consider a repeated two-person matrix played between an agent and an arbitrary opponent. (The agent selects his actions from a finite set A, while the opponent selects his actions from a finite set Z. (...) In each instance n = 1, 2,... the agent selects his action from a predetermined reward function. (...) The average reward vector reached by the agent up to a specified time is then the agent's R-1-1 action. (...) A mixed action by the agent is a probability vector p. (A), where p (a) specifies the probability of the action. (A) The average reward vector reached by the agent up to a specified time. (A) denotes the number of probability vectors above A. (Z) denotes a mixed action by the agent p. (A), where p (a) the probability of the action."}, {"heading": "2.2 Approachability and No-Regret Algorithms", "text": "Next, we will present the problem of repentance minimization in repetitive matrix games and show how these problems of approach q can be formulated with a suitably defined reward vector and target. We will begin with Blackwell's original formulation and move on to the alternative formulation of Hart and Mas-Colell (2001). In the last subsection, we will briefly consider the more complicated problem of internal repentance minimization. We will mainly emphasize the role of the dual condition and the simple calculation of the answer to these problems, and refer to the respective references for details of the (original) resulting algorithms. As before, we will consider the agent facing an arbitrarily varying environment (the opponent). The repeated game model is the same as above, except that the vector reward function r is replaced by a scalar reward (or usefulness), which is a function u: A \u00d7 Z \u2192 R. Consider the agent facing an arbitrarily changing environment (the opponent)."}, {"heading": "2.2.1 Blackwell\u2019s Formulation", "text": "Echoing Hannan's groundbreaking work, Blackwell (1954) used approach theory to elegantly demonstrate the existence of repentance-minimizing algorithms: define the vector value rewards Rn, (Un, 1 (zn) and R (Z), where 1 (z) is the probability vector in \"Z\" based on z, and the corresponding average reward is R-n, n-1-n-k = 1Rk = (U-n, q-n). Finally, define the target value setS = {(u, q) and R-x (Z): u (p, q). It is easy to prove that this set is a D set: for each q, there is a S-response p-argmaxp (Z) u (p, q), so that r (p, q) = (u (p, q), q))."}, {"heading": "2.2.2 Regret Matching", "text": "An alternative formulation proposed in Hart and Mas-Colell (2001) leads to a simple and explicit no-regrets algorithm for this problem. LetLn (a \u2032), 1nn \u2211 k = 1 (u (a \u2032, zk) \u2212 u (ak, zk))) (3) denote the regret that has accumulated because no action has been taken. Conversely, the no-regrets requirement in definition 5 is then equivalent to vector payout r = (ra \u2032). RA, defined as ra \u2032 (a, z) = A (4) almost certainly applies to any strategy of the opponent. Conversely, this goal corresponds to the attainability of the non-positive orthan S = RA \u2212 in the game with vector payout r = (ra \u2032). RA, defined as ra \u2032 (a, z) = u (a \u2032, z) \u2212 u (a) \u2212 m.To verify the double condition, it is worth noting that an adjustment of ra (p), a \u2212 q (p), a \u2212 p (p)."}, {"heading": "2.2.3 Internal Regret", "text": "We conclude this section with a further application of the approach to the stronger notion of internal regret. In view of two different actions a, a, a, a, a, we assume that the agent would replace the action a with a each time a was played. His reward at the time k = 1,..., n would be: Wk (a, a \"), {u (a,\" zk), if ak = a, u (ak, zk), otherwise. The agent's internal average regret for not playing a \"a\" instead of a \"a\" is then given by In (a \"), 1nn, k = 1 (Wk (a,\" a \") \u2212 Uk. (6) A strategy without internal regret must ensure that the AIn (a,\" a \"), (a\"), 1nn, k = 1 (a \"), indicates the existence of such strategies that defines the vector-rated reward function by displaying (a) an az (1), an az (1), an az (1), a (1), a (2), a (1)."}, {"heading": "3. Response-Based Approachability", "text": "(D). (D). (D). (D). (D). (D). (D). D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "4. Interpretation and Extensions", "text": "We will begin this section with an illuminating interpretation of the proposed algorithm with regard to a particular problem of accessibility in an auxiliary game and introduce several variants and extensions of the basic algorithm. Although each of these variants is presented separately, they can also be combined if necessary."}, {"heading": "4.1 An Auxiliary Game Interpretation", "text": "A central part of algorithm 1 is the choice of the pair (pn, q \u00b2 n), so the choice of (pn, q \u00b2 n) in (8) - (9) can be interpreted as Blackwell's strategy for a specific approach problem in an auxiliary game that we next.Suppose the agent chooses a pair of actions (a, z \u00b2 n) in due course, and the opponent chooses a pair of actions (a, z \u00b2).The vector payout function that is now given by v (a, z) is given by v (a, z \u00b2), (a) that we (a, z \u00b2 n), z \u00b2), so thatVn = r."}, {"heading": "4.2 Idling when Inside S", "text": "Remember that in the original approach of Blackwell, an arbitrary action that can be chosen by the agent whenever the average reward is also possible in our algorithm, and adds another degree of freedom that can be used to optimize other criteria. Such arbitrary choice of one (or pn) when the average reward is also possible in our algorithm. It turns out that it is simply necessary to move the average target."}, {"heading": "4.3 Directionally Unbounded Target Sets", "text": "In some areas of interest, the target sentence S may be unlimited in certain directions. In fact, this is the case with the requirement formulation of the no regrets problem, where the goal is essentially to make the average reward as large as possible. In particular, in Blackwell's formulation (Section 2.2.1), the sentence S = {(u, q): u \u00b2 u \u00b2 (q)} is unlimited in the direction of the first coordinate u \u2212 n. In Hart and Mas-Collel's formulation (Section 2.2.2), the sentence S = {L \u2264 0} is unlimited in the negative direction of all coordinates of L. In such cases, the requirement that \u03bbn = r \u00b2 n \u2212 r \u00b2 n \u2212 r \u00b2 n the DS algorithm is a property of our basic algorithm may be too strong and may even be counterproductive. In Blackwell's no regrets formulation mentioned above, we would like to increase the first coordinate of DS algorithms as much as possible."}, {"heading": "4.4 Using the Non-smoothed Rewards", "text": "In the basic algorithm of section 3, the definition of the control policy \"n\" is applied almost exclusively to the smoothed rewards r (pk, zk) and not to the actual rewards (pk, zk), namely Rk = \u2212 n (ak, zk). We are looking at the case in which the latter are used. This is important when the opponent's action is not observed, so that r (pk, zk) cannot be calculated, but rather the reward vector Rk is directly observed. It is also useful because the quantity we are actually interested in is the average reward R-n, not its smoothed version r-2. So we have replaced the reward r-2 \"n\" n \u2212 1 with the reward vector n \u2212 1 \u2212 R-1. The rest of the algorithm is the same as algorithm 1. We have the following result for this diversity."}, {"heading": "5. Generalized No-Regret Algorithms", "text": "The proposed approach can be applied to several generalized repentance minimization problems, in which the calculation of an approach is applied to the target, but an answer is easily attainable. We start by providing a general description of the problem using a generic fixed target function, and then specialize in some specific target functions that have been considered in the current literature. We do not look at the convergence rates in this section, but focus on asymptotic convergence rates by referring to our boundaries in the previous sections; see e.g. (10) or (16).Consider a repeated matrix game like before, where the vector-rated reward is replaced by v (a, z)."}, {"heading": "5.1 Global Cost Functions", "text": "Suppose that the objective of the agent is to minimize a general (i.e., non-linear) function of the average reward vector V + n. We define the best reward cases when we specify a mixed action q + n of the opponent, asG + R, and the goal is to minimize G + V + n (A) G (p, q + n) so that the satisfactory payout amount can be defined asV + V0 = V0: G (v)."}, {"heading": "5.2 Reward-to-Cost Maximization", "text": "Consider the repeated game model as before, where the agent's goal is to have the ratio of q q = q q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q (= = = = = =). This problem is mathematically equivalent to the problem of repentance minimization in repeated variable-payout games, which was considered in Mannor and Shimkin (2008) (in this paper) the cost specifically taken as stage duration). Note that this problem is a specific case of the global cost problem presented in Section 5.1, with vector-rated payout functions v (a, z) = (a, z) and G (v) = \u2212 u / c. However, here G (v) is not convex in v. We will therefore have to apply specific analyses to achieve similar limits for those of position 13 (ii)."}, {"heading": "5.3 Constrained Regret Minimization", "text": "We must finally confront the problem of limited repentance, which is in Mannor et al. (2009). We must confront the problem of limited repentance, which is in Mannor et al. (2009). We must confront the problem of limited repentance (2009). (2009) We must set the allowed values for the long-term averages. (2009) A specific common case is that of a linear restriction to each cost component, that is, there is a closed and convex prediction, i = 1,..., s) for some proposed vector values it is assumed that the proposed measure is feasible (or cannot be excluded) in the sense that for each q component there is one. (A) that there is such a prediction that c (p), q).Let C, n \u2212 1 ck denote, as before, the average costs."}, {"heading": "6. Conclusion", "text": "The proposed algorithms are based directly on the availability of a response function and not on the projection to the set target (or related geometric quantities), and are therefore convenient for certain problems where the latter may be difficult to calculate. At the same time, the additional calculation requirements are generally comparable to those of the standard Blackwell algorithm and its variants, and the proposed algorithms have been applied to a class of general problems where there is no regret, including maximizing reward and maximizing reward, which are subject to average cost constraints. The resulting algorithms appear to be the first computationally efficient algorithms in this generalized environment. In this paper, we have focused on a repeated matrix game model where the action models of the agent and the opponent are discrete at the stage of game."}, {"heading": "Acknowledgements", "text": "We thank Shie Mannor for useful discussions and for pointing out the request to regret minimizing with global cost functions. We also thank Elad Hazan for helpful comments on the appendix. This research was supported by the Israel Science Foundation with grant number 1319 / 11."}, {"heading": "Appendix A", "text": "This version avoids the lifting method used in this paper, which treats general (convex) targets by lifting them to convex cones in a higher dimension. Let's use the support function of S: hS (1), sup r: S (2), sup r: S (2), sup r: S (3), sup r: S (3), sup r: S (3), sup r: S (4), sup r: S (4), sup: S (4), p: S (4), p: S (4), p: S (4), p: S (4), p: S (4), p: S (4), p: S (4), p: S (4), p: S (4)."}], "references": [{"title": "Blackwell approachability and low-regret learning are equivalent", "author": ["J. Abernethy", "P.L. Bartlett", "E. Hazan"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Repeated Games with Incomplete Information", "author": ["R.J. Aumann", "M. Maschler"], "venue": null, "citeRegEx": "Aumann and Maschler.,? \\Q1995\\E", "shortCiteRegEx": "Aumann and Maschler.", "year": 1995}, {"title": "Online classification with specificity", "author": ["A. Bernstein", "S. Mannor", "N. Shimkin"], "venue": "Proceedings of the Conference on Neural Information Processing Systems (NIPS", "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Opportunistic approachability and generalized no-regret problems", "author": ["A. Bernstein", "S. Mannor", "N. Shimkin"], "venue": "To appear in Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2013}, {"title": "Controlled random walks", "author": ["D. Blackwell"], "venue": "In Proceedings of the International Congress of Mathematicians,", "citeRegEx": "Blackwell.,? \\Q1954\\E", "shortCiteRegEx": "Blackwell.", "year": 1954}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Blackwell.,? \\Q1956\\E", "shortCiteRegEx": "Blackwell.", "year": 1956}, {"title": "From external to internal regret", "author": ["A. Blum", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Online learning with global cost functions", "author": ["E. Even-Dar", "R. Kleinberg", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Even.Dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "A proof of calibration via blackwell\u2019s approachability theorem", "author": ["D. Foster"], "venue": "Games and Economic Behavior,", "citeRegEx": "Foster.,? \\Q1999\\E", "shortCiteRegEx": "Foster.", "year": 1999}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D.K. Levine"], "venue": null, "citeRegEx": "Fudenberg and Levine.,? \\Q1998\\E", "shortCiteRegEx": "Fudenberg and Levine.", "year": 1998}, {"title": "Approximation to Bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "A simple adaptive procedure leading to correlated", "author": ["S. Hart", "A. Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2000\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2000}, {"title": "A general class of adaptive strategies", "author": ["S. Hart", "A. Mas-Colell"], "venue": "Journal of Economic Theory,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2001\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2001}, {"title": "Calibration is computationally hard", "author": ["E. Hazan", "S. Kakade"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT", "citeRegEx": "Hazan and Kakade.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kakade.", "year": 2012}, {"title": "Approachability in infinite dimensional spaces", "author": ["E. Lehrer"], "venue": "International Journal of Game Theory,", "citeRegEx": "Lehrer.,? \\Q2002\\E", "shortCiteRegEx": "Lehrer.", "year": 2002}, {"title": "Learning to play", "author": ["E. Lehrer", "E. Solan"], "venue": "partially-specified equilibrium. Manuscript,", "citeRegEx": "Lehrer and Solan.,? \\Q2007\\E", "shortCiteRegEx": "Lehrer and Solan.", "year": 2007}, {"title": "Approachability with bounded memory", "author": ["E. Lehrer", "E. Solan"], "venue": "Games and Economic Behavior,", "citeRegEx": "Lehrer and Solan.,? \\Q2009\\E", "shortCiteRegEx": "Lehrer and Solan.", "year": 2009}, {"title": "A general internal regret-free strategy", "author": ["Bernstein", "Shimkin E. Lehrer", "E. Solan"], "venue": null, "citeRegEx": "Bernstein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2013}, {"title": "A geometric approach to multi-criterion reinforcement learning", "author": ["S. Mannor", "N. Shimkin"], "venue": null, "citeRegEx": "Mannor and Shimkin.,? \\Q2003\\E", "shortCiteRegEx": "Mannor and Shimkin.", "year": 2003}, {"title": "Calibration and internal no-regret with partial monitoring", "author": ["V. Perchet"], "venue": null, "citeRegEx": "Perchet.,? \\Q2006\\E", "shortCiteRegEx": "Perchet.", "year": 2006}, {"title": "Strategic Learning and Its Limits", "author": ["H.P. Young"], "venue": null, "citeRegEx": "1992", "shortCiteRegEx": "1992", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "Abstract Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup.", "startOffset": 47, "endOffset": 64}, {"referenceID": 5, "context": "In Blackwell\u2019s approachability problem (Blackwell, 1956), the agent\u2019s goal is to ensure that the long-term average reward vector approaches a given target set S, namely converges to S", "startOffset": 39, "endOffset": 56}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 178, "endOffset": 205}, {"referenceID": 9, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 230, "endOffset": 244}, {"referenceID": 12, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000).", "startOffset": 287, "endOffset": 314}, {"referenceID": 14, "context": "However, the algorithms in these papers are essentially based on the computation of calibrated forecasts of the opponent\u2019s actions, a task which is known to be computationally hard (Hazan and Kakade, 2012).", "startOffset": 181, "endOffset": 205}, {"referenceID": 8, "context": ", 2009), regret minimization with global cost functions (Even-Dar et al., 2009), regret minimization in variable duration repeated games (Mannor and Shimkin, 2008), and regret", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies.", "startOffset": 179, "endOffset": 458}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies.", "startOffset": 179, "endOffset": 534}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2).", "startOffset": 179, "endOffset": 734}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2).", "startOffset": 179, "endOffset": 842}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006).", "startOffset": 179, "endOffset": 1073}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006).", "startOffset": 179, "endOffset": 1087}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms.", "startOffset": 179, "endOffset": 1123}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment.", "startOffset": 179, "endOffset": 1338}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies.", "startOffset": 179, "endOffset": 2478}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies.", "startOffset": 179, "endOffset": 2514}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies. An explicit approachability algorithm which is based on computing the response to calibrated forecasts of the opponent\u2019s actions has been proposed in Perchet (2009), and further analyzed in Bernstein et al.", "startOffset": 179, "endOffset": 2730}, {"referenceID": 1, "context": "Blackwell\u2019s approachability results have been broadly used in theoretical work on learning in games, including equilibrium analysis in repeated games with incomplete information (Aumann and Maschler, 1995), calibrated forecasting (Foster, 1999), and convergence to correlated equilibria (Hart and Mas-Colell, 2000). The earliest application, however, concerned the notion of regret minimization, or no-regret strategies, that was introduced in Hannan (1957). Even before Hannan\u2019s paper was published, it was shown in Blackwell (1954) that regret minimization can be formulated as a particular approachability problem, which led to a distinct class of no-regret strategies. More recently, approachability was used in Rustichini (1999) to prove an extended no-regret result for games with imperfect monitoring, while Hart and Mas-Colell (2001) proposed an alternative formulation of no-regret as an approachability problem (see Section 2). An extensive overview of approachability and no-regret in the context of learning is games can be found in Fudenberg and Levine (1998), Young (2004), and Cesa-Bianchi and Lugosi (2006). The latter monograph also makes the connection to the modern theory of on-line learning and prediction algorithms. In a somewhat different direction, approachability theory was applied in Mannor and Shimkin (2004) to a problem of multi-criterion reinforcement learning in an arbitrarily-varying environment. Standard approachability algorithms require, at each stage of the game, the computation the direction from the current average reward vector to a closest point in the target set S. This is implied by Blackwell\u2019s primal geometric separation condition, which is a sufficient condition for approachability of a target set. For convex sets, this step is equivalent to computing the projection direction of the average reward onto S. In this paper, we introduce an approachability algorithm that avoids this projection computation step. Instead, the algorithm relies on availability of a response map, that assigns to each mixed action q of the opponent a mixed action p of the agent so that r(p, q), the expected reward vector under these two mixed actions, is in S. Existence of such a map is based on the Blackwell\u2019s dual condition, which is also a necessary and sufficient condition for approachability of a convex target set. The idea of constructing an approachable set in terms of a general response map was employed in Lehrer and Solan (2007) (updated in Lehrer and Solan (2013)), in the context of internal no-regret strategies. An explicit approachability algorithm which is based on computing the response to calibrated forecasts of the opponent\u2019s actions has been proposed in Perchet (2009), and further analyzed in Bernstein et al. (2013). However, the algorithms in these papers are essentially based on the computation of calibrated forecasts of the opponent\u2019s actions, a task which is known to be computationally hard (Hazan and Kakade, 2012).", "startOffset": 179, "endOffset": 2779}, {"referenceID": 19, "context": "minimization in stochastic game models (Mannor and Shimkin, 2003).", "startOffset": 39, "endOffset": 65}, {"referenceID": 4, "context": "Below is the classical definition of an approachable set from Blackwell (1956). Definition 1 (Approachable Set) A closed set S \u2286 Rl is approachable by the agent\u2019s strategy \u03c0 if the average reward R\u0304n = n \u22121\u2211n k=1Rk converges to S in the Euclidian pointto-set distance d(\u00b7, S), almost surely for every strategy \u03c3 of the opponent, at a uniform rate over all strategies \u03c3 of the opponent.", "startOffset": 62, "endOffset": 79}, {"referenceID": 5, "context": "Next, we present a formulation of Blackwell\u2019s results (Blackwell, 1956) which provides us with conditions for approachability of general and convex sets.", "startOffset": 54, "endOffset": 71}, {"referenceID": 3, "context": "Since Blackwell\u2019s original construction, some other approachability algorithms that are based on similar geometric ideas have been proposed in the literature. Hart and MasColell (2001) proposed a class of approachability algorithms that use a general steering direction with separation properties.", "startOffset": 6, "endOffset": 185}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework).", "startOffset": 10, "endOffset": 34}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets.", "startOffset": 10, "endOffset": 469}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets.", "startOffset": 10, "endOffset": 627}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets. In Shimkin and Shwartz (1993) and Milman (2006), approachability was extended", "startOffset": 10, "endOffset": 767}, {"referenceID": 0, "context": "Recently, Abernethy et al. (2012) proposed an elegant scheme which generates the required steering directions through a no-regret algorithm (in the online convex programming framework). We provide in the Appendix a somewhat simplified version of that algorithm which is meant to clarify the geometric basis of the algorithm, which involves the support function of the target set. We mention in passing some additional theoretical results and extensions. Vieille (1992) studied the weaker notions of weak approachability and excludability, and showed that these notions are complimentary even for non-convex sets. Spinat (2002) formulated a necessary and sufficient condition for approachability of general (not necessarily convex) sets. In Shimkin and Shwartz (1993) and Milman (2006), approachability was extended", "startOffset": 10, "endOffset": 785}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory.", "startOffset": 96, "endOffset": 110}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory.", "startOffset": 96, "endOffset": 141}, {"referenceID": 15, "context": "An extension of approachability theory to infinite dimensional reward spaces was carried out in Lehrer (2002), while Lehrer and Solan (2009) considered approachability strategies with bounded memory. Recently, Mannor et al. (2011) proposed a robust approachability algorithm for repeated games with partial monitoring and applied it to the corresponding regret minimization problem.", "startOffset": 96, "endOffset": 231}, {"referenceID": 4, "context": "We start with Blackwell\u2019s original formulation, and proceed to the alternative one by Hart and Mas-Colell (2001). In the final subsection, we consider briefly the more elaborate problem of internal regret minimization.", "startOffset": 14, "endOffset": 113}, {"referenceID": 4, "context": "1 Blackwell\u2019s Formulation Following Hannan\u2019s seminal paper, Blackwell (1954) used approachability theory in order to elegantly show the existence of regret minimizing algorithms.", "startOffset": 2, "endOffset": 77}, {"referenceID": 12, "context": "2 Regret Matching An alternative formulation, proposed in Hart and Mas-Colell (2001), leads to a a simple and explicit no-regret algorithm for this problem.", "startOffset": 58, "endOffset": 85}, {"referenceID": 10, "context": "It was shown in Hart and Mas-Colell (2001) that application of Blackwell\u2019s approachability strategy in this formulation leads to the so-called regret matching algorithm, where the probability of action a at time step n is given by:", "startOffset": 16, "endOffset": 43}, {"referenceID": 10, "context": "The formulation of internal-no-regret as the approachability problem above, along with explicit approaching strategies, in due to Hart and Mas-Colell (2000). The importance of internal regret in game theory stems from the fact that if each player in a repeated N -player game uses such a no-internal regret strategy, then the empirical distribution of the players\u2019 actions convergence to the set of correlated equilibria.", "startOffset": 130, "endOffset": 157}, {"referenceID": 6, "context": "Some interesting relations between internal and external (Hannan\u2019s) regret are discussed in Blum and Mansour (2007).", "startOffset": 92, "endOffset": 116}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.", "startOffset": 48, "endOffset": 86}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.1 in Shimkin and Shwartz (1993) to deduce that a sequence \u2016\u03bb\u0303n\u2016 satisfying (17) converges to zero almost surely at a uniform rate that depends only on \u03c1.", "startOffset": 48, "endOffset": 151}, {"referenceID": 4, "context": "Now, we can proceed as in the original proof of Blackwell\u2019s theorem (Blackwell (1956), Theorem 1) or use Proposition 4.1 in Shimkin and Shwartz (1993) to deduce that a sequence \u2016\u03bb\u0303n\u2016 satisfying (17) converges to zero almost surely at a uniform rate that depends only on \u03c1. In particular, using the proof of Proposition 4.1 in Shimkin and Shwartz (1993) we know that for every \u01eb > 0 and \u03b4 > 0, there exists N = N(\u01eb, \u03b4, \u03c1) so that P {", "startOffset": 48, "endOffset": 353}, {"referenceID": 8, "context": "1 Global Cost Functions The following problem of regret minimization with global cost functions was introduced in Even-Dar et al. (2009). Suppose that the goal of the agent is to minimize a general (i.", "startOffset": 114, "endOffset": 137}, {"referenceID": 8, "context": "In particular, we have the following immediate result (a slight extension of Even-Dar et al. (2009)).", "startOffset": 77, "endOffset": 100}, {"referenceID": 8, "context": "Example 2 (Load Balancing) The following model was considered in Even-Dar et al. (2009), motivated by load balancing and job scheduling problems.", "startOffset": 65, "endOffset": 88}, {"referenceID": 8, "context": "Even-Dar et al. (2009) analyzed the case where G is either the d-norm with d > 1, or the infinity norm (the makespan).", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "This problem is mathematically equivalent to the problem of regret minimization in repeated games with variable stage duration considered in Mannor and Shimkin (2008) (in that paper, the cost was specifically taken as the stage duration).", "startOffset": 141, "endOffset": 167}, {"referenceID": 19, "context": "This problem is mathematically equivalent to the problem of regret minimization in repeated games with variable stage duration considered in Mannor and Shimkin (2008) (in that paper, the cost was specifically taken as the stage duration). Observe that this problem is a particular case of the global cost function problem presented in Section 5.1, with vector-valued payoff function v(a, z) = (u(a, z), c(a, z)) and G(v) = \u2212u/c. However, here G(v) is not convex in v. We will therefore need to apply specific analysis in order to obtain similar bounds to those of Proposition 13(ii). We mention that similar bounds to the ones established below were obtained in Mannor and Shimkin (2008). The algorithm there was based on playing a best-response to calibrated forecasts of the opponent\u2019s mixed actions.", "startOffset": 141, "endOffset": 688}, {"referenceID": 19, "context": "It is easily verified that the maximum can always be obtained here in pure actions (Mannor and Shimkin (2008); see also the proof of Prop.", "startOffset": 84, "endOffset": 110}, {"referenceID": 19, "context": "Now, this is exactly the definition of the so-called calibration envelope in Mannor and Shimkin (2008), and the claims of the lemma follow by Lemma 6.", "startOffset": 77, "endOffset": 103}, {"referenceID": 2, "context": "We observe that a concrete learning application for the constrained regret minimization problem was proposed in Bernstein et al. (2010). There, we considered the on-line problem of merging the output of multiple binary classifiers, with the goal of maximizing the truepositive rate, while keeping the false-positive rate under a given threshold 0 < \u03b3 < 1.", "startOffset": 112, "endOffset": 136}], "year": 2013, "abstractText": "Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup. Given a repeated game with vector payoffs, a target set S is approachable by a certain player (the agent) if he can ensure that the average payoff vector converges to that set no matter what his adversary opponent does. Blackwell provided two equivalent sets of conditions for a convex set to be approachable. The first (primary) condition is a geometric separation condition, while the second (dual) condition requires that the set be non-excludable, namely that for every mixed action of the opponent there exists a mixed action of the agent (a response) such that the resulting payoff vector belongs to S. Existing approachability algorithms rely on the primal condition and essentially require to compute at each stage a projection direction from a given point to S. In this paper, we introduce an approachability algorithm that relies on Blackwell\u2019s dual condition. Thus, rather than projection, the algorithm relies on computation of the response to a certain action of the opponent at each stage. The utility of the proposed algorithm is demonstrated by applying it to certain generalizations of the classical regret minimization problem, which include regret minimization with side constraints and regret minimization for global cost functions. In these problems, computation of the required projections is generally complex but a response is readily obtainable.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}