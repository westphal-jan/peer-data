{"id": "1401.0116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2013", "title": "Controlled Sparsity Kernel Learning", "abstract": "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p does not have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can achieve better accuracies than the previous formulations through the right choice of t.", "histories": [["v1", "Tue, 31 Dec 2013 09:13:09 GMT  (2517kb,D)", "http://arxiv.org/abs/1401.0116v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dinesh govindaraj", "raman sankaran", "sreedal menon", "chiranjib bhattacharyya"], "accepted": false, "id": "1401.0116"}, "pdf": {"name": "1401.0116.pdf", "metadata": {"source": "CRF", "title": "Controlled Sparsity Kernel Learning\u2217", "authors": ["Dinesh Govindaraj", "Raman Sankaran"], "emails": ["dinesh.govindaraj@alcatel-lucent.com", "ramans@csa.iisc.ernet.in", "sreedal.menon@alcatel-lucent.com", "chiru@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "Multiple Kernel Learning (MKL) on Support Vector Machines (SVMs) has lately been a popular front of research due to its success in application problems such as object categorization. This success is due to the fact that MKL is able to select from a variety of feature cores to identify the optimal core combination. However, the original formulation of MKL was only able to select the best of the features presented and overlooks many other informative cores. To overcome this, Kloft et. al proposed the formulation based on the Lp standard. This formulation is able to select a not sparse set of cores through a control parameter. Unfortunately, the parameter type has no direct relevance for the number of selected cores. We have observed that tighter control over the number of selected cores gives us an advantage over these techniques in terms of accuracy of classification and helps us adapt the algorithms to the time requirements."}, {"heading": "1 Introduction", "text": "The key to solving this problem lies first in Lanckriet et. al. [2], where the problem of multiple kernel learning (MKL) was first introduced. They have been successfully applied to a variety of domains, e.g. texts, object recognition [10, 13], protein structures [20]. Although the idea was to explore the space of all possible linear combinations of the specified kernels, the functional structure associated with them could only be selected from the range of specified kernels."}, {"heading": "2 Related Work", "text": "Multiple Kernel Learning (MKL) was originally proposed by Lanckriet et. al. [2] They introduced a SemiDefinite Programming (SDP) approach to solve the combination core. As SDP becomes insoluble with increasing size and number of cores, Bach et.al [3] was reformulated by treating each function as a block and applying the l1 standard across blocks and l2 standard within each block. Multiple algorithms were proposed for this formulation to speed up the optimization process. [4] provides a Semi-Infinite Linear Programming (SLIP) based algorithm that greatly reduces training time. SimpleMKL [5] is derived from Rakotomamonjy et.al. a formulation that complies with the Block l1 standard and provides a reduced gradient descent based algorithm that is faster than the previously proposed SIP algorithm."}, {"heading": "3 Controlled Sparsity Kernel Learning", "text": "In this section we present the new Controlled Sparsity Kernel Learning (CSKL) = > formulation and prove that this formula can explicitly control the sparsity of the core selection by a parameter. We derive the CSKL formulation by changing the dual of MKL [2], starting with the dual MKL [2], where Sm = {3), where Sm = {3), K \u00b2 (K \u00b2) (K \u00b2) (K \u00b2) and K \u00b2 (K \u00b2), where Sm = {3) and K \u00b2 (K \u00b2), where Sm \u00b2 (0 \u00b2), K \u00b2 (0 \u00b2), K \u00b2) and K \u00b2) cannot be used. Denote dj tj \u00b2, the solution = \u03b1 > Y KjY and tj = Trace (Kj)."}, {"heading": "4 Algorithms for solving CSKL formulations", "text": "We present an alternating optimization scheme to solve the question \u2212 CSKL formula. For a fixed case, we solve the following maximization for \u03b1, max \u03b1 \u2212 \u03b3T ds.t. 0 \u2264 \u03b1i \u2264 1m, m \u2211 i = 1 \u03b1iyi = 0, valid solution for i = 1 \u03b1idj = \u03b1 > Y KjY \u03b1 (4.20) Note that in the above problem \u03b3 n = 1 \u03b3i = t, 0 \u2264 \u03b3i \u2264 1,. We can use the standard solution for the above problem, Sequential Minimal Optimization (SMO). Once the optimal \u03b1 is calculated, we calculate d as, dj = \u03b1 > Y KjY \u03b1. The next step is to find the optimal solution for the case. We can find the optimal solution for the case by gt (d) with reduced gradient descent or linear programming based on gradient descent."}, {"heading": "5 Experiments.", "text": "To illustrate the advantages of the CSKL formulation, we give the results of Synthetic Data and Caltech101 = = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 0, 3 = 2 = 3 = 0, 4 = 4 = 0, 4 = 4 = 4 = 4 = 4 = 4 = 4 = 5 = 5 = 5 = 5 = 5 = 5 = 5 = 5 * 5 = 5 = 5 = 5 = 5 * 5 = 5 = 5 * 5 = 5 * 5 = 5 * 5 = 5 = 5 * 5 = 5 * * 5 = 5 = 5 * * 5 = 5 = 5 * 5 * * 5 = 5 = 5 * * * 5 = 5 = 5 = 5 * * * * * * * * 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 ="}, {"heading": "6 Conclusion.", "text": "As we have seen, both Sparse and Non-Sparse MKL have their handicaps according to the existing classification problem. Niehter of them is always the best solution. Even with such problems, the time required to calculate the characteristics is one of the biggest bottlenecks. For all these reasons, a formulation with strict control of sparseness would be the best solution. It is then possible to set the Sparsity parameter t and select the best set of cores for each specific classification problem. We have described such a formulation in this paper together with the associated solution algorithms. We have also shown the superior performance of this formulation both in terms of Sparse and Non-Sparse MKL formulations for the application problem of object categorization."}], "references": [{"title": "Learning the Kernel Matrix with Semidefinite Programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Multiple Kernel Learning", "author": ["F. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Conic Duality, and the SMO Algorithm, International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y Grandvalet"], "venue": "Journal of Machine Learning Research, vol 9, pages 2491-2521", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation", "author": ["Saketha Nath Jagarlapudi", "Dinesh Govindaraj", "Raman S", "Chiranjib Bhattacharyya", "Aharon Ben-Tal", "K.R. Ramakrishnan"], "venue": "Proceedings of the Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Efficient and Accurate Lp-Norm Multiple Kernel Learning", "author": ["Marius Kloft", "Ulf Brefeld", "Soeren Sonnenburg", "Pavel Laskov", "Klaus-Robert Mller", "Alexander Zien"], "venue": "Proceedings of the Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Composite Kernel Learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A Visual Vocabulary for Flower Classification", "author": ["Maria-Elena Nilsback", "Andrew Zisserman"], "venue": "Proceedings  of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning the Discriminative Power Invariance Trade-off", "author": ["M. Varma", "D. Ray"], "venue": "Proceedings of the International Conference on Computer Vision", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated Flower Classification over a Large Number of Classes", "author": ["Maria-Elena Nilsback", "Andrew Zisserman"], "venue": "Proceedings of the Sixth Indian Conference on Computer Vision, Graphics & Image Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["Peter Gehler", "Sebastian Nowozin"], "venue": "Proceedings of the Twelfth IEEE International Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Support kernel machines for object recognition", "author": ["A. Kumar", "C. Sminchisescu"], "venue": "IEEE International Conference on Computer Vision", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning with Kernels Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Schlkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Numerical Optimization: Theoretical and Practical Aspects (Universitext)", "author": ["Bonnans", "J. Fr\u00e9d\u00e9ric", "Gilbert", "Jean Charles", "Lemar\u00e9chal", "Claude", "Sagastiz\u00e1bal", "Claudia A"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["R. Fergus L. Fei-Fei", "P. Perona"], "venue": "In IEEE", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology", "author": ["Damoulas", "T. Girolami M"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "\u2217Work Done in the year 2011 accurate classification using SVMs is the choice of Kernel functions(for definition of Kernel function please see [16]).", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "[2] where the problem of Multiple kernel learning(MKL) was first introduced.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 25, "endOffset": 33}, {"referenceID": 10, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 25, "endOffset": 33}, {"referenceID": 14, "context": "text, object recognition [10, 13], protein structures[20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Recently, many other approaches have been proposed to overcome this limitation[6, 7].", "startOffset": 78, "endOffset": 84}, {"referenceID": 4, "context": "Recently, many other approaches have been proposed to overcome this limitation[6, 7].", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "While sparse solutions[2, 5] overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels, once again, there is no way to control the sparsity of the solution.", "startOffset": 22, "endOffset": 28}, {"referenceID": 9, "context": "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 3, "context": "In the case of applications like Object Recognition, the necessity of Non-Sparse solutions have been brought to light [12, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 0, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "al [3] reformulated MKL by considering each feature as a block and applying the l1 norm across the blocks and l2 norm within each block.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.", "startOffset": 39, "endOffset": 48}, {"referenceID": 3, "context": "For this formulation several algorithms[4, 5, 6] were proposed to speed up the optimization process.", "startOffset": 39, "endOffset": 48}, {"referenceID": 2, "context": "SimpleMKL [5] proposed by Rakotomamonjy et.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "To acheive non-sparsity, [6] group the kernels and apply l\u221e norm across the groups and l1 norm within the groups.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "[7] apply general lp norm to kernels and they show that Non-Sparse MKL generalizes much better than sparse MKL.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We have also experimentally shown that our formulation will be able to better state-of-the-art performance on the Caltech101[19] dataset for object categorization through strict control of sparsity.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "We derive the CSKL formulation by modifying the dual of MKL [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "Lets start with the MKL dual [2]", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "6) results in a sparse selection of kernels as shown in [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "\u03b3 [17].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "To illustrate the benefits of CSKL formulation, we give results on Synthetic data and the Caltech101 [19] realworld Object Categorization dataset.", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 10, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 9, "context": "It has been shown by [10, 13, 12] that multiple image descriptors aid in the generalization ability of the learnt classifier.", "startOffset": 21, "endOffset": 33}, {"referenceID": 7, "context": "Using the method followed by [10] , we extract the following 4 descriptors : PhowColor, PhowGray, GeometricBlur and SelfSimilarity.", "startOffset": 29, "endOffset": 33}], "year": 2014, "abstractText": "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p doesnot have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can acheive better accuracies than the previous formulations through the right choice of t.", "creator": "LaTeX with hyperref package"}}}