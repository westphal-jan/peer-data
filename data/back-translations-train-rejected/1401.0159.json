{"id": "1401.0159", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2013", "title": "Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions", "abstract": "This is an overview paper written in style of research proposal. In recent years we introduced a general framework for large-scale unconstrained optimization -- Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal/image denoising, deconvolution, compressive sensing, computed tomography, diffraction imaging, support vector machines. We explored its combination with Parallel Coordinate Descent and Separable Surrogate Function methods, obtaining state of the art results in above-mentioned areas. There are several methods, that are faster than plain SESOP under specific conditions: Trust region Newton method - for problems with easily invertible Hessian matrix; Truncated Newton method - when fast multiplication by Hessian is available; Stochastic optimization methods - for problems with large stochastic-type data; Multigrid methods - for problems with nested multilevel structure. Each of these methods can be further improved by merge with SESOP. One can also accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints.", "histories": [["v1", "Tue, 31 Dec 2013 15:25:50 GMT  (150kb)", "http://arxiv.org/abs/1401.0159v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["michael zibulevsky"], "accepted": false, "id": "1401.0159"}, "pdf": {"name": "1401.0159.pdf", "metadata": {"source": "CRF", "title": "Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions", "authors": ["Michael Zibulevsky"], "emails": ["mzib@cs.technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 140 1.01 59v1 [cs.NThis is a review paper written in the style of a research proposal. In recent years, we have introduced a general framework for large-scale unrestricted optimization - Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal / image denosis, deconvolution, compressive sampling, computed tomography, diffraction imaging, support vector machines. We have studied its combination with methods of parallel coordinate descend and separable surrogate function, obtaining state-of-the-art results in the above areas. There are several methods that are faster than pure SESOP under certain conditions: Trust Region Newton Method - for problems with easily invertable Hessian matrix; Truncated Newton Method - when rapid multiplication by Hessians is available; stochastic optimization methods - for problems with large multichastic data; easy-to-reversible structure."}, {"heading": "1 Background", "text": "Many problems in science and technology are treated as optimization tasks, often with very high dimensions. Their solution requires the use of iterative methods of various kinds. If the problem size exceeds several thousand variables, the storage and inversion of the Hessian matrix required for the Newton Method becomes prohibitively expensive. Therefore, increased attention is paid to methods that only use gradient information. Some of them exhibit optimal complexity: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7]. Nevertheless, solving large, poorly conditioned problems with high accuracy remains a challenge. Worst-case limits are too pessimistic in many real cases."}, {"heading": "1.1 SESOP \u2013 Sequential Subspace Optimization", "text": "The history of the SESOP method [8] begins with the method of conjugate gradients (CG) at any point in the space. [9] The SESOP method begins with the method of conjugate gradients (CG) at any point in the space. [9] The SESOP method (i.e. CG) at the point where it is oriented towards a square CG number of the target is much better than the steepest descent rate. [10] One can also rely on a 1 / k2 sub-linear convergence of the square CG number, which does not depend on Hessian conditioning (see for example [4, 8]): f (xk + 1) \u2212 f-optimal L-x0 \u2212 x-optimal 2k2 (1), where the iteration index and L are the lipschitz constant of f-gradients."}, {"heading": "1.2 Newton-type optimization methods", "text": "In this area, we have to deal with two basic approaches, which are able to orient themselves in one direction, in which they orient themselves in one direction, namely in the direction in which they move: in the direction in which they move, in the direction in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they, in which they, in which they, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in this area, in which they, in which they, in which they, live, in which they, in"}, {"heading": "1.3 Mini-batch stochastic optimization", "text": "Machine learning presents data-driven optimization problems, where the objective function includes the sum of the loss concepts over a data set to be modeled. Classical optimization techniques must calculate this sum in full for each assessment of the object or its gradient. As the available data sets grow, such \"batch\" optimizers become increasingly inefficient; they are also unsuitable for online (incremental) settings, where partial data must be modeled as it arrives. Stochastic (online) gradient-based methods, on the other hand, work with gradient estimates derived from small sub-samples (mini-batches) of training data, which can significantly reduce computing requirements: for large, redundant data sets, simple stochastic gradient deviation routinely outperforms reasonably sophisticated second-order batch methods by orders of magnitude, despite the slow convergence of first-order gradient deviations."}, {"heading": "2 Merging existing algorithms with with SESOP", "text": "The above mentioned methods are faster than pure SESOP methods under certain conditions: Trust region Newton method - for problems with easily reversible Hessian matrix; Truncated Newton method - when rapid multiplication using the Hessian method is available; Stochastic optimization methods - for problems with large stochastic data; Multigrid methods - for problems with nested multi-level structure. Each of these methods has its own weakness, which can be alleviated by subspace optimization; Acceleration of the Augmented Lagrange method for limited optimization problems; and Alternating Direction Method of Multipliers for problems with divisible objective functions and non-divisible limitations using the SESOP concept."}, {"heading": "2.1 Merging trust region and line search approaches within SESOP framework", "text": "The known problem with the trust region approach (6) is twofold. First, the search for the minimum of the model is often 2-3 times more expensive than the search for the unrestricted minimum. Second, the model can insert the original function well in some directions and poorly in others, therefore, an ideal trust region should be a kind of ellipsoid instead of the Euclidean ball. However, setting the parameters of such an ellipsoid is quite difficult. In our proposal we will address this difficulty with SESOP.Usually the model qk in the trust region step (6) is built in such a way that its gradient at point xk is equal to the gradient of the original function f. This is the case for Newton method, if the model is the second order Taylor expansion around xk."}, {"heading": "2.2 Combining SESOP with Truncated Newton method", "text": "As we have already mentioned, we are able to imitate the next CG levels. (D) We will not solve this problem. (D) We will not solve it. (D) We will not solve it. (D) We will not do it. (D) We will do it. (D) We will not do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D) We will do it. (D). (D). (D). (D). (D). (D). (D). (D). \"(D). (D. D. (D). (D) D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D. (D). (D. (D). (D). (D). (D). (D. (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D"}, {"heading": "2.3 Mini-batch stochastic optimization using SESOP", "text": "Inspired by the recent success of the Quasi-Newton method L-BFGS [20, 21, 22] in the field of machine learning, we plan to adapt SESOP to similar conditions. SESOP has demonstrated state-of-the-art efficiency in many classes of major problems, see e.g. [11, 15]. The basic SESOP iteration consists in calculating a minimum objective function over the subspace, which is spanned by the current gradient and several previous steps. In instochastic mode, one can optimize the partial objective function calculated in the current mini-batch over the subspace, which is spanned by steps in several previous mini-batches and the previous subgradient. By restricting the use of the latest gradient, we hope to stabilize the method by preventing overmatch to the latest mini-batch data."}, {"heading": "2.4 Incorporating SESOP into multigrid/multilevel optimization", "text": "In this context, it should be noted that the solution to the problem is not a purely formal matter, but a purely formal matter, which is a purely formal matter."}, {"heading": "3 Perspectives of convergence analysis", "text": "In this section we only share some thoughts about a possible convergence analysis of the SESOP-related methods.Composite Functions As we have already mentioned, SESOP, similar to the ORTH and the Nesterov method, exhibits an optimal 1 / k2 convergence of the worst-case, expressed in (1).The error in this formula is proportional to the Lipschitz constant L of the gradient of the objective function f.This limit becomes unsatisfactory when it comes to the composite object of the type (2), which contains a non-smooth L1 standard term. Therefore, the Nesterov method for composite objective and FISTA mitigates this difficulty by using the SSF direction instead of the gradient in the update formula to evaluate the next step. Therefore, only the Lipschitz constant of the gradient of the first smooth term in the lens is involved in an error estimation. In the spirit, the next SSF-SOP method uses several steps in the previous calculation."}, {"heading": "4 Some preliminary experiments:", "text": "First, we want to demonstrate the \"proof of concept\" using a pure quadratic function. We solve the problem of the linear smallest squares with n = 400 variables, with the quadratic random matrix A zero mean i.i.d. Gaussian entries with variance 1 / n. As we see in Figure 2, the SESOP-TN orbit does not, as expected, depend on the number of CG iterations in the TN step. Standard TN (the correct graph) lacks this property. Two non-linear examples The first problem are exponent-and-squares [27] with n = 200 variables: min e \u2212 1 Tx + 12n-j = 1j2x2j. The second example is the linear support vector engine (SVM), see [28] for further details on the unrestricted formulation of SVM data selected from the two steps SVM-729-882 and SO-458."}, {"heading": "5 Conclusions", "text": "In this paper, we presented different perspectives on the acceleration of optimization algorithms using the SESOP framework and shared some thoughts on convergence analysis."}], "references": [{"title": "Interior-point polynomial algorithms in convex programming", "author": ["Y. Nesterov", "A.S. Nemirovskii", "Y. Ye"], "venue": "vol. 13. SIAM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Information-based complexity of mathematical programming (in Russian)", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Izvestia AN SSSR, Ser. Tekhnicheskaya Kibernetika (the journal is translated to English as Engineering Cybernetics. Soviet J. Computer & Systems Sci.), vol. 1, 1983.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "Orth-method for smooth convex optimization (in Russian)", "author": ["A. Nemirovski"], "venue": "Izvestia AN SSSR, Ser. Tekhnicheskaya Kibernetika (the journal is translated to English as Engineering Cybernetics. Soviet J. Computer & Systems Sci.), vol. 2, 1982.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1982}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/n) (in Russian)", "author": ["Y. Nesterov"], "venue": "Doklady AN SSSR (the journal is translated to English as Soviet Math. Docl.), vol. 269, no. 3, pp. 543\u2013547, 1983.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "CORE discussion paper, Universit\u00e8 catholique de Louvain, Belgium, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Sequential subspace optimization method for large-scale unconstrained problems", "author": ["G. Narkiss", "M. Zibulevsky"], "venue": "Technical Report CCIT 559, Technion \u2013 Israel Institute of Technology, Faculty of Electrical Engineering, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Methods of conjugate gradients for solving linear systems", "author": ["M.R. Hestenes", "E. Stiefel"], "venue": "J. Res. Natl. Bur. Stand., vol. 49, pp. 409\u2013436, 1952.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1952}, {"title": "Numerical optimization. Series in operations research and financial engineering", "author": ["J. Nocedal", "S. Wright"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization", "author": ["M. Elad", "B. Matalon", "M. Zibulevsky"], "venue": "Applied and Computational Harmonic Analysis, vol. 23, no. 3, pp. 346\u2013367, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Why simple shrinkage is still relevant for redundant representations", "author": ["M. Elad"], "venue": "Information Theory, IEEE Transactions on, vol. 52, no. 12, pp. 5559\u20135569, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics, vol. 57, no. 11, pp. 1413\u2013 1457, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "An em algorithm for wavelet-based image restoration", "author": ["M.A. Figueiredo", "R.D. Nowak"], "venue": "Image Processing, IEEE Transactions on, vol. 12, no. 8, pp. 906\u2013916, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "L1-L2 optimization in signal and image processing", "author": ["M. Zibulevsky", "M. Elad"], "venue": "Signal Processing Magazine, IEEE, vol. 27, no. 3, pp. 76\u2013 88, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "On the performance of algorithms for the minimization of L1-penalized functionals", "author": ["I. Loris"], "venue": "Inverse Problems, vol. 25, no. 3, p. 035008, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Truncated-Newton algorithms for largescale unconstrained optimization", "author": ["R.S. Dembo", "T. Steihaug"], "venue": "Mathematical Programming, vol. 26, pp. 190\u2013212, June 1983. 16", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1983}, {"title": "A survey of Truncated-Newton methods", "author": ["S.G. Nash"], "venue": "Journal of Computational and Applied Mathematics, no. 124, pp. 45\u201359, 2000.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "A stochastic quasi-Newton method for online convex optimization", "author": ["N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "SGD-QN: Careful quasinewton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1737\u20131754, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A. Ng", "Q.V. Le"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 265\u2013 272, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Why multigrid methods are so efficient", "author": ["I. Yavneh"], "venue": "Computing in science & engineering, vol. 8, no. 6, pp. 12\u201322, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "A multigrid approach to discretized optimization problems", "author": ["S.G. Nash"], "venue": "Optimization Methods and Software, vol. 14, no. 1-2, pp. 99\u2013116, 2000.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Recursive trust-region methods for multiscale nonlinear optimization", "author": ["S. Gratton", "A. Sartenaer", "P.L. Toint"], "venue": "SIAM Journal on Optimization, vol. 19, no. 1, pp. 414\u2013444, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S.", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "UCTP - test problems for unconstrained optimization", "author": ["H. Nielsen"], "venue": "Technical Report IMM-REP-2000-18, Department of Mathematical Modelling, DTU, 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Support Vector Machine via sequential subspace optimization", "author": ["G. Narkiss", "M. Zibulevsky"], "venue": "Technical Report CCIT 557, Technion \u2013 Israel Institute of Technology, Faculty of Electrical Engineering, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2006. 17", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Interior-point polynomial complexity methods provide fast and robust solution of convex problems, where Newton optimization is applicable [1, 2].", "startOffset": 138, "endOffset": 144}, {"referenceID": 1, "context": "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 4, "context": "Several of them possess optimal worst-case complexity [3]: ORTH method by Nemirovsky [4], Nesterov method [5, 6] and FISTA method by Beck and Teboulle [7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 5, "context": "The story of SESOP method [8] begins with the method of Conjugate Gradients (CG) [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "The story of SESOP method [8] begins with the method of Conjugate Gradients (CG) [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "CG applied to a quadratic function) has remarkable convergence properties: Its linear convergence rate (see for example [10]) is \u221a r\u22121 \u221a r+1 , where r is the condition number of the Hessian of the objective.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):", "startOffset": 149, "endOffset": 158}, {"referenceID": 1, "context": "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):", "startOffset": 149, "endOffset": 158}, {"referenceID": 5, "context": "One can also rely on a 1/k sub-linear worst-case convergence of the quadratic CG, which does not depend on the Hessian conditioning (see for example [4, 3, 8]):", "startOffset": 149, "endOffset": 158}, {"referenceID": 2, "context": "In order to alleviate this problem, Nemirovski [4] suggested to restrict the optimization subspace just to three directions: the current gradient, the sum of all previous steps and a \u201ccleverly\u201d weighted sum of all previous gradients (a version of such weights is given in [8]).", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "In order to alleviate this problem, Nemirovski [4] suggested to restrict the optimization subspace just to three directions: the current gradient, the sum of all previous steps and a \u201ccleverly\u201d weighted sum of all previous gradients (a version of such weights is given in [8]).", "startOffset": 272, "endOffset": 275}, {"referenceID": 5, "context": "The SESOP method [8] extends the ORTH subspaces with several directions of the last propagation steps.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "In order to improve efficiency further we can substitute gradient direction with direction of parallel coordinate descent (PCD) or direction, provided by minimizer of a separable surrogate function (SSF) [11] .", "startOffset": 204, "endOffset": 208}, {"referenceID": 9, "context": "For such problems we use Parallel Coordinate Descent [12]: Staying at current point we evaluate coordinate descent steps for all coordinates without moving along them (this can be performed analytically at cost of two fast matrix-vector multiplications).", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "Method of separable surrogate functions (SSF) is another efficient alternative to gradient descent proposed by [13, 14].", "startOffset": 111, "endOffset": 119}, {"referenceID": 11, "context": "Method of separable surrogate functions (SSF) is another efficient alternative to gradient descent proposed by [13, 14].", "startOffset": 111, "endOffset": 119}, {"referenceID": 12, "context": "This partially explains extremal efficiency of these methods on difficult problems, where they are often significantly faster [15] than popular Nesterov method and FISTA [16, 7].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "This partially explains extremal efficiency of these methods on difficult problems, where they are often significantly faster [15] than popular Nesterov method and FISTA [16, 7].", "startOffset": 170, "endOffset": 177}, {"referenceID": 12, "context": "In Figure 1 we present results of an experiment [15] adopted from [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "In Figure 1 we present results of an experiment [15] adopted from [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "can see, FISTA converges much faster then SSF, which corresponds to the observations in [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Truncated Newton (TN) method [18, 19] is used when computing and inverting Hessian matrix is prohibitively expensive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 16, "context": "Truncated Newton (TN) method [18, 19] is used when computing and inverting Hessian matrix is prohibitively expensive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 17, "context": "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].", "startOffset": 61, "endOffset": 73}, {"referenceID": 18, "context": "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].", "startOffset": 61, "endOffset": 73}, {"referenceID": 19, "context": "For example of mini-batch limited memory quasi-Newton L-BFGS [20, 21, 22].", "startOffset": 61, "endOffset": 73}, {"referenceID": 5, "context": "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.", "startOffset": 85, "endOffset": 99}, {"referenceID": 5, "context": "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.", "startOffset": 85, "endOffset": 99}, {"referenceID": 8, "context": "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.", "startOffset": 85, "endOffset": 99}, {"referenceID": 12, "context": "Whenever the Newton direction is nor effective, it is at least as efficient as SESOP [8, 8, 11, 15], which is usually much faster than steepest descent or nonlinear conjugate gradient method; when Newton direction is good, local quadratic convergence rate of Newton is preserved.", "startOffset": 85, "endOffset": 99}, {"referenceID": 17, "context": "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.", "startOffset": 87, "endOffset": 99}, {"referenceID": 18, "context": "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.", "startOffset": 87, "endOffset": 99}, {"referenceID": 19, "context": "Inspired by the recent success of mini-batch limited memory quasi-Newton method L-BFGS [20, 21, 22] in various machine learning tasks, we plan to adapt SESOP to similar conditions.", "startOffset": 87, "endOffset": 99}, {"referenceID": 8, "context": "g [11, 15].", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": "g [11, 15].", "startOffset": 2, "endOffset": 10}, {"referenceID": 20, "context": "In multigrid and multilevel techniques (see basic introduction in [23]) the problem whose solution is sought is represented in a hierarchy of resolutions, and each such version of the problem is \u201cresponsible\u201d for eliminating errors on a scale compatible with its typical resolution.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "Second, by \u201dclever\u201d sequential iteration between fine and coarse grid, an accurate solution can be obtained much faster, then just iterating on fine grid with a good initial guess [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "[24, 25].", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[24, 25].", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "It is known that SESOP is equivalent to CG when used for minimization of quadratic function [8], but usually converges faster than CG, when the function is non-quadratic.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "Standard iteration of SESOP consists of optimization of the objective function over affine subspace spanned by directions of several previous steps, current [preconditioned] gradient (or other reasonable directions, like parallel coordinate descent, [11]).", "startOffset": 250, "endOffset": 254}, {"referenceID": 23, "context": "[26]), we can include corresponding alternating directions into SESOP subspace.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We have some results for quadratic functions [11], however they could be extended for smooth nonlinear case as well as to the composite non-smooth functions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Two non-linear examples The first problem is Exponents-and-Squares [27] with n = 200 variables:", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "The second example is Linear Support Vector Machine (SVM), see [28] for more details on unconstrained formulation of SVM.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "We used data set Astrophysics-29882 [29] with 99758 variables, and selected randomly 1495 training examples from there.", "startOffset": 36, "endOffset": 40}], "year": 2014, "abstractText": "This is an overview paper written in style of research proposal. In recent years we introduced a general framework for large-scale unconstrained optimization \u2013 Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal/image denoising, deconvolution, compressive sensing, computed tomography, diffraction imaging, support vector machines. We explored its combination with Parallel Coordinate Descent and Separable Surrogate Function methods, obtaining state of the art results in above-mentioned areas. There are several methods, that are faster than plain SESOP under specific conditions: Trust region Newton method for problems with easily invertible Hessian matrix; Truncated Newton method when fast multiplication by Hessian is available; Stochastic optimization methods for problems with large stochastic-type data; Multigrid methods for problems with nested multilevel structure. Each of these methods can be further improved by merge with SESOP. One can also accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints.", "creator": "LaTeX with hyperref package"}}}