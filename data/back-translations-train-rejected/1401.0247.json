{"id": "1401.0247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jan-2014", "title": "Robust Hierarchical Clustering", "abstract": "One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise \\cite{qcluster2005}. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm consistently achieves better performance than other hierarchical algorithms in the presence of noise.", "histories": [["v1", "Wed, 1 Jan 2014 04:16:21 GMT  (896kb,D)", "https://arxiv.org/abs/1401.0247v1", "35 pages"], ["v2", "Sun, 13 Jul 2014 01:51:05 GMT  (896kb,D)", "http://arxiv.org/abs/1401.0247v2", "37 pages"]], "COMMENTS": "35 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["maria-florina balcan", "yingyu liang", "pramod gupta"], "accepted": false, "id": "1401.0247"}, "pdf": {"name": "1401.0247.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maria-Florina Balcan", "Yingyu Liang", "Pramod Gupta"], "emails": ["ninamf@cs.cmu.edu.", "yliang39@gatech.edu.", "pramodg@google.com."], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "1.1 Our Results", "text": "In particular, in Section 3, we show that the data fulfill a natural good neighborhood, then our algorithm can be used to well penetrate the tree model, that is, to output a hierarchy designed to remove the remaining good points in the dataset, in the neighborhood of their target. Most of their closest neighbors are removed from their target cluster. We show that our algorithm produces a hierarchy in which all good points are correctly assigned."}, {"heading": "1.2 Related Work", "text": "In agglomerative hierarchical clusters (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single division of data, but a hierarchy (generally represented by a tree) of partitions, which may reveal an interesting structure in the data at multiple levels of granularity. Traditionally, only clusters are considered at a certain level, but as we argue in Section 2, it is more desirable to consider all the properties of the tree, as we can then handle much more general situations. As mentioned above, it is well known that standard agglomerative hierarchical cluster techniques are not tolerant of noise (Nagy, 1968; Narasimhan et al., 2006). Several algorithms have been proposed to make the hierarchical cluster techniques more robust to noise, such as Wishart's method (1969) and CURE (Guha et al, 1998)."}, {"heading": "1.3 Structure of the Paper", "text": "The rest of the work is structured as follows: In Section 2, we formalize our model and define the good neighborhood ownership; we describe our algorithm and prove that it is successful under the good neighborhood ownership in Section 3; we then prove that it is also successful under a generalization of the good neighborhood ownership in Section 4; in Section 5, we show how to adjust our algorithm to the inductive environment with formal correctness guarantees; we deliver the experimental results in Section 6 and finish the work in Section 7."}, {"heading": "2 Definitions. A Formal Setup", "text": "We look at a clustering problem (S '1), which is defined as follows: \"We assume we have a strict set of n objects.\" Each x'S has some (unknown) \"basic truth\" label (x) in Y = 1,.., k, \"where we will imagine ourselves to be much smaller than n. Let's leave Ci = {x) an abbreviation of Cl (x), and nC will name the size of a cluster that is another proposed cluster (that could be empty), and let's name the target as C = {C1,. Let C (x) be an abbreviation of Cl (x), and nC will name the size of another proposed cluster C.Given another proposed cluster formation h, h: S \u2192 Y, let's define the error of h with respect to the target cluster to bury (h) = min-Sk [Pr x]."}, {"heading": "2.1 Properties of the Similarity Function", "text": "We describe here some of the natural characteristics of immediate neighbors that we analyze in this paper. We start with a loud version of the simple strict separation of property (mentioned above) introduced in (Balcan et al., 2008) and we then define an interesting and natural generalization of property 1. However, the similarity function K satisfies strict separation for the clustering problem (S \"), if for some S\" S the size (1 \u2212 \u03bd) n, K \"fulfills strict separation of property (S,\"). That is, for all x, x, \"x\" S \"with x.\" and x \"\" \"\". C \"\" (x \") we have K (x\") > K, \"x.\" So, in other words, we demand that strict separation be satisfied after a number of bad points. A slightly different condition is that each point can have some bad immediate neighbors as long as most of its immediate neighbors are good."}, {"heading": "2.2 Standard Linkage Based Algorithms Are Not Robust", "text": "As we show below, even if the data fulfills the good neighbourliness characteristics, the default single linkage, average linkage and complete linkage algorithms may fail. The contribution of our work is to develop a robust linkage-based algorithm that will succeed under these natural conditions. Specifically, we can show an example in which the default single linkage, average linkage and complete linkage algorithms would work very poorly, but in which our algorithm would work well. In this example, let us slightly modify the example in Figure 1 by adding a little noise to form linkages of high similarity between dots in different inner spots. See Figure 2 for an accurate description of the similarity. In this example, all individual linkages, average linkages and complete linkage algorithms would merge the matching dots in the first n / 2 level, regardless of how they work."}, {"heading": "3 Robust Median Neighborhood Linkage", "text": "In this section, we propose a new algorithm that brings a lot of data to us and shows that we are able to satisfy the convenience of people. (...) We first initialized the threshold to a value that is not too big and not too small (...). (...) First, it builds a graph in which there are two or more blobs like this. (...) It builds a graph Ft, whose2Da, normally, the similarity between the objects is constructed. (...) This could happen in practice. (...)"}, {"heading": "3.1 Intuition of the Algorithm under the Good Neighborhood Property", "text": "Let's start with some practical terminologies and a simple fact about good neighbourliness. In the definition of (\u03b1, \u03bd) -good neighbourhood property (see property 3), let's call the points in S \"good points and the points in B = S\" bad points. Let's call Gi = Ci \"S\" the good series of terms i. \"Let G =\" iGi \"denote the whole set of good points; so G = S.\" Obviously, G. \"n\" fulfils. \"nCi\" is the number of points in the cluster Ci. \"Note that the following is a useful sequence of the (\u03b1) -good neighbourliness relationship. Let's assume that the similarity of the functionK is\" the good neighbourliness for the problem (S \").\" As long as t is smaller than nCi, we are x \"Ci for every good point, everything, but not for most.\""}, {"heading": "3.2 Correctness under the Good Neighborhood Property", "text": "In this section, we prove theorem 1 for our algorithms. (2) Correctness follows from lemma 1 and runtime follows from lemma 2. Before we prove these lemmas, we start with a useful fact that follows directly from the design of the algorithm. (4) This is clearly true if the blob contains only one threshold, if a blob in Ct contains at least one good point, then it contains at least 3 / 4 fractions of the points used in that blob. (4) Then follows the assertion that there is at least one bad point. (1) The following assertions are true in algorithm 1: (1) For each Ci such that t containing good points in Ci. (1) Every blob in Ci, every blob in Ci contains no good points from Ci. (2) The following assertions are true in algorithm 1: (1) For each Ci so that we contain good points in Ci."}, {"heading": "4 A More General Property: Weak Good Neighborhood", "text": "In this section, we present a weaker idea of good neighborhood characteristics and prove that our algorithm also applies to data to satisfy this weaker trait. To motivate the property, we consider a point x with the following neighborhood structure. In practice, points near the boundaries between different target clusters typically have such neighborhood structure; for this reason, points with such neighborhood are referred to as boundary points. We present an example in Figure 7. A document near the boundary between the two fields AI and statistics has the following neighborhood structure: from its n / 4 closest neighbors it has all its neighbors from its own field; but from its n / 2 closest neighbors it has n / 4 neighbors, it has n / 4 neighbors outside its field."}, {"heading": "4.1 Relating Different Versions of Good Neighborhood Properties", "text": "The relationships between these properties are illustrated in Figure 8. Relations between the weak good neighbourhood properties and other neighbourhood properties are discussed below, while the other relationships in the figure from the facts in Section 2.1.By defining Ap = Ci for p-Ci, we can see that the weak (\u03b1, \u03b2) -good neighbourhood property is a generalisation of the alpha-good neighbourhood property if each target group is larger than 6\u03b1n. Formally speaking, fact 5. If the similarity function K is the alpha-good neighbourhood property for the cluster problem (S, \u03b1) -good neighbourhood property (Alpha) and Mini | Alpha, then K also meets the weak (Alpha, \u03b2) -good neighbourhood property for the cluster problem (S, ') -good neighbourhood property (S,')."}, {"heading": "4.2 Correctness under the Weak Good Neighborhood Property", "text": "We prove that our algorithm can only be merged with other points in S / B. (...) We prove that our algorithm will only be merged with other points in S / B. (...) We prove that our algorithm will only be merged with other points in S / B. (...) We prove that our algorithm will only be merged with other points in S / A. (...) We prove that our algorithms will only be compared with other points in S / B. (...) We prove that we have established and proved the correctness of Lemma 5 and that the runtime of Lemma 2 follows. (...) Intuition is as follows. (...) Firstly, with similar arguments as for good neighbourliness. (...) Every point in S / B / B follows a similar argument. (...) The proof of correctness is given by Lemma 5 and proved that the runtime of Lemma 2 follows. (...) Intuition is as follows. (...)"}, {"heading": "5 The Inductive Setting", "text": "Many cluster applications have recently experienced an explosion of data, such as in astrophysics and biology. For large datasets, it is often resource-intensive and time-consuming to run an algorithm over the entire dataset. Therefore, it is increasingly important to develop algorithms that eliminate dependence on the actual size of the data and still work reasonably well. In this section, we will consider an inductive model formalizing this problem. In this model, the data given is only a small random subset of dots from a much larger dataset. The algorithm creates a hierarchy over the sample, which also implicitly represents a hierarchy over the dataset. Below, we describe the inductive version of our algorithm and demonstrate that if the data meets the good neighborhood characteristics, the algorithm achieves small errors over the entire dataset, requiring only a small random sample whose size is independent of the whole dataset."}, {"heading": "5.1 Formal Definition", "text": "First of all, we describe the formal definition of the inductive model. In this setting, the given data S represents only a small random subset of points from a much larger abstract instance space X. For simplicity, we assume that X is finite and that the underlying distribution over X is equal. Let us let N = | X | specify the size of the entire instance space, and let us specify n = | S | the size of the sample. Our goal is to design an algorithm that, based on the sample, produces a hierarchy of small errors in relation to the entire distribution. Formally, let us assume that each node u induces a cluster (a subset of X) in the hierarchy derived from the sample. For simplicity, u is also used to denote the block of sampled points it represents. Formally, the cluster u induced over X is implicitly represented as a function fu: X \u2192 {0,} that is, for each error, we say, x = 1, if in the cluster x is 1, then we have an error (x)."}, {"heading": "5.2 Inductive Robust Median Neighborhood Linkage", "text": "The inductive version of our algorithm is described in Algorithm 2. To analyze the algorithm, let us first present the following results: if the data satisfy the good neighbourliness characteristic, a sample of sufficiently large size can satisfy the weak good neighbourliness. Algorithm 2: \"There is no neighbourliness relationship.\" - \"There is no neighbourliness relationship.\" - \"There is no neighbourliness relationship.\" - \"There is no neighbourliness relationship.\" - \"There is no neighbourliness relationship.\" - \"There is no neighbourliness relationship.\" - \"There is no.\" - \"There is no.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"There is.\" - \"- There is.\" - \"- There is.\" - There. \"- There is.\" - There is. \"- There is.\" - there. \"- there is.\" - there. \"- there is.\" - there. \"- there is.\" - there. \"- there is.\" - there. \"- there.\" - there. \"- there.\" - - there. \"- - there.\" - there. \"- - there. - there. - there. - there. - - there. - there. - there. - there. - there."}, {"heading": "1\u2212 \u03b4, K satisfies the (2\u03b1, 2\u03bd)-good neighborhood with respect to the clustering induced over the sample S.", "text": "The proof has been provided. To see this, let us find a solution. (...) Since we have a probability of at least 1 \u2212 \u03b4 / 2, at most 2\u03bdn bad points in the sample. (...) Next, we have to calculate a probability of at least 1 \u2212 6 points in the sample (...). (...) Then we have to calculate a probability of at least 1 \u2212 6 points in the sample (...). (...) If we have a probability of at least 1 \u2212 6 points in the sample (...), then we have a probability of at least 1 \u2212 2 points in the sample (...). (...) If we have a probability of at least 1 \u2212 2 points in the sample (...), then we have a probability of at least 1 \u2212 2 points in the sample. (...) The similarity function then fulfils the (...) good neighborhood with respect to the sample. (...) It is now sufficient to show n that the probability is large enough for n (...) (...)."}, {"heading": "6 Experiments", "text": "In this section, we compare our algorithms (called RMNL for convenience) with popular hierarchical clustering algorithms = 5,000 results, including standard concatenation algorithms (Ssemble and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart's Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward's Minimum Variance Method (Ward, 1963), CURE (Guha et al., 1998), and EigenCluster (Cheng et al., 2006).To evaluate the performance of the algorithms, we use the model discussed in Section 2. Faced with a hierarchical output by an algorithm, we generate all possible prunings of size k5, where the number of clusters in the target clustering is. Then we calculate the classification error of each pruning in relation to the target clustering and report the best error."}, {"heading": "6.1 Synthetic Data", "text": "Here we compare the performance of the algorithms on the synthetic data AIStat. Let us remember that clustering {AI, Statistics} with a high probability fulfils the weak (\u03b1, \u03b2, \u03bd) good neighbourliness property for \u03b1 = 1 / 32, \u03b2 = 7 / 8, \u03bd = 0 (see Figure 7 in section 4). We perform three experiments in which we vary the values of \u03b1 and \u03bd by quickly modifying the similarities between the points. (a) For each point x we select \u2206 \u03b1n points y from the other field and determine the similarities K (x, y) = K (y, x) = 1, so that the value of \u03b1 is increased to 1 / 32 + \u03b1. By selecting the property \u03b1 = 1 / 32 + i / 256 for i / 256 points y from the other field and specifying the similarities K (x, y) = K (y, x) = 1, so that the value of \u03b1 is set to 1 / 32 + +."}, {"heading": "6.2 Real-World Data", "text": "In this section, we compare the performance of our algorithm with the other algorithms on real data sets and show that our algorithm consistently outperforms the others."}, {"heading": "6.2.1 Transductive Setting", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "6.2.2 Inductive Setting", "text": "In this setting, the algorithm generates a hierarchy on a small random sample of the dataset and inserts the remaining points to generate a new hierarchy across the entire dataset. We repeat the sampling and evaluation five times and report the average results. We compare our inductive algorithm with the random sample algorithm in (Eriksson, 2012). These algorithms sample a fraction of the similarities and use only these similarities. The percentage of sampled similarities can be matched in these algorithms so that we can compare their performance when using the same amount of sampled similarities. Figure 14 shows the results for eight configurations (using 5% or 10% similarities on four different datasets). Our algorithm consistently exceeds the random sample algorithms. Figure 15 shows the results on PFAM1 to PFAM10, showing all significant similarities on four different datasets."}, {"heading": "7 Discussion", "text": "In this thesis, we propose and analyze a new robust algorithm for the networking of agglomerations. We show that our algorithm can be used precisely in cases where the data fulfill a number of natural properties and where the traditional agglomeration algorithms fail. In particular, if the data fulfill the good properties, the algorithm will be successful in creating a hierarchy, so that the goal is close to truncating this hierarchy."}, {"heading": "B Additional Proofs for Section 5", "text": "Similarly, we can prove that the second condition of weak good neighbourliness is also satisfied. Then, similarity K fulfils the weak good neighbourliness in terms of clustering generated by the sample (Lemma 8). Our final guarantee, Theorem 4, then follows from lemmas.Lemma 9: Let K be a symmetrical similarity function that satisfies the weak neighbourliness (Lemma 8: \") - good neighbourliness for the cluster problem (X,\")."}], "references": [{"title": "Clustering oligarchies", "author": ["M. Ackerman", "S. Ben-David", "D. Loker", "S. Sabato"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ackerman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ackerman et al\\.", "year": 2013}, {"title": "Basic local alignment search tool", "author": ["S.F. Altschul", "W. Gish", "W. Miller", "E.W. Myers", "D.J. Lipman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Altschul et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Altschul et al\\.", "year": 1990}, {"title": "Modeling and detecting community hierarchies", "author": ["M.F. Balcan", "Y. Liang"], "venue": "In Proceedings of the International Workshop on Similarity-Based Pattern Analysis and Recognition,", "citeRegEx": "Balcan and Liang.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Liang.", "year": 2013}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["M.F. Balcan", "A. Blum", "S. Vempala"], "venue": "In Proceedings of the Annual ACM symposium on Theory of Computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Clustering under approximation stability", "author": ["M.F. Balcan", "A. Blum", "A. Gupta"], "venue": "Journal of ACM,", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Separating populations with wide data: A spectral analysis", "author": ["A. Blum", "A. Coja-Oghlan", "A. Frieze", "S. Zhou"], "venue": "In Algorithms and Computation", "citeRegEx": "Blum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2007}, {"title": "A structured family of clustering and tree construction methods", "author": ["D. Bryant", "V. Berry"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Bryant and Berry.,? \\Q2001\\E", "shortCiteRegEx": "Bryant and Berry.", "year": 2001}, {"title": "Rates of convergence for the cluster tree", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta.,? \\Q2010\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta.", "year": 2010}, {"title": "A divide-and-merge methodology for clustering", "author": ["D. Cheng", "R. Kannan", "S. Vempala", "G. Wang"], "venue": "ACM Transaction on Database Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2006}, {"title": "Performance guarantees for hierarchical clustering", "author": ["S. Dasgupta", "P. Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Dasgupta and Long.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta and Long.", "year": 2005}, {"title": "Hierarchical clustering using randomly selected measurements", "author": ["B. Eriksson"], "venue": "In Proceedings of the IEEE Statistical Signal Processing Workshop,", "citeRegEx": "Eriksson.,? \\Q2012\\E", "shortCiteRegEx": "Eriksson.", "year": 2012}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["J. Feldman", "R. O\u2019Donnell", "R.A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2008}, {"title": "Maximum likelihood clustering with outliers. In Classification, Clustering, and Data Analysis", "author": ["M.T. Gallegos"], "venue": null, "citeRegEx": "Gallegos.,? \\Q2002\\E", "shortCiteRegEx": "Gallegos.", "year": 2002}, {"title": "A robust method for cluster analysis", "author": ["M.T. Gallegos", "G. Ritter"], "venue": "The Annals of Statistics,", "citeRegEx": "Gallegos and Ritter.,? \\Q2005\\E", "shortCiteRegEx": "Gallegos and Ritter.", "year": 2005}, {"title": "Robustness properties of k means and trimmed k means", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Garc\u0131\u0301a.Escudero and Gordaliza.,? \\Q1999\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero and Gordaliza.", "year": 1999}, {"title": "A general trimming approach to robust cluster analysis", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "The Annals of Statistics,", "citeRegEx": "Garc\u0131\u0301a.Escudero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero et al\\.", "year": 2008}, {"title": "A review of robust clustering methods", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "Advances in Data Analysis and Classification,", "citeRegEx": "Garc\u0131\u0301a.Escudero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero et al\\.", "year": 2010}, {"title": "Programmable clustering", "author": ["S. Gollapudi", "R. Kumar", "D. Sivakumar"], "venue": "In Symposium on Principles of Database Systems,", "citeRegEx": "Gollapudi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gollapudi et al\\.", "year": 2006}, {"title": "A comparison of some methods of cluster analysis", "author": ["J.C. Gower"], "venue": null, "citeRegEx": "Gower.,? \\Q1967\\E", "shortCiteRegEx": "Gower.", "year": 1967}, {"title": "CURE: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "Guha et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Guha et al\\.", "year": 1998}, {"title": "Dissolution point and isolation robustness: robustness criteria for general cluster analysis methods", "author": ["C. Hennig"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Hennig.,? \\Q2008\\E", "shortCiteRegEx": "Hennig.", "year": 2008}, {"title": "Algorithms for clustering", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain and Dubes.,? \\Q1981\\E", "shortCiteRegEx": "Jain and Dubes.", "year": 1981}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys,", "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Hierarchical clustering schemes", "author": ["S.C. Johnson"], "venue": "Psychometrika,", "citeRegEx": "Johnson.,? \\Q1967\\E", "shortCiteRegEx": "Johnson.", "year": 1967}, {"title": "Step-wise clustering procedures", "author": ["B. King"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "King.,? \\Q1967\\E", "shortCiteRegEx": "King.", "year": 1967}, {"title": "The Hungarian method for the assignment algorithm", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Kuhn.,? \\Q1955\\E", "shortCiteRegEx": "Kuhn.", "year": 1955}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "An experimental comparison of model-based clustering methods", "author": ["M. Meil\u0103", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Meil\u0103 and Heckerman.,? \\Q2001\\E", "shortCiteRegEx": "Meil\u0103 and Heckerman.", "year": 2001}, {"title": "A polynomial time algorithm for lossy population recovery", "author": ["A. Moitra", "M. Saks"], "venue": "In Proceddings of the IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Moitra and Saks.,? \\Q2013\\E", "shortCiteRegEx": "Moitra and Saks.", "year": 2013}, {"title": "State of the art in pattern recognition", "author": ["G. Nagy"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nagy.,? \\Q1968\\E", "shortCiteRegEx": "Nagy.", "year": 1968}, {"title": "The pfam protein families database", "author": ["M. Punta", "P.C. Coggill", "R.Y. Eberhardt", "J. Mistry", "J. Tate", "C. Boursnell", "N. Pang", "K. Forslund", "G. Ceric", "J. Clements", "A. Heger", "L. Holm", "E.L.L. Sonnhammer", "S.R. Eddy", "A. Bateman", "R.D. Finn"], "venue": "Nucleic Acids Research,", "citeRegEx": "Punta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Punta et al\\.", "year": 2012}, {"title": "Numerical taxonomy. The principles and practice of numerical classification", "author": ["P.H.A. Sneath", "R.R. Sokal"], "venue": null, "citeRegEx": "Sneath and Sokal.,? \\Q1973\\E", "shortCiteRegEx": "Sneath and Sokal.", "year": 1973}, {"title": "Active clustering of biological sequences", "author": ["K. Voevodski", "M.F. Balcan", "H. R\u00f6glin", "S.-H. Teng", "Y. Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J.H. Ward"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward.,? \\Q1963\\E", "shortCiteRegEx": "Ward.", "year": 1963}, {"title": "Population recovery and partial identification", "author": ["A. Wigderson", "A. Yehudayoff"], "venue": "In Proceedings of the IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Wigderson and Yehudayoff.,? \\Q2012\\E", "shortCiteRegEx": "Wigderson and Yehudayoff.", "year": 2012}, {"title": "Mode analysis: a generalization of nearest neighbour which reduces chaining effects", "author": ["D. Wishart"], "venue": "Numerical Taxonomy,", "citeRegEx": "Wishart.,? \\Q1969\\E", "shortCiteRegEx": "Wishart.", "year": 1969}], "referenceMentions": [{"referenceID": 18, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 6, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 8, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 9, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 17, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 21, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 22, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 23, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 9, "context": "Average linkage defines the similarity between two clusters as the average similarity between points in these two different clusters (Dasgupta and Long, 2005; Jain et al., 1999).", "startOffset": 133, "endOffset": 177}, {"referenceID": 22, "context": "Average linkage defines the similarity between two clusters as the average similarity between points in these two different clusters (Dasgupta and Long, 2005; Jain et al., 1999).", "startOffset": 133, "endOffset": 177}, {"referenceID": 3, "context": "In order to formally analyze correctness of our algorithm we use the framework introduced by Balcan et al. (2008). In this framework, we assume there is some target clustering (much like a k-class target function in the multi-class learning setting) and we say that an algorithm correctly clusters data satisfying property P if on any data set having property P , the algorithm produces a tree such that the target is some pruning of the tree.", "startOffset": 93, "endOffset": 114}, {"referenceID": 9, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 21, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 22, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 29, "context": "As mentioned above, it is well known that standard agglomerative hierarchical clustering techniques are not tolerant to noise (Nagy, 1968; Narasimhan et al., 2006).", "startOffset": 126, "endOffset": 163}, {"referenceID": 35, "context": "Several algorithms have been proposed to make the hierarchical clustering techniques more robust to noise, such as Wishart\u2019s method (Wishart, 1969), and CURE (Guha et al.", "startOffset": 132, "endOffset": 147}, {"referenceID": 19, "context": "Several algorithms have been proposed to make the hierarchical clustering techniques more robust to noise, such as Wishart\u2019s method (Wishart, 1969), and CURE (Guha et al., 1998).", "startOffset": 158, "endOffset": 177}, {"referenceID": 33, "context": "Ward\u2019s minimum variance method (Ward, 1963) is also more preferable in the presence of noise.", "startOffset": 31, "endOffset": 43}, {"referenceID": 16, "context": "For general clustering beyond hierarchical clustering, there are also works proposing robust algorithms and analyzing robustness of the algorithms; see (Garc\u0131\u0301a-Escudero et al., 2010) for a review.", "startOffset": 152, "endOffset": 183}, {"referenceID": 14, "context": "In particular, the trimmed k-means algorithm (Garc\u0131\u0301a-Escudero and Gordaliza, 1999), a variant of the classical k-means algorithm, updates the centers after trimming points that are far away and thus are likely to be noise.", "startOffset": 45, "endOffset": 83}, {"referenceID": 3, "context": "On the theoretical side, Balcan et al. (2008) analyzed the \u03bd-strict separation property, a generalization of the simple strict separation property discussed above, requiring that after a small number of outliers have been removed all points are strictly more similar to points in their own cluster than to points in other clusters.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "On the theoretical side, Balcan et al. (2008) analyzed the \u03bd-strict separation property, a generalization of the simple strict separation property discussed above, requiring that after a small number of outliers have been removed all points are strictly more similar to points in their own cluster than to points in other clusters. They provided an algorithm for producing a hierarchy such that the target clustering is close to some pruning of the tree, but via a much more computationally expensive (non-agglomerative) algorithm. Our algorithm is simpler and substantially faster. As discussed in Section 2.1, the good neighborhood property is much broader than the \u03bd-strict separation property, so our algorithm is much more generally applicable compared to their algorithm specifically designed for \u03bd-strict separation. In a different statistical model, Chaudhuri and Dasgupta (2010) proposed a generalization of Wishart\u2019s method.", "startOffset": 25, "endOffset": 888}, {"referenceID": 15, "context": "An algorithm combining the above two approaches is then proposed in (Garc\u0131\u0301a-Escudero et al., 2008).", "startOffset": 68, "endOffset": 99}, {"referenceID": 20, "context": "(Hennig, 2008; Ackerman et al., 2013) studied the robustness of the classical algorithms such as k-means from the perspective of how the clusters are changed after adding some additional points.", "startOffset": 0, "endOffset": 37}, {"referenceID": 0, "context": "(Hennig, 2008; Ackerman et al., 2013) studied the robustness of the classical algorithms such as k-means from the perspective of how the clusters are changed after adding some additional points.", "startOffset": 0, "endOffset": 37}, {"referenceID": 27, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 4, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 32, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 3, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012). We will be considering clustering algorithms whose only access to their data is via a pairwise similarity function K(x, x\u2032) that given two examples outputs a number in the range [\u22121, 1]. We will say that K is a symmetric similarity function ifK(x, x\u2032) = K(x\u2032, x) for all x, x\u2032. In this paper we assume that the similarity function K is symmetric. Our goal is to produce a hierarchical clustering that contains a pruning that is close to the target clustering. Formally, the goal of the algorithm is to produce a hierarchical clustering: that is, a tree on subsets such that the root is the set S, and the children of any node S\u2032 in the tree form a partition of S\u2032. The requirement is that there must exist a pruning h of the tree (not necessarily using nodes all at the same level) that has error at most . Balcan et al. (2008) have shown that this type of output is necessary in order to be able to analyze non-trivial properties of the similarity function.", "startOffset": 76, "endOffset": 950}, {"referenceID": 3, "context": "Given a similarity function satisfying the strict separation property (see Figure 1 for an example), we can efficiently construct a tree such that the ground-truth clustering is a pruning of this tree (Balcan et al., 2008).", "startOffset": 201, "endOffset": 222}, {"referenceID": 3, "context": "We start with a noisy version of the simple strict separation property (mentioned above) which was introduced in (Balcan et al., 2008) and we then define an interesting and natural generalization of it.", "startOffset": 113, "endOffset": 134}, {"referenceID": 3, "context": "Unfortunately the algorithm presented in (Balcan et al., 2008) is computationally very expensive: it first generates a large list of \u03a9(n2) candidate clusters and repeatedly runs pairwise tests in order to laminarize these clusters; its running time is a large unspecified polynomial.", "startOffset": 41, "endOffset": 62}, {"referenceID": 31, "context": "In this section, we compare our algorithm (called RMNL for convenience) with popular hierarchical clustering algorithms, including standard linkage algorithms (Sneath and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 159, "endOffset": 217}, {"referenceID": 24, "context": "In this section, we compare our algorithm (called RMNL for convenience) with popular hierarchical clustering algorithms, including standard linkage algorithms (Sneath and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 159, "endOffset": 217}, {"referenceID": 35, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 7, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 33, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al., 1998), and EigenCluster (Cheng et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 8, "context": ", 1998), and EigenCluster (Cheng et al., 2006).", "startOffset": 26, "endOffset": 46}, {"referenceID": 19, "context": "For reporting results, we follow the methodology in (Guha et al., 1998): for all algorithms accepting input parameters (including (Generalized) Wisharts\u2019 Method, CURE, and RMNL), the experiments are repeated on the same data over a range of input parameter values, and the best results are considered.", "startOffset": 52, "endOffset": 71}, {"referenceID": 26, "context": "We also consider the MNIST data set (LeCun et al., 1998) and use two subsets of the test set for our experiments: Digits0123 that contains the examples of the digits 0, 1, 2, 3, and Digits4567 that contains the examples of the digits 4, 5, 6, 7.", "startOffset": 36, "endOffset": 56}, {"referenceID": 32, "context": "We additionally consider the 10 data sets (PFAM1 to PFAM10) from (Voevodski et al., 2012), which are created by randomly choosing 8 families (of size between 1000 and 10000) from the biology database Pfam (Punta et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 30, "context": ", 2012), which are created by randomly choosing 8 families (of size between 1000 and 10000) from the biology database Pfam (Punta et al., 2012), version 24.", "startOffset": 123, "endOffset": 143}, {"referenceID": 1, "context": "The similarities for the PFAM data sets are generated by biological sequence alignment software BLAST (Altschul et al., 1990).", "startOffset": 102, "endOffset": 125}, {"referenceID": 25, "context": "To compute this error for a computed clustering in polynomial time, we first find its best match to the target clustering using the Hungarian Method (Kuhn, 1955) for min-cost bipartite matching in time O(n), and then calculate the error as the fraction of points misclassified in matched clusters.", "startOffset": 149, "endOffset": 161}, {"referenceID": 5, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 11, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 34, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 28, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 10, "context": "We compare our inductive algorithm with the random sample algorithm in (Eriksson, 2012).", "startOffset": 71, "endOffset": 87}, {"referenceID": 2, "context": "Additionally, our subsequent work (Balcan and Liang, 2013) showed that the algorithm can be applied to the closely related community detection task and compares favorably with existing approaches.", "startOffset": 34, "endOffset": 58}, {"referenceID": 2, "context": "Additionally, our subsequent work (Balcan and Liang, 2013) showed that the algorithm can be applied to the closely related community detection task and compares favorably with existing approaches. It would be interesting to see if our algorithmic approach can be shown to work for other natural properties on the input similarity function. For example, it would be particularly interesting to analyze a noisy version of the max stability property in Balcan et al. (2008) which was shown to be a necessary and sufficient condition for single linkage to succeed, or of the average stability property which was shown to be a sufficient condition for average linkage to succeed.", "startOffset": 35, "endOffset": 471}], "year": 2014, "abstractText": "One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise.", "creator": "LaTeX with hyperref package"}}}