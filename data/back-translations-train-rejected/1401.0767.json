{"id": "1401.0767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2014", "title": "From Kernel Machines to Ensemble Learning", "abstract": "Ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner. Here we propose a principled framework for directly constructing ensemble learning methods from kernel methods. Unlike previous studies showing the equivalence between boosting and support vector machines (SVMs), which needs a translation procedure, we show that it is possible to design boosting-like procedure to solve the SVM optimization problems.", "histories": [["v1", "Sat, 4 Jan 2014 02:28:48 GMT  (542kb,D)", "http://arxiv.org/abs/1401.0767v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["chunhua shen", "fayao liu"], "accepted": false, "id": "1401.0767"}, "pdf": {"name": "1401.0767.pdf", "metadata": {"source": "CRF", "title": "From Kernel Machines to Ensemble Learning", "authors": ["Chunhua Shen", "Fayao Liu"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most people who are able, are able, are able, are able to learn, are able, are able to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be able, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in."}, {"heading": "II. RELATED WORK", "text": "The general link between SVM and top-up has been discussed by some researchers [2], [6] at a high level (> FF). To our knowledge, the work here is the first to attempt to build ensemble models by solving the SVM optimization problem. We are reviewing some of the next work. Top-up has been extensively studied over the past ten years [1] - [3], [7], [8]. Our methods are close to [3], [7] by also using column generation (CG) to select weak learners and fully update weak learning coefficients. Because we solve the SVM problem rather than applying the \"1 regulated top-up problem,\" conventional CG \u2212 cannot be applied directly. \u2212 Instead of looking at the double constraints, we rely on the CCT conditions."}, {"heading": "III. KERNEL METHODS AND ENSEMBLE MODELS", "text": "We then show the connection between these two methods and show how to design column generation based on learning methods that directly solve the optimization problems in core methods. Consider the binary classification for time. Suppose that the input data points (xi, yi) are implicitly defined by a core function (x, x). It is well known that the original data x is implicitly mapped to an attribute space by a mapping function. The function is implicitly defined by a core function (x, x)."}, {"heading": "IV. FROM SVM TO ENSEMBLE LEARNING", "text": "We show how to learn directly from core methods like SVM (strong duality problems), and our goal is to explicitly solve both problems (1) without using the kernel trick. In other words, it is unclear how to apply the idea of CG to develop a procedure similar to LPBoost, as discussed above. To add a weak learner () by finding the most violated dual constraint - as a starting point - we need to have a dual constraint that includes (2). From the dual problem of SVM (2), the main difficulty is that the dual constraints are two types of simple linear constraints on the dual variable. The dual constraints have no condition at all for the application of CG is that the duality gap between the primary and dual problems is zero (strong duality problems)."}, {"heading": "V. MULTI-CLASS ENSEMBLE LEARNING", "text": "Since most problems consist per se of several classes, the Multiclass Learning is always important. (\u00b7 J) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "VI. EXPERIMENTS", "text": "We conduct experiments on binary and multi-class classification problems and compare our methods with classical boosting methods."}, {"heading": "A. Binary classification", "text": "We conduct experiments on both synthetic and real datasets, namely, 2D toys, 13 UCI benchmark datasets4, and then on multiple levels, such as looking for the number of data collected, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, captured, and captured. We can see our method faster than AdaBoost because CGENS is completely corrected.UCI benchmark For the UCI experiment we use three different weak learners, namely Decision Problems and Fourier4http: / www.raetschlab.org / Members / benchmarkweak learners, with each compared to the corresponding kernel SVMs and other methods."}, {"heading": "B. Multi-class classification", "text": "In fact, we are able to set out in search of new paths that will lead us into the future."}, {"heading": "VII. CONCLUSION", "text": "Kernel methods are also popular in areas outside of computer science mainly because they are easy to use and highly optimized software is available. On the other hand, ensemble learning is moving in a different direction and is also being applied in different areas. In this work, we show that ensemble learning methods can be designed directly from core methods such as SVMs. In other words, you can solve the optimization problems of core methods directly by using column generation techniques. The ensemble model learned corresponds to learning the explicit feature mapping functions of core methods. This new knowledge about the exact correspondence allows us to design new algorithms. In particular, we have shown two examples of new ensemble learning methods rooted in SVMs. Extensive experiments show the advantages of these new ensemble methods over conventional methods for increasing classification accuracy and computing efficiency."}, {"heading": "VIII. APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Solutions of the multi-class SLS-SVM and ensemble learning", "text": "The Lagrangian of the Problem (10) is L (w1, \u00b7 \u00b7 J > J = > J > J = > J J (J, O, U) = 12 (J, J) = 1 (J) + J, J) (J) (J) = 12 (J) = 12 (J) + 1 (J) + 1 (J) \u2212 J (J, J) = J (J) = J (J) = J (J) + 11 (J) + J) + 1 (J) + 1 (J) \u2212 J (J) + 1 (J) + 1 (J) + 11 (J) + 11) \u2212 J (J) + J (J) + J + J) + J (J) + J (J) + J + J) + J + J (J) + J + J + J + J + J) + 11 (J) + 11 (J) + 11) \u2212 J (J) + J (J) + J \u2212 J (J) + J (J) + J (J) + J + J + J) + J + J + J (J) + 11 (J) + J + J + J + J + J + J (J) + 11 (J) + J + J + J + 11 (J) + J + 11 (J) + J + J (J) + J + 11 (J) + J + J + J + 11 (J) + J + 11 (J) + J + J + J + 11 (J) + J + 11 (J) + J + 11 (J) + J + 11 (J) + 11 - J (J (J) + 11 - J (J + 11) - J (J - J - J + 11) - J (J (J) + J (J - J) + J + J + J + J + J + J + J + J (J) + J (J + J + J + J + J + J) + 11 (J (J) + 11 (J + 11) - J + 11 (J + 11) - J (J + 11) - J (J - J (J + 11) - J + 11) - J (J + 11) - J (J (J + 11) - J + 11 - J (J (J + 11) - J (J"}, {"heading": "B. Binary classification on Spambase data set", "text": "We conducted experiments with the UCI spam dataset to demonstrate the functionality of the proposed method when we use decision stump as a weak learner; the task is to distinguish spam e-mails by word frequency; we use AdaBoost as the starting point; maximum iterations are set to 60 due to rapid convergence and without any overmatch that was observed later; we use 5-fold cross validation to select the best hyperparameter C in CGENS; the results shown in Table IV are displayed over 20 different runs with a training / test ratio of 3: 2. Fig. 5 records the average frequencies over the 20 rounds. As can be seen, both algorithms select important features such as \"free\" (feature # 16), \"hp\" (25) and \"!\" (52) with high detection rates; as for the other features, the two methods showed different inclinations on GENS (feature # 16) and \"high\" (feature # 19)."}], "references": [{"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comp. & Syst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Boosting the margin: a new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "Annals of Statistics, vol. 26, pp. 322\u2013330, 1998. 8", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "On the dual formulation of boosting algorithms", "author": ["C. Shen", "H. Li"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "RandomBoost: Simplified multi-class boosting through randomization", "author": ["S. Paisitkriangkrai", "C. Shen", "Q. Shi", "A. van den Hengel"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Boosting through optimization of margin distributions", "author": ["C. Shen", "H. Li"], "venue": "IEEE Transactions on Neural Networks, vol. 21, no. 4, pp. 659\u2013666, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Constructing boosting algorithms from SVMs: An application to one-class classification", "author": ["G. R\u00e4tsch", "S. Mika", "B. Sch\u00f6lkopf", "K.-R. M\u00fcller"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 9, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Mach. Learn., vol. 46, no. 1-3, pp. 225\u2013254, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "A direct formulation for totally-corrective multi-class boosting", "author": ["C. Shen", "Z. Hao"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011, pp. 2585\u20132592.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "J. Mach. Learn. Res., vol. 2, pp. 265\u2013292, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "1-norm support vector machines", "author": ["J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparseness of support vector machines", "author": ["I. Steinwart"], "venue": "J. Mach. Learn. Res., vol. 4, pp. 1071\u20131105, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.J. Slotine"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2012, http://arxiv.org/abs/1209.1360.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector machinery for infinite ensemble learning", "author": ["H.-T. Lin", "L. Li"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 285\u2013312, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Continuous neural networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Int. Conf. Artificial Intelli. & Stat., 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Nystr\u00f6m method vs. random Fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient classification for additive kernel SVMs", "author": ["S. Maji", "A.C. Berg", "J. Malik"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 3, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res., vol. 9, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Mach. Learn., vol. 37, no. 3, pp. 297\u2013336, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Multiclass learning, boosting, and error-correcting codes", "author": ["V. Guruswami", "A. Sahai"], "venue": "Proc. Ann. Conf. Computat. Learn. Theory, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "The MOSEK interior point optimizer", "author": ["Mosek"], "venue": "http://www. mosek.com.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 0}, {"title": "The PASCAL visual object classes challenge 2007", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "2th PASCAL Challenge Workshop, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "LabelMe: A database and web-based tool for image annotation", "author": ["B. Russell", "A. Torralba", "K. Murphy", "W. Freeman"], "venue": "Int. J.  Comp. Vis., vol. 77, no. 1, pp. 157\u2013173, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal semisupervised learning for image classification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010, pp. 902\u2013909.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Comp. Vis., vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010, pp. 3485\u2013 3492.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble learning methods, with a typical example being boosting [1]\u2013[5], have been successfully applied to many machine learning and computer vision applications.", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "Ensemble learning methods, with a typical example being boosting [1]\u2013[5], have been successfully applied to many machine learning and computer vision applications.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "[2], and R\u00e4tsch et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] developed a mechanism to convert SVM algorithms to boosting-like algorithms by translating the quadratic programs (QP) of SVMs into linear programs (LP) of boosting (similar to LPBoost [7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] developed a mechanism to convert SVM algorithms to boosting-like algorithms by translating the quadratic programs (QP) of SVMs into linear programs (LP) of boosting (similar to LPBoost [7]).", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "Following this vein, a direct approach to multi-class boosting was developed in [8] by using the loss function in Crammer and Singer\u2019s multi-class SVM [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Following this vein, a direct approach to multi-class boosting was developed in [8] by using the loss function in Crammer and Singer\u2019s multi-class SVM [9].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "The recipe to transfer algorithms is essentially [6]: \u201cThe SV-kernel is replaced by an appropriately constructed hypothesis space for leveraging where the optimization of an analogous mathematical program is done using `1 instead of `2-norm.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "We suspect that this is due to the widely-adopted belief that boosting methods need the sparsity-inducing `1-norm regularization so that the final ensemble model only relies on a subset of weak learners [3], [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "We suspect that this is due to the widely-adopted belief that boosting methods need the sparsity-inducing `1-norm regularization so that the final ensemble model only relies on a subset of weak learners [3], [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "Unlike [6], [8], no mathematical transform is needed.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "Unlike [6], [8], no mathematical transform is needed.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "At each iteration, compared with the `1 optimization involved in the indirect approach [3], [6]\u2013[8], our optimization problems are much simpler.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "At each iteration, compared with the `1 optimization involved in the indirect approach [3], [6]\u2013[8], our optimization problems are much simpler.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "At each iteration, compared with the `1 optimization involved in the indirect approach [3], [6]\u2013[8], our optimization problems are much simpler.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "3) As the fully-corrective boosting methods in [3], [8], our", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "3) As the fully-corrective boosting methods in [3], [8], our", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "1At the same time, standard SVM needs `2 regularization so that the kernel trick can be applied, although `1 SVM [10] takes a different approach.", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "Recall that the number of support vectors is linearly proportional to the number of training data [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Additional contributions of this work include: 1) To exemplify the usefulness of this proposed framework, we introduce a new multiclass boosting method based on the recent multi-class SVM [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 1, "context": "The general connection between SVM and boosting has been discussed by a few researchers [2], [6] at a high level.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "The general connection between SVM and boosting has been discussed by a few researchers [2], [6] at a high level.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Boosting has been extensively studied in the past decade [1]\u2013[3], [7], [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Boosting has been extensively studied in the past decade [1]\u2013[3], [7], [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Boosting has been extensively studied in the past decade [1]\u2013[3], [7], [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Boosting has been extensively studied in the past decade [1]\u2013[3], [7], [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Our methods are close to [3], [7] in that we also use column generation (CG) to select weak learners and fully-correctively update weak learners\u2019 coefficients.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "Our methods are close to [3], [7] in that we also use column generation (CG) to select weak learners and fully-correctively update weak learners\u2019 coefficients.", "startOffset": 30, "endOffset": 33}, {"referenceID": 12, "context": "If one uses an infinitely many weak learners in boosting [13] (or hidden units in neural networks [14]), the model is equivalent to SVM with a certain kernel.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "If one uses an infinitely many weak learners in boosting [13] (or hidden units in neural networks [14]), the model is equivalent to SVM with a certain kernel.", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Loosely speaking, boosting can be seen as explicitly computing the kernel mapping functions because, as pointed out in [6], a kernel constructed by the inner product of weak learners\u2019 outputs satisfies the Mercer\u2019s condition.", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "Random Fourier features (RFF) [15] have been applied to large-scale kernel methods.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "show that RFF does not perform well due to its data-independent sampling strategy when there is a large gap in the eigen-spectrum of the kernel matrix [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "In [17], [18], it shows that for homogeneous additive kernels, the kernel mapping function can be exactly computed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [17], [18], it shows that for homogeneous additive kernels, the kernel mapping function can be exactly computed.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": ", LIBLINEAR [19].", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "Next let us take LPBoost as an example to see how CG is used to explicitly learn weak learners, which is the core of most boosting methods [3], [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "Next let us take LPBoost as an example to see how CG is used to explicitly learn weak learners, which is the core of most boosting methods [3], [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 6, "context": "That is the main idea of LPBoost [7] and its extension [3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "That is the main idea of LPBoost [7] and its extension [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "In our experiments, we have used LIBLINEAR [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Having shown how to solve the standard SVM problem using CG, we provide another example application of the proposed framework by developing a new multi-class ensemble method using the idea of simplex coding [12].", "startOffset": 207, "endOffset": 211}, {"referenceID": 19, "context": "MO [20], AdaBoost.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "ECC [21].", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "Shen and Hao proposed a direct approach to multi-class boosting in [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "Here we proffer a new multi-class ensemble learning method based on the simplex least-squares SVM (SLS-SVM) introduced in [12].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "The maximum iteration for AdaBoost, LPBoost [7] and our CGENS are searched from {25, 50, 100, 250, 500}.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "Results of SVMs with the stump kernel are also reported [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "In the second experiment, we compare our method (using 500 weak learners) against several other methods such as SVM using (1) perceptrons sign(\u03b8>x \u2212 \u03ba) as weak learners and the perceptron kernel for SVM, and (2) Fourier cosine functions [15] cos(\u03b8>x\u2212\u03ba) as weak learners and Gaussian RBF kernel for SVM.", "startOffset": 237, "endOffset": 241}, {"referenceID": 12, "context": "Instead, we sample 2000 pairs of {\u03b8, \u03ba} according to their distributions as described in [13] and [15], and then pick the one that maximizes the weak learner selection criterion in Equ.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "Instead, we sample 2000 pairs of {\u03b8, \u03ba} according to their distributions as described in [13] and [15], and then pick the one that maximizes the weak learner selection criterion in Equ.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "We have also compared our CGENS with RFF [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "We use the state-of-the-art commercial solver Mosek [22] to solve the dual problem (5).", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Fourier weak learner/RBF kernel Perceptron/Perceptron kernel RFF [15] SVM CGENS AdaBoost LPBoost SVM CGENS", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "ECC [21] , AdaBoost.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "MH [20] and MultiBoost [8] using the exponential loss.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "MH [20] and MultiBoost [8] using the exponential loss.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "The regularization parameters in our CGENS-SLS and MultiBoost [8] are both determined by 5fold cross validation.", "startOffset": 62, "endOffset": 65}, {"referenceID": 22, "context": "Image classification We then apply the proposed CGENS-SLS for image classification on several datasets: PASCAL07 [23], LabelMe [24] and CIFAR10.", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "Image classification We then apply the proposed CGENS-SLS for image classification on several datasets: PASCAL07 [23], LabelMe [24] and CIFAR10.", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "For PASCAL07, we use 5 types of features provided in [25].", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "For LabelMe, we use the LabelMe-12-50k subset and generate the GIST [26] features.", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "2: Training time of LPBoost [7] and our CGENS in log-scale.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "use the GIST [26] features and use the provided test and training sets.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "MH MultiBoost [8] CGENS-SLS wine\u2217 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 26, "context": "HOG features described in [27] are used as the image feature.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "MH and MultiBoost [8].", "startOffset": 18, "endOffset": 21}], "year": 2014, "abstractText": "Ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner. Here we propose a principled framework for directly constructing ensemble learning methods from kernel methods. Unlike previous studies showing the equivalence between boosting and support vector machines (SVMs), which needs a translation procedure, we show that it is possible to design boosting-like procedure to solve the SVM optimization problems. In other words, it is possible to design ensemble methods directly from SVM without any middle procedure. This finding not only enables us to design new ensemble learning methods directly from kernel methods, but also makes it possible to take advantage of those highlyoptimized fast linear SVM solvers for ensemble learning. We exemplify this framework for designing binary ensemble learning as well as a new multi-class ensemble learning methods. Experimental results demonstrate the flexibility and usefulness of the proposed framework.", "creator": "LaTeX with hyperref package"}}}