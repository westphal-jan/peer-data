{"id": "1401.1549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2014", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "abstract": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.", "histories": [["v1", "Wed, 8 Jan 2014 00:49:01 GMT  (242kb,D)", "http://arxiv.org/abs/1401.1549v1", null], ["v2", "Sat, 28 Jun 2014 04:24:47 GMT  (185kb,D)", "http://arxiv.org/abs/1401.1549v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["zheng wen", "daniel o'neill", "hamid reza maei"], "accepted": false, "id": "1401.1549"}, "pdf": {"name": "1401.1549.pdf", "metadata": {"source": "CRF", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "authors": ["Zheng Wen", "Daniel O\u2019Neill", "Hamid Reza Maei"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS rescheduling problem as a problem of enhanced learning (called the device-based RL problem) and show that this RL problem decomposes via devices under reasonable assumptions. Compared to existing formulations, our new formulation (1) does not require explicit modeling of the user's dissatisfaction with workplace restructuring, (2) allows the EMS to create self-initiated workplaces, (3) allows the user to initiate more flexible requirements, and (4) has a computational complexity in the number of devices. We also propose several new performance indicators for the EMS, the RQ results based on the most popular algorithms and the use of RSL classifications:"}, {"heading": "1 Introduction", "text": "In this sense, it is also necessary for consumers to change their energy consumption habits in other ways, which in turn can improve operational efficiency, improve capital efficiency and reduce the risk of blackouts and blackouts. Renewable energy variability can lead to a better alignment of household energy consumption with unforeseen changes in electrical power generation, with the benefit that a reduction in electricity flows to renewable energy sources is often used to secure energy supplies. In a direct way, the energy consumption of users changes directly through adaptation to the operation of the appliances. Interruptive tariffs enable a company to supply electricity under predefined conditions."}, {"heading": "2 Description of fully-automated EMS", "text": "A fully automated EMS signal (henceforth referred to as EMS) is a necessary prerequisite for real energy prices in residential and small commercial buildings. In addition, an EMS must learn how to make optimal decisions for the user while interacting with them and the real-time network signals. In this section, we describe how a fully automated EMS should interact with the user and the network signals. Generally speaking, a fully automated EMS can observe the network signals, receive requests and evaluations from the user and terminate the workstations via the devices (Figure 1). We detail the interaction mechanisms in the rest of this subsector. SignalsThe EMS monitors the network signals through a communication network in which the infrastructure that supplies network signals to the EMS. Any exogenous information that is effectively supplied by the communication network is useful for EMS decisions, and can be regarded as a network signal."}, {"heading": "3 Device Based DR Model", "text": "This section proceeds as follows: We first motivate and define the dissatisfaction function and the immediate cost function for a rational consumer in Section 3.1 and 3.2 respectively. Explicit dissatisfaction functions are not required in practice, but in this section we assume that they are known to facilitate a simple representation. Then, Section 3.3 formulates the problem of optimal demand response as a collection of device-based MDPs under appropriate assumptions."}, {"heading": "3.1 Dissatisfaction Function and Rational Consumers", "text": "In order to formalize the concept of optimal demand management, we must define and propose the dissatisfaction function for a consumer (user) that registers their preferences (dissatisfaction) through job retraining. We must also define and propose a specific class of functions for the dissatisfaction functions of rational consumers (users). Let's start by defining some useful notations for which a dissatisfied request will be cancelled in due course. Note: D (t), C (t), 2, \u00b7, N} asD (t) = {Devices that do a job. C (t) = {Devices for which a dissatisfied request will be cancelled in due course. Note: D (t), C (t), if device n does a job in the time span, then there is no dissatisfied requirement for device n at the end of the time span."}, {"heading": "3.2 Instantaneous Cost Function", "text": "We assume that the direct cost function of the consumer at a given time t has the following form: Pt-n-D (t) Cn + \u03b3U-D (t) (Ht), (7) where Pt is the price of electricity at a given time and U-N is a dissatisfaction function that captures the consumer's dissatisfaction with job rescheduling at a given time. We assume that the consumer's dissatisfaction with debt rescheduling at a given time t-T and thus Pt-n-D (t) Cn is the electricity bill paid by the consumer at a given time. We assume that the consumer's dissatisfaction with debt rescheduling at a given time t-T depends on the \"debt history\" and vice versa > 0 is the trade-off between the electricity bill paid and the consumer's dissatisfaction with debt rescheduling."}, {"heading": "3.3 Device Based MDP Model", "text": "(We.). (We.). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"). (\" We. \"). (\" We. \"). (\" We. \"(\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"). (\" We. \"(\"). (\"We.\"). (\"We.\" (\"). (\" We. \"(\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (\"We.\"). \"(\" We. \"). (\" We. \"(\"). (\"We.\"). (\"We.\" (\"We.\"). \"(.\" We. \"(\"). \"(\" We. \").\" (\"). (\" We. \"(\"). \"(\" We. \").\" (\"(\" We. \").\" (\").\" (\"We.).\" (\"). (\" (\"We.).\" (\"We.\" (\"). (.).\" (\"). (\" We. (\").\" (.). (\"We.). (. (\"). (\"We.). (. (.). (\" We.). (. (\"). (\"). (\"). (\" We.). (\"). (\" We. (\"). (. (.). (\"). (\"We. (.). (.). (\"). (We."}, {"heading": "4 Reinforcement Learning and Performance Metrics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Reinforcement Learning Formulation", "text": "However, as we have explained in Figure 5, formulating the optimal workplace problem, the job likelihood and dissatisfaction function of the user is initially unknown. Therefore, the EMS needs to learn how to make optimal decisions for the user based on the incrementally collected data of the user and the real-time energy price. However, learning reinforcement (RL) is a collection of techniques for a decision maker to learn how to make optimal decisions while the user interacts with an unknown \"environment.\" \"[Sutton and Barto, 1998] RL attempts to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other areas such as artificial intelligence and robotics [Coates et al, 2010]. As discussed in [ONeill et al], it is natural to use RL to solve the EMS learning problem, as we need to learn to make decisions as we interact with the user and the energy price."}, {"heading": "4.2 Baseline and Demand Response Potential", "text": "To justify an RL algorithm, we need to compare its experimental performance against a reasonable baseline. In this subsection, we propose a reasonable baseline and potentially define the term Demand Response (DR). Let's start by defining a meaningful notation: We use \u00b5: S \u00b7 A \u2192 [0, 1] to designate a (randomized and stationary) policy of a device based on MDP. In practice, it might be more reasonable to assume that user evaluation based on a job is the same as the redeployment of that workplace plus a (zero-mean variance, statistically independent) \"behavioral noise.\""}, {"heading": "4.3 Performance Metrics", "text": "In this subsection, we propose several performance measurements for an RL algorithm based on our proposed device (RL problem). We begin by defining some notations: for each single RL algorithm asV = 0, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5 Simulation", "text": "In this section, we will first briefly discuss the classic Q-Learning algorithm in Section 5.1, then a representative simulation example in Section 5.2, and finally the simulation results in Section 5.3."}, {"heading": "5.1 The Q-Learning Algorithm", "text": "As we discussed in Section 4, many RL algorithms can be applied to our proposed device based on Qtoxt (Qtoxt). In this subsection, we implement one of the most popular and classic RL algorithms, known as Q-Learning [Watkins, 1989]. Q-Learning is an off-policy learning episode that allows the learning agent to follow an exploratory policy while learning about optimal policy. Another desirable feature of Q-Learning is that it is online, incremental and easy to implement on real-time data. Q-Learning works on the basis of time difference learning [Sutton and Barto, 1998]. At each step of the time, the learning algorithm receives an input file in the form of (xt, at, ext + 1), with Qtoxt def = (xt, at, xt + 1) the observed instantaneous costs after grasping the state-level action algorithm of the Qtoxt and then updating the state-level learning algorithm."}, {"heading": "5.2 Simulation Setup", "text": "In this subsection, we propose a representative example to which we apply the Q-Learning algorithm described in detail in Algorithm 1. Specifically, in this example, we assume that the exogenous Markov price chain has a state space P = {10, 12, 15, 20} and that consumer inquiries have two different priorities, \"high\" and \"normal.\" We set the timeframes W = 4 and W = 5, the discrete time discount \u03b1 = 0.9995 and the energy consumption per workplace C = 1. Therefore, in this example, there are | S | = 96 states. The satisfaction functions U-r, U-c and U-E are illustrated in Figure 6 (a), 6 (b) and 6 (c). Note that these satisfaction functions satisfy the characteristics of rational consumer preferences discussed in Section 3.1, as well as the satisfaction functions U-c and U-E as state truthfulness."}, {"heading": "5.3 Performance", "text": "In this subsection, we present the simulation results of the Q-Learning algorithm using the representative example described in subsection 5.2. We begin by describing how to implement the Q-Learning algorithm. We select the \"exploration probability\" = 0.05 and the \"temperature\" of the soft mine policy \u03b7 = 0.1. For episode j, we select the step size \u03b2j = 10 20 + j. We initialize the Q-Learning algorithm by running Q0 = 0.1, 0.2, \u00b7 \u00b7 we run the Q-Learning algorithm for 2 - 1 \u2212 \u03b1 = 4000 episodes and repeat the simulation 200 times. Then, we approach V and V (j)."}, {"heading": "6 Conclusion", "text": "We have proposed a novel EMS formulation for the DR problem in the residential and small commercial construction sector, which we call the device-based RL problem. Specifically, we have shown that our proposed EMS formulation does not, under reasonable assumptions, require a pre-specified diversity function that models consumer dissatisfaction with workplace rescheduling and has a computational complexity that increases linearly with the number of devices. Our new EMS formulation also allows the EMS to create jobs on its own and allows users to initiate more flexible requirements. We have also proposed several performance indicators for RL algorithms that are applied to the device-based RL problem, motivates and proposes and demonstrates the simulation results when the classical Qlearning algorithm is applied to a representative example. Simulation results suggest that for a wide range of target parameters of the Q-Learning algorithm the policy should be redefined without exceeding the workplace policy."}, {"heading": "Acknowledgment", "text": "We thank Prof. Benjamin Van Roy for his insightful comments on this paper."}, {"heading": "Appendix A Bellman Equation", "text": "It is worth pointing out that if the transition model of the device based on MDP and the dissatisfaction function of the consumer Q = \u0445 Q \u00b2 is known, the device based on MDP can be solved by dynamic programming (DP). Following ideas in the classic DP, in this section, we derive the Bellman equation from which we derive the optimal Q function.Remember that xt = [Pt, st, gt] T and the scope of action in each state A = {off, on}, we have \u2022 If the smart device in the current episode did not receive a consumer request, we have Q functionality (xt, on) = PtC + \u03b3U (st) + \u03b1E {min a AQ theory (Pt + 1, st + 1] where the expectation is about Pt + 1, st + 1 is that the expectation is about Pt + 1, st + 1 and gt + 1."}, {"heading": "Appendix B Proof for Theorem 1", "text": "Evidence for theory 1: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note: note"}, {"heading": "Appendix C Possible Extensions", "text": "It is obvious to note that many assumptions about this model are suggested in order to simplify the issue. We can easily loosen some of these assumptions to obtain a more general, device-based MDP model. We will now discuss two relaxations that are of particular interest: stack requirement and dependent devices.Stack requirement Under the current device-based MDP model, the smart device will ignore a new user requirement if it currently has an unfinished requirement. If the user wishes to replace the existing unfinished requirement with a new request, he must first cancel the existing requirement and then send the new request to the device. However, in some cases, the user may want to start a new request while maintaining the existing unfinished requirement. For example, a user has sent a request with the target time of 4PM to the AC; to 2PM, this request was not completed, and the user may want to send a new request to the device."}], "references": [{"title": "A survey of utility experience with real time pricing", "author": ["G. Barbose", "C. Goldman", "B. Neenan"], "venue": "Lawrence Berkeley National Laboratory: Lawrence Berkeley National Laboratory,", "citeRegEx": "Barbose et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Barbose et al\\.", "year": 2004}, {"title": "Dynamic Programming and Optimal Control", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas.", "year": 2005}, {"title": "Dynamic pricing, advanced metering, and demand response in electricity markets", "author": ["S. Borenstein", "M. Jaske", "A. Rosenfeld"], "venue": "UC Berkeley: Center for the Study of Energy Markets,", "citeRegEx": "Borenstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Borenstein et al\\.", "year": 2002}, {"title": "The role of demand response in electric power market design", "author": ["S. Braithwait", "K. Eakin"], "venue": "Edison Electric Institute,", "citeRegEx": "Braithwait and Eakin.,? \\Q2002\\E", "shortCiteRegEx": "Braithwait and Eakin.", "year": 2002}, {"title": "Autonomous helicopter flight using reinforcement learning", "author": ["Adam Coates", "Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Encyclopedia of Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2010}, {"title": "Quantifying customer response to dynamic pricing", "author": ["Ahmad Faruqui", "Stephen George"], "venue": "The Electricity Journal,", "citeRegEx": "Faruqui and George.,? \\Q2005\\E", "shortCiteRegEx": "Faruqui and George.", "year": 2005}, {"title": "Residential implementation of critical-peak pricing of electricity", "author": ["Karen Herter"], "venue": "Energy Policy,", "citeRegEx": "Herter.,? \\Q2007\\E", "shortCiteRegEx": "Herter.", "year": 2007}, {"title": "An exploratory analysis of California residential customer response to critical peak pricing of electricity", "author": ["Karen Herter"], "venue": "January 2007b. URL http://www.sciencedirect. com/science/article/B6V2S-4JG5F91-2/2/bb70d54%6082f9f5483829aabeef5279e", "citeRegEx": "Herter.,? \\Q2007\\E", "shortCiteRegEx": "Herter.", "year": 2007}, {"title": "Architecture concepts and technical issues for an open, interoperable automated demand response infrastructure", "author": ["E. Koch", "M.A. Piette"], "venue": "In Grid Interop Forum,", "citeRegEx": "Koch and Piette.,? \\Q2007\\E", "shortCiteRegEx": "Koch and Piette.", "year": 2007}, {"title": "An integrated architecture for demand response communications and control", "author": ["M. LeMay", "R. Nelli", "G. Gross", "C.A. Gunter"], "venue": "in Proc. of the 41st Hawaii International Conference on System Sciences,", "citeRegEx": "LeMay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "LeMay et al\\.", "year": 2008}, {"title": "Residential demand response using reinforcement learning", "author": ["D. ONeill", "M. Levorato", "A.J. Goldsmith", "U. Mitra"], "venue": "In IEEE SmartGridComm,", "citeRegEx": "ONeill et al\\.,? \\Q2010\\E", "shortCiteRegEx": "ONeill et al\\.", "year": 2010}, {"title": "Development and evaluation of fully automated demand response in large facilities", "author": ["M.A. Piette", "O. Sezgen", "D.Watson", "N. Motegi", "C. Shockman", "L. ten Hope"], "venue": "URL http://escholarship.org/uc/item/4r45b9zt", "citeRegEx": "Piette et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Piette et al\\.", "year": 2005}, {"title": "Automated critical peak pricing field tests: 2006 pilot program description and results", "author": ["M.A. Piette", "D.Watson", "N. Motegi", "S. Kiliccote"], "venue": "In LBNL Report 62218,", "citeRegEx": "Piette et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Piette et al\\.", "year": 2007}, {"title": "Industrial power demand response analysis for one-part real-time pricing", "author": ["J.G. Roos", "I.E. Lane"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Roos and Lane.,? \\Q1998\\E", "shortCiteRegEx": "Roos and Lane.", "year": 1998}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 10, "context": "Further many of these decisions have limited financial impact on the consumer [ONeill et al., 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as \u201cdecision fatigue\u201d in [ONeill et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 10, "context": ", 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as \u201cdecision fatigue\u201d in [ONeill et al., 2010]).", "startOffset": 219, "endOffset": 240}, {"referenceID": 10, "context": "Recently, [ONeill et al., 2010] proposed a fully-automated EMS algorithm based on reinforcement learning (RL), which learns how to make optimal decisions for consumers.", "startOffset": 10, "endOffset": 31}, {"referenceID": 10, "context": "In [ONeill et al., 2010], the authors assume these functions have particular mathematical properties, but do not address how these functions might be determined.", "startOffset": 3, "endOffset": 24}, {"referenceID": 10, "context": "In this paper, we propose a novel EMS formulation that addresses the limitations of [ONeill et al., 2010] described above.", "startOffset": 84, "endOffset": 105}, {"referenceID": 10, "context": "In other words, our new RL formulation has eliminated the impractical assumption in [ONeill et al., 2010] that consumers\u2019 dissatisfactions with delay can be captured by known disutility functions.", "startOffset": 84, "endOffset": 105}, {"referenceID": 10, "context": "\u2022 Consumer requests/reservations can have different priorities, whereas in [ONeill et al., 2010], all the consumer requests have the same priority.", "startOffset": 75, "endOffset": 96}, {"referenceID": 10, "context": "[ONeill et al., 2010]) shows that Pt can be modeled as a Markov process.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Specifically, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q\u2217 (see [Bertsekas, 2005]).", "startOffset": 111, "endOffset": 128}, {"referenceID": 4, "context": "RL tries to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other fields, such as artificial intelligence and robotics [Coates et al., 2010].", "startOffset": 190, "endOffset": 211}, {"referenceID": 10, "context": "As has been discussed in [ONeill et al., 2010], it is natural to use RL to solve EMS\u2019s learning problem since EMS needs to learn to make decisions while interacting with the user and the real-time energy price.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "In this subsection, we implement one of the most popular and classical RL algorithms, known as Q-learning [Watkins, 1989].", "startOffset": 106, "endOffset": 121}], "year": 2017, "abstractText": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS\u2019s rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existent formulations, our new formulation (1) does not require explicitly modeling the user\u2019s dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.", "creator": "LaTeX with hyperref package"}}}