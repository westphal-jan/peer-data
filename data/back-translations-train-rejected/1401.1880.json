{"id": "1401.1880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2014", "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation", "abstract": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A relatively well known fact in music cognition is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a reinforcement-learning based framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a learned model of preferences for both individual songs and song transitions. To reduce exploration time, we initialize a model based on user feedback. This model is subsequently updated by reinforcement. We show our algorithm outperforms a more naive approach, using both real song data and real playlist data to validate our approach.", "histories": [["v1", "Thu, 9 Jan 2014 01:50:09 GMT  (862kb,D)", "http://arxiv.org/abs/1401.1880v1", null], ["v2", "Wed, 25 Mar 2015 18:40:46 GMT  (831kb,D)", "http://arxiv.org/abs/1401.1880v2", "-Updated to the most recent and completed version (to be presented at AAMAS 2015) -Updated author list. in Autonomous Agents and Multiagent Systems (AAMAS) 2015, Istanbul, Turkey, May 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad liebman", "maytal saar-tsechansky", "peter stone"], "accepted": false, "id": "1401.1880"}, "pdf": {"name": "1401.1880.pdf", "metadata": {"source": "CRF", "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation", "authors": ["Elad Liebman", "Peter Stone"], "emails": ["eladlieb@cs.utexas.edu", "pstone@cs.utexas.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, recommendation systems have come to the forefront as a field of research [2]. Music has been one of the main domains where recommendation systems have been developed and tested, both academically [2, 4, 15] and commercially [4]. Among well-known commercial applications, one can consider Pandora (and the Music Genome Project), Jango, Last.fm, and many others. Some details of how such recommendation systems predict individual song preferences are well known (machine learning and collaborative filtering are widely used), but music listeners usually listen to music over an extended session that goes beyond a sequence of songs, rather than using a single song in isolation. More importantly, music is experienced in the temporal context and sequence. In other words, the pleasure of listening to a song can be increased or diminished by its relative position in a sequence."}, {"heading": "2. RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3. DATA", "text": "A big part of this work was the extra production of data from the real world for songs and playlists to formalize and test our approach. In this section we present the different data sources we use to model songs and playlists."}, {"heading": "3.1 The Million Song Dataset", "text": "The Million Song Dataset [6] is a freely available collection of audio features and metadata for a million contemporary popular music tracks. At the heart of the dataset is feature analysis and metadata for a million songs, provided by The Echo Nest.1 The dataset includes 44, 745 different performers and, as the name suggests, 106 different tracks. Among the most important acoustic features provided for each song are pitches, timbre and loudness extracted for segments separated by sets of notes."}, {"heading": "3.2 The Yes.com Archive", "text": "This playlist record was collected by Chen et al. [8] The playlists and tag data were each searched by Yes.com and Last.fm. Yes.com is a website that provides radio playlists from hundreds of radio stations in the United States. Using the web-based API http: / / api.yes.com, you can retrieve the playlist record of a particular station for the last 7 days. Chen et al. collected as many playlists as possible by specifying all possible genres and retrieving playlists from all possible stations, collecting data over an extended period from December 2010 to May 2011, resulting in a record of 75, 262 songs and 2, 840, 553 transitions. 3.3 The Art of the Mix Archive 1http: / / the.echonest.com / This corpus was collected by Berenzweig et al. [5] For this corpus, the authors collected about 29,000 playlists from The Art of the Mix (www.fartofechonet.com)."}, {"heading": "3.4 Connecting Records across Data Sources", "text": "After using multiple sources of song data, linking song entries from different sources has proved surprisingly more difficult than expected. First, most songs that appear in one data source do not appear in the other. Furthermore, there may be discrepancies between performer and song names (for example, \"The Smashing Pumpkins\" vs. \"Smashin'Pumpkins\"). To make things even more complicated, we use the following scheme, similar to the procedure in [14]: For each song that is observed in a playlist, we take the artist name as it appears in the playlist, and all other artist names associated with it (if such an identifier exists). We then take all of these potential names and divide them into individual words after we have \"\" cleaned \"non-letter elements such as. We then generate a unified and expanded list of candidate names that includes all potential names as well as their broken down substances."}, {"heading": "4. MODELING", "text": "In this section, we discuss the way both songs and listeners have been modeled. Any vector representation of songs can be used in our framework; the algorithm is more dependent on our specific choices when it comes to representing listeners, but we believe that the framework could be modified or expanded to support other listener architectures."}, {"heading": "4.1 Modeling Songs", "text": "We assume that each song can be included as a result of scalar characteristics. Such scalar characteristics should include details of the song's spectral fingerprint, its rhythmic characteristics, its general loudness, etc. For the scope of this work, we have used the acoustic characteristics of the Million Song Dataset representation to extract 12 meta-characteristics, 2 of which are 12-dimensional, resulting in a 34-dimensional feature vector: 1. 10th percentile of tempo - calculated by calculating a cumulative distribution of bar durations and determining its 10th percentile. 2. 90th percentile of tempo - calculated by calculating a cumulative distribution of bar durations and determining its 90th percentile. 3. The variance in tempo - calculated as a variance of bar duration and width - calculated as an average difference in bar duration. 4. The average of tempo - calculated as an average time duration of the 5th percentile - 10th percentile."}, {"heading": "4.2 Modeling Listeners", "text": "Despite the abundance of literature on the psychology of human perception [20], there is no canonical model of the human listening experience. For the purposes of this work, we model listening as dependent on preferences about characteristics, and the preference for characteristics is consistent with many of the observed characteristics in literature. [7] For each characteristic, we collect statistics on the entire music database and quantify them in percentives. For each percentile, the listener indicates a value that represents the reward. [7]"}, {"heading": "4.3 Expressiveness of the Transition Model", "text": "In order for our features to have a chance to model the transition preferences of different users, they need to be able to at least roughly grasp the difference between sequences commonly perceived as \"good\" or \"bad.\" To gather evidence that the characteristics are meaningful in this way, we study the transition profile for two families of transitions, \"poor\" or \"fair,\" but with the same population of songs. We create \"fair\" transitions by scanning pairs of songs that originally appeared in sequence. We create \"bad\" transitions by merging songs into each other, so that each has a distinctly different character (for example, a heavy metal track followed by a soft jazz piece). The difference between the two profiles is Figure 2. Although we would expect different profiles for different people and for different sets of data, the clear differences (19 out of 34 are discriminatory for this song set) indicate that our feature allows discriminatory user preferences to be predicted."}, {"heading": "4.4 Predictive Power of the Model", "text": "To test how predictable our models are, we took the corpus of Yes.com playlists (see Section 3.2), divided them into a test set and a training set, and then trained a handset model based on the training set. After this step, we examined the cumulative reward that our handset model yielded on the test set. We compared this reward to the cumulative reward that our model received on randomly generated playlists. To obtain sufficient statistics, we repeated this process 50 times and calculated the results. As we hoped, our model yields a much higher reward on the test playlists than on randomly generated playlists. This is clearly shown in Figure 3, which shows the differences in expected reward that the model achieves over real playlists for different train set sizes compared to randomly generated playlists. The exact procedure for training a handset model based on playlists is explained in Section 6.3."}, {"heading": "5. RL FRAMEWORK", "text": "In this section we discuss our amplification learning approach to a playlist-oriented music recommendation system. Two main principles guide the system: \u2022 In order to generate an effective sequence of songs, we need to model not only the myopic preferences of the listener, but also his transition preferences. \u2022 In any online playlist generation task, the extent of explicit exploration is very limited.The first principle is simple. Regarding the second, we note that the potential transition space, even for databases of moderate size, is extremely large. Effectively, no listener would endure a long degree of pure exploration between songs and genres (say, from Russian folk music to Japanese noise rock) until the system \"locks\" on a viable model of his preferences. To make exploration as focused and as effective as possible, we present a model learning scheme that we dub \"Learning Estimates Process from Feedback Generation\" decision, AcLEDP or Model AFLEDP."}, {"heading": "5.1 Model Learning", "text": "The answer to the question of the \"why,\" according to the title, \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \"\", \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \"\", \"\", \",\" \",\" \",\" \",\", \"\" \",\" \"\" \",\" \"\", \"\" \",\" \",\", \"\" \"\" \"\", \",\" \"\", \"\", \"\" \"\", \"\" \",\" \",\" \"\" \",\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \",\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\", \"\", \"\" \",\" \"\" \"\", \"\", \"\", \"\" \"\" \",\" \",\" \"\" \"\" \"\" \"\" \",\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\""}, {"heading": "5.2 Planning Via Tree Search", "text": "After getting estimates of song rewards and song transition rewards, we use a simple tree search heuristically for planning, similar to those used in [21]. As before, weAlgorithm 2 Learning Transition Preferences1: Input: representative representative set = \u03b4 \u2212 Medoids (complete dataset, \u03b4 = 10th percentile pairwise distances) 2: prevSong = a song randomly from representative set 3: while playlist did not reach its end 4: nextSong = argmaxi bevorzuredSet (totalRewardi) (chosen by the listener without disclosing internals for DJMC) 5: for f featureSet 6: Find narrowest percentiles for the value of f in prevSong, p1 7: Find narrowest percentiles for the value of f in nextSong, p2: rewardfp1, fp2 = rewardfp1 9: trend tratratra.we"}, {"heading": "5.3 Credit Assignment for Reward", "text": "As soon as a user (explicitly or implicitly) gives feedback for a transition, this occurs both in real life and in our model in the form of a unified signal that combines both the individual song reward and the transition reward. In order to effectively update our song and transition model, we must recognize each component separately. We achieve this goal by calculating the factors of the expected song and transition rewards in the overall expected reward and using these factors as weighting in the update phase. In other words, since we have observed reward dirt and we expected song and transition rewards to be resonant and r e transition, we update the current song feature profile by resong + r e transition and the current feature transition profile by retransition resong + r e transition."}, {"heading": "5.4 DJ-MC: Complete Agent Architecture", "text": "Finally, we describe the complete architecture of the DJ agent: \u2022 Learning Estimates from Active Feedback (LEAF): 1. The agent collects a preferred number of songs that the listener likes best, independently (i.e. the m-songs that maximize myopic song rewards), and uses this amount to train a model of song rewards. 2. The agent calculates a paired distance matrix between all the songs in the database, and the 10th percentile of distances. 3. The agent distils a representative set from the entire song database using the \u03b4-medoid algorithm and precompressed data. 4. For k-steps, the algorithm selects songs from a representative set and asks the listener which song from a representative set (or a subset of it if the full set is too large)."}, {"heading": "6. EXPERIMENTAL SETUP", "text": "To evaluate our results, we test our algorithm on a large number of random listeners as well as on listener models based on real human playlist data (see Section 3.3). To obtain a song corpus, we try a subset of 1000 songs from the Million Song Dataset in each experiment and use it as a corpus for playlist generation."}, {"heading": "6.1 Baselines", "text": "To measure the improvement offered by our agent, we compare our DJ-MC algorithm with two different baselines: 1. A random agent - an agent who randomly selects songs from the record, and absolute baseline.2. A greedy agent - an agent who uses algorithm 1 to generate an estimate of short-sighted rewards for songs and uses it only without considering transitions.The algorithm generates a model of short-sighted preferences, ranks all songs according to the expected short-sighted reward, and then plays them in order from the highest ranking to the end of the playlist. This method can be seen as closer to the simple approach of learning songs independently of each other.We show that our algorithm performs better by modelling transitions (even partially and roughly) than a simpler approach that ignores transition rewards."}, {"heading": "6.2 Random Listeners", "text": "First of all, we evaluate DJ-MC based on the agent's ability to adapt to purely random agents. A random value was assigned to each listener for characteristics and feature transitions in the range [0, 1] uniformly and independently for each feature and function transition. This is a challenging task, as such listeners have a greater variety and unpredictability than most human listeners, which are easier to categorize. We tested DJ-MC over 20 randomly generated listeners and calculated the results for each of the three approaches we tested (random, greedy and our agent architecture) on average. For these experiments, we used a playlist length of 50 songs, a planning horizon of 10 songs in advance, a computational budget of 100 random trajectories for planning, a query size of 10 songs for modeling the song reward and 10 songs for the transition reward. The results are shown in Figure 4.The DJ-Agent is the most conspicuous at the beginning of the calculation - which is easier to work with the DJ-Agent."}, {"heading": "6.3 Modeling Listeners via Playlists", "text": "To generate potential human listeners, we use the Art of the Mix data set, which, as already indicated, was collected from real user submissions. Then, we use Kmean clusters on the playlists (represented as interpreter frequency vectors) to generate 10 playlist clusters. Each such cluster represents a basic listener type. To generate listeners, we try out 70% of the song transition pairs in a playlist cluster and use them to train a listener model, similar to how DJ-MC learns a transition model for a user of given examples. We repeat this process 20 times, first selecting a random cluster and then selecting a random transition environment to train the listener on it. As before, we test DJ-MC over 20 randomly generated listeners and evaluate the results for each of the three approaches we test (random, greedy and DJ-MC), one out of 100, one out of 10 for these experiments, one out of 50 for each track length, one out of 10 for each track length."}, {"heading": "7. DISCUSSION & FUTURE WORK", "text": "This year, we will be able to establish ourselves in the region until we are able to assert ourselves in the region."}, {"heading": "8. REFERENCES", "text": "[1] Representative selection in non metric datasets. In The 31stInternational Conference on Machine Learning (ICML 2014) - submitted for review, 2014. [2] G. Adomavicius and A. Tuzhilin. Toward the next generation of commender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering, IEEE Transactions on, 17 (6): 734-749, 2005. [3] N. Aizenberg, Y. Koren, and O. Somekh. Build your own music commender by modeling internet radio streams. In Proceedings of the 21st international conference on World Wide Web, pages 1-10. ACM, 2012. [4] N. Barrington, R. Oda, and G. Lanckriet. Smarter than genius? human evaluation of music recommender systems. In International Symposium on Music Information Retval, 2009."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Build your own music recommender by modeling internet radio streams", "author": ["N. Aizenberg", "Y. Koren", "O. Somekh"], "venue": "Proceedings of the 21st international conference on World Wide Web, pages 1\u201310. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Smarter than genius? human evaluation of music recommender systems", "author": ["L. Barrington", "R. Oda", "G. Lanckriet"], "venue": "International Symposium on Music Information Retrieval", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A large-scale evaluation of acoustic and subjective music-similarity measures", "author": ["A. Berenzweig", "B. Logan", "D.P. Ellis", "B. Whitman"], "venue": "Computer Music Journal, 28(2):63\u201376", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida, pages 591\u2013596. University of Miami", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Working memory in music: A theoretical model", "author": ["W.L. Berz"], "venue": "Music Perception, pages 353\u2013364", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Playlist prediction via metric embedding", "author": ["S. Chen", "J.L. Moore", "D. Turnbull", "T. Joachims"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 714\u2013722. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Hang the dj: Automatic sequencing and seamless mixing of dance-music tracks", "author": ["D. Cliff"], "venue": "HP LABORATORIES TECHNICAL REPORT HPL, (104)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic playlist composition in a dynamic music landscape", "author": ["M. Crampes", "J. Villerd", "A. Emery", "S. Ranwez"], "venue": "Proceedings of the 2007 international workshop on Semantically aware document processing and indexing, pages 15\u201320. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "The psychology of music", "author": ["J.B. Davies", "J.B. Davies"], "venue": "Hutchinson London", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "Speech and language processing: An introduction to natural language processing", "author": ["D. Jurafsky", "J.H. Martin", "A. Kehler", "K. Vander Linden", "N. Ward"], "venue": "computational linguistics, and speech recognition, volume 2. MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["F. Maillet", "D. Eck", "G. Desjardins", "P. Lamere"], "venue": "Steerable playlist generation by learning song similarity from radio station playlists. In ISMIR, pages 345\u2013350", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The natural language of playlists", "author": ["B. McFee", "G.R. Lanckriet"], "venue": "ISMIR, pages 537\u2013542", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Trust in recommender systems", "author": ["J. O\u2019Donovan", "B. Smyth"], "venue": "In Proceedings of the 10th international conference on Intelligent user interfaces,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Papa: Physiology and purpose-aware automatic playlist generation", "author": ["N. Oliver", "L. Kreger-Stickles"], "venue": "Proc. 7th Int. Conf. Music Inf. Retrieval, pages 250\u2013253", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast embedding of sparse similarity graphs", "author": ["J.C. Platt"], "venue": "Advances in Neural Information Processing Systems, page None", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Psychology of music", "author": ["C.E. Seashore"], "venue": "New York: Dover Publications", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1938}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in artificial intelligence, 2009:4", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Psychology of music: From sound to significance", "author": ["S.-L. Tan", "P. Pfordresher", "R. Harr\u00e9"], "venue": "Psychology Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A learning agent for heat-pump thermostat control", "author": ["D. Urieli", "P. Stone"], "venue": "In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multi-tasking with joint semantic spaces for large-scale music annotation and retrieval", "author": ["J. Weston", "S. Bengio", "P. Hamel"], "venue": "Journal of New Music Research, 40(4):337\u2013348", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical models of music-listening sessions in social media", "author": ["E. Zheleva", "J. Guiver", "E. Mendes Rodrigues", "N. Mili\u0107-Frayling"], "venue": "Proceedings of the 19th international conference on World wide web, pages 1019\u20131028. ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, recommender systems have risen in prominence as a field of research [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 2, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 118, "endOffset": 128}, {"referenceID": 2, "context": "Music has been one of the main domains in which recommender systems have been developed and tested, both academically [2, 4, 15] and commercially [4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 9, "context": "More importantly, it is a relatively well known fact in music cognition that music is experienced in temporal context and in sequence [11,18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 16, "context": "More importantly, it is a relatively well known fact in music cognition that music is experienced in temporal context and in sequence [11,18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 7, "context": "This fact is also well known among DJs, who perceive playlist-building as a whole [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 8, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 14, "context": "Indeed, some work in automated playlist building has attempted to attack this problem from the DJs perspective [9, 10, 16].", "startOffset": 111, "endOffset": 122}, {"referenceID": 15, "context": "[17] use semantic tags to learn a Gaussian process kernel function between pairs of songs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] learn an embedding in a shared space of social tags, acoustic features and artist entities by optimizing an evaluation metric for various music retrieval tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[3] put forth a probabilistic approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "These works have close ties with the larger body of research dealing with sequence modeling in natural language processing [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "via collaborative filtering [19]) are known, the specifics of how a certain order is determined are not revealed, nor is it known which criteria are used.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "[13] approach the playlist prediction problem from a supervised binary classification perspective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Mcfee and Lanckriet [14] consider playlists as a natural language induced over songs, training a bigram model for transitions and observing playlists as Markov chains.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "[8] take on a similar Markov approach, but do not rely on any acoustic or semantic information about the songs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[23] adapt a Latent Dirichlet Allocation model to capture music taste from listening activities across users, and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The Million Song Dataset [6] is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "To overcome these issues, we use the following scheme, somewhat similar to the procedure in [14]: for each song observed in a playlist, we take the artist name as it appears in the playlist, and all other artist names associated with it (if such an identifier exists).", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Despite an abundance of literature on the psychology of human musical perception [20], there is no canonical model of the human listening experience.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 9, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 16, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "Despite its relative simplicity, this model is fairly consistent with many of the observed properties of human perception as indicated in the literature [7, 11,18,20].", "startOffset": 153, "endOffset": 166}, {"referenceID": 19, "context": "Once we\u2019ve obtained estimates of song rewards and song transition rewards, we employ a simple tree-search heuristic for planning, similar to that used in [21].", "startOffset": 154, "endOffset": 158}], "year": 2017, "abstractText": "In recent years, there has been growing focus on the study of automated recommender systems. Music recommendation systems serve as a prominent domain for such works, both from an academic and a commercial perspective. A relatively well known fact in music cognition is that music is experienced in temporal context and in sequence. In this work we present DJ-MC, a reinforcement-learning based framework for music recommendation that does not recommend songs individually but rather song sequences, or playlists, based on a learned model of preferences for both individual songs and song transitions. To reduce exploration time, we initialize a model based on user feedback. This model is subsequently updated by reinforcement. We show our algorithm outperforms a more naive approach, using both real song data and real playlist data to validate our approach.", "creator": "LaTeX with hyperref package"}}}