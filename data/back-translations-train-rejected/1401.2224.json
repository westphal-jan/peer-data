{"id": "1401.2224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "A Comparative Study of Reservoir Computing for Temporal Signal Processing", "abstract": "Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks. In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. A readout layer is then trained to reconstruct a target output from the reservoir's state. The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC. In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively. We compare the performance of the three systems while computing three common benchmark time series: H{\\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs. The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN.", "histories": [["v1", "Fri, 10 Jan 2014 03:39:28 GMT  (752kb,D)", "http://arxiv.org/abs/1401.2224v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["alireza goudarzi", "peter banda", "matthew r lakin", "christof teuscher", "darko stefanovic"], "accepted": false, "id": "1401.2224"}, "pdf": {"name": "1401.2224.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study of Reservoir Computing for Temporal Signal Processing", "authors": ["Alireza Goudarzi", "Peter Banda", "Matthew R. Lakin", "Christof Teuscher", "Darko Stefanovic"], "emails": ["alirezag@cs.unm.edu."], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. OBJECTIVES", "text": "Our main objective is to compare the calculation of time series in the RC paradigm, in which memory and computation are integrated, with two basic calculation methods for time series: firstly, a device with perfect memory and no computing power, ordinary linear regression on the delay line (DL), and secondly, a device with limited memory and arbitrary computing power, a nonlinear auto-regressive exogenous (NARX) neural network. This is a first step towards a systematic investigation of topology, memory, compression and dynamics in the RC. In this article, we will limit ourselves to ESNs with a fully networked reservoir and input Xiv: 140 1.22 24v1 [cs.NE] 1.0Ja n20 14layer. We will examine the performance of ESN and the auto-regressive model in solving three time series problems: the compression of the NARMA time series 10th order, the comparison of the NARMA series 20th order and the autoregressive order of the order [ESN 24]."}, {"heading": "III. A BRIEF SURVEY OF PREVIOUS WORK", "text": "The first conception of the RC paradigm in the recursive neural network (RNN) of the community Jaeger's Echo State Network (ESN) [26]. In this early ESN, the reservoir consisted of N fully connected sigmoidal nodes. However, the reservoir connectivity was represented by a weight matrix with elements taken from a uniform distribution at the interval [\u2212 1,1]. The weight matrix was then mapped to a spectral radius of \u03bb < 1, a sufficient condition for ESN connectivity was associated with all reservoir nodes and their weight proportions. The weight matrix was randomly mapped from the set {\u2212 1,1}. Later, Jaeger [27] suggested that the scarcity of the connection weight matrix would improve performance and therefore only 20% of the connections were mapped."}, {"heading": "IV. MODELS", "text": "To understand the calculation of the reservoirs, we compare their behavior to a system with perfect memory and no computing power, and a system with limited memory and arbitrary computing power. We select delay line systems, NARX neural networks, and echo state networks as described below. We use U (t), Y (t), and Y (t) to designate the time-dependent input signal, time-dependent output signal, and time-dependent target signal, respectively."}, {"heading": "A. Delay line", "text": "In order to compare the calculation in a reservoir with the DL, we use a linear display layer and connect it to read the states of the DL. Figure 2 (a) is a scheme for this architecture. Note that this architecture differs from the delay line used in [24] in that the input is connected to only a single unit in the delay line. No calculation is performed by the delay units. The system is then fed with a teacher input and the weights are trained using an ordinary linear regression on the teacher output as follows: Wout = (XT \u00b7 X) \u2212 1 \u00b7 XT \u00b7 Y, (1) where each line of X represents the state of the DL at a given time X (t0) and the rows of Y for the corresponding time Y (t0) are the teacher output. Note that the DL states are supplemented by a distortion unit with a constant value b = 1."}, {"heading": "B. NARX Network", "text": "The NARX network is an auto-regressive neural network architecture with a tapped delay input and one or more hidden layers. Both the hidden layers and the output layer have a bias input with a constant value b = 1. We use tanh as the transfer function for the hidden layer and a linear transfer function for the output layer. The network is trained using the Marquardt algorithm [35]. This architecture performs a nonlinear regression on the teacher output, using the previous values of the input accessible via the tapped delay line. A scheme of this architecture is shown in Figure 2 (b). Since we want to investigate the effects of regression complexity on performance, we set the length of the tapped delay to 10 and vary the number of hidden nodes."}, {"heading": "C. Echo State Network", "text": "In our ESN, the reservoir consists of a fully connected network of N nodes with a constant bias node b = 1. The input and output nodes are connected to all reservoir nodes. The input weight matrix is an I \u00b7 N matrix Win = [wini, j], where I is the number of input nodes and winj, i is the weight of the connection from the input node i to the reservoir node j. The connection weights within the reservoir are represented by a N \u00b7 N matrix Wres = [wresj, k], where wresj, k is the weight of the connection from the node k to the node j in the reservoir. The output weight matrix is one (N + 1 (\u00b7 O matrix Wout = [woutl, k], where O is the number of output nodes and woutl, k is the weight of the connection from the node k to the node j in the reservoir."}, {"heading": "V. EVALUATION METHODOLOGY", "text": "Studying task-solving power and analyzing computing power in the RC is a major challenge because there are a variety of RC architectures, each of which has a unique set of parameters that potentially affect performance; the optimal RC parameters are task-specific and must be adapted experimentally; in addition, not all studies use the same tasks and the same performance indicators to evaluate their results; in contrast to classic computational models, in which a programmed automaton acts on a storage device, the RC is also a dynamic system in which memory and computing power are inseparable components of a single phenomenon; in other words, in the RC, the same dynamic process that performs the calculation retains the memory of previous results and inputs; therefore, it is not clear how much of the RC's performance is due to its storage capacity and how much to its computing power. As a method of approaching this problem, we are trying to create a functional comparison between the ESDL, NARX and NARX."}, {"heading": "A. Tasks", "text": "1) He \u0301 non Map Time Series: This time series is generated by the following system: yt = 1 \u2212 1.4y2t \u2212 1 + 0.3yt \u2212 2 + zt, (4) where zt is a white noise term with standard deviation 0.001. This is an example of a task that requires limited calculation and memory and can therefore be used as a starting point for evaluating ESN performance. 2) NARMA 10 Time Series: Nonlinear autoregressive moving average (NARMA) is a discrete time task with 10th order time delay. To simplify notation, we use yt to denote (t). NARMA 10 Time Series is given by: yt = \u03b1yt \u2212 1 + \u03b2yt \u2212 1 n \u2211 i = 1 yt \u2212 i + ctuut \u2212 mother \u2212 1 + \u03b4, (5) where n = 10, \u03b1 = 0.3, \u03b2 = 0.05, \u03b3 = 1.5 = 0.1."}, {"heading": "B. Error Calculation", "text": "In the case of time series analysis, each study can use a different error calculation to measure the performance of the presented methods. We present three different error calculations, which are commonly used in the literature to analyze time series. We use y to refer to time-dependent output and y to refer to target output. Expectation operator < \u00b7 > refers to the time average of its operand.1) Root of normalized average error to square: The most commonly used measurement, however, is a normalized average error to square (RNMSE) calculated as follows: RNMSE = \u221a < (y) 2 > RNMSE-based average error to square: However, the most commonly used measurement is a normalized average error to square (RNMSE) calculated as follows: RNMSE = (y)"}, {"heading": "C. Reservoir optimization", "text": "These parameters are optimised by offline cross-validation [24], [36] or by online adaptation [19], [37]. This is a preliminary stage before the function comparison. We are interested in scaling these parameters, which we are systematically examining. Figure 3 (a) -3 (c) shows the resulting error surface by averaging the result of the 10 runs of each \u03c3w-N combination. We observe that the ESN performance reacts more sensitively to changes in \u0441w as a non-linearity of the task and its required memory gain, and favours a more heterogeneous weight allocation (greater weight). We find the optimum standard deviation w as a function of N for each task: \u043c w (N) = arg min Prospectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropectropec"}, {"heading": "D. Functional Comparison", "text": "Here we are trying to compare the ESN size with the size of an equivalent device that has only memory capacity and no computing power, and with a device with limited memory and theoretically arbitrary computing power. A DL of length n stores the perfect memory of the past n inputs, and a NARX network with N hidden units can be a universal approximator for any time series. We want to compare the performance of a linear reader with access to the reservoir state with a linear display with access to the delay line states and also to the performance of the NARX network. It is clear that the ESN, DL and NARX network are very different, i.e. memory and computing power of ESN, DL and NARX networks differ significantly, even with identical N. Therefore, we are performing a functional comparison in which we examine the ESN, DL and NARX network with the same NRNON SEMI task."}, {"heading": "E. Experimental Setup", "text": "In this section, we describe the parameters and datasets used in our simulations. Training and testing for delay lines and NARX networks are done by generating 10 time series of 4,000 time steps each. We used 20 time series of the same length for ESNs. We used the first 2,000 time steps of each time series to train the model and the second 2,000 steps to test the model. Training and test results are then averaged over the 10 runs. Model-specific settings are as follows: 1) Delay line: A delay line is a fixed system with only one parameter N defining the number of taps. For this study, we limit ourselves to 1 \u2264 N \u2264 2,000. We have also experimented with 2,000 \u2264 N \u2264 3,000, but the performance of the delay line does not change for N > 2,000. We take N = 2,000 as the largest delay line for this study. 2) NARX neural network: Since the initial NX is sensitive to the initial network, we place the NX weights in each case."}, {"heading": "VI. RESULTS", "text": "In all three tasks, the delay line shows a significant decrease in the error rate once the system acquires enough capacity to hold all the required information for the task. However, this is N = 2 for He \u0301 non Map, N = 10 for NARMA 10 task, and N = 20 for NARMA 20 task. After this point, the error slowly decreases to N = 2,000, where RNMSE \u2248 0.14 occurs after a sharp drop in the error rate due to the \"dimensionality curse\": the specified teacher time series is not representative of the extended state space of the delay line. This is expected to be reflected in the high test errors in Figure 4 (f) -4. Another expected behavior of the overmatch to training data is that when the distribution of the data is larger and for narrower distributions of the errors."}, {"heading": "VII. DISCUSSION AND OPEN PROBLEMS", "text": "The reservoir in an ESN is a dynamic system in which memory and computation are inextricably linked. To understand this type of information processing, we compared the ESN performance with a storage device, the DL, and a limited memory, but computationally powerful device, the NARX network. However, our results show that the performance of ESN is not only due to its storage capacity; ESN reading does not create authority regression of the input, as in the DL or the NARX network. The information processing that takes place within a reservoir is fundamentally different from other types of neural networks. The exact mechanism of this process remains to be investigated. The investigation of reservoir computing usually takes place by analyzing the system performance for task resolution with various com-8 putative and storage requirements. To understand the details of information processing in a reservoir, we need to understand the impact of the reservoir architecture on its basic storage capacity."}, {"heading": "VIII. CONCLUSION", "text": "Reservoir computing is an approach to neural network formation that has been successful in machine learning, time series analysis, robot control, and sequence learning. There have been many studies aimed at understanding the functioning of the RC and the factors that influence its performance. However, due to the complexity of the reservoir in the RC, none of these studies were entirely satisfactory and often led to conflicting conclusions. In this paper, we compared the performance of three approaches to time series analysis: the delay line, the NARX network, and the ESN. These methods vary in their storage capacity and computing power. The delay line preserves a perfect memory of the past, but has no computing power. The NARX network has limited memory of the past, but can in principle perform any calculation. Finally, the ESN does not have explicit access to past memory, but its reservoir represents the past by implicitly computing it."}, {"heading": "ACKNOWLEDGMENT", "text": "The work was supported by NSF grants # 1028238 and # 1028120. M.R.L. is grateful for the support of the New Mexico Cancer Nanoscience and Microsystems Training Center."}, {"heading": "APPENDIX A", "text": "FITTING \u03c3wBefore we can adjust \u03c3 \u0445 w, we must interpolate the data points on the error surface with a linear fit, which allows us to use all the values of N and \u03c3w and generate a smooth fit. Table I shows the quality of the fit statistics for the linear fit to the error surface. Low SSE and high R2 statistics on this fit show that the surface accurately represents the data points. We then calculate the task SSE R2 He'non Map 1.5486 \u00d7 10 \u2212 31 1 NARMA 10 2.989 \u00d7 10 \u2212 31 1 NARMA 20 7.3725 \u00d7 10 \u2212 31 1Table I GOOD OF FIT STATISTICS FOR THE ERROR SURFACE (FIGURE 3) 1 NARMA 10 2.989 \u00d7 10 \u2212 31 1 NARMA 20 7.3725 \u00d7 10 \u2212 31 1Table I GOOD OF FIT STATISTICS FOR THE INTERROR SURFICT FOR THE SUPERFICT FOR THE SUACTER SUACR."}, {"heading": "APPENDIX B THE PERFORMANCE RESULTS", "text": "Tables III, IV and V show the average test and training errors of optimal ESNs, delay lines and NARX networks for the three different tasks based on three different metrics: RNMSE, NRMSE and SAMP."}], "references": [{"title": "An overview of reservoir computing: theory, applications and implementations", "author": ["B. Schrauwen", "D. Verstraeten", "J.V. Campenhout"], "venue": "Proceedings of the 15th European Symposium on Artificial Neural Networks, 2007, pp. 471\u2013482.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparative study of reservoir computing strategies for monthly time series prediction", "author": ["F. Wyffels", "B. Schrauwen"], "venue": "Neurocomputing, vol. 73, no. 10\u201312, pp. 1958\u20131964, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1958}, {"title": "Adaptive nonlinear system identification with echo state networks", "author": ["H. Jaeger"], "venue": "NIPS, 2002, pp. 593\u2013600.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Reservoir computing approaches to  10 H\u00e9non Map SSE 0.0011 R2 0.5670 a \u22125.733\u00d710\u22128\u00b12.9430\u00d710\u22127 b 1.795\u00b10.7450 c 0.02053\u00b10.0016 NARMA 10 SSE 2.9686\u00d710\u22124 R2 0.9926 a 0.306\u00b10.0095 b \u22120.2609\u00b10.0249 c  \u22120.02537\u00b10.0079 NARMA 20 SSE 5.8581\u00d710\u22124 R2 0.9909 a \u22120.03441\u00b10.0143 b 0.2156\u00b10.0408 c 0.1766\u00b10.0210 Table II GOODNESS OF FIT STATISTICS FOR THE POWER-LAW FIT axb + c TO \u03c3\u2217  w (FIGURE 3) AS A FUNCTION OF RESERVOIR SIZE N FOR ALL THREE TASKS. recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review, vol. 3, no. 3, pp. 127\u2013149, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A theoretical and experimental study of neuromorphic atomic switch networks for reservoir computing", "author": ["H.O. Sillin", "R. Aguilera", "H.-H. Shieh", "A.V. Avizienis", "M. Aono", "A.Z. Stieg", "J.K. Gimzewski"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384004, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamics of coupled cavities for optical reservoir computing", "author": ["M. Fiers", "B. Maes", "P. Bienstman"], "venue": "Proceedings of the 2009 Annual Symposium of the IEEE Photonics Benelux Chapter, S. Beri, P. Tassin, G. Craggs, X. Leijtens, and J. Danckaert, Eds. VUB Press, 2009, pp. 129\u2013132.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Integration of nanoscale memristor synapses in neuromorphic computing architectures", "author": ["G. Indiveri", "B. Linares-Barranco", "R. Legenstein", "G. Deligeorgis", "T. Prodromakis"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384010, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Nano-scale reservoir computing", "author": ["O. Obst", "A. Trinchi", "S.G. Hardin", "M. Chadwick", "I. Cole", "T.H. Muster", "N. Hoschke", "D. Ostry", "D. Price", "K.N. Pham", "T. Wark"], "venue": "Nano Communication Networks, vol. 4, no. 4, pp. 189\u2013196, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Optoelectronic reservoir computing", "author": ["Y. Paquot", "F. Duport", "A. Smerieri", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Scientific Reports, vol. 2, 02 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern recognition in a bucket", "author": ["C. Fernando", "S. Sojakka"], "venue": "Advances in Artificial Life, ser. Lecture Notes in Computer Science, W. Banzhaf, J. Ziegler, T. Christaller, P. Dittrich, and J. Kim, Eds. Springer Berlin Heidelberg, 2003, vol. 2801, pp. 588\u2013597.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "DNA reservoir computing: A novel molecular computing approach", "author": ["A. Goudarzi", "M.R. Lakin", "D. Stefanovic"], "venue": "DNA Computing and Molecular Programming, ser. Lecture Notes in Computer Science, D. Soloveichik and B. Yurke, Eds. Springer International Publishing, 2013, vol. 8141, pp. 76\u201389.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, vol. 304, no. 5667, pp. 78\u201380, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Real-time computing without stable states: a new framework for neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Computation, vol. 14, no. 11, pp. 2531\u201360, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Information dynamics and emergent computation in recurrent circuits of spiking neurons", "author": ["T. Natschl\u00e4ger", "W. Maass"], "venue": "Proc. of NIPS 2003, Advances in Neural Information Processing Systems, S. Thrun, L. Saul, and B. Schoelkpf, Eds., vol. 16. Cambridge: MIT Press, 2004, pp. 1255\u20131262.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["N. Bertschinger", "T. Natschl\u00e4ger"], "venue": "Neural Computation, vol. 16, no. 7, pp. 1413\u20131436, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Computational capabilities of random automata networks for reservoir computing", "author": ["D. Snyder", "A. Goudarzi", "C. Teuscher"], "venue": "Phys. Rev. E, vol. 87, p. 042808, Apr 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectivity, dynamics, and memory in reservoir computing with binary and analog neurons.", "author": ["L. B\u00fcsing", "B. Schrauwen", "R. Legenstein"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Initialization and self-organized optimization of recurrent neural network connectivity", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "HFSP Journal, vol. 3, no. 5, pp. 340\u2013349, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \u201c echo state network\u201d approach", "author": ["H. Jaeger"], "venue": "German National Research Center for Information Technology, St. Augustin- Germany, Tech. Rep. GMD Report 159, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrent kernel machines: Computing with infinite echo state networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "Neural Computation, vol. 24, no. 1, pp. 104\u2013133, 2013/11/22 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Reservoir computing trends", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger", "B. Schrauwen"], "venue": "KI - K\u00fcnstliche Intelligenz, vol. 26, no. 4, pp. 365\u2013371, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "New results on recurrent network training: unifying the algorithms and accelerating convergence", "author": ["A. Atiya", "A. Parlos"], "venue": "Neural Networks, IEEE Transactions on, vol. 11, no. 3, pp. 697\u2013709, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimum complexity echo state network", "author": ["A. Rodan", "P. Tino"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 1, pp. 131\u2013144, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A two-dimensional mapping with a strange attractor", "author": ["M. H\u00e9non"], "venue": "Communications in Mathematical Physics, vol. 50, no. 1, pp. 69\u201377, 1976.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1976}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "St. Augustin: German National Research Center for Information Technology, Tech. Rep. GMD Rep. 148, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Short term memory in echo state networks", "author": ["\u2014\u2014"], "venue": "GMD- Forschungszentrum Informationstechnik, Tech. Rep. GMD Report 152, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "An experimental unification of reservoir computing methods", "author": ["M.D.D. Verstraeten", "B. Schrauwen", "D. Stroobandt"], "venue": "Neural Networks, vol. 20, no. 3, pp. 391\u2013403, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Effects of spectral radius and settling time in the performance of echo state networks", "author": ["G.K. Venayagamoorthy", "B. Shishir"], "venue": "Neural Networks, vol. 22, no. 7, pp. 861 \u2013 863, 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Architectural and markovian factors of echo state networks", "author": ["C. Gallicchio", "A. Micheli"], "venue": "Neural Networks, vol. 24, no. 5, pp. 440 \u2013 456, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Effects of connectivity structure of complex echo state network on its prediction performance for nonlinear time series", "author": ["Q. Song", "Z. Feng"], "venue": "Neurocomputing, vol. 73, no. 10\u201312, pp. 2177 \u2013 2185, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Complex brain networks: graph theoretical analysis of structural and functional systems", "author": ["E. Bullmore", "O. Sporns"], "venue": "Nat Rev Neurosci, vol. 10, no. 4, pp. 312\u2013312, 04 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Small world topology of dynamic reservoir for effective solution of memory guided tasks", "author": ["S. Dasgupta", "P. Manoonpong", "F. Woergoetter"], "venue": "Frontiers in Computational Neuroscience, no. 177.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 0}, {"title": "Mean-field theory of echo state networks", "author": ["M. Massar", "S. Massar"], "venue": "Phys. Rev. E, vol. 87, p. 042809, Apr 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Training feedforward networks with the marquardt algorithm", "author": ["M. Hagan", "M.-B. Menhaj"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 6, pp. 989\u2013993, 1994.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Optimization and applications of echo state networks with leaky- integrator neurons", "author": ["H. Jaeger", "M. Luko\u0161evi\u010dius", "D. Popovici", "U. Siewert"], "venue": "Neural Networks, vol. 20, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Information theoretic self-organised adaptation in reservoirs for temporal memory tasks", "author": ["S. Dasgupta", "F. Wrgtter", "P. Manoonpong"], "venue": "Engineering Applications of Neural Networks, ser. Communications in Computer and Information Science, C. Jayne, S. Yue, and L. Iliadis, Eds. Springer Berlin Heidelberg, 2012, vol. 311, pp. 31\u201340.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression error characteristic curves.", "author": ["J. Bi", "K.P. Bennett"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "A comparison of different ROC measures for ordinal regression", "author": ["W. Waegeman", "B. De Baets", "L. Boullart"], "venue": "Proceedings of the 3rd International Workshop on ROC Analysis in Machine Learning., N. Lachiche, C. Ferri, and S. Macskassy, Eds., 2006, pp. 63\u201369.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "ROC analysis in ordinal regression learning", "author": ["\u2014\u2014"], "venue": "Pattern Recogn. Lett., vol. 29, no. 1, pp. 1\u20139, Jan. 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "A generalization error estimate for nonlinear systems", "author": ["J. Larsen"], "venue": " 11 in Neural Networks for Signal Processing, 1992. II., Proceedings of the 1992 IEEE Workshop on, 1992, pp. 29\u201338.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive regularization in neural network modeling", "author": ["J. Larsen", "C. Svarer", "L. Andersen", "L. Hansen"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, G. Orr and K.-R. Mller, Eds. Springer Berlin Heidelberg, 1998, vol. 1524, pp. 113\u2013132.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Design and regularization of neural networks: the optimal use of a validation set", "author": ["J. Larsen", "L. Hansen", "C. Svarer", "M. Ohlsson"], "venue": "Neural Networks for Signal Processing, 1996. VI. Proceedings of the 1996 IEEE Workshop on, 1996, pp. 62\u201371.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Empirical generalization assessment of neural network models", "author": ["J. Larsen", "L. Hansen"], "venue": "Neural Networks for Signal Processing, 1995. V. Proceedings of the 1995 IEEE Workshop on, 1995, pp. 30\u201339.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "Generalization performance of regularized neural network models", "author": ["\u2014\u2014"], "venue": "Neural Networks for Signal Processing, 1994. IV. Proceedings of the 1994 IEEE Workshop on, 1994, pp. 42\u201351.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "Lessons in neural network training: Overfitting may be harder than expected", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi"], "venue": "In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI-97. AAAI Press, 1997, pp. 540\u2013545.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1997}, {"title": "Network information criterion-determining the number of hidden units for an artificial neural network model", "author": ["N. Murata", "S. Yoshizawa", "S.-I. Amari"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 6, pp. 865\u2013872, 1994.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1994}, {"title": "Prediction risk and architecture selection for neural networks", "author": ["J. Moody"], "venue": "From Statistics to Neural Networks, ser. NATO ASI Series, V. Cherkassky, J. Friedman, and H. Wechsler, Eds. Springer Berlin Heidelberg, 1994, vol. 136, pp. 147\u2013165.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1994}, {"title": "Pruning from adaptive regularization", "author": ["L.K. Hansen", "C.E. Rasmussen"], "venue": "Neural Computation, vol. 6, no. 6, pp. 1223\u20131232, 2013/12/11 1994.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Quantifying multivariate classification performance: the problem of overfitting", "author": ["B.R. Stallard", "J.G. Taylor"], "venue": "pp. 426\u2013436, 1999.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "The problem of overfitting", "author": ["D.M. Hawkins"], "venue": "Journal of Chemical Information and Computer Sciences, vol. 44, no. 1, pp. 1\u201312, 2004.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "What size net gives valid generalization?", "author": ["E. Baum", "D. Haussler"], "venue": "Neural Computation,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1989}, {"title": "What size network is good for generalization of a specific task of interest?", "author": ["B. Amirikian", "H. Nishimura"], "venue": "Neural Networks,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1994}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R.E. Schapire", "L. Sellie"], "venue": "Proceedings of the Twenty-sixth Annual ACM Symposium on Theory of Computing, ser. STOC \u201994. New York, NY, USA: ACM, 1994, pp. 273\u2013282.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1994}, {"title": "Quantifying a critical training set size for generalization and overfitting using teacher neural networks", "author": ["R. Lange", "R. M\u00e4nner"], "venue": "ICANN \u201994, M. Marinaro and P. G. Morasso, Eds. Springer London, 1994, pp. 497\u2013500.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1994}, {"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM, vol. 27, no. 11, pp. 1134\u20131142, 1984.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Reservoir computing is a recent development in recurrent neural network research with applications to temporal pattern recognition [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 10, "context": "RC\u2019s performance in time series processing tasks and its flexible implementation has made it an intriguing concept in machine learning and unconventional computing communities [2]\u2013[12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 193, "endOffset": 197}, {"referenceID": 13, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 199, "endOffset": 203}, {"referenceID": 13, "context": "Several studies attributed this property to the dynamical regime of the reservoir and showed it to be optimal when the system operates in the critical dynamical regime\u2014a regime in which perturbations to the system\u2019s trajectory in its phase space neither spread nor die out [15]\u2013[19].", "startOffset": 273, "endOffset": 277}, {"referenceID": 17, "context": "Several studies attributed this property to the dynamical regime of the reservoir and showed it to be optimal when the system operates in the critical dynamical regime\u2014a regime in which perturbations to the system\u2019s trajectory in its phase space neither spread nor die out [15]\u2013[19].", "startOffset": 278, "endOffset": 282}, {"referenceID": 12, "context": "[14] proved that given the two properties of separation and approximation, a reservoir system is capable of approximating any time series.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Jaeger [20] proposed that an ideal reservoir needs to have the so-called echo state property (ESP), which means that the reservoir states asymptotically depend on the input and not the initial state of the reservoir.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "It has also been suggested that the reservoir dynamics acts like a spatiotemporal kernel, projecting the input signal onto a high-dimensional feature space [5], [21].", "startOffset": 156, "endOffset": 159}, {"referenceID": 19, "context": "It has also been suggested that the reservoir dynamics acts like a spatiotemporal kernel, projecting the input signal onto a high-dimensional feature space [5], [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "RC\u2019s robustness to the underlying implementation as well as its efficient training algorithm makes it a suitable choice for time series analysis [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 178, "endOffset": 182}, {"referenceID": 23, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 202, "endOffset": 206}, {"referenceID": 24, "context": "The first conception of the RC paradigm in the recurrent neural network (RNN) community was Jaeger\u2019s echo state network (ESN) [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "Later, Jaeger [20], [27] proposed that the sparsity of the connection weight matrix would improve performance and therefore only 20% of the connections were assigned weights from the set {\u221247,47}.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Later, Jaeger [20], [27] proposed that the sparsity of the connection weight matrix would improve performance and therefore only 20% of the connections were assigned weights from the set {\u221247,47}.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "[28] used a 50% sparse reservoir and a normal distribution for the connection weights, and scaled the weight matrix posteriori to ensure the ESP; also, only 10% of the nodes were connected to the input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "This study indicated that, contrary to the earlier report by Jaeger [26], the performance of the reservoir was sensitive to the spectral radius and showed optimality for \u03bb \u2248 1.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "Venayagamoorthy and Shishir [29] demonstrated experimentally that the spectral radius also affects training time, but, they did not study spectral radii larger than one.", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "Gallicchio and Micheli [30] provided evidence that the sparsity of the reservoir has a negligible effect on ESN performance, but depending on the task, input weight heterogeneity can significantly improve performance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "[18] reported, from private communication with Jaeger, that different reservoir structures, such as the scale-free and the small-world topologies, do not have any significant effect on ESN performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Song and Feng [31] demonstrated that in ESNs, with complex network reservoirs, high average path length and low clustering coefficient improved the performance.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "This finding is at odds with what has been observed in complex cortical circuits [32] and other studies of ESN [33].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "This finding is at odds with what has been observed in complex cortical circuits [32] and other studies of ESN [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "Rodan and Tino [24] studied an ESN model with a very simple reservoir consisting of nodes that are interconnected in a cycle with homogeneous input weights and homogeneous reservoir weights, and showed that its performance can be made arbitrarily close to that of the classical ESN.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "Massar and Massar [34] formulated a meanfield approximation to the ESN reservoir and demonstrated that the optimal standard deviation of a normally distributed weight matrix \u03c3w is an inverse power-law of the reservoir size N with exponent \u22120.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Note that this architecture is different from the delay line used in [24] in that the input is only connected to a single unit in the delay line.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "The network is trained using the Marquardt algorithm [35].", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 94, "endOffset": 98}, {"referenceID": 35, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "It is noteworthy that this power-law behavior is qualitatively consistent with what we expect from the theoretical result in [34], although the exact power-law coefficient is task-dependent.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "17 reported by Atiya and Parlos [23] for the same network size N = 40.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Our ESN testing results are similar to those reported by Rodan and Tino [24] for the same system size.", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "One solution would be to extend the receiver operation characteristic (ROC) and receiver error characteristic (REC) curve methods to decide on the quality of generalization in ESN [38]\u2013[40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 38, "context": "One solution would be to extend the receiver operation characteristic (ROC) and receiver error characteristic (REC) curve methods to decide on the quality of generalization in ESN [38]\u2013[40].", "startOffset": 185, "endOffset": 189}, {"referenceID": 39, "context": "In the neural network community, methods based on pruning, regularization, cross-validation, and information criterion have been used to alleviate the overfitting problem [41]\u2013 [53].", "startOffset": 171, "endOffset": 175}, {"referenceID": 51, "context": "In the neural network community, methods based on pruning, regularization, cross-validation, and information criterion have been used to alleviate the overfitting problem [41]\u2013 [53].", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "Among these methods, regularization has been successfully used in ESNs [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 52, "context": "Another area that requires more research is the amount of training that the ESN requires to guarantee a certain performance, as is described in probably approximately correct methods [54]\u2013[56].", "startOffset": 183, "endOffset": 187}, {"referenceID": 54, "context": "Another area that requires more research is the amount of training that the ESN requires to guarantee a certain performance, as is described in probably approximately correct methods [54]\u2013[56].", "startOffset": 188, "endOffset": 192}], "year": 2014, "abstractText": "Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks. In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. A readout layer is then trained to reconstruct a target output from the reservoir\u2019s state. The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC. In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively. We compare the performance of the three systems while computing three common benchmark time series: H\u00e9non Map, NARMA10, and NARMA20. We find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs. The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN.", "creator": "LaTeX with hyperref package"}}}