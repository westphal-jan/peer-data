{"id": "1401.2258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "abstract": "This work compares concept models for cross-language retrieval: First, we adapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents. Experiments with different weighting schemes show that a weighting method favoring documents of similar length in both language sides gives best results. Considering that both monolingual and multilingual Latent Dirichlet Allocation (LDA) behave alike when applied for such documents, we use a training corpus built on Wikipedia where all documents are length-normalized and obtain improvements over previously reported scores for LDA. Another focus of our work is on model combination. For this end we include Explicit Semantic Analysis (ESA) in the experiments. We observe that ESA is not competitive with LDA in a query based retrieval task on CLEF 2000 data. The combination of machine translation with concept models increased performance by 21.1% map in comparison to machine translation alone. Machine translation relies on parallel corpora, which may not be available for many language pairs. We further explore how much cross-lingual information can be carried over by a specific information source in Wikipedia, namely linked text. The best results are obtained using a language modeling approach, entirely without information from parallel corpora. The need for smoothing raises interesting questions on soundness and efficiency. Link models capture only a certain kind of information and suggest weighting schemes to emphasize particular words. For a combined model, another interesting question is therefore how to integrate different weighting schemes. Using a very simple combination scheme, we obtain results that compare favorably to previously reported results on the CLEF 2000 dataset.", "histories": [["v1", "Fri, 10 Jan 2014 08:50:54 GMT  (736kb)", "http://arxiv.org/abs/1401.2258v1", "74 pages; MSc thesis at Saarland University"]], "COMMENTS": "74 pages; MSc thesis at Saarland University", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["benjamin roth"], "accepted": false, "id": "1401.2258"}, "pdf": {"name": "1401.2258.pdf", "metadata": {"source": "CRF", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "authors": ["Dietrich Klakow", "Benjamin Roth", "Saeedeh Momtazi", "Grzegorz Chrupala", "Michael Wiegand"], "emails": ["beroth@CoLi.Uni-SB.de"], "sections": [{"heading": null, "text": "Saarland University 4.7 General Linguistics Computational Linguistics"}, {"heading": "Assessing Wikipedia-Based", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Cross-Language Retrieval Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Submitted for the degree of M.Sc.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Supervisor: Dietrich Klakow", "text": "Benjamin Rothberoth @ CoLi.Uni-SB.deSaarbru \"cken, November 14, 2009Affidavit Nov 14, 2009Benjamin Rothberoth I hereby declare that I compose this work independently and do not use any other means and sources than those given here.Saarbru\" cken, November 14, 2009Benjamin RothDanksAcknowledgement First of all, my thanks go to my helpful supervisor Prof. Dietrich Klakow, on whose helpfulness and experience I have always been able to rely, and who has always been able to create an open atmosphere and to give concrete form to ideas brought up for discussion through precise and critical questions. It was from this atmosphere that the entire Question Answer Group was supported, from whose helpfulness and experience I have gained an abundant benefit. I am very impressed by their members, Saeedeh Momtazi, Grzegorz Chrupala, Michael Wiegand and and Fang Xu, for their participation in my work and the exchange of ideas over the past months."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related Work and Ressources 4", "text": "2.1. Return methods..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Models and Theory 12", "text": ". Probabilistic.. Semantic......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Experiments 33", "text": "4.1 Preliminary experiments........................................................................................................................."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "4.2 Mate Retrieval on Multext JOC...................................... 42 4.3 Query-based Retrieval with CLEF2000................. 464.3.1 Experiment Setup and Results................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Towards Retrieval Based on Only Wikipedia 51", "text": "5.1 Model........................................................................................................................."}, {"heading": "6 Conclusion 61", "text": "References 632"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "The task of interlingual information retrieval is to find documents relevant to a query in which queries and documents do not belong to the same language. While many systems translate either queries or documents and then perform monolingual retrievals, better or less resource-intensive approaches may incorporate translation knowledge as an integral part of the retrieval model. Our focus will be on integrating knowledge from the multilingual, freely available online lexicon Wikipedia1 as a cross-lingual bridge to retrieval. Dimensionality reduction techniques have traditionally been of interest in retrieving information as a means of mitigating the word discrepancy problem. Translingual retrieval of information can be considered an extreme case of word mismatch. More generally, as a dimensionality reduction, the concept model is used to transform a word from space into another representation."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Chapter 6 summarizes our findings and points to promising directions for further research."}, {"heading": "2 Related Work and Ressources", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Retrieval Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Vector Space Retrieval", "text": "The classical methods of IR follow the vector space model: documents and queries are vectors, while similarity is measured by a distance function, usually by taking the cosine between its angles. The simplest models of this kind work directly with word vectors, tending to be minor and lacking word overlaps. In practice, many features associated with documents can be achieved by dimension reduction techniques such as latent semantic analysis [Deerwester et al., 1990] or term expansion as in the general vector space model [Wong et al., 1985] While vector space query models are defined in a simple and precise mathematical theory that makes them directly accessible for many other tasks such as document clusters or classification, the connection with the actual query process is not very clear, especially after applying term weighting functions such as the empirically well-functioning model."}, {"heading": "2.1.2 Language Modeling Retrieval", "text": "Language model-based query is a newer paradigm [Ponte and Croft, 1998] where documents are usually treated as probabilistic models for generating queries, and the document that provides the best model is considered the most relevant. Language model query systems are among the most popular today because they model the query process in a statistically accurate and consistent manner, and perform at levels comparable or even better than fine4 in even the simplest form."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The probability models derived from document D are, in the simplest case, the maximum probability estimates of the unigram word probabilities P (w | D). These estimates are zero for invisible words, which means that the probability of query P (Q) = \"w\" QP (w | D) n (w, Q) n (w, Q) n (Q) is zero if a single query word has no evidence in the documentation. To overcome this problem, two different strategies are usually used: the first is to interpolate the document estimates with a naturally more even distribution, estimated from a large collection of documents [Hiemstra, 1998, Miller et al., 1999, Song and Croft, 1999]. The second is to smooth out the estimates of both distributions using one of the many originally developed speech recognition techniques [Chen and Goodman, 1999 and Zaffty, 2004]."}, {"heading": "2.1.3 Latent Variable Models", "text": "Latent variable modeling lies somewhere between vector space and language modeling. Since such models perform a kind of dimensional reduction (for each document or query they store only the distribution of themes), parallels can be drawn with LSA. On the other hand, they are probabilistic in nature, so that even parts can be identified that directly correspond to generative language models. Indeed, both the use of theme distributions (or other parameter vectors) directly for distance comparison (as in [Hofmann, 1999, 2001, Blei et al., 2003]) and the use as components in a generative model (as in [Azzopardi et al., 2004, Wei et al., 2004, Croft, 2006] are common practices. The two most prominent latent variable models are probabilistic latent semantic analyses [Azzopardi et al., 2004, Wei et Croft, 2006]."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "and is also advantageous in terms of scalability [Wang et al., 2009]. Latin variable models used for cross-language search play a central role in this thesis. Probabilistic latent semantic analysis and latent Dirichlet allocation are described in more detail in chapters 3.1 and 3.2 respectively."}, {"heading": "2.2 Using Cross-language Knowledge Sources", "text": "Sources of knowledge to establish a link between two or more languages can be edited manually: encyclopedias, parallel corporas, probabilistic encyclopedias extracted from them, and comparable corporas. Parallel corporas, such as the Europarl corpus of EU parliamentary proceedings [Koehn, 2005], provide translations of the same text into different languages, corresponding pairs of phrases can be automatically extracted [Koehn et al., 2003]. Somewhat is different are comparable corporas Wikipedia. They contain texts that have equivalents in other languages to which they are linked by dealing with the same subject. In this context, one generally cannot expect to find corresponding pairs of phrases, and it is an interesting research question to what extent this type of information can contribute to effective restoration. In addition, comparable corporas are much easier to acquire, cheaper, and available in more languages than parallel corporas."}, {"heading": "2.2.1 Comparable Corpora", "text": "Comparable corpora represent a promising source of knowledge for information retrieval. Since comparable corpora only provide links at the text level, the information they provide should be appropriately captured by dimensionality reduction techniques aimed at modelling and smoothing document term statistics. Indeed, dimensionality reduction for cross-linguistic data retrieval has been addressed in several theoretical frameworks and is still the subject of ongoing research. We will now present work relevant to the use of comparable corpora, and outline the strengths and weaknesses of these, as well as possible extensions and modifications. The first approach [Dumais et al., 1997] to using comparable corpora and dimensionality reduction has been performed using Latent Semantic Analysis (LSA), a technique that uses the approximation of subordinate matrix by singular value substitution. As an effect, terms and documents are represented by low-dimensional vectors, making them comparable [Detopically] in 1990."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "While the method works well and is still used as a starting point [Cimiano et al., 2009], it, like the monolingual LSA, lacks a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding corpora in gigabyte-size like Wikipedia. A more modern approach could be based on probabilistic latent semantic analysis (pLSA Hofmann [2001]). The underlying assumption in this type of model is that words and documents are conditioned independently of an unobserved class variable Z, which can take different values on K."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "These results do not convincingly demonstrate the inadequacy of LDA, as the authors have embedded multilingual Wikipedia articles without any length normalization or model adjustment. An indicator that this effect may actually be at work is that the greatest increase in data probability is to be expected through effective capture of the dominant language vocabulary."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "was developed on the basis of plausibility considerations and adapted to experimental conditions that could render optimal parameterization unstable in various experiments. Some characteristics make ESA an interesting method that we should include in our experiments: conceptual simplicity, scalability, a novel approach and trained by default on a comparable corpus. In chapter 3.3 we discuss the monolingual and multilingual ESA."}, {"heading": "2.2.2 Parallel Corpora", "text": "[Bader and Chew, 2008] uses the fact that the SVD of a matrix X can be calculated by the eigenvalue decomposition of the composite matrix B = \ufffd 0 X XT 0 \ufffd. Authors include weighted word-word alignments in the upper left block of the matrix (in the case of a parallel document, the document vector may contain entries that are nonzero in term dimensions of more than one long eye), evaluate their system based on the performance of retrieving translations of Bible verses, and show a modest but statistically significant improvement over the mere execution of LSA (without term alignment). A coherent conceptual interpretation of the entire embedded matrix P is difficult to give, the weighting scheme of Bible verses is found and shows a statistically significant improvement over the mere execution of LSA (without term alignment)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Similarly, links between word and translation could be interpolated with the counting of word and document. However, we see two problems in such an approach: firstly, the interpretability of the model parts for the different \"ontological\" status that a word has as a translation from a lexicon or occurs in a document would not be completely uniform; secondly, it is not obvious to us that the EM revaluation equations in Cohn and Hofmann [2001], while at first glance reasonable, actually stem from the original problem; an interesting alternative based on LDA was recently proposed by Boyd-Graber and Lead [2009], without making additional, non-trivial assumptions, should the complexity rise from square to cubical by adding an additional dimension of the information contained in this way."}, {"heading": "2.3 Evaluation", "text": "Ideally, baseline systems for comparison in our experiments should be conceptually simple, widely used, and trained on the same data; evaluation tasks should be designed to be accepted by the research community and provide reproducible results that allow direct comparison; general competitions such as the Cross-Language Evaluation Forum (CLEF, [Peters and Braschler, 2001]) and data sets that allow more specific comparisons should be considered; the second scenario consists of classical evaluation scenarios with queries in one language and relevant documents in another; the advantage of a mate-retrieval approach to evaluation is that many such companies are available for evaluation, starting with the Bible (as in [Chew and Abthali, Chethali, 2008 and Chethali, 2008)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "In fact, it is an unrealistic setting in which real word information scenarios and recommendations appear (see also [Ni et al., 2009, Wu and Oard, 2008, Olsson et al., 2005]). Also, to see that there are a lot of reviews being done on such corpora, this does not mean that the results are comparable. In fact, it is an unrealistic setting to appear in real word information scenarios and to do a perhaps too simple task. Also, to see that there are a lot of reviews being done on such corpora does not mean that the results are comparable."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Models and Theory", "text": "The models described in this chapter are all capable of presenting documents in vector representations that are not direct word-sack-of-words vectors of vocabulary terms, and may differ drastically from them in terms of dimensionality or other qualities. As these representations can be directly comparable for more than one language, such models are of interest to cross-language queries, especially when training them requires less stringent characteristics of the training material than are required for machine translation. For all models presented in this section, comparable corpora are sufficient, while all common statistical machine translation systems have a parallel training corporative.In monolingual queries, such models are usually justified by the existence of two problems: the first is synonymous, the case that two words have the same meaning while having different surface shapes, possibly leading to an underestimation of the similarity of two texts."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Probabilistic Latent Semantic Analysis", "text": "Probabilistic Latent Semantic Analysis (pLSA) [Hofmann, 1999, 2001] is a latent variable model that begins with a simple model statement and is trained by maximizing the probability of parameters for a training corpus.The most important assumption in pLSA is that the probability distribution of words in a document depends only on the distribution of the themes in this document.This assumption is the same for newer models. We want to discuss pLSA for several reasons: it is the first latent variable model to be widely applied to document clustering and retrieval, and its basic architecture has been influential for its successors, it is easy to understand and deduce, it is easy to adapt and implement, and we will address the shortcomings of the pLSA in sections 3.1.2 and 3.2."}, {"heading": "3.1.1 Probabilistic Model", "text": "The pLSAmodel starts with a collection of N documents via a vocabulary of M words. The document collection is represented by the coexistence number n (di, wj), which indicates how often document di contains the word wj. In view of such a document collection, the goal of pLSA is to find a probability distribution P (D, W) (of document and word random variables, which can take values di and wj, respectively), which maximizes the probability of n (d (\u00b7), w (\u00b7). While P (D, W) would thus prove to be the relative frequencies, it is additionally assumed that wi and dj are independent of the value of a topic variable Z, which can take different K values, which follows from this that the common probability can be factored, using the dependency assumption in the last step: P (di, wj) = P (wj) P (wj | etsch) = 1k (if applicable)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Asymmetrical factorization can also be explained as a generative process of the three steps: 1. Choose a document di2. select a latent class zk | di3. generate a word wi | zkMan could call this process not completely generative, because a document is selected from an already existing set and not generated by a random process. The model is determined by KN + KM parameters, which (assuming k \u00b2 M, N) are substantially lower than MN as in the case without the conditional assumption of independence. The variable Z acts as a bottleneck in determining the interdependence between documents and words. From the equivalent symmetrical factorization given in Eq.4, some parallels arise to the latent semantic analysis [Deerwester et al et al, 1990], based on truncated singular value substitution, can be drawn: The values P (di | \u00b7) and P (wj | \u00b7) matrix play a role similar to that of K's latent sectoral K = K (K = dectorals = 4), K (K = dectorals K = [K =)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "When the model is specified, the next step is to find a suitable parameterization, i.e. values for all p (di), P (wj | zk) and P (zk | di). Often, asymmetric factorization is used, because it characterizes the documents by their topic distribution. Parameters are estimated according to the aforementioned maximum probability principle, so that those parameters are searched for that produce the highest probability of the observed data. The probability function will be: L = N, M \ufffd i = 1, j = 1P (di, wj) n (di, wj) n (di, wj) n (di, wj) n (di) n (di, wj) n (di) results in a simple calculation, whereby the optimization is performed on the log of this function, which givesL \u0445 = N, M \ufffd i = 1, j = 1n (di, wj) log p (di) = N, wj) log (v \ufffd k)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The EM algorithm [Dempster et al., 1977, Bilmes, 1998] aims to find a maximum probability solution for problems concerning observable and unobservable (hidden) variables. X values denote the observable data and Z denotes a possible mapping of values to the hidden variables (we only discuss discrete variables). A parameterization of a common probability distribution across all variables is denoted by area names. P (X, Z | B) denotes the complete data probability, a function of the hidden variables for a fixed mapping to both observed and unobserved variables. The simple goal of optimizing the observed data is achieved by marginalizing all possible mappings to the hidden variables."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Using the assumption of independence and Bayes' formula: P (zk | di, wj) = P (wj | zk) P (zk | di) \ufffd K k = 1 P (wj | zk) P (zk | di, wj) The optimal parameters are found by taking the derivative of the complete data probability function E [Lc] with respect to each parameter and setting it to zero. Since the maximizing parameters must form probability distributions, appropriate Lagrange multipliers must be added for normalization and the resulting reestimation equations are: p (wj | zk) = \ufffd N i = 1 n (di, wj) P (zk | di, wj) \ufffd N (zk | di) \ufffd M i = 1, m = 1 n (di, wj) P (zk | di) = 1 n (di, wj) wj, wj (P)"}, {"heading": "3.1.2 Theoretical Considerations", "text": "Let's say a parameterization that includes all the values found for p (di), P (wj | zk) and P (zk | di), and the data D (which are the statistics for the documents in our training corpus). For the maximizing probability estimate, we are looking for a maximizing P (D | \u043a) estimate. This estimate is called the maximum a posteriori estimate with an uninformative (uniform) previous p (\u043a) estimate in Bayesian statistics: the use of this estimate has several disadvantages. The main criticism from Bayesian point of view is that a certain choice of p values could maximize the distribution over the parameters p, but does not take into account our uncertainty regarding this choice2, and other parameters that might be similarly reasonable but happen to have received a little less evidence from the data. Therefore, the more practical argument is that the maximum probability for calculating D (uneven) should contain (uneven) probability over D."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "We want to compare the maximum probability (formula 7) with the Bayesian (formula 9) approach, assigning a probability estimate to some new data elements Dnew, given training data Dold. In both cases, it is assumed that the probability of the data depends only on parameterization. p (Dnew | Dold) = P (Dnew | argmax; p (Dold))) (7) = P (Dnew | \u044b) (8) p (Dnew | Dold; \u03bb) = \"P (Dnew | Dold; p (Dold)) (7) = P (Dold; p) (Dold;) (10) Many problems, of course, disappear when a Bayean approach is taken, for example the problem that pLSA p (d) distributes its probability mass only to documents suitable for solving these problems."}, {"heading": "3.1.3 Practical Issues", "text": "In the EM algorithm, the following equations must be evaluated repeatedly: p (zk | di, wj) = p (wj | zk) p (zk | di) \ufffd K = 1 p (wj | zk) p (wj | zk) p (wj | zk) = \ufffd N i = 1 n (di, wj) p (zk | di, wj) \ufffd N, M i = 1 n (di, wm) p (zk | di, wm) p (zk | di) = \ufffd M j = 1 n (di, wj) p (zk | di, wj) n (di) In a naive implementation, this would result in space and time requirements for O (NMK) on the basis of the first of the three formulas. However, since p (zk | di, wj) is only multiplied by the corresponding number n (di, wj), only values of 18 need to be evaluated."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The complexity is therefore only O (fK), with f = \ufffd N, Mi = 1, m = 1 1n (di, wm) > 0. We have adjusted the vector multiplication of the mattresses so that it evaluates the result only at certain positions of the result matrix. It has been reported that the pLSA model hangs in the local optima and is missed. To alleviate these problems, two strategies have been applied: the first is tempering, an annealing method that attempts to increase the entropy of the posterior distribution p (Z, D, W) by applying it to the strength of a decreasing 0 < \u03b2 \u2264 1 in the reappraisal equations (details can be found in [Hofmann et al., 1999, Ueda and Nakano, 1998]). Another commonly used technique is the interpolation of the parameters of different runs."}, {"heading": "3.1.4 Multilingual pLSA", "text": "In this section, we present our own PLSA model, which is designed to capture the current composition of multilingual documents. We want to adopt a document that consists of parts in different languages that do not have to be of the same length. We can see for each word which part of the language it belongs to. In order to model multilingualism, we introduce an additional variable L, which can assume different values for S. We now have two bottleneck variables, Z and L, one of which is observable and the other not. In this case, we transfer the assumptions of independence made for Z with two such variables: The language and the subject that produce a word are conditional on the document. Words and documents are independent of language and subject. The generative process then is: 1. Select a document di19"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The common probability of a word wj occurring in a document di can be written as follows: p (di, wj) = p (di) p (wj | di) = p (di) S \ufffd s = 1K \ufffd k = 1p (wj | zk, ls, di) p (zk, ls | di). Using the assumption of independence and the fact that we can observe the language of a word, this means p (wj | zk, ls) = 0 for all languages ls except the language s (wj) of the word considered: p (di, wj) = p (wj), s (wj) k = 1p (wj), s (wj) k = 1p (wj | zk), s (wj | di) s (zames' law and using knowledge (wj, wj, wj), k = 1p (wj), k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "H = N, M \ufffd i = 1, j = 1n (di, wj) K \ufffd k = 1p (zk | di, wj) log p (wj | zk, s (wj)) + N, M \ufffd i = 1, j = 1n (di, wj) K \ufffd k = 1p (zk | di, wj) log p (zk | di) + K, S \ufffd k = 1, s (1 \u2212 {wj | s (wj | s) p (wj | zk) p (wj) p (wj) p (wj | zk, s (wj) p (wj | s) p (wj | s) p (wj | s) p (wj | s) p (wj | s) p (wj | s) p (wj | s) (wj | s) p (wj | s) p (wj | s) p (wj) p (wj | zk) p, s (wj) p (wj) p (wj | zk) p (wj) p (wj | s) p (wj | s) p (wj | s) p (wj | s) p (wj | s) p (wj | p (wj | s) p (wj | s) p (wj) p (wj) p (wj) p (wj) p (wj | wj | wj) p (wj | wj) p (wj) p (wj | wj) p (wj | wj) p (wj) p (wj) p (wj) p (wj | zk, k, k, k wk, k, k wk, k, k wk, k, k wdi, k (k, k, k k, k, k, k k, k k k k, k, k k, k, k wj), k wj (wj, k wj), k wk wj, k wk, k, k wk, k, k wk wk, k, k, k (wj), k wk (wk, k, k, k, k"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Latent Dirichlet Allocation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Probabilistic Model", "text": "The latent Dirichlet allocation (LDA) [Lead et al., 2003, Griffiths and Steyvers, 2004, Steyvers and Griffiths, 2007] is a latent variable model that provides a completely generative representation of documents in a training corpus and for invisible documents. Each document is characterized by a topic distribution, words are output according to an emission probability depending on a topic. The main difference to pLSA is that both topic distributions and word emission distributions are generated by Dirichlet priors. A Dirichlet distribution with the T-parameters Dir (\u03b11 \u00b7 \u03b1T) (Dir (\u03b1) for short) is the mulative probability of distribution for a T-dimensional multinomial distribution sign Mult (p1 \u00b7 pT) that indicates the density:"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Generative model in plate notation."}, {"heading": "3.2.2 Sampling for LDA", "text": "The first approach to estimating such a model [Lead et al., 2003] was to explicitly present and estimate economics and economics, leading to different conclusions to solve and combine. Later approaches focus on obtaining a sample of the mapping of words to topics instead [Griffiths and Steyvers, 2004]. We now give a brief overview of the conceptual steps associated with the sampling process. For a more detailed description of the theory behind the sampling process, refer to generic textbooks such as [Newman and Barkema, 1999, MacKay, 2003, Bishop, 2006].Gibb's sampling is a simple technique based on the fact that when the result of a variable is sampled based on the previously sampled results of the other variables, the total distribution, when iteratively sampled, matches the underlying distribution.For LDA, this sample is particularly simple because the distributions involved are multinomial, due to the diversity."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "One could consider the initial parameters \u03b1 as pseudo-counts, which are added up before any actual data has been determined. Thus, the expectation of a multinomial distribution p \u0445 Dir (\u03b1) isE [pi] = \u03b1i \ufffd j \u03b1jTherefore, the estimates of the word emission probabilities \u0443 (zk) = p (wj | zk) and the subject probability \u03b8 (d) = p (zk | d), as observations covering samples for all positions, are the following:. (w) j = n (w) j + \u03b2n (\u00b7) j + \u00df.p (d) j = n (d) j + \u03b1n (d) \u00b7 + \u03b1n (d) \u00b7 + T\u03b1The sample equation can easily be achieved by excluding the current position from the observations: P (zi = j | z \u2212 the same text, w) = P (w \u2212 doc) = \u2212 doc (wi), j \u00b7 j \u2212 i (the position \u2212 j), which was assigned \u2212 j \u2212 \u2212 \u2212."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The sample itself does not distinguish between training and inference documents. In practice, the number of topics for the training set is determined after it is assumed that the sample has reached a stable state. Sample iterations for inferencing only change the topic assignments for individual invisible documents."}, {"heading": "3.2.3 Practical Issues", "text": "In order to determine the similarity between two documents, one can compare either their sampled topic vectors or the probability vectors derived from them [Griffiths and Steyvers, 2004]. If other variational EM estimation techniques are used, other parameter vectors can also be available and used [Cimiano et al., 2009, Lead et al., 2003]. Comparison between these vectors can be made either by the cosmic similarity of their angles or, if the vector is actually a probability distribution (as for \u03b8), by using probability divergence measures, such as the (symmetrized) Kullback Leibler divergence or the Jensen-Shannon divergence. For language model-based information that is retrieved, one is interested in the probability of a query, since a document p (q | di) is specified. Wei and Croft [2006] interpolate a language model based on LDA with an unigramic model that is found directly on the document, the LDA-word is estimated in this document."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.4 Multilingual LDA", "text": "LDA has been extended for several languages [Ni et al., 2009], see also [Mimno et al., 2009] for an investigation of the semantic clustering of this model. Most components remain the same, the main difference being that for each language a different word emission distribution is assumed. Depending on the language of a position in a document, a word is generated by the corresponding distribution. The sample equation is, with similar notation as before: P (zi = j | z \u2212 i, w). - i, j + \u03b2lin (li) \u2212 i, j + Wli\u00dflin (di) \u2212 i, j + \u03b1n (di) \u2212 i, T\u03b1Figure 3 shows the model in plate notation. This model allows to deal with multilingual topics in an elegant manner. The model is truly multilingual, as it does not use the subject variably to estimate the language, as it could be the case for a monolingual model."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "We assume that the words in all languages can be clearly identified as belonging to a particular language (which is easy to achieve in practice by adding appropriate prefixes), so that n (wi, li) = n (wi) counts. For the sake of simplicity, we also assume the same vocabulary size in all languages (without smoothing this assumption depending on the language). We start with the multilingual model: P (zi = j | z \u2212 i, w), n (wi) \u2212 i, j + \u03b2i, j + 1 L W\u03b2n (di) \u2212 i, j + \u03b2n (di) \u2212 i, \u00b7 + T\u03b1For this multilingual model, as for the monolingual model, rehearsals are carried out, because the constant factor L can be ignored."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3 Explicit Semantic Analysis", "text": "Explicit Semantic Analysis (ESA) [Gabrilovich and Markovitch, 2007, Potthast et al., 2008, Sorg and Cimiano, 2008, 2009] is another scheme for overcoming the word mismatch problem. In ESA, the associative strength of words to the documents in a training collection is calculated, and vectors of these associations are used to represent the words in the semantic space spanned by the document names. These word representations can be used to compare words, they can also be combined to compare texts."}, {"heading": "3.3.1 Formalization", "text": "The basic components that determine an implementation are: \u2022 Word vectors: For each word w, a vector w is calculated that indicates its associative strength to the documents. Formally: w = \ufffd as (w, a1), \u00b7 \u00b7, as (w, aN) \ufffd Where as (w, an) indicates the associative strength from w to training document a (mnemonic for articles, since the most commonly used articles are Wikipedia documents) of N training documents as a whole. \u2022 Text vectors: For a new document (or query) d, a representation d is calculated from the word vectors: d = f (w | w, d} b) The subscript b should indicate that one generally wants to consider the multiset (\"bag\") of word vectors, taking into account the number of words in d. \u2022 similarity function: Giving two documents d1 and d2 the similarity."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Note that in this formulation the relative frequency of the term is used. In fact, it is a length normalization, whereby all documents contribute equally strongly to it. We use this choice of word vectors in our experiments. Several settings have been suggested for the text similarity. In [Gabrilovich and Markovitch, 2007] a weighting of the word vectors is used, they are multiplied by scalars, which in turn correspond to a tf.idf weighting of the terms, and then summarized. However, it is not very clearly described which exact instantiation of the tf.idf function was used in the experiments. Sorg and Cimiano [2009] investigate further combination schemes, which correspond to a tf.idf weighting of the terms and then summarized."}, {"heading": "3.3.2 Multilingual ESA", "text": "Applying this model in a multilingual environment is straightforward: For L languages, consider document term matrices A (1) \u00b7 \u00b7 \u00b7 A (L). Construct the matrices so that the document lines match. For all languages, each of the lines A (\u00b7) n contains documents on the same subject in all languages. Therefore, only documents that are available in all languages considered can be included in it.30"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Since the relative frequency is used in the tf.idf weighting, all documents are normalized and there are no biases in documents that occur longer in one language than in another. Different comparable text collections can be used. The easiest available and most powerful is Wikipedia, originally the ESA was also tested on the data of the Open Dictionary Project (ODP) 3, with significantly worse results. Sorg and Cimiano [2008] note that even if the ESA model is used to compare documents from two languages from the defined French, German and English, it slightly improves the query performance to limit the training corpus to documents that are available in all three languages. This finding is presented as a side note and not backed up with further evaluation results. As our work aims to find practices for transnational evaluation, which we best apply in most corporate languages, we need for most of the corporate ones."}, {"heading": "3.3.3 Implementation", "text": "Wikipedia dumps can be larger than the amount of memory common computers have, for example the English Wikipedia dump is 27G xml."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "In the only open academic implementation known to us4, this inversion is done using an index and a database. In our implementation, we use an iterative block-by-block inversion that adapts to different memory capacities and can be used without installing external libraries. Since our implementation also focuses on efficient xml processing, we observe an acceleration of more than factor 10 in processing the English Wikipedia dump. 4 research-esa, http: / / code.google.com / p / research-esa / 32"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Experiments", "text": "As Table 1 shows, three corpora were used for our experiments: a Wikipedia subset compiled by us, the multext JOC corpus and the German-English dataset of the ad hoc track CLEF 2000. A detailed discussion of the characteristics is given in the respective chapters."}, {"heading": "4.1 Preliminary Experiments", "text": "All three models, ESA, LDA and our adaptation to pLSA, will be compared. As for the latent variable models, it is of particular interest how data weighting and normalization affect retrieval performance, and we will make some decisions and consider a smaller set of hypotheses in subsequent large-scale experiments, starting from hypotheses and questions arising from the theoretical models we have introduced and from experiments reported elsewhere. We will summarize them as follows: 1. How does the multilingual pLSA model work? 2. How important are \"adhesive documents\" in this model (i.e., truly multilingual documents)? 4. What weighting schemes favor important documents? 3. How does a monolingual LDA model work on the basis of multilingual data \u2022 in the form reported in [Cimiano et al., 2009]? 4. How does a monolingual LDA model compare to E5?"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Weight scheme experiments are performed with the pLSA model because it can handle real values that can be scaled continuously. Integer values are required for the LDA model due to the sampling process. In any case, weighting schemes are often ad hoc. Therefore, we want to use them only as indicators of important properties and as a justification for limiting our data in a meaningful way."}, {"heading": "4.1.1 Experiment Setup", "text": "Our experiments were conducted using a bilingual data set. We performed a partner search task, that is, we found a document in a foreign language that matched a document in the original language. In a collection of English Wikipedia articles, for example, we found the document dealing with \"French fries,\" given the German article corresponding to \"French fries.\" Our data set was constructed from a German and an English Wikipedia dump5. We randomly selected 2,000 documents belonging to the German Wikipedia category \"Philosophy,\" for each of which there is an English Wikipedia article referencing (1) the German article and (2) the very German article. We ignored certain Wikipedia pages that we do not expect to contain encyclopaedic knowledge, such as diversions or clarification pages. Tokenization occurred by looking at all characters that are not in a Unicode letter block as delimiters, and all of them were found to be just a snowball or a token."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The data is organized in a matrix of three blocks (see also Figure 5): A comparable block and two monolingual blocks. The comparable block consists of 1000 multilingual documents, each line representing the term statistics of a German and an English Wikipedia article. The evaluation framework is as follows: For each document d (de, i) in the German monolingual block, we classify the document according to the English monolingual block. Likewise, the English monolingual block contains 1000 articles, each line representing its counterpart in the German monolingual block."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Such scaling is theoretically permissible since it does not affect the meaning of the other concepts and can give clues as to which data is the most useful cross-language bridge. We used the parameterization described in 3.3 for the ESA model and used the comparable block as the training corpus. Note that the cut-off parameter is larger than the size of the training corpus and therefore has no influence. Again, the ranking is evaluated by the cosmic similarity. For the LDA model, we estimated the parameters P (W | Z) with the comparable block. The highly efficient and parallel LDA implementation plate [Wang et al., 2009] was used. We used the dirichlet parameter values proposed by Griffiths and Steyvers [2004], \u03b1 = 50 (number of topics) and \u03b2 = 0.1. The model was trained by performing 100 sampling iterations in order to convert the distributions of samples, the subsequent conlection of the results for the sampling and the 50 for the subsequent sampling."}, {"heading": "4.1.2 Results", "text": "In the first runs, however, we tested Benjamin Rothent's multilingual pLSA models in \"rooms.\" However, the adhesive documents are the only cross-language scheme in itself and therefore of particular importance. Instead of varying the number of adhesive documents (which is 1000), we gave them a higher weight. Both individual runs and combined runs with concatenated parameter vectors of different dimensions were evaluated. The first applied weighting of the terms counts in the comparable block by a constant weight (we tried with values of 2,4,8 and 16). As Figure 6 and Table 2 show, this improved retrieval performance is drastically compared to the original model. The higher weights are chosen, which are more influenced by the comparable documents, the monolingual doc-36Wikipedia-based retrieval models Benjamin Rothent's."}, {"heading": "4.1.3 Conclusion", "text": "The first experiments were carried out with a rather small collection of training and targets. However, the number of query documents is high and therefore some interesting observations can be made. Obviously, it is important to store model capacities for cross-language bridging. We can assume that performance improves \u2022 if the model does not attempt to structure the monolingual spaces."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "This is the downweighting experiment for unbalanced documents in the PLSA model. \u2022 When the model does not attempt to guess the language, the upscaling experiment of the shorter language page indicates this. Furthermore, performance increases dramatically when different runs of latent variable models are combined and latent variable models are interpolated with ESA.41."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Mate Retrieval on Multext JOC", "text": "In recent years, it has become clear that it is not only a question of whether and in what form, but also of whether and in what form and in what form and in what form and in what form people are able to change and change the world. In recent years, it has become clear that people are able to change and change the world. In the last few years, it has become clear that people are able to change and change the world. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed. In the second half of the 20th century, the world has changed."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "A further improvement can be observed by combining the results of different models, a technique normally used for pLSA Hofmann [2001]. In this case, the cosinal values of runs with other dimensional models were simply averaged (corresponding to the concatenation of the L2-normalized sampling statistical vectors), resulting in a score of mrr = 0.68 for the truncated model, which shows the performance in the same order of magnitude as for ESA. Figure 11 and Table 4 give an overview of the results obtained with LDA. Values significantly better than in the corresponding row above p 0.005 in the paired t test are marked with the B value. (Of course, we could not test against values reported elsewhere, for lack of the original numerical data.) How different are the ESA and LDA models, to what extent can they contribute to any other solution? In this order, the DA values were combined by \u2264 1 and \u2264 4 multidimensional values respectively."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Average precision measurements for each document relevant to a query q ranked the precision of ri, q up to its rank. The APi determined per document, q = relevant documents j with 1 \u2264 rj, q \u2264 ri, qri, qis combines these values to a value per query by averaging them over all values of the set of relevant documents Relq: APq = 1 | Relq | \ufffd i \u01c0 RelqAPi, qMean Average Precision combines these values for the set Q of all queries based on their arithmetic mean: MAP = 1 | Q | \ufffd q, QAPqGeometric Mean Average Precision instead uses the geometric mean: GMAP = | Q | \ufffd q, QAPq"}, {"heading": "4.3 Query-Based Retrieval with CLEF2000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Experiment Setup and Results", "text": "Mate retrieval experiments can be criticized as an unrealistic retrieval scenario.We evaluated both models http: / / www.DA-improvement.Therefore, a second evaluation was conducted on the CLEF12 German-English ad hoc trail of 2000 < The target corpus consists of approximately 110,000 English newspaper articles along with 33 German queries for which relevant articles could be bundled.For our experiments, the title and description fields of the queries were used and the narrative ignored.A common strategy for retrieving foreign languages is first to translate the query and then to perform a monolingual retrieval. While the translation process for the multext corpus would have taken prohibitively long, we performed the query translation on the CLEF2000 map, training Moses \"standard translation model on Europarl. Retrieval was performed by using the cosintator models weighted on this page (we weighted the wordf.11)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.2 Error Analysis", "text": "Query-oriented error analysis is difficult because the inner workings of quantitative methods are often opaque to human analysis. On page 50, we report on the score achieved by machine translation and the increase in combination with the concept models. We analyzed how often a word was apparently unknown from the machine translation system trained on Europarl and was therefore easily copied erroneously. It would be possible to automatically detect this type of error. In addition, for each translated query, we counted how many words in it had no semantic meaning related to the purpose of the query and were therefore useless (these words are therefore called junk words). Junk words, for example, are functional words that were not filtered through the stopword list, machine translation errors of several types and artifacts from the query formulation, mainly from the descriptive part."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "between translation and the concept model, which is based on the reliability value of machine translation, improves performance."}, {"heading": "4.4 Discussion", "text": "We have shown that with LDA, a highly significant transnational query component can be achieved, since standard and multilingual LDA behave the same for length normalized data. We found the two very simple techniques most effective are document disabling and model combination, which gives us an improvement of 325% mrr compared to a non-competitive value previously reported for LDA on the multi-text corpus. Combining LDA with ESA increases performance in partner search experiments by 15.6% mrr compared to ESA alone. In cross-language query-based query for CLEF 2000, a monolingual scheme based on machine translation performance can be improved by 21.1% in combination with concept models. While ESA performs better in partner search alone, we find that LDA is superior in query-based query, perhaps because commonly used ESA parameters for direct translation optimization have been 4950."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Towards Retrieval Based on Only Wikipedia", "text": "Conceptual models trained at the article level can contribute to cross-linguistic queries, but the main contribution comes from the statistical module of machine translation trained at Europarl. Rough thematic contexts alone may not capture the specific meaning contained in a query, so word-to-word translation is necessary. However, encyclopaedias and parallel corporas are often only accessible to some European language pairs, and even if they do exist, their vocabulary is limited per se, as opposed to an ever-growing resource such as Wikipedia. Thus, the question arises as to how parallel texts or word-to-word mappings, which are less dispersed than those derived from co-occurrences at the documentation level, can be replaced by information derived from freely available knowledge sources such as Wikipedia.Several attempts have been made in this direction. Frequently, a Wikipedia title in the source language is linked to a word or the corresponding title, and the corresponding title in the target language is used as the translation model."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The method we use is based on the anchor text of links. In Wikipedia, a text that is expected to be of particular interest to the reader is linked to another Wikipedia article if it contributes to understanding it. The key policy set out in the Wikipedia Guidelines is: \"Ask yourself:\" How likely is it that the reader will want to read this other article? \"These links should be inserted where the reader is most likely to want to use them; [...] Always link to the article on the most specific topic from which you link: it will usually contain more focused information, as well as links to more general topics.13 Any text can be linked to any page. For example, the German texts\" Brands laid in Forests, \"\" Bushfires, \"Bushfires,\" and \"Forest Fires\" are valid contexts that can be linked to the article \"Forest Fires\" (the English equivalent). \""}, {"heading": "5.1 Model", "text": "For the sake of simplicity, if a universal model is used, the probability is P (l | w), P (a | w, l) and P (w | a, l). In a bilingual environment, l is a variable that indicates whether the source word is linked, a is the bilingual article with which the source word is linked, and wE and wF are words in the source or target language, respectively. It reads: P (wF | wE) = P (wF | l = true, wE) P (l = true | wE) + P (wF | l = false, wE) P (l = false | wE) Assuming that the translation of linked words depends only on the articles with which they are linked, you get: 13http: / / en.wikipedia.org / wiki / Wikipedia: Linking Found on October 13, 200952"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "P (wF | l = true, wE) = \ufffd aP (wF | a, l = true) P (a | l = true, wE) If Wikipedia links are the only cross-language bridge, a general language model LMF could be used in the unlinked case to estimate the likelihood that other translation sources could also be used. P (wF | l = false, wE) = P (wF | LMF) In [Mihalcea and Csomai, 2007] several methods were tried to predict whether a word was linked or not. The most effective was based on the relative frequency of linked events. We also take this approach for the other distributions and getP (l = true | wE) = n (wE) n (wE) P (a | l = true, wE) n (wE) P (a = true, wE) = n (b = true, wE)."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Viewed as a whole, the specificity of the articles would correspond better to the specificity of the texts. There are many ways in which such a model can be used, for example, for query translation. First, we note that the probability P (l = true | wE) can act as term weighting, provided that the importance of the terms is correlated with their likelihood of linking. We use this assumption and use P (wF, l = true | wE) for a translation model. It is important to bear in mind that l indicates the linking of the source word. Furthermore, translation and linking should be independent of the document, with a source word (but not independently of each other). The translation model of a document DE is the probability distribution, which can be circumscribed as follows: The probability that a word selected from D is linked and has wF as translation."}, {"heading": "5.2 Vector Space Experiments", "text": "The queries are translated from German to English according to the following scheme: When translating the original Qorig query, words are stored with P (wtransl | l = true, Qorig) \u2265 1. Counts are assigned to them proportionally to this probability. It is retrieved using the translated query by tf.idf weighting and adopting the cosine in the same way as for queries translated with Europarl. In addition, we interpolated the cosinal values of this run with the values of the other models trained on Wikipedia, which include the LDA model, the ESA model and the combination of LDA and ESA that worked best in the multi-text experiments of section 4.2. In combination with the link translation model, LDA showed the best and most stable performance of up to card = 0.234. This is better than restoring LDA queries (0.0 = DA-3), with the translation of the SDA component being improved with the SV component with the SAV component at the least."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "As an alternative way to use the link model, the cosmic similarity for vectors with association strengths to Wikipedia articles was used, similar to the ESA model. In the link model, we used the distribution vector P (a | l = true, Qorig) and P (a | l = true, D) to determine a similarity between query Q and document D. Using the article distribution vectors was a less successful method of applying the link model, but still better than the ESA model. Figure 16 and Table 7 provide an overview of the results obtained with the link transformation model."}, {"heading": "5.3 Language Modelling Experiments", "text": "In a query probability model [Ponte and Croft, 1998], a document provides a probabilistic model for a query, which is usually preceded by: logP (Q | D) = \ufffd w-Qn (w, Q) log P (w | D) In principle, all components are provided by the link model to perform such a query. The probability that article a is the target when a linked word is selected from document D (assuming the same assumptions of independence as before) isP (a | D, l) = \ufffd wE P (a | l = true, wE) P (wE | D) a \u2032, wE P (l = true, wE) isP (wE) P (wE | D) and P (wF | a, l) is the probability that the source word associated with article a has a probability."}, {"heading": "5.3.1 Model Combination on Query Level", "text": "The zero probability problem is particularly severe if plink (wF | D, l) = \ufffd aP (wF | a, l) P (a | D, l) = 0leaves the protocol probability undefined, if P (wF | a, l) = 0 for the articles a with unequal probability for document D. However, smoothing a probability P (wF | a, l) > 0 for all word and document combinations would result in a complete matrix where the number of entries would be equal to the vocabulary size multiplied by Wikipedia size. Furthermore, smooth distributions P (wF | a, l) and P (a | D, l) would require the complete summation of all articles, while in the sparse case only the matching non-zeros need to be taken into account."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "By smoothing P (wF | a, l) with Laplace (add-one) Q = Q = QDA estimates, they are multiplied by probability.Plink (wF | D, l) = max \ufffd \ufffd aP (wF | a, l) P (a | D, l), Pmin (wF | l) \ufffd \ufffd, because \ufffd aPmin (wF | l) P (a | D, l) Pmin (w) P (a), l \ufffd \ufffd w (wF | l), the highest number of links observed for an article, and V is the vocabulary size. The first, Plink (Q | l, D), calculates the probability of the query that we will directly use the defined components PLPLPL.The second, Plink \u2032 (Q | l, D), attempts to take into account the training problem by multiplying all QDA estimates by probability."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.2 Model Combination on Word Level", "text": "The second possible form of the model combination is to interpolate word distributions on the basis of a document. Here, the question arises as to how the query words should be weighted, since a weighting according to the link probability seems reasonable and proves successful for the link model, but is not justified for the LDA. Therefore, we used two model parameters \u03b1 and \u03b2 to interpolate distributions and weightings, respectively. The query probability will be: logP\u03b1, \u03b2 (Q | D) = \ufffd w-Qn (w, Q) [\u03b2P (l | w) + (1 \u2212 \u03b2)] log [\u03b1Plink (w | D, l) + (1 \u2212 \u03b1) PLDA (w | D)]] The LDA distributions are smooth by definition, so that there is no zero probability problem for 0 \u2264 \u03b1 < 1. Since the smoothing method used in the query combination 2001, which was applied to this level of LEc-\u03b1, was necessary for this level = 30d."}, {"heading": "5.4 Discussion", "text": "We have investigated three possibilities of using translation models based on Wikipedia links and evaluating them on the basis of the CLEF2000 dataset: the first is to construct a bag-of-words representation and retrieve it using a tf.idf scheme; the second is to compare multinomial distribution vectors with association strengths to Wikipedia articles; and the third is based on language modeling formulation. The purely link-based bag-of-words approach failed to achieve the performance of vector space retrievals achieved with Moses translation results trained on Europarl. However, when combined with LDA vectors, it yielded better results and almost as good results as the machine translation output in the same combination. However, the distribution vectors showed even worse performance than the ESA vectors on the same dataset. The approach to language modeling is clearly promising, as there is much room for exploration of different combinations and combinations."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Two different schemes of interpolating language models with LDA were tried, one based on the combination at the query level, the other on the combination at the word level. Although the query level approach has fewer parameters, we achieved better results with the word level approach. The language modeling approach not only exceeded all of our results achieved with Moses machine translation, but also provided results that could be compared favorably with values reported for CLEF 2000 ad hoc track14."}, {"heading": "6 Conclusion", "text": "In this paper, we compare three concept models for cross-language recovery: First, we adapt probabilistic latent semantic analysis (pLSA) for multilingual documents. Experiments with different weighting schemes show that a weighting method that favors the bridging of documents of equal length on both language pages yields the best results. Together with the finding that both the monolingual and the multilingual latent semantic funnel allocation (LDA) are the same when applied to such documents, we use a training corpus based on Wikipedia in which all documents are length standardized. Thus, we note a considerable improvement over previously reported results of the multilingual JOC corpus for LDA. Another focus of our work is on the model combination. To this end, we include the explicit semantic analysis (ESA) in the experiments. We observe that apart from the direct translation finding, where it was originally evaluated on the LEDA translation question, the results of a translation engine are not capable of being performed in the LEDA."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The approach to language modeling leaves a lot of room for further experimentation, as combining different models is not easy. Therefore, the need for smoothing raises interesting questions about solidity and efficiency. Linkage models capture only a certain type of information and suggest weighting schemes to emphasize certain words. Therefore, a combined model raises another interesting question about how different weighting schemes can be integrated. Using a very simple combination scheme, we obtained values that can be compared favorably with results previously published on the CLEF 2000 datasets. We have shown how probabilistic techniques that work without parallel training texts can be used to replace other cross-language retrieval methods. In principle, our methods can be used wherever language barriers at the text level are to be overcome. Finally, it would be interesting to see if our combination techniques that are successful for cross-language retrievaluation also contribute to monolingual retrieval.62"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "P. Cimiano, TU Delft, A. Schultz, S. Sizov, P. Sorg, and S. Staab. Explicit VersusLatent Concept Models for Cross-Language Information Retrieval. 2009.D. Cohn and T. Hofmann. The Missing Link - A Probabilistic Model of DocumentContent and Hypertext Connectivity. Advances in Neural Information Processing System, pp. 430-436, 2001.S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. Indexingby latent semantic analysis. Journal of the American Society for Information Science, 41 (6): 391-407, 1990.A.P. Dempster, N.M. Laird, D.B. Rubin, et al. Maximum likelihood from incom-plete data via the EM algorithm."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "T. Hofmann, J. Puzicha, and MI Jordan. Unsupervised learning from dyadic data.Advances in Neural Information Processing Systems, 11, 1999.P. Koehn. Europarl: A parallel corpus for statistical machine translation. In MTsummit, Volume 5, 2005.P. Koehn, F.J. Och, and D. Marcu. Statistical phrase-based translation. In Proceed-ings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pp. 48-54. Association for Computational Linguistics Morristown, NJ, USA, 2003.X. Liu and W.B. Croft. Statistical language modeling for information retrieval. AnnualReview of Information Science and Technology 2005: Volume 39, Page 1, 2004.D.J.C. MacKay. Information theory, inference, and learning algorithms."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "M.E.J. Newman and GT Barkema. Monte Carlo Methods in Statistical Physics. Oxford University Press, USA, 1999.D. Nguyen, A. Overwijk, C. Hauff, RB Trieschnigg, D. Hiemstra, and F.M.G. de Jong.WikiTranslate: Query Translation for Cross-lingual Information Retrieval using only Wikipedia. In Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008, Revised Selected Papers, page 58. Springer, 2009.X. Ni, J.T. Sun, J. Hu, and Z. Chen. Mining multilingual topics from wikipedia. InProceedings of the 18th International Conference on World Wide Web, pages 1155- 1156. ACM New York, NY, USA, 2009.J.S. Olsson, and J. Hajic New York Pro-York 652 annual information."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "ACM New York, NY, USA, 1999.P. Sorg and P. Cimiano. Cross-lingual information retrieval with explicit semantic analysis. In Working Notes of the Annual CLEF Meeting, 2008.P. Sorg and P. Cimiano. An Experimental Comparison of Explicit Semantic AnalysisImplementation of Cross-Language Retrieval. In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB '09). 2009.M. Steyvers and T. Griffiths. Handbook of Latent Semantic Analysis. In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems. (NLDB' 09). 200.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Y. Wu and D.W. Oard. Bilingual Topic Classification with a Few Training Examples. In Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval, pp. 203-210. ACM New York, NY, USA, 2008.C. Zhai and J. Lafferty. A study of smoothing methods for language models appliedto information retrieval. ACM Transactions on Information Systems (TOIS), 22 (2): 214, 2004.68"}], "references": [{"title": "Finding similar sentences across multiple languages in wikipedia", "author": ["S.F. Adafre", "M. de Rijke"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Adafre and Rijke.,? \\Q2006\\E", "shortCiteRegEx": "Adafre and Rijke.", "year": 2006}, {"title": "Topic based language models for ad hoc information retrieval", "author": ["L. Azzopardi", "M. Girolami", "CJ Van Rijsbergen"], "venue": "IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Azzopardi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Azzopardi et al\\.", "year": 2004}, {"title": "Enhancing Multilingual Latent Semantic Analysis with Term Alignment Information", "author": ["B.W. Bader", "P.A. Chew"], "venue": "Proceedings of COLING", "citeRegEx": "Bader and Chew.,? \\Q2008\\E", "shortCiteRegEx": "Bader and Chew.", "year": 2008}, {"title": "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hiddenMarkov models", "author": ["J.A. Bilmes"], "venue": "International Computer Science Institute,", "citeRegEx": "Bilmes.,? \\Q1998\\E", "shortCiteRegEx": "Bilmes.", "year": 1998}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Springer New York:,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multilingual Topic Models for Unaligned Text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "The 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Boyd.Graber and Blei.,? \\Q2009\\E", "shortCiteRegEx": "Boyd.Graber and Blei.", "year": 2009}, {"title": "CLEF 2000-Overview of results. In Cross-language information retrieval and evaluation: workshop of the Cross-Language Evaluation Forum, CLEF", "author": ["M. Braschler"], "venue": null, "citeRegEx": "Braschler.,? \\Q2000\\E", "shortCiteRegEx": "Braschler.", "year": 2000}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech and Language,", "citeRegEx": "Chen and Goodman.,? \\Q1999\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "Benefits of the Passively Parallel Rosetta Stone? CrossLanguage Information Retrieval with over 30 Languages", "author": ["P. Chew", "A. Abdelali"], "venue": "In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Chew and Abdelali.,? \\Q2007\\E", "shortCiteRegEx": "Chew and Abdelali.", "year": 2007}, {"title": "Explicit Versus Latent Concept Models for Cross-Language Information Retrieval", "author": ["P. Cimiano", "TU Delft", "A. Schultz", "S. Sizov", "P. Sorg", "S. Staab"], "venue": null, "citeRegEx": "Cimiano et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2009}, {"title": "The Missing Link \u2013 A Probabilistic Model of Document Content and Hypertext Connectivity", "author": ["D. Cohn", "T. Hofmann"], "venue": "Advances in Neural Information Processing System,", "citeRegEx": "Cohn and Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Cohn and Hofmann.", "year": 2001}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Automatic crosslanguage retrieval using latent semantic indexing", "author": ["S.T. Dumais", "T.A. Letsche", "M.L. Littman", "T.K. Landauer"], "venue": "AAAI Spring Symposuim on Cross-Language Text and Speech Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1997}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Gabrilovich and Markovitch.,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Bilingual tests with Swedish, Finnish, and German queries: Dealing with morphology, compound words, and query structure", "author": ["T. Hedlund", "H. Keskustalo", "A. Pirkola", "M. Sepponen", "K. Jarvelin"], "venue": "Lecture notes in computer science,", "citeRegEx": "Hedlund et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hedlund et al\\.", "year": 2001}, {"title": "A linguistically motivated probabilistic model of information retrieval", "author": ["D. Hiemstra"], "venue": "Lecture notes in computer science,", "citeRegEx": "Hiemstra.,? \\Q1998\\E", "shortCiteRegEx": "Hiemstra.", "year": 1998}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann.", "year": 2001}, {"title": "Unsupervised learning from dyadic data", "author": ["T. Hofmann", "J. Puzicha", "MI Jordan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hofmann et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 1999}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical language modeling for information retrieval", "author": ["X. Liu", "W.B. Croft"], "venue": "Annual Review of Information Science and Technology 2005: Volume 39,", "citeRegEx": "Liu and Croft.,? \\Q2004\\E", "shortCiteRegEx": "Liu and Croft.", "year": 2004}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J.C. MacKay"], "venue": "Cambridge Univ Pr,", "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Creating Sentence-Aligned Parallel Text Corpora from a Large Archive of Potential Parallel Text using BITS and Champollion", "author": ["K. Maeda", "X. Ma", "S. Strassel"], "venue": "In Proceedings of the Sixth Language Resources and Evaluation Conference,", "citeRegEx": "Maeda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maeda et al\\.", "year": 2008}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["R. Mihalcea", "A. Csomai"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Mihalcea and Csomai.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea and Csomai.", "year": 2007}, {"title": "A hidden Markov model information retrieval system", "author": ["D.R.H. Miller", "T. Leek", "R.M. Schwartz"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Miller et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1999}, {"title": "Polylingual Topic Models", "author": ["David Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Newman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2007}, {"title": "Monte Carlo methods in statistical physics", "author": ["M.E.J. Newman", "GT Barkema"], "venue": null, "citeRegEx": "Newman and Barkema.,? \\Q1999\\E", "shortCiteRegEx": "Newman and Barkema.", "year": 1999}, {"title": "WikiTranslate: Query Translation for Cross-lingual Information Retrieval using only Wikipedia", "author": ["D. Nguyen", "A. Overwijk", "C. Hauff", "RB Trieschnigg", "D. Hiemstra", "F.M.G. de Jong"], "venue": "Revised Selected Papers,", "citeRegEx": "Nguyen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2008}, {"title": "Mining multilingual topics from wikipedia", "author": ["X. Ni", "J.T. Sun", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Ni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2009}, {"title": "Haji\u010d. Cross-language text classification", "author": ["J.S. Olsson", "D.W. Oard"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Olsson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Olsson et al\\.", "year": 2005}, {"title": "European Research Letter: cross-language system evaluation: the CLEF campaigns", "author": ["C. Peters", "M. Braschler"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Peters and Braschler.,? \\Q2001\\E", "shortCiteRegEx": "Peters and Braschler.", "year": 2001}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Ponte and Croft.,? \\Q1998\\E", "shortCiteRegEx": "Ponte and Croft.", "year": 1998}, {"title": "A wikipedia-based multilingual retrieval model", "author": ["M. Potthast", "B. Stein", "M. Anderka"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Potthast et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Potthast et al\\.", "year": 2008}, {"title": "Okapi at TREC-3", "author": ["SE Robertson", "S. Walker", "S. Jones", "M. Gatford", "MM Hancock-Beaulieu", "N. Square", "E.C.V. London"], "venue": "In Overview of the Third Text REtrieval Conference", "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Term weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": null, "citeRegEx": "Salton and Buckley.,? \\Q1987\\E", "shortCiteRegEx": "Salton and Buckley.", "year": 1987}, {"title": "What Types of Translations Hide in Wikipedia", "author": ["J. Sjobergh", "O. Sjobergh", "K. Araki"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Sjobergh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sjobergh et al\\.", "year": 2008}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B. Croft"], "venue": "In Proceedings of the eighth international conference on Information and knowledge management,", "citeRegEx": "Song and Croft.,? \\Q1999\\E", "shortCiteRegEx": "Song and Croft.", "year": 1999}, {"title": "Cross-lingual information retrieval with explicit semantic analysis", "author": ["P. Sorg", "P. Cimiano"], "venue": "In Working Notes of the Annual CLEF Meeting,", "citeRegEx": "Sorg and Cimiano.,? \\Q2008\\E", "shortCiteRegEx": "Sorg and Cimiano.", "year": 2008}, {"title": "An Experimental Comparison of Explicit Semantic Analysis Implementations for Cross-Language Retrieval", "author": ["P. Sorg", "P. Cimiano"], "venue": "In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB\u201909),", "citeRegEx": "Sorg and Cimiano.,? \\Q2009\\E", "shortCiteRegEx": "Sorg and Cimiano.", "year": 2009}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of Latent Semantic Analysis,", "citeRegEx": "Steyvers and Griffiths.,? \\Q2007\\E", "shortCiteRegEx": "Steyvers and Griffiths.", "year": 2007}, {"title": "Deterministic annealing EM algorithm", "author": ["N. Ueda", "R. Nakano"], "venue": "Neural Networks,", "citeRegEx": "Ueda and Nakano.,? \\Q1998\\E", "shortCiteRegEx": "Ueda and Nakano.", "year": 1998}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Yi Wang", "Hongjie Bai", "Matt Stanton", "Wen-Yen Chen", "Edward Y. Chang"], "venue": "In Proc. of 5th International Conference on Algorithmic Aspects in Information and Management,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval,", "citeRegEx": "Wei and Croft.,? \\Q2006\\E", "shortCiteRegEx": "Wei and Croft.", "year": 2006}, {"title": "Building a multilingual lexical resource for named entity disambiguation, translation and transliteration", "author": ["W. Wentland", "J. Knopp", "C. Silberer", "M. Hartung"], "venue": "Proc. of LREC08,", "citeRegEx": "Wentland et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wentland et al\\.", "year": 2008}, {"title": "Generalized vector spaces model in information retrieval", "author": ["SKMWong", "W. Ziarko", "P.C.N. Wong"], "venue": "In Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "SKMWong et al\\.,? \\Q1985\\E", "shortCiteRegEx": "SKMWong et al\\.", "year": 1985}, {"title": "On the convergence properties of the EM algorithm", "author": ["C.F.J. Wu"], "venue": "The Annals of Statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "Bilingual topic aspect classification with a few training examples", "author": ["Y. Wu", "D.W. Oard"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval,", "citeRegEx": "Wu and Oard.,? \\Q2008\\E", "shortCiteRegEx": "Wu and Oard.", "year": 2008}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Zhai and Lafferty.,? \\Q2004\\E", "shortCiteRegEx": "Zhai and Lafferty.", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Such a representation may, for example, be obtained by matrix approximation [Deerwester et al., 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 76, "endOffset": 101}, {"referenceID": 44, "context": ", 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 36, "endOffset": 66}, {"referenceID": 15, "context": ", 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 152, "endOffset": 186}, {"referenceID": 12, "context": "The most simple models of that kind work directly on bag-ofword vectors and are prone to sparseness and missing word overlap, some backing-off can be achieved by dimensionality reduction techniques such as Latent Semantic Analysis [Deerwester et al., 1990] or term expansion as in the generalized vector space model [Wong et al.", "startOffset": 231, "endOffset": 256}, {"referenceID": 39, "context": "idf scheme [Salton and Buckley, 1987].", "startOffset": 11, "endOffset": 37}, {"referenceID": 38, "context": "Similar starting points and theoretical properties exhibit tuned ranking functions like the popular Okapi-BM25 family [Robertson et al., 1995], which are still achieving state-of-the-art results [Armstrong et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 36, "context": "2 Language Modeling Retrieval Language model based retrieval is a newer paradigm [Ponte and Croft, 1998], where usually documents are treated as providing probabilistic models of generating the queries, and the document that provides the best model is regarded as most relevant.", "startOffset": 81, "endOffset": 104}, {"referenceID": 24, "context": "See [Liu and Croft, 2004] for an excellent overview.", "startOffset": 4, "endOffset": 25}, {"referenceID": 20, "context": "The two most prominent latent variable models are probabilistic Latent Semantic Analysis [Hofmann, 2001], which implements the ideas of variable modeling in a straightforward way but does not define inference on documents not present in the training collection, and, developed some years later, Latent Dirichlet Allocation [Blei et al.", "startOffset": 89, "endOffset": 104}, {"referenceID": 5, "context": "The two most prominent latent variable models are probabilistic Latent Semantic Analysis [Hofmann, 2001], which implements the ideas of variable modeling in a straightforward way but does not define inference on documents not present in the training collection, and, developed some years later, Latent Dirichlet Allocation [Blei et al., 2003], which defines a fully generative model", "startOffset": 323, "endOffset": 342}, {"referenceID": 46, "context": "and is also favorable in terms of scalability [Wang et al., 2009].", "startOffset": 46, "endOffset": 65}, {"referenceID": 22, "context": "Parallel corpora, such as the Europarl corpus of EU-Parliament proceedings [Koehn, 2005], provide translations of the same text in different languages, corresponding phrase pairs can be extracted automatically [Koehn et al.", "startOffset": 75, "endOffset": 88}, {"referenceID": 23, "context": "Parallel corpora, such as the Europarl corpus of EU-Parliament proceedings [Koehn, 2005], provide translations of the same text in different languages, corresponding phrase pairs can be extracted automatically [Koehn et al., 2003].", "startOffset": 210, "endOffset": 230}, {"referenceID": 14, "context": "The first approach [Dumais et al., 1997] to using comparable corpora and dimensionality reduction was done with Latent Semantic Analysis (LSA), a technique making use of lower-rank matrix approximation by singular value decomposition.", "startOffset": 19, "endOffset": 40}, {"referenceID": 12, "context": "As an effect, terms and documents are represented by lower-dimensional vectors, making them topically comparable [Deerwester et al., 1990].", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "While the method works well and is still used as a baseline [Cimiano et al., 2009], it lacks, as does monolingual LSA, a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding gigabyte-sized corpora such as Wikipedia.", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "While the method works well and is still used as a baseline [Cimiano et al., 2009], it lacks, as does monolingual LSA, a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding gigabyte-sized corpora such as Wikipedia. A more modern approach could be based on probabilistic Latent Semantic Analysis (pLSA Hofmann [2001]).", "startOffset": 61, "endOffset": 374}, {"referenceID": 10, "context": "LDA experiments for cross-lingual IR have been published [Cimiano et al., 2009] with disappointing outcomes compared to other techniques.", "startOffset": 57, "endOffset": 79}, {"referenceID": 29, "context": "[2009] for web-page classification (see also [Mimno et al., 2009]).", "startOffset": 45, "endOffset": 65}, {"referenceID": 10, "context": "The evaluation results on the method are inconsistent: while it has been found to work well on mate-retrieval tasks in a multilingual setting and is reported to surpass LDA there [Cimiano et al., 2009], in an actual query-based document retrieval setting it showed no competitive performance [Sorg and Cimiano, 2008].", "startOffset": 179, "endOffset": 201}, {"referenceID": 42, "context": ", 2009], in an actual query-based document retrieval setting it showed no competitive performance [Sorg and Cimiano, 2008].", "startOffset": 98, "endOffset": 122}, {"referenceID": 20, "context": "These findings may be explained in different ways: for the first finding there might have been problems in how LDA was applied and compared, for the second finding, no interpolation with a word-vector based representation was undertaken, as it is usually done for dimensionality reduction techniques such as LSA and pLSA [Hofmann, 2001].", "startOffset": 321, "endOffset": 336}, {"referenceID": 10, "context": "LDA experiments for cross-lingual IR have been published [Cimiano et al., 2009] with disappointing outcomes compared to other techniques. These results do not convincingly show the inappropriateness of LDA, as the authors have embedded multilingual Wikipedia articles without any length normalization or model adaptation. For a method that tries to model characteristics of term statistics, the biggest increase in data-likelihood with limited model capacities should be expected by effectively capturing the predominant language vocabulary instead of doing topical modeling. An indicator that this effect might indeed be at work is that in another run of experiments, trained with in-domain data that have almost constant length ratio (because they are parallel), LDA performs as well as the other methods. Dealing with the supposed length ratio problem could be done either by processing the data accordingly or by adapting the model in a similar fashion as suggested by Ni et al. [2009] for web-page classification (see also [Mimno et al.", "startOffset": 58, "endOffset": 990}, {"referenceID": 2, "context": "In [Bader and Chew, 2008] the fact is used that SVD of a matrix X can be computed by the Eigenvalue Decomposition of the composite matrix", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "Cohn and Hofmann [2001] use pLSA to include as an additional source of information links to a document (from another document).", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Secondly, to us it is not evident that the EM-re-estimation equations in Cohn and Hofmann [2001], while looking reasonable at a first glance, really derive from the initial problem statement.", "startOffset": 73, "endOffset": 97}, {"referenceID": 6, "context": "An interesting alternative based on LDA has been proposed recently by Boyd-Graber and Blei [2009]. Here, multingual pairings of words are extracted from parallel or comparable corpora in an unsupervised way and assigned to a topic.", "startOffset": 70, "endOffset": 98}, {"referenceID": 35, "context": "General competitions like the Cross-Language Evaluation Forum (CLEF, [Peters and Braschler, 2001]) as well as datasets allowing for more specific comparisons are to be taken into consideration.", "startOffset": 69, "endOffset": 97}, {"referenceID": 6, "context": ", 2008]), translated news collections and parliament debates (as in [Boyd-Graber and Blei, 2009]), the Official Journal of the European Community and european law texts (as in [Potthast et al.", "startOffset": 68, "endOffset": 96}, {"referenceID": 10, "context": "For our experiments, we evaluate on two datasets: To establish comparability with the findings on LDA and ESA in [Cimiano et al., 2009] and [Sorg and Cimiano, 2009], an evaluation on the Multext JOC corpus is undertaken.", "startOffset": 113, "endOffset": 135}, {"referenceID": 43, "context": ", 2009] and [Sorg and Cimiano, 2009], an evaluation on the Multext JOC corpus is undertaken.", "startOffset": 12, "endOffset": 36}, {"referenceID": 12, "context": "From the equivalent symmetric factorization given in equation 4 some parallels to Latent Semantic Analysis [Deerwester et al., 1990], based on truncated singular value decomposition, can be drawn: the values P (di|\u00b7) and P (wj |\u00b7) play a role similar to that of the left and right singular vectors: in both cases co-occurrence matrices can be constructed from them for each of the assumed topics; P (zk) corresponds to singular values by weighting the topical co-occurrence matrices.", "startOffset": 107, "endOffset": 132}, {"referenceID": 20, "context": "For standard pLSA, we refer to [Hofmann, 2001] for a full derivation.", "startOffset": 31, "endOffset": 46}, {"referenceID": 5, "context": "2 Sampling for LDA The first approach to estimate such a model [Blei et al., 2003] was to represent and estimate \u03c8 and \u03b8 explicitly, resulting in different inference tasks to be solved and combined.", "startOffset": 63, "endOffset": 82}, {"referenceID": 16, "context": "Later approaches concentrate on getting a sample of the assignment of words to topics instead [Griffiths and Steyvers, 2004].", "startOffset": 94, "endOffset": 124}, {"referenceID": 16, "context": "3 Practical Issues To determine the similarity between two documents, one can compare either their sampled topic vectors or the probability vectors obtained from them [Griffiths and Steyvers, 2004].", "startOffset": 167, "endOffset": 197}, {"referenceID": 5, "context": ", 2009, Blei et al., 2003]. The comparison between these vectors can be done either by taking the cosine similarity of their angles or, in case the vector is indeed a probability distribution (as for \u03b8), by using probability divergence measures, such as the (symmetrized) Kullback Leibler divergence or the Jensen-Shannon divergence. For language model based information retrieval, one is interested in the probability of a query, given a document p(q|di). Wei and Croft [2006] interpolate a language model based on LDA with a unigram language model directly estimated on the document.", "startOffset": 8, "endOffset": 478}, {"referenceID": 33, "context": "4 Multilingual LDA LDA has been extended for several languages [Ni et al., 2009], see also [Mimno et al.", "startOffset": 63, "endOffset": 80}, {"referenceID": 29, "context": ", 2009], see also [Mimno et al., 2009] for an investigation of the semantic clustering of this model.", "startOffset": 18, "endOffset": 38}, {"referenceID": 29, "context": "It is crucial [Mimno et al., 2009] how many \u201cglue documents\u201d, i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 15, "context": "The word vectors that were used in [Gabrilovich and Markovitch, 2007] and found to be optimal in [Sorg and Cimiano, 2009] are obtained by taking the respective columns of the tf.", "startOffset": 35, "endOffset": 69}, {"referenceID": 43, "context": "The word vectors that were used in [Gabrilovich and Markovitch, 2007] and found to be optimal in [Sorg and Cimiano, 2009] are obtained by taking the respective columns of the tf.", "startOffset": 97, "endOffset": 121}, {"referenceID": 15, "context": "In [Gabrilovich and Markovitch, 2007] a weigting of the word vectors is used, they are multiplied with scalars equal to, again, an tf.", "startOffset": 3, "endOffset": 37}, {"referenceID": 15, "context": "In [Gabrilovich and Markovitch, 2007] a weigting of the word vectors is used, they are multiplied with scalars equal to, again, an tf.idf weighting of the terms, and then summed up. It is however not very clearly described which exact instantiation of the tf.idf function was used in the experiments. Sorg and Cimiano [2009] explore further combination schemes, including the sum of the elements of either the multiset (considering term frequency) or of the set (not considering term frequency) of word vectors.", "startOffset": 4, "endOffset": 325}, {"referenceID": 42, "context": "In Sorg and Cimiano [2009] it was found that a cut-off value of c = 10000 works best.", "startOffset": 3, "endOffset": 27}, {"referenceID": 42, "context": "Sorg and Cimiano [2008] observe that even when the ESA model is used to compare documents taken from two languages out of the set French, German and English it slightly improves retrieval performance to restrict the training corpus to documents available in all of the three languages.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "How does a monolingual LDA model perform on multilingual data \u2022 in the form reported in [Cimiano et al., 2009]? \u2022 when using a scheme that takes theoretical considerations into account? 4.", "startOffset": 88, "endOffset": 110}, {"referenceID": 19, "context": "\ufffdK k=1 P (zk|d (2))2 This ranking scheme was applied and found to work best by Hofmann [2001] for a monolingual retrieval task and standard pLSA.", "startOffset": 79, "endOffset": 94}, {"referenceID": 46, "context": "The highly efficient and parallel LDA implementation plda [Wang et al., 2009] was used.", "startOffset": 58, "endOffset": 77}, {"referenceID": 16, "context": "We used the Dirichlet parameter values suggested by Griffiths and Steyvers [2004], \u03b1 = 50 (number of topics) and \u03b2 = 0.", "startOffset": 52, "endOffset": 82}, {"referenceID": 10, "context": "For LDA we performed three types of experiments: First, we sampled a model for the comparable documents with standard LDA without any length normalization, in the manner of [Cimiano et al., 2009].", "startOffset": 173, "endOffset": 195}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart.", "startOffset": 6, "endOffset": 30}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart. Our experiments were done with English as the query language and German as the target language. Only preprocessing steps that are clear and easy to reproduce were performed. Exactly those questions were retained that to which an answer was assigned and had the same id in English and German. This resulted in a set of 3212 texts in each language, 157 more than were used in Sorg and Cimiano [2008]8.", "startOffset": 6, "endOffset": 601}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart. Our experiments were done with English as the query language and German as the target language. Only preprocessing steps that are clear and easy to reproduce were performed. Exactly those questions were retained that to which an answer was assigned and had the same id in English and German. This resulted in a set of 3212 texts in each language, 157 more than were used in Sorg and Cimiano [2008]8. Sequences of characters in Unicode letter blocks were considered words. Words with length = 1 or length > 64 and words contained in the Snowball stopword list were ignored. All other words were stemmed with the publicly available Snowball stemmer9. In contrast to Sorg and Cimiano [2008], no compound splitting was done.", "startOffset": 6, "endOffset": 891}, {"referenceID": 42, "context": "The ESA retrieval experiment was performed using the same parametrization as discribed before and the result of Sorg and Cimiano [2008] was reproduced to a difference of 1% (in our experiments we obtained a score of mrr = 0.", "startOffset": 112, "endOffset": 136}, {"referenceID": 46, "context": "The Google plda package Wang et al. [2009] was used with the suggested parameters (\u03b1 = 50 #topics and \u03b2 = 0.", "startOffset": 24, "endOffset": 43}, {"referenceID": 42, "context": "A drastic improvement over non-normalized LDA can be observed: while Sorg and Cimiano [2008] report a score of mrr = 0.", "startOffset": 69, "endOffset": 93}, {"referenceID": 19, "context": "Another improvement can be observed by combining the results of different models, a technique that is usually applied for pLSA Hofmann [2001]. In this case, the cosine scores of runs with different dimensional models were simply averaged (this corresponds to concatenating the L2-norm normalized sampling statistics vectors).", "startOffset": 127, "endOffset": 142}, {"referenceID": 31, "context": ", 2008, Nguyen et al., 2009]. However, Sjobergh et al. [2008] note that the vocabulary distribution of titles has a skew to certain words: 33% are normal nouns, 66% proper nouns and only 0.", "startOffset": 8, "endOffset": 62}, {"referenceID": 27, "context": "P (wF |l = false, wE) = P (wF |LMF ) In [Mihalcea and Csomai, 2007] several methods were tried to predict whether a word was linked or not.", "startOffset": 40, "endOffset": 67}, {"referenceID": 36, "context": "3 Language Modelling Experiments In a query likelihood model [Ponte and Croft, 1998] a document provides a probabilistic model for a query.", "startOffset": 61, "endOffset": 84}, {"referenceID": 47, "context": "The third, PLDA(Q|D), estimates the probability according to the estimates obtained by the LDA models, in the same way as done in [Wei and Croft, 2006] for monolingual retrieval.", "startOffset": 130, "endOffset": 151}], "year": 0, "abstractText": null, "creator": "cairo 1.10.2 (http://cairographics.org)"}}}