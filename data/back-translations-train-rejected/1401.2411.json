{"id": "1401.2411", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "Clustering, Coding, and the Concept of Similarity", "abstract": "This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\\bf x})$, and its gradient, $\\nabla U({\\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.", "histories": [["v1", "Fri, 10 Jan 2014 17:36:23 GMT  (2078kb,D)", "http://arxiv.org/abs/1401.2411v1", "55 pages, 13 figures"]], "COMMENTS": "55 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["l thorne mccarty"], "accepted": false, "id": "1401.2411"}, "pdf": {"name": "1401.2411.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. Introduction.", "text": "This year is the highest in the history of the country."}, {"heading": "2. Mathematical Background.", "text": "We will start with a model that most physicists will be familiar with: the Feynman-Kac formula [Feyn48] [Kac49]. We will write this formula as follows: (1) u (t, x) = 0 (Xt) exp [t 0 V (Xs) ds] Wx (dX) However, here is a solution to the problem of the continuous path in Rn, and Wx denotes Viennese measure of all such paths starting at X0 = x. If V: Rn \u2192 R is limited at the bottom, then it is a solution to the Cauchy initial value problem: (2) Xp \u2212 the ability of the continuous path in Rn, and Wx denotes Viennese measure of all such paths starting at X0 = x. If V: Rnamic \u2192 R is limited at the bottom, then it is a solution to the Cauchy initial value problem: (2)."}, {"heading": "E[ Mt | Fs ] = Ms for all 0 \u2264 s \u2264 t,", "text": "In fact, it is not the case that it is a \"normal\" process in which it is about a \"marginalized system,\" but rather a \"marginalized system,\" in which it is about \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system,\" a system, \"a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system,"}, {"heading": "3. Prototype Coding.", "text": "In discussing the mathematical background of the work in the previous section, we actually implicitly developed the main elements of our geometric and probabilistic system. The potential function, U (x), and its gradient, are presented in conjunction with equations (6) and (7) and theorem 1 (6). Equation (6) is a diffusion equation with a drift term, and it has an invariant probability density equal to eU (x), modulo a normalization factor. The stochastic process described by equation (6) can also be written as an Ito process using equations (9) and (10) and theorem 2, or it can be written in Stratonovich form, using equation (13) and lemma 2. An alternative view of the equation (6) is given by Stroock's result, theorem 4."}, {"heading": "6. Diffusion Coefficients and Dissimilarity Metrics", "text": "Recall the main results from Section 3: We started with a diffusion process represented by an itostochastic differential equation method (in Cartesian coordinates; we converted this into a Stratonovich equation in coordinates); and we turned it back into an itoprocess characterized by a differential operator with coefficients (in a simple Gaussian case) and \u03b2i (in another case); the only necessary ingredient was the jacobic matrix of coordinate transformation. As an illustration, we try a brutal solution of these equations in the simple Gaussian case discussed in Section 5.1. Jacobian is given by an equation (32). Equation (23), rewritten for the lightness of reference, is for the three-dimensional coordinate system (in another case, in another case): dX (in another case)."}, {"heading": "7. Future Work", "text": "The theory of differential similarity combines a stochastic model with a geometric model, and it works because there is a common mathematical object in both models: the gradients, the gradients, the gradients that represent a potential function, U (x). In the stochastic model, it is the drift vector that guarantees the existence of an invariant probability measure (x). In the geometric model, it is the existence of an orthogonal integral multiplicity. We have seen in section 6 that there is a theoretical connection between these two models in which the two models play a decisive role, and we have the practical consequences of this connection in the computational examples in sections 5.1 and 5.2.The main deficiency in the theory as presented in this paper is the limitation of the geometric model to the three-dimensional case. We have imposed this limitation in order to simplify the calculations and make it easy to illustrate the examples in the theory."}], "references": [{"title": "Introduction to Differentiable Manifolds", "author": ["Louis Auslander", "Robert E. MacKenzie"], "venue": "Dover Publications,", "citeRegEx": "AM77", "shortCiteRegEx": null, "year": 1977}, {"title": "Pure and applied mathematics", "author": ["Richard L. Bishop", "Richard J. Crittenden. Geometry of Manifolds"], "venue": "Academic Press,", "citeRegEx": "BC64", "shortCiteRegEx": null, "year": 1964}, {"title": "Tensor Analysis on Manifolds", "author": ["Richard L. Bishop", "Samuel I. Goldberg"], "venue": "Macmillan,", "citeRegEx": "BG68", "shortCiteRegEx": null, "year": 1968}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, volume 19, pages 153\u2013160", "citeRegEx": "BLPL06", "shortCiteRegEx": null, "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, 15(6):1373\u20131396", "citeRegEx": "BN03", "shortCiteRegEx": null, "year": 2003}, {"title": "Dover Books on Mathematics Series", "author": ["Henri Cartan. Differential Forms"], "venue": "Dover Publications,", "citeRegEx": "Car71", "shortCiteRegEx": null, "year": 1971}, {"title": "Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds", "author": ["M. Chen", "J. Silva", "J.W. Paisley", "C. Wang", "D.B. Dunson", "L. Carin"], "venue": "IEEE Transactions on Signal Processing, 58(12):6140\u20136155", "citeRegEx": "CSP10", "shortCiteRegEx": null, "year": 2010}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data", "author": ["D. Donoho", "C. Grimes"], "venue": "Proceedings of National Academy of Sciences, 100:5591\u20135596", "citeRegEx": "DG03", "shortCiteRegEx": null, "year": 2003}, {"title": "John Willey & Sons", "author": ["Richard O. Duda", "Peter E. Hart. Pattern Classification", "Scene Analysis"], "venue": "New York,", "citeRegEx": "DH73", "shortCiteRegEx": null, "year": 1973}, {"title": "chapter 10: Unsupervised Learning and Clustering", "author": ["Richard O. Duda", "Peter E. Hart", "David G. Stork. Pattern Classification"], "venue": "Wiley & Sons, Inc., New York, 2nd edition,", "citeRegEx": "DHS01", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic Calculus in Manifolds", "author": ["Michel Emery", "Paul A. Meyer"], "venue": "World Publishing Company,", "citeRegEx": "EM89", "shortCiteRegEx": null, "year": 1989}, {"title": "Space-time approach to non-relativistic quantum mechanics", "author": ["R.P. Feynman"], "venue": "Rev. Mod. Phys., 20:367\u2013387", "citeRegEx": "Fey48", "shortCiteRegEx": null, "year": 1948}, {"title": "Hypoelliptic second order differential equations", "author": ["L. H\u00f6rmander"], "venue": "Acta Mathematica, 119:147\u2013171", "citeRegEx": "H\u00f6r67", "shortCiteRegEx": null, "year": 1967}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554", "citeRegEx": "HOT06", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "Advances in Neural Information Processing Systems, volume 15, pages 833\u2013 840", "citeRegEx": "HR02", "shortCiteRegEx": null, "year": 2002}, {"title": "Contemporary Mathematics", "author": ["Elton P. Hsu. Stochastic Analysis on Manifolds"], "venue": "American Mathematical Society,", "citeRegEx": "Hsu02", "shortCiteRegEx": null, "year": 2002}, {"title": "Stochastic differentials", "author": ["K. It\u00f4"], "venue": "Applied Mathematics & Optimization, 1(4):374\u2013381", "citeRegEx": "It\u00f475", "shortCiteRegEx": null, "year": 1975}, {"title": "On distributions of certain Wiener functionals", "author": ["M. Kac"], "venue": "Trans. Amer. Math. Soc., 65:1\u201313", "citeRegEx": "Kac49", "shortCiteRegEx": null, "year": 1949}, {"title": "Pure and Applied Mathematics", "author": ["David Lovelock", "Hanno Rund. Tensors", "Differential Forms", "Variational Principles"], "venue": "Wiley,", "citeRegEx": "LR75", "shortCiteRegEx": null, "year": 1975}, {"title": "Spectral connectivity analysis", "author": ["A.B. Lee", "L. Wasserman"], "venue": "Journal of the American Statistical Association, 105(491):1241\u20131255", "citeRegEx": "LW10", "shortCiteRegEx": null, "year": 2010}, {"title": "Path integral methods for parabolic partial differential equations with examples from computational finance", "author": ["A. Lyasoff"], "venue": "Mathematica Journal, 9(2):399\u2013422", "citeRegEx": "Lya04", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic Differential Equations: An Introduction With Applications", "author": ["Bernt K. \u00d8ksendal"], "venue": "Springer, sixth edition,", "citeRegEx": "\u00d8ks03", "shortCiteRegEx": null, "year": 2003}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems, volume 24, pages 2294\u20132302", "citeRegEx": "RDV11", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, volume 19, pages 1137\u20131144", "citeRegEx": "RPCL06", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "RS00", "shortCiteRegEx": null, "year": 2000}, {"title": "volume 1", "author": ["Michael Spivak. A Comprehensive Introduction to Differential Geometry"], "venue": "Publish or Perish, third edition,", "citeRegEx": "Spi99", "shortCiteRegEx": null, "year": 1999}, {"title": "Diffusions as integral curves", "author": ["D.W. Stroock", "S. Taniguchi"], "venue": "or Stratonovich without It\u00f4. In The Dynkin Festschrift. Markov processes and their applications. In celebration of Eugene B. Dynkin\u2019s 70th birthday, pages 333\u2013369. Boston, MA: Birkh\u00e4user", "citeRegEx": "ST94", "shortCiteRegEx": null, "year": 1994}, {"title": "Diffusions as integral curves on manifolds and Lie groups", "author": ["D.W. Stroock", "S. Taniguchi"], "venue": "Probability theory and mathematical statistics. Lectures presented at the semester held in St. Petersburg, Russia, March 2\u2013April 23, 1993, pages 219\u2013226. Amsterdam: Gordon and Breach Publishers", "citeRegEx": "ST96", "shortCiteRegEx": null, "year": 1996}, {"title": "A new representation for stochastic integrals and equations", "author": ["R.L. Stratonovich"], "venue": "SIAM Journal on Control, 4(2):362\u2013371", "citeRegEx": "Str66", "shortCiteRegEx": null, "year": 1966}, {"title": "On the growth of stochastic integrals", "author": ["D.W. Stroock"], "venue": "Z. Wahrscheinlichkeitstheor. Verw. Geb., 18:340\u2013344", "citeRegEx": "Str71", "shortCiteRegEx": null, "year": 1971}, {"title": "Probability Theory: An Analytic View", "author": ["Daniel W. Stroock"], "venue": "Cambridge University Press,", "citeRegEx": "Str93", "shortCiteRegEx": null, "year": 1993}, {"title": "Gaussian measures in traditional and not so traditional settings", "author": ["D.W. Stroock"], "venue": "Bulletin (New Series) of the American Mathematical Society, 33(2):135\u2013155", "citeRegEx": "Str96", "shortCiteRegEx": null, "year": 1996}, {"title": "Mathematical Surveys and Monographs", "author": ["Daniel W. Stroock. An Introduction to the Analysis of Paths on a Riemannian Manifold"], "venue": "American Mathematical Society,", "citeRegEx": "Str00", "shortCiteRegEx": null, "year": 2000}, {"title": "It\u00f4\u2019s Perspective", "author": ["K Daniel W. Stroock. Markov Processes from"], "venue": "Annals of Mathematics Studies. Princeton University Press,", "citeRegEx": "Str03", "shortCiteRegEx": null, "year": 2003}, {"title": "Probability Theory: An Analytic View", "author": ["Daniel W. Stroock"], "venue": "Cambridge University Press, second edition,", "citeRegEx": "Str11", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Neural Computation, 11(2):443\u2013482", "citeRegEx": "TB99", "shortCiteRegEx": null, "year": 1999}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["J.B. Tenenbaum", "V. Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "TSL00", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 8, "context": "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].", "startOffset": 60, "endOffset": 66}, {"referenceID": 9, "context": "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].", "startOffset": 137, "endOffset": 144}, {"referenceID": 36, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 131, "endOffset": 138}, {"referenceID": 24, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 146, "endOffset": 152}, {"referenceID": 22, "context": "[RDV11], outline three hypotheses that motivate much of this work:", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "Let\u2019s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].", "startOffset": 91, "endOffset": 98}, {"referenceID": 17, "context": "Let\u2019s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].", "startOffset": 99, "endOffset": 106}, {"referenceID": 30, "context": "See [Str93], Section 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 30, "context": "See [Str93], Section 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 34, "context": "2, or [Str11], Section 8.", "startOffset": 6, "endOffset": 13}, {"referenceID": 31, "context": "haunts every attempt to deal with Brownian paths,\u201d [Str96], p.", "startOffset": 51, "endOffset": 58}, {"referenceID": 10, "context": "See [EM89] or [Hsu02].", "startOffset": 4, "endOffset": 10}, {"referenceID": 15, "context": "See [EM89] or [Hsu02].", "startOffset": 14, "endOffset": 21}, {"referenceID": 30, "context": "36 in [Str93] or Theorem 10.", "startOffset": 6, "endOffset": 13}, {"referenceID": 34, "context": "33 in [Str11].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "Sources: These results appear in [Str93], Section 4.", "startOffset": 33, "endOffset": 40}, {"referenceID": 34, "context": "See [Str11], Section 10.", "startOffset": 4, "endOffset": 11}, {"referenceID": 21, "context": "16 of his text [\u00d8ks03].", "startOffset": 15, "endOffset": 22}, {"referenceID": 21, "context": "3 in [\u00d8ks03].", "startOffset": 5, "endOffset": 12}, {"referenceID": 21, "context": "See [\u00d8ks03], Chapter 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 28, "context": "See [Str66] or [It\u00f475].", "startOffset": 4, "endOffset": 11}, {"referenceID": 16, "context": "See [Str66] or [It\u00f475].", "startOffset": 15, "endOffset": 22}, {"referenceID": 12, "context": "See [H\u00f6r67].", "startOffset": 4, "endOffset": 11}, {"referenceID": 32, "context": "For these reasons, Stroock relies on the H\u00f6rmander formalism extensively in his book on the analysis of Brownian paths on Riemannian manifolds [Str00].", "startOffset": 143, "endOffset": 150}, {"referenceID": 21, "context": "Sources: For the basic results on stochastic differential equations, using It\u00f4\u2019s formalism, the reader should consult [\u00d8ks03], but \u00d8ksendal\u2019s text provides only a cursory treatment of Stratonovich\u2019s formalism.", "startOffset": 118, "endOffset": 125}, {"referenceID": 28, "context": "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by It\u00f4 [It\u00f475].", "startOffset": 35, "endOffset": 42}, {"referenceID": 16, "context": "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by It\u00f4 [It\u00f475].", "startOffset": 153, "endOffset": 160}, {"referenceID": 33, "context": "Chapter 8 of [Str03] is an excellent contemporary account of Stratonovich\u2019s theory, set in a broader context.", "startOffset": 13, "endOffset": 20}, {"referenceID": 31, "context": "There remains the problem that \u201chaunts every attempt to deal with Brownian paths,\u201d [Str96], p.", "startOffset": 83, "endOffset": 90}, {"referenceID": 30, "context": "23 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "32 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "10 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 26, "context": "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in H\u00f6rmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].", "startOffset": 137, "endOffset": 143}, {"referenceID": 27, "context": "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in H\u00f6rmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].", "startOffset": 236, "endOffset": 242}, {"referenceID": 32, "context": "The theory is explicated further in [Str00], where it serves as the foundation for Stroock\u2019s construction and analysis of Brownian motion on a Riemannian manifold.", "startOffset": 36, "endOffset": 43}, {"referenceID": 32, "context": "1 of [Str00] includes a generalization of Lemma 4 above, and Theorem 2.", "startOffset": 5, "endOffset": 12}, {"referenceID": 32, "context": "40 of [Str00] is a generalization of Theorem 4.", "startOffset": 6, "endOffset": 13}, {"referenceID": 25, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "See [BC64], Problem 29, p.", "startOffset": 4, "endOffset": 10}, {"referenceID": 5, "context": "23; [Car71], pp.", "startOffset": 4, "endOffset": 11}, {"referenceID": 18, "context": "97\u201398; [LR75], pp.", "startOffset": 7, "endOffset": 13}, {"referenceID": 2, "context": ", [BG68], Theorem 3.", "startOffset": 2, "endOffset": 8}, {"referenceID": 25, "context": "1, and [Spi99], Lemma 5.", "startOffset": 7, "endOffset": 14}, {"referenceID": 2, "context": ", [BG68], Theorem 3.", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "1, or [BC64], Theorem 1.", "startOffset": 6, "endOffset": 12}, {"referenceID": 20, "context": "See, for example, [Lya04].", "startOffset": 18, "endOffset": 25}, {"referenceID": 29, "context": "The earliest example is in [Str71] and [It\u00f475].", "startOffset": 27, "endOffset": 34}, {"referenceID": 16, "context": "The earliest example is in [Str71] and [It\u00f475].", "startOffset": 39, "endOffset": 46}, {"referenceID": 32, "context": "1 of [Str00] or Theorem 3.", "startOffset": 5, "endOffset": 12}, {"referenceID": 15, "context": "4 in [Hsu02].", "startOffset": 5, "endOffset": 12}, {"referenceID": 32, "context": "37 in [Str00].", "startOffset": 6, "endOffset": 13}, {"referenceID": 29, "context": "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and we will start with a construction borrowed from [Str71] and [It\u00f475], but", "startOffset": 187, "endOffset": 194}, {"referenceID": 16, "context": "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and we will start with a construction borrowed from [Str71] and [It\u00f475], but", "startOffset": 199, "endOffset": 206}, {"referenceID": 36, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 44, "endOffset": 51}, {"referenceID": 24, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 59, "endOffset": 65}, {"referenceID": 7, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "Belkin and Niyogi [BN03], for example, work with the eigenvectors of the graph Laplacian and the eigenfunctions of the LaplaceBeltrami operator, and show that the solution to these eigenproblems yields an \u201coptimal\u201d embedding of a low-dimensional manifold into a higher-dimensional space, but their arguments are geometric rather than probabilistic.", "startOffset": 18, "endOffset": 24}, {"referenceID": 14, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 48, "endOffset": 54}, {"referenceID": 35, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 55, "endOffset": 61}, {"referenceID": 6, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 62, "endOffset": 69}, {"referenceID": 35, "context": "Tipping and Bishop [TB99] work with a mixture of low-dimensional Gaussians embedded in a higher-dimensional space, each with its own mean and covariance matrix, and they use the EM algorithm to estimate the parameters of this model.", "startOffset": 19, "endOffset": 25}, {"referenceID": 6, "context": ", [CSP10] adopt a similar model, along with the assumption that the Gaussian mixture covers a low-dimensional manifold, and they estimate both the number of components in the mixture and the dimensionality of the subspaces, using Bayesian techniques.", "startOffset": 2, "endOffset": 9}, {"referenceID": 19, "context": "One exception to this dichotomy between geometric and probabilistic approaches is a paper by Lee and Wasserman [LW10], which has some interesting connections to the present work.", "startOffset": 111, "endOffset": 117}, {"referenceID": 4, "context": "Lee and Wasserman are primarily interested in the eigenfunctions of A ,m and At, which have applications to various spectral clustering problems, following the work of Belkin and Niyogi [BN03] and others.", "startOffset": 186, "endOffset": 192}, {"referenceID": 19, "context": "The other important contribution of Lee and Wasserman [LW10] is their analysis of the statistical estimators for the population quantities, At and D 2 t .", "startOffset": 54, "endOffset": 60}, {"referenceID": 3, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 99, "endOffset": 107}, {"referenceID": 13, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 108, "endOffset": 115}, {"referenceID": 23, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 116, "endOffset": 124}, {"referenceID": 22, "context": "[RDV11], cited and quoted in Section 1, is an example of just such an approach.", "startOffset": 0, "endOffset": 7}], "year": 2014, "abstractText": "This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, gij(x), which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, U(x), and its gradient, \u2207U(x). We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.", "creator": "LaTeX with hyperref package"}}}