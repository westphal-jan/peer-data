{"id": "1401.2482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "STIMONT: A core ontology for multimedia stimuli description", "abstract": "Affective multimedia documents such as images, sounds or videos elicit emotional responses in exposed human subjects. These stimuli are stored in affective multimedia databases and successfully used for a wide variety of research in psychology and neuroscience in areas related to attention and emotion processing. Although important all affective multimedia databases have numerous deficiencies which impair their applicability. These problems, which are brought forward in the paper, result in low recall and precision of multimedia stimuli retrieval which makes creating emotion elicitation procedures difficult and labor-intensive. To address these issues a new core ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and extends W3C EmotionML format with an expressive and formal representation of affective concepts, high-level semantics, stimuli document metadata and the elicited physiology. The advantages of ontology in description of affective multimedia stimuli are demonstrated in a document retrieval experiment and compared against contemporary keyword-based querying methods. Also, a software tool Intelligent Stimulus Generator for retrieval of affective multimedia and construction of stimuli sequences is presented.", "histories": [["v1", "Fri, 10 Jan 2014 23:36:51 GMT  (837kb)", "http://arxiv.org/abs/1401.2482v1", "27 pages, 13 figures"]], "COMMENTS": "27 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.MM cs.AI", "authors": ["marko horvat", "nikola bogunovi\\'c", "kre\\v{s}imir \\'cosi\\'c"], "accepted": false, "id": "1401.2482"}, "pdf": {"name": "1401.2482.pdf", "metadata": {"source": "CRF", "title": "STIMONT: A core ontology for multimedia stimuli description", "authors": ["Marko Horvat", "Nikola Bogunovi\u0107", "Kre\u0161imir \u0106osi\u0107"], "emails": ["Marko.Horvat2@fer.hr,"], "sections": [{"heading": null, "text": "These stimuli are stored in affective multimedia databases and are successfully used for a variety of research in psychology and neuroscience in the fields of attention and emotion processing. Although all affective multimedia databases have numerous deficiencies that impair their applicability, the problems presented in the paper lead to low memory and precision in the restoration of multimedia stimuli, making it difficult and labor-intensive to create methods of emotion elicitation. In order to solve these problems, a new STIMONT core ontology is introduced. Written in OWL-DL formalism, STIMONT expands the W3C EmotionML format to include an expressive and formal representation of affective concepts, high-grade semantics, stimuli document metadata, and the physiology induced. The benefits of ontology in the description of affective multimedia stimuli are demonstrated in a documentary sequence experiment, using contemporary keyword-based interrogations."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Related work", "text": "This section gives a brief introduction to affective multimedia databases, multimedia emotion annotation models, and ontology-based methods for displaying high-level metadata and retrieving documents. The development of an ontology tailored to the description of stimuli is directly motivated by the need to improve retrieval and retrieval from affective multimedia databases."}, {"heading": "2.1 Affective multimedia databases", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2.2 Ontology-based representation of high-level stimuli content", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3 The STIMONT model", "text": "STIMONT is an upper-core ontology designed to provide an integrated and formal description of emotions, semantics, context and physiological content of a multimedia stimulus. STIMONT's main feature is that it provides a formal framework to support an explicit, human and machine-processed definition of affective multimedia content, which also facilitates the storage of stimuli in emotionally annotated databases, the retrieval and restoration of stimuli, and the construction of stimulus sequences. STIMONT enables a common understanding of the perceived importance of multimedia stimuli, their affective dimensions and contexts. Using a suitable interference machine, this knowledge enables new facts to be derived about stimuli indexed in affective multimedia databases."}, {"heading": "3.1 STIMONT concepts", "text": "This year, it has come to the point that it has never been as far as this year."}, {"heading": "3.2 Modeling stimuli emotion", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live."}, {"heading": "3.3 Representation of stimuli semantics", "text": "The question is whether it is, whether it is, whether it is, whether it is, whether it is, whether it is, whether it is, whether it is, whether it is, whether it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is, who it is who it is, who it is who it is who it is who it is, who it is who it is who it is who is who is who it is, who is who is who it is who is who is who is who it is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who, who is who is who is who is who is who is who is who is who is who, who is who is who is who is who is who is who is who is who is who is who is who is who is who is who, who is who is who is who is who is who is who is who is who is who is who is who, who is who is who is who is who is who is who is who is who is who is who is who is who, who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is who is"}, {"heading": "3.4 Stimuli context and elicited physiology", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the if\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "4 Ontology implementation and validation", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "4.1 Multimedia stimuli retrieval", "text": "In fact, most people are able to recognize themselves and understand what they are doing."}, {"heading": "5 Discussion and future work", "text": "In practice, the STIMMONT is primarily designed for the representation of emotions. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <, \"< < < < < <,\" < < < < < < < < < < < < < <, \"< < < < < < <,\" < < < <, \"< < < < < <,\" < < < < <, \"< < < <,\" <, \"< < <,\" < < <"}, {"heading": "6 Conclusion", "text": "The proposed ontology offers a number of advantages over the existing methods of presenting knowledge in emotionally commented multimedia. In the evaluation experiment, the ontology-based approach achieved up to 24.9369% higher accuracy in ranking IAPS images. Compared to the existing method of searching with keywords, this represents an increase in all performance and correctness measurements. Although ontologies require more complex prerequisites and argumentation infrastructure than markup or keyword-based annotations, they are much more advantageous than the simpler methods and provide a better multimedia query. STIMONT provides a practical model for the formal representation of semantics, emotions and related states, documenting context and stimulated physiology that collectively define a multimedia stimulus. The ontology presented builds on the W3C EmotionML format and expands it with additional emotional vocabularies. Ontology uses the human MO and MO Ontology to form a basis."}], "references": [{"title": "The Handbook of Emotion Elicitation and Assessment", "author": ["J.A. Coan", "Allen", "J.J.B. (eds."], "venue": "Oxford University Press Series in Affective Science. Oxford University Press, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization", "author": ["D. Grandjean", "D. Sander", "K.R. Scherer"], "venue": "Consciousness and Cognition, Vol. 17.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Tagging multimedia stimuli with ontologies", "author": ["M. Horvat", "S. Popovi\u0107", "N. Bogunovi\u0107", "K. \u0106osi\u0107"], "venue": "Proceedings of the 32nd International Convention MIPRO 2009, Croatian Society for Information and Communication Technology, Electronics and Microelectronics \u2013 MIPRO, Opatija, Croatia", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimedia stimuli databases usage patterns: a survey report", "author": ["M. Horvat", "S. Popovi\u0107", "K. \u0106osi\u0107"], "venue": "Proceedings of the 36th International Convention MIPRO 2013, Croatian Society for  25  Information and Communication Technology, Electronics and Microelectronics \u2013 MIPRO, Opatija, Croatia", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": "Technical Report A-8. University of Florida, Gainesville, FL", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "The International Affective Digitized Sounds (2nd Edition; IADS-2): affective ratings of sounds and instruction manual", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Technical report B-3. University of Florida, Gainesville, FL", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Measuring emotion: Behavior, feeling and physiology", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Lane, R., Nadel, L. (eds.): Cognitive neuroscience of emotion. Oxford University Press, New York", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "The Geneva Affective PicturE Database (GAPED): A new 730 picture database focusing on valence and normative significance", "author": ["E.S. Dan-Glauser", "K.R. Scherer"], "venue": "Behavior Research Methods, Vol. 43(2).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The Nencki Affective Picture System (NAPS)", "author": ["A. Marchewka", "\u0141. \u017burawski", "K. Jednor\u00f3g", "A. Grabowska"], "venue": "Introduction to a novel standardized wide range high quality realistic pictures database", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "The NimStim set of facial expressions: Judgments from untrained research participants", "author": ["N. Tottenham", "J.W. Tanaka", "T A.C. Leon", "McCarry", "M. Nurse", "T.A. Hare", "D.J. Marcus", "A. Westerlund", "B.J. Casey", "C. Nelson"], "venue": "Psychiatry Research, Vol. 168(3).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Technical report C-1. Gainesville, FL. The Center for Research in Psychophysiology, University of Florida", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Affective Norms for English Text (ANET): Affective ratings of text and instruction manual", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "(Tech. Rep. No. D-1). University of Florida, Gainesville, FL", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "Proceedings of LREC-10, 7th Conference on Language Resources and Evaluation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A survey of affect recognition methods: audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31(1).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Face Databases", "author": ["R. Gross"], "venue": "Li, S., Jain, A. (eds): Handbook of Face Recognition. Springer-Verlag, The Robotics Inistitute, Carnegie Mellon University Forbes Avenue, Pittsburgh", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "A circumplex model of affect", "author": ["J.A. Russell"], "venue": "Journal of Personality and Social Psychology, Vol. 39.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament", "author": ["A. Mehrabian"], "venue": "Current Psychology, Vol. 14(4).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Emotion representation and physiology assignments in digital systems", "author": ["C Peter", "A. Herbon"], "venue": "Interacting with Computers, Vol. 18.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Measuring emotion: The Self-Assessment Manikin and the semantic differential", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Journal of Behavior Therapy & Experimental Psychiatry, Vol. 25.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Are there basic emotions? Psychological Review, Vol", "author": ["P. Ekman"], "venue": "99.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Multidimensional normative ratings for the international affective picture system, Behavior research", "author": ["T.M. Libkuman", "H. Otani", "R. Kern", "S.G. Viger", "N. Novak"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Affective auditory stimuli: Characterization of the International Affective Digitized Sounds (IADS) by discrete emotional categories", "author": ["R.A. Stevenson", "T.W. James"], "venue": "Behavior Research Methods. Vol. 40(1)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Characterization of affective norms for English words by discrete emotional categories", "author": ["R.A. Stevenson", "J.A. Mikels", "T.W. James"], "venue": "Behavior Research Methods, Vol. 39.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Handbook on ontologies", "author": ["S. Staab", "Studer", "R. (Eds.)."], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Ontology-based reasoning techniques for multimedia interpretation and retrieval", "author": ["B. Neumann", "R. M\u04e7ller"], "venue": "Semantic Multimedia and Ontologies: Theory and Applications, Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "EmotionML \u2013 An Upcoming Standard for Representing Emotions and Related States", "author": ["M. Schr\u00f6der", "P. Baggia", "F. Burkhardt", "C. Pelachaud", "C. Peter", "E. Zovato"], "venue": "Affective Computing and Intelligent Interaction. Lecture Notes in Computer Science Volume 6974.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "The MPEG-7 visual standard for content description-an overview", "author": ["T. Sikora"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 11(6), 696\u2013702.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion Description with MPEG-7", "author": ["H. Agius"], "venue": "Affective Computing and Intelligent Interaction. Emotion in HCI\u2013Designing for People, 13.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "The landscape of multimedia ontologies in the last decade", "author": ["M.C. Su\u00e1rez-Figueroa", "G.A. Atemezing", "O. Corcho."], "venue": "Multimedia Tools and Applications, Vol. 55(3).", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards an ontology for describing emotions", "author": ["J.M. L\u00f3pez", "R. Gil", "R. Garc\u00eda", "I. Cearreta", "N. Garay"], "venue": "Lecture Notes in Computer Science, 5288, 96\u2013104.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Basic Description Logics (Description Logic Handbook)", "author": ["F. Baader", "W. Nutt"], "venue": "Edited by F. Baader, D. Calvanese, D.L. McGuinness, D. Nardi, P.F. Patel-Schneider, Cambridge University Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Tool Support for Ontology Engineering", "author": ["I. Horrocks"], "venue": "Fensel, D. (ed.): Foundations for the Web of Information and Services, Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "The Suggested Upper Merged Ontology: A Large Ontology for the Semantic Web and its Applications", "author": ["A. Pease", "I. Niles", "J. Li"], "venue": "Working Notes of the AAAI-2002 Workshop on Ontologies and the Semantic Web", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Linking Lexicons and Ontologies: Mapping WordNet to the Suggested Upper Merged Ontology", "author": ["I. Niles", "A. Pease"], "venue": "Proceedings of the IEEE International Conference on Information and Knowledge Engineering", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}, {"title": "SPARQL Query Language for RDF", "author": ["E. Prud\u2019hommeaux", "A. Seaborne"], "venue": "W3C Candidate Recommendation,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Writing Rules for the Semantic Web Using SWRL and Jess", "author": ["M. O\u2019Connor"], "venue": "8th International Protege Conference, Prot\u00e9g\u00e9 with Rules Workshop. Madrid", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Using Dublin core", "author": ["D. Hillmann"], "venue": "Recommendation. Dublin Core Metadata Initiative", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Toward Recognizing Individual's Subjective Emotion from Physiological Signals in Practical Application", "author": ["O. Villon", "S. Antipolis", "C. Lisetti"], "venue": "Twentieth IEEE International Symposium on Computer-Based Medical Systems.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies", "author": ["M. Horvat", "A. Grbin", "G. Gledec"], "venue": "Frontiers in artificial intelligence and applications, Vol. 243.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Information retrieval by semantic similarity", "author": ["A. Hliaoutakis", "G. Varelas", "E. Voutsakis", "E.G. Petrakis", "E. Milios"], "venue": "International Journal on Semantic Web and Information Systems (IJSWIS), 2(3).", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "STIMONT: A core ontology for multimedia stimuli description \u2013 Supplementary Online Data", "author": ["M. Horvat"], "venue": "http://goo.gl/PocuK.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Linking Lexicons and Ontologies: Mapping WordNet to the Suggested Upper Merged Ontology", "author": ["I. Niles", "A. Pease"], "venue": "Proceedings of the IEEE International Conference on In-formation and Knowledge Engineering", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}, {"title": "WordNet-Affect: an Affective Extension of WordNet", "author": ["C. Strapparava", "A. Valitutti"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "Sweetening Ontologies with DOLCE", "author": ["A. Gangemi", "N. Guarino", "C. Masolo", "A. Oltramari", "L. Schneider"], "venue": "Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web, Lecture Notes in Computer Science, Vol. 2473. Springer Berlin Heidelberg", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "ConceptNet: A practical commonsense reasoning toolkit", "author": ["H. Liu", "Singh. P."], "venue": "BT Technology Journal, Vol. 22.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Largescale concept ontology for multimedia", "author": ["M. Naphade", "J.R. Smith", "J. Tesic", "S.F. Chang", "W. Hsu", "A. Hauptmann", "J. Curtis"], "venue": "IEEE Multimedia, Vol. 13(3).", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "COMM: Designing a WellFounded Multimedia Ontology for the Web", "author": ["R. Arndt", "R. Troncy", "S. Staab", "L. Hardman", "M. Vacuram"], "venue": "The Semantic Web, Lecture Notes in Computer Science, Vol. 4825. Springer Berlin Heidelberg", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Can High-Level Concepts Fill the Semantic Gap in Video Retrieval? A Case Study With Broadcast News", "author": ["A. Hauptmann", "R. Yan", "Lin", "W.-H.", "M. Christel", "H. Wactlar"], "venue": "IEEE Transactions on Multimedia, Vol. 9(5).", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": "MIT Press", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Affective Image Classification using Features Inspired by Psychology and Art Theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM Multimedia 2010 - Multimedia Content Track Full Paper, Florence, Italy", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Stress inoculation training supported by physiology-driven adaptive virtual reality stimulation", "author": ["S. Popovi\u0107", "M. Horvat", "D. Kukolja", "B. Dropulji\u0107", "K. \u0106osi\u0107"], "venue": "Studies in Health Technology and Informatics, Vol. 144.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion elicitation using films", "author": ["Rottenberg", "R.D.J. Ray", "J.J. Gross"], "venue": "The Handbook of Emotion Elicitation and Assessment, Oxford University Press, USA", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "By being exposed to multimedia stimuli individuals\u2019 emotional states may be modulated [1, 2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 1, "context": "By being exposed to multimedia stimuli individuals\u2019 emotional states may be modulated [1, 2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 2, "context": "This paper addresses multiple drawbacks of contemporary affective multimedia databases [3] and proposes an ontology-based approach for formal description of stimuli metadata which aims to optimize both the annotation and retrieval processes from these databases.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Motivation for this work was supported by an online survey on usage patterns of multimedia stimuli databases [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "The International Affective Picture System (IAPS) [5] and the International Affective Digital Sounds system (IADS) [6] are two of the most cited databases in the area of affective stimulation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "The International Affective Picture System (IAPS) [5] and the International Affective Digital Sounds system (IADS) [6] are two of the most cited databases in the area of affective stimulation.", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "They were created with three goals in mind [7]: 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 198, "endOffset": 201}, {"referenceID": 9, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 314, "endOffset": 318}, {"referenceID": 10, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 400, "endOffset": 404}, {"referenceID": 11, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 447, "endOffset": 451}, {"referenceID": 12, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 469, "endOffset": 473}, {"referenceID": 13, "context": "Additional audio-visual affective multimedia databases with category or dimensional emotion annotations are listed here [16].", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "A more detailed overview of these databases is given in [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Two predominant theories used to describe emotion are the discrete category model and the dimensional model of affect (also sometimes called Circumplex model of affect [18] or PAD [19]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Two predominant theories used to describe emotion are the discrete category model and the dimensional model of affect (also sometimes called Circumplex model of affect [18] or PAD [19]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "All affective multimedia databases have been characterized according to at least one of these models [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Dimensions are chosen on their ability to statistically characterize subjective emotional ratings with the least number of dimensions possible [21].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "Instead, supporters of these theories propose that there are a number of emotions that are universal across cultures and have an evolutionary and biological basis [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 22, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "By definition ontologies are a representation of a shared understanding about a specific domain and enable the derivation of implicit knowledge from the existing explicit knowledge and automated inference with reasoning engines [26].", "startOffset": 228, "endOffset": 232}, {"referenceID": 24, "context": "Ontologies have been successfully applied for description of high-level image content, concept semantics, object labels and relationships defined in the upper levels of the image representation hierarchy [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 25, "context": "Except the W3C EmotionML format [28] much work has been done to enable describing information about emotions in multimedia, especially in the video.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "MPEG-7 multimedia standard, which is based on XML and can be expanded with additional tools, provides a method for describing emotions with its Affective Description Scheme [29].", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "Researchers have proposed new description tools that rely on MPEG-7 to provide a broader description of affective terms that can be used in video annotation [30].", "startOffset": 157, "endOffset": 161}, {"referenceID": 28, "context": "Also, several multimedia ontologies which are potentially applicable in emotion description have been proposed in the last decade [31] including ontologies specially designed for high-level description of cognitive-emotional related concepts [32].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Also, several multimedia ontologies which are potentially applicable in emotion description have been proposed in the last decade [31] including ontologies specially designed for high-level description of cognitive-emotional related concepts [32].", "startOffset": 242, "endOffset": 246}, {"referenceID": 30, "context": "These two types of knowledge are the basic components of a knowledge-based system based on Description Logics (DLs) [33] as a set of structured knowledge-representation formalisms with decidable-reasoning algorithms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 31, "context": "A variety of tools for knowledge engineering exist [34] which allow construction, management, reuse and reasoning with OWLbased ontologies.", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "The STIMONT is currently using Suggested Merged Upper Ontology (SUMO) [35] in OWL DL format as foundation ontology for formal representation of stimuli high-level content.", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "Each of first five classes formally represents one of the category vocabularies in EmotionML [28]: emotions that frequently occur in everyday life, six primary emotions universal in all cultures, categories related to Frijda\u2019s proposal of action tendencies, categories compromising Ortony, Clore and Collins appraisal model, and finally categories used in a study by Fontaine, Scherer, Roesch and Ellsworth, respectively.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The second implemented construct for expression of confidence with a real number in the closed interval [0, 1].", "startOffset": 104, "endOffset": 110}, {"referenceID": 33, "context": "Available SUMO to WordNet mappings help to express concepts in natural language terms [36] which facilitates extension of the framework towards existing tools for informal representation of multimedia (particularly images) with semantic networks and lexical ontologies.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Retrieving multimedia assets in the proposed architecture can be achieved by using semantic query languages such as the SPARQL query language [37].", "startOffset": 142, "endOffset": 146}, {"referenceID": 35, "context": "The query may be executed in the Prot\u00e9g\u00e9 ontology editor extended with the Jess rule engine [38].", "startOffset": 92, "endOffset": 96}, {"referenceID": 36, "context": "Simple level of Dublin Core metadata element set [39] is a standard for crossdomain information resource description and can ensure simple and standardized set of conventions for description of these additional context data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 37, "context": "Since affective multimedia stimulates generation of physiology in exposed subjects and physiology has been shown to be an important and objective channel for automated estimation of emotion and related states [40], it is useful to connect physiology data with Stimulus class.", "startOffset": 209, "endOffset": 213}, {"referenceID": 38, "context": "It follows previous work in developing an online tool WNtags for collaborative annotation and retrieval of IAPS images using WordNet [41].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "The implemented measures are: Levenshtein, path length, Wu Palmer, Leacock Chodorow and Li algorithm [42].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "owl\u201d and is available for research purposes together with stimuli metadata used in the experiment and installation of the intStimGen tool [43].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Furthermore, since emotions in affective multimedia databases are described as vectors in Euclidian space [18, 19] with well known distribution while high-level multimedia meaning is more uncertain, semantics is a more important parameter in evaluation.", "startOffset": 106, "endOffset": 114}, {"referenceID": 16, "context": "Furthermore, since emotions in affective multimedia databases are described as vectors in Euclidian space [18, 19] with well known distribution while high-level multimedia meaning is more uncertain, semantics is a more important parameter in evaluation.", "startOffset": 106, "endOffset": 114}, {"referenceID": 41, "context": "Then WordNet to SUMO mappings [44] were used to obtain semantically equivalent SUMO annotations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 40, "context": "For example, the result set 1 in [43] containing pictures queried with SUMO concept \u201cMan\u201d and ranked with Wu Palmer algorithm has the maximum lift factor for = 5 implying that to achieve the highest precision in this set only pictures with \u2264 5 have to be classified as True and all other pictures with > 5 as False.", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "Integration of WordNet-Affect [45] taxonomy of emotion related terms in the STIMONT would be beneficial for establishment of relationships and transformations between different emotion concepts.", "startOffset": 30, "endOffset": 34}, {"referenceID": 43, "context": "Other upper ontologies like DOLCE [46] and ConceptNet [47] could be used instead of SUMO for representation of highlevel objects, events and scenes.", "startOffset": 34, "endOffset": 38}, {"referenceID": 44, "context": "Other upper ontologies like DOLCE [46] and ConceptNet [47] could be used instead of SUMO for representation of highlevel objects, events and scenes.", "startOffset": 54, "endOffset": 58}, {"referenceID": 45, "context": "23 ontologies like Large-Scale Concept Ontology for Multimedia (LSCOM) [48] and Core Ontology for MultiMedia (COMM) [49] could also be a natural choice for annotation of multimedia stimuli.", "startOffset": 71, "endOffset": 75}, {"referenceID": 46, "context": "23 ontologies like Large-Scale Concept Ontology for Multimedia (LSCOM) [48] and Core Ontology for MultiMedia (COMM) [49] could also be a natural choice for annotation of multimedia stimuli.", "startOffset": 116, "endOffset": 120}, {"referenceID": 47, "context": "High accuracy concept-based video retrieval in some closed domains has been reported for these top ontologies [50].", "startOffset": 110, "endOffset": 114}, {"referenceID": 48, "context": "Lack of integration with other knowledge representations used in image annotation frameworks based on semantic networks such as WordNet [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 49, "context": "The presented ontology captures high-level information in semantics and emotion but it has been shown that low-level features like color, hue, brightness and texture influence perception and can be positively correlated to arousal and discrete emotion categories [52].", "startOffset": 263, "endOffset": 267}, {"referenceID": 50, "context": "Also, the STIMONT can be applied for representation of multimedia and Virtual Reality (VR) stimuli in psychotherapy as in exposure therapy (ET) or stress inoculation training (SIT) [53].", "startOffset": 181, "endOffset": 185}, {"referenceID": 51, "context": "Films, video-clips and related dynamic presentation technologies are often used for induction of emotion in the laboratory because of their relatively high attention capture and intensity [54].", "startOffset": 188, "endOffset": 192}], "year": 2013, "abstractText": "Affective multimedia documents such as images, sounds or videos elicit emotional responses in exposed human subjects. These stimuli are stored in affective multimedia databases and successfully used for a wide variety of research in psychology and neuroscience in areas related to attention and emotion processing. Although important all affective multimedia databases have numerous deficiencies which impair their applicability. These problems, which are brought forward in the paper, result in low recall and precision of multimedia stimuli retrieval which makes creating emotion elicitation procedures difficult and labor-intensive. To address these issues a new core ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and extends W3C EmotionML format with an expressive and formal representation of affective concepts, high-level semantics, stimuli document metadata and the elicited physiology. The advantages of ontology in description of affective multimedia stimuli are demonstrated in a document retrieval experiment and compared against contemporary keyword-based querying methods. Also, a software tool Intelligent Stimulus Generator for retrieval of affective multimedia and construction of stimuli sequences is presented.", "creator": "PScript5.dll Version 5.2.2"}}}