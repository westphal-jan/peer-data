{"id": "1401.2517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2014", "title": "The semantic similarity ensemble", "abstract": "Computational measures of semantic similarity between geographic terms provide valuable support across geographic information retrieval, data mining, and information integration. To date, a wide variety of approaches to geo-semantic similarity have been devised. A judgment of similarity is not intrinsically right or wrong, but obtains a certain degree of cognitive plausibility, depending on how closely it mimics human behavior. Thus selecting the most appropriate measure for a specific task is a significant challenge. To address this issue, we make an analogy between computational similarity measures and soliciting domain expert opinions, which incorporate a subjective set of beliefs, perceptions, hypotheses, and epistemic biases. Following this analogy, we define the semantic similarity ensemble (SSE) as a composition of different similarity measures, acting as a panel of experts having to reach a decision on the semantic similarity of a set of geographic terms. The approach is evaluated in comparison to human judgments, and results indicate that an SSE performs better than the average of its parts. Although the best member tends to outperform the ensemble, all ensembles outperform the average performance of each ensemble's member. Hence, in contexts where the best measure is unknown, the ensemble provides a more cognitively plausible approach.", "histories": [["v1", "Sat, 11 Jan 2014 10:35:37 GMT  (428kb)", "http://arxiv.org/abs/1401.2517v1", "Special feature on Semantic and Conceptual Issues in GIS (SeCoGIS)"]], "COMMENTS": "Special feature on Semantic and Conceptual Issues in GIS (SeCoGIS)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrea ballatore", "michela bertolotto", "david c wilson"], "accepted": false, "id": "1401.2517"}, "pdf": {"name": "1401.2517.pdf", "metadata": {"source": "CRF", "title": "The Semantic Similarity Ensemble\u2217", "authors": ["Andrea Ballatore", "Michela Bertolotto", "David C. Wilson"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 1,25 17Computational measures of semantic similarity between geographic terms are valuable support across geographic information retrieval, data mining, and information integration. To date, a variety of approaches to geo-semantic similarity have been developed. Assessing similarity is not inherently right or wrong, but maintains a certain degree of cognitive plausibility depending on how strongly it imitates human behavior. Therefore, selecting the most appropriate measure for a given task is a significant challenge. To address this problem, we establish an analogy between computational similarity measures and obtaining domain expert opinions that incorporate a subjective set of beliefs, perceptions, hypotheses, and epistemic distortions. Following this analogy, we define the semantic similarity ensemble (SSE) as a composition of various measures of similarity that act as a panel of experts who must make a decision on the geographical similarity."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to survive on their own."}, {"heading": "2 Related work", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to survive on their own."}, {"heading": "3 WordNet similarity measures", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves, \"he told the Deutsche Presse-Agentur in an interview with the Frankfurter Allgemeine Zeitung (Friday)."}, {"heading": "4 The semantic similarity ensemble (SSE)", "text": "A comparable measure of the semantic similarity of the terms ta and tb (ta, ta, tb), a comparable comparability of the pairs of terms, the pairs of which can be evaluated differently depending on the subjective set of terms, perceptions, hypotheses and epistemic distortions. If the performance of an expert can be compared against a gold standard, it is a reasonable policy to trust the expert who performs best. Unfortunately, such gold standards are difficult to construct and validate, and the choice of the most appropriate terms remains highly problematic. To overcome this problem, we propose the semantic similarity of the ensemble (SSE), a technique for combining different semantic similarities, remains highly problematic as a jury or panel of human experts contemplating a complex case. Formally, the similarity sim quantifies the semantic similarity of a pair of geographical terms ta and tb (ta, tb)."}, {"heading": "5 Evaluation", "text": "This section discusses an empirical evaluation of the SSE in real-world environments, the purpose of which is to evaluate the performance of the SSE in detail, highlighting strengths and weaknesses. Ten semantic similarity measures are tested using pairs of geographic terms used in OpenStreetMap. A preliminary evaluation of a small-scale analog technique was performed in [7]. Cardinality groups 2,3 and 4 were generated from eight similarity measures for a total of 154 ensembles. The evaluation described below is performed on a larger scale, using a larger set of geographic terms classified as basic truth by 203 human subjects. In order to get a complete picture of the performance of the ensemble, the entire combinatorial space is considered, covering a total of 1,012 unique ensembles. The remainder of this section outlines the evaluation criteria used to evaluate the performance of the SSE (section 5.1), the comparison of the people generated (section 5.2), section 5.3 (section 5.3), the preliminary results (section 5.2)."}, {"heading": "5.1 Evaluation criteria", "text": "Ensemble E's performance is measured by its cognitive plausibility in terms of the plausibility of its individual members \u03c1sim. Intuitively, an ensemble is successful when it produces rankings that are more cognitively plausible than those of JOSIS, article preprint members. Four criteria are formally defined in this assessment: \u2212 overall success. The plausibility of the ensemble is strictly greater than all its members: sim, E: \u03c1E > occsim \u2212 partial success. The plausibility of the ensemble is strictly greater than one member: sim, E: \u03c1E > occsim \u2212 success over average. The plausibility of the ensemble is strictly greater than the average plausibility of its members: \u03c1E > average (occsim1, occsim2... simn) \u2212 success over average. The plausibility of the ensemble is strictly greater than the average plausibility of its members: \u03c1E > average (moccsim1, m2) success over average."}, {"heading": "5.2 Ground truth", "text": "In order to assess the cognitive plausibility of the benchmarks and ensembles, a human-generated basic truth must be selected. In the preliminary assessment, a human-generated series of similarity rankings was extracted from an existing dataset [7]. This dataset contains similarity rankings of 50 term pairs across 29 geographical terms originally collected by Rodr\u00edguez and Egenhofer and available online. [1] In order to provide a thorough assessment of the SSE in this article, a new and larger human-generated dataset was adopted as the basic truth. As part of a more comprehensive study of geo-semantic similarity, we selected 50 pairs of geographical terms commonly used in OpenStreetMap, including 97 human-made and natural characteristics. The terms were then assigned to the corresponding terms in WordNet, as illustrated in Table 2."}, {"heading": "5.3 Experiment setup", "text": "In order to investigate the performance of an SSE in relation to individual measures, we selected a set of ten WordNet-based measures of similarity as a case study. Table 4 summarizes the resources of this experiment. The ten measures of similarity were not applied directly to the term pairs, but they were applied to their lexical definitions. To explore the space of all possible ensembles, we looked at the full range of ensemble sizes for M: the total power set of M was calculated. Increasing ensemble cardinality from 2 to 10 or 45, 120, 210, 252, 210, 120, 45, 9, 1 ensembles were calculated for a total of 1,012 ensembles."}, {"heading": "5.4 Experiment results", "text": "The experiment was conducted on two types of ensembles, one with As (mean of results), and one with Ar (mean of rankings). < 2 approaches get very close results, with a slightly better performance for Ar, with each evaluation criterion always within a distance of 5% of As. To avoid repetition, only cases with Ar are included in the discussion. All cognitive plausibility correlations get statistically significant results at p <.01% of the experiments are summarized in Table 5, which shows the cognitive plausibility of each measurement, and the four evaluation criteria in all ensemble categories. For example, the ensembles of Cardinality 2, some of which contain successes in 86.1% of the cases. The cognitive plausibility of the ten metrics is within range. [.562,.737] where vector is the best measurement, and lin the worst change."}, {"heading": "6 Conclusions", "text": "In fact, most of them are able to set out in search of a solution that originates in the real world."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Computational measures of semantic similarity between geographic terms provide<lb>valuable support across geographic information retrieval, data mining, and informa-<lb>tion integration. To date, a wide variety of approaches to geo-semantic similarity have<lb>been devised. A judgement of similarity is not intrinsically right or wrong, but ob-<lb>tains a certain degree of cognitive plausibility, depending on how closely it mimics<lb>human behaviour. Thus selecting the most appropriate measure for a specific task is<lb>a significant challenge. To address this issue, we make an analogy between computa-<lb>tional similarity measures and soliciting domain expert opinions, which incorporate<lb>a subjective set of beliefs, perceptions, hypotheses, and epistemic biases. Following<lb>this analogy, we define the semantic similarity ensemble (SSE) as a composition of differ-<lb>ent similarity measures, acting as a panel of experts having to reach a decision on the<lb>semantic similarity of a set of geographic terms. The approach is evaluated in com-<lb>parison to human judgements, and results indicate that an SSE performs better than<lb>the average of its parts. Although the best member tends to outperform the ensem-<lb>ble, all ensembles outperform the average performance of each ensemble\u2019s member.<lb>Hence, in contexts where the best measure is unknown, the ensemble provides a more<lb>cognitively plausible approach.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}