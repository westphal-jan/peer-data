{"id": "1401.3322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "A Subband-Based SVM Front-End for Robust ASR", "abstract": "This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels.", "histories": [["v1", "Tue, 24 Dec 2013 08:45:07 GMT  (114kb)", "http://arxiv.org/abs/1401.3322v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["jibran yousafzai", "zoran cvetkovic", "peter sollich", "matthew ager"], "accepted": false, "id": "1401.3322"}, "pdf": {"name": "1401.3322.pdf", "metadata": {"source": "CRF", "title": "A Subband-Based SVM Front-End for Robust ASR", "authors": ["Jibran Yousafzai", "Peter Sollich", "Matthew Ager"], "emails": [], "sections": [{"heading": null, "text": "This is an essential feature based on the presence of environmental distortions, particularly additive and convolutive interference signals based on non-linear language compression, and the central premise behind the design of state-of-the-art ASR systems is that combining frontends based on non-linear language compressions, such as the Melanquency Cepstral Coefficient in speech recognition (MFCC) and perception of linear predictions (PLP) coefficients [2], with appropriate language and context modeling techniques, can bring ASR's recognition power close to humans. However, the effectiveness of context and speech modeling depends crucially on the underlying accuracy predicted to the underlying units of underlying precision."}, {"heading": "II. SUBBAND CLASSIFICATION USING SUPPORT VECTOR MACHINES", "text": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalisation properties [17, 35, 42, 43, 45-48]. To this end, we construct a fixed length representation that could potentially be used as a front-end for a continuous speech recognition system based, for example, on hidden Markov models (HMMs) [42-44]. Variable phoneme length handling has been addressed by generative cores such as Fisher cores [47, 49] and dynamic time-warping cores [50], but is outside the scope of this paper. Therefore, the features of the proposed front-end are derived from segments of fixed length acoustic waveforms and these are investigated in comparison to the characteristics of the MFCC derived from the same language segments."}, {"heading": "A. Support Vector Machines", "text": "A binary SVM classification estimates a decision interface that collectively maximizes the margin between the two classes, thus minimizing the misclassification error on the training set. (x1,.) The SVM classifies a test point x by calculating a ranking. (x1,.) The SVM classifies a test point x by calculating a ranking. (x2,.) The SVM classifies a test point x by calculating a ranking. (x2,.) The S class is then called a comparison class (x, xi), in which the Lagrange multipliers are classified according to the ith training sample, xi, b is the classification bias - these are optimized during the training. The class name of x is then predicted as sgn (x). While the simplest kernel class K (x, x) = < x, x, x."}, {"heading": "B. Ensemble Methods", "text": "In our context it is synonymous with the formation of a meta-level score function (x).The decision of the subband scanners in the Ent scanner scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners scanners analogue scanners scanners analogue analogue analogues analogues scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogues Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanner Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanner Scanalogue Scanner Scanalogue Scanalogue Scanner Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue Scanalogue"}, {"heading": "III. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental Setup", "text": "The training set consists of 3696 sets of 168 different speakers that are not included in the training or core test set, with speakers from 8 different dialect regions. In training, we use a small subset of 1152 sets pronounced by 96 male and 48 female speakers that are not included in the training or core test set. In training, we use a small subset that selects one eighth of the data points in the complete TIMIT development. Glottal stops / q / are from the class labels and certain allophones are grouped into their corresponding phoneme classes that use the standard Kai-Fu Lee clustering [59], resulting in a total of M = 48 phoneme classes and N = 1128 classes."}, {"heading": "B. Results: Robustness to Additive Noise", "text": "This year it is more than ever before in the history of the city."}, {"heading": "C. Results: Robustness to Linear Filtering", "text": "We consider classification into the presence of additive noise as a linear filter. First, Figure 5 presents the results of the ensemble subgroup classification using stacked generalization with multiple training scenarios (see Section III-A). To repeat this, three different scenarios are considered for training the multi-level subband classifiers: it is about training the meta-level classifiers with the base level SVM score vectors of development, consisting of clean and white subsets (0-dB), the corrupt anechoic data, and secondly, it is about training with the score vectors of the same development data, which match the same conditions."}, {"heading": "IV. CONCLUSIONS", "text": "In this paper, we investigated an SVM frontend for robust speech recognition that operates in frequency subbands of high-dimensional acoustic waveforms (43% match-43 R). We addressed the issues of kernel design for subband components of acoustic waveforms and the aggregation of individual subband classifiers using ensemble methods. However, the experiments showed that the subband classifiers can outperform the Cepstral classifiers in the presence of noise and linear filtering for SNRs below 12 dB. While the subband classifiers do not work as well as the MFCC classifiers in low noise conditions, large gains can be achieved at all noise levels through a convex combination [17].9 \u2212 18 \u2212 6 0 18 Q 30 50 70 80 90 100SNR [dB] E RR OR [%] MFCC + VTS (Anechoic) Multi-style \u2212 Anechoic (Anechoic) combinations."}], "references": [{"title": "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences", "author": ["S.B. Davis", "P. Mermelstein"], "venue": "IEEE Trans. ASSP, vol. 28, pp. 357\u2013366, 1980.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1980}, {"title": "Perceptual Linear Predictive (PLP) Analysis of Speech", "author": ["H. Hermansky"], "venue": "J. Acoust. Soc. Amer., vol. 87, no. 4, pp. 1738\u20131752, April 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Speech Recognition by Machines and Humans", "author": ["R. Lippmann"], "venue": "Speech Comm., vol. 22, no. 1, pp. 1\u201315, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "An Analysis of Perceptual Confusions among some English Consonants", "author": ["G. Miller", "P. Nicely"], "venue": "J. Acoust. Soc. Amer., vol. 27, no. 2, pp. 338\u2013352, 1955.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1955}, {"title": "How do humans process and recognize speech?", "author": ["J.B. Allen"], "venue": "IEEE Trans. Speech & Audio Proc.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Human and Machine Consonant Recognition", "author": ["J. Sroka", "L. Braida"], "venue": "Speech Comm., vol. 45, no. 4, pp. 401\u2013423, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Phoneme Confusions in Human and Automatic Speech Recognition", "author": ["B. Meyer", "M. W\u00e4chter", "T. Brand", "B. Kollmeier"], "venue": "Proc. INTERSPEECH, pp. 2740\u20132743, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic Speech Recognition: a Communication Perspective", "author": ["B. Atal"], "venue": "Proc. ICASSP, pp. 457\u2013460, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "On the Limits of Speech Recognition in Noise", "author": ["S.D. Peters", "P. Stubley", "J. Valin"], "venue": "Proc. ICASSP, pp. 365\u2013368, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Towards Increasing Speech Recognition Error Rates", "author": ["H. Bourlard", "H. Hermansky", "N. Morgan"], "venue": "Speech Comm., vol. 18, no. 3, pp. 205\u2013231, 1996.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "On the Usefulness of STFT Phase Spectrum in Human Listening Tests", "author": ["K.K. Paliwal", "L.D. Alsteris"], "venue": "Speech Comm., vol. 45, no. 2, pp. 153\u2013170, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Further Intelligibility Results from Human Listening Tests using the Short-Time Phase Spectrum", "author": ["L.D. Alsteris", "K.K. Paliwal"], "venue": "Speech Comm., vol. 48, no. 6, pp. 727\u2013736, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Cepstral Domain Segmental Feature Vector Normalization for Noise Robust Speech Recognition", "author": ["O. Viikki", "K. Laurila"], "venue": "Speech Comm., vol. 25, pp. 133\u2013147, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "MVA Processing of Speech Features", "author": ["C. Chen", "J. Bilmes"], "venue": "IEEE Trans. ASLP, vol. 15, no. 1, pp. 257\u2013270, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A Vector Taylor Series Approach for Environment-Independent Speech Recognition", "author": ["P.J. Moreno", "B. Raj", "R.M. Stern"], "venue": "Proc. ICASSP, pp. 733\u2013736, 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Speech processing, Transmission and Quality aspects (STQ): Advanced front-end feature extraction", "author": ["E. standard doc."], "venue": "ETSI ES 202 050, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Combined Features and Kernel Design for Noise Robust Phoneme Classification Using Support Vector Machines", "author": ["J. Yousafzai", "Z. Cvetkovi\u0107", "P. Sollich", "B. Yu"], "venue": "To appear in the IEEE Trans. ASLP, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech and Hearing in Communication", "author": ["H. Fletcher"], "venue": "New York: Van Nostrand,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1953}, {"title": "Subband correlation and robust speech recognition", "author": ["J. McAuley", "J. Ming", "D. Stewart", "P. Hanna"], "venue": "IEEE Trans. on Speech and Audio Proc., vol. 13, no. 5, pp. 956 \u2013 964, 2005.  10", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust speech recognition using probabilistic union models", "author": ["J. Ming", "P. Jancovic", "F. Smith"], "venue": "IEEE Trans. on Speech and Audio Proc., vol. 10, no. 6, pp. 403 \u2013 414, Sep. 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminative Multiresolution Sub-band and Segmental Phonetic Model Combination", "author": ["P. McCourt", "N. Harte", "S. Vaseghi"], "venue": "IET Electronics Letters, vol. 36, no. 3, pp. 270 \u2013271, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Recognition Of Reverberant Speech Using Frequency Domain Linear Prediction", "author": ["S. Thomas", "S. Ganapathy", "H. Hermansky"], "venue": "IEEE Signal Process. Letters, vol. 15, pp. 681\u2013684, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Subband Based Recognition Of Noisy Speech", "author": ["S. Tibrewala", "H. Hermansky"], "venue": "Proc. ICASSP, pp. 1255\u20131258, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-Resolution Cepstral Features for Phoneme Recognition across Speech Sub- Bands", "author": ["P. McCourt", "S. Vaseghi", "N. Harte"], "venue": "Proc. ICASSP, pp. 557\u2013560, 1998.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Subband-based Speech Recognition", "author": ["H. Bourlard", "S. Dupont"], "venue": "Proc. ICASSP, pp. 1251\u20131254, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-band Speech Recognition in Noisy Environments", "author": ["S. Okawa", "E. Bocchieri", "A. Potamianos"], "venue": "Proc. ICASSP, pp. 641\u2013644, 1998.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "A Recombination Model for Multi-band Speech Recognition", "author": ["C. Cerisara", "J.-P. Haton", "J.-F. Mari", "D. Fohr"], "venue": "ICASSP, pp. 717 \u2013720 vol.2, 1998.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Computer- Steered Microphone Arrays for Sound Transduction in Large Rooms", "author": ["J. Flanagan", "J. Johnston", "R. Zahn", "G. Elko"], "venue": "J. Acoust. Soc. Amer., vol. 78, no. 11, pp. 1508\u20131518, 1985.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1985}, {"title": "A Two-Stage Algorithm for One- Microphone Reverberant Speech Enhancement", "author": ["M. Wu", "D. Wang"], "venue": "IEEE Trans. ASLP, vol. 14, pp. 774\u2013784, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical Large-Margin Gaussian Mixture Models for Phonetic Classification", "author": ["H. Chang", "J. Glass"], "venue": "Proc. ASRU, pp. 272\u2013275, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden Conditional Random Fields with Distribution Constraints for Phone Classification", "author": ["D. Yu", "L. Deng", "A. Acero"], "venue": "Proc. INTERSPEECH, pp. 676\u2013679, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Large Margin Gaussian Mixture Modeling for Phonetic Classification and Recognition", "author": ["F. Sha", "L.K. Saul"], "venue": "Proc. ICASSP, pp. 265\u2013268, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Noise Robust Phonetic Classification with Linear Regularized Least Squares and Second-Order Features", "author": ["R. Rifkin", "K. Schutte", "M. Saad", "J. Bouvrie", "J. Glass"], "venue": "Proc. ICASSP, pp. 881\u2013 884, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Heterogeneous Acoustic Measurements for Phonetic Classification", "author": ["A. Halberstadt", "J. Glass"], "venue": "Proc. EuroSpeech, pp. 401\u2013 404, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "On the Use of Support Vector Machines for Phonetic Classification", "author": ["P. Clarkson", "P.J. Moreno"], "venue": "Proc. ICASSP, pp. 585\u2013 588, 1999.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1999}, {"title": "Hidden Conditional Random Fields for Phone Classification", "author": ["A. Gunawardana", "M. Mahajan", "A. Acero", "J.C. Platt"], "venue": "Proc. INTERSPEECH, pp. 1117\u20131120, 2005.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Augmented Statistical Models for Speech Recognition", "author": ["M. Layton", "M. Gales"], "venue": "Proc. ICASSP, pp. I29\u2013132, 2006.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Analysis and Classification of Speech Signals by Generalized Fractal Dimension Features", "author": ["V. Pitsikalis", "P. Maragos"], "venue": "Speech Comm., vol. 51, pp. 1206\u20131223, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Relevance of Some Spectral and Temporal Patterns for Vowel Classification", "author": ["S. Dusan"], "venue": "Speech Comm., vol. 49, pp. 71\u201382, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Sub-banded Reconstructed Phase Spaces for Speech Recognition", "author": ["K.M. Indrebo", "R.J. Povinelli", "M.T. Johnson"], "venue": "Speech Comm., vol. 48, no. 7, pp. 760\u2013774, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Heterogeneous Measurements and Multiple Classifiers for Speech Recognition", "author": ["A. Halberstadt", "J. Glass"], "venue": "Proc. ICSLP, pp. 995\u2013998, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "Applications of Support Vector Machines to Speech Recognition", "author": ["A. Ganapathiraju", "J.E. Hamaker", "J. Picone"], "venue": "IEEE Trans. Signal Proc., vol. 52, no. 8, pp. 2348\u20132355, 2004.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech Recognition with Support Vector Machines in a Hybrid System", "author": ["S.E. Kr\u00fcger", "M. Schaffn\u0308er", "M. Katz", "E. Andelic", "A. Wendemuth"], "venue": "Proc. INTERSPEECH, pp. 993\u2013996, 2005.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Support Vector Machines for Continuous Speech Recognition", "author": ["J. Padrell-Sendra", "D. Mart\u0131\u0301n-Iglesias", "F. D\u0131\u0301az-de Mar\u0131\u0301a"], "venue": "Proc. EUSIPCO, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "New York: Springer-Verlag,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1995}, {"title": "Speech Recognition using SVMs", "author": ["N. Smith", "M. Gales"], "venue": "Adv. Neural Inf. Process. Syst., vol. 14, 2002, pp. 1197\u20131204.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Feature Space Mahalanobis Sequence Kernels: Application to SVM Speaker Verification", "author": ["J. Louradour", "K. Daoudi", "F. Bach"], "venue": "IEEE Trans. ASLP, vol. 15, no. 8, pp. 2465\u20132475, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Support Vector Machine Training for Improved Hidden Markov Modeling", "author": ["A. Sloin", "D. Burshtein"], "venue": "IEEE Trans. Signal Proc., vol. 56, no. 1, pp. 172\u2013188, 2008.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting Generative Models in Discriminative Classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "Adv. Neural Inf. Process. Syst., vol. 11, 1999, pp. 487\u2013493.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1999}, {"title": "Robust ASR using Support Vector Machines", "author": ["R. Solera-Urena", "D. Mart\u0131\u0301n-Iglesias", "A. Gallardo-Antol\u0131\u0301n", "C. Pel\u00e1ez-Moreno", "F. D\u0131\u0301az-de Mar\u0131\u0301a"], "venue": "Speech Comm., vol. 49, no. 4, pp. 253\u2013267, 2007.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes", "author": ["T. Dietterich", "G. Bakiri"], "venue": "J. Artif. Intell. Res., vol. 2, pp. 263\u2013286, 1995.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1995}, {"title": "In Defense of One-Vs-All Classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "J. Mach. Learn. Res., vol. 5, pp. 101\u2013141, 2004.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Speaker-Independent Isolated Word Recognition using Dynamic Features of Speech Spectrum", "author": ["S. Furui"], "venue": "IEEE Trans. ASSP, vol. 34, no. 1, pp. 52\u201359, 1986.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1986}, {"title": "Ensemble Methods in Machine Learning", "author": ["T. Dietterich"], "venue": "Lecture Notes in Computer Science: Multiple Classifier Systems, pp. 1\u201315, 2000.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural Network Ensembles", "author": ["L. Hansen", "P. Salamon"], "venue": "IEEE Trans. PAMI, vol. 12, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1990}, {"title": "Stacked Generalization", "author": ["D. Wolpert"], "venue": "Neural Networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1992}, {"title": "TIMIT Acoustic-Phonetic Continuous Speech Corpus", "author": ["J. Garofolo", "L. Lamel", "W. Fisher", "J. Fiscus", "D. Pallet", "N. Dahlgren"], "venue": "Linguistic Data Consortium, 1993.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1993}, {"title": "Speaker-Independent Phone Recognition Using Hidden Markov Models", "author": ["K.F. Lee", "H.W. Hon"], "venue": "IEEE Trans. ASSP, vol. 37, no. 11, pp. 1641\u20131648, 1989.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1989}, {"title": "1/f Noise in Music: Music from 1/f Noise", "author": ["R.F. Voss", "J. Clarke"], "venue": "J. Acoust. Soc. Amer., vol. 63, no. 1, pp. 258\u2013263, 1978.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1978}, {"title": "The Noise in Natural Phenomena", "author": ["B.J. West", "M. Shlesinger"], "venue": "American Scientist, vol. 78, no. 1, pp. 40\u201345, 1990.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1990}, {"title": "A Theory of 1/f Noise in Human Cognition", "author": ["P. Grigolini", "G. Aquino", "M. Bologna", "M. Lukovic", "B.J. West"], "venue": "Physica A: Stat. Mech. and its Appl., vol. 388, no. 19, pp. 4192\u20134204, 2009.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic Speech Recognition with an Adaptation Model Motivated by Auditory Processing", "author": ["M. Holmberg", "D. Gelbart", "W. Hemmert"], "venue": "IEEE Trans. ASLP, vol. 14, no. 1, pp. 43\u201349, 2006.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-Style Training for Robust Isolated-Word Speech Recognition", "author": ["R. Lippmann", "E.A. Martin"], "venue": "Proc. ICASSP, pp. 705\u2013 708, 1987.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1987}, {"title": "Robust Continuous Speech Recognition using Parallel Model Combination", "author": ["M. Gales", "S. Young"], "venue": "IEEE Trans. SAP, vol. 4, pp. 352\u2013359, Sept. 1996.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1996}, {"title": "Towards Robust Phoneme Classification with Hybrid Features", "author": ["J. Yousafzai", "Z. Cvetkovi\u0107", "P. Sollich"], "venue": "Proc. ISIT, pp. 1643\u20131647, 2010.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech Enhancement Using a Minimum Mean-Square Error Short-time Spectral Amplitude Estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. ASSP, vol. ASSP-32, pp. 1109\u20131121, 1984.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1984}, {"title": "Speech Enhancement Using a Minimum Mean-Square Log-Spectral Amplitude Estimator", "author": ["\u2014\u2014"], "venue": "IEEE Trans. ASSP, vol. ASSP-33, pp. 443\u2013445, 1985.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "The central premise behind the design of state-of-the-art ASR systems is that combining frontends based on the non-linear compression of speech, such as MelFrequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) coeffcients [2], with appropriate language and context modelling techniques can bring the recognition performance of ASR close to humans.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "The central premise behind the design of state-of-the-art ASR systems is that combining frontends based on the non-linear compression of speech, such as MelFrequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) coeffcients [2], with appropriate language and context modelling techniques can bring the recognition performance of ASR close to humans.", "startOffset": 250, "endOffset": 253}, {"referenceID": 2, "context": "However, the effectiveness of context and language modelling depends critically on the accuracy with which the underlying sequence of elementary phonetic units is predicted [3], and this is where there are still significant performance gaps between humans and ASR systems.", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Humans recognize isolated speech units above the level of chance already at \u221218-dB SNR, and significantly above it at \u22129-dB SNR [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Even in quiet conditions, the machine phone error rates for nonsense syllables are significantly higher than human error rates [3, 5\u20137].", "startOffset": 127, "endOffset": 135}, {"referenceID": 4, "context": "Even in quiet conditions, the machine phone error rates for nonsense syllables are significantly higher than human error rates [3, 5\u20137].", "startOffset": 127, "endOffset": 135}, {"referenceID": 5, "context": "Even in quiet conditions, the machine phone error rates for nonsense syllables are significantly higher than human error rates [3, 5\u20137].", "startOffset": 127, "endOffset": 135}, {"referenceID": 6, "context": "Even in quiet conditions, the machine phone error rates for nonsense syllables are significantly higher than human error rates [3, 5\u20137].", "startOffset": 127, "endOffset": 135}, {"referenceID": 6, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 7, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 8, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 9, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 10, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 11, "context": "Although there are a number of factors preventing the conventional ASR systems to reach the human benchmark, several studies [7\u201312] have attributed the marked difference between human and machine performance to the fundamental limitations of the ASR front-ends.", "startOffset": 125, "endOffset": 131}, {"referenceID": 0, "context": "These studies suggest that the large amount of redundancy in speech signals, which is removed in the process of the extraction of cepstral features such as Mel-Frequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) coefficients [2], is in fact needed to cope with environmental distortions.", "startOffset": 199, "endOffset": 202}, {"referenceID": 1, "context": "These studies suggest that the large amount of redundancy in speech signals, which is removed in the process of the extraction of cepstral features such as Mel-Frequency Cepstral Coefficients (MFCC) [1] and Perceptual Linear Prediction (PLP) coefficients [2], is in fact needed to cope with environmental distortions.", "startOffset": 255, "endOffset": 258}, {"referenceID": 6, "context": "Among these studies, the work on human speech perception [7, 9, 11, 12] has shown explicitly that the information reduction that takes place in the conventional frontends leads to a severe degradation in human speech recognition performance and, furthermore, that in noisy environments there is", "startOffset": 57, "endOffset": 71}, {"referenceID": 8, "context": "Among these studies, the work on human speech perception [7, 9, 11, 12] has shown explicitly that the information reduction that takes place in the conventional frontends leads to a severe degradation in human speech recognition performance and, furthermore, that in noisy environments there is", "startOffset": 57, "endOffset": 71}, {"referenceID": 10, "context": "Among these studies, the work on human speech perception [7, 9, 11, 12] has shown explicitly that the information reduction that takes place in the conventional frontends leads to a severe degradation in human speech recognition performance and, furthermore, that in noisy environments there is", "startOffset": 57, "endOffset": 71}, {"referenceID": 11, "context": "Among these studies, the work on human speech perception [7, 9, 11, 12] has shown explicitly that the information reduction that takes place in the conventional frontends leads to a severe degradation in human speech recognition performance and, furthermore, that in noisy environments there is", "startOffset": 57, "endOffset": 71}, {"referenceID": 12, "context": "Over the years, techniques such as cepstral mean-andvariance normalization (CMVN) [13, 14], vector Taylor series (VTS) compensation [15] and ETSI advanced front-end (AFE) [16] have been developed that aim to explicitly reduce the effects of noise on the short-term spectra in order to make the ASR front-ends less sensitive to noise.", "startOffset": 82, "endOffset": 90}, {"referenceID": 13, "context": "Over the years, techniques such as cepstral mean-andvariance normalization (CMVN) [13, 14], vector Taylor series (VTS) compensation [15] and ETSI advanced front-end (AFE) [16] have been developed that aim to explicitly reduce the effects of noise on the short-term spectra in order to make the ASR front-ends less sensitive to noise.", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Over the years, techniques such as cepstral mean-andvariance normalization (CMVN) [13, 14], vector Taylor series (VTS) compensation [15] and ETSI advanced front-end (AFE) [16] have been developed that aim to explicitly reduce the effects of noise on the short-term spectra in order to make the ASR front-ends less sensitive to noise.", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "Over the years, techniques such as cepstral mean-andvariance normalization (CMVN) [13, 14], vector Taylor series (VTS) compensation [15] and ETSI advanced front-end (AFE) [16] have been developed that aim to explicitly reduce the effects of noise on the short-term spectra in order to make the ASR front-ends less sensitive to noise.", "startOffset": 171, "endOffset": 175}, {"referenceID": 13, "context": "However, the distortion of the cepstral features caused by additive noise and linear filtering critically depends on the speech signal, filter characteristics, noise type and noise level in a complex fashion that makes effective feature compensation or adaptation very intricate and not sufficiently effective [14].", "startOffset": 310, "endOffset": 314}, {"referenceID": 16, "context": "In our previous work we showed that using acoustic waveforms directly, without any compression or nonlinear transformation can improve the robustness of ASR front-ends to additive noise [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "This approach draws its motivation primarily from the experiments conducted by Fletcher [18], which suggest that the human decoding of linguistic messages is based on decisions within narrow frequency subbands that are processed quite independently of each other.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "While this theory has not been proved and some studies on the subband correlation of speech signals [19, 20] have even put its validity into question, there are some technical reasons for considering classification in frequency subbands.", "startOffset": 100, "endOffset": 108}, {"referenceID": 19, "context": "While this theory has not been proved and some studies on the subband correlation of speech signals [19, 20] have even put its validity into question, there are some technical reasons for considering classification in frequency subbands.", "startOffset": 100, "endOffset": 108}, {"referenceID": 20, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 21, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 22, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 23, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 24, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 25, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 26, "context": "Previously, the subband approach has been used in [21\u201327] which resulted in marginal improvements in recognition performance over its full band counterparts.", "startOffset": 50, "endOffset": 57}, {"referenceID": 27, "context": "Several speech dereverberation techniques that rely on multi-channel recordings of speech such as [28, 29] exist in the literature.", "startOffset": 98, "endOffset": 106}, {"referenceID": 28, "context": "Several speech dereverberation techniques that rely on multi-channel recordings of speech such as [28, 29] exist in the literature.", "startOffset": 98, "endOffset": 106}, {"referenceID": 20, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 29, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 30, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 31, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 32, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 33, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 34, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 35, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 36, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 37, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 38, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 39, "context": "Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC frontend on a phoneme classification task; this task remains important in comparing different methods and representations [21, 30\u201340].", "startOffset": 246, "endOffset": 257}, {"referenceID": 40, "context": "The improvements achieved on the classification task can be expected to extend to continuous speech recognition tasks [41, 42] as SVMs have been employed in hybrid frameworks [42, 43] with hidden Markov models (HMMs) as well as in frame-based architectures using the token passing algorithm [44] for recognition of continuous speech.", "startOffset": 118, "endOffset": 126}, {"referenceID": 41, "context": "The improvements achieved on the classification task can be expected to extend to continuous speech recognition tasks [41, 42] as SVMs have been employed in hybrid frameworks [42, 43] with hidden Markov models (HMMs) as well as in frame-based architectures using the token passing algorithm [44] for recognition of continuous speech.", "startOffset": 118, "endOffset": 126}, {"referenceID": 41, "context": "The improvements achieved on the classification task can be expected to extend to continuous speech recognition tasks [41, 42] as SVMs have been employed in hybrid frameworks [42, 43] with hidden Markov models (HMMs) as well as in frame-based architectures using the token passing algorithm [44] for recognition of continuous speech.", "startOffset": 175, "endOffset": 183}, {"referenceID": 42, "context": "The improvements achieved on the classification task can be expected to extend to continuous speech recognition tasks [41, 42] as SVMs have been employed in hybrid frameworks [42, 43] with hidden Markov models (HMMs) as well as in frame-based architectures using the token passing algorithm [44] for recognition of continuous speech.", "startOffset": 175, "endOffset": 183}, {"referenceID": 43, "context": "The improvements achieved on the classification task can be expected to extend to continuous speech recognition tasks [41, 42] as SVMs have been employed in hybrid frameworks [42, 43] with hidden Markov models (HMMs) as well as in frame-based architectures using the token passing algorithm [44] for recognition of continuous speech.", "startOffset": 291, "endOffset": 295}, {"referenceID": 16, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 34, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 41, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 42, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 44, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 45, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 46, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 47, "context": "Support vector machines (SVMs) are receiving increasing attention as a tool for speech recognition applications due to their good generalization properties [17, 35, 42, 43, 45\u201348].", "startOffset": 156, "endOffset": 179}, {"referenceID": 41, "context": "hidden Markov models (HMMs) [42\u201344].", "startOffset": 28, "endOffset": 35}, {"referenceID": 42, "context": "hidden Markov models (HMMs) [42\u201344].", "startOffset": 28, "endOffset": 35}, {"referenceID": 43, "context": "hidden Markov models (HMMs) [42\u201344].", "startOffset": 28, "endOffset": 35}, {"referenceID": 46, "context": "Dealing with variable phoneme length has been addressed by means of generative kernels such as Fisher kernels [47, 49] and dynamic time-warping kernels [50], but lies beyond the scope of this paper.", "startOffset": 110, "endOffset": 118}, {"referenceID": 48, "context": "Dealing with variable phoneme length has been addressed by means of generative kernels such as Fisher kernels [47, 49] and dynamic time-warping kernels [50], but lies beyond the scope of this paper.", "startOffset": 110, "endOffset": 118}, {"referenceID": 49, "context": "Dealing with variable phoneme length has been addressed by means of generative kernels such as Fisher kernels [47, 49] and dynamic time-warping kernels [50], but lies beyond the scope of this paper.", "startOffset": 152, "endOffset": 156}, {"referenceID": 50, "context": "For multiclass problems, binary SVMs are combined via errorcorrecting output codes (ECOC) methods [51, 52].", "startOffset": 98, "endOffset": 106}, {"referenceID": 51, "context": "For multiclass problems, binary SVMs are combined via errorcorrecting output codes (ECOC) methods [51, 52].", "startOffset": 98, "endOffset": 106}, {"referenceID": 16, "context": "For classification in frequency subbands, an SVM kernel is constructed by partly following steps from our previous work [17], which attempted to capture known invariances or express explicitly the waveform qualities which are known to correlate with phoneme identity.", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "On the other hand, the standard polynomial kernel Kp is used for classification with the cepstral representations where feature standardization by cepstral mean-and-variance normalization (CMVN) [13] already ensures that feature vectors typically have unit norm.", "startOffset": 195, "endOffset": 199}, {"referenceID": 52, "context": "Finally, time differences [54] of \u03c9 are evaluated to form the dynamic subband feature vector \u03a9 as \u03a9 = [", "startOffset": 26, "endOffset": 30}, {"referenceID": 53, "context": "(10) Therefore, in ideal conditions, the ensemble error decreases exponentially in S even with this simple aggregation scheme [55, 56].", "startOffset": 126, "endOffset": 134}, {"referenceID": 54, "context": "(10) Therefore, in ideal conditions, the ensemble error decreases exponentially in S even with this simple aggregation scheme [55, 56].", "startOffset": 126, "endOffset": 134}, {"referenceID": 18, "context": "However, it has been shown that there exists a correlation between the subband components of speech and the resulting speech recognition errors in individual frequency subbands [19, 20].", "startOffset": 177, "endOffset": 185}, {"referenceID": 19, "context": "However, it has been shown that there exists a correlation between the subband components of speech and the resulting speech recognition errors in individual frequency subbands [19, 20].", "startOffset": 177, "endOffset": 185}, {"referenceID": 55, "context": "To remedy this, we use stacked generalization [57] as discussed next, to explicitly learn weighting functions specific to each pair of phonemes for nonuniform aggregation of the outputs of base-level SVMs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 55, "context": "2) Stacked Generalization: Our practical implementation of stacked generalization [57] consists of a hierarchical two-layer SVM architecture, where the outputs of subband base-level SVMs are aggregated by a meta-level linear SVM.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "In a preceding paper [17], we proposed an ASR front-end based on the full-band acoustic waveform representation of speech where a spectral shape adaptation of the features was performed in order to account for the varying strength of contamination of the frequency components due to the presence of colored noise.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The experiments presented in this paper show that the subband approach attains major gains in classification performance over its full-band counterpart [17] as well as the state-of-the-art front-ends such as MFCC.", "startOffset": 152, "endOffset": 156}, {"referenceID": 56, "context": "Experiments are performed on the \u2018si\u2019 (diverse) and \u2018sx\u2019 (compact) sentences of the TIMIT database [58].", "startOffset": 99, "endOffset": 103}, {"referenceID": 57, "context": "The glottal stops /q/ are removed from the class labels and certain allophones are grouped into their corresponding phoneme classes using the standard Kai-Fu Lee clustering [59], resulting in a total of M = 48 phoneme classes and N = M(M \u2212 1)/2 = 1128 classifiers.", "startOffset": 173, "endOffset": 177}, {"referenceID": 34, "context": "Among these classes, there are 7 groups for which the contribution of within-group confusions toward multiclass error is not counted, again following standard practice [35, 59].", "startOffset": 168, "endOffset": 176}, {"referenceID": 57, "context": "Among these classes, there are 7 groups for which the contribution of within-group confusions toward multiclass error is not counted, again following standard practice [35, 59].", "startOffset": 168, "endOffset": 176}, {"referenceID": 58, "context": "[60\u201362].", "startOffset": 0, "endOffset": 7}, {"referenceID": 59, "context": "[60\u201362].", "startOffset": 0, "endOffset": 7}, {"referenceID": 60, "context": "[60\u201362].", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "ally, cepstral mean-and-variance normalization (CMVN) [13, 14] is performed to standardize the cepstral features, fixing their range of variation for both training and test data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "ally, cepstral mean-and-variance normalization (CMVN) [13, 14] is performed to standardize the cepstral features, fixing their range of variation for both training and test data.", "startOffset": 54, "endOffset": 62}, {"referenceID": 61, "context": "Again, this is an impractical target; nevertheless, we present the results (only in the presence of additive noise) as a reference, since this setup is considered to give the optimal achievable performance with cepstral features [64\u201366].", "startOffset": 229, "endOffset": 236}, {"referenceID": 62, "context": "Again, this is an impractical target; nevertheless, we present the results (only in the presence of additive noise) as a reference, since this setup is considered to give the optimal achievable performance with cepstral features [64\u201366].", "startOffset": 229, "endOffset": 236}, {"referenceID": 63, "context": "Again, this is an impractical target; nevertheless, we present the results (only in the presence of additive noise) as a reference, since this setup is considered to give the optimal achievable performance with cepstral features [64\u201366].", "startOffset": 229, "endOffset": 236}, {"referenceID": 12, "context": "Furthermore, note that the MFCC features of both training and test data are standardized using CMVN [13] in all scenarios.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "On the other hand, decomposition of speech in sufficiently narrow subbands improves classification performance as demonstrated in [22], but at the cost of an increase in the overall computational complexity.", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "5% over the composite acoustic waveforms [17] and is therefore selected for further experiments.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "Composite Waveform [17] 36.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "In Figure 2, we compare the classification in frequency subbands using ensemble methods with composite acoustic waveform classification (results as reported in [17]) in the presence of white and pink noise.", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "2: Ensemble methods for aggregation of subband classifiers and their comparison with composite acoustic waveform classifiers (results as reported in [17]) in the presence of white noise (top) and pink noise (bottom).", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "Next, we compare the performance of the multi-style subband classifier with the VTS-compensated MFCC classifier and the composite acoustic waveform classifier [17] in the presence of additive white and pink noise.", "startOffset": 159, "endOffset": 163}, {"referenceID": 64, "context": "In [67] we showed that the MFCC classifiers suffer performance degradation in case of a mismatch of the noise type between training and test data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "In comparison to the result reported in [33], where a 77.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "Since an obvious performance crossover between the subband and MFCC classifiers exists at moderate SNRs, we therefore consider a convex combination of the scores of the SVM classifiers with a combination parameter \u03bb as discussed in [17].", "startOffset": 232, "endOffset": 236}, {"referenceID": 16, "context": "This approximated function was determined empirically in our previous experiments [17] and is given by \u03bbemp(\u03c32) = \u03b7 + \u03b6/[1 + (", "startOffset": 82, "endOffset": 86}, {"referenceID": 65, "context": "Note that \u03bbemp(\u03c3 ) also requires an estimate of the noise variance (\u03c3) which was explicitly measured using the decisiondirected estimation algorithm [68, 69].", "startOffset": 149, "endOffset": 157}, {"referenceID": 66, "context": "Note that \u03bbemp(\u03c3 ) also requires an estimate of the noise variance (\u03c3) which was explicitly measured using the decisiondirected estimation algorithm [68, 69].", "startOffset": 149, "endOffset": 157}, {"referenceID": 16, "context": "While the subband classifiers do not perform as well as the MFCC classifiers in low noise conditions, major gains across all noise levels can be attained by a convex combination [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 41, "context": "Alternatively, the proposed technique can also be integrated with other approaches such as the hybrid phone-based HMM-SVM architecture [42, 43] and the token-passing algorithm [44] for continuous speech recognition.", "startOffset": 135, "endOffset": 143}, {"referenceID": 42, "context": "Alternatively, the proposed technique can also be integrated with other approaches such as the hybrid phone-based HMM-SVM architecture [42, 43] and the token-passing algorithm [44] for continuous speech recognition.", "startOffset": 135, "endOffset": 143}, {"referenceID": 43, "context": "Alternatively, the proposed technique can also be integrated with other approaches such as the hybrid phone-based HMM-SVM architecture [42, 43] and the token-passing algorithm [44] for continuous speech recognition.", "startOffset": 176, "endOffset": 180}, {"referenceID": 41, "context": "This approach has provided improvements in recognition performance over HMM baselines on both small and large vocabulary recognition tasks, even though the SVM classifiers were constructed solely from the cepstral representations [42, 43].", "startOffset": 230, "endOffset": 238}, {"referenceID": 42, "context": "This approach has provided improvements in recognition performance over HMM baselines on both small and large vocabulary recognition tasks, even though the SVM classifiers were constructed solely from the cepstral representations [42, 43].", "startOffset": 230, "endOffset": 238}, {"referenceID": 43, "context": "On the other hand, a recognizer based solely on SVMs as discussed in [44] can also employed which makes decisions at a frame level via SVMs and determines the chain of recognized phonemes and words using the token-passing algorithm.", "startOffset": 69, "endOffset": 73}], "year": 2014, "abstractText": "This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels.", "creator": "LaTeX with hyperref package"}}}