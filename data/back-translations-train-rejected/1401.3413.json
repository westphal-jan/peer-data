{"id": "1401.3413", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Infinite Mixed Membership Matrix Factorization", "abstract": "Rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques. Current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization, which factorizes a user-item ratings matrix into latent user and item vectors. Most of these methods fail to model significant variations in item ratings from otherwise similar users, a phenomenon known as the \"Napoleon Dynamite\" effect. Recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in which an item is rated by a user. In this work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and derive Gibbs sampling inference procedures for our model. We evaluate our approach on the MovieLens 1M dataset, and show significant improvements over the optimal parametric baseline, more than twice the improvements previously encountered for this task. We also extract and evaluate a DBLP dataset, wherein we predict the number of papers co-authored by two authors, and present improvements over the parametric baseline on this alternative domain as well.", "histories": [["v1", "Wed, 15 Jan 2014 02:39:15 GMT  (217kb,D)", "http://arxiv.org/abs/1401.3413v1", "For ICDM 2013 Workshop Proceedings"]], "COMMENTS": "For ICDM 2013 Workshop Proceedings", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["avneesh saluja", "mahdi pakdaman", "dongzhen piao", "ankur p parikh"], "accepted": false, "id": "1401.3413"}, "pdf": {"name": "1401.3413.pdf", "metadata": {"source": "CRF", "title": "Infinite Mixed Membership Matrix Factorization", "authors": ["Avneesh Saluja", "Mahdi Pakdaman", "Dongzhen Piao", "Ankur P. Parikh"], "emails": [], "sections": [{"heading": null, "text": "Most of these methods, however, have at their core a matrix-factorization-based approach in which each element is evaluated by a user. We can use the users and their ratings for articles such as the U-M matrix R (hereinafter referred to as the rating matrix or the user item matrix), which contains the ratings for the U-M elements, and in which each element is evaluated by at least one user. The idea is to place the latent factors in the background, in which ATB \u00d7 U is the latent user matrix, and the B-M matrix, which contains the ratings for the users, and in which each element is evaluated by at least one user. The idea is to explicitly state the RD-U matrix is the latent factors, the latent is the latent is the latent user matrix, and the B matrix is the latent user matrix. \""}, {"heading": "II. RELATED WORK", "text": "As a way to address recommendation problems of the system, the factorization of matrix factors is one of the popular approaches along with content-based filtering [2] and has proven to be the superior alternative of the two in recent years [3]. Probabilistic matrix factorization (PMF) was first introduced by Salakhutdinov and Mnih [4], whereby ratings are generated from a Gaussian model, with the latent factors themselves being generated by Gaussian distributions, with normal Wishart priors on the hyperparameters. Our work is primarily based on the extension of the M3F model [BPMF) subsequently proposed by the same authors [5], assuming that the latent factors themselves were generated by Gaussian distributions, with normal Wishart priors on the hyperparameters. Our work is primarily based on the extension of the M3F model [1] of latent preference factors taken, but we are using the latent factors of a mixed factor in addition to the M3F and M3F models in user models."}, {"heading": "III. APPROACH", "text": "Our proposed method, iM3F, is based on the extension of the M3F approach to probabilistic modelling of recommendation systems. In iM3F, we add an infinite dimensional priority for user and article topics. This enhancement is realized in our model by using a non-parametric Bayesian prior for the topic distributions, which allows the model to have a potentially unlimited number of user and article topics. We present the schematic and symbolic views of our model and derive a Gibbs sampling inference scheme using the Chinese restaurant representation of the non-parametric prior."}, {"heading": "A. Chinese Restaurants", "text": "The non-parametric previously on the mixtures of topics for users and articles is motivated by its ability to dictate the data, the optimal number of users and article groups, rather than parameterizing these values beforehand. Using such a prior makes it unnecessary to perform extensive \"tuning\" experiments for the number of topics, and is arguably conceptually simpler and more elegant. We provide a brief background to the ideas, with an emphasis on intuition. For a more rigorous introduction, we refer the reader to [14]. For the purposes of exposure, we will discuss the case that user is defined as mixtures of user topics (i.e., a user is defined as a mixture of different \"moods\"), and note that the item case is completely symmetrical. In the final case, we can present this mixture as a multinomic distribution across the various user topics."}, {"heading": "B. Generative Process", "text": "We present the generative process of the iM3F model shown in Figure 2. (For comparison purposes, the schematic view of the finite, parametric case shown in Figure 1 corresponds to the original M3F model. (First, the hyperparameters for the latent user and item factors are2Such a \"separate\" CRP model has been implemented and evaluated, but the results were worse than a parametric solution.) We also generate the global probability measures used to define a set of common clusters for the user and item themes: G0, v0, \u00b5U. ("}, {"heading": "IV. EXPERIMENTS", "text": "The proposed iM3F model was evaluated on the MovieLens and DBLP datasets. On the MovieLens set, we broadly divided our results into internal comparisons, i.e. hyperparameter variation experiments, and external comparisons, which implies improvements over M3F. For the DBLP dataset, we present final numbers compared to M3F. While previous work evaluated their approaches in the Netflix price dataset, we are unable to do so due to the unavailability of data. All of the experiments were performed on a quad-core Linux 2.5 GHz and 8GB RAM calculator, and implementation took place in MATLAB and C."}, {"heading": "A. Datasets", "text": "We used two versions of the datasets (ml\u03b2 and ml1m), each containing 100,000 and 1 million ratings. Table I shows some summarized statistics. In both datasets, each user rated at least 20 movies, each rating being an integer from 1 to 5. Comparing the performance of the different models, the performance of the different models was measured. We performed this random split twice, and all results for the MovieLens experiments are obtained via the two splits.3http: / www.grouplens.orgWe also extracted a publication dataset from the latest DBLP XML data dated January 6, 2013."}, {"heading": "V. FUTURE WORK & CONCLUSION", "text": "With the CRF in mind, it is also possible to experiment with variants based on the nature of the data itself. For example, reciprocal datasets, where the rating of other users is based on mutual compatibility, are becoming more common, such as academic publication databases such as our DBLP dataset or the Microsoft Academic Scholar database, or online dating sites such as eHarmony or OkCupid. In the future, we would like to model the dataset of co-authorship (and other similar datasets) in a more direct \"relational\" manner. Bidirectional assessments must somehow be combined to incorporate the principle of reciprocity into these assessments. Handling such changes is easy in our work, e.g. Figure 6. In this paper, we have designed and implemented an important addition to the general field of probability matrix factorization."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Lester Mackey for his help in modifying his original code base to the non-parametric version, Eric P. Xing for his comments and the anonymous reviewer for their feedback."}], "references": [{"title": "Mixed membership matrix factorization", "author": ["L. Mackey", "D. Weiss", "M.I. Jordan"], "venue": "Proceedings of the 27th International Conference on Machine Learning, June 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "John", "T. Riedl"], "venue": "ACM Transactions on Information Systems, vol. 22, pp. 5\u201353, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, IEEE Computer Society, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Advances in Neural Information Processing Systems, vol. 20, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["\u2014\u2014"], "venue": "Proceedings of the International Conference on Machine Learning, vol. 25, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1981\u20132014, Jun. 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, Mar. 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["K. Nowicki", "T.A.B. Snijders"], "venue": "Journal of the American Statistical Association, vol. 96, no. 455, pp. 1077\u20131087, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201911, 2011, pp. 448\u2013456.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast nonparametric matrix factorization for large-scale collaborative filtering", "author": ["K. Yu", "S. Zhu", "J. Lafferty", "Y. Gong"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, ser. SIGIR \u201909. New York, NY, USA: ACM, 2009, pp. 211\u2013218.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonparametric bayesian matrix factorization by power-ep", "author": ["N. Ding", "Y.A. Qi", "R. Xiang", "I. Molloy", "N. Li"], "venue": "Journal of Machine Learning Research - Proceedings Track, vol. 9, pp. 169\u2013176, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric max-margin matrix factorization for collaborative prediction", "author": ["M. Xu", "J. Zhu", "B. Zhang"], "venue": "NIPS, 2012, pp. 64\u201372.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Exact and approximate sum representations for the dirichlet process", "author": ["H. Ishwaran", "M. Zarepour"], "venue": "Canadian Journal of Statistics, vol. 30, no. 2, pp. 269\u2013283, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A Bayesian Analysis of Some Nonparametric Problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics, vol. 1, no. 2, pp. 209\u2013230, 1973.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1973}, {"title": "Ferguson distributions via P\u00f3lya urn schemes", "author": ["D. Blackwell", "J.B. Macqueen"], "venue": "The Annals of Statistics, vol. 1, pp. 353\u2013355, 1973.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1973}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, vol. 101, no. 476, pp. 1566\u20131581, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Exact and Efficient Parallel Inference for Nonparametric Mixture Models", "author": ["S.A. Williamson", "A. Dubey", "E.P. Xing"], "venue": "ICML 2013, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The recently proposed mixed membership matrix factorization (MF) model [1] aims to tackle this problem by explicitly modeling ratings not only with static latent factors, but with an additional bias term that takes into account the context of the rating.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "RELATED WORK As a way to attack recommender system problems, matrix factorization has been one of the popular approaches along with content-based filtering [2], and in recent years has proven to be the superior alternative of the two [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "RELATED WORK As a way to attack recommender system problems, matrix factorization has been one of the popular approaches along with content-based filtering [2], and in recent years has proven to be the superior alternative of the two [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "Probabilistic matrix factorization (PMF) was first introduced by Salakhutdinov and Mnih [4], whereby ratings are generated from a Gaussian with a mean computed as the dot product of the user and item latent factors, with fixed variance.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "A fully Bayesian version of PMF (BPMF) was subsequently proposed by the same authors [5], where they assumed the latent factors were themselves generated by Gaussian distributions, with normalWishart priors on the hyperparameters.", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Our work is primarily based on extending the MF model [1] .", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 271, "endOffset": 274}, {"referenceID": 7, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 304, "endOffset": 307}, {"referenceID": 8, "context": "The MF framework\u2019s concept of modeling user and item biases through user-item topic and item-user topic interactions is derived from mixed membership stochastic blockmodels (MMSBs) [6], which itself is influenced by the original mixed membership topic modeling framework [7], and stochastic block models [8], [9].", "startOffset": 309, "endOffset": 312}, {"referenceID": 9, "context": "Collaborative topic regression [10] also combines matrix factorization and mixed membership topic modeling, but their method solves a different problem, the cold start issue of outof-matrix predictions, rather than contextual bias.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "[11] do not use a probabilistic approach and instead use SVD, whereas [12], [13] adopt an Indian Buffett Process to model the latent factor dimensionality, and", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "For a more rigorous introduction, we refer the reader to [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "If we take the number of mixture components to infinity, then the conjugate prior becomes a Dirichlet process (DP) [15], a distribution over an infinitely fine-grained real line.", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "1: The original MF model, as proposed in [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "practice, we do not have a density defined for the infinitedimensional DP prior, so we define it indirectly via a sampling scheme that is used to generate samples from this prior [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "Otherwise, if we are simply interested in partition or cluster assignments for the data, we can marginalize the mixture weights and sample the assignments directly, through a process known as a Chinese restaurant process (CRP) [16].", "startOffset": 227, "endOffset": 231}, {"referenceID": 16, "context": "This requirement necessitates a hierarchical Dirichlet process (HDP) prior [17] on the mixture components.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "When comparing against MF, we use the hyperparameter settings that yield the best performance as in the original paper [1]: \u2022 Number of user topics K : 2 \u2022 Number of item topics K : 1 \u2022 Latent factor dimensionality D: 40 As we do not alter the latent factor terms, we maintain D = 40 for the iMF model.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "iMF and MF We then compared our CRF iMF model with the MF implementation of [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "However, we note that recent efforts that develop exact, distributed MCMC procedures for Dirichlet process-based models [18] can easily be applied in our scenario to accelerate Gibbs sampling, which is a simple solution for today\u2019s multicore computers.", "startOffset": 120, "endOffset": 124}], "year": 2014, "abstractText": "Rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques. Current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization, which factorizes a user-item ratings matrix into latent user and item vectors. Most of these methods fail to model significant variations in item ratings from otherwise similar users, a phenomenon known as the \u201cNapoleon Dynamite\u201d effect. Recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in which an item is rated by a user. In this work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and derive Gibbs sampling inference procedures for our model. We evaluate our approach on the MovieLens 1M dataset, and show significant improvements over the optimal parametric baseline, more than twice the improvements previously encountered for this task. We also extract and evaluate a DBLP dataset, wherein we predict the number of papers co-authored by two authors, and present improvements over the parametric baseline on this alternative domain as well.", "creator": "LaTeX with hyperref package"}}}