{"id": "1401.3428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains", "abstract": "We consider the problem of optimal planning in stochastic domains with resource constraints, where the resources are continuous and the choice of action at each step depends on resource availability. We introduce the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables, where the continuous variables represent monotonic resources. Like other heuristic search algorithms, HAO* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy. We show that this approach is especially effective when resource constraints limit how much of the state space is reachable. Experimental results demonstrate its effectiveness in the domain that motivates our research: automated planning for planetary exploration rovers.", "histories": [["v1", "Wed, 15 Jan 2014 04:46:00 GMT  (941kb)", "http://arxiv.org/abs/1401.3428v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nicolas meuleau", "emmanuel benazera", "ronen i brafman", "eric a hansen", "mausam"], "accepted": false, "id": "1401.3428"}, "pdf": {"name": "1401.3428.pdf", "metadata": {"source": "CRF", "title": "A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains", "authors": ["Nicolas Meuleau", "Emmanuel Benazera", "Ronen I. Brafman", "Eric A. Hansen"], "emails": ["nicolas.f.meuleau@nasa.gov", "ebenazer@laas.fr", "brafman@cs.bgu.ac.il", "hansen@cse.msstate.edu", "mausam@cs.washington.edu"], "sections": [{"heading": null, "text": "We introduce the HAO * algorithm, a generalization of the AO * algorithm that performs the search in a hybrid state space, modelled by both discrete and continuous state variables, with the continuous variables representing monotonous resources. Like other heuristic search algorithms, HAO * uses initial state knowledge and permissible heuristics to focus computational effort on those parts of the state space that could be reached from the initial state by pursuing optimal policies. We show that this approach is particularly effective when resource constraints limit the attainable state space. Experimental results show its effectiveness in the area that motivates our research: automated planning for planetary exploration rovers."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Problem Formulation and Background", "text": "We start with a formal definition of the planning problem we are addressing. It is a specific case of a hybrid Markov decision process, so we first define this model, then discuss how to incorporate resource constraints and formalize oversubscription planning into this model. Finally, we review a class of dynamic programming algorithms to solve hybrid MDPs, as some of these algorithmic techniques will be incorporated into the heuristic search algorithm we are developing in Section 3."}, {"heading": "2.1 Hybrid-State Markov Decision Process", "text": "A hybrid state Markov decision process, or hybrid state MDP, is a factored Markov decision process that has both discrete and continuous state variables. We define it as a tuple (N, X, A, P, R), where N is a discrete state variable, X = {X1, X2,..., Xd} is a set of continuous state variables, A is a set of actions, P is a stochastic state transition model, and R is a reward function. We will describe these elements in more detail below. A hybrid state MDP is sometimes simply referred to as a hybrid state MDP. The term \"hybrid\" does not refer to the dynamics of the model, which are discrete, and R is a reward function. Another term for a hybrid state MDP, which has its origin in the Markov chain literature, is a generalized state MDP. Although a hybrid state MDP is a discrete state, and R is a reward function. Another term for a hybrid state MDP, which has its origin the Markov chain literature, is a generalized state MDP. Although a discrete state MDP is a discrete state, and R is a rewards function."}, {"heading": "2.2 Resource Constraints and Over-Subscription Planning", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2.3 Optimality Equation", "text": "The rover scheduling problem that we are considering is a special case of a finite horizon hybrid-state MDP in which termination occurs after an indefinite number of steps. The Bellman optimality equation for this problem assumes the following form: Vn (x) = 0, if (n, x) is an end state; otherwise, Vn (x) = max a \"A\" (x) [p \"n\" n \"n Pr (n\" n, x, \"x,\" a \") (Rn\" + Vn \"(x\")) dx \"\" [p \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"pr\" (n \",\" x, \"\" x, \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "2.4 Dynamic Programming for Continuous-State and Hybrid-State MDPs", "text": "Since the planning problem we are considering is used by other dynamics algorithms, a finite horizon hybrid state is MDP, which can be solved by any algorithm for solving problems with a limited horizon and hybrid states. A common approach is to divide the continuous state space into a limited number of network points and to solve the resulting problems with dynamic programming and interpolation (Rust, 1997; Munos & Moore, 2002). Another approach is the parametric functional approximation associated with the dynamic programming problem - such as the value function or political function - which is achieved by a smooth functioning of parameters. Piepiepiepiepiepiepiepiepiepiepieced functional approximation is faster than a network-based approximation, but has the disadvantage that it may fail at a conversion or a conversion to an incorrect solution."}, {"heading": "3. Heuristic Search in a Hybrid State Space", "text": "In this section, we present the primary contribution of this paper: an approach to solving a specific class of hybrid-state MDPs using a novel generalization of the heuristic search algorithm AO *. In particular, we describe a generalization of this algorithm for solving hybrid-state MDPs, in which the continuous variables represent monotonous and limited resources, and the acyclic plan found by the search algorithm allows a branching out on the availability of these resources. The motivation for deploying the heuristic search is the potentially enormous size of the state space, which makes dynamic programming impossible. One reason for this size is the existence of continuous variables. However, even if we consider only the discrete component of the state space, the size of the state space is exponential in the number of discrete variables. As is well known, AO * can be very effective in solving planning problems that have a large state space, because it takes into account only the initial state of a hybrid space, and allows it to be reached from a forensic one."}, {"heading": "3.1 AO*", "text": "Remember that AO * is an algorithm for AND / OR graph search problems (Nilsson, 1980; Pearl, 1984). Such graphs arise in problems in which there are decisions (the OR components), and any choice can have multiple sequences (the AND component), as is the case in planning under uncertainty. Hansen and Zilberstein (2001) show how AND / OR graph search techniques can be used to solve MDPs. Following Nilsson (1980) and Hansen and Zilberstein (2001) we define an AND / OR graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-pergraph in a graph-graph-graph, like a graph-graph-graph-graph-graph in a graph-pergraph, or a graph-graph-graph-graph in a graph-like a pair of graphs."}, {"heading": "3.2 Hybrid-State AO*", "text": "The challenge we face when applying AO * to this problem is to perform the search for a state-space in a hybrid state-space. In other words, each node of the AND / OR diagram represents a region of the continuous state-space in which the discrete value is the same. In view of this continuous state-space division, we use AND / OR diagram techniques to resolve the MDP for those parts of the state-space that are accessible from the start state under the best policy. However, AND / OR diagram search techniques need to be modified in important ways to enable searching in a hybrid state-space that is represented in this way. Specifically, there is no longer any correlation between the nodes of the AND / OR diagram and the individual state functions of the hybrid space, each of which is seeking an optimal state."}, {"heading": "3.2.1 Data Structures", "text": "Each node of the explicit AND / OR diagram G \u00b2 consists of the following: \u2022 The value of the discrete state variables is considered constant. \u2022 Pointers to their parents and children in the explicit graph and the political graph. \u2022 Openn (\u00b7) \u2192 {0, 1}: the \"closed list.\" For each x-X, Openn (x) indicates whether (n, x) the respective state is at the boundary of the explicit graph, i.e., it is created but not yet expanded. \u2022 Closed (\u00b7 0, 1): the \"closed list.\" For each x-X, Closedn (x) indicates whether (n, x) is already expanded inside the explicit graph, i.e., has expanded."}, {"heading": "3.2.2 Algorithm", "text": "In some cases, this means adding a new node to the AND / R diagram. In others, it is simply a matter of tapping an area. In this case, it is the same as the AO * algorithm and consists of iterating the same three steps; solution (or policy) expansion, using dynamic programming to update the current value function and policy, and analyzing the accessibility of solutions that are eligible for expansion. In detail, it is modified in several important areas to allow the search for a hybrid state space. Below, we discuss the modifications to each of these three steps. All nodes of the current solution are identified and one or more open regions associated with these nodes are selected for expansion. That is one or more regions of the hybrid state space explicitly selected for expansion. All measures applicable to the states in these open regions are simulated, and the results of these measures are added to the diagram."}, {"heading": "3.3 Convergence and Error Bounds", "text": "Next, we will look at some of the theoretical properties of HAO *. First, under reasonable assumptions, we can prove that HAO * approaches an optimal policy after a finite number of steps. Then, we will discuss how to use HAO * to find suboptimal strategies with error limits. Evidence of convergence after a finite number of steps depends, among other things, on the assumption that a hybrid state has a finite branching factor (MDP). In our implementation, this means that for each region of the state that can be represented by a hyperrectangle, the number of successor regions can be represented after a finite series of measures."}, {"heading": "3.4 Heuristic Function", "text": "The heuristic function Hn focuses the search on attainable states that are most likely useful. The more informative the heuristics, the more scalable the search algorithm is. In our implementation of HAO * for the planning problem of the robot, which is described in detail in the next section, we used the simple admissible heuristic function, which assigns to each node the sum of all the rewards associated with previously unachieved goals. Note that this heuristic function depends only on the discrete component of the state and not on the continuous variables; that is, the function Hn (x) is constant across all values of x. It is obvious that this heuristic is permissible as it represents the maximum additional reward that could be achieved by continued plan execution. Although it is not obvious that heuristics could be so easily disuseful, the experimental results we present in Section 4 show that it is the most likely to be the maximum additional reward that could be achieved by continued plan execution."}, {"heading": "3.5 Expansion Policy", "text": "HAO * works correctly and converges to an optimal solution, regardless of the continuous region (s) from which node (s) are expanded in each iteration (step 2.a), but the quality of the solution can improve faster by using some \"heuristics\" to choose which region (s) is most likely to be reached with current policy. A simple strategy is to select a node and expand all continuous regions of that node that are open and reachable. In a preliminary implementation, we (the open regions) expand the node that is most likely to be reached with current policy. Changes in the value of these states will have the greatest effect on the value of previous nodes that require additional work related to maintaining the probability associated with each state. If such probabilities are available, one could also focus on expanding the most promising node, that is, the node where the integral of Hn (x) times the probability of all values is explix."}, {"heading": "3.6 Updating Multiple Regions", "text": "The expansion policies described above are based on the simultaneous expansion of all open regions of one or more nodes. However, they allow to use hybrid state dynamic programming techniques such as those of Feng et al. (2004) and Li and Littman (2005). These techniques can compute constant and linear value functions in a single iteration, covering a wide range of continuous states, possibly the entire space of possible values. Specifically, they can back up in one iteration all continuous states enclosed between given boundaries. Thus, if several open regions of the same node are extended in the same iteration of HAO *, we can update all at the same time by securing a subset of continuous states encompassing all of these regions. Thus, for example, one can record lower boundaries and upper boundaries for each continuous variable across the extended regions and then calculate a value function that extends the hyper-computational invoicing module to all of the region that does not fully cover the triangle of the region."}, {"heading": "4. Experimental Evaluation", "text": "This year it is more than ever before."}, {"heading": "4.1 Example", "text": "We start with a very simple example of the planning problem of the robot to illustrate the steps of the algorithm. We solve this example with the same implementation of HAO * that we use to solve the more realistic examples considered in Section 4.2. In this example, the objectives are two rocks, R1 and R2, positioned at locations L1 and L2. The robot's starting point is L1, and there is a direct path between L1 and L2. The analysis of rocks R1 yields a reward of 10 and the analysis of rocks R2 yields a reward of 20. The action set of the robot is simplified. It is noteworthy that it has a single action Pic (Rx) to represent all the steps of the analysis of fields Rx, and the \"stop tracking\" actions have been removed. Figure 6 shows the optimal value function and the optimal policy found by HAO * for the start discrete values of step 8, the problem 8, the entire resource range of step 8, and Figure 8 show the problem 8."}, {"heading": "4.2 Performance", "text": "The characteristics of these problems are illustrated in Tables 3. Columns two to six show the size of the problems in relation to the locations, paths and goals of the robot. They also show the total number of flows (Boolean state variables) and actions in each problem. Columns seven to ten report the size of the discrete state space. The total number of discrete states is increased by two to achieve the performance of the number of flows. Although it is a huge state space, only a limited number of states can be achieved from the start state, depending on the initial resource levels. the eighth column in Table 3 shows the number of achievable discrete states when the initial time and energy level are set to their maximum value. (The maximum initial resource levels are based on the scenario of the 2004 IS demonstration and represent several hours of robot activity.) It shows that a simple accessibility analysis based on the availability of resources makes a huge difference, partly due to the fact that the nine is very narrow."}, {"heading": "4.2.1 Efficiency of Pruning", "text": "In a first series of simulations, we try to evaluate the efficiency of heuristic truncation in HAO *, that is, the part of the discrete search space that is spared from exploration by the use of permissible heuristics. To this end, we compare the number of discrete states that are achievable for a given resource level with the number of nodes generated and extended by HAO *. We also look at the number of nodes in the optimal policy found by the algorithm. Results for the four benchmark problems are shown in Figure 11. These curves are obtained by setting one resource to its highest possible value and the other to its maximum from 0. Therefore, they represent problems where mainly one resource is limited. In particular, these results show that a single resource is sufficient to limit the accessibility of the state space. It is not surprising that problems become greater when initial resources increase because more discrete states are reached. In spite of the simplicity of the greater hypocrisy problem, the HAO is not a time to use."}, {"heading": "4.2.2 Search Time", "text": "However, these curves do not have the same monotonicity and instead appear to show a significant amount of noise. It is surprising that the search time does not always increase with an increase in the initial resource levels, even though the search space is larger. This shows that the search complexity does not depend solely on the size of the search space. Other factors must explain the complexity peaks observed in Figure 12. Since the number of nodes generated and extended by the algorithm does not include such noise, the reason for the peak computing time is the time spent in dynamic program backups. Furthermore, the search time appears to be closely related to the complexity of the optimal policy. Figure 13 shows the number of nodes and branches in the policy found by the algorithm, as well as the number of goals pursued by this policy."}, {"heading": "4.2.3 Expansion Horizon", "text": "The results of Section 4.2.1 show that HAO * itself allows a simple problem that truncates a large part of the search space. However, it does not necessarily follow that HAO * can perform a smaller graph than such an exhaustive search, but it needs to evaluate the graph more frequently. In Section 3.5, we introduced an expansion horizon parameter to allow adjustment of a trade-off between the time we spent evaluating nodes. We are now investigating the effect of this parameter on the algorithm."}, {"heading": "5. Conclusion", "text": "The HAO * algorithm is a variant of the AO * algorithm, which, to the best of our knowledge, is the first algorithm to deal with all of the following: limited continuous resources, uncertain outcomes, and oversubscription planning. We tested HAO * in a realistic NASA simulation of a planetary rover, a complex domain of practical importance, and our results show its effectiveness in solving problems that are too big to be solved by the simple application of dynamic programming. It is effective because heuristic search can exploit resource constraints, as well as permissible heuristics, to limit the available state space. In our implementation, the HAO * algorithm is combined with the dynamic programming algorithm by Feng et al. (2004) HAO * hierarchy can be combined with other dynamic state plans."}, {"heading": "Acknowledgments", "text": "Eric Hansen was supported in part by a NASA Summer Faculty Fellowship and funds from the Mississippi Space Grant Consortium, while Emmanuel Benazera worked at NASA Ames Research Center and Ronen Brafman attended NASA Ames Research Center, both as advisors to the Research Institute for Advanced Computer Science. Ronen Brafman was partially supported by the Lynn and William Frankel Center for Computer Science, the Paul Ivanier Center for Robotics and Production Management, and ISF Grant # 110707. Nicolas Meuleau is an advisor to Carnegie Mellon University at NASA Ames Research Center."}], "references": [{"title": "Constrained Markov Decision Processes. Chapman and HALL/CRC", "author": ["E. Altman"], "venue": null, "citeRegEx": "Altman,? \\Q1999\\E", "shortCiteRegEx": "Altman", "year": 1999}, {"title": "Neural Dynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Exact solutions to time-dependent MDPs", "author": ["J. Boyan", "M. Littman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Boyan and Littman,? \\Q2000\\E", "shortCiteRegEx": "Boyan and Littman", "year": 2000}, {"title": "Planning under continuous time and resource uncertainty: A challenge for AI", "author": ["J. Bresina", "R. Dearden", "N. Meuleau", "S. Ramakrishnan", "D. Smith", "R. Washington"], "venue": "In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bresina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bresina et al\\.", "year": 2002}, {"title": "Activity planning for the mars exploration rovers", "author": ["J. Bresina", "A. Jonsson", "P. Morris", "K. Rajan"], "venue": "In Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling,", "citeRegEx": "Bresina et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bresina et al\\.", "year": 2005}, {"title": "Admissibility of AO* when heuristics overestimate", "author": ["P. Chakrabarti", "S. Ghose", "S. DeSarkar"], "venue": "Aritificial Intelligence,", "citeRegEx": "Chakrabarti et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 1988}, {"title": "Dynamic programming for structured continuous Markov decision problems", "author": ["Z. Feng", "R. Dearden", "N. Meuleau", "R. Washington"], "venue": "In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Feng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2004}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["J. Friedman", "J. Bentley", "R. Finkel"], "venue": "ACM Trans. Mathematical Software,", "citeRegEx": "Friedman et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1977}, {"title": "LAO*: A heuristic search algorithm that finds solutions with loops", "author": ["E. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence,", "citeRegEx": "Hansen and Zilberstein,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 2001}, {"title": "An efficient algorithm for searching implicit AND/OR graphs with cycles", "author": ["P. Jimenez", "C. Torras"], "venue": "Artificial Intelligence,", "citeRegEx": "Jimenez and Torras,? \\Q2000\\E", "shortCiteRegEx": "Jimenez and Torras", "year": 2000}, {"title": "Solving factored MDPs with hybrid state and action variables", "author": ["B. Kveton", "M. Hauskrecht", "C. Guestrin"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kveton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2006}, {"title": "Lazy approximation for solving continuous finite-horizon MDPs", "author": ["L. Li", "M. Littman"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence,", "citeRegEx": "Li and Littman,? \\Q2005\\E", "shortCiteRegEx": "Li and Littman", "year": 2005}, {"title": "A fast analytical algorithm for solving markov decision processes with real-valued resources", "author": ["J. Marecki", "S. Koenig", "M. Tambe"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence", "citeRegEx": "Marecki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marecki et al\\.", "year": 2007}, {"title": "Planning with continuous resources in stochastic domains", "author": ["Mausam", "E. Benazera", "R. Brafman", "N. Meuleau", "E. Hansen"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Mausam et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mausam et al\\.", "year": 2005}, {"title": "Hierarchical heuristic forward search in stochastic domains", "author": ["N. Meuleau", "R. Brafman"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence", "citeRegEx": "Meuleau and Brafman,? \\Q2007\\E", "shortCiteRegEx": "Meuleau and Brafman", "year": 2007}, {"title": "Variable resolution discretization in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "Machine Learning,", "citeRegEx": "Munos and Moore,? \\Q2002\\E", "shortCiteRegEx": "Munos and Moore", "year": 2002}, {"title": "Principles of Artificial Intelligence", "author": ["N. Nilsson"], "venue": "Tioga Publishing", "citeRegEx": "Nilsson,? \\Q1980\\E", "shortCiteRegEx": "Nilsson", "year": 1980}, {"title": "Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Mission planning and target tracking for autonomous instrument placement", "author": ["L. Pedersen", "D. Smith", "M. Deans", "R. Sargent", "C. Kunz", "D. Lees", "S. Rajagopalan"], "venue": "In Proceedings of the 2005 IEEE Aerospace Conference.,", "citeRegEx": "Pedersen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2005}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Using randomization to break the curse of dimensionality", "author": ["J. Rust"], "venue": "Econimetrica,", "citeRegEx": "Rust,? \\Q1997\\E", "shortCiteRegEx": "Rust", "year": 1997}, {"title": "Choosing objectives in over-subscription planning", "author": ["D. Smith"], "venue": "In Proceedings of the Fourteenth International Conference on Automated Planning and Scheduling,", "citeRegEx": "Smith,? \\Q2004\\E", "shortCiteRegEx": "Smith", "year": 2004}, {"title": "Decision-theoretic planning with non-Markovian rewards", "author": ["S. Thiebaux", "C. Gretton", "J. Slaney", "D. Price", "F. Kabanza"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Thiebaux et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Thiebaux et al\\.", "year": 2006}, {"title": "Effective approaches for partial satisfation (over-subscription) planning", "author": ["M. van den Briel", "R. Sanchez", "M. Do", "S. Kambhampati"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "Briel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Briel et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 20, "context": "Much of this work is formalized in the framework of Markov decision processes (Puterman, 1994; Boutilier, Dean, & Hanks, 1999).", "startOffset": 78, "endOffset": 126}, {"referenceID": 22, "context": "This refers to a problem in which it is infeasible to achieve all goals, and the objective is to achieve the best subset of goals within resource constraints (Smith, 2004).", "startOffset": 158, "endOffset": 171}, {"referenceID": 17, "context": "It is a generalization of the classic AO* heuristic search algorithm (Nilsson, 1980; Pearl, 1984).", "startOffset": 69, "endOffset": 97}, {"referenceID": 18, "context": "It is a generalization of the classic AO* heuristic search algorithm (Nilsson, 1980; Pearl, 1984).", "startOffset": 69, "endOffset": 97}, {"referenceID": 4, "context": "However, as Bresina et al. (2002) point out, important aspects of the rover planning problem are not adequately handled by traditional planning algorithms, including algorithms for Markov decision processes.", "startOffset": 12, "endOffset": 34}, {"referenceID": 7, "context": "To handle hybrid domains, HAO* builds on earlier work on dynamic programming algorithms for continuous and hybrid-state MDPs, in particular, the work of Feng et al. (2004). Generalizing AND/OR graph search for hybrid state spaces poses a complex challenge, and we only consider a special case of the problem.", "startOffset": 153, "endOffset": 172}, {"referenceID": 0, "context": "One way to model an MDP with resource constraints is to formulate it as a constrained MDP, a model that has been widely studied in the operations research community (Altman, 1999).", "startOffset": 165, "endOffset": 179}, {"referenceID": 0, "context": "One way to model an MDP with resource constraints is to formulate it as a constrained MDP, a model that has been widely studied in the operations research community (Altman, 1999). In this model, each action a incurs a transition-dependent resource cost, C a(s, s \u2032), for each resource i. Given an initial allocation of resources and an initial state, linear programming is used to find the best feasible policy, which may be a randomized policy. Although a constrained MDP models resource consumption, it does not include resources in the state space. As a result, a policy cannot be conditioned upon resource availability. This is not a problem if resource consumption is either deterministic or unobservable. But it is not a good fit for the rover domain, in which resource consumption is stochastic and observable, and the rover should take different actions depending on current resource availability. We adopt a different approach to modeling resource constraints in which resources are included in the state description. Although this increases the size of the state space, it allows decisions to be made based on resource availability, and it allows a stochastic model of resource consumption. Since resources in the rover domain are continuous, we use the continuous variables of a hybrid-state MDP to represent resources. Note that the duration of actions is one of the biggest sources of uncertainty in our rover problems, and we model time as one of the continuous resources. Resource constraints are represented in the form of executability constraints on actions, where An(x) denotes the set of actions executable in state (n,x). An action cannot be executed in a state that does not satisfy its minimum resource requirements. Having discussed how to incorporate resource consumption and resource constraints in a hybridstate MDP, we next discuss how to formalize over-subscription planning. In our rover planning problem, scientists provide the planner with a set of \u201cgoals\u201d they would like the rover to achieve, where each goal corresponds to a scientific task such as taking a picture of a rock or performing an analysis of a soil sample. The scientists also specify a utility or reward for each goal. Usually only a subset of these goals is feasible under resource constraints, and the problem is to find a feasible plan that maximizes expected utility. Over-subscription planning for planetary exploration rovers has been considered by Smith (2004) and van den Briel et al.", "startOffset": 166, "endOffset": 2468}, {"referenceID": 0, "context": "One way to model an MDP with resource constraints is to formulate it as a constrained MDP, a model that has been widely studied in the operations research community (Altman, 1999). In this model, each action a incurs a transition-dependent resource cost, C a(s, s \u2032), for each resource i. Given an initial allocation of resources and an initial state, linear programming is used to find the best feasible policy, which may be a randomized policy. Although a constrained MDP models resource consumption, it does not include resources in the state space. As a result, a policy cannot be conditioned upon resource availability. This is not a problem if resource consumption is either deterministic or unobservable. But it is not a good fit for the rover domain, in which resource consumption is stochastic and observable, and the rover should take different actions depending on current resource availability. We adopt a different approach to modeling resource constraints in which resources are included in the state description. Although this increases the size of the state space, it allows decisions to be made based on resource availability, and it allows a stochastic model of resource consumption. Since resources in the rover domain are continuous, we use the continuous variables of a hybrid-state MDP to represent resources. Note that the duration of actions is one of the biggest sources of uncertainty in our rover problems, and we model time as one of the continuous resources. Resource constraints are represented in the form of executability constraints on actions, where An(x) denotes the set of actions executable in state (n,x). An action cannot be executed in a state that does not satisfy its minimum resource requirements. Having discussed how to incorporate resource consumption and resource constraints in a hybridstate MDP, we next discuss how to formalize over-subscription planning. In our rover planning problem, scientists provide the planner with a set of \u201cgoals\u201d they would like the rover to achieve, where each goal corresponds to a scientific task such as taking a picture of a rock or performing an analysis of a soil sample. The scientists also specify a utility or reward for each goal. Usually only a subset of these goals is feasible under resource constraints, and the problem is to find a feasible plan that maximizes expected utility. Over-subscription planning for planetary exploration rovers has been considered by Smith (2004) and van den Briel et al. (2004) for deterministic domains.", "startOffset": 166, "endOffset": 2500}, {"referenceID": 21, "context": "to discretize the continuous state space into a finite number of grid points and solve the resulting finite-state MDP using dynamic programming and interpolation (Rust, 1997; Munos & Moore, 2002).", "startOffset": 162, "endOffset": 195}, {"referenceID": 4, "context": "Figure 1 shows the optimal value functions from the initial state of a typical Mars rover problem as a function of two continuous variables: the time and energy remaining (Bresina et al., 2002).", "startOffset": 171, "endOffset": 193}, {"referenceID": 3, "context": "The initial idea for this approach is due to the work of Boyan and Littman (2000), who describe a class of MDPs called time-dependent MDPs, in which transitions take place along a single, irreversible continuous dimension.", "startOffset": 57, "endOffset": 82}, {"referenceID": 3, "context": "The initial idea for this approach is due to the work of Boyan and Littman (2000), who describe a class of MDPs called time-dependent MDPs, in which transitions take place along a single, irreversible continuous dimension. They describe a dynamic programming algorithm for computing an exact piecewise-linear value function when the transition probabilities are discrete and rewards are piecewise linear. Feng et al. (2004) extend this approach to continuous state spaces of more than one dimension, and consider MDPs with discrete transition probabilities and two types of reward models: piecewise constant and piecewise linear.", "startOffset": 57, "endOffset": 424}, {"referenceID": 3, "context": "The initial idea for this approach is due to the work of Boyan and Littman (2000), who describe a class of MDPs called time-dependent MDPs, in which transitions take place along a single, irreversible continuous dimension. They describe a dynamic programming algorithm for computing an exact piecewise-linear value function when the transition probabilities are discrete and rewards are piecewise linear. Feng et al. (2004) extend this approach to continuous state spaces of more than one dimension, and consider MDPs with discrete transition probabilities and two types of reward models: piecewise constant and piecewise linear. Li and Littman (2005) further extend the approach to allow transition probabilities that are piecewise-constant, instead of discrete, although this extension requires some approximation in the dynamic programming algorithm.", "startOffset": 57, "endOffset": 652}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported.", "startOffset": 10, "endOffset": 35}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported. A relative outcome can be viewed as shifting a region by a constant \u03b4. That is, for any two states x and y in the same region, the transition probabilitiesPr(x\u2032|x, a) and Pr(y\u2032|y, a) are defined in term of the probability of \u03b4, such that \u03b4 = (x\u2032 \u2212 x) = (y\u2032 \u2212 y). An absolute outcome maps all states in a region to a single state. That is, for any two states x and y in the same region, Pr(x\u2032|x, a) = Pr(x\u2032|y, a). We can view a relative outcome as a pair (\u03b4, p), where p is the probability of that outcome, and we can view an absolute outcome as a pair (x\u2032, p). This assumes there is only a finite number of non-zero probabilities, i.e., the probability distribution is discretized, which means that for any state and action, a finite set of states can be reached with non-zero probability. This representation guarantees that a dynamic programming update of a piecewise-constant value function results in another piecewise-constant value function. Feng et al. (2004) show that for such transition functions and for any finite horizon, there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piecewise constant or linear.", "startOffset": 10, "endOffset": 1057}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported. A relative outcome can be viewed as shifting a region by a constant \u03b4. That is, for any two states x and y in the same region, the transition probabilitiesPr(x\u2032|x, a) and Pr(y\u2032|y, a) are defined in term of the probability of \u03b4, such that \u03b4 = (x\u2032 \u2212 x) = (y\u2032 \u2212 y). An absolute outcome maps all states in a region to a single state. That is, for any two states x and y in the same region, Pr(x\u2032|x, a) = Pr(x\u2032|y, a). We can view a relative outcome as a pair (\u03b4, p), where p is the probability of that outcome, and we can view an absolute outcome as a pair (x\u2032, p). This assumes there is only a finite number of non-zero probabilities, i.e., the probability distribution is discretized, which means that for any state and action, a finite set of states can be reached with non-zero probability. This representation guarantees that a dynamic programming update of a piecewise-constant value function results in another piecewise-constant value function. Feng et al. (2004) show that for such transition functions and for any finite horizon, there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piecewise constant or linear. The restriction to discrete transition functions is a strong one, and often means the transition function must be approximated. For example, rover power consumption is normally distributed, and thus must be discretized. (Since the amount of power available must be non-negative, our implementation truncates any negative part of the normal distribution and renormalizes.) Any continuous transition function can be approximated by an appropriately fine discretization, and Feng et al. (2004) argue that this provides an attractive alternative to function approximation approaches in that it approximates the model but then solves the approximate model exactly, rather than finding an approximate value function for the original model.", "startOffset": 10, "endOffset": 1758}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported. A relative outcome can be viewed as shifting a region by a constant \u03b4. That is, for any two states x and y in the same region, the transition probabilitiesPr(x\u2032|x, a) and Pr(y\u2032|y, a) are defined in term of the probability of \u03b4, such that \u03b4 = (x\u2032 \u2212 x) = (y\u2032 \u2212 y). An absolute outcome maps all states in a region to a single state. That is, for any two states x and y in the same region, Pr(x\u2032|x, a) = Pr(x\u2032|y, a). We can view a relative outcome as a pair (\u03b4, p), where p is the probability of that outcome, and we can view an absolute outcome as a pair (x\u2032, p). This assumes there is only a finite number of non-zero probabilities, i.e., the probability distribution is discretized, which means that for any state and action, a finite set of states can be reached with non-zero probability. This representation guarantees that a dynamic programming update of a piecewise-constant value function results in another piecewise-constant value function. Feng et al. (2004) show that for such transition functions and for any finite horizon, there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piecewise constant or linear. The restriction to discrete transition functions is a strong one, and often means the transition function must be approximated. For example, rover power consumption is normally distributed, and thus must be discretized. (Since the amount of power available must be non-negative, our implementation truncates any negative part of the normal distribution and renormalizes.) Any continuous transition function can be approximated by an appropriately fine discretization, and Feng et al. (2004) argue that this provides an attractive alternative to function approximation approaches in that it approximates the model but then solves the approximate model exactly, rather than finding an approximate value function for the original model. (For this reason, we will sometimes refer to finding optimal policies and value functions, even when the model has been approximated.) To avoid discretizing the transition function, Li and Littman (2005) describe an algorithm that allows piecewise-constant transition functions, in exchange for some approximation in the dynamic programming algorithm.", "startOffset": 10, "endOffset": 2205}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported. A relative outcome can be viewed as shifting a region by a constant \u03b4. That is, for any two states x and y in the same region, the transition probabilitiesPr(x\u2032|x, a) and Pr(y\u2032|y, a) are defined in term of the probability of \u03b4, such that \u03b4 = (x\u2032 \u2212 x) = (y\u2032 \u2212 y). An absolute outcome maps all states in a region to a single state. That is, for any two states x and y in the same region, Pr(x\u2032|x, a) = Pr(x\u2032|y, a). We can view a relative outcome as a pair (\u03b4, p), where p is the probability of that outcome, and we can view an absolute outcome as a pair (x\u2032, p). This assumes there is only a finite number of non-zero probabilities, i.e., the probability distribution is discretized, which means that for any state and action, a finite set of states can be reached with non-zero probability. This representation guarantees that a dynamic programming update of a piecewise-constant value function results in another piecewise-constant value function. Feng et al. (2004) show that for such transition functions and for any finite horizon, there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piecewise constant or linear. The restriction to discrete transition functions is a strong one, and often means the transition function must be approximated. For example, rover power consumption is normally distributed, and thus must be discretized. (Since the amount of power available must be non-negative, our implementation truncates any negative part of the normal distribution and renormalizes.) Any continuous transition function can be approximated by an appropriately fine discretization, and Feng et al. (2004) argue that this provides an attractive alternative to function approximation approaches in that it approximates the model but then solves the approximate model exactly, rather than finding an approximate value function for the original model. (For this reason, we will sometimes refer to finding optimal policies and value functions, even when the model has been approximated.) To avoid discretizing the transition function, Li and Littman (2005) describe an algorithm that allows piecewise-constant transition functions, in exchange for some approximation in the dynamic programming algorithm. Marecki et al.(2007) describe a different approach to this class of problems in which probability distributions over resource consumptions are represented with phase-type distributions and a dynamic programming algorithm exploits this representation.", "startOffset": 10, "endOffset": 2374}, {"referenceID": 3, "context": "Following Boyan and Littman (2000), both relative and absolute transitions are supported. A relative outcome can be viewed as shifting a region by a constant \u03b4. That is, for any two states x and y in the same region, the transition probabilitiesPr(x\u2032|x, a) and Pr(y\u2032|y, a) are defined in term of the probability of \u03b4, such that \u03b4 = (x\u2032 \u2212 x) = (y\u2032 \u2212 y). An absolute outcome maps all states in a region to a single state. That is, for any two states x and y in the same region, Pr(x\u2032|x, a) = Pr(x\u2032|y, a). We can view a relative outcome as a pair (\u03b4, p), where p is the probability of that outcome, and we can view an absolute outcome as a pair (x\u2032, p). This assumes there is only a finite number of non-zero probabilities, i.e., the probability distribution is discretized, which means that for any state and action, a finite set of states can be reached with non-zero probability. This representation guarantees that a dynamic programming update of a piecewise-constant value function results in another piecewise-constant value function. Feng et al. (2004) show that for such transition functions and for any finite horizon, there exists a partition of the continuous space into hyper-rectangles over which the optimal value function is piecewise constant or linear. The restriction to discrete transition functions is a strong one, and often means the transition function must be approximated. For example, rover power consumption is normally distributed, and thus must be discretized. (Since the amount of power available must be non-negative, our implementation truncates any negative part of the normal distribution and renormalizes.) Any continuous transition function can be approximated by an appropriately fine discretization, and Feng et al. (2004) argue that this provides an attractive alternative to function approximation approaches in that it approximates the model but then solves the approximate model exactly, rather than finding an approximate value function for the original model. (For this reason, we will sometimes refer to finding optimal policies and value functions, even when the model has been approximated.) To avoid discretizing the transition function, Li and Littman (2005) describe an algorithm that allows piecewise-constant transition functions, in exchange for some approximation in the dynamic programming algorithm. Marecki et al.(2007) describe a different approach to this class of problems in which probability distributions over resource consumptions are represented with phase-type distributions and a dynamic programming algorithm exploits this representation. Although we use the work of Feng et al. (2004) in our implementation, the heuristic search algorithm we develop in the next section could use any of these or some other approach to representing and computing value functions and policies for a hybrid-state MDP.", "startOffset": 10, "endOffset": 2651}, {"referenceID": 17, "context": "1 AO* Recall that AO* is an algorithm for AND/OR graph search problems (Nilsson, 1980; Pearl, 1984).", "startOffset": 71, "endOffset": 99}, {"referenceID": 18, "context": "1 AO* Recall that AO* is an algorithm for AND/OR graph search problems (Nilsson, 1980; Pearl, 1984).", "startOffset": 71, "endOffset": 99}, {"referenceID": 9, "context": "Hansen and Zilberstein (2001) show how AND/OR graph search techniques can be used in solving MDPs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "Hansen and Zilberstein (2001) show how AND/OR graph search techniques can be used in solving MDPs. Following Nilsson (1980) and Hansen and Zilberstein (2001), we define an AND/OR graph as a hypergraph.", "startOffset": 0, "endOffset": 124}, {"referenceID": 9, "context": "Hansen and Zilberstein (2001) show how AND/OR graph search techniques can be used in solving MDPs. Following Nilsson (1980) and Hansen and Zilberstein (2001), we define an AND/OR graph as a hypergraph.", "startOffset": 0, "endOffset": 158}, {"referenceID": 17, "context": "For state-space search problems that are formalized as AND/OR graphs, an optimal solution graph can be found using the heuristic search algorithm AO* (Nilsson, 1980; Pearl, 1984).", "startOffset": 150, "endOffset": 178}, {"referenceID": 18, "context": "For state-space search problems that are formalized as AND/OR graphs, an optimal solution graph can be found using the heuristic search algorithm AO* (Nilsson, 1980; Pearl, 1984).", "startOffset": 150, "endOffset": 178}, {"referenceID": 7, "context": "In our implementation of HAO*, described in Section 4, we use the piecewise-constant partitioning of a continuous state space proposed by Feng et al. (2004). However, any method of discrete partitioning could be used, provided that the condition above holds; for example, Li and Littman (2005) describe an alternative method of partitioning.", "startOffset": 138, "endOffset": 157}, {"referenceID": 7, "context": "In our implementation of HAO*, described in Section 4, we use the piecewise-constant partitioning of a continuous state space proposed by Feng et al. (2004). However, any method of discrete partitioning could be used, provided that the condition above holds; for example, Li and Littman (2005) describe an alternative method of partitioning.", "startOffset": 138, "endOffset": 294}, {"referenceID": 7, "context": "Second, the continuous state space associated with a particular node is further partitioned into smaller regions based on a piecewise-constant representation of a continuous function, such as the one used by Feng et al. (2004). In addition to this more complex representation of the nodes of an AND/OR graph, our algorithm requires a more complex definition of the the best (partial) solution.", "startOffset": 208, "endOffset": 227}, {"referenceID": 7, "context": "In our implementation, we use the dynamic programming algorithm of Feng et al. (2004). As reviewed in Section 2.", "startOffset": 67, "endOffset": 86}, {"referenceID": 7, "context": "In our implementation, we use the dynamic programming algorithm of Feng et al. (2004). As reviewed in Section 2.4, they show that the continuous integral over x\u2032 can be computed exactly, as long as the transition and reward functions satisfy certain conditions. Note that, with some hybrid-state dynamic programming techniques such as Feng et al. (2004), dynamic programming backups may increase the number of pieces of the value function attached to the updated regions (Figure 3(a)).", "startOffset": 67, "endOffset": 354}, {"referenceID": 6, "context": "We show this by using a decomposition of the value function described by Chakrabarti et al.(1988) and Hansen and Zilberstein (2001).", "startOffset": 73, "endOffset": 98}, {"referenceID": 6, "context": "We show this by using a decomposition of the value function described by Chakrabarti et al.(1988) and Hansen and Zilberstein (2001). We note that at any point in the algorithm, the value function can be decomposed into two parts, gn(x) and hn(x), such that gn(x) = 0 when (n,x) is an open state, on the fringe of the greedy policy; otherwise, gn(x) = \u2211", "startOffset": 73, "endOffset": 132}, {"referenceID": 9, "context": "Hansen and Zilberstein (2001) observed that, in the case of LAO*, the algorithm is more efficient if we expand several nodes in the fringe before performing dynamic programming in the explicit graph.", "startOffset": 0, "endOffset": 30}, {"referenceID": 7, "context": "They allow leveraging hybrid-state dynamic programming techniques such as those of Feng et al. (2004) and Li and Littman (2005).", "startOffset": 83, "endOffset": 102}, {"referenceID": 7, "context": "They allow leveraging hybrid-state dynamic programming techniques such as those of Feng et al. (2004) and Li and Littman (2005). These techniques may compute in a single iteration piecewise constant and linear value functions that cover a large range of continuous states, possibly the whole space of possible values.", "startOffset": 83, "endOffset": 128}, {"referenceID": 19, "context": "The simulation uses a model of the K9 rover (see Figure 5) developed for the Intelligent Systems (IS) demo at NASA Ames Research Center in October 2004 (Pedersen et al., 2005).", "startOffset": 152, "endOffset": 175}, {"referenceID": 7, "context": "Our implementation of HAO* uses the dynamic programming algorithm developed by Feng et al. (2004) and summarized in Section 2.", "startOffset": 79, "endOffset": 98}, {"referenceID": 7, "context": "In our implementation, the HAO* algorithm is integrated with the dynamic programming algorithm of Feng et al. (2004). However HAO* can be integrated with other dynamic programming algorithms for solving hybrid-state MDPs.", "startOffset": 98, "endOffset": 117}, {"referenceID": 13, "context": "efficiency (Li & Littman, 2005; Marecki et al., 2007).", "startOffset": 11, "endOffset": 53}], "year": 2009, "abstractText": "We consider the problem of optimal planning in stochastic domains with resource constraints, where the resources are continuous and the choice of action at each step depends on resource availability. We introduce the HAO* algorithm, a generalization of the AO* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables, where the continuous variables represent monotonic resources. Like other heuristic search algorithms, HAO* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy. We show that this approach is especially effective when resource constraints limit how much of the state space is reachable. Experimental results demonstrate its effectiveness in the domain that motivates our research: automated planning for planetary exploration rovers.", "creator": "TeX"}}}