{"id": "1401.3434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach", "abstract": "The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, nu-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented.", "histories": [["v1", "Wed, 15 Jan 2014 04:50:50 GMT  (811kb)", "http://arxiv.org/abs/1401.3434v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs csan\\'ad cs\\'aji", "l\\'aszl\\'o monostori"], "accepted": false, "id": "1401.3434"}, "pdf": {"name": "1401.3434.pdf", "metadata": {"source": "META", "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach", "authors": ["Bal\u00e1zs Csan\u00e1d Cs\u00e1ji", "L\u00e1szl\u00f3 Monostori"], "emails": ["balazs.csaji@sztaki.hu", "laszlo.monostori@sztaki.hu"], "sections": [{"heading": "1. Introduction", "text": "Resource allocation problems (RAPs) are of high practical importance because they occur in many different areas, such as production control (e.g. production planning), warehousing (e.g. warehouse allocation), fleet management (e.g. freight traffic), human resources management (e.g. in an office), computer program planning (e.g. in massively parallel GRID systems), construction project management or mobile network management. RAPs are also central to management science (Powell & Van Roy, 2004). In the paper, we look at optimization problems that involve the allocation of a limited number of reusable resources to non-preventive, interconnected tasks with stochastic durability and effects.Our goal is to investigate efficient reactive (closed) decision-making processes that can address the allocation of scarce resources over time, with the goal of optimizing objectives."}, {"heading": "1.1 Industrial Motivations", "text": "With respect to modern manufacturing systems, difficulties arise from unexpected tasks and events, nonlinearity, and a variety of interactions in trying to control various activities in dynamic workshops. Complexity and uncertainty seriously limit the effectiveness of conventional manufacturing control approaches (e.g. deterministic planning). In the paper, we apply mathematical programming and machine learning (ML) techniques to achieve sub-optimal control of a general class of stochastic RAPs that can be critical to an intelligent manufacturing system (IMS). The term IMS can be traced back to a preliminary forecast by Hatvany and Nemes (1978). In the early 1980s, IMS was outlined as the next generation of manufacturing systems that use the results of artificial intelligence research and were expected to solve, within certain limits, unprecedented, unpredictable problems based on even incomplete and inaccurate information."}, {"heading": "1.2 Curse(s) of Dimensionality", "text": "Different types of RAPs have a large number of exact and approximate solutions, for example in the case of scheduling problems (Pinedo, 2002), but these methods deal primarily with the static (and often strictly deterministic) variants of the various problems and are largely unaware of the uncertainties and changes. Specific (deterministic) RAPs that occur in the area of combinatorial optimization, such as the problem of the moving vendor (TSP) or the problem of workplace planning (JSP), are highly NP-hard and, moreover, do not have good polynomial time approximation. In the stochastic case, RAPs can often be formulated as Markov decision processes (MDPs) and theoretically optimally solved by applying Dynamic Programming Methods (DP). However, due to the phenomenon described by Bellman (1961) as a curse of dimensionality, these methods are highly insoluble in practice."}, {"heading": "1.3 Related Literature", "text": "Our solution belongs to the class of approximate dynamic programming (ADP) algorithms, which represent a broad class of discrete time control techniques. Note that ADP methods that take the point of view of the actor and critic are often referred to as reinforcement learning (RL). Zhang and Dietterich (1995) were the first to apply an RL technique to a specific RAP. They used the TD (\u03bb) method with iterative repair to solve a static scheduling problem, namely the NASA Space Shuttle Payload Processing Problem. Since then, a number of papers have been published suggesting the use of RL strategies for various RAPs. The first reactive (closed) solution to scheduling problems using ADP-i algorithms was briefly described by Schneider, Boyan and Moore (1998)."}, {"heading": "1.4 Main Contributions", "text": "As a summary of the main contributions of the paper, it can be emphasized that: 1. We propose a formal framework for the investigation of stochastic resource allocation problems with scarce, reusable resources and non-preeminent, time-dependent, interrelated tasks. This approach represents a natural generalization of several standard resource management problems, such as planning problems, transport problems, inventory control problems, or maintenance and repair problems. This general RAP is reframed as a stochastic short-term problem (a special MDP) with favorable characteristics, as it is aperiodic, its state and scope for action are finite, all policies are appropriate, and the space of control policy can certainly be limited. Reactive solutions are defined as strategies of the reformulated problem to calculate a good approximation of optimal control policies, ADP methods are proposed, especially adapted Q-learning systems."}, {"heading": "2. Markovian Resource Control", "text": "This section aims at precisely defining and reformulating RAPs so that they can be effectively solved by the ML methods presented in Section 3. First, we give a brief introduction to RAPs, followed by the formulation of a general framework for resource allocation. We start with deterministic variants and then expand the definition to the stochastic case. Then, we give a brief overview of Markov decision-making processes (MDPs) as they are a basic theory of our approach. Next, we reformulate the reactive control problem of RAPs as a stochastic short-term problem (a specific MDP)."}, {"heading": "2.1 Classical Problems", "text": "In this section we will give a brief introduction to RAPs through two strongly NP combinatorial optimization problems: the problem of job planning and the problem of the moving salesman. Later we will apply these two basic problems to demonstrate the results."}, {"heading": "2.1.1 Job-Shop Scheduling", "text": "First, we look at the classic work scheduling problem (JSP), which is a standard deterministic RAP (Pinedo, 2002). We have a set of tasks, J = {J1,.., Jn}, that must be handled by a set of machines, M = {M1,.., Mk}. Each J consists of a sequence of nj tasks, for each task tji-T, in which i-N,...., a machine mji-M that can handle the task, and a processing time pji-N. The goal of optimization is to find a workable schedule that minimizes a given performance. A solution, i.e. a schedule, is a suitable \"task to start the time\" assignment. Figure 1 illustrates an example plan by using a Gantt chart. Note that a Gantt chart (Pinedo, 2002) is a figure with bars to illustrate the problem of starting and understanding the tasks related to it."}, {"heading": "2.1.2 Traveling Salesman", "text": "One of the basic logistical problems is the problem of the travelling salesman (TSP), which can be defined as follows: given a number of cities and the cost of the journey between them, which is the cheapest round trip that each city visits exactly once and then returns to the starting city. Several variants of the TSP are known, the most standard can be formally characterised by a contiguous, undirected, edge-weighted chart G = < V, E, w >, where V = {1,..., n} is the vertex corresponding to the set of \"cities,\" E V \u00b7 V is the number of edges that represent the \"roads\" between cities, and w: E \u2192 N defines the edge weights: the duration of journeys. The aim of the optimisation is to find a Hamilton circuit with the lowest possible weight. Note that a Hamilton circuit is a chart cycle that begins at a vertex, runs through each vertex and returns exactly to the starting point."}, {"heading": "2.2 Deterministic Framework", "text": "This framework can be treated as a generalization of several classic RAPs, such as JSP and TSP. First, a deterministic resource allocation problem is considered: an instance of the problem can be characterized by an 8-fold < R, S, O, T, C, d, e, i >. Specifically, the problem consists of a set of reusable resources R together with S corresponding to the set of possible resource states. A set of allowed operations O is also specified by a subset of T'O indicating the target operations or tasks. R, S and O should be finite and they are separated in pairs. There may be priority constraints between tasks represented by a partial arrangement C'T'T'T. The duration of the operations depending on the state of the executing resource is relevant."}, {"heading": "2.2.1 Feasible Resource Allocation", "text": "A solution to a deterministic RAP is a subfunction, the resource allocator function,%: R \u00b7 N \u2192 O, which allocates the start times of operations to the resources. Note that the operations should not be preventive (they must not be interrupted).A solution is called feasible if and only if the following four properties are met: 1. Each resource performs at most one operation at a time: \u00ac u, v O: u =% (r, t1), v =% (r, t2), t1 < t1 + d (s (r, t1), u).3. The precedent constraints on tasks are met: < u, v > tC: [u =%, t2), t2 < t1 + d (s, t1), (t1), (lt1), (t1), (t2), (t2), (t2), (t2)."}, {"heading": "2.2.2 Performance Measures", "text": "The set of practicable solutions is referred to by S. Each solution includes a performance (or cost) defined by a performance scale S \u2192 R, which often depends only on the completion time of the task. Typical performance measures that occur in practice are: maximum completion time or average lead time. The goal of the resource allocation system is to calculate a practicable solution with maximum performance (or minimum cost). Note that performance measures can impose penalties for violating release and due dates (if available) or may even reflect the priority of tasks. A possible generalization of the given problem is when operations require more resources at the same time, which is important for modeling, e.g. resource-constrained project planning problems. However, it is easy to expand the scope to this case: The definition of d and e should be changed to d: S < k and e >."}, {"heading": "2.2.3 Demonstrative Examples", "text": "As illustrative examples, we will now give (F) JSP and TSP within the given framework. < j = > S = > It is easy to formulate planning problems like JSP within the resource allocation framework presented: JSP's tasks can be directly linked to the tasks in the framework, machines can be associated with resources and processing times with runtimes. Priority constraints are determined by the linear order of tasks in each job. Note that there is only one possible resource state for each machine. Finally, workable schedules can be linked to workable solutions. If there were setup times in the problem, there would be several states for each resource (according to their current lineup) and the \"set-up\" procedures could be associated with the non-task operations. A RAP formulation of TSP can be given as follows. The resource amount consists of only one element, the \"vendor,\" i.e. R = {r}."}, {"heading": "2.2.4 Computational Complexity", "text": "If we use a performance measurement that has the property that a solution can be precisely defined by a limited sequence of operations (which includes all tasks) with their allocation to resources, and in addition an optimal one can be found among the solutions generated in this way, then the RAP becomes a combinatorial optimization problem. Any performance measurement monotonous in the completion times, called regular, has this property. Since the RAP defined above is a generalization of e.g. JSP and TSP, it is strongly NP-hard and also not a good polynomial approximation of the optimal algorithm for resource allocation (Papadimitriou, 1994)."}, {"heading": "2.3 Stochastic Framework", "text": "Until now, our model was deterministic, now we turn to stochastic RAPs. The stochastic variant of the described general class of RAPs can be defined by randomizing the functions d, e, and i. Consequently, the operating time becomes random, d: S \u00b7 O \u2192 \u2206 (N), where \u2206 (N) is the space of the probability distributions over N. The effects of the operations are also uncertain, e: S \u00b7 O \u2192 (S) and the initial states of the resources can be stochastical, i: R \u2192 (S). Note that the ranges of the functions contain d, e, and i probability distributions, we denote the corresponding random variables with D, E, and I. The notation X-f indicates that random variable X has a probability distribution f. Thus, D (s, o) \u0445 d (s, o), E (s, o) \u0445 e (r) \u0445 i (r) for all S problems and O-R. Consider Figure 4. SP."}, {"heading": "2.3.1 Stochastic Dominance", "text": "In stochastic RAPs, the performance of a solution is also a random variable, so in order to compare the performance of different solutions, we need to compare random variables. There are many ways to make this comparison: for example, one can say that a random variable has stochastic dominance over another random variable \"almost certainly,\" \"in probability,\" \"stochastical,\" \"increasingly convex\" or \"in expectation.\" Different types of comparisons may be appropriate in different applications, but probably the most natural one is based on the expected values of the random variables."}, {"heading": "2.3.2 Solution Classification", "text": "Now we are classifying the basic types of resource allocation techniques. First, in order to give a proper classification, we start with the return to the concepts of \"open loop\" and \"closed loop\" controllers. An open loop controller, also referred to as a non-feedback controller, calculates its input into a system by using only the current state and model of the system. Therefore, an open loop controller does not use feedback to determine whether its input has reached the desired goal, and it does not observe the output of the process being controlled. Conversely, a closed loop controller uses feedback to control the states or outputs of a dynamic system (Sontag, 1998). Closed loop control has a significant advantage over open loop solutions in dealing with uncertainties."}, {"heading": "2.4 Markov Decision Processes", "text": "In this section, the basic definitions, the applied notations and some preliminaries are presented. By a Markov decision-making process (finite, discrete, stationary, fully observable) we mean a stochastic system characterized by a 6-fold < X, A, p, g, \u03b1 >, the components being as follows: X is a finite set of discrete states and A is a finite set of control measures. Mapping A: P (A) is the availability function that makes each state a set of measures available in the state in which P denotes the power set. The transition probability function is given by p: X \u00d7 A \u2192 0 (X), in which the space of probability distributions is defined by XY (X)."}, {"heading": "2.4.1 Control Policies", "text": "A (stationary, Markov) control policy determines the measures to be taken in every possible state. A deterministic policy, \u03c0: X \u2192 A, is simply a function of states to control actions. A randomized policy, \u03c0: X \u2192 \u0445 (A), is a function of states to probability distributions over actions. We refer to the probability of the execution of action a in the state x by \u03c0 (x) (a) or, in short, by \u03c0 (x, a). Of course, deterministic strategies are special cases more randomized and therefore, unless otherwise specified, we consider randomized control policies. For each x-0-0-1 = P-probability distribution of states, the transition probabilities p together with a control policy determine the progress of the system in stochastic sense completely, namely they define a homogeneous Markov chain on X, x-t + 1-3 = P (\u03c0) x-t, whereby x-t is the state probability vector of the system at a certain time, and 1-2-3-3-3-3-3-P-4-4-3-3-3-4-3-3-4-3-4-3-4-3-4-3-4-3-3-4-3-4-3-4-4-3-3-4-3-3-4-4-4-3-3-4-3-4-4-4-3-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4"}, {"heading": "2.4.2 Value Functions", "text": "The value or the cost-to-go function of a policy \u03c0 is a function of states to costs. It is defined for each state: J\u03c0: X \u2192 R. Function J\u03c0 (x) indicates the expected value of the cumulative (discounted) costs, if the system is in state x and then follows the policy \u03c0, J\u03c0 (x) = E [N \u2211 t = 0\u03b1tg (Xt, A \u03c0 t), as well as X0 = x], (1) where Xt and A \u03c0 t are random variables, A \u03c0 t is selected according to the control policy \u03c0 and the distribution of Xt + 1 is p (Xt, A \u03c0 t). The horizon of the problem is designated by N \u2194 N \u0432\u043e {\u043c}. Unless otherwise stated, we always assume that the horizon is infinite, N = \u043c. Similar to the definition of J\u0445 + 1, one can define action value functions of the control policy, Q\u03c0 (x, a) = E [N \u0421T = 0t\u03b1g, where the equations are equal to Q)."}, {"heading": "2.4.3 Bellman Equations", "text": "A control policy is (uniformly) optimal if it is smaller or equal to all other control policies. There is always at least one optimal policy (Sutton & Barto, 1998).Although there are many optimal policies, they all share the same unique optimal cost-to-go function, which is defined by J *.This function must satisfy the Bellman optimality equation (Bertsekas & Tsitsiklis, 1996), TJ *, where T is the Bellman operator, defined for all x * X, as (TJ) (x) = min a * A (x, a) + \u03b1 y * Xp (y | x, a) J (y), J (y) *, where T * is the Bellman equation for any (stationary, Markov, randomized) policy."}, {"heading": "2.5 Reactive Resource Control", "text": "In this section, we formulate reactive solutions to stochastic RAPs as control guidelines for adequately reformulated SSP issues. Current task durations and resource states will only be available incrementally during the resource allocation control process."}, {"heading": "2.5.1 Problem Reformulation", "text": "A state x-X is defined as a 4-tuple x = < \u03c4, p = >, p = \u03b2 > where p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p (p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "2.5.2 Favorable Features", "text": "Let's call the introduced SSPs that describe stochastic RAPs, RAP MDPs. In this section, we give an overview of some basic characteristics of RAP MDPs. First, it is easy to see that these MDPs have finite action spaces, because | A | \u2264 | R | | O | + 1 is always finite. Although the state space of a RAP MDP is generally denumerable when the allowed number of non-task operations is limited and the random variables describing the duration of the operation are finite, the state space of the reformed MDP becomes finite as well. We can also observe that RAP MDPs are acyclic (or aperiotic), namely that none of the states can appear several times, because during the resource allocation process the administration and management (%) are not reduced, and that each time the state changes, the quantity of administration (%) increases strictly."}, {"heading": "2.5.3 Composable Measures", "text": "In order to achieve this, we must demand that the performance measure can be composed with a suitable function. In general, we call a function f: P: P: P: R: R: R: R: R: composable if for any group A, B: X: A, B: A, B: \u2205 B: \u2205. This definition can be applied directly to performance measures. For example, if a performance measure is called Q: A: A (B) = f (A: A): R: R: R: R: R: R: R: R: composition function, and X: any quantity. This definition can be applied directly to performance measures. If a performance measure is composable, for example Q: A: composable, it indicates that the value of any complete solution can be calculated from the values of its fragmented partial solutions (solutions to partial problems) with the function Q. In practical situations, the composition function is often the max: the minimum or the \"function +\" that can be represented as performance measured if Q: is the past."}, {"heading": "2.5.4 Reactive Solutions", "text": "Now we are able to define the concept of reactive solutions for stochastic RAPs. A reactive solution is a (stationary, Markov) control policy of the newly formulated SSP problem. A reactive solution performs a closed loop control, as the controller is informed at all times about the current state of the system and can choose a control measure based on this information. Section 3 deals with the calculation of effective control guidelines."}, {"heading": "3. Solution Methods", "text": "In this section, we will use various approaches to machine learning to provide an effective solution for large-scale RAPs in uncertain and changing environments. First, we will give an overview of some approximate dynamic programming methods for calculating a \"good\" policy. Then, we will examine two alignment techniques to improve the solution. Clustering, rollout algorithms and space-of-action decomposition and distributed samples will also be considered, as they can significantly accelerate the calculation of a good control policy and are therefore important additions when we face major problems."}, {"heading": "3.1 Approximate Dynamic Programming", "text": "In the previous sections, we have formulated RAPs as acyclic (aperiodic) SSP problems. Now, we are faced with the challenge of finding a good policy. Theoretically, the optimal value function of a finite MDP can be precisely calculated by dynamic programming methods (DP), such as value titeration or the Gauss-Seidel method. Alternatively, an exact optimal policy can be calculated directly by policy iteration. However, due to the \"curse of dimensionality,\" it is practically impossible to calculate an exact optimal solution using these methods, e.g. typically, both the required computing effort and the required memory, i.e. the memory, grows rapidly with the size of the problem. In order to overcome the \"curse,\" we should apply approximate dynamic programming techniques (ADP) to achieve a good approximation of an optimal policy. Here, we propose to use sample-adjusted Q-Learning (QL function) on each attempt at a QP."}, {"heading": "3.1.1 Fitted Q-learning", "text": "Watkins' Q-Learning is a very popular model-free reinforcement learning algorithm (Even-Dar & Mansour, 2003) that works with action-value functions and approaches iteratively the optimal value function. The one-step Q-Learning rule is defined as follows Qi + 1 (x, a) = (1 \u2212 \u03b3i (x, a)) Qi (x, a) + \u03b3i (x, a) (T-iQi) (x, (T-iQi) (x, a) = g (x, a) + \u03b1 min B-A (Y) Qi (Y, B), where \u03b3i (x, a) the learning rates and Y are a random variable representing a state generated from the pair (x, a) by simulation, that is, according to the probability distribution p (x, a)."}, {"heading": "3.1.2 Evaluation by Simulation", "text": "Therefore, we run simulations of the Markov chain Monte Carlo (MCMC) (Hastings, 1970; Andrieu, Freitas, Doucet, & Jordan, 2003) to generate patterns based on the model used to calculate the new approximation of the estimated cost-to-go function. Thus, the states to be updated in episode i, namely Xi, are updated by simulation. Since RAP MDPs are acyclic, we apply prioritized feeling, which means that after each iteration, the cost-to-go estimates are updated in the reverse order in which they appeared during the simulation. For example, suppose that Xi = {xi1, x i 2,.., x i ti} is the set of states for updating the value function after iteration i, where j < k implies that xij appeared earlier during the simulation than xik. In this case, the problem in the order that the function is relevant in order that it is insufficient to maintain the optimum state 1, it is not sufficient to optimize the one."}, {"heading": "3.1.3 The Boltzmann Formula", "text": "In order to ensure the convergence of the FQL algorithm, it must be guaranteed that any cost estimate is constantly updated. A technique often used to balance exploration and exploitation is the Boltzmann formula (also referred to as softmin action selection): \u03c0i (x, a) = exp (\u2212 Qi (x, a) / \u03c4 b \u00b2 A (x) exp (\u2212 Qi (x, b) / \u03c4), where \u03c4 = 0 is the temperature of Boltzmann (or Gibbs), i is the episode number. It is easy to see that high temperatures cause the actions to be (almost) equivalent, low ones cause a greater difference in the probability of selection for actions that differ in their estimates."}, {"heading": "3.2 Cost-to-Go Representations", "text": "In Section 3.1, we proposed FQL for an iterative approach to the optimal value function. However, the question of a suitable function space on which the resulting value functions can be projected effectively remained open. In order to solve large-scale problems (or problems with continuous state spaces), this question is crucial. In this section, we first propose characteristics for stochastic RAPs and then describe two methods that can be applied to compact value functions: the first and simpler uses hash tables, while the second, more complex, is based on the theory of support vector machines."}, {"heading": "3.2.1 Feature Vectors", "text": "In order to apply a function approximation value efficiently, the states and actions of the reformulated MDP should first be associated with numeric vectors representing, for example, typical characteristics of the system. In the case of stochastic RAPs, we suggest using characteristics as follows: \u2022 For each resource in R, the resource state ID, the operation ID of the operation currently being processed by the resource (could be idle), and the start time of the last (and currently incomplete) operation should be a feature. If the model is available to the system, the expected standby time of the resource should be stored instead. \u2022 For each task in T, the task status ID could be treated as a feature that can assume one of the following values: \"unavailable\" (e.g., some preference constraints are not met), \"ready to execute,\" \"processed\" or \"ready.\" It is also recommended to use \"1-out-of-n\" encoding as a feature that should be assigned to each function (if any function should be associated with an actionvalue)."}, {"heading": "3.2.2 Hash Tables", "text": "Suppose we have a vector w = < w1, w2,. wk >, where each component wi corresponds to a characteristic of a state or action. Normally, the estimates for all these vectors are not stored in memory. In this case, one of the simplest methods that can be used is to represent the estimate in a hash table. A hash table is basically a dictionary in which keys are mapped to array positions by hash functions. If all components can take finite values, for example, in our finite state, then a key could be generated as follows. Suppose that for all of us we have 0 \u2264 wi < mi, then w can be considered a number in a mixed radix number system and thus a unique key can be calculated."}, {"heading": "3.2.3 Support Vector Regression", "text": "A promising choice for the compact representation of the cost-to-go function is the use of support vector regression (SVR) from statistical learning theory. To maintain the value function estimates, we propose to use the support vectors proposed by Scholkopf, Smola, Williamson and Bartlett (2000). They have an advantage over classic SVRs, according to which the number of support vectors can be controlled. Furthermore, the core ideas of SVRs can generally be eliminated. Generally, SVR addresses the problem as follows: We get a sample, a set of data points {< x1 >, y1 >. < xl, yl >}, so that we are an input X where X is a measurable space, and yi-R is the target output."}, {"heading": "3.3 Additional Improvements", "text": "Calculating a (near) optimal solution using RL methods, such as (customized) Q-Learning, could be very inefficient in large systems, even if we use prioritized sweeping and competent presentation. In this section, we present some additional improvements to speed up the optimization process, even at the cost of suboptimal solutions."}, {"heading": "3.3.1 Rollout Algorithms", "text": "During our experiments presented in Section 4, it has been found that the use of a sub-optimal base policy, such as a greedy policy on immediate costs, to guide exploration considerably accelerates optimization. To introduce the concept more precisely, we propose to apply a rollout policy that is a limited forward-looking policy, approximating the optimal costs through the cost-to-go patterns of basic policy (Bertsekas, 2001). To introduce the concept more precisely, let us apply the greedy policies. Instant costs, \u03c0 (x), argmin a (x) g (x, a). The value function of the \u03c0 simulation is cited by J-Plan. The single-stage forward-looking rollout strategy is based on the policy that represents an improvement of these strategies."}, {"heading": "3.3.2 Action Space Decomposition", "text": "In the case of major problems, the number of available actions in one state can be very large, which can slow down the system considerably. In the current formulation of the RAP, the number of available actions in one state is O (| T | | R |). Although the system itself is usually not very large in real situations, but could contain thousands of tasks, in this case we suggest splitting the action space as shown in Figure 7. Firstly, the system selects only one task, and it moves to a new state in which this task is defined and an executing resource should be selected. In this case, the state description can be extended by a new variable, namely the case where no task has yet been selected. In any other case, the system should select an executing resource for the selected task."}, {"heading": "3.3.3 Clustering the Tasks", "text": "The idea of divide-and-conquer is widely used in artificial intelligence and has recently appeared in the theory of dealing with large-scale MDPs. Partitioning a problem into several smaller sub-problems is also commonly used to reduce computational complexity in combinatorial optimization problems, such as planning theory. We propose a simple yet efficient partitioning method for a practically very important class of performance metrics. In the real world, tasks are very often associated with release dates and due dates, such as total latency and number of late tasks depending on meeting deadlines. We highlight the (possibly randomized) functions that define the release and due dates of tasks. T \u2192 N and B: T \u2192 N In this section, we limit ourselves to performance measures and the number of late tasks."}, {"heading": "3.3.4 Distributed Sampling", "text": "Finally, we argue that the approach presented can be slightly modified to allow the calculation of a policy across multiple processors in a distributed way.Suppose we have k-processors and designate the set of all processors by P = {p1, p2,.., pk}.If we have a parallel system with a shared storage architecture, e.g. UMA (unified memory access), then it is easy to parallelise the calculation of a control policy. Namely, each processor p, P, can independently scan the search space while we use the same shared value function. (common) policy can be calculated using this shared global value function, e.g. the greedy policy w.r.t. This function can be accelerated by using a distributed memory architecture."}, {"heading": "4. Experimental Results", "text": "In this section, some experimental results on both benchmark and industry-related issues are presented, highlighting some of the features of the solution."}, {"heading": "4.1 Testing Methodology", "text": "To test our resource control approach experimentally, we developed a simulation environment in C + + J with the first J value. We used FQL and, in most cases, SVRs implemented by the free library LIBSVM for support vector machines (Chang & Lin, 2001). After centering and scaling the data to the interval [0, 1], we used Gaussian cores and shrinkage techniques. We always used rollout algorithms and action decomposition, but clustering was only used in tests presented in Section 4.5, and the distributed sampling method was only used in tests shown in Section 4.3. In both of the latter cases (clustering tests and distributed sampling) we used hash tables with approximately 256Mb of hash memory. The performance of the algorithm was measured as follows. The testing took place in two main fields: the first was a benchmark for scheduling, the second was a hard one."}, {"heading": "4.2 Benchmark Datasets", "text": "The ADP-based resource control approach has been tested on Hurink's benchmark dataset (Hurink, Jurisch, & Thole, 1994) and includes flexible workplace planning problems (FJSPs) with 6-30 jobs (30-225 tasks) and 5-15. The measure of performance used is the maximum task completion time (Makespan), which is \"hard,\" meaning that standard scheduling rules or heuristics perform poorly on them. This dataset consists of four subsets, each subset containing about 60 problems, and the subsets (sdata, edata, rdata, vdata) differ in the ratio of machine interchangeability (flexibility) found in the \"Flex (ib)\" columns in Tables 3 and 2. The columns labeled \"n\" iters \"(and\" avg err \") show the average error after the\" n \"iterations."}, {"heading": "4.3 Distributed Sampling", "text": "The possible parallels of the presented method were also investigated, i.e. the acceleration of the system in relation to the number of processors (in practice, the multi-processor environment was emulated on a single processor only), the average number of iterations was investigated until the system reached a solution with an error of less than 5% on Hurink's dataset, and the average speed of a single processor was treated as a unit for comparison purposes. Figure 9 shows two cases: In the first case (rear dark bars), each processor was able to access a common global value function, meaning that each processor could read and write the same global value function, but otherwise searched (sampled the search space) independently. Figure 9 shows that in this case the acceleration was almost linear. In the second case (front light bar), each processor had its own (local) value function (which is more realistic in a highly distributed system, like a GRID)."}, {"heading": "4.4 Industry Related Tests", "text": "We initiated experiments at a simulated factory by modeling the structure of a real plant that produced customized mass products, especially light bulbs. These industrial data came from a huge National Industrial Academia project for research and development of solutions that help manufacturing companies meet the requirements of adaptability, realism, and cooperation (Monostori, Kis, Ka \u0301 r, Va \ufffd ncza, & Erdo \"s, 2008).Since we did not have access to historical data on past orders, we used randomly generated orders (jobs) with random expirations. However, the tasks and process schedules of the jobs included real products; as well as the resources included real machine types. In this plant, product-type set-up times require, and there are some specific tasks that have durations but do not require resources to be processed, for example, cooling. Another feature of the plant is that, for example, pre-defined workstations have been allowed to work places."}, {"heading": "4.5 Clustering Experiments", "text": "We looked at a system of 60 resources and 1000 random tasks spread over 400-500 workstations (there were about 1000-2000 precedent constraints), and the tasks were constructed in such a way that, ideally, none of them would be late and the slip ratio would be about 20%. First, the tasks were sorted according to their slip times, and then grouped together. We applied 104 iterations to each cluster. Computing time when using only one cluster was treated as a unit. Table 6 shows the average and standard deviation of the error and the computational acceleration relative to the number of tasks in a cluster. Results show that partitioning the search space not only leads to higher speed, but is often accompanied by better solutions. The latter phenomenon can be explained by the fact that the use of smaller sample traps and the computational acceleration produces smaller variances that are preferable for learning."}, {"heading": "4.6 Adaptation to Disturbances", "text": "In order to verify the proposed algorithm in changing environments, experiments were initiated and carried out on random JSPs with the aim of minimizing the Makespan. The adaptive properties of the system were tested by confronting it with unexpected events, such as: resource collapses, new resource availability (Figure 10), new hires or job cancellations (Figure 11). In Figures 10 and 11, the horizontal axis represents time, while the vertical one represents the performance measurement reached. The figures were generated by averaging one hundred random samples. In these tests, 20 machines with a few dozen workstations were used. During each test phase, there was an unexpected event (malfunction) at a time of t = 100. After the change occurred, we considered two options: Either we started the iterative planning process from scratch, or we continued learning by using the current (outdated) value function much more efficiently, which is the latter."}, {"heading": "5. Concluding Remarks", "text": "In fact, most of them are able to move to another world, in which they are able to move, and in which they are able to move."}, {"heading": "6. Acknowledgments", "text": "The work was supported by the Hungarian Research Fund (OTKA) with funding number T73376 and by the EU-funded project \"Coll-Plexity\" with funding number 12781 (NEST). Bala'zs Csana \u0301 d Csa'ji greatly appreciates the scholarship of the Hungarian Academy of Sciences. The authors thank Tama's Kis for his contribution to the tests with industrial data and Csaba Szepesva \u0301 ri for the helpful discussions on machine learning."}], "references": [{"title": "An introduction to MCMC (Markov Chain Monte Carlo) for machine learning", "author": ["C. Andrieu", "N.D. Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "Dynamic job-shop scheduling using reinforcement learning agents", "author": ["M.E. Aydin", "E. \u00d6ztemel"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Aydin and \u00d6ztemel,? \\Q2000\\E", "shortCiteRegEx": "Aydin and \u00d6ztemel", "year": 2000}, {"title": "Proactive algorithms for job shop scheduling with probabilistic durations", "author": ["J.C. Beck", "N. Wilson"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Beck and Wilson,? \\Q2007\\E", "shortCiteRegEx": "Beck and Wilson", "year": 2007}, {"title": "Adaptive Control Processes", "author": ["R.E. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1961", "shortCiteRegEx": "Bellman", "year": 1961}, {"title": "Dynamic programming and suboptimal control: A survey from ADP to MPC", "author": ["D.P. Bertsekas"], "venue": "European Journal of Control, 11 (4\u20135), 310\u2013334.", "citeRegEx": "Bertsekas,? 2005", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "Dynamic Programming and Optimal Control (2nd edition)", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont, Massachusetts.", "citeRegEx": "Bertsekas,? 2001", "shortCiteRegEx": "Bertsekas", "year": 2001}, {"title": "Learning in real-time search: A unifying framework", "author": ["V. Bulitko", "G. Lee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bulitko and Lee,? \\Q2006\\E", "shortCiteRegEx": "Bulitko and Lee", "year": 2006}, {"title": "LIBSVM: A library for support vector machines. Software available on-line at http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm", "author": ["C.C. Chang", "C.J. Lin"], "venue": null, "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Adaptive Resource Control: Machine Learning Approaches to Resource Allocation in Uncertain and Changing Environments", "author": ["Cs\u00e1ji", "B. Cs."], "venue": "Ph.D. thesis, Faculty of Informatics, E\u00f6tv\u00f6s Lor\u00e1nd University, Budapest.", "citeRegEx": "Cs\u00e1ji and Cs.,? 2008", "shortCiteRegEx": "Cs\u00e1ji and Cs.", "year": 2008}, {"title": "Improving multi-agent based scheduling by neurodynamic programming", "author": ["Cs\u00e1ji", "B. Cs", "B. K\u00e1d\u00e1r", "L. Monostori"], "venue": "In Proceedings of the 1st International Conference on Holonic and Mult-Agent Systems for Manufacturing,", "citeRegEx": "Cs\u00e1ji et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cs\u00e1ji et al\\.", "year": 2003}, {"title": "Adaptive sampling based large-scale stochastic resource control", "author": ["Cs\u00e1ji", "B. Cs", "L. Monostori"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Cs\u00e1ji et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cs\u00e1ji et al\\.", "year": 2006}, {"title": "Resource allocation among agents with MDP-induced preferences", "author": ["D.A. Dolgov", "E.H. Durfee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dolgov and Durfee,? \\Q2006\\E", "shortCiteRegEx": "Dolgov and Durfee", "year": 2006}, {"title": "Learning rates for Q-learning", "author": ["E. Even-Dar", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Even.Dar and Mansour,? \\Q2003\\E", "shortCiteRegEx": "Even.Dar and Mansour", "year": 2003}, {"title": "Handbook of Markov Decision Processes: Methods and Applications", "author": ["E.A. Feinberg", "A. Shwartz"], "venue": null, "citeRegEx": "Feinberg and Shwartz,? \\Q2002\\E", "shortCiteRegEx": "Feinberg and Shwartz", "year": 2002}, {"title": "Improving iterative repair strategies for scheduling with the SVM", "author": ["K. Gersmann", "B. Hammer"], "venue": null, "citeRegEx": "Gersmann and Hammer,? \\Q2005\\E", "shortCiteRegEx": "Gersmann and Hammer", "year": 2005}, {"title": "Monte Carlo sampling methods using Markov chains and their application", "author": ["W.K. Hastings"], "venue": "Biometrika, 57, 97\u2013109.", "citeRegEx": "Hastings,? 1970", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Intelligent manufacturing systems - a tentative forecast", "author": ["J. Hatvany", "L. Nemes"], "venue": "Proceedings of the 7th IFAC World Congress,", "citeRegEx": "Hatvany and Nemes,? \\Q1978\\E", "shortCiteRegEx": "Hatvany and Nemes", "year": 1978}, {"title": "Tabu search for the job shop scheduling problem with multi-purpose machines", "author": ["E. Hurink", "B. Jurisch", "M. Thole"], "venue": "Operations Research Spektrum,", "citeRegEx": "Hurink et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hurink et al\\.", "year": 1994}, {"title": "Effective neighborhood functions for the flexible job shop problem", "author": ["M. Mastrolilli", "L.M. Gambardella"], "venue": "Journal of Scheduling,", "citeRegEx": "Mastrolilli and Gambardella,? \\Q2000\\E", "shortCiteRegEx": "Mastrolilli and Gambardella", "year": 2000}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A. Rosenbluth", "M. Rosenbluth", "A. Teller", "E. Teller"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Real-time cooperative enterprises for mass-customized production", "author": ["L. Monostori", "T. Kis", "B. K\u00e1d\u00e1r", "J. V\u00e1ncza", "G. Erd\u0151s"], "venue": "International Journal of Computer Integrated Manufacturing,", "citeRegEx": "Monostori et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Monostori et al\\.", "year": 2008}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley.", "citeRegEx": "Papadimitriou,? 1994", "shortCiteRegEx": "Papadimitriou", "year": 1994}, {"title": "Scheduling: Theory, Algorithms, and Systems", "author": ["M. Pinedo"], "venue": "Prentice-Hall.", "citeRegEx": "Pinedo,? 2002", "shortCiteRegEx": "Pinedo", "year": 2002}, {"title": "Handbook of Learning and Approximate Dynamic Programming, chap. Approximate Dynamic Programming for High-Dimensional Resource Allocation Problems, pp. 261\u2013283", "author": ["W.B. Powell", "B. Van Roy"], "venue": null, "citeRegEx": "Powell and Roy,? \\Q2004\\E", "shortCiteRegEx": "Powell and Roy", "year": 2004}, {"title": "A neural reinforcement learning approach to learn local dispatching policies in production scheduling", "author": ["S. Riedmiller", "M. Riedmiller"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Riedmiller and Riedmiller,? \\Q1999\\E", "shortCiteRegEx": "Riedmiller and Riedmiller", "year": 1999}, {"title": "Value function based production scheduling", "author": ["J.G. Schneider", "J.A. Boyan", "A.W. Moore"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Schneider et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 1998}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["S. Singh", "T. Jaakkola", "M. Littman", "Szepesv\u00e1ri", "Cs"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Mathematical Control Theory: Deterministic Finite Dimensional Systems", "author": ["E.D. Sontag"], "venue": "Springer, New York.", "citeRegEx": "Sontag,? 1998", "shortCiteRegEx": "Sontag", "year": 1998}, {"title": "Reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A distributed decision-making structure for dynamic resource allocation using nonlinear function approximators", "author": ["H. Topaloglu", "W.B. Powell"], "venue": "Operations Research,", "citeRegEx": "Topaloglu and Powell,? \\Q2005\\E", "shortCiteRegEx": "Topaloglu and Powell", "year": 2005}, {"title": "A reinforcement learning approach to job-shop scheduling", "author": ["W. Zhang", "T. Dietterich"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang and Dietterich,? \\Q1995\\E", "shortCiteRegEx": "Zhang and Dietterich", "year": 1995}], "referenceMentions": [{"referenceID": 16, "context": "The term of IMS can be attributed to a tentative forecast of Hatvany and Nemes (1978). In the early 80s IMSs were outlined as the next generation of manufacturing systems that utilize the results of artificial intelligence research and were expected to solve, within certain limits, unprecedented, unforeseen problems on the basis of even incomplete and imprecise information.", "startOffset": 61, "endOffset": 86}, {"referenceID": 22, "context": "Different kinds of RAPs have a huge number of exact and approximate solution methods, for example, in the case of scheduling problems (Pinedo, 2002).", "startOffset": 134, "endOffset": 148}, {"referenceID": 4, "context": "This has motivated approximate approaches that require a more tractable computation, but often yield suboptimal solutions (Bertsekas, 2005).", "startOffset": 122, "endOffset": 139}, {"referenceID": 3, "context": "However, due to the phenomenon that was named curse of dimensionality by Bellman (1961), these methods are highly intractable in practice.", "startOffset": 73, "endOffset": 88}, {"referenceID": 3, "context": "However, due to the phenomenon that was named curse of dimensionality by Bellman (1961), these methods are highly intractable in practice. The \u201ccurse\u201d refers to the combinatorial explosion of the required computation as the size of the problem increases. Some authors, e.g., Powell and Van Roy (2004), talk about even three types of curses concerning DP algorithms.", "startOffset": 73, "endOffset": 301}, {"referenceID": 31, "context": "Zhang and Dietterich (1995) were the first to apply an RL technique for a special RAP.", "startOffset": 0, "endOffset": 28}, {"referenceID": 20, "context": "Riedmiller and Riedmiller (1999) used a multilayer perceptron (MLP) based neural RL approach to learn local heuristics.", "startOffset": 0, "endOffset": 33}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem.", "startOffset": 0, "endOffset": 305}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints.", "startOffset": 0, "endOffset": 515}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints.", "startOffset": 0, "endOffset": 778}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs).", "startOffset": 0, "endOffset": 1018}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop scheduling problems based on the combination of Monte Carlo simulation, solutions of the associated deterministic problem, and either constraint programming or tabu-search.", "startOffset": 0, "endOffset": 1247}, {"referenceID": 1, "context": "Aydin and \u00d6ztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling problems were also suggested (Cs\u00e1ji, K\u00e1d\u00e1r, & Monostori, 2003; Cs\u00e1ji & Monostori, 2006). Powell and Van Roy (2004) presented a formal framework for RAPs and they applied ADP to give a general solution to their problem. Later, a parallelized solution to the previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal is to accomplish a set of tasks that can have widely different stochastic durations and precedence constraints between them, while Powell and Van Roy\u2019s (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands having unit durations but not precedence constraints. Recently, support vector machines (SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop scheduling problems based on the combination of Monte Carlo simulation, solutions of the associated deterministic problem, and either constraint programming or tabu-search.", "startOffset": 0, "endOffset": 1280}, {"referenceID": 22, "context": "First, we consider the classical job-shop scheduling problem (JSP) which is a standard deterministic RAP (Pinedo, 2002).", "startOffset": 105, "endOffset": 119}, {"referenceID": 22, "context": "Note that a Gantt chart (Pinedo, 2002) is a figure using bars, in order to illustrate the starting and finishing times of the tasks on the resources.", "startOffset": 24, "endOffset": 38}, {"referenceID": 21, "context": ", JSP and TSP, it is strongly NP-hard and, furthermore, no good polynomial-time approximation of the optimal resource allocating algorithm exits, either (Papadimitriou, 1994).", "startOffset": 153, "endOffset": 174}, {"referenceID": 28, "context": "Conversely, a closed-loop controller uses feedback to control states or outputs of a dynamical system (Sontag, 1998).", "startOffset": 102, "endOffset": 116}, {"referenceID": 15, "context": "Therefore, we perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas, Doucet, & Jordan, 2003) to generate samples with the model, which are used for computing the new approximation of the estimated cost-to-go function.", "startOffset": 66, "endOffset": 124}, {"referenceID": 26, "context": "Using Lagrange multiplier techniques, we can rewrite the regression problem in its dual form (Sch\u00f6lkopf et al., 2000) and arrive at the final \u03bd-SVR optimization problem.", "startOffset": 93, "endOffset": 117}, {"referenceID": 5, "context": "Therefore, at the initial stage we suggest applying a rollout policy, which is a limited lookahead policy, with the optimal cost-to-go approximated by the cost-to-go of the base policy (Bertsekas, 2001).", "startOffset": 185, "endOffset": 202}, {"referenceID": 18, "context": "The best performance on these benchmark datasets was achieved by Mastrolilli and Gambardella (2000). Though, their algorithm performs slightly better than ours, their solution exploits the (unrealistic) specialties of the dataset, e.", "startOffset": 65, "endOffset": 100}], "year": 2008, "abstractText": "The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, \u03bd-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented.", "creator": "dvips(k) 5.96dev Copyright 2007 Radical Eye Software"}}}