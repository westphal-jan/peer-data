{"id": "1401.3436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Online Planning Algorithms for POMDPs", "abstract": "Partially Observable Markov Decision Processes (POMDPs) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. However, solving a POMDP is often intractable except for small problems due to their complexity. Here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. Our objectives here are to survey the various existing online POMDP methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results indicate that state-of-the-art online heuristic search methods can handle large POMDP domains efficiently.", "histories": [["v1", "Wed, 15 Jan 2014 04:52:25 GMT  (443kb)", "http://arxiv.org/abs/1401.3436v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["st\\'ephane ross", "joelle pineau", "s\\'ebastien paquet", "brahim chaib-draa"], "accepted": false, "id": "1401.3436"}, "pdf": {"name": "1401.3436.pdf", "metadata": {"source": "CRF", "title": "Online Planning Algorithms for POMDPs", "authors": ["St\u00e9phane Ross", "Joelle Pineau", "S\u00e9bastien Paquet", "Brahim Chaib-draa"], "emails": ["stephane.ross@mail.mcgill.ca", "jpineau@cs.mcgill.ca", "spaquet@damas.ift.ulaval.ca", "chaib@damas.ift.ulaval.ca"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have generated considerable interest in the AI community and made many attempts to approach it."}, {"heading": "2. POMDP Model", "text": "Partially observable Markov decision-making processes (POMDPs) provide a general framework for acting in partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973; Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998).A POMDP is a generalization of the MDP model for planning under uncertainty, which gives the agent the ability to effectively estimate the outcome of his actions even if he cannot accurately observe the state of his environmental actions. Formally, a POMDP is presented as a tuple (S, A, R, Z, O) where the ability to act of all environmental states is given. A state is a description of the environment at a given time and it should capture all information relevant to the agent's decision-making process. \u2022 T: S \u00b7 A \u00b7 S \u00b7 S \u2192 is the transition function in which T (s, s)."}, {"heading": "2.1 Optimal Value Function Algorithm", "text": "(Probe, 1971) This algorithm uses dynamic programming to calculate ever more accurate values for each faith state. The value iteration algorithm begins by evaluating the value of a faith state over the immediate horizon. (10) The value function at horizon t is a value function that takes a faith state as a parameter and returns a numerical value in R of that faith state. (10) The value function at horizon t is constructed from the value function at horizon t \u2212 1 by using the following recursive equation: Vt (b) = max a \u00b2 A [RB (b) + g \u00b2 A \u00b2 A \u00b2 A \u00b2 A = a \u00b2 A \u00b2 A = b \u00b2 A \u00b2 A \u00b2 A = b \u00b2 A \u00b2 A \u00b2 A \u00b2 A = b \u00b2 A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A. \"(b) The value function at the horizon is constructed by the value function at horizon s \u00b2 A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A = A \u00b2 A \u00b2 A \u00b2 A \u00b2 A.\""}, {"heading": "2.2 Offline Approximate Algorithms", "text": "Due to the high complexity of exact approaches, many researchers have worked to improve the applicability of POMDP approaches by developing approximate offline approaches that can be applied to larger problems. In the online methods discussed below, approximate offline algorithms are often used to calculate lower and upper limits of optimal value function.These limits are used to orient the search in promising directions, to apply industry and industry-specific circumcision techniques, and to estimate the long-term reward of beliefs, as we will show in Section 3. Generally, however, we will want to use approximate methods that require very low computational costs. We will be particularly interested in approaches that use the underlying MDP1 to calculate lower limits (Blind Policy) and upper limits (MDP, QMDP, FIB) in terms of exact value function."}, {"heading": "2.2.1 Blind policy", "text": "Blind politics (Hausrecht, 2000; Smith & Simmons, 2005) is a policy in which the same action is always carried out, regardless of the state of belief; the value function of each blind.1 The MDP, defined by the (S, A, T, R) components of the POMDP model, is obviously a lower limit than V *, since it corresponds to the value of a specific policy that the agent could execute in the environment.The resulting value function is specified by a series of | A | \u03b1 vectors, each \u03b1 vector indicating the long-term expected reward for following its corresponding blind policies.These \u03b1 vectors can be calculated using a simple update rule: \u03b1at + 1 (s) = R (s, a) + \u03b3 (s \u00b2 s \u00b2 s \"ST (s, a, s \u00b2) \u03b1at (s \u00b2), (s \u00b2) \u03b1at (s \u00b2), (14), where \u03b1a0 = mins \u00b2 S (s, a) / 1 \u03b3)."}, {"heading": "2.2.2 Point-Based Algorithms", "text": "In order to achieve narrower lower limits, point-based methods can be applied (Lovejoy, 1991; Hauskrecht, 2000; Pineau et al., 2003).This popular approach approaches the value function by updating it only for some selected states of faith. These point-based methods circumvent the complexity of exact states of faith by simulating some random interactions of the agent with the POMDP environment and then maintaining the value function and its gradient relative to these sampled states of faith. These approaches circumvent the complexity of exact states of faith by sampling a small number of states of faith and at most one \u03b1 vector per sampled state of faith. Let B represent the amount of sampled states of faith (s), then the number of \u03b1 vectors at a given time t are obtained as follows: \u03b1a (s) = R (s) = a (s), \u0430 (s), a), a), zt = z (a, a, a, S, S, S, S, (), S, S, () (), S, S, S, () (), S, S, S, () (), S, S, (), S, S, (), (), (Lovejoy, 1991; Hauskrecht, 2000; Hauskrecht, Pineau et al, 2000; Pineau et al., 2003)."}, {"heading": "2.2.3 MDP", "text": "The MDP approximation is the approximation of the value function V \u0445 of the POMDP by the value function of the underlying MDP (Littman et al., 1995).This value function is an upper limit of the value function of the POMDP and can be calculated using Bellman's equation: V MDPt + 1 (s) = max a \u00b2 A [R (s, a) + \u03b3 \u00b2 s \u00b2 s \u00b2 s \u00b2 ST (s, a, s \u00b2) V MDPt (s \u00b2)]. (16) The value V \u00b2 (b) of a faith state b is then calculated as V \u00b2 (b) = \u0445 S \u00b2 S V MDP (s) b (s).This can be calculated very quickly, since each iteration of the equation can be 16 in O (| A | S | 2)."}, {"heading": "2.2.4 QMDP", "text": "The QMDP approximation is a slight variation of the MDP approximation (Littman et al., 1995). The basic idea behind QMDP is to take into account that all partial observability disappears after a single step. It assumes that the MDP solution is calculated to generate V MDPt (Eq.16). (17) This approximation defines an \u03b1 vector for each action and specifies an upper limit for V \u0445 that is narrower than V MDP (i.e. V QMDPt (b) \u2264 V MDP t (b) for each belief b)."}, {"heading": "2.2.5 FIB", "text": "To solve this problem, Hauskrecht (2000) proposed a new method for calculating the upper limits, called Fast Informed Bound (FIB), which is able to take into account (to some extent) the partial observability of the environment. (18) The \u03b1 vector update process is described as follows: \u03b1at + 1 (s) = R (s, a) + \u03b3 Z vectors found by QMDP at convergence, i.e.: \u03b1at + 1 (s) = R (s, a) = T (s, s \u00b2). \u03b1 vectors \u03b1a0 can be initialized to the \u03b1 vectors found by QMDP at convergence, i.e. \u03b1a0 (s) = QMDP (s, a)."}, {"heading": "3. Online Algorithms for POMDPs", "text": "For offline approaches, the algorithm provides a policy that defines what actions need to be taken in each possible state of belief. Such approaches tend to be applicable only when small to medium ranges are involved, as the political construction step takes a significant amount of time. In large POMDPs, the use of a very rough approximation of values (as shown in Section 2.2) tends to significantly impair the execution of the resulting guidelines. Even newer point-based methods lead to solutions of limited quality in very large ranges (Paquet et al., 2006). Therefore, in large POMDPs, a potentially better alternative is to use an online approach that merely seeks to find a good local policy for the current state of belief of the actor. The advantage of such an approach is that it only has to take into account states of faith that are achievable from the current state of belief. This focuses the calculation on a small group of beliefs. Since online planning is performed at each step of 2001 (and thus a maximum reconstruction time between the state of belief is sufficient), the calculation of the current state of the overview is sufficient."}, {"heading": "3.1 General Framework for Online Planning", "text": "Subsequently, we will discuss specific approaches from the literature and describe how they differ in coping with various aspects of this general framework. An online algorithm is divided into a planning phase, and an execution phase, which is applied alternately in each time stage, in the planning phase the algorithm in the current phase of action and the observations that can be derived from the current belief, is the best action to perform in this belief. This is usually achieved in two steps. First, a tree of achievable beliefs is constructed from the current belief state by looking at several possible sequences of actions and observations that can be derived from the current belief. In this tree, the current belief set of the root knot and subsequent achievable beliefs (as calculated by the views (b, a, z) function of Equation 3) are added to the tree as a child knot of their immediate previous beliefs. Belief nodes are selected from each action (we have to choose from the OR action) to which one of the nodes is used."}, {"heading": "3.2 Branch-and-Bound Pruning", "text": "To achieve this in the AND-OR tree, a lower and an upper boundary are maintained to the value Q * (b, a) of each action a, for each belief b in the tree. These boundaries are calculated by first evaluating the lower and upper boundaries for the boundary nodes of the tree. These boundaries are then extended to parental nodes according to the following equations: LT (b) = {L (b), if b (T) maxa (b, a) LT (b) = RB (b, a) + g \u00b2 z \u00b2 ZPr (z (z, a) + g \u00b2 z \u00b2 ZPr (z | b, a) LT (b) = {L (b, z), if the boundaries of the z \u00b2 action are exceeded, if (T) maxa (b) = RB (b, a) + g \u00b2 z \u00b2 ZPr (z (z, b), b), b), b \u00b2 b (b), b \u00b2 b (b), b \u00b2 b (b), b (b), b (b), b (b), panb, b, b, UT, b, UT (b), UT (b), b, UT (b), b, b, UT (b), b, b, b, UT (b)."}, {"heading": "3.2.1 RTBSS", "text": "The real way that people in the countries of the world enter the world from top to bottom and into the world from top to bottom from top to bottom from top to bottom from bottom to top from bottom to top from top to top from bottom to top from top to top from bottom to top from bottom to top from top to top from top to top from top to bottom from top to top from top to top from top from bottom to top from top from top from bottom to top from top from bottom to top from top from top from top to top from bottom to top from bottom to top from bottom to top from bottom to top from top to top from top from bottom to top from top from top from bottom to top from bottom to top from top from top from bottom to top from top from top from top to top from top from top to bottom from bottom to top."}, {"heading": "3.3 Monte Carlo Sampling", "text": "As mentioned above, a complete expansion of the search tree to a large number of observations is not feasible except at shallow depths. In such cases, it would be a better alternative to capture a subset of observations at each expansion, taking into account only the beliefs attained by these sampled observations. This reduces the branching factor of the search and allows for a deeper search within a given planning time. This is the strategy employed by Monte Carlo algorithms."}, {"heading": "3.3.1 McAllester and Singh", "text": "The approach presented by McAllester and Singh (1999) is an adaptation of the online MDP algorithm, which is often sufficient to obtain a good estimate. (Alleb) The approach presented by Kearns, Mansour and Ng (1999) consists of a depth-limited search for the AND-OR tree up to a certain fixed horizon D, where, instead of examining all observations in each action selection, C observations are taken from a generative model. Pr (z-OR, a) is then calculated based on the observed frequencies in the sample. The advantage of such an approach is that an observation from the distribution Pr (z | b, a) can be achieved very efficiently in O (log | S | Z, a)."}, {"heading": "3.3.2 Rollout", "text": "Another similar Monte Carlo approach is the rollout algorithm (Bertsekas & Castanon, 1999), which requires an initial policy (possibly calculated offline) to estimate the future expected value of each action at each point in time, provided that the initial policy is followed in future increments and executes the initial action with the highest estimated value. These estimates are obtained by calculating the average discounted return achieved via a series of M sampled trajectories of depth D. These trajectories are generated by first taking the measures to be evaluated and then following the initial policy in subsequent states, evaluating the observations using a generative model. As this approach only needs to take into account different actions at the root of the faith node, the number of actions only affects the branching factor at the first level of the tree. Consequently, it is generally more scalable than McAllister's and Singh's approach."}, {"heading": "3.4 Heuristic Search", "text": "Instead of selecting the next node in the AND / OR list, the usual three references for these three references should be reduced. (Most relevant references have been suggested in the past: Satia and Lave (1973), BI-POMDP (Washington, 1997) and AEMS (Ross & Chaib-draa, 2007) These references maintain both lower and upper references for each node in the tree (using Equations 19-22) and differ only in the specific references to select the next node."}, {"heading": "19: H\u2217", "text": "(b) \u2190 HT (b, a) H (b, aT) T (b, b) T (b, b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T (b) T ("}, {"heading": "3.4.1 Satia and Lave", "text": "The approach of Satia and Lave (1973) follows the heuristic search frame shown above. The main feature of this approach is to explore the fringe node b in the current search tree T, which maximizes the following term: HT (bc, b) = \u03b3 d (hbc, b) Pr (hbc, bT, z | bc, h bc, b \u2212 \u2212 \u2212 value (b) \u2212 LT (b))))), (27) where b (b (b (b (T) and bc the root node of T. The intuition behind this haysticism is simple: If we remember the definition of V *, we find that the weight of the value V \u2212 LT (b) of a fringe node b (bc) would be exactly d (hbc, b) and bc the root node of T. The intuition behind this haysticism is simple: If we remember the definition of V *, we find that the weight of the value V \u2212 LT (b) would be b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, and Lave and Lave, b, b, b."}, {"heading": "3.4.2 BI-POMDP", "text": "Washington (1997) proposed a slightly different approach, inspired by the AO * algorithm (Nilsson, 1980), where the search is carried out only in the best solution curve. (In the case of online POMDPs, this corresponds to the partial tree of all faith nodes, which are reached by action sequences maximizing the upper limit in their parental convictions HG. The amount of marginal nodes in the best solution curve of G, which we call F (G), can be formally defined as F (G). (HG, b), Pr (HG, b), \u03c0 (G) > 0}, where the root activity of G (b, a) = 1, if a = argmaxa \u00b2 -A UG (b, a) and \u03c0 (HG, a) - HG (HG, a) - HG (HG, b -, b - (HG) - (HG) - (HG) (HG, b -, b -) (HG - (HG) (HG) (HG) (HG, b -, b -) (HG -) (HG) (HG -) (HG - (HG) (HG), HG (HG) (HG -) (HG) (HG) (HG, b -) (HG) (HG)."}, {"heading": "3.4.3 AEMS", "text": "It is based on a theoretical error analysis of the tree search in POMDPs by Ross et al. (2008).The exact error contribution eT (bc, b) of fringe node b) of fringe b) of fringe b) of fringe b) of fringe b) of fringe b (bc, b) of fringe b) of bc in tree T is defined by the following equation: eT (bc, b) of hbc, bT (hbc, bc) of fringe b (b) of fringe b (b) of fringe b in tree T is defined by the following equation: eT (bc, b) of hbc, bT (bc, bc) of fringe (b) -LT (b)."}, {"heading": "3.4.4 HSVI", "text": "A heuristic similar to AEMS2 was also used by Smith and Simmons (2004) for their offline value iteration algorithm HSVI to select the next faith point at which \u03b1 vector backups can be performed, the main difference being that HSVI progresses through a greedy search that descends the tree from root node b0 down to the action that maximizes the upper limit and then maximizes the observation Pr (z | b, a) (U (b, a, z)) \u2212 L (\u03c4 (b, a, z)) at each level until it reaches a faith b in depth d where \u03b3 \u2212 d (U (b) \u2212 L (b)) < this heuristic could be used in an online heuristic search algorithm by instead stopping the greedy search process when it reaches one edge node of the tree and then extends that node as the next."}, {"heading": "3.5 Alternatives to Tree Search", "text": "We now present two alternative online approaches that are not based on a forward-looking search in faith MDP. In all of the online approaches presented so far, one problem is that no learning is achieved over time, i.e. whenever the agent encounters the same belief, he must recalculate his policy based on the same initial upper and lower limits that are calculated offline. The two online approaches presented next address this problem by identifying alternative ways to update the initial value functions calculated offline so that the agent's performance improves over time as it stores updated values that are calculated at each step. However, as argued below and in the discussion (Section 5.2), these techniques lead to other disadvantages in terms of memory consumption and / or time complexity."}, {"heading": "3.5.1 RTDP-BEL", "text": "An alternative approach to searching in the Evaluate Q (bc, a) is the RTDP algorithm (Barto et al., 1995), which was adapted to solve POMDPs by Geffner and Bonet (1998). Their algorithm, called RTDP-BEL, learns approximate values for the states of belief attended by the respective beliefs in the area. At each belief, the agent will evaluate all possible actions by assessing the expected reward of the actions and their approximate beliefs."}, {"heading": "3.5.2 SOVI", "text": "A newer online approach called SOVI (Shani et al., 2005) expands HSVI (Smith & Simmons, 2004, 2005) to an online value iteration algorithm. This approach maintains a priority list of faith states that occur during execution, and goes further by performing alpha vector updates for the current faith state and k faith states, each with the highest priority. Priority of a faith state is calculated according to how much the value function has changed since the last faith state update. The main drawback of this approach is that it is hardly applicable in large environments with short real-time constraints, such as a more efficient alpha vector pruning technique and the avoidance of linear programs for updating and evaluating the upper limit. Additionally, the authors propose further improvements to the HSVI algorithm to improve scalability, such as having a more efficient alpha vector pruning technique and avoiding linear programs for updating and evaluating the upper limit."}, {"heading": "3.6 Summary of Online POMDP Algorithms", "text": "In summary, most online POMDP approaches are based on looking for perspectives. To improve scalability, various techniques are used: branch-and-bound printing, search heuristics, and Monte Carlo sampling. These techniques reduce complexity from different angles. Branch-and-bound printing reduces complexity in terms of the size of the action space. Monte-Carlo sampling has been used to reduce complexity in terms of the size of the observation space, and could potentially also be used to reduce complexity in terms of the size of the action space (by sampling a subset of actions). Search heuristics reduce complexity in terms of actions and observations by focusing the search on the most relevant actions and observations."}, {"heading": "4. Empirical Study", "text": "In this section we compare different online approaches in two areas of the POMDP literature: Tag (Pineau et al., 2003) and RockSample (Smith & Simmons, 2004). We consider a modified version of the RockSample, called FieldVisionRockSample (Ross & Chaib-draa, 2007), which has a larger observation space than the original RockSample. This environment is introduced to test and compare the different algorithms in environments with large observation spaces."}, {"heading": "4.1 Methodology", "text": "To this end, we implemented the various search heuristics (Satia and Lave, BI-POMDP, HSVI-BFS and AEMS) into the same best-first search algorithm, so that we can directly measure the efficiency of heuristics themselves. Results were also obtained for various lower limits (Blind and PBVI) to check how this choice affects the efficiency of heuristics. Finally, we compare how online and offline times affect the performance of each approach. Except where otherwise stated, all experiments were conducted on an Intel Xeon 2.4 Ghz with 4GB of RAM; processes were limited to 1GB of RAM."}, {"heading": "4.1.1 Metrics to compare online approaches", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "4.3.1 Real-Time Performance of Online Search", "text": "In the second half of the last decade, when the disease risks in the US and Europe are getting worse, the situation in the US has got worse, both in emerging and emerging countries, \"he said in an interview with the New York Times, in which he addressed the question:\" What kind of country is this?, \"\" What kind of country is this?, \"\" What is this country?, \"\" What is this country?, \"\" What is it?, \"What is it?,\" What is it?, \"What is it?,\" What is it?, \"What is it?,\" What is it?, \"What is it?,\" What is it?, \"What is it.\""}, {"heading": "4.3.2 Long-Term Error Reduction of Online Heuristic Search", "text": "In order to compare the long-term performance of the various heuristics, we run the algorithms from the initial faith state of the environment in offline mode and log changes in the lower and upper limits of that initial faith state over 1000 seconds. Here, the initial lower and upper limits are provided by the Blind policy or QMDP. Figure 4 shows that Satia and Lave, AEMS1 and BI-POMDP are not as efficient as HSVI-BFS and AEMS2 in reducing error within the limits. It is also interesting that the upper limit is slowly but continuously decreasing, while the lower limit often increases gradually. We believe this is due to the fact that the upper limit is much narrower than the lower limit. We also observe that most of the error-related reduction occurs in the first seconds of the search, confirming that the nodes that have expanded earlier in the tree have much more influence on the errors of the hundreds of seconds (e.g. those that were expanded by far)."}, {"heading": "4.3.3 Influence of Offline and Online Time", "text": "This year, we have reached the point where we feel we can find a solution in such a situation, where we feel we can find a solution that paves the way to another world."}, {"heading": "4.4.1 Real-Time Performance of Online Search", "text": "In Table 4, we present 95% confidence intervals for our metrics of interest. We consider two cases of this environment, FVRS [5,5] (801 states, 5 actions, 32 observations) and FVRS [5,7] (3201 states, 5 actions, 128 observations). In both cases, we use the QMDP upper limit and Blind lower limit, applying the real-time limitations of 1 second per activity. In terms of yield, we do not see a clear winner. BI-POMDP performs surpringly well in FVRS [5,5], but significantly worse than AEMS2 and HSVI-BFS in FVRS [5,7]."}, {"heading": "4.4.2 Long-Term Error Reduction of Online Heuristic Search", "text": "While Table 4 confirms the consistent performance of HSVI-BFS and AEMS2, the overall difference to other heuristics is modest. Considering the complexity of this environment, this could be due to the fact that the algorithms do not have enough time to expand a significant number of nodes within a second. Long-term analysis of the evolution of boundaries in Figures 7 and 8 confirms this. We observe in these figures that the lower limit converges slightly faster with AEMS2 than with other heuristics. AEMS1 heuristics also performs well in this problem in the long term and appears to be the second best heuristics, while Satia and Lave are not far behind. On the other hand, HSVI-BFS heuristics is much worse on this problem than in the RockSample. This seems to be partly due to the fact that this heuristics takes more time to find the next node to expand than the others, and therefore less studied."}, {"heading": "5. Discussion", "text": "In the previous sections, various online POMDP algorithms were presented and evaluated. We will now discuss important questions that arise in practice when using online methods and summarize some of their advantages and disadvantages to help researchers decide whether online algorithms are a good approach to solving a particular problem."}, {"heading": "5.1 Lower and Upper Bound Selection", "text": "Online algorithms can be combined with many valid lower and upper boundaries. However, there are some properties that should meet these boundaries in order for the online search to be efficient in practice. One of the desired properties is that the lower and upper boundary functions should be monotonous; the monotonous property states that the lower boundary and the lower boundary should be monotonous; the monotonous property states that the lower boundary and the upper boundary should be monotonous; this property guarantees that when a particular boundary node is extended, its lower boundary does not decrease and its upper boundary does not rise."}, {"heading": "5.2 Improving the Bounds over Time", "text": "As we mentioned in our survey on online algorithms, one disadvantage of many online approaches is that they do not store improvements in offline boundaries during the online search, so if the same state of belief occurs again, the same calculations must be done again, from the same offline boundaries. However, a trivial way to improve this is to maintain a large hashtable (or database) of states of belief for which we have improved the lower and upper boundaries of beliefs in the previous search, with the new boundaries that come with it. However, there are many disadvantages in doing so. In addition, every time we want to evaluate the lower and upper boundaries of a marginal belief, a search must be conducted through that hashtable to verify that we have better boundaries available. This can take considerable time if the hashtable is large (e.g. millions of beliefs). Furthermore, experiments that are conducted with RTDP balls in large domains, such as RockSample [7,8], such a process usually runs out of memory."}, {"heading": "5.3 Factored POMDP Representations", "text": "The efficiency of online algorithms relies to a large extent on the ability to quickly calculate \u03c4 (b, a, z) and Pr (z | b, a), as these must be calculated for a uniform state of belief in the search tree. Using factored POMDP representations is an effective method to reduce the time complexity of calculating these sets. However, since most environments with large state ranges are structured and can be described by attribute sets, factored representation of complex systems should not be a problem in most cases. However, in areas with significant dependencies between state characteristics, it may be useful to use algorithms proposed by Boyen and Koller (1998) and Poupart (2005) to find approximate factored representations where most features are independent, with minimal deterioration in the quality of the solution. While the upper and lower limits may no longer hold up when calculated via the approximate factored representation, they can still provide good results in practice."}, {"heading": "5.4 Handling Graph Structure", "text": "As we have already mentioned, the generic tree search algorithm used by online algorithms will duplicate multiple faith states if there are multiple paths leading to the same posterior belief, greatly simplifying the complexity associated with updating the values of ancestor nodes, and also reducing the complexity associated with finding the best marginal node for expansion (with the technique in Section 3.4, which applies only to trees). The disadvantage of using a tree structure is that some compilations will inevitably be redundant, since the algorithm will potentially expand the same subtree under each duplicated belief. To avoid this, we could use the LAO algorithm proposed by Hansen and Zilberstein (2001) as an extension of the AO structure that can handle generic graph structure, including cyclic graphs that run Hheub graphs, a value (or a policy) iteration algorithm to actualize between ancestral nodes."}, {"heading": "5.5 Online vs. Offline Time", "text": "An important aspect that determines the efficiency and applicability of online algorithms is the time available for planning during execution. Of course, this often depends on the task at hand. In real-time problems such as robot navigation, this time can be very short, for example, between 0.1 and 1 second per action. In tasks such as portfolio management, where there is no need to act for seconds, it can easily take several minutes to plan a buy / sell action. As we have seen from our experiments, the impact of the offline value function becomes negligible, so that a very rough offline value function is sufficient to achieve good performance. In such cases, it is often necessary to allow sufficient time to calculate a good offline policy. As more and more planning time is available online, the impact of the offline value function becomes negligible, so that a very rough offline value function is sufficient to achieve good performance. The best compromise between online - and secondary problem depends on how big the offline problem is | | is, as small as time is required |."}, {"heading": "5.6 Advantages and Disadvantages of Online Algorithms", "text": "We now discuss the advantages and disadvantages of online planning algorithms in general."}, {"heading": "5.6.1 Advantages", "text": "\u2022 Most online algorithms can be combined with any other offline solution algorithm, provided it provides a lower or upper limit for V \u043a, such as to improve the quality of policies found offline. \u2022 Online algorithms require very little offline computation before they can be executed in an environment, as they work well even with very loose limits that are quick to calculate. \u2022 Online methods can take advantage of the knowledge of the current belief to focus the computation on the most relevant future beliefs for the current decision, so that they can scale well to large spaces of action and observation. \u2022 Online methods are always applicable in real-time environments, as they can be stopped at any time when the planning time expires, and still represent the best solution found so far."}, {"heading": "5.6.2 Disadvantages", "text": "The branching factor depends on the number of actions and observations, so if there are many observations and / or actions, it may be impossible to search deep enough to achieve a significant improvement in offline policy. In such cases, sampling methods may be useful to reduce the branching factor. Although we cannot guarantee that the lower and upper limits are still valid when using the samples, we can guarantee that they are likely to be valid as enough samples are taken. \u2022 Most online algorithms do not store improvements in offline policy through online search, and so the algorithm must plan with the same limits every time the environment is restarted. If time is available, it may be beneficial to add alpha vector updates for some beliefs being studied in the tree, so that the offline limits improve over time."}, {"heading": "6. Conclusion", "text": "POMDPs provide a rich and elegant framework for planning in stochastic, partially observable domains, but their time complexity was a major problem that prevented their application to complex real-world systems. This paper thoroughly examines the various existing online algorithms and the key techniques and approaches used to more efficiently solve POMDPs. We compare these online approaches empirically in several POMDP domains under different metrics: average discounted return, average error-related reduction, and average improvement of lower and upper limits: PBVI, Blind, FIB, and QMDP. From the empirical results, we find that some of the heuristic search methods, namely AEMS2 and HSVI-BFS, perform very well, even in domains with large branched factors and large government spaces. These two methods are very similar and work well because they orient the search to nodes that can improve the current approximate value function as quickly as possible."}, {"heading": "Acknowledgments", "text": "This research was supported by the Natural Sciences and Engineering Council of Canada and the Que'be \u0301 cois de la Recherche sur la Nature et les Technologies Fund. We would also like to thank the anonymous reviewers for their helpful comments and suggestions."}], "references": [{"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K.J. Astrom"], "venue": "Journal of Mathematical Analysis and Applications, 10, 174\u2013205.", "citeRegEx": "Astrom,? 1965", "shortCiteRegEx": "Astrom", "year": 1965}, {"title": "Learning to act using real-time dynamic programming", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singhe"], "venue": "Artificial Intelligence,", "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press, Princeton, NJ, USA.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Rollout algorithms for stochastic scheduling problems", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": "Journal of Heuristics,", "citeRegEx": "Bertsekas and Castanon,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1999}, {"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boyen and Koller,? \\Q1998\\E", "shortCiteRegEx": "Boyen and Koller", "year": 1998}, {"title": "Stochastic local search for POMDP controllers", "author": ["D. Braziunas", "C. Boutilier"], "venue": "In The Nineteenth National Conference on Artificial Intelligence", "citeRegEx": "Braziunas and Boutilier,? \\Q2004\\E", "shortCiteRegEx": "Braziunas and Boutilier", "year": 2004}, {"title": "Incremental pruning: a simple, fast, exact method for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Parallel rollout for online solution of partially observable Markov decision processes", "author": ["H.S. Chang", "R. Givan", "E.K.P. Chong"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Chang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2004}, {"title": "Solving large POMDPs using real time dynamic programming", "author": ["H. Geffner", "B. Bonet"], "venue": "In Proceedings of the Fall AAAI symposium on POMDPs,", "citeRegEx": "Geffner and Bonet,? \\Q1998\\E", "shortCiteRegEx": "Geffner and Bonet", "year": 1998}, {"title": "Solving POMDPs by searching in policy space", "author": ["E.A. Hansen"], "venue": "Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98), pp. 211\u2013219.", "citeRegEx": "Hansen,? 1998", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "LAO * : A heuristic search algorithm that finds solutions with loops", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence,", "citeRegEx": "Hansen and Zilberstein,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 2001}, {"title": "Value-function approximations for partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research, 13, 33\u201394.", "citeRegEx": "Hauskrecht,? 2000", "shortCiteRegEx": "Hauskrecht", "year": 2000}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "A sparse sampling algorithm for nearoptimal planning in large markov decision processes", "author": ["M.J. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Agent-centered search", "author": ["S. Koenig"], "venue": "AI Magazine, 22 (4), 109\u2013131.", "citeRegEx": "Koenig,? 2001", "shortCiteRegEx": "Koenig", "year": 2001}, {"title": "Algorithms for sequential decision making", "author": ["M.L. Littman"], "venue": "Ph.D. thesis, Brown University.", "citeRegEx": "Littman,? 1996", "shortCiteRegEx": "Littman", "year": 1996}, {"title": "Learning policies for partially observable environments: scaling up", "author": ["M.L. Littman", "A.R. Cassandra", "L.P. Kaelbling"], "venue": "In Proceedings of the 12th International Conference on Machine Learning", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Computationally feasible bounds for POMDPs", "author": ["W.S. Lovejoy"], "venue": "Operations Research, 39 (1), 162\u2013175.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "author": ["O. Madani", "S. Hanks", "A. Condon"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence", "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "Approximate Planning for Factored POMDPs using Belief State Simplification", "author": ["D. McAllester", "S. Singh"], "venue": "In Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "McAllester and Singh,? \\Q1999\\E", "shortCiteRegEx": "McAllester and Singh", "year": 1999}, {"title": "A survey of partially observable Markov decision processes: theory, models and algorithms", "author": ["G.E. Monahan"], "venue": "Management Science, 28 (1), 1\u201316.", "citeRegEx": "Monahan,? 1982", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "Principles of Artificial Intelligence", "author": ["N. Nilsson"], "venue": "Tioga Publishing.", "citeRegEx": "Nilsson,? 1980", "shortCiteRegEx": "Nilsson", "year": 1980}, {"title": "The complexity of Markov decision processes", "author": ["C. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Distributed Decision-Making and Task Coordination in Dynamic, Uncertain and Real-Time Multiagent Environments", "author": ["S. Paquet"], "venue": "Ph.D. thesis, Laval University.", "citeRegEx": "Paquet,? 2006", "shortCiteRegEx": "Paquet", "year": 2006}, {"title": "Hybrid POMDP algorithms", "author": ["S. Paquet", "B. Chaib-draa", "S. Ross"], "venue": "In Proceedings of The Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Paquet et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2006}, {"title": "An online POMDP algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "In Proceedings of The fourth International Joint Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Point-based value iteration: an anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Anytime point-based approximations for large POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Tractable planning under uncertainty: exploiting structure", "author": ["J. Pineau"], "venue": "Ph.D. thesis, Carnegie Mellon University.", "citeRegEx": "Pineau,? 2004", "shortCiteRegEx": "Pineau", "year": 2004}, {"title": "Exploiting structure to efficiently solve large scale partially observable Markov decision processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis, University of Toronto.", "citeRegEx": "Poupart,? 2005", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "Bounded finite state controllers", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Poupart and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2003}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, Inc.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Aems: An anytime online search algorithm for approximate policy refinement in large POMDPs", "author": ["S. Ross", "B. Chaib-draa"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence", "citeRegEx": "Ross and Chaib.draa,? \\Q2007\\E", "shortCiteRegEx": "Ross and Chaib.draa", "year": 2007}, {"title": "Theoretical analysis of heuristic search methods for online POMDPs", "author": ["S. Ross", "J. Pineau", "B. Chaib-draa"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Markovian decision processes with probabilistic observation of states", "author": ["J.K. Satia", "R.E. Lave"], "venue": "Management Science,", "citeRegEx": "Satia and Lave,? \\Q1973\\E", "shortCiteRegEx": "Satia and Lave", "year": 1973}, {"title": "Adaptation for changing stochastic environments through online POMDP policy learning", "author": ["G. Shani", "R. Brafman", "S. Shimony"], "venue": "In Proceedings of the Workshop on Reinforcement Learning in Non-Stationary Environments,", "citeRegEx": "Shani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2005}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Smith and Simmons,? \\Q2004\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2004}, {"title": "Point-based POMDP algorithms: improved analysis and implementation", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the 21th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "The optimal control of partially observable Markov processes", "author": ["E.J. Sondik"], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Sondik,? 1971", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research, 26 (2), 282\u2013304.", "citeRegEx": "Sondik,? 1978", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "A point-based POMDP algorithm for robot planning", "author": ["M.T.J. Spaan", "N. Vlassis"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation", "citeRegEx": "Spaan and Vlassis,? \\Q2004\\E", "shortCiteRegEx": "Spaan and Vlassis", "year": 2004}, {"title": "Perseus: randomized point-based value iteration for POMDPs", "author": ["M.T.J. Spaan", "N. Vlassis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Spaan and Vlassis,? \\Q2005\\E", "shortCiteRegEx": "Spaan and Vlassis", "year": 2005}, {"title": "A fast point-based algorithm for POMDPs", "author": ["N. Vlassis", "M.T.J. Spaan"], "venue": "Benelearn", "citeRegEx": "Vlassis and Spaan,? \\Q2004\\E", "shortCiteRegEx": "Vlassis and Spaan", "year": 2004}, {"title": "BI-POMDP: bounded, incremental partially observable Markov model planning", "author": ["R. Washington"], "venue": "Proceedings of the 4th European Conference on Planning, pp. 440\u2013451.", "citeRegEx": "Washington,? 1997", "shortCiteRegEx": "Washington", "year": 1997}, {"title": "Speeding up the convergence of value iteration in partially observable Markov decision processes", "author": ["N.L. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "In the last few years, POMDPs have generated significant interest in the AI community and many approximation algorithms have been developed (Hauskrecht, 2000; Pineau, Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons, 2005; Spaan & Vlassis, 2005).", "startOffset": 140, "endOffset": 279}, {"referenceID": 29, "context": "In the last few years, POMDPs have generated significant interest in the AI community and many approximation algorithms have been developed (Hauskrecht, 2000; Pineau, Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons, 2005; Spaan & Vlassis, 2005).", "startOffset": 140, "endOffset": 279}, {"referenceID": 44, "context": "On the other hand, online approaches (Satia & Lave, 1973; Washington, 1997; Barto, Bradtke, & Singhe, 1995; Paquet, Tobin, & Chaib-draa, 2005; McAllester & Singh, 1999; Bertsekas & Castanon, 1999; Shani, Brafman, & Shimony, 2005) try to circumvent the complexity of computing a policy by planning online only for the current information state.", "startOffset": 37, "endOffset": 229}, {"referenceID": 14, "context": "Online algorithms are sometimes also called agent-centered search algorithms (Koenig, 2001).", "startOffset": 77, "endOffset": 91}, {"referenceID": 11, "context": "We present various combinations of online algorithms with various existing offline algorithms, such as QMDP (Littman, Cassandra, & Kaelbling, 1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005) and PBVI (Pineau et al.", "startOffset": 153, "endOffset": 171}, {"referenceID": 11, "context": "We present various combinations of online algorithms with various existing offline algorithms, such as QMDP (Littman, Cassandra, & Kaelbling, 1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005) and PBVI (Pineau et al.", "startOffset": 179, "endOffset": 220}, {"referenceID": 26, "context": "We present various combinations of online algorithms with various existing offline algorithms, such as QMDP (Littman, Cassandra, & Kaelbling, 1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005) and PBVI (Pineau et al., 2003).", "startOffset": 230, "endOffset": 251}, {"referenceID": 0, "context": "Partially observable Markov decision processes (POMDPs) provide a general framework for acting in partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973; Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998).", "startOffset": 132, "endOffset": 226}, {"referenceID": 20, "context": "Partially observable Markov decision processes (POMDPs) provide a general framework for acting in partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973; Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998).", "startOffset": 132, "endOffset": 226}, {"referenceID": 0, "context": "Instead, it is possible to summarize all relevant information from previous actions and observations in a probability distribution over the state space S, which is called a belief state (Astrom, 1965).", "startOffset": 186, "endOffset": 200}, {"referenceID": 40, "context": "Since there always exists a deterministic policy that maximizes V \u03c0 for any belief states (Sondik, 1978), we will generally only consider deterministic policies (i.", "startOffset": 90, "endOffset": 104}, {"referenceID": 2, "context": "The value function V \u2217 of the optimal policy \u03c0\u2217 is the fixed point of Bellman\u2019s equation (Bellman, 1957):", "startOffset": 89, "endOffset": 104}, {"referenceID": 39, "context": "One can solve optimally a POMDP for a specified finite horizon H by using the value iteration algorithm (Sondik, 1971).", "startOffset": 104, "endOffset": 118}, {"referenceID": 36, "context": "A key result by Smallwood and Sondik (1973) shows that the optimal value function for a finite-horizon POMDP can be represented by hyperplanes, and is therefore convex and piecewise linear.", "startOffset": 16, "endOffset": 44}, {"referenceID": 39, "context": "A number of exact value function algorithms leveraging the piecewise-linear and convex aspects of the value function have been proposed in the POMDP literature (Sondik, 1971; Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang, 2001).", "startOffset": 160, "endOffset": 260}, {"referenceID": 20, "context": "A number of exact value function algorithms leveraging the piecewise-linear and convex aspects of the value function have been proposed in the POMDP literature (Sondik, 1971; Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang, 2001).", "startOffset": 160, "endOffset": 260}, {"referenceID": 15, "context": "A number of exact value function algorithms leveraging the piecewise-linear and convex aspects of the value function have been proposed in the POMDP literature (Sondik, 1971; Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang, 2001).", "startOffset": 160, "endOffset": 260}, {"referenceID": 11, "context": "Some recent publications provide a more comprehensive overview of offline approximate algorithms (Hauskrecht, 2000; Pineau, Gordon, & Thrun, 2006).", "startOffset": 97, "endOffset": 146}, {"referenceID": 11, "context": "A Blind policy (Hauskrecht, 2000; Smith & Simmons, 2005) is a policy where the same action is always executed, regardless of the belief state.", "startOffset": 15, "endOffset": 56}, {"referenceID": 17, "context": "To obtain tighter lower bounds, one can use point-based methods (Lovejoy, 1991; Hauskrecht, 2000; Pineau et al., 2003).", "startOffset": 64, "endOffset": 118}, {"referenceID": 11, "context": "To obtain tighter lower bounds, one can use point-based methods (Lovejoy, 1991; Hauskrecht, 2000; Pineau et al., 2003).", "startOffset": 64, "endOffset": 118}, {"referenceID": 26, "context": "To obtain tighter lower bounds, one can use point-based methods (Lovejoy, 1991; Hauskrecht, 2000; Pineau et al., 2003).", "startOffset": 64, "endOffset": 118}, {"referenceID": 26, "context": "Different algorithms have been developed using the point-based approach: PBVI (Pineau et al., 2003), Perseus (Spaan & Vlassis, 2005), HSVI (Smith & Simmons, 2004, 2005) are some of the most recent methods.", "startOffset": 78, "endOffset": 99}, {"referenceID": 16, "context": "The MDP approximation consists in approximating the value function V \u2217 of the POMDP by the value function of its underlying MDP (Littman et al., 1995).", "startOffset": 128, "endOffset": 150}, {"referenceID": 16, "context": "The QMDP approximation is a slight variation of the MDP approximation (Littman et al., 1995).", "startOffset": 70, "endOffset": 92}, {"referenceID": 11, "context": "To address this problem, Hauskrecht (2000) proposed a new method to compute upper bounds, called the Fast Informed Bound (FIB), which is able to take into account (to some degree) the partial observability of the environment.", "startOffset": 25, "endOffset": 43}, {"referenceID": 24, "context": "Even more recent point-based methods produce solutions of limited quality in very large domains (Paquet et al., 2006).", "startOffset": 96, "endOffset": 117}, {"referenceID": 14, "context": "Consequently, the overall time for the policy construction and execution is normally less for online approaches (Koenig, 2001).", "startOffset": 112, "endOffset": 126}, {"referenceID": 31, "context": "(Puterman, 1994; Hauskrecht, 2000) Let V\u0302 be an approximate value function and = supb |V \u2217(b) \u2212 V\u0302 (b)|.", "startOffset": 0, "endOffset": 34}, {"referenceID": 11, "context": "(Puterman, 1994; Hauskrecht, 2000) Let V\u0302 be an approximate value function and = supb |V \u2217(b) \u2212 V\u0302 (b)|.", "startOffset": 0, "endOffset": 34}, {"referenceID": 24, "context": "In some cases, their results showed a tremendous improvement of the policy given by the offline algorithm (Paquet et al., 2006).", "startOffset": 106, "endOffset": 127}, {"referenceID": 17, "context": "The approach presented by McAllester and Singh (1999) is an adaptation of the online MDP algorithm presented by Kearns, Mansour, and Ng (1999).", "startOffset": 26, "endOffset": 54}, {"referenceID": 17, "context": "The approach presented by McAllester and Singh (1999) is an adaptation of the online MDP algorithm presented by Kearns, Mansour, and Ng (1999). It consists of a depth-limited search of the AND-OR tree up to a certain fixed horizon D where instead of exploring all observations at each action choice, C observations are sampled from a generative model.", "startOffset": 26, "endOffset": 143}, {"referenceID": 4, "context": "The authors also apply belief state factorization as in Boyen and Koller (1998) to simplify the belief state calculations.", "startOffset": 56, "endOffset": 80}, {"referenceID": 4, "context": "The authors also apply belief state factorization as in Boyen and Koller (1998) to simplify the belief state calculations. For the implementation of this algorithm, the Expand subroutine expands the tree up to fixed depth D, using Monte Carlo sampling of observations, as mentioned above (see Algorithm 3.3). At the end of each time step, the tree T is reinitialized to contain only the new current belief at the root. Kearns et al. (1999) derive bounds on the depth D and the number of samples C needed to obtain an -optimal policy with high probability and show that the number of samples required grows exponentially in the desired accuracy.", "startOffset": 56, "endOffset": 440}, {"referenceID": 3, "context": "Bertsekas and Castanon (1999) also show that with enough sampling, the resulting policy is guaranteed to perform at least as well as the initial policy with high probability.", "startOffset": 0, "endOffset": 30}, {"referenceID": 3, "context": "The original Rollout algorithm by Bertsekas and Castanon (1999) is the same algorithm in the special case where the set of initial policies \u03a0 contains only one policy.", "startOffset": 34, "endOffset": 64}, {"referenceID": 44, "context": "There are three different online heuristic search algorithms for POMDPs that have been proposed in the past: Satia and Lave (1973), BI-POMDP (Washington, 1997) and AEMS (Ross & Chaib-draa, 2007).", "startOffset": 141, "endOffset": 159}, {"referenceID": 34, "context": "There are three different online heuristic search algorithms for POMDPs that have been proposed in the past: Satia and Lave (1973), BI-POMDP (Washington, 1997) and AEMS (Ross & Chaib-draa, 2007).", "startOffset": 109, "endOffset": 131}, {"referenceID": 33, "context": "Now that we have covered the basic subroutines, we present the different heuristics proposed by Satia and Lave (1973), Washington (1997) and Ross and Chaib-draa (2007).", "startOffset": 96, "endOffset": 118}, {"referenceID": 33, "context": "Now that we have covered the basic subroutines, we present the different heuristics proposed by Satia and Lave (1973), Washington (1997) and Ross and Chaib-draa (2007).", "startOffset": 96, "endOffset": 137}, {"referenceID": 32, "context": "Now that we have covered the basic subroutines, we present the different heuristics proposed by Satia and Lave (1973), Washington (1997) and Ross and Chaib-draa (2007). We begin by introducing some useful notation.", "startOffset": 141, "endOffset": 168}, {"referenceID": 34, "context": "The approach of Satia and Lave (1973) follows the heuristic search framework presented above.", "startOffset": 16, "endOffset": 38}, {"referenceID": 21, "context": "Washington (1997) proposed a slightly different approach inspired by the AO\u2217 algorithm (Nilsson, 1980), where the search is only conducted in the best solution graph.", "startOffset": 87, "endOffset": 102}, {"referenceID": 44, "context": "Washington (1997) recommends exploring the fringe node in F\u0302(G) (where G is the current acyclic search graph) that maximizes UG(b)\u2212LG(b).", "startOffset": 0, "endOffset": 18}, {"referenceID": 33, "context": "In particular, Ross et al. (2008) considered two possible approximations for \u03c0\u2217.", "startOffset": 15, "endOffset": 34}, {"referenceID": 33, "context": "As a final note, Ross et al. (2008) determined sufficient conditions under which the search algorithm using this heuristic is guaranteed to find an -optimal action within finite time.", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "The AEMS2 heuristic was also used for a policy search algorithm by Hansen (1998).", "startOffset": 67, "endOffset": 81}, {"referenceID": 33, "context": "(Ross et al., 2008) Let > 0 and bc the current belief.", "startOffset": 0, "endOffset": 19}, {"referenceID": 37, "context": "A heuristic similar to AEMS2 was also used by Smith and Simmons (2004) for their offline value iteration algorithm HSVI as a way to pick the next belief point at which to perform \u03b1-vector backups.", "startOffset": 46, "endOffset": 71}, {"referenceID": 1, "context": "An alternative approach to searching in AND-OR graphs is the RTDP algorithm (Barto et al., 1995) which has been adapted to solve POMDPs by Geffner and Bonet (1998).", "startOffset": 76, "endOffset": 96}, {"referenceID": 1, "context": "An alternative approach to searching in AND-OR graphs is the RTDP algorithm (Barto et al., 1995) which has been adapted to solve POMDPs by Geffner and Bonet (1998). Their algorithm, called RTDP-BEL, learns approximate values for the belief states visited by successive trials in the environment.", "startOffset": 77, "endOffset": 164}, {"referenceID": 8, "context": "Supported by experimental data, Geffner and Bonet (1998) suggest choosing k \u2208 [10, 100], as it usually produces the best results.", "startOffset": 32, "endOffset": 57}, {"referenceID": 35, "context": "A more recent online approach, called SOVI (Shani et al., 2005), extends HSVI (Smith & Simmons, 2004, 2005) into an online value iteration algorithm.", "startOffset": 43, "endOffset": 63}, {"referenceID": 26, "context": "In this section, we compare several online approaches in two domains found in the POMDP literature: Tag (Pineau et al., 2003) and RockSample (Smith & Simmons, 2004).", "startOffset": 104, "endOffset": 125}, {"referenceID": 28, "context": "This environment has also been used more recently in the work of several authors (Poupart & Boutilier, 2003; Vlassis & Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas & Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005).", "startOffset": 81, "endOffset": 266}, {"referenceID": 26, "context": "Tag was initially introduced by Pineau et al. (2003). This environment has also been used more recently in the work of several authors (Poupart & Boutilier, 2003; Vlassis & Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas & Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005).", "startOffset": 32, "endOffset": 53}, {"referenceID": 26, "context": "Tag was initially introduced by Pineau et al. (2003). This environment has also been used more recently in the work of several authors (Poupart & Boutilier, 2003; Vlassis & Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas & Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005). For this environment, an approximate POMDP algorithm is necessary because of its large size (870 states, 5 actions and 30 observations). The Tag environment consists of an agent that has to catch (Tag) another agent while moving in a 29-cell grid domain. The reader is referred to the work of Pineau et al. (2003) for a full description of the domain.", "startOffset": 32, "endOffset": 636}, {"referenceID": 37, "context": "The RockSample problem was originally presented by Smith and Simmons (2004). In this domain, an agent has to explore the environment and sample some rocks (see Figure 3), similarly to what a real robot would do on the planet Mars.", "startOffset": 51, "endOffset": 76}, {"referenceID": 37, "context": "The value d0 defined for the different instances of RockSample in the work of Smith and Simmons (2004) is too high for the FVRS problem (especially in the bigger instances of RockSample), making it almost completely observable.", "startOffset": 78, "endOffset": 103}, {"referenceID": 33, "context": "Note however that monotonicity is not necessary for AEMS to converge to an -optimal solution, as shown in previous work (Ross et al., 2008); boundedness is sufficient.", "startOffset": 120, "endOffset": 139}, {"referenceID": 23, "context": "requires more than 1 GB) before good performance is achieved and requires several thousands episodes before performing well (Paquet, 2006).", "startOffset": 124, "endOffset": 138}, {"referenceID": 23, "context": "The authors of RTBSS have also tried combining their search algorithm with RTDPBel such as to preserve the improvements made by the search (Paquet, 2006).", "startOffset": 139, "endOffset": 153}, {"referenceID": 4, "context": "However, in domains with significant dependencies between state features, it may be useful to use algorithms proposed by Boyen and Koller (1998) and Poupart (2005) to find approximate factored representations where most features are independent, with minimal degradation in the solution quality.", "startOffset": 121, "endOffset": 145}, {"referenceID": 4, "context": "However, in domains with significant dependencies between state features, it may be useful to use algorithms proposed by Boyen and Koller (1998) and Poupart (2005) to find approximate factored representations where most features are independent, with minimal degradation in the solution quality.", "startOffset": 121, "endOffset": 164}, {"referenceID": 9, "context": "To avoid this, we could use the LAO\u2217 algorithm proposed by Hansen and Zilberstein (2001) as an extension of AO\u2217 that can handle generic graph structure, including cyclic graphs.", "startOffset": 59, "endOffset": 89}, {"referenceID": 23, "context": "A good example is the succesful applications of the RTBSS algorithm to the RobocupRescue simulation by Paquet et al. (2005). This environment is very challenging as the state space is orders of magnitude beyond the scope of current algorithms.", "startOffset": 103, "endOffset": 124}], "year": 2008, "abstractText": "Partially Observable Markov Decision Processes (POMDPs) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. However, solving a POMDP is often intractable except for small problems due to their complexity. Here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. Our objectives here are to survey the various existing online POMDP methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results indicate that state-of-the-art online heuristic search methods can handle large POMDP domains efficiently.", "creator": "dvips(k) 5.96dev Copyright 2007 Radical Eye Software"}}}