{"id": "1401.3439", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Interactive Policy Learning through Confidence-Based Autonomy", "abstract": "We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration. The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. CBA and its individual components are compared and evaluated in a complex simulated driving domain. The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.", "histories": [["v1", "Wed, 15 Jan 2014 04:53:48 GMT  (1226kb)", "http://arxiv.org/abs/1401.3439v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sonia chernova", "manuela veloso"], "accepted": false, "id": "1401.3439"}, "pdf": {"name": "1401.3439.pdf", "metadata": {"source": "CRF", "title": "Interactive Policy Learning through Confidence-Based Autonomy", "authors": ["Sonia Chernova", "Manuela Veloso"], "emails": ["soniac@cs.cmu.edu", "veloso@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2. Related Work", "text": "A wide range of political learning algorithms from the demonstration has been proposed within the machine learning and robotics communities, and in the context of enhanced learning (Sutton & Barto, 1998), the demonstration has been considered a source of reliable information that can be used to accelerate the learning process, and a number of approaches to use this information have been developed, such as deriving or modifying the reward function based on demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng, 2004; Papudesi, 1997; Atkeson & Schaal, to determine the value function or agent model)."}, {"heading": "3. Confidence-Based Autonomy Overview", "text": "In fact, it is the case that one is able to put oneself at the top, without being able to step into the role that one holds."}, {"heading": "4. Confident Execution Algorithm", "text": "This is a political learning algorithm in which the agent must select demonstration examples, in real time, as he interacts with the environment. At any time, the algorithm uses thresholds to determine whether a demonstration of correct action in the agent's current state will provide useful information and improve the agent's confidence threshold. If a demonstration is required, the agent requests help from the teacher and updates his policy based on the resulting action label. Otherwise, the agent continues to perform his task autonomously based on his policy. There are two different situations in which the agent needs help from the teacher, unfamiliar states and ambiguous states. An unknown state occurs when the agent encounters a situation that is significantly different from any previously demonstrated state, as represented by the points in Figure 2. While we do not want to demonstrate every possible state and therefore need our model for generalization, we would like to prevent a generalization to truly different states.Ambiguous states from occurring when multiple agents are unable to perform actions in the situation."}, {"heading": "4.1 Distance Threshold", "text": "The purpose of the distance threshold is to evaluate the similarity between the current state of the agent and previous demonstrations. Our evaluation metric uses the nearest adjacent distance, defined as the Euclidean distance between a query and the next point in the dataset. For each query by the agent, we obtain the closest adjacent distance representing the most similar previously detected state. This value is then distanced with the distance threshold \u03c4.The value of the distance threshold is calculated as a function of the average nearest adjacent distance within the dataset of the demonstrations. Evaluating the average similarity between the states provides the algorithm with a domain-independent method for detecting outliers, points that are abnormally far away from previously occurred states. For experiments in this article, the value of \u03c4dist was set to three times the average closest adjacent distance within the dataset. An alternative method for detecting outliers would be the use of classification security and requirement predictions in states with confidence."}, {"heading": "4.2 Confidence Threshold", "text": "The confidence threshold is used to select areas of uncertainty where points from multiple classes overlap. In the facilitator's view, points in these regions represent demonstrations of two different actions by states that appear similar based on sensor data and are difficult to distinguish. This problem often occurs in demonstration learning for a number of reasons, such as the teacher's inability to consistently represent the task, noise in sensor measurements, or an inconsistency between the perceptual abilities of the facilitator and the teacher. We would like to set the confidence threshold to a value that prevents both models from classifying the overlapping region with high confidence1. In the following section, we will discuss the use and limitations of a single fixed threshold. We will then present an algorithm for using multiple adjustable thresholds in Section 4.2.2.1. See Section 7.2 for further discussion of these data regions."}, {"heading": "4.2.1 Single Fixed Threshold", "text": "A single, fixed confidence threshold provides a simple mechanism for approximation to the high confidence regions of the state area. Previous algorithms using a classification confidence threshold for behavioral arbitration have all used a manually selected unit threshold (Inamura et al., 1999; Lockerd & Breazeal, 2004; Grollman & Jenkins, 2007). However, selecting a suitable value can be difficult for a constantly changing data set and model. Figure 3 shows examples of three common problems. Figure 3 (a) presents a case in which two action classes are distinguished and fully separable. A model trained on this data set is able to classify the points with complete accuracy, without misclassification. However, the current threshold classifies only 72% of the points with high reliability, with the remaining 28% of the points being marked as uncertain. In this case, a lower threshold would generally prefer the model."}, {"heading": "4.2.2 Multiple Adjustable Thresholds", "text": "In this area, we believe that we are able to consult the classifier and get a handle on the probability that a certain threshold will be reached; the algorithm starts with the division of the data sets and the training of the classifier; the resulting models are used to classify the retained tests; the algorithm then calculates a unique access threshold for each decision based on the access threshold."}, {"heading": "5. Corrective Teacher Demonstration", "text": "The self-confident execution algorithm presented enables the agent to identify unusual and ambiguous states and prevent autonomous execution in these situations. However, states where faulty execution is selected with high confidence for autonomous execution still occur, typically due to the over-generalization of the classifier. In this article we present the corrective execution algorithm which, together with self-confident execution, enables the teacher to correct errors made by the agent. Algorithm 2 combines corrective execution with self-confident execution and presents the complete self-determined execution of the algorithm. The corrective execution technique comes into play every time the agent performs an autonomous execution. As an action is selected for autonomous execution, the algorithm takes the state of the agent that led to this decision and saves this value within the variable sc (line 11)."}, {"heading": "6. Evaluation and Comparison", "text": "In this section we present an evaluation and comparison of the complete confidence-based autonomy algorithm and its components in the field of simulated driving (Abbeel & Ng, 2004), shown in Figure 7."}, {"heading": "6.1 Domain Description", "text": "In fact, the fact is that most of them will be able to play by the rules they have established over the years, and that they will be able to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, we have to play by the rules. \""}, {"heading": "6.2 Experimental Results", "text": "We present the performance evaluation and comparison of the following demonstration selection techniques: \u2022 TG - Teacher-Guided, all demonstrations selected by the teacher without trust feedback from the algorithm and without the ability to perform subsequent corrections \u2022 CES - Confident Execution, all demonstrations selected by the teacher in response to errors made by the agent \u2022 CBA - The Complete Confidence-Based Autonomy Algorithm, combining Confidence-Based Autonomy Algorithm, with multiple adjustable confidence thresholds \u2022 CD - Corrective Demonstration, all demonstrations selected by the teacher and executed as corrections. \u2022 CBA - The Complete Confidence-Based Autonomy Algorithm, combining Confident Execution with multiple adjustable confidence thresholds, with Corrective DemonstrationFor each demonstration selection method, the underlying policy of the agent was learned using multiple Gaussian blend models, one for each action class, presenting the results of each action task (Chernova & Velnova).The 5% of the trust are presented by the algorithms."}, {"heading": "6.2.1 TG Demonstration Selection", "text": "The top line in Figure 8 summarizes the performance of the teacher-selection approach. In this approach, the teacher conducted training by observing the agent's performance and selecting demonstrations that they believed would improve mileage. However, the teacher was removed without further improvement in the agent's performance; the results of the agent's performance evaluations were demonstrated at intervals of 100. Instead, the teacher had to anticipate what data would improve policy; the training process was terminated after the teacher saw no further improvement in the agent's performance; Figure 8 shows the results of the agent's performance evaluation at intervals; the similarity in the agent's lane preference improves, with significant fluctuations, with significant fluctuations in fluctuations."}, {"heading": "6.2.4 CD Demonstration Selection", "text": "This year it is so far that it only takes a few days to reach an agreement."}, {"heading": "7. Discussion", "text": "In this section we discuss several promising directions for future work as well as a number of existing extensions of the learning methods presented."}, {"heading": "7.1 Evaluation with Non-Technical Users", "text": "The demonstration learning algorithm presented provides a fast and intuitive method for programming and adapting the behavior of autonomous agents. We believe that its general representation and classification-independent approach makes CBA usable for a wide range of applications, with a particular application being the use of demonstration learning to enable non-technical users to program autonomous agents. We believe that CBA would be highly suitable for this application as it does not assume that the teacher has technical knowledge of policy learning, but only that the teacher is an expert in the task. The results presented in this article were obtained with a single teacher, one of the authors."}, {"heading": "7.2 Representation of Action Choices", "text": "Demonstration-based learning provides a natural and intuitive user interface for transferring human task knowledge to autonomous agents. However, when agents operate in a rich environment, they inevitably face situations where multiple actions are equally applicable. For example, an agent who encounters an obstacle directly on his way has the ability to move left or right to avoid it. If the surrounding space is empty, both directions are equally valid for performing the desired task. Human demonstrators facing a choice of equivalent actions typically do not consistently perform demonstrations, but choose arbitrarily each time between the applicable actions. As a result, the training data collected by the agent lacks consistency, so that identical or near-identical states are associated with different actions. In the presented CBA algorithm, such contradictory demonstrations would lead to a persistent region of low trust, resulting in the agent repeatedly requesting demonstrations within the contradictory domain."}, {"heading": "7.3 Improvement Beyond Teacher Performance", "text": "Assuming that the teacher is an expert in the task, our approach aims to mimic the behavior of the teacher. However, in many areas, teacher demonstrations can be suboptimal and limited by human skills. Several demonstration learning approaches have been developed that allow an agent to learn from his own experiences in addition to demonstrations, thereby improving performance beyond the capabilities of the teacher (Stolle & Atkeson, 2007; Smart, 2002). Extending the CBA algorithm to similar skills remains a promising direction for future work. Possible approaches include the inclusion of high-level feedback (Argall, Browning, & Veloso, 2007) or reward signals (Thomaz & Breazeal, 2006) from the teacher, as well as filtering loud or inaccurate demonstrations."}, {"heading": "7.4 Policy Use After Learning", "text": "The CBA algorithm considers learning to be complete once the agent is able to perform the required behavior repeatedly and correctly without requiring further demonstrations and making corrections. Once political learning is complete, the standard procedure for the vast majority of policy learning algorithms is to shut down the learning process and freeze policy. Although this approach can also be used with our algorithm, we suggest that continued use of the Confident Execution component could have long-term benefits beyond political learning. Specifically, the algorithm's ability to detect anomalies could allow the agent to detect system errors and unexpected inputs and communicate them to the user. While further studies are needed to evaluate this use of the algorithm, we believe that such a mechanism would provide a useful safety feature for long-term autonomous operation at negligible cost of performing the threshold comparison at any time."}, {"heading": "7.5 Richer Interaction", "text": "The demonstration approach presented is based on a limited form of agent-teacher interaction; the agent requests demonstrations from the teacher, while the teacher responds with a single recommendation for action. Although this type of interaction is typical of traditional active learning approaches, it does not fully utilize the teacher's vast task knowledge. We believe that extending the algorithm to include more comprehensive interaction skills could provide a faster and more intuitive training method. In this area, there are many promising directions for future research. For example, the development of a domain-independent dialogue exchange between agent and teacher, which includes high-level clarification questions and advice, could accelerate learning and enable the agent to represent the high objectives of the task. The ability to play or \"rewind\" demonstration sequences would additionally enable the teacher and agent to review and re-evaluate past learning experiences."}, {"heading": "7.6 Application to Single-Robot and Multi-Robot Systems", "text": "In other work, we have shown that the CBA algorithm is very effective in learning a variety of individual robot tasks (Chernova & Veloso, 2007, 2008a).In addition, many complex tasks require multi-robot collaboration; one of the biggest challenges that has so far prevented most demonstration learning algorithms from generalizing to multi-robot areas is the problem of limited human attention, the fact that the teacher is unable to pay attention to and interact with all robots at the same time. Based on the CBA algorithm, we have developed the first multi-robot demonstration learning system that addresses the limited human attention problem by taking advantage of the fact that the Reliable Execution component of CBA prevents autonomous execution of actions in low-trust states (Chernova & Veloso, 2008b)."}, {"heading": "8. Conclusion", "text": "In this article, we presented Confidence-Based Autonomy, an interactive algorithm for political learning through demonstration. Using this algorithm, an agent learns incrementally an action policy from demonstrations gained during the exercise of the task. CBA algorithm includes two methods to obtain demonstrations; the Confident Execution component allows the agent to select demonstrations in real time while interacting with the environment, using trust and distance thresholds to attack states that are unfamiliar or where current political action is uncertain; the Corrective Demonstration component allows the teacher to perform additional corrective demonstrations when a wrong action is selected by the agent; and the teacher provides retroactive demonstrations for specific cases of error, rather than attempting to anticipate mistakes in advance of time."}, {"heading": "Acknowledgments", "text": "This research was partially sponsored by the Department of the Interior, the National Business Center under contract number NBCHD030010, and SRI International under contract number 03-000211, and BBNT Solutions under contract number 950008572 under Prime Air Force Contract number SA-8650-06-C-7606. The views and conclusions contained in this document are those of the authors and should not be interpreted to represent the official policies of any supporting institution, the U.S. government, or any other entity, either explicitly or implicitly. Further, thanks to Paul Rybski for providing his simulation package."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["B. Argall", "S. Chernova", "B. Browning", "M. Veloso"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Learning from demonstration with the critique of a human teacher", "author": ["B. Argall", "B. Browning", "M. Veloso"], "venue": "In Second Annual Conference on Human-Robot Interactions (HRI \u201907),", "citeRegEx": "Argall et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2007}, {"title": "Robot learning from demonstration", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Atkeson and Schaal,? \\Q1997\\E", "shortCiteRegEx": "Atkeson and Schaal", "year": 1997}, {"title": "Learning to act from observation and practice", "author": ["D.C. Bentivegna", "A. Ude", "C.G. Atkeson", "G. Cheng"], "venue": "International Journal of Humanoid Robotics,", "citeRegEx": "Bentivegna et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bentivegna et al\\.", "year": 2004}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial Intelligence,", "citeRegEx": "Blum and Langley,? \\Q1997\\E", "shortCiteRegEx": "Blum and Langley", "year": 1997}, {"title": "Skill acquisition and use for a dynamicallybalancing soccer robot", "author": ["B. Browning", "L. Xu", "M. Veloso"], "venue": "In Proceedings of Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "Browning et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Browning et al\\.", "year": 2004}, {"title": "Confidence-based policy learning from demonstration using gaussian mixture models", "author": ["S. Chernova", "M. Veloso"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Chernova and Veloso,? \\Q2007\\E", "shortCiteRegEx": "Chernova and Veloso", "year": 2007}, {"title": "Learning equivalent action choices from demonstration", "author": ["S. Chernova", "M. Veloso"], "venue": "Proceedings of the International Conference on Intelligent Robots and Systems,", "citeRegEx": "Chernova and Veloso,? \\Q2008\\E", "shortCiteRegEx": "Chernova and Veloso", "year": 2008}, {"title": "Teaching collaborative multi-robot tasks through demonstration", "author": ["S. Chernova", "M. Veloso"], "venue": "In Proceedings of the IEEE-RAS International Conference on Humanoid Robots", "citeRegEx": "Chernova and Veloso,? \\Q2008\\E", "shortCiteRegEx": "Chernova and Veloso", "year": 2008}, {"title": "On integrating apprentice learning and reinforcement learning", "author": ["J.A. Clouse"], "venue": "Ph.D. thesis,", "citeRegEx": "Clouse,? \\Q1996\\E", "shortCiteRegEx": "Clouse", "year": 1996}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Dogged learning for robots", "author": ["D. Grollman", "O. Jenkins"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Grollman and Jenkins,? \\Q2007\\E", "shortCiteRegEx": "Grollman and Jenkins", "year": 2007}, {"title": "Acquisition of probabilistic behavior decision model based on the interactive teaching method", "author": ["T. Inamura", "M. Inaba", "H. Inoue"], "venue": "In Proceedings of the Ninth International Conference on Advanced Robotics,", "citeRegEx": "Inamura et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Inamura et al\\.", "year": 1999}, {"title": "Tutelage and socially guided robot learning", "author": ["A. Lockerd", "C. Breazeal"], "venue": "In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Lockerd and Breazeal,? \\Q2004\\E", "shortCiteRegEx": "Lockerd and Breazeal", "year": 2004}, {"title": "A framework for learning from demonstration, generalization and practice in human-robot domains", "author": ["M.N. Nicolescu"], "venue": "Ph.D. thesis,", "citeRegEx": "Nicolescu,? \\Q2003\\E", "shortCiteRegEx": "Nicolescu", "year": 2003}, {"title": "Integrating advice with reinforcement learning. Master\u2019s thesis, University of Texas at Arlington", "author": ["V. Papudesi"], "venue": null, "citeRegEx": "Papudesi,? \\Q2002\\E", "shortCiteRegEx": "Papudesi", "year": 2002}, {"title": "Accelerating reinforcement learning through implicit imitation", "author": ["B. Price", "C. Boutilier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Price and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Price and Boutilier", "year": 2003}, {"title": "Teaching robots by moulding behavior and scaffolding the environment", "author": ["J. Saunders", "C.L. Nehaniv", "K. Dautenhahn"], "venue": "In Proceeding of the 1st ACM SIGCHI/SIGART conference on Human-robot interaction,", "citeRegEx": "Saunders et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 2006}, {"title": "Learning from demonstration", "author": ["S. Schaal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schaal,? \\Q1997\\E", "shortCiteRegEx": "Schaal", "year": 1997}, {"title": "Making Reinforcement Learning Work on Real Robots", "author": ["W.D. Smart"], "venue": "Ph.D. thesis,", "citeRegEx": "Smart,? \\Q2002\\E", "shortCiteRegEx": "Smart", "year": 2002}, {"title": "Knowledge transfer using local features", "author": ["M. Stolle", "C.G. Atkeson"], "venue": "In Proceedings of IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Stolle and Atkeson,? \\Q2007\\E", "shortCiteRegEx": "Stolle and Atkeson", "year": 2007}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A hierarchical multi-module learning system based on self-interpretation of instructions by coach", "author": ["Y. Takahashi", "K. Hikita", "M. Asada"], "venue": "In Proceedings of RoboCup 2003: Robot Soccer World Cup VII,", "citeRegEx": "Takahashi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Takahashi et al\\.", "year": 2004}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "In Proceedings of the Twenty-First Conference on Artificial Intelligence,", "citeRegEx": "Thomaz and Breazeal,? \\Q2006\\E", "shortCiteRegEx": "Thomaz and Breazeal", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "Compared to exploration-based methods, demonstration learning often reduces the learning time and eliminates the frequently difficult task of defining a detailed reward function (Smart, 2002; Schaal, 1997).", "startOffset": 178, "endOffset": 205}, {"referenceID": 19, "context": "Compared to exploration-based methods, demonstration learning often reduces the learning time and eliminates the frequently difficult task of defining a detailed reward function (Smart, 2002; Schaal, 1997).", "startOffset": 178, "endOffset": 205}, {"referenceID": 16, "context": "A number of approaches for taking advantage of this information have been developed, such as deriving or modifying the reward function based on demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng, 2004; Papudesi, 2002; Atkeson & Schaal, 1997), and using the demonstration experiences to prime the agent\u2019s value function or model (Takahashi, Hikita, & Asada, 2004; Price & Boutilier, 2003; Smart, 2002; Schaal, 1997).", "startOffset": 159, "endOffset": 243}, {"referenceID": 20, "context": "A number of approaches for taking advantage of this information have been developed, such as deriving or modifying the reward function based on demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng, 2004; Papudesi, 2002; Atkeson & Schaal, 1997), and using the demonstration experiences to prime the agent\u2019s value function or model (Takahashi, Hikita, & Asada, 2004; Price & Boutilier, 2003; Smart, 2002; Schaal, 1997).", "startOffset": 330, "endOffset": 416}, {"referenceID": 19, "context": "A number of approaches for taking advantage of this information have been developed, such as deriving or modifying the reward function based on demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng, 2004; Papudesi, 2002; Atkeson & Schaal, 1997), and using the demonstration experiences to prime the agent\u2019s value function or model (Takahashi, Hikita, & Asada, 2004; Price & Boutilier, 2003; Smart, 2002; Schaal, 1997).", "startOffset": 330, "endOffset": 416}, {"referenceID": 20, "context": "Demonstration has also been coupled with supervised learning algorithms for policy learning, including Locally Weighted Regression for low level skill acquisition (Grollman & Jenkins, 2007; Browning, Xu, & Veloso, 2004; Smart, 2002), Bayesian networks for high level behaviors (Lockerd & Breazeal, 2004; Inamura, Inaba, & Inoue, 1999), and the k-nearest neighbors algorithm for fast-paced games and robot navigation tasks (Saunders, Nehaniv, & Dautenhahn, 2006; Bentivegna, Ude, Atkeson, & Cheng, 2004).", "startOffset": 163, "endOffset": 232}, {"referenceID": 10, "context": "In the context of reinforcement learning, the \u2018Ask For Help\u2019 framework enables an agent to request advice from other agents when it is \u201cconfused\u201d about what action to take, an event characterized by relatively equal quality estimates for all possible actions in a given state (Clouse, 1996).", "startOffset": 276, "endOffset": 290}, {"referenceID": 15, "context": "Similarly motivated techniques have been used in robotics to identify situations in which a robot should request a demonstration from its teacher (Grollman & Jenkins, 2007; Lockerd & Breazeal, 2004; Nicolescu, 2003; Inamura et al., 1999).", "startOffset": 146, "endOffset": 237}, {"referenceID": 13, "context": "Similarly motivated techniques have been used in robotics to identify situations in which a robot should request a demonstration from its teacher (Grollman & Jenkins, 2007; Lockerd & Breazeal, 2004; Nicolescu, 2003; Inamura et al., 1999).", "startOffset": 146, "endOffset": 237}, {"referenceID": 13, "context": "Previous algorithms utilizing a classification confidence threshold for behavior arbitration have all used a manually-selected single threshold value (Inamura et al., 1999; Lockerd & Breazeal, 2004; Grollman & Jenkins, 2007).", "startOffset": 150, "endOffset": 224}, {"referenceID": 20, "context": "Several demonstration learning approaches have been developed that enable an agent to learn from its own experiences in addition to demonstrations, thereby improving performance beyond the abilities of the teacher (Stolle & Atkeson, 2007; Smart, 2002).", "startOffset": 214, "endOffset": 251}], "year": 2009, "abstractText": "We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration. The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. CBA and its individual components are compared and evaluated in a complex simulated driving domain. The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.", "creator": null}}}