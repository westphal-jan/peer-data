{"id": "1401.3443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Computational Logic Foundations of KGP Agents", "abstract": "This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agents state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.", "histories": [["v1", "Wed, 15 Jan 2014 04:54:59 GMT  (401kb)", "http://arxiv.org/abs/1401.3443v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["antonis kakas", "paolo mancarella", "fariba sadri", "kostas stathis", "francesca toni"], "accepted": false, "id": "1401.3443"}, "pdf": {"name": "1401.3443.pdf", "metadata": {"source": "CRF", "title": "Computational Logic Foundations of KGP Agents", "authors": ["Antonis Kakas", "Paolo Mancarella", "Fariba Sadri", "Kostas Stathis", "Francesca Toni"], "emails": ["antonis@ucy.ac.cy", "paolo.mancarella@unipi.it", "fs@doc.ic.ac.uk", "kostas@cs.rhul.ac.uk", "ft@doc.ic.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. KGP Model: Outline", "text": "In this section, we will give an overview of the KGP agent model and its components and provide some informal examples of how it works. This model is based on \u2022 an internal (or mental) state that holds the agent's knowledge base (beliefs), goals (desires) and plans (intentions), \u2022 a set of reasoning skills, \u2022 a set of physical skills, \u2022 a set of transition rules that determine how the agent's state changes and are defined by the above skills, \u2022 a set of selection perators that enable and provide appropriate input for the transitions, \u2022 a cycle theory that provides the control for deciding which transitions should be applied when. The model is defined in a modular way by grouping different activities into different abilities and transitions, and the control is a separate module. Furthermore, the model has a hierarchical structure that is shown in Figure 1."}, {"heading": "2.1 Internal State", "text": "This is a tuple < KB0, F, C, \u03a3 >, where: OPERATORSCYCLE THEORYSELECTION TRANSITIONST EAS TPHYSICAL CAPABILITY CAPABILITIES REASONING \u2022 C is the Temporal Constraint Store, i.e. a set of constraint atoms in a specific underlying constraint language. These restrict the time variables of the targets in F. For example, they can set a time window within which the time of an action can be instantiated at execution time."}, {"heading": "2.2 Reasoning Capabilities", "text": "KGP supports the following reasoning skills: \u2022 Planning, which generates plans for mental goals that are given as input. These plans consist of time-limited partial goals and actions designed to achieve the plaster goals. \u2022 Reactivity, which is used to provide new reactive goals at the highest level, in response to perceived changes in the environment and the agent's current plans. \u2022 Target decision, which is used to review the non-reactive goals at the highest level and to adjust the agent's condition to changes in the environment. \u2022 Identification of conditions and effects for actions that are used to determine appropriate sensor actions, to verify whether actions can be carried out safely (if their conditions are known) and whether recently implemented actions have been successful (by verifying whether some of their known effects persist). \u2022 Temporal reasoning, which allows the agent to think about the evolving environment, and to make predictions about characteristics, including non-viable objectives, based on the parasitic nature of the long term."}, {"heading": "2.3 Physical Capabilities", "text": "In addition to reasoning skills, a KGP agent is equipped with \"physical\" skills that connect the agent to his environment, consisting of \u2022 a sensor ability that allows the agent to observe that properties hold or do not hold and that other agents have performed actions. \u2022 An actuability to perform (physical and communicative) actions."}, {"heading": "2.4 Transitions", "text": "The condition < KB0, F, C, \u03a3 > of an agent develops through the application of transitional rules that use the capabilities as follows: \u2022 Target Introduction (GI), possibly changing the highest-level targets in F and C and using target decisions. \u2022 Plan Introduction (PI), possibly changing F and C and using planning. \u2022 Reactivity (RE), possibly changing the highest-level reactive targets in F and C and using the reactivity capability. \u2022 Sensing Introduction (SI), possibly introducing new sensor measures in F to check the preconditions of measures already in F. \u2022 Passive Observation Introduction (POI), updating KB0 by recording adverse information from the environment and using sensing. \u2022 Active Observation Introduction (AOI), possibly updating \u03a3 and KB0, by recording the result of (actively sought) sensor actions and using sensing. \u2022 Action Execution (AE), execution of all types of actions and execution of Sint (SOI)."}, {"heading": "2.5 Cycle and Selection Operators", "text": "The behavior of an agent is determined by the application of transitions in sequences, repeatedly changing the state of the agent. These sequences are not determined by fixed behavior cycles, as in conventional agency architectures, but by reasoning with cycle theories. Cycle theories define preference strategies compared to the sequence of application of transitions, which may depend on the environment and internal state of an agent. They rely on the use of selection performers to determine which transitions are possible and what their inputs should be, as follows: \u2022 Action selection for inputs in AOI; this selection perator uses the Temporal Reasoning and Constraint Solving Capabilities; \u2022 Target selection for inputs in PI; this selection perator uses the Temporal Reasoning and Constraint Solving Capabilities; \u2022 Action selection for inputs in AOI; this selection perator uses the identification of effects in Rationskriving Capabilities and Solving Conditions; \u2022 This selection performer is used in this selection process to determine the capability of the input in the input conditionI;"}, {"heading": "2.6 Examples", "text": "We take all our examples from an omnipresent computer scenario, which we call the San Vincenzo scenario, presented by de Bruijn and Stathis (2003) and summarized as follows: A businessman travels to Italy for work and, in order to facilitate his journey, carries with him a personal communicator, a device that is a hybrid between mobile phone and PDA: this device is the businessman's KGP agent and can be considered a personal service provider (Mamdani, Pitt, & Stathis, 1999) (or PSA for short), because it offers proactive information management and flexible connectivity to intelligent services available in the global environment in which the businessman moves."}, {"heading": "2.6.1 Setting 1", "text": "The contractor's PPE requests to an agent of the \"San Vincenzo Station,\" svs, the arrival time of the train tr01 from Rome. Since svs does not have this information, it replies with a refusal. Later, svs receives information about the arrival time of the train tr01 from an agent of the \"Central Office,\" Co. If the PSA again requests the arrival time of tr01, svs accepts the request and provides the information. This first example requires that one uses the responsiveness to model rules of interaction and the RE transition (a) to achieve an interaction between agents, and (b) to make dynamic adjustments to the behavior of the agent to changes that allow different responses to the same request, depending on the current situation of the agent. In this case, the interaction is a form of negotiation of resources between agents where resources are objects of information. Thus, the current situation of the agents amounts to the refusal of what resources / information the agents currently possess."}, {"heading": "2.6.2 Setting 2", "text": "In preparation for the businessman's next trip, his psa aims to get a plane ticket from Madrid to Denver 3. may part plans 1. as well as obtain a visa for the U.S. One possible way to buy plane tickets is via the Internet. Purchasing tickets in this way is usually possible, but not for all destinations (depending on whether the airlines flying to the destinations sell tickets over the Internet or not) and not without an Internet connection. At present, the psa has no connection, nor the information that Denver is actually a destination for which the tickets can be purchased online. It plans to purchase the ticket anyway via the Internet, but verifies the conditions before executing the planned action. After successfully purchasing the ticket, psa focuses on the second goal, obtaining a visa. This can be achieved by contacting the U.S. Embassy in Madrid, but the application requires an address in the U.S. This address can be reached by checking for a hotel in Denver.This example illustrates the form of \"divisive\" plans, the planning of the partial goals, the 2. Where partial plans are accepted as well as partial plans in the U.S."}, {"heading": "3. Background", "text": "In this section we provide the necessary background for the reasoning skills and cycle theory of KGP agents, namely: \u2022 Constraint Logic Programming, which permeates the entire model; \u2022 Abductive Logic Programming, the heart of planning, reactivity and temporal reasoning capabilities; and \u2022 Priority logic programming, the heart of goal decision ability and cycle theories."}, {"heading": "3.1 Constraint Logic Programming", "text": "These predicates are typically used to limit the values that variables can take in the conclusion of a rule (along with a unification that is also dealt with via an equality predicate).In the CGP model, constraints are used to determine the value of time variables, in goals and actions, with appropriate time constraints. The CLP framework is defined by a structure < consisting of a domain D (<) and a set of constraints that include equality, along with an assignment of relationships to D (<)."}, {"heading": "3.2 Abductive Logic Programming with Constraints", "text": "The question is whether this is a program that does not occur in the head of a clause of P. (without loss of generality), see (Kakas et al., 1998). Atoms whose predicate is called abductible are called abductible atoms or simply as abductible limitations, that is, a series of judgments in the language of P. All integrity limitations in the KGP model have the implicit formal model."}, {"heading": "3.3 Logic Programming with Priorities", "text": "For the purposes of this paper, a logic program with priorities over a constraint structure <, referred to as T, consists of four parts: (i) a low-threshold or basic P consisting of a logic program with constraints; each rule in P is assigned a name that is a term; e.g., such a rule (X, Y) could be p (X) such a priority could be beh (X), r (Y) named n (X) naming each basis of the rule; (ii) a high-level H specifying conditional, dynamic priorities under the rules in P or H; e.g., such a priority could be beh (X): m (X) n (X). The rule itself is to be read: if (an instance of) condition c (X) applies, then (the corresponding instance of) the rule named by m (X) should have a higher priority than (the corresponding instance of n (X)."}, {"heading": "4. The State of KGP Agents", "text": "In this section, we will formally define the concept of the state for a CGP agent. We will also present all the notations that we will use in the rest of the paper to refer to state components. Where necessary, we will also try to illustrate our discussion with simple examples."}, {"heading": "4.1 Preliminaries", "text": "In the KGP model we assume (possibly infinite) vocabulary of: \u2022 fluent, with f, f,.., \u2022 action operators using a, a,..., \u2022 time variables, with the exception of c, c,.., \u2022 time constants using t, t,.,.., 1, 2,.., stands for natural numbers (we also often use the constant to indicate the current time) \u2022 names of agents that are specified with c, c,.,. \u2022 constants, other than the above, normally are specified with lower case, e.g. r, r1,. \u2022 a given constraint language, including constraints, dictated < \u2264, >, 6 =, with respect to some structure < (such as the natural numbers) and equipped with an idea of constraint satisfaction."}, {"heading": "4.2 Forest: F", "text": "Each node in each tree in F is: \u2022 Either a non-executable target, namely a (non-down-to-earth) target (non-down-to-earth) flowing literally, \u2022 or an executable target, namely a (non-down-to-earth) timed action literally. An example of a tree in F is given in Figure 2, where p2 is a specific problem that the agent (c1) must solve by obtaining two resources r1 and r2, and where the agent has already decided to obtain r1 from another agent c2 and has already planned to ask c2 through the communication action (c1, c2, request (r1), d, perc4). For example, in the San Vincenzo scenario, p2 can \"arrange transfer to airport needs,\" r1 can be a taxi, and c2 can be a taxi company needed for transportation to any station, and finally r2 can ticket.Note that the temporal is not executable in targets."}, {"heading": "4.3 Temporal Constraint Store: C", "text": "This is a set of constraints called time constraints, in the given underlying constraint language. Temporary constraints refer to both time constants and time variables associated with goals (current or former) in the state. \u2022 For example, a forest with the tree in Figure 2, C \u03c41 > 10, \u03c41 \u2264 20, indicating that the ultimate goal (fixing the problem p2) must be achieved within the time interval (10, 20], \u03c42 < \u03c41, \u03c43 < \u03c41, indicating that resources r1 and r2 must be acquired before the ultimate goal can be achieved, and \u03c44 < \u03c42, indicating that the agent must first ask the agent c2. Let's not consider that we must impose their properties < \u03c41 that resources r2 and r2 must be acquired before the goal can be achieved, and the ultimate goal cannot be achieved."}, {"heading": "4.5 Instantiation of Time Variables: \u03a3", "text": "If a time variable occurring in a non-executable target \"[\u03c4] or an action a [\u03c4] in F is instantiated at a time constant t (e.g. at the time of execution of an action), the actual instance \u03c4 = t is recorded in the \u03a3 component of the agent. For example, if the action referred to in Figure 2 as \u03c44 is performed at a time 7, then \u03c44 = 7 is added to the time at which it is executed. Using \u03a3 makes it possible to capture the instantiation of time variables while distinguishing different targets with the same fluid. It is worth pointing out for each time variable that there is at most one equality Equility = t."}, {"heading": "5. Reasoning Capabilities", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "5.1 Temporal Reasoning, Planning, Reactivity, Identification of Preconditions and Effects: EC-based Capabilities", "text": "These reasoning skills are all specified in the Event Calculus (EC) to argue actions, events, and changes (Kowalski & Sergot, 1986).In the following, we first specify the core EC and then show how to use it to define the various skills in this section."}, {"heading": "5.1.1 Preliminaries: Core Event Calculus", "text": "The most important meta-predicates of formalism are: \u2022 clicked off (T1, F, T2) - a flowing F is cut off (from durability) - a flowing F is cut off (from durability) between timeT1 and T2; \u2022 sanded off (T1, F, T2) - a flowing F is cut off (from durability) between timeT1 and T2; \u2022 sanded off (T1, F, T2) - a flowing F is cut off (from durability) between times T1 and T2; initially (F) - a flowing F stops at the initial time, say time 0; \u2022 happens (O, T) - an operation O occurs at a time T; \u2022 initiated (O, F) - a flowing F begins to hold O after an operation."}, {"heading": "5.1.2 Temporal Reasoning", "text": "The temporal reasoning capability is used by other components of the KGP model (i.e. the ability to make a decision, the transition to the state audit, and some of the selection operators, see Section 7) to prove or refute that a particular (possibly time-limited) fluid wording applies to a given theory KBTR. For the purposes of this work, KBTR is an EC theory defined by the domain-independent and domain-independent parts specified in Section 5.1.1 and the \"narrative\" part specified by KB0. Then, for a state S, a flowing letter 'and a possibly empty series of temporal constraints TC, the temporal reasoning capability | = TR is defined as S | = TR, \"for a state S | = TC iff KBTR | = LP (<) a fluent wording' and a possibly empty series of temporal constraints TC applies (For example, the temporal reasoning capability of 1.1 applies to section < < < if section 5.1.1 = < < < < < <) for a state S, a state of KBTR is specified in Section 5.1."}, {"heading": "5.1.3 Planning", "text": "A number of abductive variants of the EC have been suggested in the literature to deal with planning problems (e.g. the one proposed by Shanahan). (Here, we propose a novel variant inspired somewhat by the E language (Kakas & Miller, 1997) to allow situated actors to generate sub-plans in a dynamic environment. (We refer to KBplan = < Pplan, Aplan, Iplan, Iplan > as the abductive logic program, in which: 10. Here and in the rest of the paper sets are considered conjunctions where appropriate. \u2022 Aplan = {pauses, assumes that it happens}, namely, we consider two deductible predictors that correspond to the assumption that a fluent holds or that an action occurs at a particular time; \u2022 Pplan is achieved by adding to the core EC axioms and the \"narrative\" given by KB0. (O, T) the following rules pass (T), T (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T)) (T)) (T) (T)) (T) ("}, {"heading": "5.1.4 Reactivity", "text": "(KBreact = < Preact, Ireact > where \u2022 Preact = Plan 11. For the sake of simplicity, we present the case for planning only for individual goals. (Areact = Aplan \u2022 RRR is a set of reactive limitations, the body \u21d2 reaction, TCwhere \u2022 reaction is either an assumption (\", T) that is a time-controlled, fluid, literal, or an assumption. (T) that is a time-controlled action literal, 12 and \u2022 the body is a non-empty conjunction of items of form (where\" X \"is a time-controlled, literal, and a time-controlled action."}, {"heading": "5.1.5 Identification Of Preconditions", "text": "This ability is used by KGP agents to determine the prerequisites for the executability of actions that are planned. These prerequisites are defined in the domain-dependent part of the EC by a set of rules of form prerequisite (O, F), which means that the flowing F is a prerequisite for executing an action with the action operator O (see 5.1.1). Let KBpre be the subset of KBTR that contains the rules that define prerequisite (,). Then, the identification of prerequisites ability | = prerequisite is specified as follows: At a state S = < KB0, F, C, \u03a3 > and a timed action literally a [\u03c4] S, a [\u03c4] | = pre CsiffCs = ['[\u03c4] | KBpre | = LP prerequisite (a,') 14."}, {"heading": "5.1.6 Identification Of Effects", "text": "This capability is used by KGP agents to determine the effects of actions that have already been executed to check whether those actions have been successful. Note that actions may have been unsuccessful because they could not be executed or were executed but did not have the expected effect, both of which are possible in situations where the agent does not have complete knowledge of the environment in which he is located. These effects are defined in the domain-dependent part of the EC by the set of rules that initiates and terminates the predicates. Let KBeff be the theory consisting of the domain-dependent and domain-independent parts of the EC, as well as the narrative part of KB0. Subsequently, the identification of effects | = eff | = LP is specified as follows: For a state S = < KB0, F, C, \u03a3 > and an action operator a [t], S, a [t] | = eff'iff \u2022 'and KBeff | = LP (a, f \u00ac ff)."}, {"heading": "5.2 Constraint Solving", "text": "The ability to solve constraints can be defined simply by the structure < and the term set forth in Section 3.1 | = < namely, in the face of a state S = < KB0, F, C, \u03a3 > and a series of constraints TC: \u2022 S | = cs TC iff | = < C \u0435\u043d\u044b\u0445 TC; \u2022 there is an overall evaluation \u03c3, so that S, \u03c3 | = cs TC iff exists an overall evaluation \u03c3, so \u03c3 | = < C \u0445\u0445\u0445\u043e\u0433\u043e\u0441\u0442\u0430 TC."}, {"heading": "5.3 Goal Decision", "text": "The decision-making ability allows the agent to decide at a given time whether the (non-reactive) targets should be pursued at the highest level, for which he will then be able to create plans that aim to achieve them. < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p > p > p > p > p > p > p > p > p > p > p p > p > p > p p p p > p > p p > p > p > p > p > p > p > p p > p > p > p > p > p > p > p > p p p p p > p p p p p p p > p > p"}, {"heading": "6. Physical Capabilities", "text": "In addition to the reasoning skills that we have defined so far, an agent is equipped with physical skills that allow him to experience the world in which he finds himself; this world consists of other agents and / or objects that constitute an environment for the agents in which he interacts and communicates. We will also assume that these sensors and actuators are part of the body of the agent that we classify as a matter of implementation (Stathis et al., 2004). The physical sensory skills models of the way an agent interacts with and influences his external environment to find out whether these sensors and actuators are part of the body that we classify as a matter of implementation (Stathis et al.). The physical sensory skills models of action that an agent interacts with his external environment to inspect them are, for example, to find out whether some fluents are held at a certain time or not."}, {"heading": "7. Transitions", "text": "The KGP model is based on the following transitions GI, PI, RE, SI, POI, AOI, AE, SR, using the following representation (T) < KB0, F, C, \u03a3 > X < KB \u2032 0, F \u2032, C \u2032, \u03a3 \u2032 > now that T is the name of the transition, < KB0, F, \u03a3 > is the state of the agent before the transition is applied, X is the input for the transition, now is the date of application of the transition, < KB \u2032 0, F \u2032, \u03a3 \"is the revised state resulting from the application of the transition T with input X now in the state < KB0, F, C, \u03a3 >. Please note that most transitions modify only some of the components of the transition."}, {"heading": "7.1 Goal Introduction", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "7.2 Reactivity", "text": "If no new reactive targets exist, the reactive part of the new state will be empty. (RE) < KB0, F, C, \u03a3 > < KB0, F \u2032, C \u2032, \u03a3 > now that S = < KB0, F, C, \u03a3 >: (i) If S | = now responds, then \u2022 F \u2032 is defined as follows: - r (F \u2032) = {} - nr (F \u2032) = nr (F \u2032) = nr (F \u2032) \u2022 C \u2032 = C (ii) otherwise, if S | = now responds (X s, TC), then \u2022 F \u2032 is defined as follows: - nr (F \u2032) = nr (F \u2032) = nr (F \u2032) - nr (F \u2032) = nr (F \u2032) - nr (F \u2032) - reactive, that it is reactive, that it is reactive, that it is \u2032 r (R \u2032) {- (R)."}, {"heading": "7.3 Plan Introduction", "text": "This transition takes as input a non-executable target in the state (selected by the target selector, see Section 8) and creates a new state by calling the scheduling capability of the agent if the selected target is a mental target, or simply by introducing a new scanning action if the target is a scanning target. (PI) < KB0, F, C, \u03a3 > G < KB0, F, C \u2032, \u03a3 > where G is the input target (selected for scheduling in any tree T in F, and thus a sheet, see Section 8) and F \u2032 = (F\\ {T | G is a sheet in T}). (New C = C \u2032 TCwhere New and TC are obtained as follows, S is < KB0, F, C, \u03a3 >. (i) If G is an mental target: leave S, G | = nowplan P. Then - either P = T} and TC = {} - or P = a child is {X, C is a new target, T is {T and T is a new element. (T)"}, {"heading": "7.4 Sensing Introduction", "text": "This transition takes as input a set of flowing dictionaries that are prerequisites for some actions in the state, and creates a new state by adding meaningful actions as leaves in (suitable) trees in its forest component. (SI) < KB0, F, \u03a3 > SPs are selected by the selection operator for the prerequisite when SI is called and selected under the prerequisites of actions that are not already known to be true (see section 8). (SI) < KB0, F, \u03a3 > SPs < KB0, F, C, \u03a3 > now with SPs a non-empty set of prerequisites for action (in the form of pairs of \"prerequisite, action\") in some trees in F, where in view of this: - New = < 'A, sense (', determination ') > < A, sense (', determination) > T (< T), lt; T, [], [] A, 'is \"fresh.\""}, {"heading": "7.5 Passive Observation Introduction", "text": "This transition updates KB0 by adding new observed facts that reflect changes in the environment. These observations are not made intentionally by the pathogen, but \"imposed\" on the pathogen by the environment. These observations can be properties in the form of positive or negative liquids (for example, that the battery is empty) or executed by other pathogens (for example, messages directed to the pathogen). (POI) < KB0, F, C, \u03a3 > < KB \u2032 0, F, C, \u03a3 > Now, if scan (now) = L, then KB0 = KB0 {observed (now) | f: true what is observed (now) | f: false what is (now) happening."}, {"heading": "7.6 Active Observation Introduction", "text": "This transition updates KB0 by adding new facts that the active ingredient is consciously observing to determine whether or not certain liquids are holding at a particular time. These liquids are selected by the action selector (see Section 8) and given as input for the transition. While POI is not \"decided\" by the active ingredient (the active ingredient is \"interrupted\" and forced to be observed by the environment), AOI is intended. Furthermore, POI can observe liquids and actions, while AOI only takes liquids into account (these are effects of actions carried out by the active ingredient, as we will see in Section 8 and Section 9). (AOI) < KB0, F, C, \u03a3 > SFs < KB \u2032 0, F, C, \u03a3 > now, SFs = {f1,..., fn}, n > 0, is a series of liquids selected to be actively perceived (by the effect selector now, now, if they are perceived (SFf), now, and now (SFf)."}, {"heading": "7.7 Action Execution", "text": "This transition updates KB0 by recording the execution of actions by the Agent. (AE) < KB0, F, C, \u03a3 > SAs < KB \"0, F, C, \u03a3\" > Now, SAs is a non-empty series of actions selected for execution (by the Action Selection Operator), and \"let A\" be the subset of all imperceptible actions in SAs and S be the subset of all perceptual actions in SAs; \"let the capture (S) (S,\" now) = L, \"where S\" = f \"(f\") = L, \"where the scanning (S,\" now) = L, \"where S\" (S, \"now) and\" S \"where S is\" imperceptible \"(S,\" now)."}, {"heading": "7.8 State Revision", "text": "The SR transition revises a state by removing all time-defined goals and actions and all goals and actions that have become obsolete so far because one of their ancestors already assumes that they have been achieved. We will use the following terminology.Notation 7.1 Given a state S, a time-flowing literal \"[\u03c4], a time-flowing literal or action-related operator x [\u03c4], and a time now: \u2022 Achieved (S,\" [\u03c4], now indicates that there is an overall valuation, that there is an overall valuation, that S, | = cs, now and S | = TR \"(T\"), a time-delimited operator x \"(S, x,\" now), does not represent an overall valuation, so that S, \u03c3 | = cs, now > now.Then the specification of the transition is as follows. (SR) < KB0, F, F, KB0, K, K > < BF, now where \"F\" is set."}, {"heading": "7.9 Setting 3", "text": "The agent psa has the goal of having a museum ticket for a (state) museum that the businessman wishes to visit and a plan to purchase the ticket. However, before executing the plan, he notes that it is European Cultural Heritage Day (ehd for short), by means of an appropriate \"message\" from another agent mus (representing the museum) declaring that all state museums in Europe issue free tickets on anyone who enters on that day. Then, the goal of the psa is already achieved and both the goal and the plan are deleted from his statute. Let's leave the initial status of the agent < KB0, F, C, \u03a3 > with: \u03a3 = {} = KB0 F = {T} C = {T} 10, \u03c42 = \u03c43, \u03c43 < \u03c41}, where T consists of a high-ranking objective g1 = have (ticket, \u04211), with two children, g2 = have money (\u04212) and 1 = purchase tickets (further) that the tickets are valid for (further), and that the remaining tickets (1) are (subject to)."}, {"heading": "8. Selection Operators", "text": "The KGP model is based on selection operators: \u2022 fGS (target selection, used to provide input to the PI transition); \u2022 fPS (precondition selection, used to provide input to the SI transition); \u2022 fES (effect selection, used to provide input to the AOI transition); \u2022 fAS (action selection, used to provide input to the AE transition).15. g1 and a1 can be reactive or not, as this does not matter in this example. Selection perators are defined in terms of (some of) capabilities (namely temporary reasoning, identification of preconditions and effects, and constraint solving).At a high description level, the selection perators can all be considered to be a return of all elements from a predefined initial group that meet a certain number of conditions. For example, a state selection is given < KB0, F, C, KBK > the target selection action does not fulfill certain conditions in all the selected trees; some actions in the selection perplexed trees do not."}, {"heading": "8.1 Goal Selection", "text": "Informal, the set of conditions for goal selection is as follows: With a state S = < KB0, F, C, \u03a3 > and a time t, the set of goals consisting of an unexecutable goal G in a tree in F, so that at the time t: 1. G is not temporary, 2. no ancestor of G is temporary, 3. no child of an ancestor of G is temporary, 4. neither G nor any ancestor of G in any tree in F. 5. G is an intuitive sheet, condition 1 ensures that G is not already temporary, conditions 2-3 stipulate that G belongs to an \"still viable\" plan for any upper goal in F, and condition 4 ensures that consideration of G is not wasted. Note that, as already mentioned in section 5.1.3, no ancestor of F is chosen. Formally, a state S = < KB0, B2, F-date is not reached."}, {"heading": "8.2 Effect Selection", "text": "Informally, the set of conditions for the effect selection operator is as follows: At a state S = < KB0, F, C, \u03a3 > and a point in time t, fES selects all liquids f so that f or \u00ac f is one of the effects of an action that has been performed \"recently.\" Note that such a f (or \u00ac f) cannot occur in F, but could be a different (observable) effect of the action performed that does not necessarily coincide with the goal that the action contributes to achieving. For example, to check whether an Internet connection is available, the agent may want to observe that he can access a Skype network, even though he is really interested in opening a browser (since he needs a browser to make an online booking). Formally, at a state S = < KB0, F, C, \u0440 > and a point in time, the set of all (time-controlled) liquids that ES chooses is flowing ()."}, {"heading": "8.3 Action Selection", "text": "Informally, the condition for the action selection operator is as follows: In view of a condition S = < KB0, F, C, \u03a3 > and a point in time t, the condition for all actions selected by fAS is defined as follows: X (S, t) is the condition for all actions A in trees in F, so that: 1. A can be executed, 2. No incident of A is temporary, 3. No incident of A is temporary, 4. No incident of A is already fulfilled, 5. No condition of A is known to be false, 6. A has not yet been executed. Then fAS (S, t) X (S, t) is such that all actions in fAS (S, t) are simultaneous executable.Conditions 2-4 impose that A belongs to a \"still executable\" plan for some goals in F."}, {"heading": "8.4 Precondition Selection", "text": "Informally, the set of conditions for the selection operator of the precondition is as follows: for a state S = < C, A > of the (timed) preconditions C and the actions A (F) such that: 1. C is a precondition of A and 2. C, which are not known to be correct in S at t, and 3. A is one of the actions that could be selected to execute if fAS were called at the present time. The reason why this selection operator returns pairs and not simply preconditions is that the transition operator using the results of this selection operator must know the actions associated with the preconditions. This is because for each precondition, SI must introduce sensor actions and place these sensor activities as siblings of the associated actions in F, as in Section 7.4.Formal, 2. a state S = > is a precondition and a precondition C, which is in Section S and a precondition C."}, {"heading": "9. Cycle Theory", "text": "The behavior of KGP agents results from the application of transitions in sequences that repeatedly change the state of the agent. These sequences are not fixed a priori, as in conventional agent architectures, but are dynamically determined by reasoning with declarative cycle theories, giving a form of flexible control. Cycle theories are given within the framework of logic programming with priorities (LPP), as described in Section 3."}, {"heading": "9.1 Formalisation of Cycle Theories.", "text": "Here we use the following new notations: \u2022 T (S, X, S, t) to represent the application of transition T (PI), to represent the application of transition T in the time t in the state S given input X and resulting from it in the state S. \"\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "9.2 Operational Trace", "text": "The cycle theory of an agent is responsible for its behavior by inducing an operative trace of the agent, namely a (typically infinite) sequence of transitions T1 (S0, X1, S1, t1),.., Ti (Si \u2212 1, Xi, Si, ti), Ti + 1 (Si, Xi + 1, Si + 1, ti + 1),.. so that 16. Note that in order to determine that T is a possible transition to T, with a rule of the previous form, one only needs to know that T was applied and led to the state S. This is conveyed by the choice of the name: RT | T \u2032 (S \u2032, X \u2032). In other words, by using a prologue notation, we would have been able to present the rule as \"T \u2032 (S \u2032, X \u2032), T (TXi \u2032,), Ti \u2212 Ti\" and not as a \"marcovian of the Si \u2022 system for the initial cycle (Si \u2022 1), but (Si \u2022 1) (Si)."}, {"heading": "9.3 Normal Cycle Theory", "text": "Normal cycle theory is a concrete example of cycle theory, which defines a pattern of action in which the actor prefers a sequence of transitions that allows him to achieve his goals in a manner consistent with an expected \"normal\" behavior; other examples of possible cycle theories can be found in literature (Kakas, Mancarella, Sadri, Stathis, & Toni, 2005; Sadri & Toni, 2006).Basically, the \"normal\" actor first introduces goals (if he has none to begin with) via GI, then responds to them via RE, and then repeats the planning process for them via PI by executing (part) the chosen plans, through AE, revising its state, via SR, until all goals are addressed (successfully or regrouped).At this point, the actor returns to the introduction of new goals via GI and repeats the above process."}, {"heading": "10. Examples", "text": "In this section, we will take up the examples presented in Section 2.6 and used throughout the paper to illustrate the various components of the KGP model. Overall, the interplay of transitions and how this interaction produces the variety of behaviors offered by the KGP model, including response to observations, creation and execution of conditional plans, and dynamic adaptation of goals and plans. Unless otherwise stated, we assume that Tcycle will be the normal cycle theory presented in Section 9.3."}, {"heading": "10.1 Setting 1 Formalised", "text": "Here we formalize the initial state, knowledge base, and behavior of svs for Setting 1 as described in Section 2.6.1."}, {"heading": "10.1.1 Initial State", "text": "For the sake of simplicity, we can assume that the observations, objectives and the plan of svs are initially empty. Specifically, the (initial) state of svs beKB0 = {} F = {} C = {} \u03a3 = {}"}, {"heading": "10.1.2 Knowledge Bases", "text": "Following Section 5.1.4, we formulate the knowledge base for the reactivity of the agent svs with respect to the statements Q \u2032 s, Q \u2032 s, Q \u2032 s (query, reject, inform, inspired by the FIPA specifications for communicative actions (FIPA, 2001a, 2001b). However, although we use the same names for communicative actions as in the FIPA specification, we do not adopt their \"mentalistic\" semantic interpretation with respect to pre and after conditions. Thus, KBsvsreaction is formulated as follows: observed (C, say (C, svs, query ref (Q), D, T0), T), continues to (have info (\u2032 \u2032, I), T) \u21d2 accept (say (svs, C, inform (Q, I), T \u2032), inform (D), inform (C, say (C, svs), D > Tobserve (C, say), D, Tobserve (say), Tobserve (C, svs), T (say), say (\u2032, I), T (svs), say (svs), T (say (svs), C (say, C, C, C, C, C, C, inform (Q, I, I, I), I (inform (I), T (say), T (T, T, T (say), T (T, T, T, T (say, T, T, T, T), T (say, T (Q, T, T), T (say, T (Q, T, T, T, T), T (say, T (say, T, T, T, T, T, T, T, T), T (say, T (say, T, T, T, T, T, T (say, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, Q \u2032, T, T, T, T, T, T, Q \u2032, Q \u2032, T, Q \u2032, Q \u2032, Q \u2032 s, Q \u2032 s, Q \u2032 s, Q \u2032 s, Q"}, {"heading": "10.1.3 Behaviour", "text": "To illustrate the behavior of the PSA, we assume that this agent receives requests from svs, at time 3, say, the arrival time of tr01. svs receives a request from psa to time 5 for the arrival time of tr01. Via POI to time 5 svs records in its arrival KB0: observed (psa, say (psa, svs, query ref (arrival (tr01), d, 3), 5) where d is the dialogue identification. Then, via RE, at time 7, we say svs changes its state by adding a tree T rooted in an action a1 to respond to psa. This action a1 is a refusal represented as: a1 = tell (svs, psa, refuse (arrival (tr01)), d, and the time restriction on its time > 7 becomes C. The refusal action is generated by the reactivity d because svs has no information about the requested arrival time."}, {"heading": "10.2 Setting 2 Formalised", "text": "Here we formalize the initial state, knowledge base and behavior of psa for Setting 2 as described in Section 2.6.2."}, {"heading": "10.2.1 Initial State", "text": "Suppose that the state of psa is initially as follows: KB0 = {} F = {T1, T2} C = {\u03c41 < 15, \u03c42 < 15} \u03a3 = {} where T1 and T2 each consist of one destination: g1 = have ticket (Madrid, Denver, \u03c41) and g2 = have visa (usa, \u03c42)."}, {"heading": "10.2.2 Knowledge Bases", "text": "To plan for Objective g1, the KB-PSA plan contains: Initiate (Buy online ticket (From, To), T, have a ticket (From, To)), Prerequisite (Buy online ticket (From, To), Available destination (To)). To plan for Objective g2, the KB-PSA plan contains: Initiate (Apply for visa (USA), T, have a visa (USA), Prerequisite (Apply for visa (USA), have an address (USA)), Initiate (Book hotel (L), T, have an address (USA))."}, {"heading": "10.2.3 Behaviour", "text": "Objective g1 acquires three children in T1. These are: g11 = available connection (\u03c411), g12 = available destination (denver, \u03c412), a13 = purchase address online (madrid, denver, \u03c413).In addition, the set of time restrictions is updated to: C = {\u03c41 < 15, \u03c411 = \u03c413 = purchase address online (\u03c413 < \u03c41, \u03c41 > 2).Action a13 is generated as an action that triggers Objective g1. Furthermore, any plan that is generated must meet the integrity restrictions in the KBplan. In particular, any prerequisite for actions in the tree that are not already available must be generated as sub targets in the tree."}, {"heading": "11. Related Work", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "12. Conclusions", "text": "The model allows the specification of heterogeneous actors who can interact with each other, and can have both proactive and reactive behaviors that allow them to function in dynamic environments by taking into account the dynamic context and preferences of the actors. The formal specification of the KGP components within commercial logic has the great advantage that they allow both formal analysis of the model and direct demonstrable implementation."}, {"heading": "Acknowledgments", "text": "This work was supported by the EU FET Global Computing Initiative within the framework of the SOCS project (IST-2001-32530). We thank all colleagues in SOCS for useful discussions during the development of KGP. We also thank Chitta Baral and the anonymous arbitrators for helpful comments on an earlier version of this paper."}, {"heading": "Appendix A. Normal Cycle Theory", "text": "Here we specify the most important parts of the normal Tcycle, but exclude others, for example the definitions for incompatible and the supplementary part (GPI = GS = GS = GS), including definitions for predictions such as empty forests, unreliable predictions, etc. For further details see (Kakas et al., 2005).Tinitial: This consists of the following rules: R0 | GI (S0, {} S now): \"GI (S0, \u2032) empty forests (S0, As):\" AE (S0, As) empty non-executable targets (S0, As) empty non-executable targets (S0, As = fAS (S0, t), As 6 = time now (t), R0 \"PI (S0, G):\" full PI (S0, G), Gs Gs Gs = fGS (S0, t), Gs 6 = fGS (GS), Gs 6 = time now (Gs), now (Gs), \"Gs\" could follow a possible transition (Gs)."}], "references": [], "referenceMentions": [], "year": 2008, "abstractText": "This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan) model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent\u2019s state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.", "creator": " TeX output 2008.11.05:0922"}}}