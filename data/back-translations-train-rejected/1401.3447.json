{"id": "1401.3447", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach", "abstract": "Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns.", "histories": [["v1", "Wed, 15 Jan 2014 05:09:07 GMT  (384kb)", "http://arxiv.org/abs/1401.3447v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["saher esmeir", "shaul markovitch"], "accepted": false, "id": "1401.3447"}, "pdf": {"name": "1401.3447.pdf", "metadata": {"source": "CRF", "title": "Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach", "authors": ["Saher Esmeir", "Shaul Markovitch"], "emails": ["esaher@cs.technion.ac.il", "shaulm@cs.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Cost-Sensitive Classification", "text": "We assume that these two phases involve different types of costs (Turney, 2000). Our primary goal in this thesis is to combine the learning speed to reduce testing and misclassification costs. To make the problem well defined, we must specify: (1) how misclassification costs are presented, (2) how we should combine both types of costs, and (3) how we should combine both types of costs. To answer these questions, we take the model described by Turney (1995). In a problem with other classes, a misclassification cost matrix M is a | C | matrix M, whose Mi, j Entry defines the penalty for assigning the class ci to an instance that actually belongs to the class. Typically, Entrys on which we assign the most important diagnosticsclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseclasseseseseclasseseseclasseclasseclasseclasseseseseclasseseseseseseseclasseseseseseseclasseseseseseclasseseseseseseseclasseseseclasseseseseseseseclasseseseseseseseseseseseseseclasseseseseseseseseseseclasseseseseseseseseseseseseseseseseseclasseseseseseseseseseseseseseseseseclasseseseseseseseseseseseseseseseseseseseclasseseseseseseseseseseseseseseseseseseseseclasseseseseseseclasseseseseseseseseseseseseclasseseseseseseseclassesesesesesesesesesesesesesesesesesesesesese"}, {"heading": "3. The ACT Algorithm", "text": "The framework we propose for the induction of cost-sensitive decision trees builds on the recently introduced LSID3 algorithm. LSID3 adopts the general top-down scheme for decision tree induction (TDIDT): it starts with the entire set of training examples, splits it into subsets by checking the value of an attribute, and then recursively builds sub-trees. Unlike greedy inducers, LSID3 invests more time to make better splitting decisions. For each candidate, LSID3 attempts are made to evaluate the size of the resulting subtree. Following Occam's shaving (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987; Esmeir & Markovitch, it favours the one with the smallest expected size."}, {"heading": "3.1 Obtaining the Sample", "text": "To this end, we have designed a stochastic version of the EG2 algorithm that attempts to build inexpensive trees greedily. In EG2, a tree is built from top to bottom, and the test that maximizes ICF is selected for splitting a node, with ICF (\u03b8) = 2 \u2206 I (cost (sector) + 1) w."}, {"heading": "3.2 Evaluating a Subtree", "text": "LSID3 is a cost-insensitive learning algorithm, so its main goal is to maximize the expected accuracy of the tree learned. Occam's razor states that with two consistent hypotheses, the smaller one is likely to be more accurate. According to Occam's razor, LSID3 uses the size of the tree as a preference advantage and prefers splits that are expected to reduce its final size. However, in a cost-sensitive setup, our goal is to minimize the expected total cost of the classification. Therefore, instead of choosing an attribute that minimizes size, we want to select one that minimizes the total cost. In the face of a decision tree, we need to develop a process that estimates the expected cost of using the tree to classify a future case. These costs consist of two components: the cost of testing and the cost of misclassification."}, {"heading": "3.2.1 Estimating Test Costs", "text": "Assuming that the distribution of future cases would be similar to that of the learning examples, we can estimate the test costs based on the training data. If we create a tree, we calculate the average test cost of the training examples and use it to estimate the test cost of new cases. For a (partial) tree T built from E, a set of m-training examples, we designate the average cost of traversing T for an example of E bytcost (T, E) = 1 m \u00b2 e \u00b2 Etcost (T, e).The estimated test cost for an invisible example e \u00b2 is therefore cost (T, e) = tcost (T, E).Note that the cost is calculated in the relevant context. If an attribute a has already been tested in the top nodes, we will not calculate it again. Likewise, if an attribute from group g has already been tested, we will add up the total cost of the sub-tree."}, {"heading": "3.2.2 Estimating Misclassification Costs", "text": "In fact, it is so that it is a very complex matter, in which it concerns only a very limited number of people who are able to see themselves able to assert themselves. (...) It is also so as if they were able to put themselves in a position. (...) \"It is also not so as if they were able to be able to hide themselves. (...)\" (...) \"(...)\" (\")\" (\") (((\") \") (((\") \") ((\") (\") (() (\") (() (\") () (() () (\" () () () (\"() () () () () (\" () () () () () () (\"() () () () () () () () () (\" () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () () () (() (() () () (() ((() () () (() (() () ((() (() () ((() ((() (() (() (() (() () () (() ((((() () (() (() () (((() (() () (((() () ((((() (() ((() () ((("}, {"heading": "3.3 Choosing a Split", "text": "Once we have decided on the sampler and the tree utility feature, we are ready to formalize the tree growth phase in ACT. A tree is built from top to bottom. The procedure for selecting a splitting test on each node is listed in Figure 5 and in Figure 6. We give a detailed example of how ACT chooses splits and explain how to modify the splitting method for numerical attributes."}, {"heading": "3.3.1 Choosing a Split: Illustrative Examples", "text": "To illustrate this, we consider a two-class problem with mc = $100 (uniform) and 6 attributes, a1,.. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Trees prefer trees that cost $10. Training data includes 400 examples, of which $200 are positive and $200 negative. Let's assume that we have to choose between a1 and a2, and that r = 1. Let's consider the trees in Figure 7, referred to as T1 and T2, as those sampled for a1 and a2 respectively. The expected error costs of T1 and T2 are: $3 million (T1) = 1400 (4 \u00b7 EE (100, 5, 0.25) \u00b7 $100 = 4 \u00b7 7.3400 $= $100 $= m \u00b2 cost (T2)."}, {"heading": "3.3.2 Choosing a Split when Attributes are Numeric", "text": "The selection procedure formalized in Figure 5 needs to be slightly modified if an attribute is numerical: instead of iterating over the values that the attribute can take, we first select r-tests (split points) with the highest information gain, and then call EG2 once for each split point. This ensures that numerical and nominal attributes receive the same resources. Chickering, Meek, and Rounthwaite (2001) introduced several techniques to generate a small number of candidate-divided points dynamically and with little effort. In the future, we intend to use these techniques to select r-points, which are each evaluated with a single call from EG2."}, {"heading": "3.4 Cost-Sensitive Pruning", "text": "In fact, most of us will be able to move to another world where we are able to change the world, where we are able to change the world, where we are able to change the world, and where we are able to change the world, \"he said."}, {"heading": "3.5 Setting the Parameters of ACT", "text": "In addition to r, the sample size, ACT is parameterized by w, which controls the weight of the test costs in EG2, and cf, the confidence factor used for both pruning and error estimation. ICET matches w and cf with the genetic search. In ACT, we consider three different alternatives: (1) maintaining EG2 and C4.5 defaults to w = 1 and cf = 0.25, (2) matching the values using cross-validation, and (3) setting the values a priori, as a function of the problem cost. While the first solution is the simplest, it does not utilize the potential of adapting the sampling mechanism to the specific problem cost. Although matching the values using the network search would achieve good results, it can be expensive in terms of runtime. For example, if we use 5 values for each parameter and 5 times cross-validation, we would have to perform ACT 125 times solely for the purpose of matching."}, {"heading": "4. Empirical Evaluation", "text": "We conducted a variety of experiments to test the performance and behavior of ACT. First, we present a new method for automatically adapting existing data sets to the cost-sensitive setup, then we describe our experimental methodology and its motivation, and finally, we present and discuss our results."}, {"heading": "4.1 Datasets", "text": "Typically, machine learning researchers use records from the UCI repository (Asuncion & Newman, 2007).However, only five UCI records have assigned test costs.5 We include 5. Cost of these records were assigned by human experts (Turney, 1995).These records in our experiments. Nevertheless, to gain a broader perspective, we have developed an automatic method that assigns cost to existing records. The method is associated with: 1. cr, the cost range.2. g, the number of desired groups as a percentage of the number of attributes. In an issue with | A | attributes, there are records that assign cost to existing records. The probability of an attribute belonging to each of these records is 1g \u00b7 | + 1, such as the probability that it does not belong to one of the group.3. d, the number of delayed tests as a percentage of the number of attributes assigned.4., the group as a percentage of the cost."}, {"heading": "4.2 Methodology", "text": "We begin our experimental evaluation by comparing ACT with a fixed resource allocation with several other cost-sensitive and cost-insensitive algorithms. Next, we compare the behavior of ACT with that of ICET. Finally, we evaluate the algorithms with6. The selected UCI datasets differ in size, type of attributes and dimension. 7. The additional 100 datasets are available at http: / / www.cs.technion.ac.il / \u0445 esaher / publications / cost.two modifications on the problem cases: random test cost assignation and inuniform misclassification costs."}, {"heading": "4.2.1 Compared Algorithms", "text": "The algorithm was reimplemented according to the details in (Quinlan, 1993) and the default parameters were used. \u2022 LSID3: A cost-insensitive any time decision tree learner. As such, it uses additional time to induce trees of higher accuracy. However, it is not possible to use additional allocated time to reduce classification costs. \u2022 IDX: A greedy top-down learner that prefers splits that maximize \u2206 Ic (Norton, 1989). The algorithm does not take into account ICS verification costs. IDX was implemented on top of C4.5 by modifying split selection criteria. \u2022 TMD3: A greedy top-down learner that prefers that splits that maximizes, I 2c. (Tan & Worse, 1989) The algorithm does not take misclassification costs into account."}, {"heading": "4.2.2 Normalized Cost", "text": "Turney (1995) points out that the use of the average cost of classification for each data set is problematic because: (1) the cost differences of the algorithms become relatively small when the cost of misclassification increases, (2) it is difficult to fairly combine the results for multiple data sets (e.g. average), and (3) it is difficult to combine the average of the different costs of misclassification. To solve these problems, Turney proposes to normalize the average cost of classification by the standard cost. Let TC be the cost when we perform all the tests. Let us cite the frequency of class i in the data. The error if the answer is always class i (1 \u2212 fi). The default cost is defined as TC + mini (1 \u2212 fi) \u00b7 maxi, j (Mi, j). The default cost is an approximation of the maximum cost in a given problem. It consists of two components: the maximum cost of testing and the base cost of misclassification costs, for example, once the cost of classification is highest, the actual cost is not reached."}, {"heading": "4.2.3 Statistical Significance", "text": "To determine the statistical significance of performance differences between ACT, ICET, and DTMC, we used two tests: \u2022 Paired t-test with \u03b1 = 5% confidence. For each problem from 105 algorithms and for each pair of algorithms, we have 10 pairs of results from the 10-fold cross-validation runs. We used paired t-test to determine whether the difference between the two algorithms on a given problem is significant (which refutes the null hypothesis that the algorithms do not differ in performance), and then we count for each algorithm how many times it has been a significant winner. \u2022 Wilcoxon test (Demsar, 2006), which compares classifiers across multiple datasets and indicates whether one method is significantly better than the other (\u03b1 = 5%)."}, {"heading": "4.3 Fixed-time Comparison", "text": "The other methods have much shorter runtimes due to their greedy nature. Table 1 summarizes the results.8 Each pair of numbers represents the average normalized costs and the associated confidence interval (\u03b1 = 5%).Figure 10 illustrates the average results and charts the normalized costs of the various algorithms and misclassification costs.Test results for ACT, ICET and DTMC are in Table 2. The algorithms are compared with both the t-test and the Wilcoxon test. Full results are available at http: / / www.cs.technion.ac.il / economiesaher / publications / cost."}, {"heading": "100 50.6 \u00b14.2 45.3 \u00b13.7 34.4 \u00b13.6 41.7 \u00b13.8 35.1 \u00b13.6 14.6 \u00b11.8 24.3 \u00b13.1 15.2 \u00b11.9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "500 49.9 \u00b14.2 43.0 \u00b13.9 42.4 \u00b13.6 45.2 \u00b13.9 42.5 \u00b13.6 37.7 \u00b13.1 36.3 \u00b13.1 34.5 \u00b13.2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1000 50.4 \u00b14.6 42.4 \u00b14.5 47.5 \u00b14.2 47.8 \u00b14.4 47.3 \u00b14.3 47.1 \u00b13.8 40.6 \u00b13.9 39.1 \u00b14.2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5000 53.3 \u00b15.9 43.6 \u00b16.1 58.1 \u00b15.9 54.3 \u00b15.9 57.3 \u00b15.9 57.6 \u00b15.2 45.7 \u00b15.6 41.5 \u00b15.7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10000 54.5 \u00b16.4 44.5 \u00b16.6 60.8 \u00b16.4 56.2 \u00b16.4 59.9 \u00b16.4 59.5 \u00b15.6 47.1 \u00b16.0 41.4 \u00b16.0", "text": "The number of t-test wins for each algorithm from the 105 datasets, as well as the winner ET, if any, if the Wilcoxon test was used. If the cost of misclassification is relatively low (mc = 100), ACT clearly surpasses ICET, with 54 significant gains as opposed to ICET's 4 significant gains. In this setup, ACT was able to produce very small trees, sometimes consisting of a node, but the accuracy of the model learned was ignored in this setup. ICET on the contrary, produces, for some of the datasets, larger and more expensive trees. DTMC achieved the best results, outperforming ACT 14 times. The Wilcoxon test also shows that DTMC is better than ACT and that ACT is better than ICET. Further research showed that for some datasets ACT is not necessarily produced larger trees. We believe that better coordination of cf would improve the effect in this scenario by becoming more aggressive. At the other costs, we are significantly inferior to ICET."}, {"heading": "4.4 Comparing the Accuracy of the Learned Models", "text": "If the cost of misclassification is prevalent, an optimal algorithm would produce a very flat tree. If the cost of misclassification is prevalent, an optimal algorithm would produce a high-precision tree. As we see, the cost of normalization increases with the increase in the cost of misclassification. Although it is relatively easy to produce flat trees, some concepts are not easy to learn and even cost-insensitive algorithms do not achieve perfect accuracy with them. Therefore, as accuracy becomes increasingly important, we also show the cost of normalization because the prediction errors affect it more dramatically. To learn more about the impact of misclassification on accuracy, we compare the accuracy of the trees built for different misclassification costs. Figure 12 shows the results. An important feature of DTMC, ICET and ACT is their ability to compromise on accuracy when it is needed. They produce inaccurate trees when accuracy is insignificant and the error is much more accurate when the penalty for trees is high."}, {"heading": "4.5 Comparison of Anytime Behavior", "text": "Both ICET and ACT, like other typical algorithms available at any time, perform better with increased resource allocation. ICET is expected to exploit the extra time by producing more generations, and thus to better align the parameters for the final call of EG2. ACT can use the extra time to acquire larger samples and thus achieve better cost estimations. mc was set to 5000. ICET was set to 2, 4, 8,... generations and ACT to a sample size of 1, 2, 4,... As in the timed comparison, we used 4 instances for each problem. Figure 13 shows the results averaged across the 4 instances. ICET was set to 2, 4, 8,.. generations and ACT to a sample size of 1, 2, 4,.... As in the set comparison, we use 4 instances for each problem."}, {"heading": "4.6 Random Costs", "text": "The costs of 100 of the 105 data sets were assigned using a semi-random mechanism that assigns higher costs to informative attributes. To ensure that the success of ACT is not due to this particular cost allocation scheme, we repeated the experiments with the costs randomly drawn from the given cost range cr, i.e. \u03c1 was set to 0. Figure 14 shows the results. As we can see, ACT retains its advantage over the other methods: it dominates them along the scale of mc values."}, {"heading": "4.7 Nonuniform Misclassification Costs", "text": "As explained in section 3, the ACT algorithm can also handle complex misclassification cost matrices where the penalty for one error type may be higher than the penalty for another type. Our next experiment examines ACT in a non-uniform scenario. Let's have FP denote the penalty for a false positive and FN the penalty for a false negative. However, if there are more than 2 classes, we divide the classes into two equal groups according to their order (or randomly if no order exists). We then assign a penalty FP for misclassifying an instance belonging to the first group and FN for one belonging to the second group. To get a broad view, we vary the ratio between FP and FN and also examine different absolute values. Table 3 and Figure 15 give the average results. Table 4 lists the number of t-tests for significant gains per algorithm."}, {"heading": "5. Related Work", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "6. Conclusions", "text": "The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work, we have introduced a new framework for operating in such complex environments. Our framework has four main advantages: \u2022 It uses a non-greedy approach to build a decision tree and is therefore able to overcome minimal local problems. \u2022 It evaluates entire trees during the search, so it can be adapted to any cost scheme defined by trees. \u2022 It shows good behavior at all times and allows learning speed to be swapped for classification costs. In many applications, we are willing to allocate more time than we would allocate to greedy methods. \u2022 Our proposed framework can take advantage of such additional resources. \u2022 The sampling process can be easily paralleled and the method benefits from distributed computing power."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the EU-funded MUSCLE Network of Excellence (FP6-507752)."}, {"heading": "Appendix A. Datasets", "text": "The following is a more detailed description of the non-UCI datasets used in our experiments: 1. Multiplexer: The multiplexer task has been used by several researchers to evaluate classifiers (e.g. Quinlan, 1993).An instance is a set of bits of length a + 2a, where a is a positive integer; the first bits represent an index into the remaining bits and the name of the instance is the value of the indexed bit. In our experiments, we looked at the 20-bit multiplexer (a = 4).The dataset contains 500 randomly drawn instances. 2. Boolean XOR: Parity-like functions are known to be problematic for many learning algorithms, but they naturally result in real data, such as the Drosophila survival concept (Page & Ray, 2003)."}], "references": [{"title": "An iterative method for multi-class costsensitive learning", "author": ["N. Abe", "B. Zadrozny", "J. Langford"], "venue": "In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004),", "citeRegEx": "Abe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abe et al\\.", "year": 2004}, {"title": "Learning policies for sequential time and cost sensitive classification. In Proceedings of the 1st international workshop on Utility-based data mining (UBDM\u201905) held with KDD\u201905", "author": ["A. Arnt", "S. Zilberstein"], "venue": null, "citeRegEx": "Arnt and Zilberstein,? \\Q2005\\E", "shortCiteRegEx": "Arnt and Zilberstein", "year": 2005}, {"title": "UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Asuncion", "D. Newman"], "venue": null, "citeRegEx": "Asuncion and Newman,? \\Q2007\\E", "shortCiteRegEx": "Asuncion and Newman", "year": 2007}, {"title": "Online choice of active learning algorithms", "author": ["Y. Baram", "R. El-Yaniv", "K. Luz"], "venue": "In Proceedings of the 20 International Conference on Machine Learning", "citeRegEx": "Baram et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Baram et al\\.", "year": 2003}, {"title": "Integrating learning from examples into the search for diagnostic policies", "author": ["V. Bayer-Zubek"], "venue": "Artificial Intelligence,", "citeRegEx": "Bayer.Zubek,? \\Q2005\\E", "shortCiteRegEx": "Bayer.Zubek", "year": 2005}, {"title": "Voila: Efficient feature-value acquisition for classification", "author": ["M. Bilgic", "L. Getoor"], "venue": "In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-2007)", "citeRegEx": "Bilgic and Getoor,? \\Q2007\\E", "shortCiteRegEx": "Bilgic and Getoor", "year": 2007}, {"title": "Deliberation scheduling for problem solving in time constrained environments", "author": ["M. Boddy", "T.L. Dean"], "venue": "Artificial Intelligence,", "citeRegEx": "Boddy and Dean,? \\Q1994\\E", "shortCiteRegEx": "Boddy and Dean", "year": 1994}, {"title": "Pruning decision trees with misclassification costs", "author": ["J. Bradford", "C. Kunz", "R. Kohavi", "C. Brunk", "C. Brodley"], "venue": "In Proceedings of the 9th European Conference on Machine Learning", "citeRegEx": "Bradford et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bradford et al\\.", "year": 1998}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Efficient determination of dynamic split points in a decision tree", "author": ["D.M. Chickering", "C. Meek", "R. Rounthwaite"], "venue": "In Proceedings of the 1st IEEE International Conference", "citeRegEx": "Chickering et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 2001}, {"title": "Cost-sensitive decision tree learning for forensic classification", "author": ["J.V. Davis", "J. Ha", "C.J. Rossbach", "H.E. Ramadan", "E. Witchel"], "venue": "In Proceedings of the 17th European Conference on Machine Learning", "citeRegEx": "Davis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2006}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar,? \\Q2006\\E", "shortCiteRegEx": "Demsar", "year": 2006}, {"title": "Metacost: A general method for making classifiers cost-sensitive", "author": ["P. Domingos"], "venue": "In Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Domingos,? \\Q1999\\E", "shortCiteRegEx": "Domingos", "year": 1999}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence", "citeRegEx": "Elkan,? \\Q2001\\E", "shortCiteRegEx": "Elkan", "year": 2001}, {"title": "When a decision tree learner has plenty of time", "author": ["S. Esmeir", "S. Markovitch"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-2006),", "citeRegEx": "Esmeir and Markovitch,? \\Q2006\\E", "shortCiteRegEx": "Esmeir and Markovitch", "year": 2006}, {"title": "Anytime learning of decision trees", "author": ["S. Esmeir", "S. Markovitch"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Esmeir and Markovitch,? \\Q2007\\E", "shortCiteRegEx": "Esmeir and Markovitch", "year": 2007}, {"title": "Occam\u2019s razor just got sharper", "author": ["S. Esmeir", "S. Markovitch"], "venue": "In Proceedings of the 20th International Joint Conference in Artificial Intelligence (IJCAI-2007),", "citeRegEx": "Esmeir and Markovitch,? \\Q2007\\E", "shortCiteRegEx": "Esmeir and Markovitch", "year": 2007}, {"title": "A multiple model cost-sensitive approach for intrusion detection", "author": ["W. Fan", "W. Lee", "S.J. Stolfo", "M. Miller"], "venue": "In Proceedings of the 11th European Conference on Machine Learning", "citeRegEx": "Fan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2000}, {"title": "Pruning Decision Trees and Lists", "author": ["E. Frank"], "venue": "Ph.D. thesis,", "citeRegEx": "Frank,? \\Q2000\\E", "shortCiteRegEx": "Frank", "year": 2000}, {"title": "The Estimation of Probabilities: An Essay on Modern Bayesian Methods", "author": ["I. Good"], "venue": null, "citeRegEx": "Good,? \\Q1965\\E", "shortCiteRegEx": "Good", "year": 1965}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A.J. Grove", "D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Greiner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Greiner et al\\.", "year": 2002}, {"title": "Monitoring and control of anytime algorithms: a dynamic programming approach", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence,", "citeRegEx": "Hansen and Zilberstein,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 2001}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Constructing optimal binary decision trees is NPcomplete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Letters,", "citeRegEx": "Hyafil and Rivest,? \\Q1976\\E", "shortCiteRegEx": "Hyafil and Rivest", "year": 1976}, {"title": "Improving accuracy and cost of two-class and multiclass probabilistic classifiers using roc curves", "author": ["N. Lachiche", "P. Flach"], "venue": "In Proceedings of the 20th International Conference on Machine Learning (ICML-2003)", "citeRegEx": "Lachiche and Flach,? \\Q2003\\E", "shortCiteRegEx": "Lachiche and Flach", "year": 2003}, {"title": "Automated computer interviews to elicit utilities: Potential applications in the treatment of deep venous thrombosis", "author": ["L. Lenert", "R. Soetikno"], "venue": "American Medical Informatics Association,", "citeRegEx": "Lenert and Soetikno,? \\Q1997\\E", "shortCiteRegEx": "Lenert and Soetikno", "year": 1997}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML-2004)", "citeRegEx": "Ling et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2004}, {"title": "Budgeted learning of naive bayes classifiers", "author": ["D.J. Lizotte", "O. Madani", "R. Greiner"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence (UAI2003),", "citeRegEx": "Lizotte et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lizotte et al\\.", "year": 2003}, {"title": "Active cost-sensitive learning", "author": ["D. Margineantu"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-2005),", "citeRegEx": "Margineantu,? \\Q2005\\E", "shortCiteRegEx": "Margineantu", "year": 2005}, {"title": "Active feature acquisition for classifier induction", "author": ["P. Melville", "M. Saar-Tsechansky", "F. Provost", "R.J. Mooney"], "venue": "In Proceedings of the 4th IEEE International Conference on Data Mining", "citeRegEx": "Melville et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Melville et al\\.", "year": 2004}, {"title": "Generating better decision trees", "author": ["S.W. Norton"], "venue": "Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,", "citeRegEx": "Norton,? \\Q1989\\E", "shortCiteRegEx": "Norton", "year": 1989}, {"title": "The use of background knowledge in decision tree induction", "author": ["M. Nunez"], "venue": "Machine Learning,", "citeRegEx": "Nunez,? \\Q1991\\E", "shortCiteRegEx": "Nunez", "year": 1991}, {"title": "Skewing: An efficient alternative to lookahead for decision tree induction", "author": ["D. Page", "S. Ray"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-2003),", "citeRegEx": "Page and Ray,? \\Q2003\\E", "shortCiteRegEx": "Page and Ray", "year": 2003}, {"title": "Reducing misclassification costs: knowledge intensive approaches to learning from noisy data", "author": ["M. Pazzani", "C. Merz", "P. Murphy", "K. Ali", "T. Hume", "C. Brunk"], "venue": "In Proceedings of the 11th International Conference on Machine Learning (ICML1994)", "citeRegEx": "Pazzani et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pazzani et al\\.", "year": 1994}, {"title": "Inductive policy: The pragmatics of bias selection", "author": ["F. Provost", "B. Buchanan"], "venue": "Machine Learning,", "citeRegEx": "Provost and Buchanan,? \\Q1995\\E", "shortCiteRegEx": "Provost and Buchanan", "year": 1995}, {"title": "Cost-sensitive decision trees with multiple cost scales", "author": ["Z. Qin", "S. Zhang", "C. Zhang"], "venue": "Lecture Notes in Computer Scienc,", "citeRegEx": "Qin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Qin et al\\.", "year": 2004}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Cost-sensitive test strategies", "author": ["S. Sheng", "C.X. Ling", "A. Ni", "S. Zhang"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-2006),", "citeRegEx": "Sheng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sheng et al\\.", "year": 2006}, {"title": "Simple test strategies for cost-sensitive decision trees", "author": ["S. Sheng", "C.X. Ling", "Q. Yang"], "venue": "In Proceedings of the 9th European Conference on Machine Learning", "citeRegEx": "Sheng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sheng et al\\.", "year": 2005}, {"title": "Roulette sampling for cost-sensitive learning", "author": ["V.S. Sheng", "C.X. Ling"], "venue": "In Proceedings of the 18th European Conference on Machine Learning", "citeRegEx": "Sheng and Ling,? \\Q2007\\E", "shortCiteRegEx": "Sheng and Ling", "year": 2007}, {"title": "Cost-sensitive concept learning of sensor use in approach and recognition", "author": ["M. Tan", "J.C. Schlimmer"], "venue": "In Proceedings of the 6th International Workshop on Machine Learning,", "citeRegEx": "Tan and Schlimmer,? \\Q1989\\E", "shortCiteRegEx": "Tan and Schlimmer", "year": 1989}, {"title": "Types of cost in inductive concept learning", "author": ["P. Turney"], "venue": "In Proceedings of the Workshop on Cost-Sensitive Learning held with the 17th International Conference on Machine Learning (ICML-2000),", "citeRegEx": "Turney,? \\Q2000\\E", "shortCiteRegEx": "Turney", "year": 2000}, {"title": "Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney,? \\Q1995\\E", "shortCiteRegEx": "Turney", "year": 1995}, {"title": "Inducing cost-sensitive non-linear decision trees. Technical report 03-052005, School of Computing, Science and Engineering, University of Salford", "author": ["S. Vadera"], "venue": null, "citeRegEx": "Vadera,? \\Q2005\\E", "shortCiteRegEx": "Vadera", "year": 2005}, {"title": "Cost-sensitive learning by cost-proportionate example weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM-2003),", "citeRegEx": "Zadrozny et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2003}, {"title": "An empirical study of the noise impact on cost-sensitive learning", "author": ["X. Zhu", "X. Wu", "T. Khoshgoftaar", "S. Yong"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007),", "citeRegEx": "Zhu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 42, "context": "The ICET algorithm (Turney, 1995) was a pioneer in non-greedy search for a tree that minimizes test and misclassification costs.", "startOffset": 19, "endOffset": 33}, {"referenceID": 31, "context": "Then it builds a tree using the EG2 algorithm (Nunez, 1991) but with the evolved costs instead of the original ones.", "startOffset": 46, "endOffset": 59}, {"referenceID": 41, "context": "These two stages involve different types of costs (Turney, 2000).", "startOffset": 50, "endOffset": 64}, {"referenceID": 41, "context": "These two stages involve different types of costs (Turney, 2000). Our primary goal in this work is to trade learning speed for a reduction in test and misclassification costs. To make the problem well defined, we need to specify: (1) how misclassification costs are represented, (2) how test costs are calculated, and (3) how we should combine both types of cost. To answer these questions, we adopt the model described by Turney (1995). In a problem with |C| different classes, a misclassification cost matrix M is a |C|\u00d7 |C| matrix whose Mi,j entry defines the penalty of assigning the class ci to an instance that actually belongs to the class cj .", "startOffset": 51, "endOffset": 437}, {"referenceID": 41, "context": "These two stages involve different types of costs (Turney, 2000). Our primary goal in this work is to trade learning speed for a reduction in test and misclassification costs. To make the problem well defined, we need to specify: (1) how misclassification costs are represented, (2) how test costs are calculated, and (3) how we should combine both types of cost. To answer these questions, we adopt the model described by Turney (1995). In a problem with |C| different classes, a misclassification cost matrix M is a |C|\u00d7 |C| matrix whose Mi,j entry defines the penalty of assigning the class ci to an instance that actually belongs to the class cj . Typically, entries on the main diagonal of a classification cost matrix (no error) are all zero. When classifying an example e using a tree T , we propagate e down the tree along a single path from the root of T to one of its leaves. Let \u0398(T, e) be the set of tests along this path. We denote by cost(\u03b8) the cost of administering the test \u03b8. The testing cost of e in T is therefore tcost(T, e) = \u2211 \u03b8\u2208\u0398 cost(\u03b8). Note that we use sets notation because tests that appear several times are charged for only once. In addition, the model described by Turney (1995) handles two special test types, namely grouped and delayed tests.", "startOffset": 51, "endOffset": 1211}, {"referenceID": 41, "context": "Alternatively, Turney (1995) suggests taking into account all possible outcomes: when a delayed test is encountered, all the tests in the subtree under it are administered and charged for.", "startOffset": 15, "endOffset": 29}, {"referenceID": 41, "context": "Alternatively, Turney (1995) suggests taking into account all possible outcomes: when a delayed test is encountered, all the tests in the subtree under it are administered and charged for. Once the result of the delayed test is available, the prediction is at hand. One problem with this setup is that it follows all paths in the subtree, regardless of the outcome of non-delayed costs. Moreover, it is not possible to distinguish between the delays different tests impose: for example, one result might be ready after several minutes while another only after a few days. In this work we do not handle delayed tests, but we do explain how ACT can be modified to take them into account. After the test and misclassification costs have been measured, an important question remains: How should we combine them? Following Turney (1995), we assume that both cost types are given in the same scale.", "startOffset": 15, "endOffset": 832}, {"referenceID": 41, "context": "Alternatively, Turney (1995) suggests taking into account all possible outcomes: when a delayed test is encountered, all the tests in the subtree under it are administered and charged for. Once the result of the delayed test is available, the prediction is at hand. One problem with this setup is that it follows all paths in the subtree, regardless of the outcome of non-delayed costs. Moreover, it is not possible to distinguish between the delays different tests impose: for example, one result might be ready after several minutes while another only after a few days. In this work we do not handle delayed tests, but we do explain how ACT can be modified to take them into account. After the test and misclassification costs have been measured, an important question remains: How should we combine them? Following Turney (1995), we assume that both cost types are given in the same scale. A more general model would require a utility function that combines both types. Qin, Zhang, and Zhang (2004) presented a method to handle the two kinds of cost scales by setting a maximal budget for one kind of cost and minimizing the other one.", "startOffset": 15, "endOffset": 1002}, {"referenceID": 36, "context": "The sample is obtained using a stochastic version of ID3 (Quinlan, 1986), which we call SID3.", "startOffset": 57, "endOffset": 72}, {"referenceID": 18, "context": "Frank (2000) reports a comprehensive study about pruning of decision trees, in which he compared pre- to post-pruning empirically in a cost-insensitive setup.", "startOffset": 0, "endOffset": 13}, {"referenceID": 42, "context": "Costs for these datasets have been assigned by human experts (Turney, 1995).", "startOffset": 61, "endOffset": 75}, {"referenceID": 30, "context": "\u2022 IDX : A greedy top-down learner that prefers splits that maximize \u2206I c (Norton, 1989).", "startOffset": 73, "endOffset": 87}, {"referenceID": 31, "context": "\u2022 EG2 : A greedy top-down learner that prefers splits that maximize 2 \u2206I(\u03b8)\u22121 (cost(\u03b8)+1) w (Nunez, 1991).", "startOffset": 92, "endOffset": 105}, {"referenceID": 26, "context": "\u2022 DTMC : DTMC was implemented by following the original pseudo-code (Ling et al., 2004; Sheng et al., 2006).", "startOffset": 68, "endOffset": 107}, {"referenceID": 37, "context": "\u2022 DTMC : DTMC was implemented by following the original pseudo-code (Ling et al., 2004; Sheng et al., 2006).", "startOffset": 68, "endOffset": 107}, {"referenceID": 41, "context": "\u2022 ICET : ICET has been reimplemented following the detailed description given by Turney (1995). To verify the results of the reimplementation, we compared them with those reported in the literature.", "startOffset": 81, "endOffset": 95}, {"referenceID": 41, "context": "2 Normalized Cost As Turney (1995) points out, using the average cost of classification for each dataset is problematic because: (1) the cost differences of the algorithms become relatively small as the misclassification cost increases, (2) it is difficult to combine the results for multiple datasets in a fair manner (e.", "startOffset": 21, "endOffset": 35}, {"referenceID": 11, "context": "\u2022 Wilcoxon test (Demsar, 2006), which compares classifiers over multiple datasets and states whether one method is significantly better than the other (\u03b1 = 5%).", "startOffset": 16, "endOffset": 30}, {"referenceID": 26, "context": "for this phenomenon is that DTMC, as introduced by Ling et al. (2004), does not perform post-pruning, although doing so might improve accuracy in some domains.", "startOffset": 51, "endOffset": 70}, {"referenceID": 26, "context": "for this phenomenon is that DTMC, as introduced by Ling et al. (2004), does not perform post-pruning, although doing so might improve accuracy in some domains. The above two extremes are less interesting: for the first we could use an algorithm that always outputs a tree of size 1 while for the second we could use cost-insensitive learners. The middle range, where mc \u2208 {500, 1000, 5000}, requires that the learner carefully balance the two types of cost. In these cases ACT has the lowest average cost and the largest number of t-test wins. Moreover, the Wilcoxon test indicates that it is superior. ICET is the second best method. As reported by Turney (1995), ICET is clearly better than the greedy methods EG2, IDX, and CSID3.", "startOffset": 51, "endOffset": 664}, {"referenceID": 41, "context": "A similar phenomenon is reported by Turney (1995).", "startOffset": 36, "endOffset": 50}, {"referenceID": 12, "context": "Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost & Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999; Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, & Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007; Sheng & Ling, 2007).", "startOffset": 91, "endOffset": 465}, {"referenceID": 13, "context": "Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost & Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999; Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, & Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007; Sheng & Ling, 2007).", "startOffset": 91, "endOffset": 465}, {"referenceID": 43, "context": "Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost & Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999; Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, & Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007; Sheng & Ling, 2007).", "startOffset": 91, "endOffset": 465}, {"referenceID": 28, "context": "Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost & Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999; Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, & Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007; Sheng & Ling, 2007).", "startOffset": 91, "endOffset": 465}, {"referenceID": 11, "context": "Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost & Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999; Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, & Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007; Sheng & Ling, 2007). These methods, however, do not consider test costs and hence are appropriate mainly for domains where test costs are not a constraint. Davis, Ha, Rossbach, Ramadan, and Witchel (2006) presented a greedy cost-sensitive decision tree algorithm for forensic classification: the problem of classifying irreproducible events.", "startOffset": 257, "endOffset": 651}, {"referenceID": 4, "context": "Bayer-Zubek and Dietterich (2005) formulated the cost-sensitive learning problem as a Markov decision process (MDP), and used a systematic search algorithm based on the AO* heuristic search procedure to solve the MDP.", "startOffset": 0, "endOffset": 34}, {"referenceID": 37, "context": "Several test strategies have been studied, including sequential, single batch and multiple batch (Sheng et al., 2006),", "startOffset": 97, "endOffset": 117}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC).", "startOffset": 0, "endOffset": 28}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time it takes. Therefore the total cost function has an additional component, which reflects the time needed to measure an attribute. Typically, is has a super-linear form: the cost of a quick result is small and fairly constant, but as the waiting time increases, the time cost grows at an increasing rate. The problem is further complicated when a sequence of time-sensitive classification instances is considered, where time spent administering tests for one case can adversely affect the costs of future instances. Arnt and Zilberstein suggest solving these problems by extending the decision theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we assume that the time it takes to administer a test is incorporated into its cost.", "startOffset": 0, "endOffset": 883}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time it takes. Therefore the total cost function has an additional component, which reflects the time needed to measure an attribute. Typically, is has a super-linear form: the cost of a quick result is small and fairly constant, but as the waiting time increases, the time cost grows at an increasing rate. The problem is further complicated when a sequence of time-sensitive classification instances is considered, where time spent administering tests for one case can adversely affect the costs of future instances. Arnt and Zilberstein suggest solving these problems by extending the decision theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we assume that the time it takes to administer a test is incorporated into its cost. In the future, we intend to extend our framework to support time-sensitive classification, both for individual cases and for sequences. Fan, Lee, Stolfo, and Miller (2000) studied the problem of cost-sensitive intrusion detection systems (IDS).", "startOffset": 0, "endOffset": 1154}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time it takes. Therefore the total cost function has an additional component, which reflects the time needed to measure an attribute. Typically, is has a super-linear form: the cost of a quick result is small and fairly constant, but as the waiting time increases, the time cost grows at an increasing rate. The problem is further complicated when a sequence of time-sensitive classification instances is considered, where time spent administering tests for one case can adversely affect the costs of future instances. Arnt and Zilberstein suggest solving these problems by extending the decision theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we assume that the time it takes to administer a test is incorporated into its cost. In the future, we intend to extend our framework to support time-sensitive classification, both for individual cases and for sequences. Fan, Lee, Stolfo, and Miller (2000) studied the problem of cost-sensitive intrusion detection systems (IDS). The goal is to maximize security while minimizing costs. Each prediction (action) has a cost. Features are categorized into three cost levels according to amount of information needed to compute their values. To reduce the cost of an IDS, high cost rules are considered only when the predictions of low cost rules are not sufficiently accurate. Costs are also involved in the learning phase, during example acquisition and during model learning. The problem of budgeted learning has been studied by Lizotte, Madani, and Greiner (2003). There is a cost associated with obtaining each attribute value of a training example, and the task is to determine what attributes to test given a budget.", "startOffset": 0, "endOffset": 1762}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time it takes. Therefore the total cost function has an additional component, which reflects the time needed to measure an attribute. Typically, is has a super-linear form: the cost of a quick result is small and fairly constant, but as the waiting time increases, the time cost grows at an increasing rate. The problem is further complicated when a sequence of time-sensitive classification instances is considered, where time spent administering tests for one case can adversely affect the costs of future instances. Arnt and Zilberstein suggest solving these problems by extending the decision theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we assume that the time it takes to administer a test is incorporated into its cost. In the future, we intend to extend our framework to support time-sensitive classification, both for individual cases and for sequences. Fan, Lee, Stolfo, and Miller (2000) studied the problem of cost-sensitive intrusion detection systems (IDS). The goal is to maximize security while minimizing costs. Each prediction (action) has a cost. Features are categorized into three cost levels according to amount of information needed to compute their values. To reduce the cost of an IDS, high cost rules are considered only when the predictions of low cost rules are not sufficiently accurate. Costs are also involved in the learning phase, during example acquisition and during model learning. The problem of budgeted learning has been studied by Lizotte, Madani, and Greiner (2003). There is a cost associated with obtaining each attribute value of a training example, and the task is to determine what attributes to test given a budget. A related problem is active feature-value acquisition. In this setup one tries to reduce the cost of improving accuracy by identifying highly informative instances. Melville, SaarTsechansky, Provost, and Mooney (2004) introduced an approach in which instances are selected for acquisition based on the accuracy of the current model and its confidence in the prediction.", "startOffset": 0, "endOffset": 2136}, {"referenceID": 1, "context": "Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification (TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of the labeling, but also the amount of time it takes. Therefore the total cost function has an additional component, which reflects the time needed to measure an attribute. Typically, is has a super-linear form: the cost of a quick result is small and fairly constant, but as the waiting time increases, the time cost grows at an increasing rate. The problem is further complicated when a sequence of time-sensitive classification instances is considered, where time spent administering tests for one case can adversely affect the costs of future instances. Arnt and Zilberstein suggest solving these problems by extending the decision theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we assume that the time it takes to administer a test is incorporated into its cost. In the future, we intend to extend our framework to support time-sensitive classification, both for individual cases and for sequences. Fan, Lee, Stolfo, and Miller (2000) studied the problem of cost-sensitive intrusion detection systems (IDS). The goal is to maximize security while minimizing costs. Each prediction (action) has a cost. Features are categorized into three cost levels according to amount of information needed to compute their values. To reduce the cost of an IDS, high cost rules are considered only when the predictions of low cost rules are not sufficiently accurate. Costs are also involved in the learning phase, during example acquisition and during model learning. The problem of budgeted learning has been studied by Lizotte, Madani, and Greiner (2003). There is a cost associated with obtaining each attribute value of a training example, and the task is to determine what attributes to test given a budget. A related problem is active feature-value acquisition. In this setup one tries to reduce the cost of improving accuracy by identifying highly informative instances. Melville, SaarTsechansky, Provost, and Mooney (2004) introduced an approach in which instances are selected for acquisition based on the accuracy of the current model and its confidence in the prediction. Greiner, Grove, and Roth (2002) were pioneers in studying classifiers that actively decide what tests to administer.", "startOffset": 0, "endOffset": 2320}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) tackled the problem of feature subset selection when costs are involved.", "startOffset": 0, "endOffset": 25}], "year": 2008, "abstractText": "Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns.", "creator": "gnuplot 4.2 patchlevel 2 "}}}