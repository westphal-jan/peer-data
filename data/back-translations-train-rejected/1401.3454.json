{"id": "1401.3454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "abstract": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received.", "histories": [["v1", "Wed, 15 Jan 2014 05:13:47 GMT  (515kb)", "http://arxiv.org/abs/1401.3454v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.MA", "authors": ["sherief abdallah", "victor lesser"], "accepted": false, "id": "1401.3454"}, "pdf": {"name": "1401.3454.pdf", "metadata": {"source": "CRF", "title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "authors": ["Sherief Abdallah", "Victor Lesser"], "emails": ["SHERIEF.ABDALLAH@BUID.AC.AE", "LESSER@CS.UMASS.EDU"], "sections": [{"heading": null, "text": "Due to the complexity of the problem, the majority of previously developed MARL algorithms assumed that the agents either had some knowledge of the underlying game (such as Nash balances) and / or observed other agents \"actions and the rewards they received. We are introducing a new MARL algorithm called Weighted Policy Learner (WPL) that allows agents to achieve a Nash balance (NE) in Benchmark 2 action games with minimal knowledge; the only feedback an agent needs is their own local reward (the agent does not observe other agents\" actions or rewards); in addition, WPL does not assume that the agents have a priori knowledge of the underlying game or the corresponding Nash equilibrium; we experimentally show that our algorithm converges in Benchmark 2 Player 2 action games, and in Benchmark 2 action games the dynamics of two player algorithms are challenged by our two-player game algorithms, as well."}, {"heading": "1. Introduction", "text": "In fact, it is the case that one sees oneself in a position to go to a place where one can go to a place where one is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to process, to process, to edit, to process, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to explore, to edit."}, {"heading": "2. Background", "text": "In this section we present the necessary background for our contribution. First, a brief overview of relevant game theory definitions is given, followed by an overview of relevant MARL algorithms, in particular gradient ascent MARL algorithms (GA-MARL), which are closely related to our algorithm."}, {"heading": "2.1 Game Theory", "text": "Game theory provides a framework for agent interaction and has been used by previous researchers to evaluate the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006).A game determines how an agent's payout depends on the actions of other agents. A (normal) game is defined by the tuple < n, A1, A1, A1, A1,..., Rn >, where the payout of an agent is not the number of players in the game."}, {"heading": "2.2 Multiagent Reinforcement Learning, MARL", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2.3 Gradient-Ascent MARL Algorithms", "text": "The first GA-MARL algorithm, whose dynamics were later developed and the resulting GGA function was called, is the Infinitesimal Gradient Ascent (IGA) (Singh et al., 2000); the first GA-MARL algorithm whose dynamics were analyzed is the Infinitesimal Gradient Ascent (IGA) (IGA) (Singh et al., 2000); the first GA-MARL algorithm is a simple gradient ascension algorithm in which each agent has updated i to follow the gradient of the expected payouts (or the value function) Vi, as illustrated by the following equations; the second GA algorithm is a simple gradient ascension algorithm in which each agent i continues its policy to follow the course of the expected payouts (or the value function) Vi, as illustrated by the following equations."}, {"heading": "2.4 Dynamics of GA-MARL Algorithms", "text": "To simplify the analysis, the authors are considered only as two-player two-action games, as we do here. We will refer to the common policy of the two players when the probability of choosing the first action (pt, qt) is not taken into account, when it does not affect clarity (for example, when we consider only one point in time). IGA's updated equations can be simplified to be (note that rij and cij are payouts for the series and the column players): pt + 1 = pt + pt + p = pt."}, {"heading": "3. Weighted Policy Learner (WPL)", "text": "In this paper, we propose the Weighted Policy Learner (WPL) algorithm, which contains the following update equations: \u2206 \u03c0i (a) \u2190 \u2202 Vi (\u03c0) \u2202 \u03c0i (a) \u00b7 \u03b7 \u00b7 {\u03c0i (a) if \u2202 Vi (\u03c0) \u0445 \u03c0i (a) < 0 1 \u2212 \u03c0i (a) otherwise\u03c0i \u2190 projection (\u03c0i + \u03c0i) The projection function is performed by GIGA (Zinkevich, 2003) with slight modification:. a: 1 \u2265 \u03c0 (a) \u2265 (the modification ensures a minimum of exploration). The algorithm works as follows: If the gradient is negative for a certain action, then the gradient is weighted according to principles (a), otherwise (gradient as positive) the gradient is weighted according to (1 \u2212 \u03c0i (a). The probability of choosing a good action increases by a rate that decreases the probability as the probability approaches the probability."}, {"heading": "4. Analyzing WPL\u2019s Dynamics", "text": "The policy of two agents following WPL (1) can be expressed as follows: 1, 2, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "4.1 Numerical Solution", "text": "Figure 4 shows the theoretical behavior predicted by our matching-penny game model. There is a clear similarity to the actual (experimental) behavior for the same game (Figure 5). Note that the time scale on the horizontal axes of both numbers is effectively the same, since what is shown on the horizontal axis in Figure 5 are decision steps in the simulation. Multiplied by the actual policy learning rate (the time step) used in the experiments, 0.001, both axes become identical. Figure 6 records p (t) versus q (t), for a game with NE = (0.9, 0.9) (u1 = 0.5, u2 = \u2212 0.45, u3 = \u2212 0.5, u4 = 0.45, and starting with 157 different initial common strategies (40 initial strategies across each side of the common political space). Figure 7 shows the convergence."}, {"heading": "4.2 Comparing Dynamics of IGA, IGA-WoLF, and WPL", "text": "Using differentiated equations that model each of the three algorithms, we now compare their dynamics and show the main characteristics of the WPL. Matlab was again used to solve the differential equations (of the three algorithms) numerically. Figure 9 shows the dynamics for a game with u1u3 < 0 and the NE = (0.5). The common strategy moves clockwise. The dynamics of the WPL is very close to the IGA-WoLF, which is a bit faster (after a complete round around the NE), IGA-WoLF is closer to the NE than to the WPL."}, {"heading": "5. Experimental Results", "text": "This section consists of three parts. Section 5.1 covers the most important learning parameters that need to be set in practice. Section 5.2 presents the experimental results for the 2x2 benchmark games. Section 5.3 presents the experimental results for areas with a larger number of actions and / or agents."}, {"heading": "5.1 Learning Parameters", "text": "Performing experiments for WPL involves setting two main parameters: the policy learning rate, \u03b7, and the value learning rate, \u03b1. In theory (as discussed in Section 4), the policy learning rate, \u03b7, should approach the NE rate. In practice, however, this is not possible and we have tried to set \u03b7 to various small values between 0.01 and 0.00001. The smaller \u03b7 is, the longer it will take for the WPL rate to approach, and the smaller the oscillations around the NE become (and vice versa), similar to previous GA-MARL algorithms. A reasonable value we have used in most of our experiments is \u03b7 = 0.002. The value learning rate \u03b1 is used to calculate the expected reward of an action at a time t or rt (a) that is a priori not known. The usual approach we have applied in previous MARGA-1 algorithms is also applied here and the MARGA-1 algorithms are rt."}, {"heading": "5.2 Benchmark 2x2 Games", "text": "The reason for this is that in an open system where the dynamics may change continuously, learning also plays a role. We have set the exploration rate to 0.1 (which comes into play in the modified projection function in Section 3), the policy-learning rate to 0.02, and the value-learning rate to 0.1 (unless it is specified). The first set of experiments shows the application of our algorithm to the three benchmark games described in Section 1. Figure 12 shows the convergence rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate-rate"}, {"heading": "5.3 Games Larger than 2x2", "text": "Figures 16 and 17 show that if the initial common strategy (0.1, 0.8, 0.1) is r, [0.8, 0.1, 0.1] c (we have also tried (13, 1 3, 1 3] r, [1 3, 1 3] c, which has yielded similar results), \u03b7 = 0.001, and for two values of \u03b1: 0.1 and 1. For the rock paper scissors game (Figure 16), both GIGA-WoLF and WPL convergence of measures, if \u03b1 = 0.1, whereas only WPL players can take action if WPL = 1. In Shapley's game (Figure 17) GIGAWoLF holds oscillation for both = 1 and \u03b1 = 0.1 (GIGA-WoLF = 0.1) the performance gets worse if it increases."}, {"heading": "5.4 Distributed Task Allocation Problem (DTAP)", "text": "We are using a simplified version of the Distributed Task Allocation Domain (DTAP) (Abdallah & Lesser, 2007), where the goal of the multi-agent system is to assign tasks to the agents so that the service time of each task is minimized. To illustrate, we are looking at the example scenario shown in Figure 20. Agent A0 is given task T1, which can be performed by any of the agents, until Task T1 accomplishes the task. Although Agent A0 does not know that Agent A0 interacts only with its immediate neighbors, Agent A0 will ultimately learn (through experience and interaction with its neighbors) that T1 is the best action without knowing that Agent A0 interacts only with its immediate neighbors."}, {"heading": "6. Conclusion and Future Work", "text": "This paper presents WPL, a gradient ascent multiagent reinforcement learning algorithm (GAMARL) that assumes that an agent does not know the underlying game or observe other agents. We demonstrate experimentally that the WPL converges into benchmark 2-player-2-action games. We also show 9. The simulator is available online at http: / / www.cs.umass.edu / \u02dc shario / dtap.html.that WPL converges into Shapley's game where none of the previous GA-MARL algorithms have successfully converged. We test the practicability of our algorithm in the area of distributed task distribution with a network of 100 agents. WPL surpasses the state-of-the-art GA-MARL algorithms in both the speed of convergence and the expected reward. We analyze the dynamics of our algorithm in distributed task distribution and show that it has continuous nonlinear dynamics."}, {"heading": "7. Acknowledgments", "text": "This paper is based on our previous conference publications (Abdallah & Lesser, 2006, 2008)."}], "references": [{"title": "Learning the task allocation game", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Abdallah and Lesser,? \\Q2006\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2006}, {"title": "Multiagent reinforcement learning and self-organization in a network of agents", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "Abdallah and Lesser,? \\Q2007\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2007}, {"title": "Non-linear dynamics in multiagent reinforcement learning algorithms", "author": ["S. Abdallah", "V. Lesser"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Abdallah and Lesser,? \\Q2008\\E", "shortCiteRegEx": "Abdallah and Lesser", "year": 2008}, {"title": "Adaptive policy gradient in multiagent learning", "author": ["B. Banerjee", "J. Peng"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and Multi Agent Systems,", "citeRegEx": "Banerjee and Peng,? \\Q2003\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2003}, {"title": "Generalized multiagent learning with performance bound", "author": ["B. Banerjee", "J. Peng"], "venue": "Autonomous Agents and Multiagent Systems,", "citeRegEx": "Banerjee and Peng,? \\Q2007\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2007}, {"title": "Convergence and no-regret in multiagent learning", "author": ["M. Bowling"], "venue": "Tech. rep., University of Alberta", "citeRegEx": "Bowling,? \\Q2004\\E", "shortCiteRegEx": "Bowling", "year": 2004}, {"title": "Convergence and no-regret in multiagent learning", "author": ["M. Bowling"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Bowling,? \\Q2005\\E", "shortCiteRegEx": "Bowling", "year": 2005}, {"title": "Multiagent learning using a variable learning rate", "author": ["M. Bowling", "M. Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "Bowling and Veloso,? \\Q2002\\E", "shortCiteRegEx": "Bowling and Veloso", "year": 2002}, {"title": "Packet routing in dynamically changing networks: A reinforcement learning approach", "author": ["J.A. Boyan", "M.L. Littman"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Boyan and Littman,? \\Q1994\\E", "shortCiteRegEx": "Boyan and Littman", "year": 1994}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "In Proceedings of the National Conference on Artificial intelligence/Innovative Applications of Artificial Intelligence,", "citeRegEx": "Claus and Boutilier,? \\Q1998\\E", "shortCiteRegEx": "Claus and Boutilier", "year": 1998}, {"title": "AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents", "author": ["V. Conitzer", "T. Sandholm"], "venue": "Machine Learning,", "citeRegEx": "Conitzer and Sandholm,? \\Q2007\\E", "shortCiteRegEx": "Conitzer and Sandholm", "year": 2007}, {"title": "Cooperative information sharing to improve distributed learning in multi-agent systems", "author": ["P.S. Dutta", "N.R. Jennings", "L. Moreau"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dutta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dutta et al\\.", "year": 2005}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hu and Wellman,? \\Q2003\\E", "shortCiteRegEx": "Hu and Wellman", "year": 2003}, {"title": "Nonlinear Systems. Prentice-Hall, Upper Saddle River, NJ, USA", "author": ["H.K. Khalil"], "venue": null, "citeRegEx": "Khalil,? \\Q2002\\E", "shortCiteRegEx": "Khalil", "year": 2002}, {"title": "Value-function reinforcement learning in Markov games", "author": ["M. Littman"], "venue": "Cognitive Systems Research,", "citeRegEx": "Littman,? \\Q2001\\E", "shortCiteRegEx": "Littman", "year": 2001}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Nash convergence of gradient dynamics in generalsum games", "author": ["S. Singh", "M. Kearns", "Y. Mansour"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1999\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1999}, {"title": "An evolutionary dynamical analysis of multi-agent learning in iterated games", "author": ["K. Tuyls", "P.J. \u2019t Hoen", "B. Vanschoenwinkel"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Tuyls et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tuyls et al\\.", "year": 2006}, {"title": "Reinforcement learning to play an optimal Nash equilibrium in team Markov games", "author": ["X. Wang", "T. Sandholm"], "venue": "In Proceedings of the Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Sandholm,? \\Q2003\\E", "shortCiteRegEx": "Wang and Sandholm", "year": 2003}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "An alternative goal that we pursue here is converging to the Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007), which is by definition a local maximum across agents (no agent can do better by deviating unilaterally from the NE).", "startOffset": 83, "endOffset": 148}, {"referenceID": 16, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 14, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 6, "context": "Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling, 2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah & Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 102, "endOffset": 338}, {"referenceID": 6, "context": "Furthermore, we show that WPL converges in the challenging Shapley\u2019s game, where state-of-theart MARL algorithms failed to converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL.", "startOffset": 132, "endOffset": 171}, {"referenceID": 16, "context": "1 Game Theory Game theory provides a framework for modeling agents\u2019 interaction and was used by previous researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006).", "startOffset": 183, "endOffset": 342}, {"referenceID": 6, "context": "1 Game Theory Game theory provides a framework for modeling agents\u2019 interaction and was used by previous researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier, 1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005; Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006).", "startOffset": 183, "endOffset": 342}, {"referenceID": 6, "context": "Table 1 and Table 2 provide example benchmark games that were used in evaluating previous MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).", "startOffset": 106, "endOffset": 195}, {"referenceID": 16, "context": "NE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002; Zinkevich, 2003).", "startOffset": 51, "endOffset": 112}, {"referenceID": 20, "context": "NE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002; Zinkevich, 2003).", "startOffset": 51, "endOffset": 112}, {"referenceID": 6, "context": "Shapley\u2019s game, in particular, has received considerable attention from MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) as it remains challenging for state-of-the-art MARL algorithms despite its apparent simplicity (and similarity to the rock-paper-scissors game which is not as challenging).", "startOffset": 87, "endOffset": 152}, {"referenceID": 16, "context": "The Infinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) and its generalization (Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE.", "startOffset": 50, "endOffset": 70}, {"referenceID": 20, "context": ", 2000) and its generalization (Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE.", "startOffset": 57, "endOffset": 74}, {"referenceID": 6, "context": "Several modifications to IGA and GIGA were proposed to avoid divergence in games with mixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee & Peng, 2003), and GIGA-WoLF (Bowling, 2005).", "startOffset": 196, "endOffset": 211}, {"referenceID": 16, "context": "3 Gradient-Ascent MARL Algorithms The first GA-MARL algorithm whose dynamics were analyzed is the Infinitesimal Gradient Ascent (IGA) (Singh et al., 2000).", "startOffset": 134, "endOffset": 154}, {"referenceID": 16, "context": "The original IGA paper (Singh et al., 2000) defined the projection function (and therefore IGA) in case each agent has only two actions to choose from.", "startOffset": 23, "endOffset": 43}, {"referenceID": 20, "context": "A general definition of the projection function was later developed and the resulting algorithm was called Generalized IGA or GIGA (Zinkevich, 2003).", "startOffset": 131, "endOffset": 148}, {"referenceID": 20, "context": "The generalized projection function projects an invalid policy to the closest valid policy within the simplex (Zinkevich, 2003).", "startOffset": 110, "endOffset": 127}, {"referenceID": 16, "context": "The IGA algorithm did not converge in all two-player-two-action games (Singh et al., 2000).", "startOffset": 70, "endOffset": 90}, {"referenceID": 6, "context": "This approximation allows GIGA-WoLF to converge in the tricky game, but not in Shapley\u2019s game (Bowling, 2005).", "startOffset": 94, "endOffset": 109}, {"referenceID": 16, "context": "4 Dynamics of GA-MARL Algorithms Differential equations were used to model the dynamics of IGA (Singh et al., 2000).", "startOffset": 95, "endOffset": 115}, {"referenceID": 16, "context": "In IGA\u2019s analysis (Singh et al., 2000), the effect of the projection function was taken into account by considering all possible locations of the simplex.", "startOffset": 18, "endOffset": 38}, {"referenceID": 20, "context": "\u2206\u03c0i(a)\u2190 \u2202Vi(\u03c0) \u2202\u03c0i(a) \u00b7 \u03b7 \u00b7 { \u03c0i(a) if \u2202Vi(\u03c0) \u2202\u03c0i(a) < 0 1\u2212 \u03c0i(a) otherwise \u03c0i \u2190 projection(\u03c0i + \u2206\u03c0i) The projection function is adopted from GIGA (Zinkevich, 2003) with minor modification:\u2200a : 1 \u2265 \u03c0(a) \u2265 (the modification ensures a minimum amount of exploration ).", "startOffset": 147, "endOffset": 164}, {"referenceID": 6, "context": "for all gradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee & Peng, 2007).", "startOffset": 40, "endOffset": 102}, {"referenceID": 16, "context": "It is important to note, however, that all gradient ascent MARL algorithms (including WPL) converge in 2x2 games cases where there is (at least) one pure NE, because the dynamics do not have any loops and eventually lead to one of the pure equilibriums (Singh et al., 2000; Bowling & Veloso, 2002).", "startOffset": 253, "endOffset": 297}, {"referenceID": 6, "context": "The performance of GIGA, PHC-WoLF, and GIGA-WoLF conforms to the results reported previously by their authors (Bowling & Veloso, 2002; Bowling, 2005).", "startOffset": 110, "endOffset": 149}, {"referenceID": 13, "context": "We are currently investigating alternative methodologies for analyzing dynamics, including the evolutionary dynamics (Tuyls, \u2019t Hoen, & Vanschoenwinkel, 2006) and Lyapunov stability theory (Khalil, 2002).", "startOffset": 189, "endOffset": 203}], "year": 2008, "abstractText": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents\u2019 decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley\u2019s game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL\u2019s convergence is difficult, because of the non-linear nature of WPL\u2019s dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL\u2019s dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "creator": "TeX"}}}