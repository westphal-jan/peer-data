{"id": "1401.3455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs", "abstract": "Partially observable Markov decision processes (POMDPs) provide a principled framework for sequential planning in uncertain single agent settings. An extension of POMDPs to multiagent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent's belief about the physical world, about beliefs of other agents, and about their beliefs about others' beliefs. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a general method for obtaining approximate solutions of I-POMDPs based on particle filtering (PF). We introduce the interactive PF, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to mitigate the belief space complexity, but it does not address the policy space complexity. To mitigate the policy space complexity -- sometimes also called the curse of history -- we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. While this approach does not completely address the curse of history, it beats back the curse's impact substantially. We provide experimental results and chart future work.", "histories": [["v1", "Wed, 15 Jan 2014 05:14:08 GMT  (574kb)", "http://arxiv.org/abs/1401.3455v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prashant doshi", "piotr j gmytrasiewicz"], "accepted": false, "id": "1401.3455"}, "pdf": {"name": "1401.3455.pdf", "metadata": {"source": "CRF", "title": "Monte Carlo Sampling Methods for Approximating Interactive POMDPs", "authors": ["Prashant Doshi", "Piotr J. Gmytrasiewicz"], "emails": ["PDOSHI@CS.UGA.EDU", "PIOTR@CS.UIC.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to outdo themselves by outdoing themselves, by outdoing each other, by outdoing themselves, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, that they are overdemanding, that they are overdemanding, that they are overdemanding, that they are outdoing each other, that they are overdemanding, that they are outdoing each other, that they are outdoing each other, that they are overdemanding, that they are outdoing each other, that they are overdoing each other, that they are overdoing each other, that they are overchallenging, that they are overchallenging, that they are overdoing each other, that they are overdemanding, that they are overdoing each other, that they are overdoing each other, that they are overchallenging, that they are overdoing each other, that they are overchallenging, that they are overdoing each other, that they are overchallenging themselves, that they are overchallenging, that they are overdoing each other, that they are overdoing each other, that they are overchallenging, that they are overdoing each other, that they are overchallenging themselves, that they are overchallenging, that they are overdoing each other, that they are overchallenging, that they are overdoing each other, that they are overchallenging, that they are overdoing each other, that they are overchallenging themselves, that they are overdemanding, that they are overdoing each other, that they are overchallenging themselves, that they are overdemanding, that they are overdemanding, that they are overdoing each other."}, {"heading": "2. Related Work", "text": "There are several approaches to tackling the Bayesian problem that are attracting a lot of attention."}, {"heading": "3. Background: Particle Filter for the Single Agent Setting", "text": "In order to act rationally in uncertain settings, agents must follow the evolution of the state over time, based on the actions they perform and the observations available. In individual settings, the estimation of the state is usually performed using a technique called the Bayes filter (Russell & Norvig, 2003).A Bayes filter allows the agent to maintain a belief about the state of the world at any given time and to update this belief every time an action is performed and new sensory information arrives. The convenience of this approach lies in the fact that the update is independent of past perceptions and action sequences. This is because the agent's belief is a sufficient statistic: it summarizes all the information contained in past actions and observations."}, {"heading": "4. Overview of Finitely Nested I-POMDPs", "text": "I-POMDPs (Gmytrasiewicz & Doshi, 2005) we generalize POMDPs = 1 > environment = 1 < j = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1 > environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment = 1, environment =, environment = 1, environment = 1, environment =, 1, environment =, 1, environment =, 1, environment =, 1, 1, environment =, 1, 1, environment =, 1, 1, environment =, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1"}, {"heading": "4.1 Belief Update", "text": "Similar to the POMDPs, an agent within the I-POMDP framework also updates his belief while acting and observing. However, there are two differences that complicate updating belief in the multi-agent settings when compared with the individual agents. \u2212 First, since the state of the physical environment depends on the actions of both agents, predicting how the physical state changes must be made on the basis of the predicted actions of the other agent. \u2212 In other words, the probabilities of the actions of other agents are obtained on the basis of their models. \u2212 Second, changes in the other agent's models must be included in the update. Specifically, since the other agent's model is intentional, the updating of the beliefs of the other agent must be included on the basis of his new observation. \u2212 In other words, the agent must update his beliefs on the basis that he expects the other agent to observe and how he / she is probing. The belief update function for the other agent is active on the basis of an active POMDP-1t is attached to the POMDP-1."}, {"heading": "4.2 Value Iteration", "text": "Each state of faith in I-POMDPi, l has an associated value that reflects the maximum payout the broker can expect in that state of faith: U t (< bi, l, \u03b8, i >) = max ai, i, i (is, ai) bi, l (is) d is + \u03b3 (is) oi, i) iPr (oi | ai, bi, l) U t \u2212 1 (< SE \u2082 i (bi, l, ai, oi), i (4). The 0th level model is a POMDP: the actions of other brokers are treated as exogenous events and divided into T, O and R.where, ERi (is, ai) = \u2211 aj Ri (is, ai, aj) Pr (aj | governj, l \u2212 1) (since is = (s, successj, l \u2212 1)."}, {"heading": "5. Example: The Multiagent Tiger Problem", "text": "To illustrate our approach methods, we use as an example the multi-agent tiger problem. The multi-agent tiger problem is a generalization of the single agent tiger problem, which is outlined in section 3 on the multi-agent problem. To make the interaction more interesting, we restrict ourselves in addition to the usual observation of growling to a two-agent problem, but the problem is easily expandable to more agents. In the two-agent tiger problem, any agent can open or listen to. To make the interaction more interesting, in addition to the usual observation of growling, we have added an observation of door ratchets, which depends on the action of the other agent. Knarren to the right (CR) is probably due to the fact that the other agent has opened or listened to the right door. In order to make the interaction more interesting, in addition to the usual observation of snarling, we have added an observation of door ratchets, which depends on the action of the other agent. Knarren to the right to the right (CR) is probably to lead back to the right (CR), that the other agent has opened or opened the right door, and in order to make the interaction more interesting. To make the interaction more interesting, we have added an observation of door ratchet. Knarren to the right to the right to the right to the action of the other agent. Knarren to the right to the right to the right to the right, to the right to the right. Knarren to the right to the right (CR) is probably lead back to the other agent to the right to the right (CR) is to the right to the right, to the right to the right, to the right, to the right to the right to the right to the right, to the right to the right to the right, to the right to the right, to the right to the right to the right, to the right to the right, to the right to the right, to the right to the right, to the right to the right, to the right to the right, to the right, to the right to the right, to the right to the right, to the right to the right to the right, to the right, to the right to the right, to the right, to the right, to the CR"}, {"heading": "6. Representing Prior Nested Beliefs", "text": "As we mentioned earlier, there are an infinite number of intentional models of an agent. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p s\" p \"p\" p \"p\" p \"p s\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "7. Decomposing the I-POMDP Belief Update", "text": "Analogous to Eqs. 1 \u2212 j \u2212 \u2212 j filter, we break down the I-POMDP faith actualization into two steps. The decomposition not only facilitates a better understanding of the faith actualization, but also plays a central role in the development of the approximation. \u2022 Prediction: If an agent, say i, performs an action at \u2212 1i and the agent j performs a t \u2212 1j, the recognized act of faith is: Pr (is at \u2212 1i, a t \u2212 1 j, b \u2212 1 i, l) = \"IS \u2212 1,\" if an action is at \u2212 1j \u2212 1i, l (is t \u2212 1) Pr (at \u2212 1 j \u2212 1) \u00d7 Ti (s t \u2212 1, b \u2212 1 \u2212 1), a \"t \u2212 1,\" a \"a,\" a \"i,\" a, \"a,\" a, \"a.\""}, {"heading": "8. Interactive Particle Filter for the Multiagent Setting", "text": "We have the algorithm for the traditional bootstrap filter in Section 3. As already mentioned, the bootstrap filter is an MC sampling-based randomized implementation of the POMDP Faith Update (Bayes Filter). We are generalizing this implementation to approximate the I-POMDP Faith Update steps previously presented in Section 7."}, {"heading": "8.1 Description", "text": "The generalization of the PF to the multi-agent case, which we call the interactive particle filter (I-PF), is only a matter of time (I-PF) until there is a fundamental PF. (I-PF) The generalization of the PF to the multi-agent case, which we call the interactive particle filter (I-PF). (I-PF) The generalization of the physical belief generated by the filter is a question of truthfulness (as calculated by Eqs), how the number of particles (N) tends toward infinity. Note that the presence of other agents does not affect the presence of other agents, because (a) the exact faith update that provides the stationary point about the presence of other agents, and (b) we explicitly model other agent actions based on which the non-stationary in the environment vanishes. The extension of the multiagent to the PF is confronted with the other setting, because we are facing the agent."}, {"heading": "8.2 Illustration of the I-PF", "text": "This year it is more than ever before."}, {"heading": "8.3 Performance of the I-PF", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they do not want. (...) It is not so that people are able to decide whether they want or not. (...) It is not so that they want. (...) It is not so that they want. (...) It is not so that they want. (...) It is not so that they want. (...) \"It is so.\" (...) \"It is so.\" (...) \"It is.\" (...) \"It is.\" (...) \"It is.\" (...) \"It is.\" (...) \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"(It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (... \"It is.\" (...). \"It is.\" (... \"it.\" It is. \"(...\" It. \"It is.\" (...). \"It is.\" It. \"(...\" It is. \"It is.\" (... \"It is.\" (...). \"It.\" It is. \"(...\" It is. \"It.\" (... \"It is.\" It. \"(...\" It is. \").\" It is. \"(It is.\" It. \"(...\" It. \"It.\" It is. (... \").\" It is. \"(it. (It.\"). (It is. (it. \"It is. (it.\"). (it. (It. \"). (It is. (It is.\"). (It is. (it. \"it.\"). (it. (it. \"). (It is. (It is. (it.\" it. \"). (It is. (It is.\"). \"it. (It is. (It is. (it.\"). (it. (it. \"). (It is. (it. (it.). (It is. (it.\" it. (it. \"). (It is.\" it. (it. \"). (It is."}, {"heading": "9. Value Iteration on Sample Sets", "text": "Since the I-PF represents the belief in the agent i, bi, l, using a set of N particles, b-i, l, a value function is required as a backup operator. If the required backup operator and U-PF specify the approximate value function, then the backup operation, U-T = H-U-T-1, is as follows: U-T (oi | ai, b-i, l-i >) = max ai-PF (i, l, ai, oi), lERi (is (n), ai) + \u03b3 oi-iPr (oi, b-i, l) U-T \u2212 1 (< I-PF (bi, ai, oi),."}, {"heading": "9.1 Convergence and Error Bounds", "text": "The use of randomization techniques such as PFs means that the titeration of values does not necessarily converge, because contrary to the exact belief survey generated by the PF with a finite number of particles, it is not guaranteed that it is identical for identical input factors. Non-determinism of approximate faith extension implies that the isotonicity and contraction of H as N value is not guaranteed. 6Our inability to guarantee the convergence of value titeration implies that we must pursue a policy of approximate end horizon with the approximate optimal end horizon horizon policy. Let us consider the value of the optimal end horizon horizon policy to be U value that does not represent the value of the approximate safe and U value of the approximate horizon horizon policy."}, {"heading": "9.2 Computational Savings", "text": "Since the complexity of the solution of I-POMDPs is dominated by the complexity of the solution of the models of other active substances, we analyze the reduction in the number of active substance models to be solved. In a K + 1 drug system with the number of particles limited by N at each level, each particle in b-t-1k, l at the level l contains K models of all level l-1. The solution of each of these models of level l-1 requires a recursive solution of the models of the lower level. The upper limit in terms of the number of models to be solved is O ((KN) l-1). Since there are K models in one particle and N such possibly different particles, we must solve O (KN) l models. Our upper limit in terms of the number of models to be solved is the polynomial level in K for a fixed nesting plane. This can be contrasted with O models (K | K) l models, which must be solved in the exact case."}, {"heading": "10. Empirical Performance", "text": "The aim of our experimental analysis is to demonstrate empirically (a) that errors are reduced with increasing sample complexity and (b) that computational time is saved when applying the approximation technology. As test problems, we are again using the previously introduced multi-agent tiger problem and a multi-agent version of the machine maintenance problem (see Appendix A). Although the one-agent versions of these problems are simple, their multi-agent versions are sufficiently complex to motivate the use of approximation techniques to solve them. Furthermore, we show that our approach scales to larger problems by also applying it to the drone reconnaissance problem."}, {"heading": "10.1 Multiagent Tiger and Machine Maintenance Problems", "text": "In order to demonstrate the reduction of the error, we construct performance profiles that show an increase in performance as more computing resources - in this case particles - are assigned to the approximation algorithm. In Figures 15 (a) and (c) we show the performance profile curves if Agent i's previous belief is level 1, previously described in Fig. 3 (a), and accordingly modified for the MMMultiagent Tiger Problem-30-25-20-10-501 10 100 1000E xpec ted Rew ardNo. of Particles (N) H = 2: Exact H = 2: ApproxH = 3: Exact H = 3: Exact H = 3: Exact H = 3: Approx-25-10-501 10 xpec."}, {"heading": "10.2 UAV Reconnaissance Problem", "text": "As we have already mentioned, this is a larger problem, which consists of 36 physical states, 5 actions and 3 observations. In Figure 16 (a), we show the level 1 performance profile for this problem for horizons 2 and 3. Due to the size of the problem, we have not been able to calculate the exact value of the optimal strategies. As before, each data point is the average reward obtained by simulating a horizon 2 or 3 policy in a two-agent setting. Therefore, the corresponding expected reward is close to the optimal value of the strategies. We also show the time needed to generate a good quality policy of horizon 2 and 3 on average. We observe that the profiles tend to become flatter as the number of particles increases. Therefore, the corresponding expected reward is close to the optimal value of the strategies. We also show the time needed to generate a good quality policy of horizon 2 and 3 on average. You indicate that strategies are getting larger than the ones we need (though it is possible for large problems to be solved later)."}, {"heading": "11. Sampling the Look Ahead Reachability Tree", "text": "The main reason for this is the exponential growth of the anticipatory accessibility tree as the horizons widen; we have called this the curse of history. At some point, there may be 18 possible states of belief (in the third step), including 324 in the third step and more than 0.1 million in the fifth step. To mitigate the curse of history, we reduce the branching factor of the anticipatory accessibility tree by sampling 18 possible states of belief from the possible observations the agent might receive, 324 in the third step, and more than 0.1 million in the fifth step. To mitigate the curse of history, we reduce the branching factor of the anticipatory accessibility tree by taking from the possible observations the agent may receive. While this approach does not fully address the curse of history, it significantly reduces the effects of that curse."}, {"heading": "11.1 Computational Savings", "text": "We look at the arithmetical savings resulting from scanning the accessibility problem in the predictive scope of accessibility. Thus, if we make random observations on each propagated belief within the accessibility tree, we will at some point obtain savings in terms of the complete accessibility tree. As our experiments show, the number of different recorded observations is less than the number of recorded observations, resulting in significant arithmetic savings. As an illustration of the arithmetic savings, we compare the durations of the calculation of the policy tree for the multi-agent tiger (Table 5) and the UAV exploration (Table 6)."}, {"heading": "11.2 Empirical Performance", "text": "We present the performance profiles in Fig. 17 for the multi-agent tiger problem, when a partial view of accessibility trees is built up by scanning the observations. Similar to our previous experiments, the performance profiles reflect the average of the rewards accumulated by the root of the approximate guideline tree, which are accumulated by i over 10 independent studies, as the number of observation samples is gradually increased. Within each run, the position of the tiger and its previous beliefs were scanned according to the previous level 1. As we have combined RTS with the I-PF, we also vary the number of particles used to approximate the beliefs. The expected reward initially increases sharply before flattening out."}, {"heading": "12. Discussion", "text": "In the past, we have always looked for solutions that would help people solve their problems, \"he told the Deutsche Presse-Agentur in an interview with\" Welt am Sonntag. \"\" We have the feeling that we are able to get the problems under control, \"he told the Deutsche Presse-Agentur in Berlin.\" But we have no idea what will happen next. \""}, {"heading": "Acknowledgments", "text": "This research is supported in part by AFOSR grant no. FA9550-08-1-0429 and in part by NSF grant no. IRI-9702132 and IRI-0119270. Versions of this article have already been published in (Doshi & Gmytrasiewicz, 2005a) and in (Doshi & Gmytrasiewicz, 2005b)."}, {"heading": "Appendix A. Multiagent Machine Maintenance Problem", "text": "We extend the traditional version of the machine maintenance problem (Smallwood & Sondik, 1973) to a cooperative version with two agents. The original MM problem involved a machine containing two internal components operated by a single agent. Either one or both components of the machine can spontaneously fail after each production cycle. If an internal component has failed, there is a certain chance that the product is defective in the operation of the product. The agent can choose to manufacture the product (M) without examining it, examine the product (E), inspect the machine (I) or repair the machine (R) before the next production cycle. By examining the product, the subject can determine that it is defective. If, of course, more components have failed, then the product is more likely to be defective. The transition function, observation functions and reward functions for the two agents, i and j, are as shown in Table 7."}, {"heading": "Appendix B. Algorithm for Value Iteration on Sample Sets", "text": "We show the algorithm for calculating a near-optimal policy tree for the limited horizon using a value titeration at l > 0. At l > 0, the algorithm is reduced to calculating the POMDP policy tree exactly, which is carried out.8 The algorithm consists of the usual two steps: Calculating the predictive accessibility tree for horizon T as part of the accessibility analysis (see Section 17.5 of Russell & Norvig, 2003) at lines 2-6. For major problems, exact POMDP solutions can be replaced by approximate ones, but our error limits are no longer applicable and those of the approximation technique must be factored.Tree at lines 7-28. The value of the convictions on the leaves of the accessibility tree is simply the single-stage expected reward resulting from the best measures."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer", "year": 2000}, {"title": "Interactive epistemology i: Knowledge", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "Hierarchies of conditional beliefs and interactive epistemology in dynamic games", "author": ["P. Battigalli", "M. Siniscalchi"], "venue": "Journal of Economic Theory,", "citeRegEx": "Battigalli and Siniscalchi,? \\Q1999\\E", "shortCiteRegEx": "Battigalli and Siniscalchi", "year": 1999}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Dynamic Programming and optimal control", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Hierarchies of beliefs and common knowledge", "author": ["A. Brandenburger", "E. Dekel"], "venue": "Journal of Economic Theory,", "citeRegEx": "Brandenburger and Dekel,? \\Q1993\\E", "shortCiteRegEx": "Brandenburger and Dekel", "year": 1993}, {"title": "Forest fire monitoring with multiple small uavs", "author": ["D. Casbeer", "R. Beard", "T. McLain", "L. Sai-Ming", "R. Mehra"], "venue": "In American Control Conference,", "citeRegEx": "Casbeer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Casbeer et al\\.", "year": 2005}, {"title": "A generalization of principal component analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta"], "venue": "R.E.Schapire", "citeRegEx": "Collins and Dasgupta,? \\Q2002\\E", "shortCiteRegEx": "Collins and Dasgupta", "year": 2002}, {"title": "A survey of convergence results on particle filtering methods for practitioners", "author": ["D. Crisan", "A. Doucet"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Crisan and Doucet,? \\Q2002\\E", "shortCiteRegEx": "Crisan and Doucet", "year": 2002}, {"title": "Mysterious computational complexity of particle filters", "author": ["F. Daum", "J. Huang"], "venue": "In Conference on Signal and Data Processing of Small Targets, SPIE Proceedings Series,", "citeRegEx": "Daum and Huang,? \\Q2002\\E", "shortCiteRegEx": "Daum and Huang", "year": 2002}, {"title": "An Introduction to Generalized Linear Models, 3rd Ed", "author": ["A. Dobson"], "venue": null, "citeRegEx": "Dobson,? \\Q2002\\E", "shortCiteRegEx": "Dobson", "year": 2002}, {"title": "Improved state estimation in multiagent settings with continuous or large dscrete state spaces", "author": ["P. Doshi"], "venue": "In Twenty Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi,? \\Q2007\\E", "shortCiteRegEx": "Doshi", "year": 2007}, {"title": "Approximating state estimation in multiagent settings using particle filters", "author": ["P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Autonomous Agents and Multi-agent Systems Conference (AAMAS),", "citeRegEx": "Doshi and Gmytrasiewicz,? \\Q2005\\E", "shortCiteRegEx": "Doshi and Gmytrasiewicz", "year": 2005}, {"title": "A particle filtering based approach to approximating interactive pomdps", "author": ["P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Twentieth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi and Gmytrasiewicz,? \\Q2005\\E", "shortCiteRegEx": "Doshi and Gmytrasiewicz", "year": 2005}, {"title": "Generalized point based value iteration for interactive pomdps", "author": ["P. Doshi", "D. Perez"], "venue": "In Twenty Third Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Doshi and Perez,? \\Q2008\\E", "shortCiteRegEx": "Doshi and Perez", "year": 2008}, {"title": "Graphical models for online solutions to interactive pomdps", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "In Autonomous Agents and Multiagent Systems Conference (AAMAS),", "citeRegEx": "Doshi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2007}, {"title": "Rao-blackwellised particle filtering for dynamic bayesian networks", "author": ["A. Doucet", "N. de Freitas", "K. Murphy", "S. Russell"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "Sequential Monte Carlo Methods in Practice", "author": ["A. Doucet", "N.D. Freitas", "N. Gordon"], "venue": null, "citeRegEx": "Doucet et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2001}, {"title": "Reasoning about Knowledge", "author": ["R. Fagin", "J. Halpern", "Y. Moses", "M. Vardi"], "venue": null, "citeRegEx": "Fagin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Fagin et al\\.", "year": 1995}, {"title": "A probabilistic approach to collaborative multi-robot localization", "author": ["D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun"], "venue": "Autonomous Robots on Heterogenous Multi-Robot Systems,", "citeRegEx": "Fox et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2000}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D.K. Levine"], "venue": null, "citeRegEx": "Fudenberg and Levine,? \\Q1998\\E", "shortCiteRegEx": "Fudenberg and Levine", "year": 1998}, {"title": "Bayesian Data Analysis, Second Edition", "author": ["A. Gelman", "J. Carlin", "H. Stern", "D. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Bayesian inference in econometric models using monte carlo integration", "author": ["J. Geweke"], "venue": null, "citeRegEx": "Geweke,? \\Q1989\\E", "shortCiteRegEx": "Geweke", "year": 1989}, {"title": "A framework for sequential planning in multiagent settings", "author": ["P. Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gmytrasiewicz and Doshi,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2005}, {"title": "Novel approach to non-linear/non-gaussian bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A. Smith"], "venue": "IEEE Proceedings-F,", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Games with incomplete information played by bayesian players", "author": ["J.C. Harsanyi"], "venue": "Management Science,", "citeRegEx": "Harsanyi,? \\Q1967\\E", "shortCiteRegEx": "Harsanyi", "year": 1967}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W.K. Hastings"], "venue": null, "citeRegEx": "Hastings,? \\Q1970\\E", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Topology-free typology of beliefs", "author": ["A. Heifetz", "D. Samet"], "venue": "Journal of Economic Theory,", "citeRegEx": "Heifetz and Samet,? \\Q1998\\E", "shortCiteRegEx": "Heifetz and Samet", "year": 1998}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Sampling in factored dynamic systems", "author": ["D. Koller", "U. Lerner"], "venue": "Sequential Monte Carlo Methods in Practice. Springer", "citeRegEx": "Koller and Lerner,? \\Q2001\\E", "shortCiteRegEx": "Koller and Lerner", "year": 2001}, {"title": "Recursive bayesian estimation using piecewise constant approximations", "author": ["S.C. Kramer", "H. Sorenson"], "venue": null, "citeRegEx": "Kramer and Sorenson,? \\Q1988\\E", "shortCiteRegEx": "Kramer and Sorenson", "year": 1988}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vitanyi"], "venue": null, "citeRegEx": "Li and Vitanyi,? \\Q1997\\E", "shortCiteRegEx": "Li and Vitanyi", "year": 1997}, {"title": "Formulation of bayesian analysis for games with incomplete information", "author": ["J. Mertens", "S. Zamir"], "venue": "International Journal of Game Theory,", "citeRegEx": "Mertens and Zamir,? \\Q1985\\E", "shortCiteRegEx": "Mertens and Zamir", "year": 1985}, {"title": "Applications for mini vtol uav for law enforcement. In SPIE 3577:Sensors, C3I, Information, and Training Technologies for Law Enforcement", "author": ["D. Murphy", "J. Cycon"], "venue": null, "citeRegEx": "Murphy and Cycon,? \\Q1998\\E", "shortCiteRegEx": "Murphy and Cycon", "year": 1998}, {"title": "Sampling methods for action selection in influence diagrams", "author": ["L. Ortiz", "L. Kaelbling"], "venue": "In Seventeenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ortiz and Kaelbling,? \\Q2000\\E", "shortCiteRegEx": "Ortiz and Kaelbling", "year": 2000}, {"title": "An online pomdp algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Anytime point-based value iteration for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Value-directed compression in pomdps", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Poupart and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2003}, {"title": "Vdcbpi: An approximate algorithm scalable for large-scale pomdps", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Poupart and Boutilier,? \\Q2004\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2004}, {"title": "Value-directed sampling methods for belief monitoring in pomdps", "author": ["P. Poupart", "L. Ortiz", "C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Poupart et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2001}, {"title": "Online planning algorithms for pomdps", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Finding approximate pomdp solutions through belief compression", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Roy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Artificial Intelligence: A Modern Approach (Second Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2003\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2003}, {"title": "Iterative Methods for Sparse Linear Systems", "author": ["Y. Saad"], "venue": null, "citeRegEx": "Saad,? \\Q1996\\E", "shortCiteRegEx": "Saad", "year": 1996}, {"title": "Survey of uav applications in civil markets", "author": ["Z. Sarris"], "venue": "In IEEE Mediterranean Conference on Control and Automation,", "citeRegEx": "Sarris,? \\Q2001\\E", "shortCiteRegEx": "Sarris", "year": 2001}, {"title": "Chernoff-hoeffding bounds for applications with limited independence", "author": ["J.P. Schmidt", "A. Siegel", "A. Srinivasan"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Schmidt et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 1995}, {"title": "Improved memory bounded dynamic programming for decentralized pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Journal of Autonomous Agents and Multiagent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "The optimal control of partially observable markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Recursive bayesian estimation using gaussian sums", "author": ["H.W. Sorenson", "D.L. Alspach"], "venue": null, "citeRegEx": "Sorenson and Alspach,? \\Q1971\\E", "shortCiteRegEx": "Sorenson and Alspach", "year": 1971}, {"title": "Kalman Filtering: Theory and Application", "author": ["Sorenson", "H.W. (Ed"], "venue": null, "citeRegEx": "Sorenson and .Ed...,? \\Q1985\\E", "shortCiteRegEx": "Sorenson and .Ed...", "year": 1985}, {"title": "Monte carlo pomdps", "author": ["S. Thrun"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Thrun,? \\Q2000\\E", "shortCiteRegEx": "Thrun", "year": 2000}, {"title": "Feature-based methods for large scale dynamic programming", "author": ["J. Tsitsiklis", "B.V. Roy"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1996\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1996}, {"title": "Bayesian sparse sampling for online reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 26, "context": "types in Bayesian games (Harsanyi, 1967; Mertens & Zamir, 1985).", "startOffset": 24, "endOffset": 63}, {"referenceID": 1, "context": "An agent\u2019s beliefs within I-POMDPs are called interactive beliefs, and they are nested analogously to the hierarchical belief systems considered in game theory (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), in theoretical computer science (Fagin, Halpern, Moses, & Vardi, 1995) and to the hyper-priors in hierarchical Bayesian models (Gelman, Carlin, Stern, & Rubin, 2004).", "startOffset": 160, "endOffset": 249}, {"referenceID": 1, "context": "An agent\u2019s beliefs within I-POMDPs are called interactive beliefs, and they are nested analogously to the hierarchical belief systems considered in game theory (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), in theoretical computer science (Fagin, Halpern, Moses, & Vardi, 1995) and to the hyper-priors in hierarchical Bayesian models (Gelman, Carlin, Stern, & Rubin, 2004). Since the interactive beliefs may be infinitely nested, Gmytrasiewicz and Doshi (2005) defined finitely nested I-POMDPs as computable specializations of the infinitely nested ones.", "startOffset": 236, "endOffset": 505}, {"referenceID": 25, "context": "We generalize the particle filter, and more specifically the bootstrap filter (Gordon et al., 1993), to the multiagent setting, resulting in the interactive particle filter (I-PF).", "startOffset": 78, "endOffset": 99}, {"referenceID": 27, "context": "Among the spectrum of MC techniques, two that have been particularly well-studied in sequential settings are Markov chain Monte Carlo (MCMC) (Hastings, 1970; Gelman et al., 2004), and particle filters (Gordon et al.", "startOffset": 141, "endOffset": 178}, {"referenceID": 22, "context": "Among the spectrum of MC techniques, two that have been particularly well-studied in sequential settings are Markov chain Monte Carlo (MCMC) (Hastings, 1970; Gelman et al., 2004), and particle filters (Gordon et al.", "startOffset": 141, "endOffset": 178}, {"referenceID": 25, "context": ", 2004), and particle filters (Gordon et al., 1993; Doucet et al., 2001).", "startOffset": 30, "endOffset": 72}, {"referenceID": 18, "context": ", 2004), and particle filters (Gordon et al., 1993; Doucet et al., 2001).", "startOffset": 30, "endOffset": 72}, {"referenceID": 22, "context": "Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state.", "startOffset": 24, "endOffset": 45}, {"referenceID": 53, "context": "Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001).", "startOffset": 138, "endOffset": 186}, {"referenceID": 17, "context": ", 1993; Doucet et al., 2001). Approximating the I-POMDP belief update using the former technique, may turn out to be computationally exhaustive. Specifically, MCMC algorithms that utilize rejection sampling (e.g. Hastings, 1970) may cause a large number of intentional models to be sampled, solved, and rejected, before one is utilized for propagation. In addition, the complex estimation process in I-POMDPs makes the task of computing the acceptance ratio for rejection sampling computationally inefficient. Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state. However, these distributions are neither efficient to compute nor easy to derive analytically. Particle filters need not reject solved models and compute a new model in replacement, propagating all solved models over time and resampling them. They are intuitively amenable to approximating the I-POMDP belief update and produce reasonable approximations of the posterior while being computationally feasible. Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001). While Thrun (2000) integrates particle filtering with Q-learning to learn the policy, Poupart et al.", "startOffset": 8, "endOffset": 1401}, {"referenceID": 17, "context": ", 1993; Doucet et al., 2001). Approximating the I-POMDP belief update using the former technique, may turn out to be computationally exhaustive. Specifically, MCMC algorithms that utilize rejection sampling (e.g. Hastings, 1970) may cause a large number of intentional models to be sampled, solved, and rejected, before one is utilized for propagation. In addition, the complex estimation process in I-POMDPs makes the task of computing the acceptance ratio for rejection sampling computationally inefficient. Although Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, it would involve sampling from a conditional distribution of the physical state given the observation history and model of other, and from the distribution of the other\u2019s model given the physical state. However, these distributions are neither efficient to compute nor easy to derive analytically. Particle filters need not reject solved models and compute a new model in replacement, propagating all solved models over time and resampling them. They are intuitively amenable to approximating the I-POMDP belief update and produce reasonable approximations of the posterior while being computationally feasible. Particle filters previously have been successfully applied to approximate the belief update in continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001). While Thrun (2000) integrates particle filtering with Q-learning to learn the policy, Poupart et al. (2001) assume the prior existence of an exact value function and present an error bound analysis of substituting the POMDP belief update with particle filters.", "startOffset": 8, "endOffset": 1490}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005).", "startOffset": 110, "endOffset": 206}, {"referenceID": 45, "context": "Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality.", "startOffset": 50, "endOffset": 62}, {"referenceID": 42, "context": "An approximate way of solving POMDPs online is the RTBSS approach (Paquet, Tobin, & Chaib-draa, 2005; Ross et al., 2008) that adopts the branch-andbound technique for pruning the look ahead reachability tree.", "startOffset": 66, "endOffset": 120}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality.", "startOffset": 111, "endOffset": 405}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.", "startOffset": 111, "endOffset": 718}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes.", "startOffset": 111, "endOffset": 1627}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes. Pineau et al. (2006) perform point-based value iteration (PBVI) by selecting a small subset of reachable belief points at each step from the belief simplex and planning only over these belief points.", "startOffset": 111, "endOffset": 1750}, {"referenceID": 4, "context": "An important class of such algorithms prescribe substituting the complex belief space with a simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart & Boutilier, 2003; Roy, Gordon, & Thrun, 2005). The premise of these methods is that the beliefs \u2013 distributions over all the physical states \u2013 contain more information than required in order to plan near-optimally. Poupart and Boutilier (2003) use Krylov subspaces (Saad, 1996) to directly compress the POMDP model, and analyze the effect of the compression on the decision quality. To ensure lossless compression, i.e. the decision quality at each compressed belief is not compromised, the transition and reward functions must be linear. Roy et al. (2005) proposed using principal component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) to uncover a low dimensional belief subspace that usually encompasses a robot\u2019s potential beliefs. The method is based on the observation that beliefs along many real-world trajectories exhibit only a few degrees of freedom. The effectiveness of these methods is problem specific; indeed, it is possible to encounter problems where no substantial belief compression may occur. When applied to the I-POMDP framework, the effectiveness of the compression techniques would depend, for example, on the existence of agent models whose likelihoods within the agent\u2019s belief do not change after successive belief updates or on the existence of correlated agent models. Whether such models exist in practice is a topic of future work. Techniques that address the curse of history in POMDPs also exist. Poupart and Boutilier (2004) generate policies via policy iteration using finite state controllers with a bounded number of nodes. Pineau et al. (2006) perform point-based value iteration (PBVI) by selecting a small subset of reachable belief points at each step from the belief simplex and planning only over these belief points. Doshi and Perez (2008) outline the challenges and develop PBVI for I-POMDPs.", "startOffset": 111, "endOffset": 1952}, {"referenceID": 25, "context": "Particle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) are specific implementations of Bayes filters tailored toward making Bayes filters applicable to non-linear dynamic systems.", "startOffset": 22, "endOffset": 64}, {"referenceID": 18, "context": "Particle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) are specific implementations of Bayes filters tailored toward making Bayes filters applicable to non-linear dynamic systems.", "startOffset": 22, "endOffset": 64}, {"referenceID": 23, "context": "Rather than sampling directly from the target distribution which is often difficult, PFs adopt the method of importance sampling (Geweke, 1989), which allows samples to be drawn from a more tractable distribution called the proposal distribution, \u03c0.", "startOffset": 129, "endOffset": 143}, {"referenceID": 25, "context": "To avoid this degeneracy, Gordon et al. (1993) suggested inserting a resampling step, which would increase the population of those particles that had high importance weights.", "startOffset": 26, "endOffset": 47}, {"referenceID": 16, "context": "The general algorithm for the particle filtering technique is given by Doucet et al. (2001). We concentrate on a specific implementation of this algorithm, that has previously been studied under various names such as MC localization, survival of the fittest, and the bootstrap filter.", "startOffset": 71, "endOffset": 92}, {"referenceID": 9, "context": "Crisan and Doucet (2002) outline a rigorous proof of the convergence of this algorithm toward the true posterior as N \u2192 \u221e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 29, "context": "Following Kaelbling et al. (1998), we assume that the value of the prize is 10, that the pain associated with encountering the tiger can be quantified as -100, and that the cost of listening is -1.", "startOffset": 10, "endOffset": 34}, {"referenceID": 26, "context": "The intentional models are analogous to types as used in Bayesian games (Harsanyi, 1967).", "startOffset": 72, "endOffset": 88}, {"referenceID": 12, "context": "As mentioned by Gmytrasiewicz and Doshi (2005), we may also ascribe the subintentional models, SMj , which constitute the remaining models in Mj,l\u22121.", "startOffset": 34, "endOffset": 47}, {"referenceID": 22, "context": "Additionally, the nested beliefs are, in general, analogous to hierarchical priors utilized for Bayesian analysis of hierarchical data (Gelman et al., 2004).", "startOffset": 135, "endOffset": 156}, {"referenceID": 12, "context": "2 For an illustration of the belief update, additional details on I-POMDPs, and how they compare with other multiagent planning frameworks, see (Gmytrasiewicz & Doshi, 2005). In a manner similar to the belief update in POMDPs, the following proposition holds for the I-POMDP belief update. The proposition results from noting that Eq. 3 expresses the belief in terms of parameters of the previous time step only. A complete proof of the belief update and this proposition is given by Gmytrasiewicz and Doshi (2005).", "startOffset": 161, "endOffset": 515}, {"referenceID": 11, "context": "We observe that the level 1 densities may be represented using any family of probability distributions such as exponential family (Dobson, 2002) or polynomials that allow approximation of any function up to arbitrary accuracy.", "startOffset": 130, "endOffset": 144}, {"referenceID": 22, "context": "At a general level, these equations represent an application of the update of hierarchical priors given observed data (Gelman et al., 2004) to the problem of state estimation in multiagent settings.", "startOffset": 118, "endOffset": 139}, {"referenceID": 18, "context": "The resulting algorithm inherits the convergence properties of the original algorithm (Doucet et al., 2001).", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": "Doshi (2007) presented a Rao-Blackwellised I-PF (RB-IPF), where the conditional distribution is updated using a variational Kalman filter.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "Gmytrasiewicz and Doshi (2005) provide an explanation of the exact I-POMDP belief update shown here.", "startOffset": 18, "endOffset": 31}, {"referenceID": 46, "context": "Unmanned agents such as UAVs are finding important applications in tasks such as fighting forest fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005; Sarris, 2001), law enforcement (Murphy & Cycon, 1998) and reconnaissance in warfare.", "startOffset": 103, "endOffset": 166}, {"referenceID": 53, "context": "For example, Thrun (2000) uses a k-nearest neighborhood approach for this purpose.", "startOffset": 13, "endOffset": 26}, {"referenceID": 38, "context": "RTS shares its conceptual underpinnings with the belief expansion models of PBVI (Pineau et al., 2006), but differs in that our method is applicable to online policy tree generation for IPOMDPs, compared to PBVI\u2019s use in offline policy generation for POMDPs.", "startOffset": 81, "endOffset": 102}, {"referenceID": 30, "context": "It is also similar to the sparse sampling technique proposed for selecting actions for exploration during reinforcement learning (Kearns et al., 2002) and for online planning in POMDPs (Ross et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 42, "context": ", 2002) and for online planning in POMDPs (Ross et al., 2008) by sampling the look ahead trees.", "startOffset": 42, "endOffset": 61}], "year": 2009, "abstractText": "Partially observable Markov decision processes (POMDPs) provide a principled framework for sequential planning in uncertain single agent settings. An extension of POMDPs to multiagent settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief systems which represent an agent\u2019s belief about the physical world, about beliefs of other agents, and about their beliefs about others\u2019 beliefs. This modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. We describe a general method for obtaining approximate solutions of I-POMDPs based on particle filtering (PF). We introduce the interactive PF, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. The interactive PF is able to mitigate the belief space complexity, but it does not address the policy space complexity. To mitigate the policy space complexity \u2013 sometimes also called the curse of history \u2013 we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. While this approach does not completely address the curse of history, it beats back the curse\u2019s impact substantially. We provide experimental results and chart future work.", "creator": "dvips(k) 5.95b Copyright 2005 Radical Eye Software"}}}