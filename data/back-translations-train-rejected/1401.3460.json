{"id": "1401.3460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Policy Iteration for Decentralized Control of Markov Decision Processes", "abstract": "Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. As a formal framework for such problems, we use the decentralized partially observable Markov decision process (DEC-POMDP). Though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem, optimal algorithms for the multiagent case have been elusive. The main contribution of this paper is an optimal policy iteration algorithm for solving DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent policies. The solution can include a correlation device, which allows agents to correlate their actions without communicating. This approach alternates between expanding the controller and performing value-preserving transformations, which modify the controller without sacrificing value. We present two efficient value-preserving transformations: one can reduce the size of the controller and the other can improve its value while keeping the size fixed. Empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm, which sacrifices convergence to optimality. This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known. While this assumption may not hold in general, it helps produce higher quality solutions in our test problems.", "histories": [["v1", "Wed, 15 Jan 2014 05:20:25 GMT  (657kb)", "http://arxiv.org/abs/1401.3460v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel s bernstein", "christopher amato", "eric a hansen", "shlomo zilberstein"], "accepted": false, "id": "1401.3460"}, "pdf": {"name": "1401.3460.pdf", "metadata": {"source": "CRF", "title": "Policy Iteration for Decentralized Control of Markov Decision Processes", "authors": ["Daniel S. Bernstein", "Christopher Amato", "Eric A. Hansen", "Shlomo Zilberstein"], "emails": ["bern@cs.umass.edu", "camato@cs.umass.edu", "hansen@cse.msstate.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": "1. Introduction", "text": "In some environments, stakeholders must base their decisions on partial information about the state of the system. In this case, it is often better to share the more general framework of partially observable Markov decision processes (POMDPs). Domains where these types of problems occur include networking, multi-robot coordination, e-commerce, and space exploration systems. The decentralized partially observable Markov decision method provides an effective framework for such problems. Although this model has been recognized for decades (Witsenhausen, 1971), there is little work on demonstrably optimal algorithms for it. On the other hand, POMDPs have been extensively studied in recent decades."}, {"heading": "2. Formal Model of Distributed Decision Making", "text": "We start with a description of the formal framework on which our work is based, which extends the well-known Markov decision-making process to enable distributed policy implementation, defines an optimal solution for this model, and discusses two different representations of these solutions."}, {"heading": "2.1 Decentralized POMDPs", "text": "A decentralized, partially observable Markov decision-making process (DEC-POMDP) is formally defined as a tuple < I, S, ~ A, \u03b2, \u03b2, R, ~, O >, where \u2022 I is a finite set of agents. \u2022 S is a finite set of states, with different starting states s0. \u2022 A = \u00d7 i, IAi is a set of joint actions, where Ai is the set of actions for agents i. \u2022 T: S \u00d7 ~ A \u2192 S is the state transitional function, which defines the distribution of states resulting from the start in a particular state, and each actor carrying out an action. \u2022 R: S \u00d7 ~ A, < is the reward function for agents for each set of joint actions and each state. \u2022 I, which includes a set of joint observations, where we include observations for agents i."}, {"heading": "2.2 Solution Representations", "text": "Dre rf\u00fc nde nlrfhEe\u00fccnlrr\u00fc c\u00fc\u00fc\u00fceegnlrfhsr\u00fc\u00fc\u00fce eeirlrfhe\u00fccnlhsrf\u00fc ide rf\u00fc ide rf\u00fc ide nlrfhtee\u00fce\u00fccnlrrgne\u00fceFnr ni rde nlrfhree\u00fccehnlrrc\u00fceaeFnlrrrgnee\u00fce\u00fceegnllcnlrrrrsrrgtee\u00fce\u00fccehngcehnlrrrc\u00fceeeeeeeeeE ni ni.nlrwdeeiD \"D eiD eiD\" s \"so os os os os os os os os, rf\u00fc os os os os os os os os os os os os os os os os os os os os os os os os os os, rf\u00fc\" rf\u00fc \"lrfos\" lrfos, rfos os os os os os, rfos os os os."}, {"heading": "3. Centralized Dynamic Programming", "text": "In this section we will cover the most important concepts of dynamic programming for the single agent case. This will form the basis for the dynamic multi-agent programming algorithm described below."}, {"heading": "3.1 Value Iteration for POMDPs", "text": "This algorithm is more complicated than its MDP counterpart and does not have guarantees of efficiency, but in practice it can provide a significant lever in solving POMDPs. We will start by explaining how each POMDP has an equivalent MDP with a continuous state space. Next, we will describe how the value functions for this MDP have a special structure that can be exploited. These ideas are central to the algorithm of value teration."}, {"heading": "3.1.1 Belief State MDPs", "text": "A convenient way to summarize an agent's observation history in a POMDP is defined by a belief state that represents a distribution across system states."}, {"heading": "3.1.2 Pruning Vectors", "text": "Of course, it is possible to use a non-minimal amount of vectors to represent the same function, as illustrated in Figure 3b. Note that the removal of certain vectors does not change the value of any faith state. Vectors such as these are not necessary to remain in memory. Formally, we say that a vector \u03b3 is dominated if there is a vector based on linear programming for all faith states b. However, for a given vector program in Table 1, the vector program determines whether the vector is dominated. Since dominated vectors are not necessary, it would be useful to have a method to remove these vectors. This task is often called pruning and has an efficient algorithm based on linear programming. For a given vector vector vector part, the linear program in Table 1 determines whether this vector is dominated. If variables can be found to generate positive vectors, then any value function improves."}, {"heading": "3.1.3 Dynamic Programming Update", "text": "In this section we describe how to implement a dynamic programming update to convert from a value function Vt + 1 to a value function Vt + 1. Each vector that could potentially be included in the implementation represents the value of an action and the mapping of vectors to observations. A combination of a rule of action and transition is hereinafter referred to as a one-step policy. The value vector for a one-step policy can be determined by looking at the measures taken, the resulting state is transferred to an observation, and the value of the assigned vector to step. This is determined via the equation equation equation + 1i (s) = R (s, \u03b1 (i) + \u03b2 \"s \u00b2 s \u00b2 s, o P (s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, \u03b1 (i) s \u00b2 s \u00b2 s \u00b2."}, {"heading": "3.1.4 Value Iteration", "text": "To implement a value variation, we simply start with an arbitrary linear and convex value function and proceed with DP updates, corresponding to a value variation in the equivalent faith state MDP, thus converging after a finite number of iterations to a -optimal value function. The value variation provides a value function, but a policy is required to execute it. As in the MDP case, we can proceed a step forward by using the equation \u03b4 (b) = argmaxa \u0441S (s, a) b (s, a) b (s, b (s, s) + \u03b2 (s) b (o | b, a), using the state of faith resulting from the starting point in faith state b, acting a (s, a), and receiving observation o (s), a maximum residual value must be used to track the state of faith."}, {"heading": "3.2 Policy Iteration for POMDPs", "text": "An earlier iteration algorithm developed by Sondik used this political representation (Sondik, 1978), but it was very complicated and did not succeed in practice. We will describe another approach that worked better on test problems, in which a policy is presented as a finite state controller."}, {"heading": "3.2.1 Finite-State Controllers", "text": "Using a finite state controller, an agent has a finite number of internal states. His actions are based only on q q | \u03b2 \u2022 his internal state, and transitions between internal states occur when observations are received. Internal states provide agents with a type of memory that can be crucial for difficult POMDPs. Naturally, an agent's memory is limited by the number of internal states he possesses. Generally, an agent cannot remember his entire history of observations, as this would require an infinite number of internal states. However, an example of a finite controller can be seen in that one considers only the controller of an agent in Figure 2b. The operation of a single controller is the same as that of each agent in the decentralized case. We formally define a controller as a tuple < Q, a procedure, a procedure > where \u2022 Q is a finite set of controller nodes to be taken. \u2022 Q is a set of controllers to be a set of \u2022 A."}, {"heading": "3.2.2 Algorithmic Framework", "text": "In the following sections we will introduce different ways of implementation. In the first phase of an iteration, the controller is initiated with an arbitrary, linear and convex value function as described above. In the second phase, the piecewise linear and convex value function is created from the evaluation of the controller. Each controller has a value when paired with each state. Thus, each node has a corresponding vector and thus a linear value function over the faith state. The selection of the best node for each faith state results from the evaluation of the controller. The second phase of an iteration is dynamic programming."}, {"heading": "3.2.3 Controller Reductions", "text": "However, these nodes cannot simply be removed as other nodes can merge into them. At this point, the dual view of dominance is useful. Remember that when a node is dominated, there is a convex combination of other nodes whose value from all states is at least as high. Thus, we can remove the dominated node and merge with the dominant convex combination by changing the transition probabilities accordingly. This operation was proposed by Poupart and Boutilier (2003) and is based on previous work by Hansen (1998b). Formally, a controller reduction attempts to replace a node q-Q by a distribution P (q) over the nodes q-Q-q so that for all s-S, V (s, q) and q-Q-Q-Q-Q-V (s, q-Q-Q-Q) a dual distribution can be found."}, {"heading": "3.2.4 Bounded Backups", "text": "The method described in this section tries to increase the value of the controller while fixing its size at the same time. It focuses on one node at a time and tries to change the parameters of the node so that the value of the controller is at least as high for all states of belief. Platzman (1980) came up with the idea for this approach and was made efficient by Poupart and Boutilier (2003). In this method, a node q is selected and the parameters for the conditional distribution P (a, q | q, o) must be determined. Determining these parameters works as follows. We assume that the original controller is used from the second step and try to replace the parameters for q with better ones, just for the first step. In other words, we look for parameters that satisfy the following inequality: V (s, q)."}, {"heading": "4. Decentralized Dynamic Programming", "text": "A central part of the POMDP theory is the fact that each POMDP has an equivalent state of belief MDP. DEC-POMDPs do not know such an outcome, which makes it difficult to generalize the value titeration to the multi-agent case. This lack of a common state of belief requires the development of new tools to solve the DEC-POMDP. As a step in this direction, we were able to develop an optimal policy iteration algorithm for DEC-POMDPs that includes the POMDP version as a special case. This algorithm is the focus of the section. First, we will show how to extend the definition of a stochastic controller to the multi-agent case. As in the single-agent case, multi-agent controllers include a correlation device that represents a source of randomness shared by all stakeholders. This shared randomness increases the solution quality with minimal increase in representation size without having to add communication."}, {"heading": "4.1 Correlated Finite-State Controllers", "text": "The common policy for the agents is represented by a stochastic finite state controller for each agent. In this section, we first define a type of controller in which the agents act independently, then we show an example demonstrating the usefulness of the correlation, and show how to broaden the definition of a controller to allow a correlation between the agents."}, {"heading": "4.1.1 Local Finite-State Controllers", "text": "In a local controller, the agent's node is based on the local observations obtained, and the agent's action is based on the current node. These local controllers are defined in the same way as the POMDP controllers mentioned above, with each agent having its own controller that operates independently of the others. As before, stochastic transitions and action selection are assigned. We formally define a local controller for the agent i as tuple < Qi, Hi, Hi, Higi, Higi >, where \u2022 Qi is a finite series of controller nodes. \u2022 i is a set of inputs that act as local observations for the agent i. \u2022 Ai is a set of outputs that are taken as actions for the agent i. \u2022 i: Qi \u2192 \u0445 Ai is an action selection function for the agent i that indicates the distribution of actions selected at each node of the agent's controller."}, {"heading": "4.1.2 The Utility of Correlation", "text": "The joint controllers described above do not allow the agents to correlate their behavior via a common source of randomness. We will use a simple example to illustrate the benefit of correlation in partially observable areas where agents have limited memory. This example generalizes the benefit of stochastic strategies in partially observable environments that involve a single agent. Let's consider this in Figure 6. This problem has two states, two agents, and two actions per agent (A and B). Agents each have only one observation and therefore cannot distinguish between the two states. In this example, let's consider only conceivable strategies. Suppose that the agents can randomize their behavior independently by using the distributions P (a1) and P (a2). If the agents each have an independent observation and therefore cannot distinguish between the two states, then they will receive an expected reward of \u2212 R2 per time step and thus an expected long-term reward of \u2212 2 (all are equal to R2)."}, {"heading": "4.1.3 Correlated Joint Controllers", "text": "In this subsection, we expand our definition of a common controller to allow correlation between the agents. To do this, we introduce an additional finite machine, called a correlation device, which provides additional signals to the agents at each time step. It operates independently of the DEC-POMDP process and therefore does not provide agents with information about the observations of the other agents. In fact, the random numbers necessary for its operation could be determined before the execution time and made available to all agents. Formally, a correlation device is a tuple < Qc >, where Qc > is a set of nodes and a c distribution: Qc \u2192 Qc is a state transitional function. At each step, the device goes through a transition, and each agent observes its status. We must modify the definition of a local controller to take the state of the correlation device as input."}, {"heading": "4.2 Policy Iteration", "text": "In this section, we describe the policy iteration algorithm. First, we extend the definitions of the exhaustive backup and value preserving transformation to the multi-agent case. Afterwards, we provide a description of the complete algorithm together with a convergence proof."}, {"heading": "4.2.1 Exhaustive Backups", "text": "In the section on dynamic programming of POMDPs, we introduced comprehensive backups. One way to perform a DP update would be to perform a comprehensive backup and then trim the nodes generated. More efficient implementations were described afterwards. These implementations included the intertwining of trimming with the node generation. In the case of multi-agents, it is an open problem whether trimming can be linked to node generation. Nodes can be removed, as we will show in a later section, but for convergence we need comprehensive backups. We do not define DP updates in the case of multi-agents and instead make exhaustive backups a central component of our algorithm. An exhaustive backup adds nodes to the local controllers for all agents at the same time and leaves the correlation device unchanged. For each agent i, | Ai | Qi | | nodes, formal backups are added, one step at a time."}, {"heading": "4.2.2 Value-Preserving Transformations", "text": "We will now expand the definition of a value preserving transformation to the multi-agent case. In the following subsection, we will show how this definition enables convergence to optimality as the number of iterations increases. Dual interpretation of dominance is helpful to understand multi-agent value reserving transformations. Remember, for a POMDP, we could say that a node is dominated if there is a convex combination of other nodes whose value is at least the same for all states. Although we have defined a value preserving transformation in terms of the value preservation function over belief states, we could have defined it equally so that each node in the original controller has a dominant convex combination in the new controller. In the multi-agent case, we do not have the concept of a belief state MDP, so we will take the second approach mentioned above. Specifically, we demand that dominant convex combinations for all local controller and correlation node combinations be applied."}, {"heading": "4.2.3 Algorithmic Framework", "text": "In the first part of an iteration, the controller is evaluated by solving a system of linear equations. Next, a comprehensive backup is performed to add the nodes to the local controllers. Finally, value-preserving transformations are performed. In contrast to the single agent case, there is no Bellman residual product for performing a test of convergence to optimization. We resort to a simpler test for optimization based on the discount rate and the number of iterations to date. Let us be the largest absolute value of an immediate reward possible in the DEC-POMDP. Our algorithm ends after iteration t, when \u03b2t + 1 | Rmax | 1 \u2264. At this point, due to the discounting, the value of any policy is less than step t. Justification for this test is provided in the convergence proof. The complete algorithm is outlined in the table before convergence."}, {"heading": "4.3 Efficient Value-Preserving Transformations", "text": "In this section we will describe how controller reductions and limited backups can be extended to the multi-agent case. We will show that both operations are value preserving transformations."}, {"heading": "4.3.1 Controller Reductions", "text": "Remember that in individual cases, a node can be removed if there is another node with a value at least as high for all faiths. The equivalent double interpretation is that a node can be removed if there is a convex combination of other nodes with a value at least as high across the entire national territory. Using the double interpretation, we can extend this to a rule for removing nodes in the multi-agent case. The rule applies to the removal of nodes from a local controller or from the correlation apparatus. To do this, we need to find a distribution of nodes from a local controller or the correlation apparatus, we consider the nodes of the other controllers as part of the hidden statute. More precisely, we assume that we remove a node from a local controller. To do this, we need to find a distribution P (q) over nodes."}, {"heading": "4.3.2 Bounded Dynamic Programming Updates", "text": "In the previous section, we described a way to reduce the size of a controller without sacrificing a value. We remember that, on a case-by-case basis, we could also use limited backups to increase the value of the controller while maintaining its size. This technique can be extended to the multi-agent case. To do this, we select an agent i, along with a node qi. Then we look for new parameters for the conditional distribution P (ai, q).The search for new parameters works as follows: We assume that the original controller will be used from the second step, and try to replace the parameters for Qi with better ones."}, {"heading": "4.4 Open Issues", "text": "We pointed out at the beginning of the section that there is no known way to convert a DECPOMDP into an equivalent state of belief MDP. Despite this fact, we have been able to develop a demonstrably convergent policy iteration algorithm. However, the policy iteration algorithm for POMDPs has other desirable properties besides convergence that we have not yet been able to extend to the multi-agent case."}, {"heading": "4.4.1 Error Bounds", "text": "The first property is the presence of a Bellman residual value. In the single agent case, it is possible to calculate a distance to optimality limit using two successive value functions. In the multi-agent case, the policy iteration results in a sequence of controllers, each of which has a value function. However, we have no way of obtaining an error limit from these value functions. To bind the distance to optimality, we must first consider the discount rate and the number of iterations completed."}, {"heading": "4.4.2 Avoiding Exhaustive Backups", "text": "When performing a DP update for POMDPs, it is possible to remove certain nodes from consideration without creating them beforehand. In Section 3, we have given a high-level description of some different approaches to do this. However, for DEC POMDPs, we have not defined a DP update and instead used exhaustive backups to extend a controller. Since exhaustive backups are expensive, it would be useful to extend the more complex pruning methods for POMDPs to the case of multiagents. Unfortunately, the correctness evidence for these methods all takes advantage of the fact that there is a Bellman equation. Broadly speaking, this equation allows us to determine whether a potential node is dominated only by analyzing the nodes that would be its successors. As we currently have no analogy of the Bellman equation for DEC POMDPs, it is not possible for us to make an exception to the above statement, but if an agent for the defined PODP type can then be made available to all of a specific agent."}, {"heading": "5. Heuristic Policy Iteration", "text": "While the optimal method of political iteration shows how to find a group of controllers with values arbitrarily close to the optimum, the resulting controllers can be very large and many unnecessary nodes can be generated along the way. This is exacerbated by the fact that the algorithm cannot exploit an initial state distribution and must try to improve the controller for any initial state. To combat these disadvantages, we have developed a heuristic version of political iteration that removes the nodes only at a certain set of centralized beliefs because of their value. We call these centralized beliefs because they are distributions across the system state that are generally known only through full observation of the problem. As a result, the algorithm will no longer be optimal, but it can often produce more forward-looking controllers with higher solution quality for a particular initial state distribution."}, {"heading": "5.1 Directed Pruning", "text": "There are two main advantages of this approach: it allows the simultaneous pruning of all actors and the preference of all nodes, which are not always able to remove all nodes without any loss."}, {"heading": "5.2 Belief Set Generation", "text": "As mentioned above, our heuristic policy iteration algorithm constructs a series of points of faith for each agent, which are later used to evaluate the common controller and remove dominated nodes. In order to generate the point of faith, we start with the initial state and by making assumptions about the other agents, we can calculate the resulting state of faith for each action and each observation pair of an agent. By establishing the guidelines for the other agents, we can calculate this faith actualization in a way that is very similar to the one described in Section 3.1.1 for POMDPs. This procedure can be repeated from each resulting state of faith until a desired number of points is generated or no new points are visited. Formally, we assume that the other agents P \u2212 P \u2212 P \u2212 have a fixed choice of action for each system state. That is, i, i, i, i, i i, i i i, i i i i, i i i, i P (~ a \u2212 i | s), then we can determine the probability of any state result, i, or no new points. Formal, we assume that the other agents have a fixed action choice for each system state i, i, i, i, i, i, i, i i i, i i, i i i, i, i i i, i, i i, i i i, i, i, i, i i, i, i, i, i, i i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i point, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "5.3 Algorithmic Framework", "text": "We offer a formal description of our approach in Table 8. Given the desired number of faith points, k, and random action and observation selections for each agent, the sets of points are generated as described above. The search starts at the initial state of the problem and continues until the specified number of points is reached. If no new points are found, this process can be repeated to ensure that a varied amount is produced. The arbitrary output controller is evaluated and the value is maintained in each state and for each initial node of the controller of an agent. The exhaustive backup procedure is exactly the same as that used in the optimal algorithm, but the update of the controller takes place in two steps. First, for each of the k faith points, the highest rated set of the controller's initial nodes is found. To achieve this, the value of starting at each combination of nodes is calculated for all agents, with the respective node for each of these nodes being weighted, and the best combination maintained."}, {"heading": "6. Dynamic Programming Experiments", "text": "Due to the flexibility of the algorithm, it is impossible to explore all possible ways of implementing it. However, we experimented with a few different implementation strategies to get an idea of how the algorithm works in practice, all of which were conducted on a 3.40GHz Intel Pentium 4 with 2GB of memory. Three main experiments were conducted on a single set of test problems. Our first set of experiments focused on exhaustive backups and controller reductions, and the results confirm that an improvement in value can be achieved by iterating the application of these two operations. Further improvements are also evident in the inclusion of limited updates. However, since exhaustive backups are expensive, the algorithm could not perform more than a few iterations of each of our test problems. In the second set of experiments, we addressed the complexity problems by using limited backups and not exhaustive backups."}, {"heading": "6.1 Test Domains", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "6.2 Exhaustive Backups and Controller Reductions", "text": "In this section, we present the results of using exhaustive backups along with controller reductions. For each domain, the initial controller for each agent contained a single node with a self-loop, and there was no correlation device. For each problem, the first action of the problem description was used. This led to the repeated actions of opening the left door in the two agent tigers problem, rising into the meeting on a grid problem and turning left into the sliding box problem. The reason for starting with the smallest possible controllers was to see how many iterations we could complete before running reminders. On each iteration, we performed an exhaustive backup and then alternated between agents who could perform controller reductions until no further nodes could be removed. For limited dynamic programming results after the reductions were completed, limited updates were performed for all agents. For these experiments, we tried to feather the nodes oach, until again could not be improved."}, {"heading": "6.3 Bounded Dynamic Programming Updates", "text": "This leads of course to the question how much improvement is possible without exhaustive feedbacks. In this section we describe an experiment in which we repeatedly apply limited feedbacks that specify the size of the controller. However, we have initialized different startup sizes for the local joysticks and the access controllers. We define a test run of the algorithm as follows: At the beginning of a test run a size is chosen for each of the local joysticks and the access controller. The action selection and the transition to the terminals are marked with the results."}, {"heading": "6.4 Heuristic Dynamic Programming Updates", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we will be able, that we will be able, that we will be able, and that we will be able, that we will be able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "6.5 Discussion", "text": "We have shown that using controller reductions together with exhaustive backups is more efficient in terms of memory than using exhaustive backups alone. However, due to the complexity of exhaustive backups, even this approach could only complete a few iterations of each of our test problems. Using limited backups alone provided a good way to deal with the complexity issues. With limited backups, we were able to find higher-rated policies than the previous approach. Through our experiments, we were able to understand how the size of the local controllers and the correlation device affected the final values achieved. With our heuristic policy iteration algorithm, we showed further improvements by addressing some of the complexity issues. The heuristic approach is often able to improve the solution quality beyond the point where the optimal algorithm exhausts the limited representation size."}, {"heading": "7. Conclusion", "text": "In fact, it is not that we are in a position to embark on a search for a solution that has the same value as ourselves. (...) It is not that we are going to engage in a solution. (...) It is not that we are going to engage in a solution. (...) It is not that we are going to engage in a solution. (...) It is that we are not going to engage in a solution. (...) It is that we are going to engage in a solution. (...) It is that we are going to engage in a solution. (...) It is that we are going to engage in a solution. (...). \"(...).\" (...). \"(...).\" (...). \"(.\" (...). \"(...). (.\" (.). \"(.).\" (...). (. \"(.).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).).\" (.). \"(.\" (.). \"(.).\" (. \"(.).).\" (.). \"(.).\" (. (.). \"(.). (.).\" (.). \"(.)..\" (. \"(.). (.).\" (.). \"(..). (. (..). (.). (.). (.). (.).). (. (.).). (. (.).). (. (.).). (. (.).). (. (.). (."}, {"heading": "Acknowledgments", "text": "We thank Martin Allen, Marek Petrik and Siddharth Srivastava for helpful discussions about this work. Marek and Siddharth in particular helped formalize and prove Theorem 1. The anonymous reviewers provided valuable feedback and suggestions. Support for this work came in part from the National Science Foundation under the grants IIS-0535061 and IIS-0812149, NASA under the cooperation agreement NCC-2-1311, and the Air Force Office of Scientific Research under the grants F49620-03-1-0090 and FA9550-08-1-0181."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "A correlation device produces a sequence of values that all agents can observe. Let X be the set of all possible infinite sequences that can be generated by a correlation device. Let Vx (~ q0, s0) is the value of the correlated common control with respect to any correlation sequence x-X, output node ~ q0 of the agent controller, and output state s0 of the problem. We will refer to Vx (~ q0, s0) simply as Vx - the value of any sequence x, given to the controllers for the agents. We define a regular sequence as a sequence that can be generated by a regular expression. Before we prove that theorem 1, we establish the following property, whether regular or non-regular, within any other sequence, we can be approximated within any other sequence. Proof: The property holds thanks to the discontinuous factor, the DPOP infinhorizon is used."}, {"heading": "Appendix B. Proof of Lemma 1", "text": "For the simplicity of the representation we prove the problem under the assumption that there is no correlation apparatus. Including a q q q is simple but unnecessarily tedious.Lemma 1 Let C and D be correlated joint controllers, and let C and D be the common nodes for these controllers ~ Q and ~ R, respectively. Consequently, there is a function fi: Qi \u2192 Ri for each controller i so that for all s, S and ~ q, QV (s, ~ q) ~ r P (s, ~ r) V (s, ~ r) V (s, ~ r).We now define functions f \u00b2 i to create the map between the two controllers C and D \u00b2 s. For the old nodes, we define f \u00b2 i to generate the same output as fi. It remains to specify the results f \u00b2 that we apply to the new noves."}], "references": [{"title": "Optimizing memory-bounded controllers for decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K.J. Astrom"], "venue": "Journal of Mathematical Analysis and Applications, 10, 174\u2013205.", "citeRegEx": "Astrom,? 1965", "shortCiteRegEx": "Astrom", "year": 1965}, {"title": "Subjectivity and correlation in randomized strategies", "author": ["R.J. Aumann"], "venue": "Journal of Mathematical Economics, 1, 67\u201396.", "citeRegEx": "Aumann,? 1974", "shortCiteRegEx": "Aumann", "year": 1974}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Bounded policy iteration for decentralized POMDPs", "author": ["D.S. Bernstein", "E.A. Hansen", "S. Zilberstein"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bernstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2005}, {"title": "Planning with incomplete information as heuristic search in belief space", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proceedings of the Fifth International Conference on AI Planning and Scheduling,", "citeRegEx": "Bonet and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2000}, {"title": "Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Algorithms for Partially Observable Markov Decision Processes", "author": ["Cheng", "H.-T."], "venue": "Ph.D. thesis, University of British Columbia.", "citeRegEx": "Cheng and H..T.,? 1988", "shortCiteRegEx": "Cheng and H..T.", "year": 1988}, {"title": "Approximate solutions for partially observable stochastic games with common payoffs", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schnieder", "S. Thrun"], "venue": "In Proceedings of the Third International Joint Conference on Autonomous Agents and Multi Agent Systems,", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2004}, {"title": "Region-based incremental pruning for POMDPs", "author": ["Z. Feng", "S. Zilberstein"], "venue": "In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Feng and Zilberstein,? \\Q2004\\E", "shortCiteRegEx": "Feng and Zilberstein", "year": 2004}, {"title": "Efficient maximization in solving POMDPs", "author": ["Z. Feng", "S. Zilberstein"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence,", "citeRegEx": "Feng and Zilberstein,? \\Q2005\\E", "shortCiteRegEx": "Feng and Zilberstein", "year": 2005}, {"title": "Solving POMDPs by searching in policy space", "author": ["E. Hansen"], "venue": "Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence, pp. 211\u2013219.", "citeRegEx": "Hansen,? 1998a", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Finite-Memory Control of Partially Observable Systems", "author": ["E.A. Hansen"], "venue": "Ph.D. thesis, University of Massachusetts Amherst, Amherst, Massachusetts.", "citeRegEx": "Hansen,? 1998b", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["E.A. Hansen", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "Hansen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2004}, {"title": "Point-based policy iteration", "author": ["S. Ji", "R. Parr", "H. Li", "X. Liao", "L. Carin"], "venue": "In Proceedings of the Twenty-Second National Conference on Artificial Intelligence,", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Multi-agent influence diagrams for representing and solving games", "author": ["D. Koller", "B. Milch"], "venue": "Games and Economic Behavior,", "citeRegEx": "Koller and Milch,? \\Q2003\\E", "shortCiteRegEx": "Koller and Milch", "year": 2003}, {"title": "Game networks", "author": ["P. La Mura"], "venue": "Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, pp. 335\u2013342.", "citeRegEx": "Mura,? 2000", "shortCiteRegEx": "Mura", "year": 2000}, {"title": "Applications of Best-First Heuristic Search to Finite-Horizon Partially Observed Markov Decision Processes", "author": ["III J.W. Lark"], "venue": "Ph.D. thesis, University of Virginia.", "citeRegEx": "Lark,? 1990", "shortCiteRegEx": "Lark", "year": 1990}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["M.L. Littman", "A.R. Cassandra", "L.P. Kaelbling"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "D. Pynadath", "M. Yokoo", "M. Tambe", "S. Marsella"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "author": ["R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence,", "citeRegEx": "Nair et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2005}, {"title": "Implementation of Symbolic Model Checking for Probabilistic Systems", "author": ["D.A. Parker"], "venue": "Ph.D. thesis, University of Birmingham, Birmingham, England.", "citeRegEx": "Parker,? 2002", "shortCiteRegEx": "Parker", "year": 2002}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Proceedings of the Sixteenth International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "A feasible computational approach to infinite-horizon partiallyobserved Markov decision processes", "author": ["L.K. Platzman"], "venue": "Tech. rep., Georgia Institute of Technology. Reprinted in Working Notes of the 1998 AAAI Fall Symposium on Planning Using Partially Observable Markov Decision Processes.", "citeRegEx": "Platzman,? 1980", "shortCiteRegEx": "Platzman", "year": 1980}, {"title": "Bounded finite state controllers", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Poupart and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2003}, {"title": "Improved memory-bounded dynamic programming for decentralized POMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Probabilistic navigation in partially observable environments", "author": ["R. Simmons", "S. Koenig"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Simmons and Koenig,? \\Q1995\\E", "shortCiteRegEx": "Simmons and Koenig", "year": 1995}, {"title": "Learning to Solve Markovian Decision Processes", "author": ["S. Singh"], "venue": "Ph.D. thesis, University of Massachusetts, Amherst, Massachusetts.", "citeRegEx": "Singh,? 1994", "shortCiteRegEx": "Singh", "year": 1994}, {"title": "Learning without state-estimation in partially observable markovian decision processes", "author": ["S.P. Singh", "T. Jaakkola", "M.I. Jordan"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "Generating exponentially smaller POMDP models using conditionally irrelevant variable abstraction", "author": ["T. Smith", "D.R. Thompson", "D.S. Wettergreen"], "venue": "In Proceedings of the Seventeenth International Conference on Applied Planning and Scheduling", "citeRegEx": "Smith et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2007}, {"title": "The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research, 26, 282\u2013304.", "citeRegEx": "Sondik,? 1978", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "An optimal best-first search algorithm for solving infinite horizon DEC-POMDPs", "author": ["D. Szer", "F. Charpillet"], "venue": "In Proceedings of the Sixteenth European Conference on Machine Learning,", "citeRegEx": "Szer and Charpillet,? \\Q2005\\E", "shortCiteRegEx": "Szer and Charpillet", "year": 2005}, {"title": "Point-based dynamic programming for DEC-POMDPs", "author": ["D. Szer", "F. Charpillet"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence,", "citeRegEx": "Szer and Charpillet,? \\Q2006\\E", "shortCiteRegEx": "Szer and Charpillet", "year": 2006}, {"title": "MAA*: A heuristic search algorithm for solving decentralized POMDPs", "author": ["D. Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Szer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2005}, {"title": "Separation of estimation and control for discrete time systems", "author": ["H.S. Witsenhausen"], "venue": "Proceedings of the IEEE, 59 (11), 1557\u20131566.", "citeRegEx": "Witsenhausen,? 1971", "shortCiteRegEx": "Witsenhausen", "year": 1971}, {"title": "Planning with partially observable Markov decision processes: Advances in exact solution methods", "author": ["N.L. Zhang", "S.S. Lee"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang and Lee,? \\Q1998\\E", "shortCiteRegEx": "Zhang and Lee", "year": 1998}, {"title": "Speeding up the convergence of value iteration in partially observable Markov decision processes", "author": ["N.L. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}], "referenceMentions": [{"referenceID": 38, "context": "Though this model has been recognized for decades (Witsenhausen, 1971), there has been little work on provably optimal algorithms for it.", "startOffset": 50, "endOffset": 70}, {"referenceID": 11, "context": "On the other hand, POMDPs have been studied extensively over the past few decades (Smallwood & Sondik, 1973; Simmons & Koenig, 1995; Cassandra, Littman, & Zhang, 1997; Hansen, 1998a; Bonet & Geffner, 2000; Poupart & Boutilier, 2003; Feng & Zilberstein, 2004; Smith & Simmons, 2005; Smith, Thompson, & Wettergreen, 2007).", "startOffset": 82, "endOffset": 319}, {"referenceID": 2, "context": "The importance of correlation has been recognized in the game theory community (Aumann, 1974), but there has been little work on algorithms for finding correlated policies.", "startOffset": 79, "endOffset": 93}, {"referenceID": 13, "context": "The first algorithm for the finite-horizon case (Hansen et al., 2004) can be extended to the infinite-horizon case and viewed as interleaving exhaustive backups and controller reductions.", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "The bounded policy iteration algorithm for DEC-POMDPs (Bernstein et al., 2005), which extends a POMDP algorithm proposed by Poupart and Boutilier (2003), can be viewed through the lens of our framework as repeated application of a specific value-preserving transformation.", "startOffset": 54, "endOffset": 78}, {"referenceID": 3, "context": "The bounded policy iteration algorithm for DEC-POMDPs (Bernstein et al., 2005), which extends a POMDP algorithm proposed by Poupart and Boutilier (2003), can be viewed through the lens of our framework as repeated application of a specific value-preserving transformation.", "startOffset": 55, "endOffset": 153}, {"referenceID": 3, "context": "This is seen in the complexity of the finite-horizon problem with at least two agents, which is NEXP-complete (Bernstein et al., 2002) and thus in practice may require double exponential time.", "startOffset": 110, "endOffset": 134}, {"referenceID": 29, "context": "A simple example illustrating this for POMDPs is given by Singh (1994), which can be easily extended to DEC-POMDPs.", "startOffset": 58, "endOffset": 71}, {"referenceID": 1, "context": "It was shown by Astrom (1965) that a belief state constitutes a sufficient statistic for the agent\u2019s observation history, and it is possible to define an MDP over belief states as follows.", "startOffset": 16, "endOffset": 30}, {"referenceID": 31, "context": "The key result in making dynamic programming practical was proved by Smallwood and Sondik (1973), who showed that the Bellman operator preserves piecewise linearity and convexity of a value function.", "startOffset": 69, "endOffset": 97}, {"referenceID": 26, "context": "This dual view of dominance was first used in a POMDP context by Poupart and Boutilier (2003), and is useful for policy iteration, as will be explained later.", "startOffset": 65, "endOffset": 94}, {"referenceID": 39, "context": "More details on the derivation and use of this formula are provided by Zhang and Zhang (2001). There are |A||\u0393t| possible one-step policies.", "startOffset": 71, "endOffset": 94}, {"referenceID": 6, "context": "The first algorithm along these lines was the incremental pruning algorithm (Cassandra et al., 1997).", "startOffset": 76, "endOffset": 100}, {"referenceID": 29, "context": "Algorithms which identify these belief states include Smallwood and Sondik\u2019s \u201cone-pass\u201d algorithm (1973), Cheng\u2019s linear support and relaxed region algorithms (Cheng, 1988), and Kaelbling, Cassandra and Littman\u2019s Witness algorithm (1998).", "startOffset": 54, "endOffset": 105}, {"referenceID": 29, "context": "Algorithms which identify these belief states include Smallwood and Sondik\u2019s \u201cone-pass\u201d algorithm (1973), Cheng\u2019s linear support and relaxed region algorithms (Cheng, 1988), and Kaelbling, Cassandra and Littman\u2019s Witness algorithm (1998). The second approach is based on generating and pruning sets of vectors.", "startOffset": 54, "endOffset": 238}, {"referenceID": 6, "context": "The first algorithm along these lines was the incremental pruning algorithm (Cassandra et al., 1997). Recently, improvements have been made to this approach (Zhang & Lee, 1998; Feng & Zilberstein, 2004, 2005). It should be noted that there are theoretical complexity barriers for DP updates. Littman et al. (1995) showed that under certain widely believed complexity theoretic assumptions, there is no algorithm for performing a DP update that is worst-case polynomial in all the quantities involved.", "startOffset": 77, "endOffset": 314}, {"referenceID": 19, "context": "It is possible to find the maximum distance between two piecewise linear and convex functions in polynomial time with an algorithm that uses linear programming (Littman et al., 1995).", "startOffset": 160, "endOffset": 182}, {"referenceID": 34, "context": "An early policy iteration algorithm developed by Sondik used this policy representation (Sondik, 1978), but it was very complicated and did not meet with success in practice.", "startOffset": 88, "endOffset": 102}, {"referenceID": 24, "context": "This operation was proposed by Poupart and Boutilier (2003) and built upon earlier work by Hansen (1998b).", "startOffset": 31, "endOffset": 60}, {"referenceID": 11, "context": "This operation was proposed by Poupart and Boutilier (2003) and built upon earlier work by Hansen (1998b). Formally, a controller reduction attempts to replace a node q \u2208 Q with a distribution P (q\u0302) over nodes q\u0302 \u2208 Q \\ q such that for all s \u2208 S, V (s, q) \u2264 \u2211", "startOffset": 91, "endOffset": 106}, {"referenceID": 25, "context": "The idea for this approach originated with Platzman (1980), and was made efficient by Poupart and Boutilier (2003).", "startOffset": 43, "endOffset": 59}, {"referenceID": 25, "context": "The idea for this approach originated with Platzman (1980), and was made efficient by Poupart and Boutilier (2003). In this method, a node q is chosen, and parameters for the conditional distribution P (a, q\u2032|q, o) are to be determined.", "startOffset": 43, "endOffset": 115}, {"referenceID": 26, "context": "Poupart and Boutilier (2003) showed that a local optimum has been reached when each node\u2019s value function is touching the value function produced by performing a full DP backup.", "startOffset": 0, "endOffset": 29}, {"referenceID": 29, "context": "This example generalizes the one given by Singh (1994) to illustrate the utility of stochastic policies in partially observable settings containing a single agent.", "startOffset": 42, "endOffset": 55}, {"referenceID": 0, "context": "Similar to how bounded policy updates can be used in conjunction with pruning in the optimal policy iteration algorithm, a nonlinear programming approach (Amato et al., 2007) can be used to improve solution quality for the heuristic case.", "startOffset": 154, "endOffset": 174}, {"referenceID": 20, "context": "The two agent tiger problem consists of 2 states, 3 actions and 2 observations (Nair et al., 2003).", "startOffset": 79, "endOffset": 98}, {"referenceID": 11, "context": "Hansen et al. (2004) presented a dynamic programming algorithm for finitehorizon POSGs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "This idea is closely related to recent work on graphical games (La Mura, 2000; Koller & Milch, 2003). Once we have a compact representation, the next question to answer is whether we can adapt policy iteration to work efficiently with the representation. This indeed seems possible. With the value-preserving transformations we presented, the nodes of the other agents are considered part of the hidden state of the agent under consideration. These techniques modify the controller of the agent to get value improvement for all possible hidden states. When an agent\u2019s state transitions and rewards do not depend on some other agent, it should not need to consider that agent\u2019s nodes as part of its hidden state. A specific compact representation along with extensions of different algorithms was proposed by Nair et al. (2005).", "startOffset": 67, "endOffset": 827}, {"referenceID": 22, "context": "We give a somewhat informal argument, but this can be more formally proven using cylinder sets as discussed by Parker (2002). We begin by first choosing some regular sequence.", "startOffset": 111, "endOffset": 125}], "year": 2009, "abstractText": "Coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. As a formal framework for such problems, we use the decentralized partially observable Markov decision process (DECPOMDP). Though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem, optimal algorithms for the multiagent case have been elusive. The main contribution of this paper is an optimal policy iteration algorithm for solving DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent policies. The solution can include a correlation device, which allows agents to correlate their actions without communicating. This approach alternates between expanding the controller and performing value-preserving transformations, which modify the controller without sacrificing value. We present two efficient value-preserving transformations: one can reduce the size of the controller and the other can improve its value while keeping the size fixed. Empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum. To broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm, which sacrifices convergence to optimality. This algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents\u2019 actions are known. While this assumption may not hold in general, it helps produce higher quality solutions in our test problems.", "creator": "TeX"}}}