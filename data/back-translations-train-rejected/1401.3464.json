{"id": "1401.3464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Learning Bayesian Network Equivalence Classes with Ant Colony Optimization", "abstract": "Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of moves are allowed. Secondly, moves can be given in terms of indices that are not based on construction graph nodes. The results of testing show that ACO-E performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes.", "histories": [["v1", "Wed, 15 Jan 2014 05:22:48 GMT  (622kb)", "http://arxiv.org/abs/1401.3464v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["r\\'on\\'an daly", "qiang shen"], "accepted": false, "id": "1401.3464"}, "pdf": {"name": "1401.3464.pdf", "metadata": {"source": "META", "title": "Learning Bayesian Network Equivalence Classes with Ant Colony Optimization", "authors": ["R\u00f3n\u00e1n Daly", "Qiang Shen"], "emails": ["RDALY@DCS.GLA.AC.UK", "QQS@ABER.AC.UK"], "sections": [{"heading": null, "text": "proposes a new algorithm called ACO-E to learn the structure of a Bayesian network by searching through the space of Bayesian network equivalence classes using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented: firstly, several types of moves are allowed; secondly, moves can be given in the form of indexes that are not based on construction graphs. Results of the tests show that ACO-E performs better in the space of equivalence classes than a greedy search and other state-of-the-art and metaheuristic algorithms."}, {"heading": "1. Introduction", "text": "The question that arises is whether one or the other is actually a human being, a human being or a person who or the other person is, a man or a woman or a woman, a man or a woman or a man or a woman, the other person or a man or a woman."}, {"heading": "2. Searching for a Bayesian Network Structure", "text": "There are, in general, three different methods that are used in learning the structure of a Bayesian network, most of which are unable to fulfill the condition and then use these conditional dependencies to produce the structure (Spirtes, Glymour, and Schschein, 2000). Probably most known algorithms that use this method are the PC algorithms of Spirtes and Glymour (1990) and the CI and FCI algorithms of Spirtes, Meek, and Richardson (1995), which are able to identify latent variables and selection bias; the second uses dynamic programming and optional clustering to construct a DAG (Imoto, & Miyano, 2003); and the third method - to be discussed here - the search for the space of Bayesian networks. This method uses a scoring function defined by the implementors."}, {"heading": "3. Searching in the Space of Equivalence Classes", "text": "According to many evaluation criteria, there are DAGs that are equivalent to each other in the sense that they generate the same score as each other. It has been known for some time that these DAGs are equivalent to each other because they have the same limitations of independence, even if the structures are different. According to a theorem by Verma and Pearl (1991), two DAGs are equal if and only if they have the same skeletons and the same V-structures. \"Skeleton\" is the undirected diagram that causes all edges in a DAG (see Figure 3) and a \"v-structure\" (sometimes referred to as morality) to be a head-to-head meeting of two arcs in which the tails of the arcs are not connected to each other. These concepts are illustrated in Figure 4. From this notion of equivalence, a class of DAGs that are equivalent to each other can be defined as a class (G)."}, {"heading": "3.1 Representation of Equivalence Classes", "text": "In fact, the fact is that most of us are able to survive on our own, and that we are not able to survive on our own."}, {"heading": "3.2 Techniques for Searching through Equivalence Classes", "text": "iiruiruiruiruiruiruiruiruiruiruiruiios os os os os, ruiuiuiruiuiruiuiios, ruiuiios, ruiuiios, ruiruiiios, ruiiios, ruiruios, ruiuiiios, ruiuiios, ruiuiios, ruiuiiiiiiios, ruiuiiios, ruiuiuios, ruiiuios, ruiuios, ruiuios, ruiuios, ruiuiuios, ruiuiuiuios, ruiuiuiuiuiuios, ruiuiuiuiuiuiuios, ruiuiuiuiuiuiuiuiuios, ruiuiuiuiuiuiuiuiuiuiuiuiuiuios, ruiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiu, ru, ru, ruiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiu, ru, ru, ru, ruiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiui"}, {"heading": "3.3 Advantages of Searching in E-space", "text": "With this representation of equivalence classes of Bayesian network structures and a set of operators that modify the CPDAGs that represent them (e.g. insert an undirected arc, insert a directed arc, etc.), a search can continue. However, what are the reasons for pursuing this type of search? Chickering (2002a) gives a list of reasons, some of which are discussed here. Firstly, an equivalence class can represent many different DAGs in a single structure. A DAG representation can waste time finding DAGs that are in the same equivalence class. And, when searching in the space of DAGs, the connectivity of the search space can mean that the ability to move into a certain adjacent equivalence class can be limited by the particular representation of a DAG. There is also the problem given by the previous probability of the ms used in the scoring function."}, {"heading": "4. Ant Colony Optimization", "text": "The optimization of the ant colony is a global optimization technology generally applied in the field of combinatorial problems, i.e. problems that are discrete solutions. Since the introduction of its current form by Dorigo (1992), ACO has been successfully applied to many difficult combinatorial problems, including the sequential ordering problem (Gambardella & Dorgio, 2000), the problem of vehicle guidance (Bullnheimer, Hartl, & Strauss, 1999), the problem of garbage packaging (Levine & Ducatelle, 2004) and many more (Costa & Hertz, 1997; Gambardella & Dorgio, 2000; Maniezzo & Colorni, 1999; St\u00fctzle, 1998). Such a wide range of applications must raise the question of what is the system that can solve it. ACO's particular form is a metaheuristic one in the field of black intelligence (Bonabeau, Dorigo, & Theraulaz, 1999)."}, {"heading": "4.1 Ant Colony Optimization", "text": "The ant colony is a swarm intelligence based on the feeding path of the real ants. In particular, it uses the principle of stigmatization (the indirect communication of the ants through the environment) as a mechanism of communication; the real ants leave a chemical trail behind them as they explore their surroundings, following a path with more pheromones rather than a path with less (or less) pheromones. This behavior was investigated by Deneubourg, Goss and Pasteels (1990), who designed an experiment with a nest of poor ants, one food source and two paths between them that can leave the nest. Ants would leave the food source and return with food."}, {"heading": "4.2 The ACO Metaheuristic", "text": "In fact, it is such that most people are in a position to embark on a search for a solution that is in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "5. Using Ant Colony Optimization in Learning an Equivalence Class", "text": "To date, many state-backed search algorithms that create Bayesian network structures rely on simple techniques such as greedy searches, which can yield good results but still have the pervasive problem of getting caught at local minima. More sophisticated heuristics have been used, such as iterated mountaineering and simulated glow (Chickering, Geiger et al., 1996), but so far none of these approaches have been applied to the e-space. A related approach by Acid and de Campos (2003) applied taboo-search to a space of limited, partially directed acyclic graphs (RPDAGs), half a house between the spaces provided by DAGs and CPDAGs. This paper attempts to apply ACO metaheuristics to the e-space, the space of equivalence classes of DAGs (LPDAGs), two extensions to the basic metaheuristic elements."}, {"heading": "5.1 Other ACO Algorithms for Learning Bayesian Network Structures", "text": "While ACO has been applied to many problems in the field of combinatorial optimization, so far there has been little research on the use of technology to learn Bayesian network structures. Two alternative methods have been defined by de Campos, Fern\u00e1ndez-Luna et al. (2002) and de Campos, G\u00e1mez and Puerta (2002): the first conducts a search in the space of DAGs orders, while the second searches in the space of DAGs. Since a major theme of this work is this problem, here is a description of both to examine the early work on this topic and to see how they can influence future studies."}, {"heading": "5.1.1 ACO-K2SN", "text": "In the first technique, known as ACO-K2SN, the various problem components are defined as follows, as taken from Section 4.2: Construction Diagram There is one node for each attribute in the data, with an additional dummy node from which to start the search. Restrictions The only limitations are that the tour is a Hamiltonic path. Pheromone paths The pheromone is associated with every arc on the diagram. Each arc in the diagram is intimized to an initial small value. Heuristic information Heuristic information The heuristics on each arc is set to the reverse of the negative log value explained below. Solution Construction The ants work on a system very similar to the ACS system. Starting at the dummy node, the ants construct a complete path that defines an order of nodes. Pheromone update."}, {"heading": "5.1.2 ACO-B", "text": "The second algorithm given by de Campos, Fern\u00e1ndez-Luna et al. (2002) is the ACO-B algorithm. Components for this algorithm are: Construction Graph There is a node for each possible directed arc between each pair of attributes (without self-directed arcs). There is also a dummy node from which the ants proceed. Constraints The only limitations are that the DAG must be acyclic at each node (i, j) is the gain in score that would occur when adding an arc. The pheromone at the node (i, j) corresponds to the directed arc j \u2192 i. Heuristic information The heuristic at each node (i, j) is the gain in score that would occur when adding an arc j."}, {"heading": "5.1.3 PERFORMANCE COMPARISON", "text": "In the results provided by both de Campos, G\u00e1mez et al. (2002) and de Campos, Fern\u00e1ndez-Luna et al. (2002), the ACO-B algorithm performs slightly better in terms of accuracy than ACO-K2SN in the gold standard networks ALARM (Beinlich, Suermondt, Chavez, & Cooper, 1989) and INSURANCE (van der Putten & van Someren, 2004), and it also contains an order of magnitude of fewer statistical tests and should therefore be faster and faster. There are more comparisons between ACO-B and other algorithms by de Campos, Fern\u00e1ndez-Luna et al. (2002)."}, {"heading": "5.2 Relation of ACO-E to the ACO Metaheuristic", "text": "The proposed algorithm, ACO-E, is largely based on the work of de Campos, Fern\u00e1ndez-Luna et al. (2002). In this work, an ACO algorithm called ACO-B. This current work differs in that it searches in e-space, uses more than one operator (add an arc) and is not limited to using matrices to store pheromones. \u2022 S, the set of candidate solutions, is the set of all CPDAGs on the nodes of the Bayesian network, that the characteristics of the SDAK network are uniform. \u2022 S, the set of all candidate solutions, is the set of all CPDAGs on the nodes of the Bayesian network, and this theorem has a massive cardinality, since it is superexponentially in the number of nodes used to evaluate a candidate."}, {"heading": "5.2.1 THE CONSTRUCTION GRAPH", "text": "The construction graph in an ACO algorithm describes the mechanism by which solutions can be assembled. It is given as a complete graph given over the solution components. As such, these components play a crucial role in the viability of the algorithm. In the ACO-E algorithm, components C of the construction graph are used because they have been verified to work correctly and effectively (2002a). Defining correct operators is difficult, as Chickering has shown by finding counter-examples of the validity of the operators of Munteanu and Cau (2000). These operators are used to work correctly and effectively through chickering (2002a). Defining corative operators is difficult by finding counter-examples of the validity of the operators of Munteanu and Cau (2000). Each ant constructs a solution by going through the construction graph. This corresponds to applying a sequence of movements to a CPAG DAG."}, {"heading": "5.2.2 THE PROBLEM HEURISTIC", "text": "In an ACO algorithm, heuristics is used to guide the search for good solutions. It often does so implicitly in the form of costs associated with the selection of a particular component to be added to the current state; adding a component with the lowest cost is often a useful method to advance in the construction of a solution.In ACO-E, heuristics is used in the same way, with the addition that the cost of adding a component to the current state can be negative, i.e. adding a component to the current state can improve the cost function.Heuristics is dynamic as it depends on the current state of the ant. Furthermore, it is linked to each component c-C, as opposed to the arcs ci-c-j between components. The value of heuristic technology is given by the score gain for each step ci-C that is possible given the current state. Essentially, it corresponds to the change in the score that is given by the execution of a specific CAG on the PAG in this table."}, {"heading": "5.2.3 THE PROBLEM PHEROMONE", "text": "The pheromone in an ACO-E algorithm conducts the search based on the results of previous searches. In many cases, it is associated with the arcs on the construction graph, but in ACO-E it is associated with the nodes of the construction graph, resulting in pheromone values \u03c4i for each ci-C. The pheromone for each \u03c4i is initialized with a value \u03c40 = 1n | SCORE (P +) |. (2) In this formula, n is the number of variables present in the data. SCORE is the objective function f as defined in Section 5.2, and P + is the best solution so far. At the beginning of the algorithm, this is initialized by a greedy search starting from the empty graph. In order for the pheromone to change to reflect the contours of the ants, pheromone actualization rules are given for NES."}, {"heading": "5.2.4 PROBABILISTIC TRANSITION RULE", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "5.2.5 PROPERTIES OF ANTS", "text": "With respect to the ants used to construct solutions, the following characteristics of the ant k must be taken into account: \u2022 The memoryMk can be equated to the current state of the problem given by the ant k. From this, the current CPDAG can be constructed to implement the limitations, calculate heuristic values \u03b7, evaluate the current solution and place pheromone on the tour. In practice, the current CPDAG is usually maintained to avoid recalculation at each step. \u2022 The starting state xks results from the empty sequence < >, i.e. the empty CPDAG. \u2022 The only condition for terminating ek is to stop the tour if no improvement in the score is possible. \u2022 The neighborhood N k (x) is the set of all valid moves drawn taking into account the current CPDAG.409DALY & SHEN."}, {"heading": "5.2.6 LOCAL SEARCH PROCEDURE", "text": "As is so often the case with ACO algorithms, ACO-E can apply a local search procedure throughout the course of the algorithm and at the end to quickly bring a solution to a local maximum. With the current heuristic and the default local search that would be used under these circumstances - greedy search using the operators defined in Table 2, here known as GREEDY-E - the local search would not offer any additional advantage over the solution found by an ant. Nevertheless, if the problem was implemented heuristically differently, the local search would be included in the algorithm. An example of this would be static heuristics obtained by scoring operations on an empty graph. As this is invalid compared to the algorithm run, it would only have to be calculated once at the beginning of the course."}, {"heading": "5.3 Description of ACO-E", "text": "This is done in conjunction with the pseudo-code specified in the algorithms 1 and 2. It is assumed that scoring criteria generally yield negative values, the higher the value, the better the model. This is the case with most of the default criteria discussed in Section 2. The meaning of the parameters is as follows: O This is a set of operators that can modify the current PDAG state in the search. Examples of these are those specified in Table 2, e.g. InsertU, DeleteU, etc. Other operators could be used, e.g. those of Munteanu and Cau (2000) and Munteanu (2001).tmax This is the number of iterations of the algorithms."}, {"heading": "5.4 Trace of Algorithm Execution", "text": "As a simple example of the execution of the ACO-E algorithm, a trace of its behavior during an actual execution will be given in this section. Consider the Bavarian network in Figure 7. This network is fully specified, with a DAG structure and parameters in the form of conditional 412 probability tables. As you can see, variable 0 can be set to values a and b, variable 1 can be set to values c and d and variable 2 can be set to values e, f and g.For the purposes of this demonstration, 90 data from this Bayesian network were sampled. The ACO-E algorithm was then started with the parameters as in Table 3 \u2212 The PDAG found running from the original GREEDY-E was the same as the sample Bayesian network structure. P + was then set to this PDAG."}, {"heading": "5.5 Implementation Issues", "text": "Second, caching the results of the validity tests required to verify which moves are applicable in a given state can dramatically increase performance, but this technique is not as easy to implement as it seems (Daly, Shen, & Aitken, 2006). Caution must also be exercised when implementing the pheromone for the moves. Traditionally, value matrices are used that allow quick access and updating, but in the case of the MakeV operator, which requires three indexes, a three-dimensional matrix would be required, which would be quickly impossible to do as the problem size increases, especially since only some of the entries would be used, which would be because the algorithm never gets to these states. Instead, a structure like a map can store this information. A map can be linked to time if the number of elements can actually be accessed."}, {"heading": "6. Experimental Methodology", "text": "This section is about testing the ACO-E algorithm presented in Section 5 and evaluating the results. In order to facilitate understanding of the experimental methodology used, the section is structured as follows: First, a presentation is given of the objects on which the tests are carried out, which are six Bayesian gold standard networks known in practice; the various properties of the networks are discussed; data can be scanned from these networks and this data can be used as input for the algorithms.Then, experiments with the ACO-E algorithm are shown; the methodology used in conducting the experiments is defined, together with a description of the various evaluation criteria, which are well-known in the industry. Two different sets of experiments are presented, one focusing on comparing ACO-E with similar algorithms, the other on comparing ACO-E with modern algorithms."}, {"heading": "6.1 Standard Bayesian Networks", "text": "This section introduces a set of six gold standard Bayesian networks, which form the basis for the tests that will be presented later. Various properties of the networks are given, covering the number of nodes in the structure, the number of edges in the structure, the average number of edges, etc."}, {"heading": "6.1.1 SIX GOLD-STANDARD NETWORKS", "text": "In the experiments shown in the next section, six gold standard networks are used: ALARM (Beinlich, Suermondt et al., 1989), Barley (Kristensen & Rasmussen, 2002), Diabetes (Andreassen, Hovorka, Benn, Olesen, & Carson, 1991), HailFinder (Abramson, Brown, Edwards, Murphy, & Winkler, 1996), Mildew (Jensen, 1995), and Win95pts networks (Microsoft Research, 1995), which were selected because they covered a wide range of domains, were readily available 415, and all contained discrete attributes; the last property was important because the scoring criterion multinomic random variables used in the experiments was implemented. Various properties of these Bayesian networks are shown in Table 5. In this table, nodes and edges indicate the number of nodes and edges of the structures."}, {"heading": "6.2 Methodology", "text": "This section contains details of the experiments performed using the ACO-E algorithm described in Section 5. First, the methodology used in conducting the experiments is presented, including an analysis of the required results, the design of five experimental conditions, and an explanation of the evaluation criteria."}, {"heading": "6.2.1 EXPERIMENTAL DESIGN", "text": "The first was to analyze the behavior of the algorithm as a function of the parameters and test networks, which is necessary to try and understand the range of values within which parameters might be useful and to show the effects of the ACO behavior on the results. \u2022 The next desired result was to test ACO-E against other similar algorithms. To this end, ACO-E was tested against another ACO algorithm and algorithms sought in the space of equivalence classes. \u2022 Finally, the last desired result was to test ACO-E against other similar algorithms from the literature. These tests would demonstrate the comparative usefulness of ACO-E against other well-known and powerful methods."}, {"heading": "6.2.2 EVALUATION CRITERIA", "text": "In conducting these experiments, different scoring metrics were selected to determine how well certain algorithms behaved: the scoring function used in conducting the experiments, a test scoring function based on a different sample, the structural hamming distance (SHD), the number of scoring functions and the number of different scoring functions, which are explained below. The scoring function for all experiments was used with different parameters, depending on the experimental condition. As these parameters were uniform, the scoring value of a Bayesian network structure could be used to compare the results of different algorithms. In terms of the BDeu score function, this means that the higher average score was achieved, the better the results. Test Scoring function As the scoring function was also used in conducting the algorithm, a separate BDeu scoring function was defined, which means that an independent sample using ASLE4S from each other was not used."}, {"heading": "7. Experimental Results", "text": "This section presents the results of experiments performed according to the methods specified in Section 6.2. In Section 6.2, five experimental conditions were given: the first condition concerned the analysis of the behavior of ACO-E with respect to its parameters and compared to other metaheuristic algorithms that shared similar behavior; the second condition concerned the comparison of ACO-E with other state-of-the-art learning algorithms for network structures; the third condition focused on the effect of sample quantities on output quality, the behavior of a scoring function defined on a separate test set, and the computational complexity of the algorithm; the fourth looked at the behavior of metaheuristic algorithms with coordinated behavior; and finally, the fifth condition dealt with the situations in which ACO-E should be used. These results are presented in this order, followed by a discussion and interpretation of these results."}, {"heading": "7.1 Condition 1", "text": "The results of the runs using experimental condition 1 are presented in two groups that reflect how they will be analyzed later. Firstly, detailed results for ACO-E are given in tables 10 and 11. In these tables, the values given are the results for all other parameters; e.g., the number for \u03c1 = 0.1 is given by calculating the mean and standard deviation for all results with \u03c1 = 0.1. In this case, the size of the samples will be 216 and will be calculated using all combinations of the other parameters. It should be noted that the specific values of \u03c1 = 0 and \u03b2 = 0 are special cases. If \u03c1 = 0, there is no pheromone evaporation and no pheromone deposit on the graph; i.e. pheromone does not play a role in the algorithm. With \u03b2 = 0, there is no heuristics while the ants pass through the construction graph."}, {"heading": "7.2 Condition 2", "text": "The results of these comparisons are in Table 13. The abbreviations given are by Tsamardinos, Brown et al. (2006) and have already been discussed in Section 6.2.1. Some of the results provided by Tsamardinos, Brown et al. are missing and are marked \"N / A\" in Table 13. If one result is out of the reach of most others, it is presented as a number indicating the median."}, {"heading": "7.3 Condition 3", "text": "The results of the experiments performed under Condition 3 are presented here. This series of experiments is designed to show the effects of sample size on the ACO-E output and also to provide a measure of the algorithm's computational complexity. The SHD results of the runs after 200 iterations are shown in Table 14, while the results after 200 iterations are shown in Table 15. Table 16 shows the score results from another test sample. The remaining results of these experiments are shown in Figures 11 and 12. These show the total number of score ratings or the unique number of score ratings for runs of the ACO-E algorithm."}, {"heading": "7.4 Condition 4", "text": "Experimental condition 4 was used to compare ACO-E with the metaheuristic algorithms used in condition 1, when the parameters were matched to the best combinations of condition 1; the other algorithms were ACO-B, EPQ and GREEDY-E. These results are summarized in Table 17, which shows the results after the runs have been completed."}, {"heading": "7.5 Condition 5", "text": "The results of the experiments under condition 5 are shown in Table 18. In these experiments, multiple searches were performed using the GREEDY-E algorithm, with the data for each experiment being sampled anew. Experiments were performed 100 times across all combinations of test networks and sample sizes. 422L E A R N IN G B A Y E S IA N E T W O R K E Q U IV A L E N C E C L A S W IT A N T C O L O N Y O P T IM IZ A T IO N423424425DALY & SHENLEARNING BAYESIAN NETWORK EQUIVALENCE CLASSES WITH ANT COLONOPTIMIZATIOND A LY H H E N428LEARNING BAYESIAN NETWORK EQUIVALENCE CLASSES WITH ANT COLONOPTIMIATY 4ATY 430DATY 4301"}, {"heading": "8. Discussion", "text": "This section will discuss the results presented in the previous section. In general, the discussion about the score and SHD values (as in Section 6.2.2) will be obtained from the algorithms. It should be noted that a better score does not necessarily mean a better SHD value and vice versa. Generally, different datasets may occur where they behave optimally. There does not seem to be a general method to find the optimal values. This problem has been considered in some depth by Silander, Kontkanen et al."}, {"heading": "8.1 ACO-E Behavior", "text": "In this section, the behavior of ACO-E OPTIMIZATION VALUE is not a value or a position. The behavior of ACO-E OPTIMIZATION VALUE is considered not only as a way, but also as a way, in which the behavior of people in the networks can be analyzed. (However, these differences are compared with the two-tailed MannWhitney U test or the T algorithm of students, which depends on the normality of the data, which can be tested with the Jarque Bera test.) The best numbers from Tables 10 to 11 are compared with the situation, in which this particular part of the ACO-E algorithm was turned off. For example, in Table 10 on the alarm line, the best number is 0.5. This is compared to the value at the level of 0 to 0.0, as in this432LEARNING BAYESIAN NETWORK EQUIVENCE CLASSET WORTUNWERWERWORK."}, {"heading": "8.1.5 GENERAL DISCUSSION", "text": "The reason for the strange behavior of the HailFinder results can possibly be explained by looking at the charts of the scoring function and the SHD against time (Figures 9 and 10). One can see that the score improves over the course of iterations, which could lead to the conclusion that there is a problem with the scoring function for the HailFinder case, perhaps with its parameters. Another plausible reason that the results of HailFinder and Win95pts are out of sync with the others is that they are larger networks that could favor a more aggressive use of the best solution so far than the smaller ones. In this case, this would correspond to lower values of B and higher values of q0. Also, heuristic information could be more useful with a large number of variables, which could lead to better results with large values of \u03b2. Note that these problems with the HailFinder network were also observed by de Campos and Castellano (2007)."}, {"heading": "8.2 Behavior of ACO-E with Respect to Test Network and Sample", "text": "In recent years, the number of unemployed in Germany has multiplied, and the number of unemployed has multiplied in recent years."}, {"heading": "8.3 Metaheuristic Algorithm Comparison", "text": "In this context, it should be noted that this is one of the greatest challenges that has ever existed in the history of the European Union."}, {"heading": "8.4 State-of-the-Art Algorithm Comparison", "text": "In this section, the comparison of ACO-E against other state-of-the-art Bayesian Network Structure Learning algorithms is not noted because in Table 13, ACO-E appears to have performed well against these other algorithms. However, the results of statistical comparisons of ACO-E against these algorithms are shown in Table 22.In this table, p-values for individual comparisons of ACO-E against the other algorithms are shown. The test that was used for all these comparisons was the Mann-Whitney U test, as the distributions proved to be abnormal. At the foot of the table, the combined p-value is found of the individual p-values above them. This is the total p-value for comparing ACO-E against all other algorithms. The method of combining these values was combined = 1 \u2212 n-i = 1 \u2212 pi, where pi is the p-value of input i in the table, there are n-values."}, {"heading": "8.5 Computational Complexity of ACO-E", "text": "The results of the experimental condition 3 show two numbers (11 and 12) related to the computational complexity of ACO-E. However, the first shows that the total number of ratings of score functions can be exceeded during the algorithm run, the second shows the number of unique ratings of score functions. Since the algorithm starts as an empty graph, it would take more movements and thus more ratings to reach a maximum. It can also be seen that the total number of ratings is generally linear in terms of the number of iterations performed. Since the algorithm starts as an empty graph, it would take more movements to reach a maximum."}, {"heading": "9. Conclusions and Future Directions", "text": "The main findings of this paper were the development of the ACO-E algorithm as an implementation of ACO metaheuristics on the problem of learning a Bayesian network structure that fits well into a dataset. In short, ACO-E has performed well in reconstructing test networks that sampled data from 439DALY & SHEN. A more detailed look at the behavior of ACO-E in relation to its parameters, the type of test network, and in comparison to other algorithms is now given."}, {"heading": "9.1 ACO-E Behavior as its Parameters are Varied", "text": "When analyzing the behavior of ACO-E depending on its parameters, the best and worst values in each parameter range were compared; the best result was found when the parameter setting yielded either the highest score or the smallest difference from the test network; the worst result was found when the parameter was \"turned off,\" i.e. when it had no effect on the behavior of the algorithm; for all parameters, there was a difference between the behavior of the best and the worst settings; whether or not this difference was significant depended on the network being used as a test; some networks responded better to the algorithm than others; for the networks with which ACO-E worked well, the following trends were observed: \u2022 For data with more characteristics, lower values of Q0 and higher values of \u03b2, higher values worked better; and \u2022 for data with fewer characteristics, higher values of Q0 and lower values of \u03b2, higher values worked better."}, {"heading": "9.2 The Utility of ACO-E as a Function of the Test Network and Sample", "text": "On closer inspection of these networks, they were found to have a high average V structure (as described in Section 6.1.1) per node value, the reason this could make a difference being that nodes with a large number of V structures imply more possible local maxima in the search space. Greedy methods would meet these maxima, while ACO-E is able to navigate around them because it is stochastical not to always select the best move. Experiments were conducted to estimate the average V structure per node value and a correspondence in the large example case. Generally, the method used in the experiment could be used to assess the usefulness of ACO-E in certain situations."}, {"heading": "9.3 ACO-E Performance Compared to Similar Algorithms", "text": "The results of sections 7.1 and 7.4 show that ACO-E performs well compared to other similar algorithms (Chickering, 2002a); \u2022 EPQ, which performs an evolutionary programming search in the space of equivalence classes (Cotta & Muruz\u00e1bal & Cotta, 2004); and \u2022 ACO-B, which performs a search with ACO in the space of DAGs (de Campos, Fern\u00e1ndezLuna et al., 2002).440LEARNING BAYESIAN NETWORK EQUIVALENCE CLASSES WITH ANT COLONY OPTIMIZATION In all cases, the BDeu score of ACO-E was better than the score of the other algorithms."}, {"heading": "9.4 ACO-E Performance Compared to Alternative State-of-the-Art Algorithms", "text": "Similar to the above section, ACO-E performs well in comparison to other state-of-the-art learning algorithms of the Bayesian network structure and performs better in 3 out of 4 tested networks: Barley, Mildew and HailFinder. The first two are networks in which it performs well in self-testing; the HailFinder network postulates that the results are good because of the search space; good results have also been achieved for the greedy Equivalence Search Algorithm (GES), which also scans the space of equivalence classes; while ACO-E did not perform particularly well in the alarm network, it also did not perform badly and came third in the ranking. Reasons for the performance in the alarm network are discussed in Section 8.2."}, {"heading": "9.5 Extending ACO-E to Increase Performance and Scalability", "text": "Since the validity check is the slowest part of the ACO-E algorithm, it remains the first problem to deal with in order to improve runtime. However, when this problem is solved, the focus will revert to the other parts of the algorithm, especially the scoring function. In this way, the scoring functions are evaluated only once per turn and result in an acceleration of the algorithm, which turns out to be more important for making an arc around the empty graph. In this way, the scoring functions are evaluated only once, and then there will be an acceleration of the algorithm."}, {"heading": "Acknowledgments", "text": "The authors thank the co-editor and the reviewers for their comments, which have been very helpful in the revision of this study."}], "references": [], "referenceMentions": [], "year": 2009, "abstractText": "Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of moves are allowed. Secondly, moves can be given in terms of indices that are not based on construction graph nodes. The results of testing show that ACO-E performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes.", "creator": "LaTeX with hyperref package"}}}