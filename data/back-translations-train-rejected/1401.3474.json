{"id": "1401.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Optimal Value of Information in Graphical Models", "abstract": "Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan.", "histories": [["v1", "Wed, 15 Jan 2014 05:30:52 GMT  (382kb)", "http://arxiv.org/abs/1401.3474v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andreas krause", "carlos guestrin"], "accepted": false, "id": "1401.3474"}, "pdf": {"name": "1401.3474.pdf", "metadata": {"source": "CRF", "title": "Optimal Value of Information in Graphical Models", "authors": ["Andreas Krause", "Carlos Guestrin"], "emails": ["KRAUSEA@CALTECH.EDU", "GUESTRIN@CS.CMU.EDU"], "sections": [{"heading": null, "text": "For example, in a sensor network, it is important to select the subset of sensors that are expected to achieve the greatest reduction in uncertainty. In medical decision-making tasks, it is necessary to choose which tests to apply before deciding on the most effective treatment. It has generally been common practice to use heuristically guided procedures when selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. Furthermore, we demonstrate a surprising result: when designing an efficient algorithm for chain graphs in Hidden Markov models (HMMs), this method can be used both to select the optimal subset of observations and to achieve an optimal conditional observation plan. Furthermore, we prove that the optimization value of information is hard even for polytrees."}, {"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves if they do not put themselves in a position to survive themselves. In fact, it is so that they are able to survive themselves, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world, in the world, in the world, in the world"}, {"heading": "2. Problem Statement", "text": "We assume that the state of the world is described by a collection of random variables XV = (X1,.., Xn), where V is an index set. Example: V could denote a series of locations, andXi models the temperature measurement of a sensor placed in place. While some of our algorithms extend to continuous distributions, we generally assume that variables XV are discrete. We take a Bayesian approach and base the results of the variables on a previous probability distribution from P (XV). We select a subset of variables, XA (for A V), and observe XA = xxxxA. For example, A is the set of places we place sensors, or a set of medical tests we perform."}, {"heading": "2.1 Optimization Criteria", "text": "In this paper, we consider a class of local reward functions Ri, which are defined on the marginal probability distributions of the variable Xi = A = 1984. This class has the computational advantage that local rewards can also be evaluated using probability techniques. Then, the total reward will be the sum of all local rewards. Let A be a subset of V. Then, P (Xj | XA = xA) denotes the marginal distribution of variable XJ values conditioned on observations XA = xA. For example, in our temperature monitoring application, Xj models the temperature at the site J-V. The conditional marginal distribution P (Xj = xj) then models the conditional distribution of temperature at the site J after observing the locations A V. For classification purposes, it may be more appropriate to look at the max-marginalsPmax (Xj = xj)."}, {"heading": "2.2 Cost of Selecting Observations", "text": "This may mean that each observation Xj has a related positive penalty Cj, which effectively reduces the reward. In our example, we might be interested in mixing accuracy with the capture of energy expenditure. Alternatively, it is also possible to define a budgetB for the selection of observations, each of which involves integer costs \u03b2j. At this point, we would like to select observations whose total costs are within the budget, but these costs do not reduce the reward. In our current example, the sensors could run on solar energy and recover a certain amount of energy per day, allowing a certain amount of capture. Our formulation of optimization problems allows both penalties and budgets. To simplify the notation, we also write C (A) = annual warming and \u03b2 (A) = annual warming. Instead of fixed penalties and costs per observation, both may depend on the state of the world."}, {"heading": "3. Decomposing Rewards", "text": "In this section, we will present the key observation that enables us to develop efficient algorithms for non-myopic optimizing information in the class of chain graphic models.The algorithms are given in Section 4.The series of random variables XV = {X1,..., Xn} forms a chain of graphic models (a chain) if Xi is conditionally independent of XV\\ {1, i + 1} given Xi \u2212 1 and Xi + 1. Without loss of generality, we can assume that the common distribution is specified by the previous P (X1) of variable X1 and the conditional probability distributions P (Xi + 1).The time series for temperature measured by a sensor in our example can be formulated as a chain graphic model.Note that the transition probabilities P (Xi + 1) of the variable distributions P (Xi + 1) and the conditional probability distributions P (Xi + 1)."}, {"heading": "4. Efficient Algorithms for Optimizing Value of Information", "text": "In this section we present algorithms for efficient and nonmyopic optimization of the value of information in graphical chain models."}, {"heading": "4.1 Efficient Algorithms for Optimal Subset Selection in Chain Models", "text": "In our current example, before using the sensors, we would identify k times that are expected to provide the most informative sensor readings according to our model. (3) The subset selection problem is to find the optimal subsetA values. (3) First, we define the objective function L to subsets of V byL (A). (A) We maximize the sum of expected local rewards minus penalties, with the total cost not exceeding the budget. (3) We solve this optimization problem using a dynamic programming algorithm in which the chain is calculated using the insights from Section 3. Consider a subchain from Xa to Xb."}, {"heading": "4.2 Efficient Algorithms for Optimal Conditional Planning in Chain Models", "text": "In the problem of the conditional plan, we want to calculate an optimal sequential query policy \u03c0: we observe a variable, pay the penalty, and, depending on all values observed in the past, select the next query as long as our budget is sufficient. The goal is to find the plan with the highest expected reward, in which the budget B is not exceeded for each possible sequence of observations. To filter, we can only select future observations, while in case of smoothing, the next observation may be located somewhere in the chain. In our current example, the filtering algorithm would be most appropriate: the sensors would follow the conditional plan and determine the most informative query times based on the previous observations. Figure 1 shows an example of such a conditional plan."}, {"heading": "4.2.1 FROM SUBSET SELECTION TO CONDITIONAL PLANNING", "text": "Note that unlike the selection of the subset we considered in Section 4.1, in conditional planning (\u03b2) j = J (J) the set of variables depends on the state of the world XV = xV. Therefore, the conditional plan \u03c0 could select another group of variables (xV). If the world is in state xV = (high, low, high), then we will consider Figure 1, where the set of possible observations is V = {morn, noon, eve} and XV = (low, high). If the world is in state xV = (high, high), then the conditional plan shown in Figure 1 would select \u03c0 (xV) = {morn, eve}, whereas if XVI = (low, high), it would select the world (xV). Since the conditional plan is a function of the (random) state of the world, it is a fixed random, variable state."}, {"heading": "4.2.2 DYNAMIC PROGRAMMING FOR OPTIMAL CONDITIONAL PLANNING IN CHAINS", "text": "We propose a dynamic programming algorithm to obtain the optimal condition plan, which is similar to the subset algorithm (b) (b). Here, too, we use the decomposition of the ones in Section 3. The difference here is that the selection of the observation and the allocation of the budget now depend on the actual values of the observations. To calculate the value function J (xA; k) for the entire value chain, we calculate the value functions Yes: b (xA; k) for the subchains Xa,., Xb. The basic case of our dynamic programming approach deals with the zero budget setting: Jflta: b (xa; 0) = b \u2212 1 + 1Rj."}, {"heading": "4.3 Efficient Algorithms for Trees with Few Leaves", "text": "In sections 4.1 and 4.2, we have presented dynamic programming algorithms that can optimize the value of information about chain models. Formally, however, a tree model is a common probability distribution P (XV) via a collection of random variables XV, where P (XV) factors asP (XV) = 1 Z (i, j) - trees I (Xi, Xj) - trees I (Xi, j) - are a nonnegative potential function, where mappings to xi and xj to the nonnegative real numbers, E V \u00d7 V are a series of edges that form an undirected tree above the index set V, and Z is a normalization constant that forces a valid probability distribution. The dynamic programming algorithms presented in the preceding sections can easily be extended to such tree models."}, {"heading": "5. Theoretical Limits", "text": "Many problems that can be solved efficiently for discrete chain models can also be solved efficiently for discrete polygons 3. Examples of this are probabilistic conclusions and the most likely explanation (MPE). However, in Section 4.3 we have seen that the complexity of dynamic chain programming algorithms increases dramatically when extended to trees: complexity increases exponentially in the number of leaves of the tree. Surprisingly, this exponential increase in complexity is not avoidable under reasonable complexity theory assumptions for the problem of optimizing the information value. Before making this statement more formal, we briefly review the complexity classes used in our results."}, {"heading": "5.1 Brief Review of Relevant Computational Complexity Classes", "text": "We briefly review the complexity classes used in the following statements by presenting a complete problem for each of the PP classes. For further details, see, for example, the references of Papadimitriou (1995) or Littman, Goldsmith and Mundhenk (1998). Class NP contains decision problems that have polynomial-time verifiable proofs. A well-known complete problem is 3SAT, for which the instances contain Boolean formulas in the normal subjunctive form with a maximum of three letters per clause (3CNF form). Complexity class # P contains counting problems. A complete problem for class # P is # 3SAT, which counts the number of satisfactory instances to a 3CNF formula. PP is a decision version of class # P: A complete problem is MAJSAT, which determines whether a given 3CNF formula for majority formation is satisfied."}, {"heading": "5.2 Complexity of Computing and Optimizing Value of Information", "text": "In order to solve the optimization problems, we most likely need to evaluate the objective function, i.e. the expected local rewards. Our first result is that even if we specialize in the decision-theoretical value of information objective functions as defined in Section 2.1, this problem is insoluble even for naive-bayes models, a special case of discrete polytrees. Naive-bayes models are often used in classification tasks (c.f., Domingos & Pazzani, 1997), where the class variable is predicted by noisy observations that are conditionally independent in view of class variables. In a sense, naive-bayes models are the \"next simple\" (from the perspective of inference) classes of Bayesian networks according to chains. Note that Naive-Bayes models correspond to the \"stars\" mentioned in Section 4.3, which have a number of subtrees exponentially in the number of variable ones."}, {"heading": "6. Experiments", "text": "In this section we evaluate our algorithms based on several real data sets. A particular focus is on comparing the optimal methods with the greedy heuristic and other heuristic methods for selecting observations and on how the algorithms can be used for interactive structured classification."}, {"heading": "6.1 Temperature Time Series", "text": "The first data set consists of temperature time series derived from a sensor network used at Intel Research Berkeley (Deshpande et al., 2004), as described in our current example, which were collected continuously for 19 days, linear interpolation was used in case of missing samples, the temperature was measured once every 60 minutes and discredited in 10 containers at 2 degrees Kelvin. To avoid overmatching, we used pseudo-counting \u03b1 = 0.5 when learning the model. Using parameter parts, we learned four sets of transition probabilities: from 12: 00 to 7: 00, from 7: 00 to 12: 00, from 12: 00 to 19: 00 and from 19: 00 to 12: 00. By combining data from three adjacent sensors, we obtained 53 time series. The goal of this task was to select from 24 time points during day k, during which sensor measurements are most informative. The experiment was designed so that the performance of the optimal algorithms, which are gifted, is almost identical, and half the observations of today are more uniform."}, {"heading": "6.2 CpG-Island Detection", "text": "Subsequently, we investigated the bioinformatics problem of finding CpG islands in DNA sequences. CpG islands are regions in the genome with a high concentration of the cytosine-guanine sequence. These regions are presumably located mainly around the promoters of genes that are frequently expressed in the cell. In our experiment, we looked at the gene loci HS381K22, AF047825 and AL133174, for which the GenBank note listed three, two and one CpG islands respectively. We used our algorithm on a 50-base window at the beginning and end of each island, using the transition and emission probabilities of Durbin, Eddy, Krogh and Mitchison (1999) for our Hidden Markov model, and we used the sum of margins as a reward function. The aim of this experiment was to pinpoint the beginning and end of the CpG islands more precisely by showing experts that the baseline values are not part of the classification or the mean values for the CG3 (although the mean values for the classification are expected to be accurate)."}, {"heading": "6.3 Part-of-Speech Tagging", "text": "In our third experiment, we investigated the structured part-of-speech (POS) classification task, in which each word is part of an entity (e.g., \"European Union\") and each entity belongs to one of five categories: location, miscellaneous, organization, person, or other. Let's imagine an application where automatic information extraction is led by an expert: Our algorithms calculate an optimal conditional plan to ask the expert, trying to optimize the classification performance while requiring as little expert interaction as possible. We used a conditional random field for the structured classification task, in which each node corresponds to a word, and the common distribution by node potentials and edge potentials is described. The sum of margins was used as a reward function. Measuring the classification performance was the F1 score, the geometric mean of precision and recall. The aim of this experiment was to analyze the function, how to increase the potential of the addition and the reward function."}, {"heading": "7. Applying Chain Algorithms for More General Graphical Models", "text": "In Section 4, we have seen algorithms that can be used to plan a single sensor, assuming that the time series of sensor measurements (e.g. temperature) form a Markov chain. This is a very natural assumption for sensor networks (Deshpande et al., 2004). However, when using sensor networks, several sensors must be planned. If the time series are independent for all sensors, we could use our algorithms to plan all sensors independently of each other. In practice, however, the measurements are correlated across the different sensors - in fact, this dependence is indispensable to allow a generalization of measurements in places where no sensor has been placed. Below, we will describe an approach to using our single-sensor scheduling algorithm to coordinate multiple sensors. Formally, we are interested in monitoring a spatial upward phenomenon in a number of locations."}, {"heading": "7.1 Approximate Sensor Scheduling by Lower Bound Maximization", "text": "The reason for the sudden increase in complexity in the case of multiple chains is that the distribution of rewards along sub-chains (as described in Section 3) is not extended to the case of multiple sensors, as the influence can flow over the individual chains. Figure 4 visualizes this problem - where the distribution for the sensor (2) depends on all three observations. We will focus on the decision-theoretical value of information that is objective (as described in Section 2.1), but other local reward functions such as residual entropy can be used as a basis. As a first approximation, we will only allow consideration of recent observations."}, {"heading": "7.2 Proof of Concept Study on Real Deployment", "text": "In the work of Singhvi et al. (2005), we presented an approach to optimize lighting control in buildings, with the aim of satisfying the preferences of the occupants of the building, while minimizing energy consumption. In our approach, a wireless sensor network is used to monitor environmental conditions (such as sunlight intensity, etc.).The sensors feed their measurements to a building controller who actuates the lighting system (lamps, blinds, etc.).At any time, the building controller can select an action that influences lighting conditions in all places.The utilization functions are Ut (a, xS, t), are specified, which will determine the chosen actions and current lighting values."}, {"heading": "8. Related Work", "text": "In this section we discuss related work in various areas."}, {"heading": "8.1 Optimal Experimental Design", "text": "There is a large literature on various approaches to experimental design (c.f., Chaloner & Verdinelli, 1995; Krause, Singh, & Guestrin, 2007).In Bayesian experimental design, a prior distribution about possible states of the world is assumed, and experiments are chosen to reduce uncertainty in posterior distribution. In its general form, Bayesian experimental design was pushed by Lindley (1956).Users encode their preferences in a utility U (P), in which the first argument, P), is a distribution about states of the world (i.e., the parameters), and the second argument, is the true state of the world."}, {"heading": "8.2 Value of Information in Graphical Models", "text": "While the decision-theoretical value of information has often been used for principle-based information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993) and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984), the value of information problems is in some ways a specific case of Bayesian experimental design problems where the prior distribution has a certain structure typically given by a graphic model as considered in this paper. Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) put forward short-sighted, i.e. greedy, approaches to selective collection of evidence in graphic models as considered in this paper, which, unlike the algorithms presented in this paper and their optimal observation models, are only used for certain types of observations (i.e., these algorithms are very generalized)."}, {"heading": "8.3 Bandit Problems and Exploration / Exploitation", "text": "An important class of sequential values of information problems is the class of bandit problems. In the classic k-armed bandit problem formalized by Robbins (1952), a slot machine is equipped with k-weapons. A pull from arm i results in a reward with probability of success pi fixed for each arm, but different (and independent) across each arm. In selecting the arms to be pulled, an important problem is to balance exploration (i.e. estimating the probability of success of the weapons) and exploitation (i.e., the repeated pulling of the best known arm) against each other. Similarly, in the sense that an optimal sequential strategy in polynomial time shows that for a specified number of draws an optimal strategy can be calculated in polynomial time, using a dynamic programming algorithm. Similarly, in the sense that an optimal sequential strategy in polynomial time shows, however, the grid dynamics can be represented in this document as different from the structure of this algorithm."}, {"heading": "8.4 Probabilistic Planning", "text": "However, the problem of optimizing the decision-theoretical value of information can be formalized in a natural way as a (finite horizon) partially observable Markov decision process (POMDP, Smallwood & Roy, 1973). Therefore, in principle, planning algorithms in POMDPs, such as the always available Pineau, Gordon and Thrun algorithm (2006), can be used to optimize the information value. Unfortunately, state space grows exponentially with the number of variables taken into account in the selection problem. Furthermore, the complexity of planning in POMDPs becomes exponentially in the cardinality of state space, i.e. exponentially in the number of variables taken into account in the selection problem."}, {"heading": "8.5 Sensor Selection and Scheduling", "text": "In the context of wireless sensor networks, where sensor nodes have limited battery power and therefore allow only a small number of measurements, optimization of the information value of the selected sensors plays a key role. The problem of when sensors need to be selectively switched on in order to save power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin and Reich (2002). Typically, sensors are thought to be associated with a fixed sensor region and a spatial area needs to be covered by the regions associated with the selected sensors. Abrams, Goel and Plotkin (2004) present an efficient approximation algorithm with theoretical guarantees for this problem. Deshpande, Khuller, Malekian and Toossi (2008) present an approach to this problem based on semi-defined programming (SDP), and unlike the approaches described above, do not foresee ways to optimize the functions based on this problem."}, {"heading": "8.6 Relationship to Machine Learning", "text": "Unfortunately, there are no guarantees for the implementation of this greedy method. However, we are not aware of any work that offers similarly strong performance guarantees as the algorithms considered in this document. The problem of the selection of observations also has a strong connection to the field of active learning (cf. Cohn, Gharamani, & Jordan, 1996; Tong & Koller, 2001), in which the learning system conceives experiments based on its observations, while the limits of sample complexity for some active learning problems have been derived (cf. Dasgupta, 2005; Balcan, Beygelzimer, & Langford, 2006)."}, {"heading": "8.7 Previous Work by the Authors", "text": "An earlier version of this paper appeared in the work of Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work of Singhvi et al. (2005). The current version is much expanded, with new algorithmic and harder results and more detailed discussions. In view of the negative results in Section 5, we cannot expect to be able to optimize the value of information in more complex models than chains. However, instead of trying to find an optimal solution, one might wonder whether it is possible to obtain good approximations. The authors (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec, Guestrin, VanBriesen, & Faloutsos, 2008) showed that a large number of practical objective functions satisfy an intuitive decrease in yield: The addition of a new observation helps more if we have so far only few observations, and less if we have already made many observations."}, {"heading": "9. Conclusions", "text": "We have described novel efficient algorithms for optimal subset selection and conditional plan calculation in chain models (and trees with few leaves), including HMMs. Our empirical assessment shows that these algorithms can improve the commonly used heuristics to reduce expected uncertainty. Our algorithms can also effectively increase performance in interactive structured classification tasks. Unfortunately, even with a low generalization of chains, optimization problems cannot be efficiently calculated. We have also identified surprising theoretical limitations that suggest that even the class of decision-theoretical values of information functions (such as those widely used in influence diagrams and POMDPs) cannot be efficiently calculated even in Naive-Bayes models."}, {"heading": "Acknowledgments", "text": "We thank Ben Taskar for providing the parts tagging model and Reuters for providing their news archive. We also thank Brigham Anderson and Andrew Moore for helpful comments and discussions. This work was supported in part by NSF grants no. CNS-0509383, CNS-0625518, ARO MURI W911NF0710287 and a gift from Intel. Carlos Guestrin was partially supported by an Alfred P. Sloan Fellowship, an IBM Faculty Fellowship and an ONR Young Investigator Award no. 00014-08-1-0752 (2008-2011). Andreas Krause was partially supported by a Microsoft Research Graduate Fellowship."}, {"heading": "Appendix A", "text": "The proof of theory 3. Membership in # P for any discrete polytrees is easy, since conclusions in such models are not possible, unless a set of clauses is chosen. Now, a Bavarian network is created with 2n + 1 variables, X1,.., Xn, U1,., Un and Y, where the Xi are conditionally independent. Let us have the clauses uniformly across the values {\u2212 n, \u2212 1),., \u2212 1,., m \u2212 1, m}, and each Ui has Bernoulli previously with p = 0.5. Let the observed variables Xi have CPTs defined the following way: Xi, Ui = u., if Xi, m}, and each Ui we have Bernoulli."}], "references": [{"title": "Set k-cover algorithms for energy efficient monitoring in wireless sensor networks", "author": ["Z. Abrams", "A. Goel", "S. Plotkin"], "venue": "In IPSN", "citeRegEx": "Abrams et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abrams et al\\.", "year": 2004}, {"title": "Agnostic active learning", "author": ["N. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In ICML", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Stat,", "citeRegEx": "Baum and Petrie,? \\Q1966\\E", "shortCiteRegEx": "Baum and Petrie", "year": 1966}, {"title": "Learning diagnostic policies from examples by systematic search", "author": ["V. Bayer-Zubek"], "venue": "UAI.", "citeRegEx": "Bayer.Zubek,? 2004", "shortCiteRegEx": "Bayer.Zubek", "year": 2004}, {"title": "A Markovian decision process", "author": ["R. Bellman"], "venue": "Journal of Mathematics and Mechanics, 6.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Voila: Efficient feature-value acquisition for classification", "author": ["M. Bilgic", "L. Getoor"], "venue": "In Twenty-Second Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Bilgic and Getoor,? \\Q2007\\E", "shortCiteRegEx": "Bilgic and Getoor", "year": 2007}, {"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Boyen and Koller,? \\Q1998\\E", "shortCiteRegEx": "Boyen and Koller", "year": 1998}, {"title": "Bayesian experimental design: A review", "author": ["K. Chaloner", "I. Verdinelli"], "venue": "Statistical Science,", "citeRegEx": "Chaloner and Verdinelli,? \\Q1995\\E", "shortCiteRegEx": "Chaloner and Verdinelli", "year": 1995}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Gharamani", "M.I. Jordan"], "venue": "J AI Research,", "citeRegEx": "Cohn et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1996}, {"title": "Conference on computational natural language learning shared task", "author": ["CoNLL"], "venue": "http://cnts.uia.ac.be/conll2003/ner/.", "citeRegEx": "CoNLL,? 2003", "shortCiteRegEx": "CoNLL", "year": 2003}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "NIPS.", "citeRegEx": "Dasgupta,? 2005", "shortCiteRegEx": "Dasgupta", "year": 2005}, {"title": "Model-driven data acquisition in sensor networks. In VLDB", "author": ["A. Deshpande", "C. Guestrin", "S. Madden", "J. Hellerstein", "W. Hong"], "venue": null, "citeRegEx": "Deshpande et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2004}, {"title": "Energy efficient monitoring in sensor networks. In LATIN", "author": ["A. Deshpande", "S. Khuller", "A. Malekian", "M. Toossi"], "venue": null, "citeRegEx": "Deshpande et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2008}, {"title": "Myopic value of information in influence diagrams", "author": ["S. Dittmer", "F. Jensen"], "venue": "In UAI,", "citeRegEx": "Dittmer and Jensen,? \\Q1997\\E", "shortCiteRegEx": "Dittmer and Jensen", "year": 1997}, {"title": "On the optimality of the simple Bayesian classifier under zero-one loss", "author": ["P. Domingos", "M. Pazzani"], "venue": "Machine Learning,", "citeRegEx": "Domingos and Pazzani,? \\Q1997\\E", "shortCiteRegEx": "Domingos and Pazzani", "year": 1997}, {"title": "Biological Sequence Analysis : Probabilistic Models of Proteins and Nucleic Acids", "author": ["R. Durbin", "S.R. Eddy", "A. Krogh", "G. Mitchison"], "venue": null, "citeRegEx": "Durbin et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Durbin et al\\.", "year": 1999}, {"title": "A dynamic allocation index for the discounted multiarmed bandit problem", "author": ["J.C. Gittins", "D.M. Jones"], "venue": null, "citeRegEx": "Gittins and Jones,? \\Q1979\\E", "shortCiteRegEx": "Gittins and Jones", "year": 1979}, {"title": "Counting unlabelled subtrees of a tree is #p-complete", "author": ["L.A. Goldberg", "M. Jerrum"], "venue": "LMS J Comput. Math.,", "citeRegEx": "Goldberg and Jerrum,? \\Q2000\\E", "shortCiteRegEx": "Goldberg and Jerrum", "year": 2000}, {"title": "An approximate nonmyopic computation for value of information", "author": ["D. Heckerman", "E. Horvitz", "B. Middleton"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Heckerman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1993}, {"title": "Information value theory", "author": ["R.A. Howard"], "venue": "IEEE Transactions on Systems Science and Cybernetics (SSC-2).", "citeRegEx": "Howard,? 1966", "shortCiteRegEx": "Howard", "year": 1966}, {"title": "Readings on the Principles and Applications of Decision Analysis II, chap. Influence Diagrams, pp. 719\u2013762", "author": ["R.A. Howard", "J. Matheson"], "venue": "Strategic Decision Group, Menlo Park. Reprinted 2005 in Decision Analysis", "citeRegEx": "Howard and Matheson,? \\Q1984\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Non-myopic multi-aspect sensing with partially observable Markov decision processes", "author": ["S. Ji", "R. Parr", "L. Carin"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Selective supervision: Guiding supervised learning with decision-theoretic active learning", "author": ["A. Kapoor", "E. Horvitz", "S. Basu"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Kapoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Decisions with Multiple Objectives: Preferences and Value Trade-offs", "author": ["R.L. Keeney", "H. Raiffa"], "venue": null, "citeRegEx": "Keeney and Raiffa,? \\Q1976\\E", "shortCiteRegEx": "Keeney and Raiffa", "year": 1976}, {"title": "An exact algorithm for maximum entropy sampling", "author": ["C. Ko", "J. Lee", "M. Queyranne"], "venue": "Operations Research,", "citeRegEx": "Ko et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ko et al\\.", "year": 1995}, {"title": "Efficient optimization of information-theoretic exploration in slam", "author": ["T. Kollar", "N. Roy"], "venue": "In AAAI", "citeRegEx": "Kollar and Roy,? \\Q2008\\E", "shortCiteRegEx": "Kollar and Roy", "year": 2008}, {"title": "Sleeping coordination for comprehensive sensing using isotonic regression and domatic partitions", "author": ["F. Koushanfary", "N. Taft", "M. Potkonjak"], "venue": "In Infocom", "citeRegEx": "Koushanfary et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Koushanfary et al\\.", "year": 2006}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proc. of Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Krause and Guestrin,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proc. of IJCAI", "citeRegEx": "Krause and Guestrin,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Efficient sensor placement optimization for securing large water distribution networks", "author": ["A. Krause", "J. Leskovec", "C. Guestrin", "J. VanBriesen", "C. Faloutsos"], "venue": "Journal of Water Resources Planning and Management,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "In JMLR", "citeRegEx": "Krause et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Inference in hybrid networks: Theoretical limits and practical algorithms. In UAI", "author": ["U. Lerner", "R. Parr"], "venue": null, "citeRegEx": "Lerner and Parr,? \\Q2001\\E", "shortCiteRegEx": "Lerner and Parr", "year": 2001}, {"title": "On a measure of the information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics, 27, 986\u20131005.", "citeRegEx": "Lindley,? 1956", "shortCiteRegEx": "Lindley", "year": 1956}, {"title": "The computational complexity of probabilistic planning", "author": ["M. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Littman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1998}, {"title": "Feature selection algorithms: A survey and experimental evaluation", "author": ["L. Molina", "L. Belanche", "A. Nebot"], "venue": "In ICDM", "citeRegEx": "Molina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Molina et al\\.", "year": 2002}, {"title": "Sequential decision models for expert system optimization", "author": ["V.S. Mookerjee", "M.V. Mannino"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Mookerjee and Mannino,? \\Q1997\\E", "shortCiteRegEx": "Mookerjee and Mannino", "year": 1997}, {"title": "Optimal testing of structured knowledge", "author": ["M. Munie", "Y. Shoham"], "venue": "In Twenty-Third Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Munie and Shoham,? \\Q2008\\E", "shortCiteRegEx": "Munie and Shoham", "year": 2008}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley.", "citeRegEx": "Papadimitriou,? 1995", "shortCiteRegEx": "Papadimitriou", "year": 1995}, {"title": "Complexity results and approximation strategies for map explanations", "author": ["J.D. Park", "A. Darwiche"], "venue": "Journal of Aritificial Intelligence Research,", "citeRegEx": "Park and Darwiche,? \\Q2004\\E", "shortCiteRegEx": "Park and Darwiche", "year": 2004}, {"title": "Anytime point-based approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": null, "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning, 1, 81\u2013106.", "citeRegEx": "Quinlan,? 1986", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Efficient deterministic approximation algorithms for non-myopic value of information in graphical models", "author": ["Y. Radovilsky", "G. Shattah", "S.E. Shimony"], "venue": "In IEEE International Conference on Systems, Man and Cybernetics (SMC),", "citeRegEx": "Radovilsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Radovilsky et al\\.", "year": 2006}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society, 58, 527\u2013535.", "citeRegEx": "Robbins,? 1952", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Active learning of partially hidden Markov models for information extraction", "author": ["T. Scheffer", "C. Decomain", "S. Wrobel"], "venue": "In ECML/PKDD Workshop on Instance Selection", "citeRegEx": "Scheffer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Scheffer et al\\.", "year": 2001}, {"title": "Global a-optimal robot exploration in slam", "author": ["R. Sim", "N. Roy"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Sim and Roy,? \\Q2005\\E", "shortCiteRegEx": "Sim and Roy", "year": 2005}, {"title": "Efficient planning of informative paths for multiple robots", "author": ["A. Singh", "A. Krause", "C. Guestrin", "W.J. Kaiser", "M.A. Batalin"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Singh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2007}, {"title": "Intelligent light control using sensor networks", "author": ["V. Singhvi", "A. Krause", "C. Guestrin", "J. Garrett", "H. Matthews"], "venue": "In Proc. of the 3rd ACM Conference on Embedded Networked Sensor Systems (SenSys)", "citeRegEx": "Singhvi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Singhvi et al\\.", "year": 2005}, {"title": "Power efficient organization of wireless sensor networks", "author": ["S. Slijepcevic", "M. Potkonjak"], "venue": "In ICC", "citeRegEx": "Slijepcevic and Potkonjak,? \\Q2001\\E", "shortCiteRegEx": "Slijepcevic and Potkonjak", "year": 2001}, {"title": "The optimal control of partially observable Markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Information gain-based exploration using raoblackwellized particle filters", "author": ["C. Stachniss", "G. Grisetti", "W. Burgard"], "venue": "In Robotics Science and Systems (RSS)", "citeRegEx": "Stachniss et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stachniss et al\\.", "year": 2005}, {"title": "Active learning for parameter estimation in Bayesian networks", "author": ["S. Tong", "D. Koller"], "venue": null, "citeRegEx": "Tong and Koller,? \\Q2001\\E", "shortCiteRegEx": "Tong and Koller", "year": 2001}, {"title": "Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research, 2, 369\u2013409.", "citeRegEx": "Turney,? 1995", "shortCiteRegEx": "Turney", "year": 1995}, {"title": "Selective evidence gathering for diagnostic belief networks", "author": ["L. van der Gaag", "M. Wessels"], "venue": "AISB Quart.,", "citeRegEx": "Gaag and Wessels,? \\Q1993\\E", "shortCiteRegEx": "Gaag and Wessels", "year": 1993}, {"title": "Information-driven dynamic sensor collaboration for tracking applications", "author": ["F. Zhao", "J. Shin", "J. Reich"], "venue": "IEEE Signal Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 19, "context": "In probabilistic reasoning, where one can choose among several possible but expensive observations, it is often a central issue to decide which variables to observe in order to most effectively increase the expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997; Lindley, 1956).", "startOffset": 224, "endOffset": 305}, {"referenceID": 33, "context": "In probabilistic reasoning, where one can choose among several possible but expensive observations, it is often a central issue to decide which variables to observe in order to most effectively increase the expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997; Lindley, 1956).", "startOffset": 224, "endOffset": 305}, {"referenceID": 52, "context": "In a medical expert system, for example, multiple tests are available, and each test has a different cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993).", "startOffset": 106, "endOffset": 159}, {"referenceID": 3, "context": "Many researchers have suggested the use of myopic (greedy) approaches to select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007).", "startOffset": 93, "endOffset": 233}, {"referenceID": 19, "context": "This general setup of selecting observations goes back in the decision analysis literature to the notion of value of information by Howard (1966) and in the statistical literature to the notion of Bayesian Experimental Design by Lindley (1956).", "startOffset": 132, "endOffset": 146}, {"referenceID": 19, "context": "This general setup of selecting observations goes back in the decision analysis literature to the notion of value of information by Howard (1966) and in the statistical literature to the notion of Bayesian Experimental Design by Lindley (1956). In this paper, we refer to the Problems (1) and (2) as the problems of optimizing value of information.", "startOffset": 132, "endOffset": 244}, {"referenceID": 33, "context": "The notion of value of information is widely used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and is formalized, e.", "startOffset": 50, "endOffset": 109}, {"referenceID": 18, "context": "The notion of value of information is widely used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and is formalized, e.", "startOffset": 50, "endOffset": 109}, {"referenceID": 37, "context": ", the references by Papadimitriou (1995) or Littman, Goldsmith, and Mundhenk (1998).", "startOffset": 20, "endOffset": 41}, {"referenceID": 37, "context": ", the references by Papadimitriou (1995) or Littman, Goldsmith, and Mundhenk (1998). The class NP contains decision problems which have polynomial-time verifiable proofs.", "startOffset": 20, "endOffset": 84}, {"referenceID": 34, "context": "NP has been introduced and found to be a natural class for modeling AI planning problems in the seminal work by Littman et al. (1998). As an example, the MAP assignment problem is NP-complete for general graphical models, as shown by Park and Darwiche (2004).", "startOffset": 112, "endOffset": 134}, {"referenceID": 34, "context": "NP has been introduced and found to be a natural class for modeling AI planning problems in the seminal work by Littman et al. (1998). As an example, the MAP assignment problem is NP-complete for general graphical models, as shown by Park and Darwiche (2004). The complexity classes satisfy the following set of inclusions (where the inclusions are assumed, but not known to be strict):", "startOffset": 112, "endOffset": 259}, {"referenceID": 34, "context": "The reductions are inspired by the works of Littman et al. (1998) and Park and Darwiche (2004), but require the development of novel techniques, such as new reductions of Boolean formulae to Naive Bayes and polytree graphical models associated with appropriate reward functions, ensuring that observation selections lead to feasible assignments to the Boolean formulae.", "startOffset": 44, "endOffset": 66}, {"referenceID": 34, "context": "The reductions are inspired by the works of Littman et al. (1998) and Park and Darwiche (2004), but require the development of novel techniques, such as new reductions of Boolean formulae to Naive Bayes and polytree graphical models associated with appropriate reward functions, ensuring that observation selections lead to feasible assignments to the Boolean formulae.", "startOffset": 44, "endOffset": 95}, {"referenceID": 11, "context": "1 Temperature Time Series The first data set consists of temperature time series collected from a sensor network deployed at Intel Research Berkeley (Deshpande et al., 2004) as described in our running example.", "startOffset": 149, "endOffset": 173}, {"referenceID": 9, "context": "3 Part-of-Speech Tagging In our third experiment, we investigated the structured classification task of part-of-speech (POS) tagging (CoNLL, 2003).", "startOffset": 133, "endOffset": 146}, {"referenceID": 11, "context": "This is a very natural assumption for sensor networks (Deshpande et al., 2004).", "startOffset": 54, "endOffset": 78}, {"referenceID": 26, "context": "This increase in complexity can be avoided using a sampling approximation: Hoeffding\u2019s inequality can be used to derive polynomial bounds on sample complexity for approximating the value of information up to arbitrarily small additive error \u03b5, similarly as done in the approach of Krause and Guestrin (2005a)4.", "startOffset": 281, "endOffset": 309}, {"referenceID": 6, "context": "Approximate inference algorithms such as the algorithm proposed by Boyen and Koller (1998) provide a viable way around this problem.", "startOffset": 67, "endOffset": 91}, {"referenceID": 47, "context": "2 Proof of Concept Study on Real Deployment In the work by Singhvi et al. (2005), we presented an approach for optimizing light control in buildings, with the purpose of satisfying building occupants\u2019 preferences about lighting conditions, and simultaneously minimizing energy consumption.", "startOffset": 59, "endOffset": 81}, {"referenceID": 33, "context": "In its general form, Bayesian experimental design was pioneered by Lindley (1956). The users encode their preferences in a utility function U(P (\u0398), \u03b8?), where the first argument, P (\u0398), is a distribution over states of the world (i.", "startOffset": 67, "endOffset": 82}, {"referenceID": 33, "context": "2 Value of Information in Graphical Models Decision-theoretic value of information has been frequently used for principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984).", "startOffset": 145, "endOffset": 204}, {"referenceID": 18, "context": "2 Value of Information in Graphical Models Decision-theoretic value of information has been frequently used for principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984).", "startOffset": 145, "endOffset": 204}, {"referenceID": 44, "context": "Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.", "startOffset": 20, "endOffset": 118}, {"referenceID": 22, "context": "Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.", "startOffset": 20, "endOffset": 118}, {"referenceID": 17, "context": ", Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984). In a sense, value of information problems are special cases of Bayesian experimental design problems, where the prior distribution has a particular structure, typically given by a graphical model as considered in this paper. Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches for selectively gathering evidence in graphical models, as considered in this paper, which, unlike the algorithms presented in this paper. While these algorithms are applicable to much more general graphical models, they do not have theoretical guarantees. Heckerman et al. (1993) propose a method to compute the maximum expected utility for specific sets of observations.", "startOffset": 31, "endOffset": 824}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models. In contrast to the algorithms described in this paper however, their approach has running time that is worst-case exponential. Munie and Shoham (2008) present algorithms and hardness results for optimizing a special class of value of information objective functions that are motivated by optimal educational testing problems.", "startOffset": 0, "endOffset": 298}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models. In contrast to the algorithms described in this paper however, their approach has running time that is worst-case exponential. Munie and Shoham (2008) present algorithms and hardness results for optimizing a special class of value of information objective functions that are motivated by optimal educational testing problems. Their algorithms apply to a different class of graphical models than chains, and only apply for specific objective functions, rather than general local reward functions as considered in this paper. Radovilsky, Shattah, and Shimony (2006) extended the previous version of our paper (Krause & Guestrin, 2005a) to obtain approximation algorithms with guarantees in the case of noisy observations (i.", "startOffset": 0, "endOffset": 711}, {"referenceID": 43, "context": "In the classical k-armed bandit problem, as formalized by Robbins (1952), a slot machine is given", "startOffset": 58, "endOffset": 73}, {"referenceID": 16, "context": "A celebrated result by Gittins and Jones (1979) shows that for a fixed number of draws, an optimal strategy can be computed in polynomial time, using a dynamic programming based algorithm.", "startOffset": 23, "endOffset": 48}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information.", "startOffset": 0, "endOffset": 500}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem.", "startOffset": 0, "endOffset": 986}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1275}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1295}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1321}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM). None of these approaches however provide guarantees about the quality of the obtained solutions. Singh, Krause, Guestrin, Kaiser, and Batalin (2007) present an approximation algorithm with theoretical guarantees for the problem of planning an informative path for environmental monitoring using Gaussian Process models.", "startOffset": 0, "endOffset": 1585}, {"referenceID": 48, "context": "The problem of deciding when to selectively turn on sensors in order to conserve power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin, and Reich (2002).", "startOffset": 110, "endOffset": 143}, {"referenceID": 48, "context": "The problem of deciding when to selectively turn on sensors in order to conserve power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin, and Reich (2002). Typically, it is assumed that sensors are associated with a fixed sensing region, and a spatial", "startOffset": 110, "endOffset": 176}, {"referenceID": 54, "context": "Zhao et al. (2002) proposed heuristics for selectively querying nodes in a sensor network in order to reduce the entropy of the prediction.", "startOffset": 0, "endOffset": 19}, {"referenceID": 41, "context": "6 Relationship to Machine Learning Decision Trees (Quinlan, 1986) popularized the value of information as a criterion for creating conditional plans.", "startOffset": 50, "endOffset": 65}, {"referenceID": 30, "context": "The authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec, Guestrin, VanBriesen, & Faloutsos, 2008) that a large number of practical objective functions satisfy an intuitive diminishing returns property: Adding a new observation helps more if we have few observations so far, and less if we have already made many observations.", "startOffset": 19, "endOffset": 125}, {"referenceID": 27, "context": "7 Previous Work by the Authors A previous version of this paper appeared in the work by Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work by Singhvi et al.", "startOffset": 88, "endOffset": 116}, {"referenceID": 27, "context": "7 Previous Work by the Authors A previous version of this paper appeared in the work by Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work by Singhvi et al. (2005). The present version is much extended, with new algorithmic and hardness results and more detailed discussions.", "startOffset": 88, "endOffset": 205}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al.", "startOffset": 18, "endOffset": 46}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al. (2006) and Krause et al.", "startOffset": 18, "endOffset": 72}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al. (2006) and Krause et al. (2007) show that this is the case for interesting classes of value of information problems.", "startOffset": 18, "endOffset": 97}], "year": 2009, "abstractText": "Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan. Furthermore we prove a surprising result: In most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as HMMs, this procedure can be generalized to polytree graphical models. We prove that the optimizing value of information is NP-hard even for polytrees. It also follows from our results that just computing decision theoretic value of information objective functions, which are commonly used in practice, is a #P-complete problem even on Naive Bayes models (a simple special case of polytrees). In addition, we consider several extensions, such as using our algorithms for scheduling observation selection for multiple sensors. We demonstrate the effectiveness of our approach on several real-world datasets, including a prototype sensor network deployment for energy conservation in buildings.", "creator": "TeX"}}}