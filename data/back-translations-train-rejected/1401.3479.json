{"id": "1401.3479", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments", "abstract": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.", "histories": [["v1", "Wed, 15 Jan 2014 05:33:57 GMT  (406kb)", "http://arxiv.org/abs/1401.3479v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["yllias chali", "shafiq rayhan joty", "sadid a hasan"], "accepted": false, "id": "1401.3479"}, "pdf": {"name": "1401.3479.pdf", "metadata": {"source": "CRF", "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments", "authors": ["Yllias Chali", "Shafiq R. Joty", "Sadid A. Hasan"], "emails": ["chali@cs.uleth.ca", "rjoty@cs.ubc.ca", "hasan@cs.uleth.ca"], "sections": [{"heading": "1. Introduction", "text": "The huge increase in online texts and the demand for access to different types of information have led to a renewed interest in a wide range of information that goes beyond just answering questions, including questions and answers that are asked in ordinary human language - perhaps the most exciting technological development of the last six or seven years (Strzalkowski & Harabagiu, etc.) that focuses on answering questions posed by users themselves; the question of the reasons and models of the past six or seven years (Strzalkowski & Harabagiu, etc.) that can be answered by answering questions (QA) - the ability of a machine to answer questions, simple or complex questions asked in ordinary human language - is perhaps the most exciting technological development of the past six or seven years (Strzalkowski & Harabagiu)."}, {"heading": "2. Related Work", "text": "There are a number of sentence retrieval systems based on IR (information retrieval) techniques and show that these systems typically do not use much linguistic information, but they still deserve special attention. Murdock and Croft (2005) propose a translation model specifically for monolingual data and show that it significantly improves sentence query on quantum probability. Translation models form a corpus of question / answer pairs. Losada (2005) presents a comparison between multiple Bernoulli models and multinomial models in the context of a sentence retrival task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for the recovery of relevant sentences."}, {"heading": "3. Our Approach", "text": "To accomplish the task of answering complex questions, we extract several important characteristics for each sentence in the document collection to measure its relevance to the query; the sentences in the document collection are analyzed at different levels and each of the document sets is presented as a vector of characteristic values; our feature set includes lexical, lexical semantics, statistical similarity, syntactic and semantic characteristics, and graph-based similarity measures (Chali & Joty, 2008b); we have reimplemented many of these characteristics, which are successfully applied to many related areas of the NLP; we use a simple local search method to refine the characteristic weights; we also use the statistical cluster algorithms: EM and K means to select the relevant sentences for summary generation; and experimental results show that our systems function better when we include tree kernel-based syntactical and syntactical characteristics, though only tactical."}, {"heading": "4. Feature Extraction", "text": "In this section, we will describe the characteristics used to evaluate the sets. We will provide detailed examples4 to show how to obtain the characteristic values. First, we will describe the syntactic and semantic characteristics that we present in this thesis, followed by a detailed description of the characteristics that are more commonly used in answering and summarizing questions.4 All the query and document sets used in the examples come from the DUC 2007 collection."}, {"heading": "4.1 Syntactic and Shallow Semantic Features", "text": "The importance of syntactic and semantic features is described in this context by Zhang and Lee (2003a), Moschitti et al. (2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b). One effective way to integrate syntactic and semantic structures into machine learning algorithms is the use of tree core functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008), which has been successfully applied to challenge classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006)."}, {"heading": "4.1.1 Encoding Syntactic Structures", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "4.1.2 Semantic Features", "text": "This year is the highest in the history of the country."}, {"heading": "4.2 Lexical Features", "text": "At this point, we will discuss the lexical features that are most commonly used in the QS and summary communities."}, {"heading": "4.2.1 N-gram Overlap", "text": "To create the query (or sentence) pool, we took the query (or document) sentence and created a series of related sentences by replacing its substantive words with their first-sense synonyms. \"John compose a poem,\" \"John writes a verse form\" along with the given sentence. \"We measured the n-gram scores based on the recall for a sentence P using the following formula: NgramScore (P) = maxi (maxj Ngram (si, qj)).\" Ngram (S, Q) = the grammar scores (gramn). \"\" Where n stands for the length of the n sentence (s). \""}, {"heading": "4.2.2 LCS and WLCS", "text": "A sequence W = [w1, w2,..., wn] is a sub-sequence of another sequence Q = Q (Q) Q (Q) (Q) (Q = [x1, x2,..., xm], if there is a strict increasing sequence [i1, i2,..., in] the indexes of X, so that for all j = 1, 2,..., n we have xij = wj (Cormen, Leiserson, & Rivest, 1989). In view of two sequences S1 and S2, the longest common subsequence (LCS) of S1 and S2 is a common maximum-length subsequence (Lin, 2004). The longer the LCS is of two sentences, the more similar are the two sentences. We used the F measure based on LCS to estimate the similarity between the document set S of length m and the query set Q of length m."}, {"heading": "S1 John shot the thief", "text": "In his opinion, it is very possible that we will be able to find a solution to this problem."}, {"heading": "4.2.3 Skip-Bigram Measure", "text": "A Skip-Bigram measures the overlap of Skip-Bigram between a candidate record and a query record (Lin, 2004). As before WordNet, we rely on the query pool and the record pool. The following sentences are taken into account:"}, {"heading": "S1 John shot the thief", "text": "S2 John shoot the thiefS3 the diief shoot JohnS4 the diief John shotwe get that each set has C (4,2) = 6 skip-bigrams8. For example, S1 has the following skip-bigrams: (\"John shot,\" \"John the,\" \"John thief,\" \"John thief,\" \"shot the,\" \"the thief,\" \"the thief,\" \"the thief,\" \"the thief,\" \"the thief,\" \"the thief,\" \"the skip bi-gram match with S1 (\" the thief \"),\" the thief bi-gram match with S1 (\"the thief\"), and S4 has two skip-gram matches with S1 (\"John shot,\" the thief, \"the thief\").The skip bi-gram score between the document set S of length m and the query set n of length n can be calculated as follows: Rskip 2 (S, Q)."}, {"heading": "4.2.4 Head and Head Related-words Overlap", "text": "To extract the heads from the sentence (or query), the sentence (or query) of Minipar9 is analyzed, and from the dependency tree we extract the heads that we call exact headers and form a series of words that we call header words. We measured the exact head note and head-related rating as follows: ExactHeadScore = w1 HeadSetCountmatch (w1) and the sentence header words and build a series of words that we call header words. We measured the exact head note and head-related rating as follows: ExactHeadScore = w1 HeadSetCountmatch (w1) HeadSetCount (w1) (19) HeadRelatedScore = Score, w1 HeadRelSet Countmatch (w1) and w1 HeadRelSet Countmatch (w1)."}, {"heading": "4.3 Lexical Semantic Features", "text": "We form a series of words, which we call QueryRelatedWords, taking the content words of the query, their synonyms in the first sense, the hypernyms / hyponyms of the nouns and the glossy definitions of the nouns using WordNet."}, {"heading": "4.3.1 Synonym Overlap", "text": "The synonym overlap measure is the overlap between the list of synonyms of the content words extracted from the candidate record and the query-related words. This can be calculated as follows: Synonym Overlap Score = \u2211 w1 \u0445 SynSetCountmatch (w1) \u0445 w1 \u0445 SynSetCount (w1) (21) Where SynSet is the synonym set of the content words in the sentence and Countmatch is the number of matches between the SynSet and query-related words."}, {"heading": "4.3.2 Hypernym/Hyponym Overlap", "text": "The hypernym / hyponym overlap measure is the overlap between the list of hypernymes (level 2) and hyponymes (level 3) of nouns and query-related words extracted from the sentence in question. This overlap can be calculated as follows: Hypernym / hyponym overlap value = \u2211 h1: HypSetCountmatch (h1) \u2211 h1: HypSetCount (h1) (22) Where HypSet is the hyponym / hyponym set of nouns in the sentence and Countmatch is the number of matches between HypSet and query-related words."}, {"heading": "4.3.3 Gloss Overlap", "text": "The overlap measurement is the overlap between the list of substantive words extracted from the glossy definition of nouns in the sentence under consideration and the query-related words. This can be calculated as follows: GlossSet is the amount of substantive words (i.e. nouns, verbs and adjectives) extracted from the glossy definition of nouns in the sentence, and Countmatch is the number of overlaps between the glossset and query related words. Example: In view of the query, the following sentence receives a synonym overlap value of 0.33333, hypernym / hyponym overlap value of 0.1860465, and gloss overlap value of 0.1359223.Query steps and worldwide reactions prior to the introduction of the euro on 1 January 1999. Consider predictions and expectations in the European overlap report of 330.The overlap of cultures:"}, {"heading": "4.4 Statistical Similarity Measures", "text": "In fact, it is so that it is a way in which people are able to put themselves in the world, in which they are able to understand the world, and in which people are able to understand the world and understand what they are doing. (...) It is as if people are able to understand the world. (...) It is as if people were able to understand the world of the world. (...) It is as if they were able to understand the world of the world of the world, the world of the world of the world, the world of the world of the world, the world of the world, the world of the world, the world, the world of the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world of the world, the world, the world, the world, the world, the world of the world, the world, the world of the world, the world, the world, the world of the world, the world, the world, the world of the world, the world, the world, the world, the world of the world, the world, the world, the world, the world of the world of the world, the world, the world, the world, the world of the world, the world of the world, the world, the world of the world, the world, the world of the"}, {"heading": "4.5 Graph-based Similarity Measure", "text": "Erkan and Radev (2004) used the concept of graph-based centrality to capture a series of sentences for creation of generic multiple document summaries. (D) We followed a similar approach to calculate this characteristic. (D) We believe that the number of sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D (D), the sentences (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D (D), the sentences (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D, the sentences (D), the sentences (D), the sentences (D (D), the sentences (D), the sentences (D (D), the sentences (D (D), the sentences (D), the sentences (D (D), the sentences (D), the sentences (D), the sentences (D, D, the sentences (D), D (D, D (D), the sentences (D, D), the sentences (D), the sentences (D,"}, {"heading": "5. Ranking Sentences", "text": "We use various methods to classify sentences, to create summaries that apply the features described in Section 4. In this section we will describe the systems in detail."}, {"heading": "5.1 Learning Feature-weights: A Local Search Strategy", "text": "To refine the weights of the characteristics, we have applied a local search technique. First, we set all characteristic weights, w1, \u00b7 \u00b7 \u00b7, wn, as equal values (i.e. 0.5) (see algorithm 1). Then we train the weights using the DUC 2006 dataset. Based on the current weights, we evaluate the sentences and generate summaries accordingly. We evaluate the summaries using input: step l, initial weight v output: A vector ~ w of the learned weights Initialize the weights wi to v. for i \u2190 1 to n dorg1 = rg2 = prev 0 while (true) doscoreSentences (~ w) generateSummaries () rg2 = evaluateROUGE (), if rg1 \u2264 rg2 thenprev = wi + = l rg2else break ending in return ~ wAlgorithm: Tuning weights of the local search we change the ROJ value using the GE automatic system (the GE)."}, {"heading": "5.2 Statistical Machine Learning Approaches", "text": "We experimented with two unattended statistical learning techniques with the traits extracted in the previous section for the sentence selection problem: 1. K-means learning 2. Expectation maximization (EM) learning"}, {"heading": "5.2.1 The K-means Learning", "text": "K-Means is a hard cluster algorithm that defines clusters by the center of their members. We start with a series of initial cluster centers that are randomly selected and by multiple iterations of assigning each object to the cluster whose center is closest. After all objects have been assigned, we calculate the center of each cluster as the center or mean (\u00b5) of its members. The distance function we use is square Euclidean distance instead of true Euclidean distance. Since the square root is a monotonously growing function, the square Euclidean distance has the same result as the true Euclidean distance, but the computational overload is smaller when the square root declines. Once we have learned the means of the clusters using the K-Mean algorithm, our next task is to arrange the sets according to a quadratic mathematical model. We have used the Bayesian model x-d to do this."}, {"heading": "5.2.2 The EM Learning", "text": "A useful result of this model is that it generates a probability value of the cluster model and the probability values can be used to select the best model from a number of different models, provided that they have the same number of parameters (i.e. the same number of clusters). Input: A sample of n data points (x) is represented by a characteristic vector of length L. Input: Number of clusters K Output: An array S of K-mean-based scores Data: Array dnK, Number of K-data Data: Array CK, ynK Randomly select K-points as initial means L Input: Number of K-values."}, {"heading": "6. Redundancy Checking and Generating Summary", "text": "In this case, we ignore other factors: such as redundancy and coherence. As we know, the summary of the text clearly includes the selection of the most important information and the compilation into a coherent summary. The response or summary consists of several separately extracted sentences from different documents. Obviously, each of the selected snippets of text should be important individually. However, when many of the competing sentences are included in the summary, the information overlaps between the parts of the output and a mechanism to address the redundancy is required. Therefore, our summary systems use two levels of analysis: first, a level of content at which each sentence is evaluated according to the characteristics or concepts it covers, and second, a textual level when, before it is added to the final output, the sentences considered important are compared."}, {"heading": "7. Experimental Evaluation", "text": "(This section describes the results of the experiments with DUC12 2007 dataset of NIST 13. Some of the questions that these experiments include: \u2022 How do the different characteristics affect the behavior of the summarer system? \u2022 Which of the algorithms (K-Means, EM and Local Search) is better for synthesizing this particular problem? We used the main task of DUC 2007 for evaluation. The task was: \"Given a complex question (topic description) and a collection of relevant documents, the task is to provide a fluent, well-organized 250-word summary of the documents that answers the question (s) in the topic.\" The documents of DUC 2007 came from the AQUAINT Corpus consisting of newswire articles and New York Times (1998-2000) and Xinhua News Agency (1996-2000)."}, {"heading": "7.1 Automatic Evaluation", "text": "It is a set of measures that determine the quality of a summary created by humans. Measures include the number of overlapping units such as n-gram, word sequences and pairs of words between the systemic summary that needs to be evaluated and the ideal summaries created by humans. Available measures include the number of overlapping units such as n-gram, word sequences and pairs that need to be evaluated, and the ideal summaries created by humans. Available measures are: ROUGE-N = 1,2,3,4), ROUGE-L and ROUGE-S. ROUGE-S is n-gram reminiscent of a candidate and a set of reference totals. ROUGE-L measures the longest common subsequence (LCS)."}, {"heading": "7.1.1 Results and Discussion", "text": "It is worth mentioning that the K-remedies are most suitable for the basic idea. It is also worth mentioning that the K-remedies are most suitable for the basic idea."}, {"heading": "7.1.2 Comparison", "text": "The above results show that our systems perform significantly better than the base system on all three algorithms. Table 13 shows the F-values of the reported ROUGE measurements, while Table 14 indicates the 95% confidence intervals for the base system, the best system in DUC 2007, and our three techniques taking into account all characteristics (ALL). We can see that the local search method outperforms the other two, and the EM algorithm performs better than the K-mean algorithm. If we analyze thoroughly, we find that the confidence intervals with the exception of ROUGE-SU do not overlap with the best DUC 2007 system on a local search."}, {"heading": "7.2 Manual Evaluation", "text": "For a sample of 105 summaries 14 from the summaries of our various systems, we perform an extensive manual evaluation to analyze the effectiveness of our approaches; the manual evaluation included a pyramid-based evaluation of content and a user evaluation to assess linguistic quality and overall responsiveness."}, {"heading": "7.2.1 Pyramid Evaluation", "text": "In the main task of the DUC 2007, 23 topics were selected for the optional community-based pyramid evaluation. Volunteers from 16 different locations created pyramids and commented on the peer summaries for the main task of the DUC using the given guidelines 15. 8 pages below them created the pyramids. We used these pyramids to comment on our peer summaries to calculate the modified pyramid values 16. To do this, we used the annotation tool DUCView.jar17. Table 15 to Table 17 shows the modified pyramid values of all our systems for the three algorithms. Also, the score of a baseline system is reported. Peer summaries of the baseline system are generated by returning all the leading sentences (up to 250 words) in the < TEXT > field of the latest document (s). These results show that all of our systems perform better than the baseline system and the inclusion of syntactical and better sectorial characteristics."}, {"heading": "7.2.2 User Evaluation", "text": "The score given is an integer between 1 (very bad) and 5 (very good) and is determined by taking into account the following factors: 1. Grammar, 2. Redundancy, 3. Clarity of Reference, 4. Focus and 5. Structure and Coherence. They also assigned a Content Responsibility Score to each of the automatic summaries. Content Score is an integer between 1 (very bad) and 5 (very good) and is based on the amount of information in the summary that helps satisfy the information needs expressed in the topic telling. These metrics were used in DUC 2007. Table 18 to Table 20 shows the average linguistic quality and general responsive scores of all our systems for the three algorithms. Values of the same base system are given for a meaningful comparison."}, {"heading": "8. Conclusion and Future Work", "text": "In this paper, we presented our work in answering complex questions."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their useful comments on the earliest version of this essay. Special thanks go to our colleagues for proofreading the essay, as well as all PhD students who participated in the user evaluation, and the research reported here has been supported by the Natural Sciences and Engineering Research Council (NSERC) research fellowship and the University of Lethbridge."}, {"heading": "Appendix A. Stop Word List", "text": "Dre rf\u00fc ide eeisrteeSrteeeeeeerrrrrr rf\u00fc ide eeisrrllteeeeeoiuiueeaeBnlrrsrteeeeeteeVrrrrrrrrrrrteeeeeVnlrrrrrrrrrteeeVrrrrrrrrrrrrrrrsrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}], "references": [{"title": "Combined syntactic and semantic kernels for text classification", "author": ["S. Bloehdorn", "A. Moschitti"], "venue": "In 29th European Conference on IR Research,", "citeRegEx": "Bloehdorn and Moschitti,? \\Q2007\\E", "shortCiteRegEx": "Bloehdorn and Moschitti", "year": 2007}, {"title": "Structure and semantics for expressive text kernels", "author": ["S. Bloehdorn", "A. Moschitti"], "venue": "In CIKM-2007,", "citeRegEx": "Bloehdorn and Moschitti,? \\Q2007\\E", "shortCiteRegEx": "Bloehdorn and Moschitti", "year": 2007}, {"title": "Improving the performance of the random walk model for answering complex questions", "author": ["Y. Chali", "S.R. Joty"], "venue": "In Proceedings of the 46th Annual Meeting of the ACL-HLT. Short Paper Section,", "citeRegEx": "Chali and Joty,? \\Q2008\\E", "shortCiteRegEx": "Chali and Joty", "year": 2008}, {"title": "Selecting sentences for answering complex questions", "author": ["Y. Chali", "S.R. Joty"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Chali and Joty,? \\Q2008\\E", "shortCiteRegEx": "Chali and Joty", "year": 2008}, {"title": "A Maximum-Entropy-Inspired Parser", "author": ["E. Charniak"], "venue": "Technical Report CS-99-12", "citeRegEx": "Charniak,? \\Q1999\\E", "shortCiteRegEx": "Charniak", "year": 1999}, {"title": "Convolution Kernels for Natural Language", "author": ["M. Collins", "N. Duffy"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Collins and Duffy,? \\Q2001\\E", "shortCiteRegEx": "Collins and Duffy", "year": 2001}, {"title": "Introduction to Algorithms", "author": ["T.R. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1989}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "Summarizing Text Documents: Sentence Selection and Evaluation Metrics", "author": ["J. Goldstein", "M. Kantrowitz", "V. Mittal", "J. Carbonell"], "venue": "In Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval,", "citeRegEx": "Goldstein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 1999}, {"title": "A New Multi-document Summarization System", "author": ["Y. Guo", "G. Stylios"], "venue": "In Proceedings of the Document Understanding Conference. NIST", "citeRegEx": "Guo and Stylios,? \\Q2003\\E", "shortCiteRegEx": "Guo and Stylios", "year": 2003}, {"title": "Shallow Semantic Parsing Using Support Vector Machines", "author": ["K. Hacioglu", "S. Pradhan", "W. Ward", "J.H. Martin", "D. Jurafsky"], "venue": "Technical Report TR-CSLR2003-03 University of Colorado", "citeRegEx": "Hacioglu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hacioglu et al\\.", "year": 2003}, {"title": "Answering complex questions with random walk models", "author": ["S. Harabagiu", "F. Lacatusu", "A. Hickl"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Dependency-based sentence alignment for multiple document summarization", "author": ["T. Hirao", "J. Suzuki", "H. Isozaki", "E. Maeda"], "venue": "In Proceedings of Coling", "citeRegEx": "Hirao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hirao et al\\.", "year": 2004}, {"title": "Automated Summarization Evaluation with Basic Elements", "author": ["E. Hovy", "C.Y. Lin", "L. Zhou", "J. Fukumoto"], "venue": "In Proceedings of the Fifth Conference on Language Resources and Evaluation Genoa, Italy", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "From Treebank to PropBank", "author": ["P. Kingsbury", "M. Palmer"], "venue": "In Proceedings of the international conference on Language Resources and Evaluation Las Palmas, Spain", "citeRegEx": "Kingsbury and Palmer,? \\Q2002\\E", "shortCiteRegEx": "Kingsbury and Palmer", "year": 2002}, {"title": "Recognizing textual entailment with tree edit distance algorithms. In Proceedings of the PASCAL Challenges Workshop: Recognising Textual Entailment Challenge", "author": ["M. Kouylekov", "B. Magnini"], "venue": null, "citeRegEx": "Kouylekov and Magnini,? \\Q2005\\E", "shortCiteRegEx": "Kouylekov and Magnini", "year": 2005}, {"title": "A Query-Focused Multi-Document Summarizer Based on Lexical Chains", "author": ["J. Li", "L. Sun", "C. Kit", "J. Webster"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C.Y. Lin"], "venue": "In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of Association for Computational Linguistics,", "citeRegEx": "Lin,? \\Q2004\\E", "shortCiteRegEx": "Lin", "year": 2004}, {"title": "An Information-Theoretic Definition of Similarity", "author": ["D. Lin"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Automatic Retrieval and Clustering of Similar Words", "author": ["D. Lin"], "venue": "In Proceedings of the International Conference on Computational Linguistics and Association for Computational Linguistics,", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Language modeling for sentence retrieval: A comparison between multiple-bernoulli models and multinomial models", "author": ["D. Losada"], "venue": "In Information Retrieval and Theory Workshop", "citeRegEx": "Losada,? \\Q2005\\E", "shortCiteRegEx": "Losada", "year": 2005}, {"title": "Highly frequent terms and sentence retrieval", "author": ["D. Losada", "R.T. Fern\u00e1ndez"], "venue": "In Proc. 14th String Processing and Information Retrieval Symposium,", "citeRegEx": "Losada and Fern\u00e1ndez,? \\Q2007\\E", "shortCiteRegEx": "Losada and Fern\u00e1ndez", "year": 2007}, {"title": "Learning to recognize features of valid textual entailments", "author": ["B. MacCartney", "T. Grenager", "M. de Marneffe", "D. Cer", "C.D. Manning"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,", "citeRegEx": "MacCartney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2006}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of structure of text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Morris and Hirst,? \\Q1991\\E", "shortCiteRegEx": "Morris and Hirst", "year": 1991}, {"title": "Efficient convolution kernels for dependency and constituent syntactic trees", "author": ["A. Moschitti"], "venue": "In Proceedings of the 17th European Conference on Machine Learning Berlin, Germany", "citeRegEx": "Moschitti,? \\Q2006\\E", "shortCiteRegEx": "Moschitti", "year": 2006}, {"title": "A Tree Kernel approach to Question and Answer Classification in Question Answering Systems", "author": ["A. Moschitti", "R. Basili"], "venue": "In Proceedings of the 5th international conference on Language Resources and Evaluation Genoa, Italy", "citeRegEx": "Moschitti and Basili,? \\Q2006\\E", "shortCiteRegEx": "Moschitti and Basili", "year": 2006}, {"title": "Kernels on linguistic structures for answer extraction. In Proceedings of the 46th Conference of the Association for Computational Linguistics (ACL\u201908)", "author": ["A. Moschitti", "S. Quarteroni"], "venue": null, "citeRegEx": "Moschitti and Quarteroni,? \\Q2008\\E", "shortCiteRegEx": "Moschitti and Quarteroni", "year": 2008}, {"title": "Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion", "author": ["A. Moschitti", "S. Quarteroni", "R. Basili", "S. Manandhar"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Moschitti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moschitti et al\\.", "year": 2007}, {"title": "A translation model for sentence retrieval", "author": ["V. Murdock", "W.B. Croft"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Murdock and Croft,? \\Q2005\\E", "shortCiteRegEx": "Murdock and Croft", "year": 2005}, {"title": "Using Random Walks for Questionfocused Sentence Retrieval", "author": ["J. Otterbacher", "G. Erkan", "D.R. Radev"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Otterbacher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "Mapping dependencies trees: An application to question answering", "author": ["V. Punyakanok", "D. Roth", "W. Yih"], "venue": "In Proceedings of AI & Math Florida, USA", "citeRegEx": "Punyakanok et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Advances in Open Domain Question Answering", "author": ["T. Strzalkowski", "S. Harabagiu"], "venue": null, "citeRegEx": "Strzalkowski and Harabagiu,? \\Q2008\\E", "shortCiteRegEx": "Strzalkowski and Harabagiu", "year": 2008}, {"title": "The pythy summarization system: Microsoft research at duc", "author": ["K. Toutanova", "C. Brockett", "M. Gamon", "J. Jagarlamudi", "H. Suzuki", "L. Vanderwende"], "venue": "In proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Toutanova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2007}, {"title": "Microsoft Research at DUC2006: Task-Focused Summarization with Sentence Simplification and Lexical Expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Vanderwende et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2006}, {"title": "Sentence Compression as a Component of a Multi-Document Summarization System", "author": ["D.M. Zajic", "J. Lin", "B.J. Dorr", "R. Schwartz"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Zajic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2006}, {"title": "Question Classification using Support Vector Machines", "author": ["A. Zhang", "W. Lee"], "venue": "In Proceedings of the Special Interest Group on Information Retrieval,", "citeRegEx": "Zhang and Lee,? \\Q2003\\E", "shortCiteRegEx": "Zhang and Lee", "year": 2003}, {"title": "A Language Modeling Approach to Passage Question Answering", "author": ["D. Zhang", "W.S. Lee"], "venue": "In Proceedings of the Twelfth Text REtreival Conference,", "citeRegEx": "Zhang and Lee,? \\Q2003\\E", "shortCiteRegEx": "Zhang and Lee", "year": 2003}, {"title": "A BE-based Multi-dccument Summarizer with Query Interpretation", "author": ["L. Zhou", "C.Y. Lin", "E. Hovy"], "venue": "In Proceedings of Document Understanding Conference", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Murdock and Croft (2005) propose a translation model specifically for monolingual data, and show that it significantly improves sentence retrieval over query likelihood.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences. Losada and Fern\u00e1ndez (2007) propose a novel sentence retrieval method based on extracting highly frequent terms from top retrieved documents.", "startOffset": 0, "endOffset": 295}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization.", "startOffset": 32, "endOffset": 55}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences.", "startOffset": 32, "endOffset": 194}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences. The system then ranks the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question. Concepts of coherence and cohesion enable us to capture the theme of the text. Coherence represents the overall structure of a multi-sentence text in terms of macro-level relations between clauses or sentences (Halliday & Hasan, 1976). Cohesion, as defined by Halliday and Hasan (1976), is the property of holding text together as one single grammat-", "startOffset": 32, "endOffset": 882}, {"referenceID": 15, "context": "For example, Li et al. (2007) uses the following formula: Score = \u03b1P (chain) + \u03b2P (query) + \u03b3P (namedEntity) where P (chain) is the sum of the scores of the chains whose words come from the candidate sentence, P (query) is the sum of the co-occurrences of key words in a topic and the sentence, and P (namedEntity) is the number of name entities existing in both the topic and the sentence.", "startOffset": 13, "endOffset": 30}, {"referenceID": 11, "context": "Harabagiu et al. (2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques.", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "Harabagiu et al. (2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques. The question decomposition procedure operates on a Markov chain. That is, by following a random walk with a mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions. There are approaches that are based on probabilistic models (Pingali, K., & Varma, 2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingali et al. (2007) rank the sentences based on a mixture model where each component of the model is a statistical model:", "startOffset": 0, "endOffset": 1068}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency.", "startOffset": 0, "endOffset": 24}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information.", "startOffset": 0, "endOffset": 522}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information. Toutanova et al. (2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 0, "endOffset": 641}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information. Toutanova et al. (2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 0, "endOffset": 683}, {"referenceID": 16, "context": "(2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 61, "endOffset": 92}, {"referenceID": 9, "context": "Guo and Stylios (2003) use verb arguments (i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Guo and Stylios (2003) use verb arguments (i.e. subjects, times, locations and actions) for clustering. For each sentence this method establishes the indices information based on the verb arguments (subject is first index, time is second, location is third and action is fourth). All the sentences that have the same or closest \u2018subjects\u2019 index are put in a cluster and they are sorted out according to the temporal sequence from the earliest to the latest. Sentences that have the same \u2018spaces/locations\u2019 index value in the cluster are then marked out. The clusters are ranked based on their sizes and top 10 clusters are chosen. Then, applying a cluster reduction module the system generates the compressed extract summaries. There are approaches in \u201cRecognizing Textual Entailment\u201d, \u201cSentence Alignment\u201d, and \u201cQuestion Answering\u201d that use syntactic and/or semantic information in order to measure the similarity between two textual units. This indeed motivated us to include syntactic and semantic features to get the structural similarity between a document sentence and a query sentence (discussed in Section 4.1). MacCartney, Grenager, de Marneffe, Cer, and Manning (2006) use typed dependency graphs (same as dependency trees) to represent the text and the hypothesis.", "startOffset": 0, "endOffset": 1179}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. Kouylekov and Magnini (2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment.", "startOffset": 0, "endOffset": 343}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. Kouylekov and Magnini (2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment. According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold. Punyakanok et al. (2004) represent the question and the sentence containing answer with their dependency trees.", "startOffset": 0, "endOffset": 735}, {"referenceID": 26, "context": "The importance of syntactic and semantic features in this context is described by Zhang and Lee (2003a), Moschitti et al.", "startOffset": 82, "endOffset": 104}, {"referenceID": 18, "context": "The importance of syntactic and semantic features in this context is described by Zhang and Lee (2003a), Moschitti et al. (2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 105, "endOffset": 129}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 8, "endOffset": 40}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b). An effective way to integrate syntactic and semantic structures in machine learning algorithms is the use of tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008) which has been successfully applied to question classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006).", "startOffset": 8, "endOffset": 105}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b). An effective way to integrate syntactic and semantic structures in machine learning algorithms is the use of tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008) which has been successfully applied to question classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006). Syntactic and semantic information are used effectively to measure the similarity between two textual units by MacCartney et al. (2006). To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences.", "startOffset": 8, "endOffset": 538}, {"referenceID": 12, "context": "1 Encoding Syntactic Structures Basic Element (BE) Overlap Measure Shallow syntactic information based on dependency relations was proved to be effective in finding similarity between two textual units (Hirao et al., 2004).", "startOffset": 202, "endOffset": 222}, {"referenceID": 13, "context": "We incorporate this information by using Basic Elements that are defined as follows (Hovy et al., 2006): \u2022 The head of a major syntactic constituent (noun, verb, adjective or adverbial phrases), expressed as a single item.", "startOffset": 84, "endOffset": 103}, {"referenceID": 13, "context": "The triples encode some syntactic information and one can decide whether any two units match or not- more easily than with longer units (Hovy et al., 2006).", "startOffset": 136, "endOffset": 155}, {"referenceID": 13, "context": "The triples encode some syntactic information and one can decide whether any two units match or not- more easily than with longer units (Hovy et al., 2006). We extracted BEs for the sentences (or query) by using the BE package distributed by ISI5. Once we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following Zhou, Lin, and Hovy (2005). Sorting BEs according to their LR scores produced a BE-ranked list.", "startOffset": 137, "endOffset": 371}, {"referenceID": 24, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999).", "startOffset": 175, "endOffset": 192}, {"referenceID": 27, "context": "The tree fragments of a tree are all of its sub-trees which include at least one production with the restriction that no production rules can be broken into incomplete parts (Moschitti et al., 2007).", "startOffset": 174, "endOffset": 198}, {"referenceID": 4, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999). Then we calculate the similarity between the two trees using the tree kernel.", "startOffset": 213, "endOffset": 229}, {"referenceID": 4, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999). Then we calculate the similarity between the two trees using the tree kernel. We reimplemented the tree kernel model as proposed by Moschitti et al. (2007). Once we build the trees, our next task is to measure the similarity between the trees.", "startOffset": 213, "endOffset": 386}, {"referenceID": 5, "context": "Because of this, Collins and Duffy (2001) define the tree kernel algorithm whose computational complexity does not depend on m.", "startOffset": 17, "endOffset": 42}, {"referenceID": 22, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007).", "startOffset": 157, "endOffset": 206}, {"referenceID": 27, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007).", "startOffset": 157, "endOffset": 206}, {"referenceID": 22, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury & Palmer, 2002) have made the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting an application of SRL to QA seems natural as pinpointing the answer to a question relies on a deep understanding of the semantics of both. For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences we first represent the annotated sentence (or query) using the tree structures like Figure 2 called Semantic Tree (ST) as proposed by Moschitti et al. (2007). In the semantic tree arguments are replaced with the most important word\u2013often referred to as the semantic head.", "startOffset": 158, "endOffset": 1138}, {"referenceID": 24, "context": "Moschitti et al. (2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows portions of an ST to match.", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "Moschitti et al. (2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows portions of an ST to match. Shallow Semantic Tree Kernel (SSTK) We reimplemented the SSTK according to the model given by Moschitti et al. (2007). The SSTK is based on two ideas: first, it changes", "startOffset": 0, "endOffset": 255}, {"referenceID": 27, "context": "empty) (Moschitti et al., 2007).", "startOffset": 7, "endOffset": 31}, {"referenceID": 17, "context": "Given two sequences S1 and S2, the longest common subsequence (LCS) of S1 and S2 is a common subsequence with maximum length (Lin, 2004).", "startOffset": 125, "endOffset": 136}, {"referenceID": 17, "context": "The basic LCS has a problem in that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004).", "startOffset": 132, "endOffset": 143}, {"referenceID": 17, "context": "Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated by Lin (2004). We use WLCS as it has the advantage of not measuring the similarity by taking the words in a higher dimension like string kernels which indeed reduces the time complexity.", "startOffset": 132, "endOffset": 143}, {"referenceID": 17, "context": "Skipbigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence (Lin, 2004).", "startOffset": 98, "endOffset": 109}, {"referenceID": 17, "context": "For example, if we set dskip to 0 then it is equivalent to bi-gram overlap measure (Lin, 2004).", "startOffset": 83, "endOffset": 94}, {"referenceID": 17, "context": "For example, if we set dskip to 0 then it is equivalent to bi-gram overlap measure (Lin, 2004). If we set dskip to 4 then only word pairs of at most 4 words apart can form skip bi-grams. In our experiment we set dskip = 4 in order to ponder at most 4 words apart to get the skip bi-grams. Modifying the equations: 15, 16, and 17 to allow the maximum skip distance limit is straightforward: following Lin (2004) we count the skip bi-gram matches, SKIP2(S,Q), within the maximum skip distance and replace the denominators of the equations with the actual numbers of within distance skip bi-grams from the reference sentence and the candidate sentence respectively.", "startOffset": 84, "endOffset": 411}, {"referenceID": 7, "context": "5 Graph-based Similarity Measure Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries.", "startOffset": 33, "endOffset": 56}, {"referenceID": 7, "context": "5 Graph-based Similarity Measure Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph each node represents a sentence. The edges between nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how important the sentence is. Figure 5 shows an example of a similarity graph for 4 sentences. Once the similarity graph is constructed, the sentences are ranked according to their eigenvector centrality. The LexRank performed well in the context of generic summarization. To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed by Otterbacher et al. (2005). We followed a similar approach in order to calculate this feature.", "startOffset": 33, "endOffset": 834}, {"referenceID": 29, "context": "For instance, if a sentence that gets a high score based on the question relevance model is likely to contain an answer to the question then a related sentence, which may not be similar to the question itself, is also likely to contain an answer (Otterbacher et al., 2005).", "startOffset": 246, "endOffset": 272}, {"referenceID": 17, "context": "the automatic evaluation tool ROUGE (Lin, 2004) (described in Section 7) and the ROUGE value works as the feedback to our learning loop.", "startOffset": 36, "endOffset": 47}, {"referenceID": 13, "context": "Following Hovy et al. (2006) we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary sentence.", "startOffset": 10, "endOffset": 29}, {"referenceID": 17, "context": "1 Automatic Evaluation ROUGE We carried out automatic evaluation of our summaries using the ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.", "startOffset": 98, "endOffset": 109}, {"referenceID": 17, "context": "Most of these ROUGE measures have been applied in automatic evaluation of summarization systems and achieved very promising results (Lin, 2004).", "startOffset": 132, "endOffset": 143}, {"referenceID": 32, "context": "Prior to that, we produced huge amount of labeled data automatically using similarity measures such as ROUGE (Toutanova et al., 2007).", "startOffset": 109, "endOffset": 133}], "year": 2009, "abstractText": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.", "creator": "dvips(k) 5.92b Copyright 2002 Radical Eye Software"}}}