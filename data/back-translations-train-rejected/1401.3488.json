{"id": "1401.3488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Content Modeling Using Latent Permutations", "abstract": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.", "histories": [["v1", "Wed, 15 Jan 2014 05:38:17 GMT  (319kb)", "http://arxiv.org/abs/1401.3488v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["harr chen", "s r k branavan", "regina barzilay", "david r karger"], "accepted": false, "id": "1401.3488"}, "pdf": {"name": "1401.3488.pdf", "metadata": {"source": "CRF", "title": "Content Modeling Using Latent Permutations", "authors": ["Harr Chen", "Regina Barzilay", "David R. Karger"], "emails": ["harr@csail.mit.edu", "branavan@csail.mit.edu", "regina@csail.mit.edu", "karger@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "This structure includes the topics addressed and the order in which these topics appear in a single domain."}, {"heading": "2. Related Work", "text": "We describe two areas of previous work related to our approach: From an algorithmic point of view, our work falls into a broad class of topic models. While previous work on topic modeling took the literal view of documents, many recent approaches have expanded topic models to capture some structural constraints. In Section 2.1, we describe these extensions and highlight their differences from our model. On the linguistic side, our work refers to research on the modeling of text structure in statistical discourse processing. We summarize this work in Section 2.2 and draw comparisons with the functionality supported by our model."}, {"heading": "2.1 Topic Models", "text": "Traditional topic models such as Latent Dirichlet Allocation (LDA) (Lead, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004) treat documents as word bags, in which each word is assigned a separate topic and words associated with the same topic are taken from a common language module. While word representation is sufficient for some applications, this structurally unconscious view is too limited in many cases. Previous research has considered extensions of LDA models in two orthogonal directions, covering both intrasentative and extrasentative constraints."}, {"heading": "2.1.1 Modeling Intrasentential Constraints", "text": "One promising direction for improving topic models is to supplement them with constraints on the mapping of keywords to adjacent words within sentences. For example, Griffiths, Steyvers, Lead and Tenenbaum (2005) propose a model that combines both syntactic and semantic information in a uniform generative framework, limiting the syntactic classes of adjacent words. In their approach, the generation of each word is controlled by two hidden variables, one specifying a semantic topic and the other specifying a syntactic class. The hidden variables of the syntactic class are concatenated as a Markov model, while semantic topic assignments for each word are assumed to be independent. Wallach (2006) also suggests a way to integrate word order information in the form of bigrams into an LDA-like model."}, {"heading": "2.1.2 Modeling Extrasentential Constraints", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they are able to live."}, {"heading": "2.2 Modeling Ordering Constraints in Statistical Discourse Analysis", "text": "The global constraints encoded in our model are closely related to discourse research on the order of information with applications for summarizing and generating texts (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007), which focuses on learning order constraints from data with the aim of reordering new texts from the same area, based on the assumption that recurring patterns in the order of topics can be discovered by analyzing patterns in the word distribution."}, {"heading": "2.2.1 Discriminative Models", "text": "In practice, however, a more mathematically tractable two-step approach is adopted: first, probabilistic models are used to determine pairs of sentences that exhibit preferences; second, these local decisions are combined to produce a consistent global order (Lapata, 2003; Althaus, Karamanis, & Koller, 2004); and training data for pairs of sentences is constructed by taking into account all pairs of sentences in a document, with supervision labels based on how they are actually ordered; primary work has shown that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al, 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006)."}, {"heading": "2.2.2 Generative Models", "text": "An example of this is the Hidden Markov Model (HMM) -based content model (Barzilay & Lee, 2004), in which states correspond with themes and state transitions, with each hidden state's emissions distribution then being a linguistic model of words. Thus, these models implicitly represent patterns at the level of the current structure. HMM is then used within the ranking framework to select an order with the highest probability. In more recent work, Elsner et al. (2007) developed a search method based on a simulated light bulb showing a high probability of ordering. Unlike ranking-based approaches, their search method can cover the entire ordering space."}, {"heading": "3. Model", "text": "In this section we describe our problem formulation and our proposed model."}, {"heading": "3.1 Problem Formulation", "text": "Our problem with content modeling can be formalized as follows: We take as input a corpus {d1,... dD} of related documents and a specification of a number of topics K.3 Each document d consists of an ordered sequence of Nd paragraphs (pd, 1,..., pd, Nd). As output, we predict a single topic assignment zd, p, p,.., K} for each paragraph p. 4 These z values should reflect the underlying content organization of each document - related content discussed within each document and across different documents should have the same z-value. Our formulation bears some resemblance to the standard LDA setup by assigning a common group of topics over a collection of documents. The difference in LDA is that the assignment of each word is conditionally independent and follows the word assignment, while our limitations on how topics are assigned establish a link between word distribution patterns at the document level."}, {"heading": "3.2 Model Overview", "text": "We propose a Bayesian generative model that explains how to create a corpus of D documents from a series of hidden variables. At a high level, the model first chooses how often each topic is expressed in the document, and how the topics are organized, and these topics then determine the choice of words for each paragraph. However, the notation used in this and subsequent sections is summarized in Figure 1. For each document d is summarized with Nd paragraphs, we generate a topic on topic D separately and a topic on the order of topic D. The disordered bag of topics td, which contains Nd elements, expresses how many paragraphs of the document are mapped to each of the K topics. Equally, the topic can be considered a vector of occurrence for each topic, with zero scores for topics that do not appear at all."}, {"heading": "3.3 The Generalized Mallows Model over Permutations", "text": "To this end, we use the Generalized Mallows Model (GMM) (Fligner & Verducci, 1986; Lebanon & Lafferty, 2002; Meila, Phadnis, Patterson, & Bilmes, 2007; Klementiev, Roth, & Small, 2008), which exhibits two appealing properties in connection with this task. First, the model focuses the probability mass on some canonical orders and minor permutations of this order, which corresponds to our limitation that documents from the same domain exhibit structural similarity. Second, its parameter set scales linearly with the number of elements to be ordered, making it sufficiently limited and traceable for inference. First, we describe the Standard Mallows Model depending on orderability (Mallows, 1957)."}, {"heading": "3.3.1 Inversion Representation of Permutations", "text": "Typically, permutations are presented directly as an ordered sequence of elements - for example, (3, 1, 2) represents the permutation of the original order by placing the third element first, followed by the first element, and then the second. GMM uses an alternative permutation representation defined by a vector (v1,.., vK \u2212 1) of the inversion counts with respect to the identity spermutation (1,.., K). Term vj counts the number of times when a topic is larger than j appears before the permutation. Note that the jth inversion vj can only count on integer values from 0 to K \u2212 j inclusive. Thus, the inversion count has vector only K \u2212 1 elements, since vK is always zero. For example, the default permutation (3, 5, 6, 4), v2, and 6 is before the vector."}, {"heading": "3.3.2 Probability Mass Function", "text": "The GMM assigns probability mass to a certain order, based on how that order is permutated by the canonical order. Specifically, it associates a distance with each permutation, with the canonical order having distance zero and permutations with many inversions having greater distance relative to that canonical order. Distance mapping is based on K \u2212 1 real propagation parameters (\u03c11,.., \u03c1K \u2212 1). The distance of a permutation with inversion counts v is then defined as \"j.\" The probability function of the GMM is exponentially at this distance: GMM (v; \u03c1) = e \u2212 1 \"j\" jvyvj \"(\u03c1) = K \u2212 1\" e \u2212 yvj \"j\" (\u03c1j), (1) where the probability probability probability of the \"j\" yyy \"is a normalization factor with value: 0 \u2212 e\" GMyvyy \"(GMD \u2212 yvy) -yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "3.3.3 Conjugate Prior", "text": "A major advantage of the GMM is that it belongs to the exponential family of distributions; this means that it is particularly susceptible to Bayesian representation, since it allows a natural independent conjugate before each parameter (Fligner & Verducci, 1990): GMM0 (\u03c1j | vj, 0, \u03bd0). (4) This previous distribution takes two parameters equal to 0 and vj, 0. Intuitively, the previous ones indicate that the total number of observed inversions in previous experiments was equal to 0, 0. This distribution can easily be updated with the observed vj to derive a subordinate distribution. Since each vj has a different range, it is inconvenient to directly specify the previous hyperparameters vj, 0."}, {"heading": "3.4 Formal Generative Process", "text": "We will now fully specify the details of our content model, whose plate diagram appears in Figure 1. We will observe a corpus of D documents, each document d being an ordered sequence of Nd paragraphs, and each paragraph being represented as a bag of words. K is assumed to be the default number of topics, and the model will induce a series of hidden variables that probably explain how the words of the corpus were produced. Our final desired output will be the rear topic distribution via the hidden topic assignment variables of the paragraphs. Below, variables marked 0 will be fixed in front of hyperparameters. 1. Draw a language model \u03b2k-Dirichlet (\u03b20) for each topic k. As with LDA, these are topic-specific word distributions. 2. Draw a topic distribution model called Dirichlet (formerly 0), which expresses how likely each topic will appear regardless of position. 3. Draw the topic using Distribution Parameters 4."}, {"heading": "3.5 Properties of the Model", "text": "In this section, we describe the rationality behind using GMM to represent the ordering component of our content model. \u2022 Representative Power The GMM focuses probability mass around a centric permutation that reflects our preferred bias toward document structures with similar themes. Furthermore, parameterizing GMM with a vector of dispersion parameters \u03c1 allows flexibility in how strongly the model reflects bias toward a single order - at one extreme (\u03c1 = \u221e) only one order has a non-zero probability, while in the other all orders are equally probable, because there is a risk that multiple permutations can contribute to the probability of a single document by zd assigning themes when there are themes that do not appear in the city. As a result, our current formulation regarding assignments with fewer themes per document is skewed."}, {"heading": "4. Inference", "text": "The variables we want to infer about the effectiveness of the LDA and its variants are the mappings z that we have preferred; which are determined by the answer to the questions and which depend on the answer to the question. (6) We perform this task by deriving the distribution of individual variable variables from the distribution of individual variable variables. (7) We can build the sampler chain over the hidden variable state space, the stationary distribution of which is the actual distribution of the common distribution. (8) Each new sample is drawn from the distribution of individual variable variables. (8) We can integrate the samplers into the model via some of the hidden variables in which the state space of the Markov chain is reduced. (8) Collapsed sampling has already shown that it is effective for LDA and its variables."}, {"heading": "5. Applications", "text": "In this section, we describe how our model can be applied to three challenging discourse tasks: aligning paragraphs with similarly up-to-date content between documents, segmenting each document into thematically related sections, and arranging new invisible paragraphs into a coherent document. In particular, we show that the back samples generated by our inference method from Section 4 can be used to derive a solution for each of these tasks."}, {"heading": "5.1 Alignment", "text": "For the alignment task, we would like to find out how the paragraphs of each document relate thematically to paragraphs of other documents. Essentially, this is a cross-document cluster task - an alignment maps each paragraph of a document to a thematically related group of K. For example, one group can represent text fragments that discuss the price, while another group consists of fragments via the reception desk. Our model can easily be used for this task: We can consider the theme mapping for each paragraph z as a cluster label. For example, for two documents d1 and d2 with topic mappings zd1 = (2, 4, 4, 1, 1, 1, 1, 1) and zd2 = (4, 3, 3, 2, 2, 2, 2), the paragraph 1 of d1 is summarized with topic mappings zd1."}, {"heading": "5.2 Segmentation", "text": "Segmentation is a well-studied discourse task in which the goal is to divide a document into thematically related, contiguous sections. Previous approaches typically relied on lexical cohesion - that is, similarity in the choice of words within a document span - to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008). Our model relies on the same conception in determining language models of topics, but connects topics across documents and limits how these topics appear to be contiguous in order to better learn the words most clearly related to them."}, {"heading": "5.3 Ordering", "text": "A third application of our model is the problem of creating structured documents from disordered text segments that should relate to earlier numbers. (This text editing task is an important step in a broader NLP task, such as text summarization and generation.) For this task, we assume that we are equipped with well-structured documents from a single domain that serve as learning examples once the model is used to induce an arrangement of previously invisible paragraphs from the same domain. (1,.,.) During the training, we expect our model to produce a highly probable arrangement of topics within the collection of language models associated with each topic. Since the GMM focuses the probabilities around canonical topics (1,.,.,.) Topics of the arrangement of highly probable words in the language models of low-numbered topics tend to appear early in a document, whereas highly probable words in the language models of higher-numbered topics tend to appear late in a document."}, {"heading": "6. Experiments", "text": "In this section, we evaluate the performance of our model in relation to the three tasks outlined in Section 5: Document Alignment, Document Segmentation, and Information Order. First, we describe some preliminary work common to all three tasks, which includes data sets, reference comparison structures, model variants, and inference algorithm settings common to each assessment, and then we examine in detail how our model performs on each task."}, {"heading": "6.1 General Evaluation Setup", "text": "The approach we describe is not the same as finding the most likely order of sales by data probability, that is, how to derive the optimal sequence for the HMM-based content model. Our proposed ordering technique essentially approaches this goal by using a pro-paragraph that allows for a maximum retrospective estimate of the topic assignments, rather than the full topic distribution. This approximation provides for a much faster prediction algorithm that also performs empirical tasks. \u2022 CitiesEn: English Wikipedia article on the 100 largest cities in the world by population. Common topics include history, culture, and demography. These articles are typically of considerable size and share similar organizational patterns. \u2022 CitiesEn500: Articles from the English Wikipedia about the 500 largest cities in the world."}, {"heading": "6.2 Alignment", "text": "First, we evaluate the model based on the task of cross-document alignment, with the goal of grouping units of text from different documents into thematically related clusters. For example, in cities, such a cluster may contain paragraphs related to transport. Before turning to the results, we first present details of the specific assessment structure aimed at this task."}, {"heading": "6.2.1 Alignment Evaluation Setup", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "6.2.2 Alignment Results", "text": "The comparative performance of the basic methods is consistent across all areas - surprisingly, clustering performs better than the more complex HTMM model - an observation consistent with previous work on cross-document alignment and summarizing multiple documents that use clustering as their main component (Radev, Jing, & Budzikowska, 2000; Barzilay, McKeown, & Elhadad, 1999).Despite the fact that HTMM captures some dependencies between adjacent paragraphs, it is not sufficiently limited. Manual verification of actual topic assignments reveals that HTMM often has the same theme for disjointed paragraphs within a document, which violates the theme-related paragraphs."}, {"heading": "6.3 Segmentation", "text": "Next, we look at the task of text segmentation. We test whether the model is able to identify the boundaries of topically coherent text segments."}, {"heading": "6.3.1 Segmentation Evaluation Setup", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "6.3.2 Segmentation Results", "text": "Table 3 presents the results of the segmentation experiments. In each data set, our model outperforms the BayesSeg and U & I baselines by a significant margin regardless of K. This result provides strong evidence that learning networked topic models via related documents leads to improved segmentation performance. Generally, the best performance is achieved by the full version of our model, with three exceptions: In two cases (CitiesEn with K = 10 using clean headings on the WindowDiff metric and CitiesFr with K = 10 on the Pk metric), the variant that performs better than the full model fills out only by one minute. In addition, in both cases, the corresponding rating of K = 20 using the full model results in the best overall results for the respective domains. The only case where a variant outperforms our complete model by a notable distance are the phone data sets. This result is not unexpected, given the formularity of this data set."}, {"heading": "6.4 Ordering", "text": "In contrast to the previous tasks, where prediction is based on hidden variable distributions, the order in a document is observed. Furthermore, the GMM model uses this information during the follow-up process, so we need to divide our data sets into training and test portions. In the past, ordering algorithms were applied to text units of different granularities, most often at sentences and paragraphs. Our ordering experiments operate at the level of a relatively larger unit - sections. We believe that this granularity corresponds to the nature of our model, as it captures patterns at the level of theme distributions rather than at the local discourse level. The order of sentences and paragraphs has been studied in the past (Karamanis et al., 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to produce a complete sequence (Elsner et 16., 2007 / mitgo.http: / www.barzilay, &)."}, {"heading": "6.4.1 Ordering Evaluation Setup", "text": "We use the CitiesEn, CitiesFr and Phones records as training materials for parameter estimation as described in Section 5. We introduce additional sets of documents in the same areas as test records. Table 4 provides statistics on the training and test set splits (note that non-vocabulary terms are discarded in the test records).18Even when we place orders at the section level, these collections still present a challenging task: for example, the average number of sections in a CitiesEn test document is 11.2, comparable to the 11.5 sentences (the unit of reorganization) per document used by the National Transportation Safety Board Corpus in previous work (Barzilay & Lee, 2004; Elsner et al., 2007).Metrics We report the Kendall's rank correlation coefficient for our ordering experiments. These metric measures, however much an order differs from the Reference Rules - the basic assumption that most of the sections should be reasonable."}, {"heading": "6.4.2 Ordering Results", "text": "Table 5 summarizes the order results for the GMM and HMM-based content models. Across all datasets, our model far exceeds content modeling. For example, the gap between the two models in the CitiesEn dataset reaches 35%. This difference is expected. In previous work, content models have been applied to short formulaic texts. In contrast, documents in our collection exhibit higher variability than the original collections. HMM does not provide explicit limitations on generated global orders, which could prevent non-local patterns from being effectively learned in the topic organization. We also note that the constrained variant outperforms our full model. Although the difference between the two is small, it is fairly consistent across domains. As it is not possible to predict idiosyncratic variations in the topic orders of the test documents, a more limited model may better capture the dominant order patterns that exist in the entire domain."}, {"heading": "6.5 Discussion", "text": "Our experiments with the three different tasks show some common trends in the results. First, we observe that our unified model of document structure can easily and successfully be applied to multiple tasks at the discourse level, while previous work has suggested separate approaches for each task. This versatility speaks strongly to the power of our thematic representation of document structure. Second, within each task, our model outperforms the state of the art by significant margins in a variety of assessment scenarios. These results support our hypothesis that the augmentation of topic models with discursive limitations extends its applicability to discursive analysis tasks. Looking at the performance of our model across different tasks, we make a few notes on the importance of each topic limitation. Topical belonging is a consistently important limitation that allows both our model variants and alternative approaches to perform."}, {"heading": "7. Conclusions and Future Work", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "Acknowledgments", "text": "We would like to thank the many people who have provided suggestions and comments on this work, including Michael Collins, Aria Haghighi, Yoong Keok Lee, Marina Meila, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder, and Luke Zettlemoyer. We are especially grateful to Marina Meila for introducing us to the Mallows Model. Again, this paper benefited greatly from the thoughtful feedback from anonymous reviewers. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of funding organizations."}], "references": [{"title": "Aggregating inconsistent information: Ranking and clustering", "author": ["N. Ailon", "M. Charikar", "A. Newman"], "venue": "Journal of the ACM,", "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Computing locally coherent discourses", "author": ["E. Althaus", "N. Karamanis", "A. Koller"], "venue": "In Proceedings of ACL", "citeRegEx": "Althaus et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Althaus et al\\.", "year": 2004}, {"title": "Remembering: a study in experimental and social psychology", "author": ["F.C. Bartlett"], "venue": "Cambridge University Press.", "citeRegEx": "Bartlett,? 1932", "shortCiteRegEx": "Bartlett", "year": 1932}, {"title": "Sentence alignment for monolingual comparable corpora", "author": ["R. Barzilay", "N. Elhadad"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Barzilay and Elhadad,? \\Q2003\\E", "shortCiteRegEx": "Barzilay and Elhadad", "year": 2003}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["R. Barzilay", "N. Elhadad", "K. McKeown"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["R. Barzilay", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Barzilay and Lapata", "year": 2008}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["R. Barzilay", "L. Lee"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Barzilay and Lee,? \\Q2004\\E", "shortCiteRegEx": "Barzilay and Lee", "year": 2004}, {"title": "Information fusion in the context of multi-document summarization", "author": ["R. Barzilay", "K. McKeown", "M. Elhadad"], "venue": "In Proceedings of ACL", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Statistical models for text segmentation", "author": ["D. Beeferman", "A. Berger", "J.D. Lafferty"], "venue": "Machine Learning,", "citeRegEx": "Beeferman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Beeferman et al\\.", "year": 1999}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Topic segmentation with an aspect hidden markov model", "author": ["D.M. Blei", "P.J. Moreno"], "venue": "In Proceedings of SIGIR", "citeRegEx": "Blei and Moreno,? \\Q2001\\E", "shortCiteRegEx": "Blei and Moreno", "year": 2001}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A bottom-up approach to sentence ordering for multi-document summarization", "author": ["D. Bollegala", "N. Okazaki", "M. Ishizuka"], "venue": "In Proceedings of ACL/COLING", "citeRegEx": "Bollegala et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2006}, {"title": "Global models of document structure using latent permutations", "author": ["H. Chen", "S. Branavan", "R. Barzilay", "D.R. Karger"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Statistical significance of MUC-6 results", "author": ["N. Chinchor"], "venue": "Proceedings of the 6th Conference on Message Understanding.", "citeRegEx": "Chinchor,? 1995", "shortCiteRegEx": "Chinchor", "year": 1995}, {"title": "Learning to order things", "author": ["W.W. Cohen", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 1999}, {"title": "Bayesian unsupervised topic segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Eisenstein and Barzilay,? \\Q2008\\E", "shortCiteRegEx": "Eisenstein and Barzilay", "year": 2008}, {"title": "A unified local and global model for discourse coherence", "author": ["M. Elsner", "J. Austerweil", "E. Charniak"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Distance based ranking models", "author": ["M. Fligner", "J. Verducci"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Fligner and Verducci,? \\Q1986\\E", "shortCiteRegEx": "Fligner and Verducci", "year": 1986}, {"title": "Posterior probabilities for a consensus", "author": ["M.A. Fligner", "J.S. Verducci"], "venue": "ordering. Psychometrika,", "citeRegEx": "Fligner and Verducci,? \\Q1990\\E", "shortCiteRegEx": "Fligner and Verducci", "year": 1990}, {"title": "Discourse segmentation of multi-party conversation", "author": ["M. Galley", "K.R. McKeown", "E. Fosler-Lussier", "H. Jing"], "venue": "In Proceedings of ACL", "citeRegEx": "Galley et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2003}, {"title": "Stochastic relaxation, gibbs distributions and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["T.L. Griffiths", "M. Steyvers", "D.M. Blei", "J.B. Tenenbaum"], "venue": "In Advances in NIPS", "citeRegEx": "Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2005}, {"title": "Attention, intentions, and the structure of discourse", "author": ["B.J. Grosz", "C.L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "Grosz and Sidner,? \\Q1986\\E", "shortCiteRegEx": "Grosz and Sidner", "year": 1986}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "M. Rosen-Zvi", "Y. Weiss"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Multi-paragraph segmentation of expository text", "author": ["M. Hearst"], "venue": "Proceedings of ACL.", "citeRegEx": "Hearst,? 1994", "shortCiteRegEx": "Hearst", "year": 1994}, {"title": "Sentence ordering with manifold-based classification in multi-document summarization", "author": ["P.D. Ji", "S. Pulman"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Ji and Pulman,? \\Q2006\\E", "shortCiteRegEx": "Ji and Pulman", "year": 2006}, {"title": "Evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus", "author": ["N. Karamanis", "M. Poesio", "C. Mellish", "J. Oberlander"], "venue": "In Proceedings of ACL", "citeRegEx": "Karamanis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Karamanis et al\\.", "year": 2004}, {"title": "Unsupervised rank aggregation with distancebased models", "author": ["A. Klementiev", "D. Roth", "K. Small"], "venue": "In Proceedings of the ICML,", "citeRegEx": "Klementiev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2008}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["M. Lapata"], "venue": "Proceedings of ACL.", "citeRegEx": "Lapata,? 2003", "shortCiteRegEx": "Lapata", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["M. Lapata"], "venue": "Computational Linguistics, 32 (4), 471\u2013484.", "citeRegEx": "Lapata,? 2006", "shortCiteRegEx": "Lapata", "year": 2006}, {"title": "Cranking: combining rankings using conditional probability models on permutations", "author": ["G. Lebanon", "J. Lafferty"], "venue": "In Proceedings of ICML", "citeRegEx": "Lebanon and Lafferty,? \\Q2002\\E", "shortCiteRegEx": "Lebanon and Lafferty", "year": 2002}, {"title": "Minimum cut model for spoken lecture segmentation", "author": ["I. Malioutov", "R. Barzilay"], "venue": "In Proceedings of ACL", "citeRegEx": "Malioutov and Barzilay,? \\Q2006\\E", "shortCiteRegEx": "Malioutov and Barzilay", "year": 2006}, {"title": "Non-null ranking models", "author": ["C.L. Mallows"], "venue": "Biometrika, 44, 114\u2013130.", "citeRegEx": "Mallows,? 1957", "shortCiteRegEx": "Mallows", "year": 1957}, {"title": "Consensus ranking under the exponential model", "author": ["M. Meil\u0103", "K. Phadnis", "A. Patterson", "J. Bilmes"], "venue": "In Proceedings of UAI", "citeRegEx": "Meil\u0103 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103 et al\\.", "year": 2007}, {"title": "Slice sampling", "author": ["R.M. Neal"], "venue": "Annals of Statistics, 31, 705\u2013767.", "citeRegEx": "Neal,? 2003", "shortCiteRegEx": "Neal", "year": 2003}, {"title": "Towards robust context-sensitive sentence alignment for monolingual corpora", "author": ["R. Nelken", "S.M. Shieber"], "venue": "In Proceedings of EACL", "citeRegEx": "Nelken and Shieber,? \\Q2006\\E", "shortCiteRegEx": "Nelken and Shieber", "year": 2006}, {"title": "Computer Intensive Methods for Testing Hypotheses", "author": ["E.W. Noreen"], "venue": "An Introduction. Wiley.", "citeRegEx": "Noreen,? 1989", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "A critique and improvement of an evaluation metric for text segmentation", "author": ["L. Pevzner", "M.A. Hearst"], "venue": "Computational Linguistics,", "citeRegEx": "Pevzner and Hearst,? \\Q2002\\E", "shortCiteRegEx": "Pevzner and Hearst", "year": 2002}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In Proceedings of SIGKDD", "citeRegEx": "Porteous et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Porteous et al\\.", "year": 2008}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["M. Purver", "K. K\u00f6rding", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "In Proceedings of ACL/COLING", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation and user studies", "author": ["D. Radev", "H. Jing", "M. Budzikowska"], "venue": "In Proceedings of ANLP/NAACL Summarization Workshop", "citeRegEx": "Radev et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2000}, {"title": "On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization", "author": ["S. Riezler", "J.T. Maxwell"], "venue": null, "citeRegEx": "Riezler and Maxwell,? \\Q2005\\E", "shortCiteRegEx": "Riezler and Maxwell", "year": 2005}, {"title": "The Handbook of Discourse", "author": ["D. Schiffrin", "D. Tannen", "H.E. Hamilton"], "venue": null, "citeRegEx": "Schiffrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schiffrin et al\\.", "year": 2001}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of WWW", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "A statistical model for domain-independent text segmentation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of ACL", "citeRegEx": "Utiyama and Isahara,? \\Q2001\\E", "shortCiteRegEx": "Utiyama and Isahara", "year": 2001}, {"title": "Text segmentation and topic tracking on broadcast news via a hidden markov model approach", "author": ["P. van Mulbregt", "I. Carp", "L. Gillick", "S. Lowe", "J. Yamron"], "venue": "Proceedings of ICSLP", "citeRegEx": "Mulbregt et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Mulbregt et al\\.", "year": 1998}, {"title": "Topic modeling: beyond bag of words", "author": ["H.M. Wallach"], "venue": "Proceedings of ICML.", "citeRegEx": "Wallach,? 2006", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "Formulaic Language and the Lexicon", "author": ["A. Wray"], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Wray,? 2002", "shortCiteRegEx": "Wray", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "The second constraint states that documents from the same domain tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002).", "startOffset": 114, "endOffset": 142}, {"referenceID": 49, "context": "The second constraint states that documents from the same domain tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002).", "startOffset": 114, "endOffset": 142}, {"referenceID": 47, "context": "As another example of intrasentential constraints, Wallach (2006) proposes a way to incorporate word order information, in the form of bigrams, into an LDA-style model.", "startOffset": 51, "endOffset": 66}, {"referenceID": 23, "context": "This formulation models text structure at the level of word transitions, as opposed to the work of Griffiths et al. (2005) where structure is modeled at the level of hidden syntactic class transitions.", "startOffset": 99, "endOffset": 123}, {"referenceID": 25, "context": "This hypothesis motivated research on models where topic assignment is guided by structural considerations (Purver, K\u00f6rding, Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly relationships between the topics of adjacent textual units.", "startOffset": 107, "endOffset": 199}, {"referenceID": 25, "context": "In a similar vein, the Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits a generative process where each sentence (textual unit) is assigned a single topic, so that all of the sentence\u2019s words are drawn from a single language model.", "startOffset": 56, "endOffset": 77}, {"referenceID": 25, "context": "This hypothesis motivated research on models where topic assignment is guided by structural considerations (Purver, K\u00f6rding, Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly relationships between the topics of adjacent textual units. Depending on the application, a textual unit may be a sentence, paragraph, or speaker utterance. A common property of these models is that they bias topic assignments to cohere within local segments of text. Models in this category vary in terms of the mechanisms used to encourage local topic coherence. For instance, the model of Purver et al. (2006) biases the topic distributions of adjacent utterances (textual units) to be similar.", "startOffset": 155, "endOffset": 627}, {"referenceID": 30, "context": "The global constraints encoded by our model are closely related to research in discourse on information ordering with applications to text summarization and generation (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007).", "startOffset": 168, "endOffset": 287}, {"referenceID": 17, "context": "The global constraints encoded by our model are closely related to research in discourse on information ordering with applications to text summarization and generation (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007).", "startOffset": 168, "endOffset": 287}, {"referenceID": 30, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006).", "startOffset": 103, "endOffset": 198}, {"referenceID": 28, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006).", "startOffset": 103, "endOffset": 198}, {"referenceID": 30, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004).", "startOffset": 121, "endOffset": 157}, {"referenceID": 1, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004).", "startOffset": 121, "endOffset": 157}, {"referenceID": 26, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). For instance, Lapata (2003) has demonstrated that lexical features, such as verb pairs from the input sentences, serve as a proxy for plausible sequences of actions, and thus are effective predictors of well-formed orderings.", "startOffset": 118, "endOffset": 228}, {"referenceID": 1, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004). While these two-step discriminative approaches can effectively leverage information about local transitions, they do not provide any means for representing global constraints. In more recent work, Barzilay and Lapata (2008) demonstrated that certain global properties can be captured in the discriminative framework using a reranking mechanism.", "startOffset": 136, "endOffset": 383}, {"referenceID": 17, "context": "In more recent work, Elsner et al. (2007) developed a search procedure based on simulated annealing that finds a high likelihood ordering.", "startOffset": 21, "endOffset": 42}, {"referenceID": 34, "context": "We first describe the standard Mallows Model over orderings (Mallows, 1957).", "startOffset": 60, "endOffset": 75}, {"referenceID": 18, "context": "The Generalized Mallows Model, first introduced by Fligner and Verducci (1986), refines the standard Mallows Model by adding an additional set of dispersion parameters.", "startOffset": 51, "endOffset": 79}, {"referenceID": 9, "context": "We accomplish this inference task through Gibbs sampling (Geman & Geman, 1984; Bishop, 2006).", "startOffset": 57, "endOffset": 92}, {"referenceID": 36, "context": "However, the distribution itself is univariate and unimodal, so we can expect that an MCMC technique such as slice sampling (Neal, 2003) should perform well.", "startOffset": 124, "endOffset": 136}, {"referenceID": 22, "context": "This form of caching is the same as what is used by Griffiths and Steyvers (2004).", "startOffset": 52, "endOffset": 82}, {"referenceID": 26, "context": "Previous approaches have typically relied on lexical cohesion \u2014 that is, similarity in word choices within a document subspan \u2014 to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008).", "startOffset": 175, "endOffset": 410}, {"referenceID": 41, "context": "Previous approaches have typically relied on lexical cohesion \u2014 that is, similarity in word choices within a document subspan \u2014 to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008).", "startOffset": 175, "endOffset": 410}, {"referenceID": 17, "context": "In contrast, previous approaches have only been able to rank a small subset of all possible document reorderings (Barzilay & Lapata, 2008), or performed a search procedure through the space of orderings to find an optimum (Elsner et al., 2007).", "startOffset": 222, "endOffset": 243}, {"referenceID": 17, "context": "In contrast, previous approaches have only been able to rank a small subset of all possible document reorderings (Barzilay & Lapata, 2008), or performed a search procedure through the space of orderings to find an optimum (Elsner et al., 2007).7 The objective function of Equation 13 depends on posterior estimates of \u03b2 and \u03b8 given the training documents. Since our collapsed Gibbs sampler integrates out these two hidden variables, we need to back out the values of \u03b2 and \u03b8 from the known posterior samples of z. This can easily be done by computing a point estimate of each distribution based on the word-topic and topic-document assignment frequencies, respectively, as is done by Griffiths and Steyvers (2004). The probability mass \u03b2\u0302w k of word w in the language model of topic k is given by: \u03b2\u0302 k = N\u03b2(k,w) + \u03b20 N\u03b2(k) +W\u03b20 , (14)", "startOffset": 223, "endOffset": 714}, {"referenceID": 38, "context": "Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinearly computed metrics such as F-score.", "startOffset": 82, "endOffset": 96}, {"referenceID": 14, "context": "This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).", "startOffset": 96, "endOffset": 137}, {"referenceID": 25, "context": "\u2022 Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): As explained in Section 2, this model represents topic change between adjacent textual units in a Markovian fashion.", "startOffset": 35, "endOffset": 56}, {"referenceID": 46, "context": "We also compare our method with the algorithm of Utiyama and Isahara (2001), which is commonly used as a point of reference in the evaluation of segmentation algorithms.", "startOffset": 49, "endOffset": 76}, {"referenceID": 28, "context": "The ordering of sentences and paragraphs has been studied in the past (Karamanis et al., 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to induce a full ordering (Elsner et al.", "startOffset": 70, "endOffset": 119}, {"referenceID": 17, "context": ", 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to induce a full ordering (Elsner et al., 2007).", "startOffset": 117, "endOffset": 138}, {"referenceID": 17, "context": "5 sentences (the unit of reordering) per document of the National Transportation Safety Board corpus used in previous work (Barzilay & Lee, 2004; Elsner et al., 2007).", "startOffset": 123, "endOffset": 166}, {"referenceID": 30, "context": "This measure has been widely used for evaluating information ordering (Lapata, 2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 70, "endOffset": 127}, {"referenceID": 17, "context": "This measure has been widely used for evaluating information ordering (Lapata, 2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 70, "endOffset": 127}, {"referenceID": 31, "context": ", 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 79, "endOffset": 93}, {"referenceID": 6, "context": "Baselines and Model Variants Our ordering method is compared against the original HMM-based content modeling approach of Barzilay and Lee (2004). This baseline delivers", "startOffset": 121, "endOffset": 145}, {"referenceID": 17, "context": "We do not include in our comparison local coherence models (Barzilay & Lapata, 2008; Elsner et al., 2007).", "startOffset": 59, "endOffset": 105}, {"referenceID": 35, "context": "However, recent advances in statistics have produced efficient approximate algorithms with theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008) and exact methods that are tractable for typical cases (Meil\u0103 et al., 2007).", "startOffset": 224, "endOffset": 244}], "year": 2009, "abstractText": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.", "creator": "TeX"}}}