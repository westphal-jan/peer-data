{"id": "1401.3579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach", "abstract": "This paper aims to find a algorithmic structure that affords to predict and explain economical choice behaviour particularly under uncertainty(random policies) by manipulating the prevalent Actor-Critic learning method that complies with the requirements we have been entrusted ever since the field of neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that might be relevant to our discussion, we will try to outline some of the important works which have so far been done to simulate choice making processes. Concerning neurological findings that suggest the existence of two specific functions that are executed through Basal Ganglia all the way down to sub- cortical areas, namely 'rewards' and 'beliefs', we will offer a modified version of actor/critic algorithm to shed a light on the relation between these functions and most importantly resolve what is referred to as a challenge for actor-critic algorithms, that is lack of inheritance or hierarchy which avoids the system being evolved in continuous time tasks whence the convergence might not be emerged.", "histories": [["v1", "Fri, 20 Dec 2013 05:54:58 GMT  (120kb,D)", "https://arxiv.org/abs/1401.3579v1", null], ["v2", "Sat, 22 Feb 2014 19:15:53 GMT  (121kb,D)", "http://arxiv.org/abs/1401.3579v2", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["keyvan yahya"], "accepted": false, "id": "1401.3579"}, "pdf": {"name": "1401.3579.pdf", "metadata": {"source": "CRF", "title": "A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach", "authors": ["Keyvanl Yahya"], "emails": ["kxy054@bham.ac.ukl."], "sections": [{"heading": null, "text": "This is the question that many people have asked themselves, and that they must ask themselves in order to form a new interdisciplinary field of neuroeconomics. [1] Neuroeconomics aims to understand human decisions and the way in which brain processes proceed. [2] In other words, it aims to understand how the brain adopts optimal behavior. [3] The relationship between neuroscience and each of the above fields is reciprocal. [4] This is neuroeconomics, which gives us novel choices and the way in which brain processes proceed."}, {"heading": "II. OBJECTIVES: FINDING A PROPER RL-BASED ALGORITHM TO EXPLAIN CHOICE BEHAVIOUR", "text": "It has been suggested that there is \"a fairly direct mapping of the model-free amplification of learning algorithms on the brain,\" in which dopamine serves as a teaching signal to train values or strategies by controlling synaptic plasticity in targets such as the ventral and dorsolateral striatum. Furthermore, it has been demonstrated that the brain sets these models of uncertainty and dopamine release in motion, along with quantities referring to the temporal difference in prediction errors, called the TD algorithm. In addition, there is another family of adaptive algorithms based on the approximation of the value function of TD. I am willing to focus my work on the latter, namely focusing these very adaptive actor-critic algorithms."}, {"heading": "III. METHODOLOGY", "text": "To make predictions about future penalties or rewards, an agent could use a model of the world, which should indicate the probability of the subject moving from one state to the next, perhaps depending on what action it takes, and what the likely outcomes of the states are, which in turn may depend on the actions. Psychologically, since these values depend on the expected outcomes and their modeled usefulness, this type of control is considered purposeful; the model should represent not only the best action, but also the expected benefit of the outcomes. The main problem is that calculating the values and processing the necessary information would prove difficult, because the memory is limited and we are dealing with a huge amount of data. [6]"}, {"heading": "A. Procedure of of computation", "text": "First, we will point out that a targeted choice is a choice based on a pair of actions (Q = A, where A is designated as a space of action next to the state space comprising the states or decisions. In order to make the right choice, the targeted actors need an appropriate policy, but before that they must assign values to the actions that are proportional to the amount of the expected reward. Afterwards, the actor must select the action to which the highest value has been assigned. This process of value assignment could also be called Ua = x p (s) r (S) r (S) where oa (S) is the result of the monitoring. Now, we want to divide these assigned values (due to the enormous amount of data) into learning about the states (classical conditioning) and learning about actions (instrumental learning). After calculating the state values V (s) = R (s) + [s) a series of actions (s)."}, {"heading": "IV. CONCLUSION", "text": "The present study suggests that an algorithm that takes into account a different type of value, inspired by Pavlovian conditioning, assigns a value to a state and action selection that influences decision-making processes. However, although these values are not linked to actions, they can influence behavior in different ways; most importantly, these values can manipulate choice by transforming information about the likelihood of a payout by an action in relation to its particular outcome. It is worth noting that everything we have done so far to offer this algorithm - this is a modified version of the actor-critic algorithm - is based on neurophysiological findings - one can find out more in [8] that on the whole it is shown that these functions, which are supposed to execute the decision-making processes, are liable to reward related circuits that exhibit interaction between cortex and strain, and that reward network consists of two habitual target-directed processes (which we have done here)."}, {"heading": "ACKNOWLEDGMENT", "text": "The author would like to thank Pouyan Rafeifard for his sincere friendship and thoughtful comments."}], "references": [{"title": "Prospect Theory: An Analysis of Decision under Risk", "author": ["D. Kahneman", "Tversky"], "venue": "Econometria,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "A framework for mesencephalic dopamine systems based on predictive Hebbian learning", "author": ["P.R. Montague", "Dayan", "T.J. Sejnowski"], "venue": "J. Neurosci.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Actor-Critic Algorithms", "author": ["V. Konda", "N. Tsitsiklis"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Supervised Learning Combined with an Actor-Critic Architecture", "author": ["M. Rosenstein", "T. Barto"], "venue": "CMPSCI Technical Report,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "A neural correlate of response bias in monkey caudate nucleus", "author": ["J. Lauwereyns", "K. Watanabe", "B. Coe", "O. Hikosaka"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "These theories such as \u201dRevealed Preference Theory\u201d or \u201dProspect Theory\u201d had been expanded upon by Tversky and Kahnemen[3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "utility could consequently be achieved through evaluating a set of goal directed actions that ends us finding a policy which defines these goal directed actions[5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "The main problem is that since the working memory is limited and we are dealing with a huge amount of data, computing the values and processing the necessary information would prove a difficult task[6].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "The environment also supply an evaluation called the immediate reward,r[7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "It is worth noting that all we have done so far to offer this algorithm - that is a modified version of actor/critic algorithm-was lean on neurophysiological findings-you can find out more in [8]- that on the whole demonstrate that those functions that are to execute choice making processes are liaised to reward related circuity that holds an interaction between cortex and straitum and also this reward based network is consisted of two distinct processes(goal directed and habitual trends) similar to what we did here to assign actor and critic mechanisms to both of them respectively.", "startOffset": 192, "endOffset": 195}], "year": 2014, "abstractText": "This paper aims to find an algorithmic structure that affords to predict and explain the economical choice behaviour particularly under uncertainty(random policies) by manipulating the prevalent Actor-Critic learning method to comply with the requirements we have been entrusted ever since the field of neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that might be relevant to our discussion, we will try to outline some of the important works which have so far been presented to simulate choice making processes. Concerning neurological findings that suggest the existence of two specific functions, namely, \u2019rewards\u2019 and \u2019beliefs\u2019 that are executed through a specific pathway from Basal Ganglia all the way up to subcortical areas, we will offer a modified version of actor/critic algorithm to shed a light on the relation between these functions and most importantly resolve what is referred to as a challenge for actor-critic algorithms, that is lack of inheritance or hierarchy which avoids the system being evolved in continuous time tasks whence the convergence might not be emerged. Keywords\u2014neuroeconomics, choice behaviour, actor-critic algorithm, decision making, reinforcement learning", "creator": "LaTeX with hyperref package"}}}