{"id": "1401.3827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Efficient Planning under Uncertainty with Macro-actions", "abstract": "Deciding how to act in partially observable environments remains an active area of research. Identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states. Towards addressing this challenge, we present an online, forward-search algorithm called the Posterior Belief Distribution (PBD). PBD leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken, given the set of observation sequences that could be received during this process. This method allows us to efficiently evaluate the expected reward of a sequence of primitive actions, which we refer to as macro-actions. We present a formal analysis of our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment, which suggests that our approach has practical potential for planning in real-world, large partially observable domains where a multi-step lookahead is required to achieve good performance.", "histories": [["v1", "Thu, 16 Jan 2014 04:36:09 GMT  (1450kb)", "http://arxiv.org/abs/1401.3827v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ruijie he", "emma brunskill", "nicholas roy"], "accepted": false, "id": "1401.3827"}, "pdf": {"name": "1401.3827.pdf", "metadata": {"source": "CRF", "title": "Efficient Planning under Uncertainty with Macro-actions", "authors": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "emails": ["RUIJIE@CSAIL.MIT.EDU", "EMMA@CS.BERKELEY.EDU", "NICKROY@CSAIL.MIT.EDU"], "sections": [{"heading": null, "text": "To meet this challenge, we present an online forward search algorithm called Posterior Belief Distribution (PBD). PBD uses a novel method to calculate the posterior distribution of beliefs that arise after a sequence of actions has been taken, based on the observational sequences that could be received during the process. This method allows us to efficiently assess the expected reward of a sequence of primitive actions that we call macro actions. We present a formal analysis of our approach and examine its performance using two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate that our algorithm for controlling a real robot helicopter is used in a target monitoring experiment, suggesting that our approach has practical potential for planning in the real, large, partially observable multi-step domain, where good performance is required to achieve in a partially observable domain."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2. Background: Planning under Uncertainty using Forward Search", "text": "Formally, we assume that our decision-making problem consists of the following known components: \u2022 S is a set of states. Each state consists of an assignment of values to each of the L variables, which can be either discrete or continuous. \u2022 P (s) is a set of actions (controls) that can be either discrete or continuous. \u2022 Z is a set of observations that can be either discrete or continuous. \u2022 P (s) is a transition function (also known as a dynamic model) that describes the likelihood of transformation into a state after taking action. We assume that the dynamics satisfy the Markov assumption that the new state is only a function of immediate observation and action. \u2022 p (z) is an observation function (also known as a measurement or sensor model) that is the likelihood of receiving observations in the state."}, {"heading": "2.1 Macro-action Construction", "text": "In fact, the fact is that most of us will be able to, and will be able to, abide by the rules that they have established in the past."}, {"heading": "3. The Posterior Belief Distribution Algorithm", "text": "In fact, it is as if most of them are able to trump themselves, and that they are able to trump themselves. (...) It is not as if they are able to trump themselves. (...) It is as if they were able to trump themselves. (...) It is not as if they were able to trump themselves. (...) It is as if they were able to trump themselves. (...). (...) It is as if. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). () It is. (...). () It is. (...). () It is. (...). () It is. (...). It is. (...). () It is. (...). (). It is. (...). (). () It is. (...). It is. (...). (). () It is. (...). (...). It is. (). It is. (). (). (). (...). It is. It is. (). (...). (...). It is. It is. (). (). (). It is. (). (). (...). It is. (). It is. (). (). (). (). (It is. It is. (). (). It is. (). (It is. (). (). It is. (). It is. (). (). (). It is. (). It is. (). It is. ()."}, {"heading": "3.1 Exact Computation of Posterior Belief Distribution", "text": "Suppose, for the moment, that the faith of the agent can be represented exactly as Gaussian distribution over a continuous state space (Q = 7), and that the observation and transition models are both linear-Gaussian. Formally, the state transition and the observation models can be represented as follows: st = branch \u2212 1 + bat + \u03b5t, \u03b5t \u0445 N (0, P) (5) zt = Cst + \u03b4, \u03b4 N (0, Q) (6), where A and B are dynamic matrices, C is the observation matrix, P is the covariance of the Gaussian dynamic process, and Q is the covariance of the measurement noises.If the state transition and observation models are normally distributed and linear functions of the state, the Kalman filter (1960) provides a closed solution for the posterior belief about the states, N (\u00b5t) there is a prior belief about the states N, (\u00b5t)."}, {"heading": "3.2 Calculating the Expected Reward", "text": "The reason for calculating this distribution, in turn, is that we take into account the expected reward of a particular set of beliefs by considering the expected reward of a particular set of beliefs and performing an L-length action that will calculate the best action for the current set of beliefs.., aL. This can be expressed as a reward for a macro action by taking into account the expected reward of a particular set of beliefs (b0, a1) + the likely reward of a particular set of beliefs (b0, a1) + the execution of a particular set of beliefs (b0, a1) +., z1p (z1, a)."}, {"heading": "3.3 Branching on Posterior Beliefs", "text": "So far, we have discussed how to map the posterior distribution in the three final stages of each macro-activity that can arise after the execution of a single macro-action process, and how to calculate the expected reward associated with this distribution. (...) This allows us to consider scenarios in the future that may be useful to take the best measures to capture the present belief in oneself. (...) For example, a large office area where a robot is trying to navigate to a destination, and the macro-activities are to go to the end of a hallway and turn left or right. (...) The summary of the robot begins far from the target position, a series of macro-activities is most likely to be needed to reach the target, and therefore it will be important to consider a search horizon of multiple macro-activity trees. (...)"}, {"heading": "3.4 The PBD Algorithm Summary", "text": "We are now ready to present our PBD forward search algorithm (algorithm 1) (algorithm 1). In view of the current belief, we select an action by constructing a macro action forward search tree. By putting the current belief at the root, we expand any possible macro action (algorithm 2), calculate the expected reward and the resulting posterior beliefs. We then try out a fixed number of posterior beliefs. We repeat this process to a fixed horizon depth and then select an action for the current belief by estimating its value from the search sheet nodes. After executing this action, an observation is received and the new belief state is calculated. The entire process is then repeated for this new belief state. Note that PBD will always select only actions that are the first action of a macro action."}, {"heading": "4. Approximate Computation of Posterior Belief Distributions", "text": "In this section, we briefly describe an extension of the PBD model to handle a broader class of observation models, namely parametric models that are members of the exponential distribution family (Barndorff-Nielsen, 1979). Techniques exist for non-linear transition models, such as extended Kalman filtering to approximate the rear class of observation models with a Gaussian; however, we do not formally consider these techniques to be incorporated into our PBD algorithms. We opt for exponential family observation models, as this family includes a wide range of distribution models, such as Gaussian, Bernoulli, and Poisson distributions, which have certain appealing mathematical properties."}, {"heading": "5. Analysis", "text": "At this point, we offer a formal analysis of the accuracy and computational complexity of our PBD algorithm. In this section, we assume that states of belief can be represented in the same way as Gaussian distributions: In other words, we assume a linear-Gaussian system. In the following sections, we will demonstrate experimentally that the PBD algorithm is useful for a greater variety of problems using an EKF or efKF described in Section 4, but the inclusion of the error of these approximate filtering techniques in an analysis of the algorithm is a topic of future research."}, {"heading": "5.1 Performance", "text": "The PBD algorithms are only approximate values underlying the adjustments (e.g. a limited political space induced by the macro activities), but during the execution only the first step of the macro activity, which cannot correspond to the known macro activities, will be performed at least as well as the entire macro activity is actually performed. However, it would be useful to determine whether any demands can be made on the faith activity values calculated as part of the PBD algorithm. Obviously, the received rewards of the implemented policies will always be less or equal to the optimal political rewards, since the political space considered during the planning is smaller than the full political space. However, the values calculated by the PBD algorithm are only non-threatening values underlying the adjustments of the implemented policies."}, {"heading": "5.2 Computational Complexity", "text": "One of the central contributions of our work is the provision of an efficient macroaction forward search algorithm that can be scaled to long horizons and large problems. We will now analyze the computational complexity of our approach. Calculation costs will be a function of two operations: calculating the posterior distribution of beliefs and calculating the expected reward of a distribution of beliefs. As we will soon see, the computational complexity of these operations is a polynomial function of the state spatial dimension.12 This relationship of low order is possible due to the particular parametric representation used for posterior distribution of beliefs: the posterior distribution of beliefs as Gauss requires a number of parameters that scales only square with the number of state dimensions.13 PBD is therefore able to scale to large domains. Our computational complexity results are summarized in Table 1."}, {"heading": "5.2.1 COMPLEXITY OF GAUSSIAN BELIEF UPDATING FOR A LENGTH L MACRO-ACTION", "text": "The calculation for the posterior distribution over beliefs resulting from a macro action was presented in Eq.53 and consists of a series of matrix multiplications and reversals. Matrix multiplication is an O (D2) calculation, where D is the state-space dimension. Matrix inversion can be performed in O (D3) time. Therefore, the calculation cost of performing a single update of posterior beliefs is an O (D3) operation. This update must be performed for each primitive action at a length-L macro action per second, resulting in the calculation cost of O (LD3) (57) for a single macro action. In Section 4, we presented a series of equations (equations 50-53) that we use to approximate the posterior distribution over beliefs when the observation model is not gaussian, but an exponential family. These equations, in turn, consist of a series of (50-53) over a single action (O) and an update of a single matrix (53) over a single action."}, {"heading": "5.2.2 COMPLEXITY OF ANALYTICALLY COMPUTING THE EXPECTED REWARD OF A LENGTH", "text": "The second component of the calculation costs comes when we evaluate the expected reward of a macro action = 48. If the reward is a weighted sum of the Nr-Gaussians, as indicated by Equation 43, this process involves evaluating the value of the NrL-Gaussians at certain fixed points. Evaluating a D-dimensional Gaussian at a single point is an O (D3) operation, based on the weighted sum of the Nr-Gaussians that must be calculated. (58) If the reward model is instead aNr-th degree polynomial function of the state, then the expected reward calculation consists of the cost of calculating the Nr-moments of the distribution of the state. If the reward is a weighted sum of the Nr-Gaussians at a single point, the assessment of a D-dimensional Gaussian at a single point is an O (D3) operation, based on the weighted sum of the Nr-Gaussians that must be calculated."}, {"heading": "5.2.3 COMPLEXITY OF CONDITIONAL MACRO-ACTION PLANNING (PBD)", "text": "A sample from the desired multivariate Gaussian N (s | \u00b5, \u03a3) is simply \u00b5 + Aq. The sample from Ns times includes the one-time cost of calculating cholesky decomposition plus matrix vector multiplication for each sample resulting in cost of O (D3 + NsD2). (62) This procedure is performed at every branch point of the forward search tree (in other words, at all macro action nodes except those on the tree leaves)."}, {"heading": "5.2.4 COMPLEXITY OF PBD WITH ARBITRARY REWARD MODELS", "text": "With arbitrary reward models, it will not be possible to analytically calculate the expected reward. Instead, the expected reward for each primitive action within the macroeconomic action can be approximately.15 The cost of sampling Ns states from a multivariate gauss is an O (D3 + NsD2) process (from Eq.62). Assuming that the calculation of the reward takes time for each sample in the state dimension, sampling the rewards adds an additional O (D3 + NsD2D) = O (D3 (Ns + 1) (65) to each primitive action within a macroeconomic action, resulting in an overall complexity of the PBD planning with reward sting of A | O."}, {"heading": "6. Experimental Results", "text": "In this section, we test our algorithm for planning problems under uncertainty problems. PBD's algorithm assumes that the transition models of problem domains are roughly comparable as linear Gaussian. Our results on problems inspired by two different research communities, scientific research from the POMDP literature (Smith & Simmons, 2005) and goal monitoring in the field of sensor resource management suggest that many domains support this assumption. More generally, the use of a linear Gaussian dynamic model is a common approximation in the control community and has been used to approximate even very complex dynamics such as the physiological changes in glucose control for diabetics (Patek, Breton, Chen, Solomon & Kovatchev, 2007). Despite the different origins and state spatial representations of the two problems, for which we will shortly present results, both involve the reasoning of several steps to make good decisions in the future in a very large area."}, {"heading": "6.1 Generic Baselines", "text": "In both cases, we compare the PBD algorithm with state-of-the-art approaches from the relevant research community - POMDP planners and sensor resource management algorithms for scientific exploration and target monitoring problems. In order to fully investigate the effects of analytical calculation of posterior distribution via beliefs, we also constructed a variety of algorithms that currently do not exist in the literature. These algorithms gain access to the same hand-coded macro responses as those used by the PBD algorithms. We initially constructed comparison algorithms that use a macro action search, but sample observation trajectories that do not work with posterior distribution via beliefs. Sampling observation sequences produce a particle approximation of the resulting distribution via beliefs, providing a baseline algorithm that does not use macro activity, but only posterior belief distribution."}, {"heading": "6.2 Rocksample", "text": "In fact, most of us are able to go in search of a suitable place to move around."}, {"heading": "6.3 Target Monitoring", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "6.4 Real-world Helicopter Experiments", "text": "In fact, it is the case that you will be able to manoeuvre yourself into a situation in which you see yourself, in which you are able, in which you are able to assert yourself, and in which you are able to assert yourself, and in which you are able to assert yourself."}, {"heading": "7. Related Work", "text": "Decisions made under uncertainty, when states are partially observable, are most often made within the Partially Observable Markov Decision Process (POMDP) 2003, although this problem has been analyzed under similar assumptions in other areas of research. While it goes beyond the scope of this paper to provide a comprehensive overview of POMDP techniques, point-based methods such as HSVI2 (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) are often considered state-of-the-art methods that use the piecemeal and convex aspects of the value function to perform value updates on selected beliefs. These approaches assume discrete representation, but suggest approaches that propose parametric representations for continuously evaluated state spaces (Brooks, Makarenko, Williams, Durrant-Whyte, 2006; Kaelbling, Lozano-Perez, Porta al."}, {"heading": "8. Conclusion", "text": "In this paper, we introduced the Posterior Belief Distribution Algorithm. PBD is a forward search algorithm for large (consisting of many variables, each of which can take many values) partially observable areas. PBD analytically and efficiently calculates the resulting distribution of posterior beliefs that are possible after a sequence of actions, thereby tracking the calculation costs for evaluating the reward associated with a macro action, which we use to facilitate a search with a longer horizon in online planning. We have presented theoretical and experimental results that evaluate the performance and computational costs of our macro action algorithms. Our algorithms have been applied to problem areas spanning multiple research communities, and have consistently achieved better results than previous approaches in large areas that require a multi-level outlook for good performance. Finally, we demonstrated our algorithm on a real robot helicopter, underscoring the applicability of our algorithm to partially observable areas for long-term planning."}, {"heading": "9. Acknowledgments", "text": "Ruijie He, E. Brunskill and N. Roy were supported by the National Science Foundation (NSF) Division of Information and Intelligent Systems (IIS) under grant number 0546467 and the Office of Naval Research under project \"Decentralized Reasoning in Reduced Information Spaces,\" contract number N00014-09-1-1052. We thank Finale Doshi-Velez, Alborz Geramifard, Josh Joseph, Brandon Luders, Javier Velez and Matthew Walter for valuable discussions and feedback. Daniel Gurdan, Jan Stumpf and Markus Achtelik provided the quadrotor helicopter and support from Ascending Technologies. Abraham Bachrach, Anton De Winter, Garrett Hemann, Albert Huang and Samuel Prentice helped develop the software and hardware for the helicopter demonstration."}, {"heading": "Appendix A: Exponential Family Kalman Filter", "text": "Building on the statistical economy for the time series analysis of non-Gaussian observations (Durbin & Koopman, 2000), we present the Kalman filter equivalent for systems with linear-Gaussian state transitions and observational models belonging to the exponential family of distributions (Durbin & Koopman, 2000).The state transition and observational models can be represented as follows: st = atst \u2212 1 + btat + btat (btat + btat), st \u2212 1 \u00b2 N (Jimmy \u2212 1), \u03b5t N (0, Pt) (68) p (zt | \u03b8t) = exp (zTt \u2212 \u03b2t) + btat (Btat \u2212 t), st \u2212 t (sp), t (Jimmy \u2212 W (Jimmy \u2212 1). (69) For the state transition model, st is the hidden state of the system, st is the control measure, at and Bt are the linear transition matrices, and the exit matrix is the noise and exit factor."}, {"heading": "Appendix B. Rock Sample Observation Model", "text": "In the rock sample problem, the Bernoulli observation function \u2212 si \u2212 si = 0.5 \u2212 si = 0.5 \u2212 si = 0.5 \u2212 si = 0.5 \u2212 si (remember that rt is the position of the agent at the time t, RBi is the location of the information emitter associated with the rock i, zi \u2212 si = 0.5 \u2212 si at the time t, and si, t is the true value ofrock i at the time t. If we then di, t = (si, t \u2212 0.5) 2 \u2212 di, t (si, t \u2212 RBi \u00b2 2) 2 \u2212 di, 2 \u2212 di, t \u2212 di, t / D0 \u2212 zi, t = si, t, rt, rt, RBi) (104) = (0.5 + (si, t \u2212 0.5) 2 \u2212 di, t (t), si, si (t \u2212 t), si (t \u2212 t), t \u2212 t."}, {"heading": "Appendix C. Target Tracking Observation Model", "text": "We assume an observation model for target tracking, where the target observation has Gaussian noise and noise covariance (b) is a function of the position of the helicopter and the target i: zxi-xi-zi = f xi-yi \u03b8i + N (0, \u041fzi) \u0432\u0438\u0441zi = g (xi, yi, xa, ya, ha), where xi, yi, \u03b8i is the pose of the target i, while xa, ya, ha correspond to the position and height of the agent in the surroundings. zxi, zyi, z\u03b8i is the observation of the target i in image coordinates. The covariance function itself is specified asg (xi, yi, xa, ya, ha, ha) = C1ha + C2 ([xi yi, ya] is the observation of the target in front of the target coordination T: [xi yi] \u2212 ya, [xi yi] \u2212 [xya] is the observation of the target model where C1, C2, C3 and C3 are."}], "references": [{"title": "Autonomous flight in unstructured and unknown indoor environments", "author": ["A. Bachrach", "R. He", "N. Roy"], "venue": "In Proceedings of the European Micro Aerial Vehicle (EMAV) Conference", "citeRegEx": "Bachrach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bachrach et al\\.", "year": 2009}, {"title": "Information and exponential families in statistical theory", "author": ["O. Barndorff-Nielsen"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "Barndorff.Nielsen,? \\Q1979\\E", "shortCiteRegEx": "Barndorff.Nielsen", "year": 1979}, {"title": "Receding horizon control of autonomous aerial vehicles", "author": ["J. Bellingham", "A. Richards", "J. How"], "venue": "In Proceedings of the American Control Conference (ACC),", "citeRegEx": "Bellingham et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bellingham et al\\.", "year": 2002}, {"title": "Dynamic Programming and Optimal Control, vol. 1 & 2, 2nd", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas,? \\Q2007\\E", "shortCiteRegEx": "Bertsekas", "year": 2007}, {"title": "Solving POMDPs: RTDP-Bel vs. point-based algorithms", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Bonet and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2009}, {"title": "Parametric POMDPs for planning in continuous state spaces", "author": ["A. Brooks", "A. Makarenko", "S. Williams", "H. Durrant-Whyte"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Brooks et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Brooks et al\\.", "year": 2006}, {"title": "Continuous-state POMDPs with hybrid dynamics", "author": ["E. Brunskill", "L. Kaelbling", "T. Lozano-Perez", "N. Roy"], "venue": "In Proceedings of the International Symposium on Artificial Intelligence and Mathematics (ISAIM)", "citeRegEx": "Brunskill et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brunskill et al\\.", "year": 2008}, {"title": "Automated hierarchy discovery for planning in partially observable environments", "author": ["L. Charlin", "P. Poupart", "R. Shioda"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Charlin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Charlin et al\\.", "year": 2007}, {"title": "Time series analysis of non-Gaussian observations based on state space models from both classical and Bayesian perspectives", "author": ["J. Durbin", "S. Koopman"], "venue": "Journal of the Royal Statistical Society: Series B (Methodological),", "citeRegEx": "Durbin and Koopman,? \\Q2000\\E", "shortCiteRegEx": "Durbin and Koopman", "year": 2000}, {"title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using Local Approximation", "author": ["T. Erez", "W. Smart"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Erez and Smart,? \\Q2010\\E", "shortCiteRegEx": "Erez and Smart", "year": 2010}, {"title": "Real-time hierarchical POMDPs for autonomous robot navigation", "author": ["A. Foka", "P. Trahanias"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Foka and Trahanias,? \\Q2007\\E", "shortCiteRegEx": "Foka and Trahanias", "year": 2007}, {"title": "Synthesis of hierarchical finite-state controllers for POMDPs", "author": ["E. Hansen", "R. Zhou"], "venue": "In Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Hansen and Zhou,? \\Q2003\\E", "shortCiteRegEx": "Hansen and Zhou", "year": 2003}, {"title": "On the design and use of a micro air vehicle to track and avoid adversaries", "author": ["R. He", "A. Bachrach", "M. Achtelik", "A. Geramifard", "D. Gurdan", "S. Prentice", "J. Stumpf", "N. Roy"], "venue": "International Journal of Robotics Research,", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "PUMA: planning under uncertainty with macro-actions", "author": ["R. He", "E. Brunskill", "N. Roy"], "venue": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Planning in information space for a quadrotor helicopter in GPS-denied environments", "author": ["R. He", "S. Prentice", "N. Roy"], "venue": "In Proceedings of the International Conference of Robotics and Automation (ICRA),", "citeRegEx": "He et al\\.,? \\Q2008\\E", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Solving POMDPs with continuous or large discrete observation spaces", "author": ["J. Hoey", "P. Poupart"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Hoey and Poupart,? \\Q2005\\E", "shortCiteRegEx": "Hoey and Poupart", "year": 2005}, {"title": "Task-driven tactile exploration", "author": ["K. Hsiao", "L. Kaelbling", "T. Lozano-P\u00e9rez"], "venue": "In Proceedings of Robotics: Science and Systems (RSS)", "citeRegEx": "Hsiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hsiao et al\\.", "year": 2010}, {"title": "Robust belief-based execution of manipulation programs", "author": ["K. Hsiao", "T. Lozano-P\u00e9rez", "L. Kaelbling"], "venue": "In Proceedings of the Workshop on the Algorithmic Foundations of Robotics (WAFR)", "citeRegEx": "Hsiao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsiao et al\\.", "year": 2008}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "Transactions of the ASME\u2013Journal of Basic Engineering,", "citeRegEx": "Kalman,? \\Q1960\\E", "shortCiteRegEx": "Kalman", "year": 1960}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Near-optimal observation selection using submodular functions", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Krause and Guestrin,? \\Q2007\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2007}, {"title": "Efficient methods of non-myopic sensor management for multitarget tracking", "author": ["C. Kreucher", "A. Hero III", "K. Kastella", "D. Chang"], "venue": "In Proceedings of the IEEE Conference on Decision and Control (CDC),", "citeRegEx": "Kreucher et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kreucher et al\\.", "year": 2004}, {"title": "Motion planning under uncertainty for robotic tasks with long time horizons", "author": ["H. Kurniawati", "Y. Du", "D. Hsu", "W. Lee"], "venue": "In Proceedings of the International Symposium of Robotics Research (ISRR)", "citeRegEx": "Kurniawati et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2009}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W. Lee"], "venue": "In Proceedings of the Robotics: Science and Systems (RSS)", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Three dimensional receding horizon control for UAVs", "author": ["Y. Kuwata", "J. How"], "venue": "In Proceedings of the AIAA Guidance, Navigation, and Control Conference and Exhibit (GNC),", "citeRegEx": "Kuwata and How,? \\Q2004\\E", "shortCiteRegEx": "Kuwata and How", "year": 2004}, {"title": "Learning policies for partially observable environments: scaling up", "author": ["M. Littman", "A. Cassandra", "L. Kaelbling"], "venue": "In Proceedings of the Twlfth International Conference on Machine Learning (ICML),", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Self-improving factory simulation using continuous-time average-reward reinforcement learning", "author": ["S. Mahadevan", "N. Marchalleck", "T. Das", "A. Gosavi"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mahadevan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 1997}, {"title": "Constrained model predictive control", "author": ["D.Q. Mayne", "J.B. Rawlings", "C.V. Rao", "P.O.M. Scokaert"], "venue": "Stability and optimality. Automatica,", "citeRegEx": "Mayne et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mayne et al\\.", "year": 2000}, {"title": "Approximate planning for factored POMDPs using belief state simplification", "author": ["D. McAllester", "S. Singh"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "McAllester and Singh,? \\Q1999\\E", "shortCiteRegEx": "McAllester and Singh", "year": 1999}, {"title": "acQuire-macros: An algorithm for automatically learning macro-actions", "author": ["A. McGovern"], "venue": "In NIPS 98 Workshop on Abstraction and Hierarchy in Reinforcement Learning", "citeRegEx": "McGovern,? \\Q1998\\E", "shortCiteRegEx": "McGovern", "year": 1998}, {"title": "Hybrid POMDP algorithms. In Workshop on MultiAgent Sequential Decision Making in Uncertain Domains (MSDM)", "author": ["S. Paquet", "B. Chaib-draa", "S. Ross"], "venue": null, "citeRegEx": "Paquet et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2006}, {"title": "An online POMDP algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "In Proceedings of the Conference on Autonomous agents and Multiagent systems (AAMAS),", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Linear quadratic gaussianbased closed-loop control of type 1 diabetes", "author": ["S. Patek", "M. Breton", "Y. Chen", "C. Solomon", "B. Kovatchev"], "venue": "Journal of Diabetes Science and Technology,", "citeRegEx": "Patek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Patek et al\\.", "year": 2007}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Policy-contingent abstraction for robust robot control", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Belief space planning assuming maximum likelihood observations", "author": ["R. Platt", "R. Tedrake", "T. Lozano-Perez", "L. Kaelbling"], "venue": "In Proceedings of Robotics: Science and Systems (RSS)", "citeRegEx": "Platt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "Point-based value iteration for continuous POMDPs", "author": ["J. Porta", "N. Vlassis", "M. Spaan", "P. Poupart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Porta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Porta et al\\.", "year": 2006}, {"title": "Exploiting Structure to Efficiently Solve Large Scale Partially Observable Markov Decision Processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis,", "citeRegEx": "Poupart,? \\Q2005\\E", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "Experimental demonstrations of real-time MILP control", "author": ["A. Richards", "Y. Kuwata", "J. How"], "venue": "In Proceeding of the AIAA Guidance,", "citeRegEx": "Richards et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Richards et al\\.", "year": 2003}, {"title": "AEMS: An anytime online search algorithm for approximate policy refinement in large POMDPs", "author": ["S. Ross", "B. Chaib-draa"], "venue": "In Proceedings of the International Joint Conference in Artificial Intelligence (IJCAI),", "citeRegEx": "Ross and Chaib.draa,? \\Q2007\\E", "shortCiteRegEx": "Ross and Chaib.draa", "year": 2007}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Bayesian reinforcement learning in continuous POMDPs with application to robot navigation", "author": ["S. Ross", "B. Chaib-draa", "J. Pineau"], "venue": "In Proceedings of the International Conference on Robotics and Automation (ICRA). IEEE", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "A POMDP framework for coordinated guidance of autonomous UAVs for multitarget tracking", "author": ["A. Scott", "Z. Harris", "E. Chong"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "Scott et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scott et al\\.", "year": 2009}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "Learning options in reinforcement learning", "author": ["M. Stolle", "D. Precup"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Stolle and Precup,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup", "year": 2002}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Approximate planning in POMDPs with macro-actions", "author": ["G. Theocharous", "L. Kaelbling"], "venue": "In Advances in Neural Processing Information Systems (NIPS)", "citeRegEx": "Theocharous and Kaelbling,? \\Q2003\\E", "shortCiteRegEx": "Theocharous and Kaelbling", "year": 2003}, {"title": "On the central moments of the multidimensional Gaussian distribution", "author": ["K. Triantafyllopoulos"], "venue": "The Mathematical Scientist,", "citeRegEx": "Triantafyllopoulos,? \\Q2003\\E", "shortCiteRegEx": "Triantafyllopoulos", "year": 2003}, {"title": "Dynamic generalized linear models and Bayesian forecasting", "author": ["M. West", "P. Harrison", "H. Migon"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "West et al\\.,? \\Q1985\\E", "shortCiteRegEx": "West et al\\.", "year": 1985}, {"title": "Open-loop plans in multi-robot POMDPs", "author": ["C. Yu", "J. Chuang", "B. Gerkey", "G. Gordon", "A. Ng"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 38, "context": "Symbolic Perseus (Poupart, 2005) was used to find a good solution to a hand-washing domain with 11 state variables, but each variable took on a relatively small number of values (at most 10 values).", "startOffset": 17, "endOffset": 32}, {"referenceID": 38, "context": "Symbolic Perseus (Poupart, 2005) was used to find a good solution to a hand-washing domain with 11 state variables, but each variable took on a relatively small number of values (at most 10 values). Recently online forward search approaches have been used to achieve encouraging performance on some large POMDPs, such as the work by Ross, Chaib-draa and Pineau (2008b) and Paquet, Tobin and Chaibdraa (2005).", "startOffset": 18, "endOffset": 369}, {"referenceID": 38, "context": "Symbolic Perseus (Poupart, 2005) was used to find a good solution to a hand-washing domain with 11 state variables, but each variable took on a relatively small number of values (at most 10 values). Recently online forward search approaches have been used to achieve encouraging performance on some large POMDPs, such as the work by Ross, Chaib-draa and Pineau (2008b) and Paquet, Tobin and Chaibdraa (2005). However, the cost of performing a generic forward search scales exponentially with the search horizon.", "startOffset": 18, "endOffset": 408}, {"referenceID": 44, "context": "Many POMDP solvers, such as those by Smith and Simmons (2005), Porta, Vlassis, Spaan, and Poupart (2006) and Kurniawati, Hsu, and Lee (2008), perform POMDP planning offline by calculating a value function over the belief space V : b \u2192 R.", "startOffset": 37, "endOffset": 62}, {"referenceID": 38, "context": "Many POMDP solvers, such as those by Smith and Simmons (2005), Porta, Vlassis, Spaan, and Poupart (2006) and Kurniawati, Hsu, and Lee (2008), perform POMDP planning offline by calculating a value function over the belief space V : b \u2192 R.", "startOffset": 90, "endOffset": 105}, {"referenceID": 38, "context": "Many POMDP solvers, such as those by Smith and Simmons (2005), Porta, Vlassis, Spaan, and Poupart (2006) and Kurniawati, Hsu, and Lee (2008), perform POMDP planning offline by calculating a value function over the belief space V : b \u2192 R.", "startOffset": 90, "endOffset": 141}, {"referenceID": 30, "context": "Borrowing the notion of sub-goal states from the fully-observable planning literature (McGovern, 1998; Stolle & Precup, 2002), PUMA uses a heuristic that macro-actions can be designed to take the agent, under the fully-observable model, from a possible start state under the current belief to a sub-goal state.", "startOffset": 86, "endOffset": 125}, {"referenceID": 19, "context": "When the state-transition and observation models are normally distributed and linear functions of the state, the Kalman filter (1960) provides a closed-form solution for the posterior belief over states, N (\u03bct,\u03a3t) given a prior belief over states, N (\u03bct\u22121,\u03a3t\u22121), \u03bct = A\u03bct\u22121 +Bat \u03bct = \u03bct +Kt(zt \u2212 C\u03bct) (7) \u03a3t = A\u03a3t\u22121A T + P \u03a3t = (C QC +\u03a3 \u22121 t ) , (8) where N (f, F ) is a D-dimensional Gaussian with mean f and covariance matrix F , Kt = \u03a3tC T (C\u03a3tC T +Q)\u22121 is the Kalman gain and \u03bct and \u03a3t are the mean and covariance after an action is taken but before incorporating the measurement.", "startOffset": 113, "endOffset": 134}, {"referenceID": 1, "context": "In this section we briefly describe an extension to the PBD algorithm to handle a wider class of observation models, namely parametric models that are members of the exponential family of distributions (Barndorff-Nielsen, 1979).", "startOffset": 202, "endOffset": 227}, {"referenceID": 1, "context": "In this section we briefly describe an extension to the PBD algorithm to handle a wider class of observation models, namely parametric models that are members of the exponential family of distributions (Barndorff-Nielsen, 1979). For non-linear transition models, there exist techniques such as the extended Kalman filter to approximate the posterior with a Gaussian; however, we do not formally consider incorporating such techniques into our PBD algorithm here. We choose to consider exponential family observation models since this family includes a wide array of distributions, such as Gaussian, Bernoulli, and Poisson distributions, and has certain appealing mathematical properties. In particular, we leverage work by West, Harrison and Migon (1985) who constructed linear-Gaussian models that approximate the non-Gaussian exponential family observation model in the neighborhood of the conditional mode, st|zt.", "startOffset": 203, "endOffset": 755}, {"referenceID": 50, "context": "The first two moments of the distribution (West et al., 1985) are", "startOffset": 42, "endOffset": 61}, {"referenceID": 29, "context": "McAllester and Singh (1999) extended these ideas to POMDPs, showing that similar bounds on the calculated values of a POMDP belief state could be computed if a sufficient number of observations were sampled, and forward search was computed out to a sufficiently large horizon.", "startOffset": 0, "endOffset": 28}, {"referenceID": 49, "context": "From the work by Triantafyllopoulos (2003) we know that if Nr is odd, the central Nr-th moments are zero, and if Nr is even (Nr = 2k) any Nr-th", "startOffset": 17, "endOffset": 43}, {"referenceID": 15, "context": "The proof of this is a simple application of Hoeffding\u2019s inequality (1963). If Ns is set such that the estimated reward of each primitive action is \u01eb L close to the true expected primitive action reward with probability at least 1\u2212 \u03b4 \u01eb , then the triangle inequality and union bound guarantee that the expected reward of the entire length-L macro-action is \u01eb-close to the true expected reward for the macro-action with probability at least 1 \u2212 \u03b4.", "startOffset": 45, "endOffset": 75}, {"referenceID": 24, "context": "Specifically, we modified the state-of-the-art POMDP planner SARSOP (Kurniawati et al., 2008) algorithm from the Approximate POMDP Planning (APPL) Toolkit and incorporated macro-actions to guide the sampling of belief points that are used for the point-based value backups.", "startOffset": 68, "endOffset": 93}, {"referenceID": 23, "context": "This implementation is also a modified version of the MiGS (Kurniawati et al., 2009) by the same authors.", "startOffset": 59, "endOffset": 84}, {"referenceID": 45, "context": "2 Rocksample The scientific exploration ROCKSAMPLE problem is a benchmark POMDP problem proposed by Smith and Simmons (2005), and subsequently extended to the FieldVisionRockSample (FVRS)", "startOffset": 100, "endOffset": 125}, {"referenceID": 41, "context": "For a formal discussion of the differences between the offline point-based and online forward search POMDP algorithms, we refer the reader to the survey paper by Ross et al. (2008a). 17.", "startOffset": 162, "endOffset": 182}, {"referenceID": 40, "context": "problem by Ross and Chaib-draa (2007). Initial experiments in these domains revealed that searching only to a shallow depth was sufficient to obtain good policies.", "startOffset": 11, "endOffset": 38}, {"referenceID": 24, "context": "As the ROCKSAMPLE family of problems originates from the POMDP literature, we compared our macro-action algorithms to existing state-of-the-art POMDP solvers: the fast upper-bound of QMDP (Littman, Cassandra, & Kaelbling, 1995), the point-based offline value-iteration techniques HSVI2 (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008), as well as RTBSS (Paquet, Chaib-draa, & Ross, 2006), an online, factored, forward search algorithm.", "startOffset": 321, "endOffset": 346}, {"referenceID": 24, "context": "While it is beyond the scope of this paper to provide a comprehensive survey of POMDP techniques, pointbased methods such as HSVI2 (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) are often considered state-of-the-art offline methods, leveraging the piece-wise and convex aspects of the value function to perform value updates at selected beliefs.", "startOffset": 166, "endOffset": 191}, {"referenceID": 37, "context": "These approaches assume a discretestate representation, but offline approaches that use parametric representations have been proposed for continuous-valued state spaces (Brooks, Makarenko, Williams, & Durrant-Whyte, 2006; Brunskill, Kaelbling, Lozano-Perez, & Roy, 2008; Porta et al., 2006).", "startOffset": 169, "endOffset": 290}, {"referenceID": 47, "context": "Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997).", "startOffset": 144, "endOffset": 165}, {"referenceID": 12, "context": "Hoey and Poupart (2005) have also addressed continuous observation spaces by finding lossless partitions of the observation space.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains.", "startOffset": 15, "endOffset": 40}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al.", "startOffset": 15, "endOffset": 517}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al.", "startOffset": 15, "endOffset": 1153}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al.", "startOffset": 15, "endOffset": 1177}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al.", "startOffset": 15, "endOffset": 1214}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al.", "startOffset": 15, "endOffset": 1241}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al.", "startOffset": 15, "endOffset": 1275}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.", "startOffset": 15, "endOffset": 1304}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs.", "startOffset": 15, "endOffset": 1336}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al.", "startOffset": 15, "endOffset": 1443}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al. (2007) provide a method for automatically discovering a problem hierarchy.", "startOffset": 15, "endOffset": 1558}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al. (2007) provide a method for automatically discovering a problem hierarchy. Yu, Chuang, Gerkey, Gordon and Ng (2005) provide an optimal algorithm for planning if no observations were available.", "startOffset": 15, "endOffset": 1667}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al. (2007) provide a method for automatically discovering a problem hierarchy. Yu, Chuang, Gerkey, Gordon and Ng (2005) provide an optimal algorithm for planning if no observations were available. Foka and Trahanias\u2019s (2007) solution involves building a hierarchy of nested representations and solutions.", "startOffset": 15, "endOffset": 1772}, {"referenceID": 4, "context": "Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use tabular representations of the value function may also be competitive with prior point-based approaches which used \u03b1-vector representations, and this alternate representation may be useful for continuous domains. The ideas in this paper are more closely related to the body of online, forward search POMDP techniques that only compute an action for the current belief, which were recently surveyed by Ross et al. (2008a). Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as \u201coptions\u201d (Sutton et al., 1999), or posed as part of a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of macro-actions could be incorporated into our approach. Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.\u2019s PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al. (2007) provide a method for automatically discovering a problem hierarchy. Yu, Chuang, Gerkey, Gordon and Ng (2005) provide an optimal algorithm for planning if no observations were available. Foka and Trahanias\u2019s (2007) solution involves building a hierarchy of nested representations and solutions. Their focus is on discrete-state problems, particularly navigation applications. Theocharous and Kaelbling\u2019s (2003) discrete-state reinforcement learning approach samples observation trajectories and solves for the expected reward of a discrete", "startOffset": 15, "endOffset": 1968}, {"referenceID": 21, "context": "Kurniawati et al. (2009) recently used macroactions to guide the sampling of belief points for use in an offline point-based POMDP solver.", "startOffset": 0, "endOffset": 25}, {"referenceID": 21, "context": "Kurniawati et al. (2009) recently used macroactions to guide the sampling of belief points for use in an offline point-based POMDP solver. However, these prior macro-action POMDP approaches compute a value function off-line, are not aimed at scaling to very large domains, and will struggle in the environments considered in this paper. An exception to this is the work by Hsiao and colleagues (2008, 2010) who used a form of macro-actions for those robot manipulation tasks that involve a large state space. The focus of their work is on robust manipulation under uncertainty, and their work only considers a very short horizon of action trajectories. Except for the work by Kurniawati et al. (2009), all these macroaction POMDP approaches, like our PBD algorithm, assume the macro-actions are provided by a domain expert.", "startOffset": 0, "endOffset": 701}, {"referenceID": 21, "context": "Kurniawati et al. (2009) recently used macroactions to guide the sampling of belief points for use in an offline point-based POMDP solver. However, these prior macro-action POMDP approaches compute a value function off-line, are not aimed at scaling to very large domains, and will struggle in the environments considered in this paper. An exception to this is the work by Hsiao and colleagues (2008, 2010) who used a form of macro-actions for those robot manipulation tasks that involve a large state space. The focus of their work is on robust manipulation under uncertainty, and their work only considers a very short horizon of action trajectories. Except for the work by Kurniawati et al. (2009), all these macroaction POMDP approaches, like our PBD algorithm, assume the macro-actions are provided by a domain expert. In the sensor resource management domain, planning under uncertainty techniques are used in the context of planning sensor placements to track single or multiple targets. Existing algorithms often adopt a myopic, or greedy strategy when it comes to planning (Krause & Guestrin, 2007), but notable exceptions include the work by Scott et al. (2009) and Kreucher, Hero III, Kastella, and Chang (2004).", "startOffset": 0, "endOffset": 1172}, {"referenceID": 21, "context": "Kurniawati et al. (2009) recently used macroactions to guide the sampling of belief points for use in an offline point-based POMDP solver. However, these prior macro-action POMDP approaches compute a value function off-line, are not aimed at scaling to very large domains, and will struggle in the environments considered in this paper. An exception to this is the work by Hsiao and colleagues (2008, 2010) who used a form of macro-actions for those robot manipulation tasks that involve a large state space. The focus of their work is on robust manipulation under uncertainty, and their work only considers a very short horizon of action trajectories. Except for the work by Kurniawati et al. (2009), all these macroaction POMDP approaches, like our PBD algorithm, assume the macro-actions are provided by a domain expert. In the sensor resource management domain, planning under uncertainty techniques are used in the context of planning sensor placements to track single or multiple targets. Existing algorithms often adopt a myopic, or greedy strategy when it comes to planning (Krause & Guestrin, 2007), but notable exceptions include the work by Scott et al. (2009) and Kreucher, Hero III, Kastella, and Chang (2004). Kreucher et al.", "startOffset": 0, "endOffset": 1223}, {"referenceID": 21, "context": "Kreucher et al. describe a multi-target tracking problem, where non-myopic sensor management is necessary for multi-target tracking. The authors use a particle filter approach to represent the agent\u2019s belief of the target\u2019s location, and seek to find paths that will result in the greatest KL divergence in density before and after the measurement. To look ahead more than one action, this algorithm uses Monte Carlo sampling to generate possible observation outcomes. They also provide an information-directed path searching scheme to reduce the complexity of the Monte Carlo sampling, as well as value heuristics that will help direct the search. It is possible that some of their insights could be used in combination with our macro-action formulation to strengthen both approaches. In the experimental section we compared our approach to the work by Scott et al. (2009), who directly formulated target tracking as a POMDP, and proposed the Nominal Belief Optimization (NBO) algorithm that computes the most likely belief after an action for deeper forward search.", "startOffset": 0, "endOffset": 874}, {"referenceID": 21, "context": "Kreucher et al. describe a multi-target tracking problem, where non-myopic sensor management is necessary for multi-target tracking. The authors use a particle filter approach to represent the agent\u2019s belief of the target\u2019s location, and seek to find paths that will result in the greatest KL divergence in density before and after the measurement. To look ahead more than one action, this algorithm uses Monte Carlo sampling to generate possible observation outcomes. They also provide an information-directed path searching scheme to reduce the complexity of the Monte Carlo sampling, as well as value heuristics that will help direct the search. It is possible that some of their insights could be used in combination with our macro-action formulation to strengthen both approaches. In the experimental section we compared our approach to the work by Scott et al. (2009), who directly formulated target tracking as a POMDP, and proposed the Nominal Belief Optimization (NBO) algorithm that computes the most likely belief after an action for deeper forward search. In contrast, our algorithm explicitly computes the entire set of possible posterior beliefs after a macro-action. Recently two groups (Erez & Smart, 2010; Platt, Tedrake, LozanoPerez, & Kaelbling, 2010) have independently proposed an approach that lies in the middle of this spectrum: beliefs are updated by assuming that the most likely observation is received, but the variance is increased. In contrast, our approach represents that each resulting belief may be fairly peaked, but the mean of the beliefs may be spread out. This more complete representation may be advantageous if there are sharp changes in the reward function. As stated in the introduction, the finite-horizon forward search, act, and re-plan strategy PBD follows can be seen as an instance of the Model Predictive Control/Receding Horizon Control (MPC/ RHC) framework from the controls community. Examples of MPC and RHC include the work by Kuwata and How (2004), Bellingham, Richards, and How (2002), and Richards, Kuwata, and How (2003).", "startOffset": 0, "endOffset": 2004}, {"referenceID": 21, "context": "Kreucher et al. describe a multi-target tracking problem, where non-myopic sensor management is necessary for multi-target tracking. The authors use a particle filter approach to represent the agent\u2019s belief of the target\u2019s location, and seek to find paths that will result in the greatest KL divergence in density before and after the measurement. To look ahead more than one action, this algorithm uses Monte Carlo sampling to generate possible observation outcomes. They also provide an information-directed path searching scheme to reduce the complexity of the Monte Carlo sampling, as well as value heuristics that will help direct the search. It is possible that some of their insights could be used in combination with our macro-action formulation to strengthen both approaches. In the experimental section we compared our approach to the work by Scott et al. (2009), who directly formulated target tracking as a POMDP, and proposed the Nominal Belief Optimization (NBO) algorithm that computes the most likely belief after an action for deeper forward search. In contrast, our algorithm explicitly computes the entire set of possible posterior beliefs after a macro-action. Recently two groups (Erez & Smart, 2010; Platt, Tedrake, LozanoPerez, & Kaelbling, 2010) have independently proposed an approach that lies in the middle of this spectrum: beliefs are updated by assuming that the most likely observation is received, but the variance is increased. In contrast, our approach represents that each resulting belief may be fairly peaked, but the mean of the beliefs may be spread out. This more complete representation may be advantageous if there are sharp changes in the reward function. As stated in the introduction, the finite-horizon forward search, act, and re-plan strategy PBD follows can be seen as an instance of the Model Predictive Control/Receding Horizon Control (MPC/ RHC) framework from the controls community. Examples of MPC and RHC include the work by Kuwata and How (2004), Bellingham, Richards, and How (2002), and Richards, Kuwata, and How (2003).", "startOffset": 0, "endOffset": 2042}, {"referenceID": 21, "context": "Kreucher et al. describe a multi-target tracking problem, where non-myopic sensor management is necessary for multi-target tracking. The authors use a particle filter approach to represent the agent\u2019s belief of the target\u2019s location, and seek to find paths that will result in the greatest KL divergence in density before and after the measurement. To look ahead more than one action, this algorithm uses Monte Carlo sampling to generate possible observation outcomes. They also provide an information-directed path searching scheme to reduce the complexity of the Monte Carlo sampling, as well as value heuristics that will help direct the search. It is possible that some of their insights could be used in combination with our macro-action formulation to strengthen both approaches. In the experimental section we compared our approach to the work by Scott et al. (2009), who directly formulated target tracking as a POMDP, and proposed the Nominal Belief Optimization (NBO) algorithm that computes the most likely belief after an action for deeper forward search. In contrast, our algorithm explicitly computes the entire set of possible posterior beliefs after a macro-action. Recently two groups (Erez & Smart, 2010; Platt, Tedrake, LozanoPerez, & Kaelbling, 2010) have independently proposed an approach that lies in the middle of this spectrum: beliefs are updated by assuming that the most likely observation is received, but the variance is increased. In contrast, our approach represents that each resulting belief may be fairly peaked, but the mean of the beliefs may be spread out. This more complete representation may be advantageous if there are sharp changes in the reward function. As stated in the introduction, the finite-horizon forward search, act, and re-plan strategy PBD follows can be seen as an instance of the Model Predictive Control/Receding Horizon Control (MPC/ RHC) framework from the controls community. Examples of MPC and RHC include the work by Kuwata and How (2004), Bellingham, Richards, and How (2002), and Richards, Kuwata, and How (2003). A special case of RHC control is Certainty Equivalence Control, or CEC (see Bertsekas, 2007 for an overview).", "startOffset": 0, "endOffset": 2080}], "year": 2011, "abstractText": "Deciding how to act in partially observable environments remains an active area of research. Identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states. Towards addressing this challenge, we present an online, forward-search algorithm called the Posterior Belief Distribution (PBD). PBD leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken, given the set of observation sequences that could be received during this process. This method allows us to efficiently evaluate the expected reward of a sequence of primitive actions, which we refer to as macro-actions. We present a formal analysis of our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment, which suggests that our approach has practical potential for planning in real-world, large partially observable domains where a multi-step lookahead is required to achieve good performance.", "creator": null}}}