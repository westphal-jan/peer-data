{"id": "1401.3831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "An Investigation into Mathematical Programming for Finite Horizon Decentralized POMDPs", "abstract": "Decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach, mainly through the framework of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS are a general and powerful modeling tool, solving them is a task with an overwhelming complexity that can be doubly exponential. In this paper, we study an alternate formulation of DEC-POMDPs relying on a sequence-form representation of policies. From this formulation, we show how to derive Mixed Integer Linear Programming (MILP) problems that, once solved, give exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be derived either by using some combinatorial characteristics of the optimal solutions of the DEC-POMDPs or by using concepts borrowed from game theory. Through an experimental validation on classical test problems from the DEC-POMDP literature, we compare our approach to existing algorithms. Results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except for some particular problems. The main contributions of this work are the use of mathematical programming for DEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions. Besides, we argue that our alternate representation of DEC-POMDPs could be helpful for designing novel algorithms looking for approximate solutions to DEC-POMDPs.", "histories": [["v1", "Thu, 16 Jan 2014 04:49:14 GMT  (445kb)", "http://arxiv.org/abs/1401.3831v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raghav aras", "alain dutech"], "accepted": false, "id": "1401.3831"}, "pdf": {"name": "1401.3831.pdf", "metadata": {"source": "CRF", "title": "An Investigation into Mathematical Programming for Finite Horizon Decentralized POMDPs", "authors": ["Raghav Aras", "Alain Dutech"], "emails": ["raghav.aras@gmail.com", "alain.dutech@loria.fr"], "sections": [{"heading": null, "text": "The most important contributions of this paper are the use of mathematical programming for DECPOMDPs and a better understanding of DEC-POMDPs and their solutions. Furthermore, we argue that our alternative representation of DEC-POMDPs may be helpful to develop new algorithms that look for approximate solutions for DEC-POMDPs."}, {"heading": "1. Introduction", "text": "Solving DEC-POMDPs is an impossible task since they belong to the class of NEXP-complete problems (see Section 1.1). In this paper, DEC-POMDPs are reformulated into sequential DEC-POMDPs to derive mixed integer linear programs, which can be solved with very efficient solvers, in order to design exact optimal solutions for DEC-POMDPs with limited horizons. Our main motivation is to investigate the benefits and limitations of this novel approach and to gain a better understanding of DEC-POMDPs (see Section 1.2). On a practical level, we provide new algorithms and heuristics for solving DEC-POMDPs and evaluate them against classical problems (see Section 1.3)."}, {"heading": "1.1 Context", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "1.2 Motivations", "text": "The main objective of our work is to investigate the use of mathematical programs, especially mixed-integer programs (MILP) (Diwekar, 2008), for the solution of mixed-integer programs. Our motivation is based on the fact that the field of linear programming is fairly mature and of great interest to the industry. As a result, there are many efficient solutions for mixed-integer programs and we want to see how these efficient solutions work within the framework of DEC-POMDPs."}, {"heading": "1.3 Contributions", "text": "This year, it has come to the point that there is only one time that there is such a process, in which there is such a process."}, {"heading": "1.4 Overview of this Article", "text": "The remainder of this article is structured as follows: Section 2 introduces the formalism of the DEC-POMDP and explains some background information on the classical algorithms that are normally based on dynamic programming; then we explain our reformulation of the DEC-POMDP in sequence form in Section 3, where we also define various terms required for the sequence form; and in Section 4, we show how we use combinatorial properties of sequence formulation policy to derive a first mixed whole-number linear program (MILP, in Table 3) to solve the DEC-POMDP. By using game theory concepts such as the Nash balance, we are inspired by previous work on games in a comprehensive form to design two more MILPs to solve the DEC-POMDP (Tables 4, 5). These MILPs are smaller and their detailed derivatives are presented in Section 5. Our contributory hypotheses to the acceleration of the POILPs of the various practical MILD solutions are presented in the MILD analysis section."}, {"heading": "2. Dec-POMDP", "text": "This section gives a formal definition of the Decentralized Partially Observed Markov Decision Processes as introduced by Bernstein et al. (2002). As described, a DECPOMDP solution is a policy that is defined on the space of information sets and has an optimal value. This section ends with a quick overview of the classic methods that have been developed to solve DEC-POMDPs."}, {"heading": "2.1 Formal Definition", "text": "A DEC-POMDP is defined as a tuple D = < I, S, {Ai}, P = 1, {Oi}, G, R, T, \u03b1 >, where: \u2022 I = {1, 2, \u00b7, n} is a set of agents. \u2022 S is a finite set of arguments. The series of probability distributions over S is denoted by (S). Members of (S) are denoted as states of belief. \u2022 For each agent i, I, Ai is a set of actions. A = \u00b7 i, IAi denotes the set of common actions. \u2022 P: S \u00b7 A \u00b7 S \u2192 S \u2192 [0, 1] is a state transitional function. For each agent i, I, Ai \"is the probability that the state of the problem is observed in a period of time."}, {"heading": "2.2 Example of DEC-POMDP", "text": "The problem known as the \"decentralized tiger problem\" (hereby referred to as MA-Tiger), introduced by Nair, Tambe, Yokoo, Pynadath and Marsella (2003), was widely used to test DEC-POMDP's algorithms. It is a variation of a problem previously introduced for POMDPs (i.e., DEC-POMDPs with an agent) by Kaelbling, Littman and Cassandra (1998).In this problem we are confronted with two agents with two closed doors. Behind one door is a tiger, behind the other an escape route. Agents do not know which door leads to what. Each agent, independently of the other, can open one of the two doors or listen carefully to recognize the tiger. If one of them opens the wrong door, the life of both is hindered by the other."}, {"heading": "2.3 Information Sets and Histories", "text": "An information collection of Agent i is a sequence (a1.o2.a2.o3 \u00b7 \u00b7 \u00b7.ot) of equal length in which the elements in odd positions are actions of the Agent (members of Ai) and those in even positions are observations of the Agent (members of Oi). An information collection of length 0 is called a zero information set, designated as a sequence (a1.o2. a2. o3 \u00b7 \u00b7.ot.at) of odd length in which the elements in odd positions are actions of the Agent (members of Ai) and those in even positions are observations of Agent i (members of Oi). We define the length of a story as zero."}, {"heading": "2.4 Policies", "text": "This choice may be based on the knowledge of the past and present that the actor has about the process at the moment. One possibility is to define an individual policy of the actor i as an illustration of sets of information about actions. More formally, the series of pure actions for the actor i is referred to as such. Pure policies can usually be defined by trajectories of past observations, since measures that are deterministically selected can be reconstructed from the observations. \u2022 Mixed policies are a probability distribution over the group of pure policies. Pure politics can also be defined by trajectories of past observations. < A mixed policy is a probability distribution over the group of pure policies. Thus, an actor that uses a mixed policy controls the DEC-POMDP by using a pure policy that is randomly selected from a series of pure policies. < A chastic policy is a probability distribution over the group of pure policies."}, {"heading": "2.5 Value Function", "text": "When executed by the agents, each T horizontal action generates a probability distribution over the possible consequences of the reward, from which the value of the policy can be calculated according to Equation 1. Thus, the value of the common policy \u03c0 is formally defined as: V (\u03b1, \u03c0) = E [T \u00b2 t = \u00b7 \u00b7 \u00b7 1R (st, at). (4) This definition requires some concepts that we are now introducing. (S) There is a recursive definition of the value function of a policy, which is also a way to calculate it when the horizon T is finite. (4) This definition requires some concepts that we are introducing now. In view of a state of belief \u03b2 (S), a common action a and a common observation o (o). (o | o) denotes the probability that the agents will receive a common observation o, o). (o) denotes the probability that the agents will receive a common observation o, o if they perform a common action within a period of time."}, {"heading": "2.6 Overview of DEC-POMDPs Solutions and Limitations", "text": "As detailed in the work of Oliehoek et al. (2008), existing methods for solving DECPOMDPs with finite horizons belong to several large families: \"brute force,\" alternately maximization, search algorithms and dynamic programming. Brute Force The simplest approach to solving a DEC-POMDP is to list and evaluate all possible common strategies in order to find the optimal one. However, such a method quickly becomes irrevocable, since the number of common strategies is twice exponential on the horizon of the problem. Alternatively, maximization by Chade, Scherrer, and Charpillet (2002) and Nair et al. (2003), a possible way to solve DEC-POMDPs is for each agent (or any small group of agents) to look alternatively for a better policy while all other agents freeze their own policies."}, {"heading": "3. Sequence-Form of DEC-POMDPs", "text": "This section presents the basic concept of policy in \"sequence form.\" Thus, a reformulation of a DEC-POMDP is possible, and this leads to a non-linear program (NLP), the solution of which defines an optimal solution for the DEC-POMDP."}, {"heading": "3.1 Policies in Sequence-Form", "text": "A history function p of an agent i is a mapping from the series of stories to the interval [0, 1]. The value p (h) is the weight of history h for the history function p < a policy \u03c0i defines a probability function over the series of stories of the agent i by saying that for each story hi of Hi, p (hi) the conditional probability of hi in the face of an observation sequence (o0i.o 1 i. \u00b7 \u00b7 \u00b7 \u00b7.o t i) and \u03c0i. If each policy defines a political function, not every political function can be associated with a valid policy. Some limitations must be fulfilled. Indeed, a history function p is a sequence form policy for the agent i if the following limitations are fulfilled: A policy (a) = 1, (13) \u2212 p (h) \u2212 p (h) form of politics is another form of politics. (h.o.a) The history function p is a sequence form for the agent ii."}, {"heading": "3.2 Policy Constraints", "text": "D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "3.3 Sequence-Form of a DEC-POMDP", "text": "We are now able to give a formula of a DEC-POMDP based on the use of sequence form policies. We would like to emphasize that this is only a reformulation, but as such will offer us new ways of solving DEC-POMDP with mathematical programming. If a \"classic\" formula of a DEC-POMDP is given (see Section 2.1), the DEC-POMDP equivalent sequence form is a tuple < I, {Hi}, preservation, R >, where: \u2022 I = {1, \u00b7, n} is a series of agents. \u2022 For each agent, the sequence form of DEC-POMDP is the sequence capability smaller than or equal to T for agent i, as defined in the previous section. Each Hi sentence is derived using the sets Ai and Oi. \u2022 This theorem is the common history conditional probability function."}, {"heading": "3.4 Non-Linear Program for Solving DEC-POMDPs.", "text": "The solution of such a problem boils down to the fact that the formulation of such an NLP can be found in the annexes. It is given for the decentralized problem with 2 agents and a horizon of 2 agents. The limitations of the program form a convective set, but the objective function is not able to find in the annexes. It is given for the decentralized problem with 2 agents and a horizon of 2 agents."}, {"heading": "4. From Combinatorial Considerations to Mathematical Programming", "text": "This section explains how it is possible to use combinatorial properties of DEC-POMDPs to convert the previous NLP into a linear mixed integer program. As shown, this mathematical program belongs to the family of 0-1 Mixed Integer Linear Programs, which means that some variables of this linear program must take integer values in the set {0, 1}."}, {"heading": "4.1 Linearization of the Objective Function", "text": "Drawing on ideas from the field of quadratic assignment problems (Papadimitriou & Steiglitz, 1982), we transform the non-linear objective function of the previous NLP into a linear objective function and linear constraints with new variables z, which must take integer values. The variable z (j) represents the product of the xi (ji) variable. (32) The objective function that was: maximized, j2, \u00b7 \u00b7, jn >. We must ensure that there is a duplicate mapping between the value of the new variable z and the x variables, which is the same for any solution of the mathematical program. (32) The objective function that is j1, j2, \u00b7, jn >."}, {"heading": "4.2 Fewer Integer Variables", "text": "The linearization of the objective function is based on the fact that we are dealing with pure policies, which means that each x and z variable is either 0 or 1. As in Section 3.2, we can relax most x variables and allow them to take non-negative values, provided that the x values for final stories are limited to integer values. Furthermore, it is important to reduce the number of integer variables in our mathematical program, as demonstrated by the following dilemma, these constraints on x also guarantee that z variables only have their value in Section 3.1. Finally, we end with the following linear program with real and integer variables, i.e. called a 0-1 mixed integer linear designated program (MILP)."}, {"heading": "4.3 Summary", "text": "As Theorem 4.1 proves, the solution of this MILP defines an optimal common policy for the DEC-POMDP. Nevertheless, this MILP is quite large, with O (kT) constraints and [i | Hi | +] i | egg | = O (k nT) variables, O (kT) of these variables must take integer values. In the next section, another method for linearising the NLP is described, which leads to a \"smaller\" mathematical program for the 2-agent case. Theorem 4.1."}, {"heading": "T -period optimal joint policy in sequence-form.", "text": "Proof: Due to the political constraints and domain limitations of each individual agent, each x \u0445 i is a pure sequence form policy of the agent i. Due to the constraints (41) - (42), each z * value is 1 if and only if the product has the value 1. By maximizing the objective function, we then effectively maximize the value of the sequence form policy < x * 1, x * 2, \u00b7 \u00b7 \u00b7, x * n >. Thus, < x * 1, x * 2, \u00b7, x * n > is an optimal common policy of the original DEC-POMDP."}, {"heading": "5. From Game-Theoretical Considerations to Mathematical", "text": "Programmers In this section, concepts such as \"Nash balance\" and \"remorse\" are taken from game theory to create another 0-1 Mixed Integer Linear Program to solve DEC POMDPs. In fact, two MILPs are designed, one that can only be used for 2 agents and the other for any number of agents, the main objective of this part being to derive a smaller mathematical program for the case of 2 agents. Indeed, MILP-2 agents (see Table 4) have slightly fewer variables and limitations than MILP (see Table 3) and may therefore prove to be easier to solve. On the other hand, the new derivative leads to a MILP that is given only for the sake of completeness, since it is larger than MILP. Connections between the areas of multi-layer systems and game theory are numerous in the literature (see, for example, Sandholm, 1999; Parsons & Wooldridge, 2002)."}, {"heading": "5.1 Nash Equilibrium", "text": "A Nash Equilibrium is a common policy in which each policy is the best response to the reduced common policy formed by the other common policy policies. In the context of a DEC-POMDP sequence, a common policy is the best response to an i-reduced common policy. (59) A common policy is a Nash Equilibrium if there is a common policy. (64) A common policy is the best response to a common policy. (64) A common policy is a common policy. (64) A common policy. (64) A common policy. (64) A common policy. (64) A common policy. (64) A common policy. (64) A common policy. (64)"}, {"heading": "5.2 Dealing with Complementarity Constraints", "text": "(c) (a) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1)) (1) (1) (1) (1) (1) (1)) (1) (1) (1))) (1) (1) ("}, {"heading": "5.3 Program for 2 Agents", "text": "If we combine the political constraints (Section 3.2), we can consider the constraints we have just seen to be the best answer for a policy (Section 5.1, 5.2) and maximize the value of the common policy. (Section 3.2) Formulating the common tiger problem for 2 agents and for a horizon of 2 is an optimal common policy for 2 agents. (Section E.4The variables of the program are the vectors xi, wi, bi and yi for each agent. (Sample The formulation of the decentralized tiger problem for 2 agents and for a horizon of 2 can be found in the appendices, in Section E.4The variables of the program are the vectors xi, bi and yi for each agent. Sample The formulation h of agent i I and for each story h of agent i, Ui (h) denotes the upper limit of regretting history h.x x Solving politics (x, y.)"}, {"heading": "5.4 Program for 3 or More Agents", "text": "If the number of agents is more than 2, the restriction (86) of the non-linear program (82) - (90) is no longer a complementarity restriction between 2 variables that could be linearized as before. Specifically, the term \"k\" of the restriction (86) contains as many variables as there are different agents. To be able to linearize this term, we again limit ourselves to pure common strategies and use some combinatorial facts about the number of stories involved. This leads to the mixed linear 0-1 program called \"MILP-n agents,\" which is shown in Table 5.The variables of the program \"MILP-n agents\" are the vectors \"xi,\" \"bi\" and \"yi\" for each agent i \"and the vector e.g. We have the following result:\" Theorem \"x.\" In the face of a solution (x, \"w,\" y, \"b,\" z \") are the agents\" i. \""}, {"heading": "5.5 Summary", "text": "Formulating the solution of a DEC-POMDP and applying the duality theorem to linear programs allows us to formulate the solution of a DEC-POMDP as a solution of a new type of 0-1 MILP. For 2 agents, this MILP has \"only\" O (kT) variables and limitations and is therefore \"smaller\" than the MILP of the previous section. Nevertheless, all these MILPS are quite large, and the next section explores heuristic ways to accelerate their resolution."}, {"heading": "6. Heuristics for Speeding up the Mathematical Programs", "text": "This section focuses on ways to accelerate the resolution of the various MILPs presented so far, using two ideas: First, we show how to trim the set of sequential strategies by removing stories that are demonstrably not part of the optimal common policy, called \"local strangers,\" and then we give some upper and lower limits to the objective function of MILPs. These limits can sometimes be used in the \"branch-and-bound\" method commonly used by MILP solvers to finalize the values of integer variables."}, {"heading": "6.1 Locally Extraneous Histories", "text": "A locally alien story is a story that is not required to find an optimal common policy if the original state of the DEC-POMDP is identical with h in all aspects except its last act. If Ai =, c) the only co-story of an agent's story h is defined as the story h of an agent who is identical with h in all aspects except for his last act. If Ai =, c) the only co-story of an agent's story h is defined as the story c.u.b.v.b, the set of co-histories of an agent's story h is designated as identical with C (h) in all aspects. Formally, a story h-Hti of the length of the agent i is designated as locally extraneous if for each probability distribution over the fixed Ht \u2212 i of the i-reduced common story t there is an optimal story h."}, {"heading": "6.2 Cutting Planes", "text": "Another option that directly aims to reduce computational time is the use of intersection planes (Cornue \u0301 jols, 2008). Intersection (Dantzig, 1960) is a particular constraint that identifies a part of the set of feasible solutions where the optimal solution is demonstrably not located. Intersections are used in conjunction with various \"branch and boundary mechanisms\" to reduce the number of possible combinations of integer variables examined by a solution. We will present two types of intersection."}, {"heading": "6.2.1 Upper Bound for the Objective Function", "text": "The first reduction we propose is the upper limit of the POMDP section. The value of an optimal T-q period common policy at \u03b1 for a given DEC-POMDP is limited from the top by the value V-P of an optimal T-period policy at \u03b1 for the POMDP derived from the DEC-POMDP. This derived POMDP is the DEC-POMDP, but assumes a centralized controller (i.e. with only one actor using joint actions).A sequential representation of the POMDP is quite simple. If the upper limit of the POMDP is called HT = 1Ht of the common history of lengths smaller or equal to T and N as the specified limit H-E of the non-terminal common history, then a policy for POMDP with horizon T in sequence form can be a function q from H to [0, 1] then the specification of the upper limit of the POMDP is such that: an optimal limit of the common history of the common history equal to the T and N is equal to the common history of the T and N."}, {"heading": "6.2.2 Lower Bound for the Objective Function", "text": "In the case of DEC POMDPs with non-negative rewards, it is trivial to show that the value of an optimal bottom-down T-period policy is limited by the value of the optimal value of the T-1 horizon. So, in general, we must consider the lowest possible reward to calculate this lower limit, and we can say the following: \u2211 j-ER (\u03b1, j) z (j) \u2265 VT-1 (\u03b1) + min a-A min s-S R (s, a) (157), where VT-1 is the value of the optimal policy with horizon T-1. The reasoning leads to an iterated calculation of DEC POMDPs of a longer and longer horizon, reminiscent of the MAA * algorithm (Szer et al., 2005). Experiments will show whether it is worth solving the larger and larger DEC POMDPs if it is better to tackle the problem of the horizon directly, without taking a smaller step of the PODP."}, {"heading": "6.3 Summary", "text": "Truncating foreign stories locally and using the limits of objective function can be of practical use to the software for solving the MILPs presented in this paper. Truncating the story means that the scope of the strategies used by the MILP is reduced, and since the formulation of the MILP depends on the combinatorial properties of the DEC-POMDP, these MILPs must be changed, as shown in Appendix D. Validity. As far as the cuts are concerned, they do not change the solution found by the MILPs, so a solution for these MILPs is still an optimal solution for the DEC-POMDP. If foreign stories are truncated, at least one valid policy remains as a solution, because in step 3 of the algorithm, a story is only truncated when other co-stories are left. Moreover, this reduced set of stories can still be used to build an optimal policy based on Theroem 6.1."}, {"heading": "7. Experiments", "text": "The mathematical programs and heuristics designed in this thesis are tested against four classical problems found in the literature. For these problems involving two agents, we mainly compared the computation time required to calculate a DEC-POMDP using mixed integer linear programming methods reported for methods found in literature. Then, we randomly designed our programs on three-agent problems.MILP and MILP-2 are solved using the \"iLog Cplex 10\" solver - a commercial set of Java packages - based on a combination of the \"Simplex\" and \"Branch and Bounds\" methods (Fletcher, 1987).The software runs on an Intel P4 with 2Gb RAM using standard configuration parameters. For the mathematical programs, different combinations of hayristics are evaluated."}, {"heading": "7.1 Multi-Access Broadcast Channel Problem", "text": "Multiple versions of the Multi-Access Broadcast Channel (MABC) problem can be found in the literature. We will neither describe Hansen et al. (2004), which allows this problem, as a DEC-POMDP. In the MABC we will get two nodes (computers), which are required to send messages to each other over a common channel for a certain period of time. Time is imagined to be split into discrete time periods. Each node has a buffer with a capacity of one message. A buffer that is empty in a period of time is replenished with a certain probability in the next period. In a period, only one node can send a message. If both nodes send a message during the same period, a collision of messages is not transmitted and no message is transmitted. In the event of a collision, each node is affected by a collision signal."}, {"heading": "7.2 Multi-Agent Tiger Problem", "text": "As explained in section 2.2, the Multi-Agent Tiger Problem (MA Tiger) has been introduced in the work of Nair et al. (2003). From the general description of the problem it is clear that we have a 2-agent, 2-state, 3-action-per-agent, 2-observations-per-agent DEC-POMDP, whose elements are as follows. \u2022 Every person is an agent. \u2022 So we have a 2-agent DEC-POMDP. \u2022 The state of the problem is described by the location of the tiger. Thus, S consists of two states to the left (tiger is located behind the left door) and to the right (tiger is located behind the right door). \u2022 The action table of each agent consists of three actions: Open Left (open the left door) and Listen (listen). \u2022 The observations of each agent consist of two observations: Noise Left (noise from the left door) and Noise Right (noise from the right door)."}, {"heading": "7.3 Fire Fighters Problem", "text": "In fact, most of them will be able to play by the rules they had in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "7.4 Meeting on a Grid", "text": "It was introduced in the work of Bernstein, Hansen and Zilberstein (2005). In this problem we have two robots that navigate on a two-by-two grid world without obstacles. Each robot can only detect if there are walls to its left or to its right, and the goal is for the robots to spend as much time as possible in the same place. Indeed, the actions are to move up, to the left or right, or to stay in the same place. If a robot tries to move into an open square, it only goes in the designated direction with a 0.6 probability, otherwise it either goes in a different direction or stays in the same square. Each step into a wall results in the robots not intervening in the same square and not being able to feel each other. The reward is 1 if the agents divide a square, and 0 otherwise."}, {"heading": "7.5 Random 3-Agent Problems", "text": "To test our approach to problems with 3 agents, we used randomly generated DECPOMDPs, in which the state transition function, the joint observation function, and the reward functions are randomly generated. DEC POMDPs have 2 actions and 2 observations per agent and 50 states. Rewards are randomly generated integers in the range of 1 to 5. The complexity of this problem family is quite similar to the complexity of the MABC problem (see Section 7.1). To assess the \"real\" complexity of this random problem, we first have a two-agent version of the problem for a horizon of 4. The results averaged over 10 runs of the programs are in Table 14. Compared to the MABC problem, which seemed of comparable complexity, the random problem turns out to be easier to solve (120 vs. 900 s). For this problem, the number of 0-1 variables is relatively low, as they are not too strong on the resolution time of the problem during the MILP horizon, and thus the result is weighted three times over the MILP horizon."}, {"heading": "8. Discussion", "text": "We have divided the discussion into two parts: In the first part, we analyze our results and give explanations about the behavior of our algorithms and the usefulness of heuristics. In the second part, we explicitly address some important questions."}, {"heading": "8.1 Analysis of the Results", "text": "This year it will be so far that it will be able to the aforementioned lcihsrcnlrVo rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the nfu the rfu the"}, {"heading": "8.2 Questions", "text": "The mathematical programming approach presented in this paper raises several questions, some of which we have explicitly addressed that seem important to us."}, {"heading": "Q1: Why is the sequence-form approach not entirely doomed by its exponential complexity?", "text": "Since the Number of Sequence Formats Formats Formats Formats FormatPolitics Formats FormatPolitics Formats FormatPolitics Formats FormatPolitics Formats FormatPolitics Formats FormatPolitics Formats FormatPolitics Formats Formats FormatPolitics Formats Formats FormatPolitics Formats Formats FormatPolitics Formats Formats Formats FormatPolitics Formats Formats FormatPolitics Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Politics Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats FormatPolitics Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Politics Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Formats Format"}, {"heading": "Q2: Why does MILP sometimes take so little time to find an optimal joint policy when compared to existing algorithms?", "text": "In fact, most of them will be able to move to a different world in which they are able to live than in a world in which they are able to live and live."}, {"heading": "Q4: What is the main contribution of this paper?", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Q6: How does this help achieve designing artificial autonomous agents ?", "text": "At first glance, our work has no direct and immediate benefits in building artificial intelligence or understanding how intelligence \"works.\" Even in the limited area of multi-agent planning, our contributions are to be found on a theoretical rather than a practical level."}, {"heading": "9. Conclusion", "text": "We have the opportunity to behave in a way that we have experienced in the past, that we have not experienced today."}, {"heading": "Appendix A. Non-Convex Non-Linear Program", "text": "Using the simplest example, this section aims to show that the non-linear program (NLP) expressed in Table 2 cannot be convex. Consider an example of two agents, each with 2 possible actions (a and b), to solve a Horizon 1 decision problem, and the set of possible joint histories is: < a, a >, < a, b >, < b, a > and < b, b >. Then the NLP to solve is: Variables: x1 (a), x1 (b), x2 (a), x2 (a) (a) Maximize R (\u03b1, < a, a >) x1 (a) + R (\u03b1, < b >) + R (< a, b >) + R (< a, b >) x1 (a >) x1 (b) (b) (158) + vector (1, < b) + vector (0) \u2212 v0 (va)."}, {"heading": "Appendix B. Linear Program Duality", "text": "Each linear program (LP) has an inverted linear program called a dual. The first LP is called the primary to distinguish it from its dual. If the primary maximizes a quantity, the dual minimizes the quantity. If there are n variables and m constraints in the primary, there are m variables and n constraints in the dual. Consider the following (primary) LP variables: x (i), \u00b7 \u00b7 \u00b7 i, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 maximizes \u00b2 i = 1c (i) x (i, j) x (i, j) x (j) x (j), j = 1, 2, \u00b7 mx (i) \u2265 0, i = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 maximizes \u2211 i = 1c (i) x (i) x (i).The data of the LP consists of numbers c (i)."}, {"heading": "Appendix C. Regret for DEC-POMDPs", "text": "The value of an information collection of an agent i for an i-reduced common policy q, referred to as \"i\" (\"i,\" \"q\"), is defined by: \"i\" (\"i,\" \"q\") = \"i\" (\"h,\" \"i\") (160) for any terminal information collection and, if \"i\" (\"h,\" \"q\") = \"i\" (\"h\") = \"i\" (\"h,\" q \") (161) Then the regret of a story h is defined for an agent i and for an i-reduced common policy q, referred to as\" i \"(h,\" q) = \"i\" (h) = \"i\" i \"(h),\" q \"j\" (\"j\"). \""}, {"heading": "Appendix D. Program Changes Due to Optimizations", "text": "The local or global extrans history reduces the size of the search area of the mathematical programs. Now, some limitations of the programs depend on the size of the search area, so we need to change some of these limitations. MILP (Table 3) and MILP-n-Agent (Table 5) programs are based on the fact that the number of histories of a certain length t to support a pure policy of each agent is set and is the same. As it cannot be the case with pruned sets, the following changes must be made: \u2022 The limitation (42) of the MILP or (121) MILP-n agents that are set is set and the same as the pruned sets, the following changes must be made: \u2022 The limitation (42) of the MILP or (121) MILP-n agents that are set."}, {"heading": "Appendix E. Example using MA-Tiger", "text": "All these examples are derived with the help of the decentralized tiger problem (MA tiger), which is described in Section 2.2. \u00b7 ao ao (ao): ao (ao) (ao), with 3 measures (al, ar, ao) and 2 observations (ol, or). We will only have problems with a horizon of 2.There are 18 (32 \u00d7 2) finishes for an agent (ao.ol.ao (ao, ao.ol.al, ao.ol.ar, ao.ol.ar, ar.ol.ar, ar.ol.ar, ar.ar.ar, ar.ol.ar, ar.ar.ar, ar.ar.ar.ar, ar.ol.ar, ar.ar, ar.ar, ar.ar, ar.ar, ar.ar, ar.ar, ar.ar, ar.ol.ar (182 = 32 \u00d7 2) (ao), ar.ao, (.ao),.ao,.alao,.alao,.alao (.alao)"}], "references": [{"title": "Optimizing memory-bounded controllers for decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proc. of the Twenty-Third Conf. on Uncertainty in Artificial Intelligence (UAI-07)", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Solving POMDPs using quadratically constrained linear programs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proc. of the Twentieth Int. Joint Conf. on Artificial Intelligence (IJCAI\u201907)", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Bounded dynamic programming for decentralized POMDPs. In Proc. of the Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains (MSDM) in AAMAS\u201907", "author": ["C. Amato", "A. Carlin", "S. Zilberstein"], "venue": null, "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Incremental policy generation for finitehorizon DEC-POMDPs", "author": ["C. Amato", "J. Dibangoye", "S. Zilberstein"], "venue": "In Proc. of the Nineteenth Int. Conf. on Automated Planning and Scheduling (ICAPS-09)", "citeRegEx": "Amato et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2009}, {"title": "Time-varying feedback laws for decentralized control", "author": ["B. Anderson", "J. Moore"], "venue": "Nineteenth IEEE Conference on Decision and Control including the Symposium on Adaptive Processes,", "citeRegEx": "Anderson and Moore,? \\Q1980\\E", "shortCiteRegEx": "Anderson and Moore", "year": 1980}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C. Goldman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Becker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Bounded policy iteration for decentralized POMDPs", "author": ["D.S. Bernstein", "E.A. Hansen", "S. Zilberstein"], "venue": "In Proc. of the Nineteenth Int. Joint Conf. on Artificial Intelligence (IJCAI),", "citeRegEx": "Bernstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2005}, {"title": "Exact dynamic programming for decentralized pomdps with lossless policy compression", "author": ["A. Boularias", "B. Chaib-draa"], "venue": "In Proc. of the Int. Conf. on Automated Planning and Scheduling (ICAPS\u201908)", "citeRegEx": "Boularias and Chaib.draa,? \\Q2008\\E", "shortCiteRegEx": "Boularias and Chaib.draa", "year": 2008}, {"title": "Planning, learning and coordination in multiagent decision processes", "author": ["C. Boutilier"], "venue": "In Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge (TARK \u201996),", "citeRegEx": "Boutilier,? \\Q1996\\E", "shortCiteRegEx": "Boutilier", "year": 1996}, {"title": "Shaping multi-agent systems with gradient reinforcement learning", "author": ["O. Buffet", "A. Dutech", "F. Charpillet"], "venue": "Autonomous Agent and Multi-Agent System Journal (AAMASJ),", "citeRegEx": "Buffet et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Buffet et al\\.", "year": 2007}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["A. Cassandra", "L. Kaelbling", "M. Littman"], "venue": "In Proc. of the 12th Nat. Conf. on Artificial Intelligence (AAAI)", "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "A heuristic approach for solving decentralized-POMDP: assessment on the pursuit problem", "author": ["I. Chad\u00e8s", "B. Scherrer", "F. Charpillet"], "venue": "In Proc. of the 2002 ACM Symposium on Applied Computing,", "citeRegEx": "Chad\u00e8s et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chad\u00e8s et al\\.", "year": 2002}, {"title": "Valid inequalities for mixed integer linear programs", "author": ["G. Cornu\u00e9jols"], "venue": "Mathematical Programming B,", "citeRegEx": "Cornu\u00e9jols,? \\Q2008\\E", "shortCiteRegEx": "Cornu\u00e9jols", "year": 2008}, {"title": "On the significance of solving linear programming problems with some integer variables", "author": ["G.B. Dantzig"], "venue": null, "citeRegEx": "Dantzig,? \\Q1960\\E", "shortCiteRegEx": "Dantzig", "year": 1960}, {"title": "A probabilistic production and inventory problem", "author": ["F. d\u2019Epenoux"], "venue": "Management Science,", "citeRegEx": "d.Epenoux,? \\Q1963\\E", "shortCiteRegEx": "d.Epenoux", "year": 1963}, {"title": "Introduction to Applied Optimization", "author": ["U. Diwekar"], "venue": null, "citeRegEx": "Diwekar,? \\Q2008\\E", "shortCiteRegEx": "Diwekar", "year": 2008}, {"title": "Multilinear programming: Duality theories", "author": ["R. Drenick"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Drenick,? \\Q1992\\E", "shortCiteRegEx": "Drenick", "year": 1992}, {"title": "Practical Methods of Optimization", "author": ["R. Fletcher"], "venue": null, "citeRegEx": "Fletcher,? \\Q1987\\E", "shortCiteRegEx": "Fletcher", "year": 1987}, {"title": "Learning to communicate and act in cooperative multiagent systems using hierarchical reinforcement learning", "author": ["M. Ghavamzadeh", "S. Mahadevan"], "venue": "In Proc. of the 3rd Int. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS\u201904)", "citeRegEx": "Ghavamzadeh and Mahadevan,? \\Q2004\\E", "shortCiteRegEx": "Ghavamzadeh and Mahadevan", "year": 2004}, {"title": "A global newton method to compute Nash equilibria", "author": ["S. Govindan", "R. Wilson"], "venue": "Journal of Economic Theory,", "citeRegEx": "Govindan and Wilson,? \\Q2001\\E", "shortCiteRegEx": "Govindan and Wilson", "year": 2001}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["E. Hansen", "D. Bernstein", "S. Zilberstein"], "venue": "In Proc. of the Nineteenth National Conference on Artificial Intelligence (AAAI-04)", "citeRegEx": "Hansen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2004}, {"title": "Global Optimization: Deterministic Approaches (3rd edition)", "author": ["R. Horst", "H. Tuy"], "venue": null, "citeRegEx": "Horst and Tuy,? \\Q2003\\E", "shortCiteRegEx": "Horst and Tuy", "year": 2003}, {"title": "Learning and discovery of predictive state representations in dynamical systems with reset", "author": ["M. James", "S. Singh"], "venue": "In Proc. of the Twenty-first Int. Conf. of Machine Learning (ICML\u201904)", "citeRegEx": "James and Singh,? \\Q2004\\E", "shortCiteRegEx": "James and Singh", "year": 2004}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Fast algorithms for finding randomized strategies in game trees", "author": ["D. Koller", "N. Megiddo", "B. von Stengel"], "venue": "In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC", "citeRegEx": "Koller et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1994}, {"title": "Finding mixed strategies with small supports in extensive form games", "author": ["D. Koller", "N. Megiddo"], "venue": "International Journal of Game Theory,", "citeRegEx": "Koller and Megiddo,? \\Q1996\\E", "shortCiteRegEx": "Koller and Megiddo", "year": 1996}, {"title": "Bimatrix Equilibrium Points and Mathematical Programming", "author": ["C. Lemke"], "venue": "Management Science,", "citeRegEx": "Lemke,? \\Q1965\\E", "shortCiteRegEx": "Lemke", "year": 1965}, {"title": "Linear and Nonlinear Programming", "author": ["D. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1984\\E", "shortCiteRegEx": "Luenberger", "year": 1984}, {"title": "Online discovery and learning of predictive state representations", "author": ["P. McCracken", "M.H. Bowling"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "McCracken and Bowling,? \\Q2005\\E", "shortCiteRegEx": "McCracken and Bowling", "year": 2005}, {"title": "Taming decentralized POMDPs: towards efficient policy computation for multiagent setting", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "In Proc. of Int. Joint Conference on Artificial Intelligence,", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["F. Oliehoek", "M. Spaan", "N. Vlassis"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Lossless clustering of histories in decentralized POMDPs", "author": ["F. Oliehoek", "S. Whiteson", "M. Spaan"], "venue": "In Proc. of The International Joint Conference on Autonomous Agents and Multi Agent Systems,", "citeRegEx": "Oliehoek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2009}, {"title": "A Course in Game Theory", "author": ["M.J. Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne and Rubinstein,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein", "year": 1994}, {"title": "Combinatorial Optimization: Algorithms and Complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": null, "citeRegEx": "Papadimitriou and Steiglitz,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou and Steiglitz", "year": 1982}, {"title": "The Complexity Of Markov Decision Processes", "author": ["C.H. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Game theory and decision theory in multi-agent systems", "author": ["S. Parsons", "M. Wooldridge"], "venue": "Autonomous Agents and Multi-Agent Systems (JAAMAS),", "citeRegEx": "Parsons and Wooldridge,? \\Q2002\\E", "shortCiteRegEx": "Parsons and Wooldridge", "year": 2002}, {"title": "Average-reward decentralized Markov decision processes", "author": ["M. Petrik", "S. Zilberstein"], "venue": "In Proc. of the Twentieth Int. Joint Conf. on Artificial Intelligence (IJCAI", "citeRegEx": "Petrik and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Petrik and Zilberstein", "year": 2007}, {"title": "A bilinear programming approach for multiagent planning", "author": ["M. Petrik", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Petrik and Zilberstein,? \\Q2009\\E", "shortCiteRegEx": "Petrik and Zilberstein", "year": 2009}, {"title": "Markov Decision Processes: discrete stochastic dynamic programming", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories And Models", "author": ["D. Pynadath", "M. Tambe"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pynadath and Tambe,? \\Q2002\\E", "shortCiteRegEx": "Pynadath and Tambe", "year": 2002}, {"title": "The application of linear programming to team decision problems", "author": ["R. Radner"], "venue": "Management Science,", "citeRegEx": "Radner,? \\Q1959\\E", "shortCiteRegEx": "Radner", "year": 1959}, {"title": "Artificial Intelligence: A modern approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q1995\\E", "shortCiteRegEx": "Russell and Norvig", "year": 1995}, {"title": "Multiagent systems, chap. Distributed rational decision making, pp. 201\u2013258", "author": ["T. Sandholm"], "venue": null, "citeRegEx": "Sandholm,? \\Q1999\\E", "shortCiteRegEx": "Sandholm", "year": 1999}, {"title": "Mixed-integer programming methods for finding nash equilibria", "author": ["T. Sandholm", "A. Gilpin", "V. Conitzer"], "venue": "In Proc. of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Sandholm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sandholm et al\\.", "year": 2005}, {"title": "Cooperative co-learning: A model based approach for solving multi agent reinforcement problems", "author": ["B. Scherrer", "F. Charpillet"], "venue": "In Proc. of the IEEE Int. Conf. on Tools with Artificial Intelligence (ICTAI\u201902)", "citeRegEx": "Scherrer and Charpillet,? \\Q2002\\E", "shortCiteRegEx": "Scherrer and Charpillet", "year": 2002}, {"title": "Memory-bounded dynamic programming for DECPOMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Proc. of the Twentieth Int. Joint Conf. on Artificial Intelligence (IJCAI\u201907)", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Learning without state estimation in partially observable markovian decision processes", "author": ["S. Singh", "T. Jaakkola", "M. Jordan"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning", "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "Learning predictive state representations", "author": ["S. Singh", "M. Littman", "N. Jong", "D. Pardoe", "P. Stone"], "venue": "In Proc. of the Twentieth Int. Conf. of Machine Learning (ICML\u201903)", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Point-based Dynamic Programming for DEC-POMDPs", "author": ["D. Szer", "F. Charpillet"], "venue": "In Proc. of the Twenty-First National Conf. on Artificial Intelligence (AAAI", "citeRegEx": "Szer and Charpillet,? \\Q2006\\E", "shortCiteRegEx": "Szer and Charpillet", "year": 2006}, {"title": "MAA*: A heuristic search algorithm for solving decentralized POMDPs", "author": ["D. Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In Proc. of the Twenty-First Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "Szer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2005}, {"title": "Interac-DEC-MDP : Towards the use of interactions in DEC-MDP", "author": ["V. Thomas", "C. Bourjot", "V. Chevrier"], "venue": "In Proc. of the Third Int. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS\u201904),", "citeRegEx": "Thomas et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2004}, {"title": "Linear Programming: Foundations and Extensions (3rd edition)", "author": ["R.J. Vanderbei"], "venue": null, "citeRegEx": "Vanderbei,? \\Q2008\\E", "shortCiteRegEx": "Vanderbei", "year": 2008}, {"title": "Handbook of Game Theory, Vol. 3, chap. 45-\u201dComputing equilibria for two-person games", "author": ["B. von Stengel"], "venue": null, "citeRegEx": "Stengel,? \\Q2002\\E", "shortCiteRegEx": "Stengel", "year": 2002}, {"title": "Mixed-integer linear programming for transitionindependent decentralized MDPs", "author": ["J. Wu", "E.H. Durfee"], "venue": "In Proc. of the fifth Int. Joint Conf. on Autonomous Agents and Multiagent Systems", "citeRegEx": "Wu and Durfee,? \\Q2006\\E", "shortCiteRegEx": "Wu and Durfee", "year": 2006}, {"title": "Communication in multi-agent Markov decision processes", "author": ["P. Xuan", "V. Lesser", "S. Zilberstein"], "venue": "In Proc. of ICMAS Workshop on Game Theoretic and Decision Theoretics Agents", "citeRegEx": "Xuan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Xuan et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 40, "context": "The decision-theoretic approach to rational behavior relies mostly on the framework of Markov Decision Processes (MDP) (Puterman, 1994).", "startOffset": 119, "endOffset": 135}, {"referenceID": 40, "context": "While the notion of \u201cintelligent behavior\u201d is difficult to assess and to measure, we prefer to refer to the concept of \u201crational behavior\u201d as formulated by Russell and Norvig (1995). As a consequence, the work presented here uses a decision-theoretic approach in order to build agents that take optimal actions in an uncertain and partially unknown environment.", "startOffset": 156, "endOffset": 182}, {"referenceID": 6, "context": "If the controller knows the dynamics of the system, which is made of a transition function and of a reward function, algorithms derived from the field of Dynamic Programming (see Bellman, 1957) allow the controller to compute an optimal deterministic policy, i.e., a decision function which associates an \u201coptimal\u201d action to every state so that the expected long term return is optimal. This process is called planning in the MDP community. In fact, using the MDP framework, it is quite straightforward to model a problem with one agent which has a full and complete knowledge of the state of the system. But agents, and especially in a multi-agent setting, are generally not able to determine the complete and exact state of the system because of noisy, faulty or limited sensors or because of the nature of the problem itself. As a consequence, different states of the system are observed as similar by the agent which is a problem when different optimal actions should be taken in these states; one speaks then of perceptual aliasing. An extension of MDPs called Partially Observable Markov Decisions Processes (POMDPs) deals explicitly with this phenomenon and allows a single agent to compute plans in such a setting provided it knows the conditional probabilities of observations given the state of the environment (Cassandra, Kaelbling, & Littman, 1994). As pointed out by Boutilier (1996), multi-agent problems could be solved as MDPs if considered from a centralized point of view for planning and control.", "startOffset": 179, "endOffset": 1397}, {"referenceID": 7, "context": "The main limitation of DEC-POMDPs is that they are provably untractable as they belong to the class of NEXP-complete problems (Bernstein et al., 2002).", "startOffset": 126, "endOffset": 150}, {"referenceID": 42, "context": "For example, the Team Decision Problem (Radner, 1959), later formulated as a Markov system in the field of control theory by Anderson and Moore (1980), led to the Markov Team Decision Problem (Pynadath & Tambe, 2002).", "startOffset": 39, "endOffset": 53}, {"referenceID": 6, "context": "The main limitation of DEC-POMDPs is that they are provably untractable as they belong to the class of NEXP-complete problems (Bernstein et al., 2002). Concretely, this complexity result implies that, in the worst case, finding an optimal joint policy of a finite horizon DEC-POMDP requires time that is exponential in the horizon if one always make good choices. Because of this complexity, there are very few algorithms for finding exact optimal solutions for DEC-POMDPs (they all have a doubly exponential complexity) and only a few more that look for approximate solutions. As discussed and detailed in the work of Oliehoek, Spaan, and Vlassis (2008), these algorithms follow either a dynamic programming approach or a forward search approach by adapting concepts and algorithms that were designed for POMDPs.", "startOffset": 127, "endOffset": 655}, {"referenceID": 4, "context": "For example, the Team Decision Problem (Radner, 1959), later formulated as a Markov system in the field of control theory by Anderson and Moore (1980), led to the Markov Team Decision Problem (Pynadath & Tambe, 2002).", "startOffset": 125, "endOffset": 151}, {"referenceID": 17, "context": "2 Motivations The main objective of our work is to investigate the use of mathematical programming, more especially mixed-integer linear programs (MILP) (Diwekar, 2008), for solving DECPOMDPs.", "startOffset": 153, "endOffset": 168}, {"referenceID": 17, "context": "2 Motivations The main objective of our work is to investigate the use of mathematical programming, more especially mixed-integer linear programs (MILP) (Diwekar, 2008), for solving DECPOMDPs. Our motivation relies on the fact that the field of linear programming is quite mature and of great interest to the industry. As a consequence, there exist many efficient solvers for mixed-integer linear programs and we want to see how these efficient solvers perform in the framework of DEC-POMDPs. Therefore, we have to reformulate a DEC-POMDP to solve it as a mixed-integer linear program. As shown in this article, two paths lead to such mathematical programs, one grounded on the work from Koller, Megiddo, and von Stengel (1994), Koller and Megiddo (1996) and von Stengel (2002), and another one grounded on combinatorial considerations.", "startOffset": 154, "endOffset": 728}, {"referenceID": 17, "context": "2 Motivations The main objective of our work is to investigate the use of mathematical programming, more especially mixed-integer linear programs (MILP) (Diwekar, 2008), for solving DECPOMDPs. Our motivation relies on the fact that the field of linear programming is quite mature and of great interest to the industry. As a consequence, there exist many efficient solvers for mixed-integer linear programs and we want to see how these efficient solvers perform in the framework of DEC-POMDPs. Therefore, we have to reformulate a DEC-POMDP to solve it as a mixed-integer linear program. As shown in this article, two paths lead to such mathematical programs, one grounded on the work from Koller, Megiddo, and von Stengel (1994), Koller and Megiddo (1996) and von Stengel (2002), and another one grounded on combinatorial considerations.", "startOffset": 154, "endOffset": 755}, {"referenceID": 17, "context": "2 Motivations The main objective of our work is to investigate the use of mathematical programming, more especially mixed-integer linear programs (MILP) (Diwekar, 2008), for solving DECPOMDPs. Our motivation relies on the fact that the field of linear programming is quite mature and of great interest to the industry. As a consequence, there exist many efficient solvers for mixed-integer linear programs and we want to see how these efficient solvers perform in the framework of DEC-POMDPs. Therefore, we have to reformulate a DEC-POMDP to solve it as a mixed-integer linear program. As shown in this article, two paths lead to such mathematical programs, one grounded on the work from Koller, Megiddo, and von Stengel (1994), Koller and Megiddo (1996) and von Stengel (2002), and another one grounded on combinatorial considerations.", "startOffset": 154, "endOffset": 778}, {"referenceID": 26, "context": "Our main inspiration comes from the work of Koller, von Stegel and Megiddo that shows how to solve games in extensive form with imperfect information and identical interests, that is how to find a Nash equilibrium for this kind of game (Koller et al., 1994; Koller & Megiddo, 1996; von Stengel, 2002).", "startOffset": 236, "endOffset": 300}, {"referenceID": 26, "context": "3, is then to adapt the sequence-form introduced by Koller, von Stegel and Megiddo to the framework of DEC-POMDPs (Koller et al., 1994; Koller & Megiddo, 1996; von Stengel, 2002).", "startOffset": 114, "endOffset": 178}, {"referenceID": 32, "context": "We were thus able to confirm that our algorithms are quite comparable to dynamic programming exact algorithms but outperformed by forward search algorithms like GMAA* (Oliehoek et al., 2008).", "startOffset": 167, "endOffset": 190}, {"referenceID": 7, "context": "Dec-POMDP This section gives a formal definition of Decentralized Partially Observed Markov Decision Processes as introduced by Bernstein et al. (2002). As described, a solution of a DECPOMDP is a policy defined on the space of information sets that has an optimal value.", "startOffset": 128, "endOffset": 152}, {"referenceID": 31, "context": "The full state transition function, joint observation function and reward function are described in the work of Nair et al. (2003).", "startOffset": 112, "endOffset": 131}, {"referenceID": 31, "context": "Proof: See proof in the work of Nair et al. (2003).", "startOffset": 32, "endOffset": 51}, {"referenceID": 31, "context": "6 Overview of DEC-POMDPs Solutions and Limitations As detailed in the work of Oliehoek et al. (2008), existing methods for solving DECPOMDPs with finite-horizon belong to several broad families: \u201cbrute force\u201d, alternating maximization, search algorithms and dynamic programming.", "startOffset": 78, "endOffset": 101}, {"referenceID": 31, "context": "6 Overview of DEC-POMDPs Solutions and Limitations As detailed in the work of Oliehoek et al. (2008), existing methods for solving DECPOMDPs with finite-horizon belong to several broad families: \u201cbrute force\u201d, alternating maximization, search algorithms and dynamic programming. Brute Force The simplest approach for solving a DEC-POMDP is to enumerate all possible joint policies and to evaluate them in order to find the optimal one. However, such a method becomes quickly untractable as the number of joint policies is doubly exponential in the horizon of the problem. Alternating Maximization Following Chad\u00e8s, Scherrer, and Charpillet (2002) and Nair et al.", "startOffset": 78, "endOffset": 647}, {"referenceID": 31, "context": "Alternating Maximization Following Chad\u00e8s, Scherrer, and Charpillet (2002) and Nair et al. (2003), one possible way to solve DEC-POMDPs is for each agent (or each small group of agents) to alternatively search for a better policy while all the other agents freeze their own policy.", "startOffset": 79, "endOffset": 98}, {"referenceID": 32, "context": "used underlying MDPs or POMDPs to compute the admissible heuristic, Oliehoek et al. (2008) introduced a better heuristic based on the resolution of a Bayesian Game with a carefully crafted cost function.", "startOffset": 68, "endOffset": 91}, {"referenceID": 32, "context": "used underlying MDPs or POMDPs to compute the admissible heuristic, Oliehoek et al. (2008) introduced a better heuristic based on the resolution of a Bayesian Game with a carefully crafted cost function. Currently, Oliehoek\u2019s method called GMAA* (for Generic Multi-Agent A*) is the quickest exact method on a large set of benchmarks. But, as every exact method, it is limited to quite simple problems. Dynamic Programming The work from Hansen, Bernstein, and Zilberstein (2004) adapts solutions designed for POMDPs to the domain of DEC-POMDPs.", "startOffset": 68, "endOffset": 478}, {"referenceID": 32, "context": "used underlying MDPs or POMDPs to compute the admissible heuristic, Oliehoek et al. (2008) introduced a better heuristic based on the resolution of a Bayesian Game with a carefully crafted cost function. Currently, Oliehoek\u2019s method called GMAA* (for Generic Multi-Agent A*) is the quickest exact method on a large set of benchmarks. But, as every exact method, it is limited to quite simple problems. Dynamic Programming The work from Hansen, Bernstein, and Zilberstein (2004) adapts solutions designed for POMDPs to the domain of DEC-POMDPs. The general idea is to start with policies for 1 time step that are used to build 2 time step policies and so on. But the process is clearly less efficient that the heuristic search approach as an exponential number of policies must be constructed and evaluated at each iteration of the algorithm. Some of these policies can be pruned but, once again, pruning is less efficient. As exposed in more details in the paper by Oliehoek et al. (2008), several others approaches have been developed for subclasses of DEC-POMDPs.", "startOffset": 68, "endOffset": 989}, {"referenceID": 26, "context": "1 in the work of Koller et al. (1994).", "startOffset": 17, "endOffset": 38}, {"referenceID": 19, "context": "An evident, but inefficient, method to find a global maximum point is to evaluate all the extreme points of the set of feasible solutions of the program since it is known that every global as well as local maximum point of a non-concave function lies at an extreme point of such a set (Fletcher, 1987).", "startOffset": 285, "endOffset": 301}, {"referenceID": 19, "context": "(Fletcher, 1987), for efficiency reasons, it is important to reduce the number of integer variables in our mathematical programs.", "startOffset": 0, "endOffset": 16}, {"referenceID": 44, "context": "Links between the fields of multiagent systems and game theory are numerous in the literature (see, for example, Sandholm, 1999; Parsons & Wooldridge, 2002). We will elaborate on the fact that the optimal policy of a DEC-POMDP is a Nash Equilibrium. It is in fact the Nash Equilibrium with the highest utility as the agents all share the same reward. For the 2-agent case, the derivation we make in order to build the MILP is similar to the first derivation of Sandholm, Gilpin, and Conitzer (2005). We give more details of this derivation and adapt it to DEC-POMDP by adding an objective function to it.", "startOffset": 113, "endOffset": 499}, {"referenceID": 22, "context": "The concept is also quite similar to the process of policy elimination in the backward step of the dynamic programming for partially observable stochastic games (Hansen et al., 2004).", "startOffset": 161, "endOffset": 182}, {"referenceID": 14, "context": "Another option which directly aims at reducing the computation time is to use cutting planes (Cornu\u00e9jols, 2008).", "startOffset": 93, "endOffset": 111}, {"referenceID": 15, "context": "A cut (Dantzig, 1960) is a special constraint that identifies a portion of the set of feasible solutions in which the optimal solution provably does not lie.", "startOffset": 6, "endOffset": 21}, {"referenceID": 36, "context": "Complexity The complexity of finding an upper bound is linked to the complexity of solving a POMDP which, as showed by Papadimitriou and Tsitsiklis (1987), can be PSPACE (i.", "startOffset": 119, "endOffset": 155}, {"referenceID": 51, "context": "The reasoning leads to an iterated computation of DEC-POMDPs of longer and longer horizon, reminiscent of the MAA* algorithm (Szer et al., 2005).", "startOffset": 125, "endOffset": 144}, {"referenceID": 19, "context": "MILP and MILP-2 are solved using the \u201ciLog Cplex 10\u201d solver \u2013 a commercial set of Java packages \u2013 that relies on a combination of the \u201cSimplex\u201d and \u201cBranch and Bounds\u201d methods (Fletcher, 1987).", "startOffset": 176, "endOffset": 192}, {"referenceID": 18, "context": "MILP and MILP-2 are solved using the \u201ciLog Cplex 10\u201d solver \u2013 a commercial set of Java packages \u2013 that relies on a combination of the \u201cSimplex\u201d and \u201cBranch and Bounds\u201d methods (Fletcher, 1987). The software is run on an Intel P4 at 3.4 GHz with 2Gb of RAM using default configuration parameters. For the mathematical programs, different combination of heuristics have been evaluated: pruning of locally extraneous histories, using a lower bound cut and using an upper bound cut, respectively denoted \u201cLOC\u201d, \u201cLow\u201d and \u201cUp\u201d in the result tables to come. The Non-Linear Program (NLP) of Section 3.4 has been evaluated by using various solvers from the NEOS website (http://www-neos.mcs.anl.gov ), even thought this method does not guarantee an optimal solution to the DEC-POMDP. Three solvers have been used: LANCELOT (abbreviated as LANC.), LOQO and SNOPT. The result tables also report results found in the literature for the following algorithms: DP stands for Dynamic Programming from Hansen et al. (2004); DP-LPC is an improved version of Dynamic Programming where policies are compressed in order to fit more of them in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008); PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is a heuristically guided forward search proposed by Szer et al.", "startOffset": 177, "endOffset": 1007}, {"referenceID": 9, "context": "(2004); DP-LPC is an improved version of Dynamic Programming where policies are compressed in order to fit more of them in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008); PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is a heuristically guided forward search proposed by Szer et al.", "startOffset": 175, "endOffset": 207}, {"referenceID": 9, "context": "(2004); DP-LPC is an improved version of Dynamic Programming where policies are compressed in order to fit more of them in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008); PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is a heuristically guided forward search proposed by Szer et al.", "startOffset": 175, "endOffset": 374}, {"referenceID": 9, "context": "(2004); DP-LPC is an improved version of Dynamic Programming where policies are compressed in order to fit more of them in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008); PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is a heuristically guided forward search proposed by Szer et al. (2005) and a generalized and improved version of this algorithm called GMAA* developed by Oliehoek et al.", "startOffset": 175, "endOffset": 452}, {"referenceID": 9, "context": "(2004); DP-LPC is an improved version of Dynamic Programming where policies are compressed in order to fit more of them in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008); PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is a heuristically guided forward search proposed by Szer et al. (2005) and a generalized and improved version of this algorithm called GMAA* developed by Oliehoek et al. (2008). The problems selected to evaluate the algorithms are detailed in the coming subsections.", "startOffset": 175, "endOffset": 558}, {"referenceID": 22, "context": "We will use the description given by Hansen et al. (2004) that allows this problem to be formalized as a DEC-POMDP.", "startOffset": 37, "endOffset": 58}, {"referenceID": 22, "context": "The state transition function P, the joint observation function G and the reward function R have been taken from Hansen et al. (2004). If both agents have full buffers in a period, and both use the channel in that period, the state of the problem is unchanged in the next period; both agents have full buffers in the next period.", "startOffset": 113, "endOffset": 134}, {"referenceID": 31, "context": "2, the Multi-Agent Tiger problem (MA-Tiger) has been introduced in the paper from Nair et al. (2003). From the general description of the problem, we ob-", "startOffset": 82, "endOffset": 101}, {"referenceID": 31, "context": "The state transition function P, joint observation function G and the reward function R are taken from the paper by Nair et al. (2003). P is quite simple.", "startOffset": 116, "endOffset": 135}, {"referenceID": 31, "context": "The state transition function P, joint observation function G and the reward function R are taken from the paper by Nair et al. (2003). P is quite simple. If one or both agents opens a door in a period, the state of the problem in the next period is set back to \u03b1. If both agents listen in a period, the state of the process in unchanged in the next period. G, given in Table (9), is also quite simple. Nair et al. (2003) describes two reward functions called \u201cA\u201d and \u201cB\u201d for this problem, here we report only results for reward function \u201cA\u201d, given in Table 10, as the behavior of the algorithm are similar for both reward functions.", "startOffset": 116, "endOffset": 422}, {"referenceID": 47, "context": "It must also be noted that for this problem, approximate methods like NLP but also other algorithms not depicted here like the \u201cMemory Bound Dynamic Programming\u201d of Seuken and Zilberstein (2007) are able to find the optimal solution.", "startOffset": 165, "endOffset": 195}, {"referenceID": 32, "context": "3 Fire Fighters Problem The problem of the Fire Fighters (FF) has been introduced as a new benchmark by Oliehoek et al. (2008). It models a team of n fire fighters that have to extinguish fires in a row of nh houses.", "startOffset": 104, "endOffset": 127}, {"referenceID": 32, "context": "For MAA* and GMAA*, value in parenthesis are taken from the work of Oliehoek et al. (2008) and should be optimal but are different from our optimal values.", "startOffset": 68, "endOffset": 91}, {"referenceID": 32, "context": "For horizon 3, the value of the optimal policy given by Oliehoek et al. (2008) (\u22125.", "startOffset": 56, "endOffset": 79}, {"referenceID": 32, "context": "For horizon 3, the value of the optimal policy given by Oliehoek et al. (2008) (\u22125.73) differs from the value found by the MILP algorithms (\u22125.98) whereas both methods are supposed to be exact. This might come from slight differences in our respective formulation of the problems. For horizon 4, Oliehoek et al. (2008) report an optimal value of (\u22126.", "startOffset": 56, "endOffset": 319}, {"referenceID": 32, "context": "For horizon 3, the value of the optimal policy given by Oliehoek et al. (2008) (\u22125.73) differs from the value found by the MILP algorithms (\u22125.98) whereas both methods are supposed to be exact. This might come from slight differences in our respective formulation of the problems. For horizon 4, Oliehoek et al. (2008) report an optimal value of (\u22126.57). For this problem, MILP methods are clearly outperformed by MAA* and GMAA*. Only NLP methods, which give an optimal solution for horizon 3, are better in term of computation time. It might be that NLP are also able to find optimal policies for horizon 4 but as our setting differs from the work of Oliehoek et al. (2008), we are not able to check if the policy found is really the optimal.", "startOffset": 56, "endOffset": 675}, {"referenceID": 32, "context": "Again, optimal value found by our method differ from the value reported by Oliehoek et al. (2008). Whereas we found that the optimal values are 1.", "startOffset": 75, "endOffset": 98}, {"referenceID": 32, "context": "For MAA* and GMAA*, value in parenthesis are taken from the work of Oliehoek et al. (2008) and should be optimal but are different from our optimal values.", "startOffset": 68, "endOffset": 91}, {"referenceID": 16, "context": "For instance, an infinite horizon MDP can be solved by a linear program (d\u2019Epenoux, 1963).", "startOffset": 72, "endOffset": 89}, {"referenceID": 16, "context": "For instance, an infinite horizon MDP can be solved by a linear program (d\u2019Epenoux, 1963). More recently, mathematical programming has been directed at infinite horizon POMDPs and DEC-POMDPs. Thus, an infinite horizon DEC-MDP (with state transition independence) can be solved by a 0-1 MILP (Petrik & Zilberstein, 2007) and an infinite horizon POMDP or DEC-POMDP can be solved (for local optima) by a nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). The finite horizon case \u2013 much different in character than the infinite horizon case \u2013 has been dealt with using dynamic programming. As stated earlier, whereas dynamic programming has been quite successful for finite horizon MDPs and POMDPs, it has been less so for finite horizon DEC-POMDPs. In contrast, in game theory, mathematical programming has been successfully directed at games of finite horizon. Lemke\u2019s algorithm (1965) for two-player normal form games, the Govindan-Wilson algorithm (2001) for n-player normal form games and the Koller, Megiddo and von Stengel approach (which internally uses Lemke\u2019s algorithm) for two-player extensive form games are all for finite-horizon games.", "startOffset": 73, "endOffset": 900}, {"referenceID": 16, "context": "For instance, an infinite horizon MDP can be solved by a linear program (d\u2019Epenoux, 1963). More recently, mathematical programming has been directed at infinite horizon POMDPs and DEC-POMDPs. Thus, an infinite horizon DEC-MDP (with state transition independence) can be solved by a 0-1 MILP (Petrik & Zilberstein, 2007) and an infinite horizon POMDP or DEC-POMDP can be solved (for local optima) by a nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). The finite horizon case \u2013 much different in character than the infinite horizon case \u2013 has been dealt with using dynamic programming. As stated earlier, whereas dynamic programming has been quite successful for finite horizon MDPs and POMDPs, it has been less so for finite horizon DEC-POMDPs. In contrast, in game theory, mathematical programming has been successfully directed at games of finite horizon. Lemke\u2019s algorithm (1965) for two-player normal form games, the Govindan-Wilson algorithm (2001) for n-player normal form games and the Koller, Megiddo and von Stengel approach (which internally uses Lemke\u2019s algorithm) for two-player extensive form games are all for finite-horizon games.", "startOffset": 73, "endOffset": 971}, {"referenceID": 22, "context": "This algorithm is based on the backward induction DP algorithm (Hansen et al., 2004).", "startOffset": 63, "endOffset": 84}, {"referenceID": 8, "context": "The computational experience of this mathematical programming approach shows that it is better (finds higher quality solutions in lesser time) than a dynamic programming approach (Bernstein et al., 2005; Szer & Charpillet, 2006).", "startOffset": 179, "endOffset": 228}, {"referenceID": 16, "context": "This algorithm is based on the backward induction DP algorithm (Hansen et al., 2004). The algorithm attempts to run in a limited amount of space. In order to do so, unlike the DP algorithm, it prunes even non-extraneous (i.e., nondominated) policy trees at each iteration. Thus, at each iteration, the algorithm retains a pre-determined number of trees. This algorithm and its variants have been used to find a joint policy for the MABC, the MA-tiger and the Box pushing problems for very long horizons (of the order of thousands of time periods). MBDP does not provide an upper bound on the loss of value. The bounded DP (BDP) algorithm presented in the paper by Amato, Carlin, and Zilberstein (2007c) does give an upper bound.", "startOffset": 64, "endOffset": 703}, {"referenceID": 0, "context": "Mathematical programming has already been applied, with some success, to solving infinite horizon DEC-POMDPs (Amato et al., 2007a). The computational experience of this mathematical programming approach shows that it is better (finds higher quality solutions in lesser time) than a dynamic programming approach (Bernstein et al., 2005; Szer & Charpillet, 2006). Nevertheless, this approach has two inter-related shortcomings. First, the approach finds a joint controller (i.e., an infinite horizon joint policy) of a fixed size and not of the optimal size. Second, much graver than the first, for the fixed size, it finds a locally optimal joint controller. The approach does not guarantee finding an optimal joint controller. This is because the program presented in the work of Amato et al. (2007a) is a (non-convex)", "startOffset": 110, "endOffset": 801}, {"referenceID": 47, "context": "For example, the work of Seuken and Zilberstein (2007), in order to limit the memory resources used by the resolution algorithm, prune the space of policies to only consider some of them; our work could help using a better estimation of the policies that are important to be kept in the search space.", "startOffset": 25, "endOffset": 55}, {"referenceID": 29, "context": "(Luenberger, 1984) If either a primal LP or its dual LP has a finite optimal solution, then so does the other, and the corresponding values of the objective functions are equal.", "startOffset": 0, "endOffset": 18}, {"referenceID": 53, "context": "(Vanderbei, 2008) Suppose that x is feasible for a primal linear program and y is feasible for its dual.", "startOffset": 0, "endOffset": 17}], "year": 2010, "abstractText": "Decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach, mainly through the framework of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS are a general and powerful modeling tool, solving them is a task with an overwhelming complexity that can be doubly exponential. In this paper, we study an alternate formulation of DEC-POMDPs relying on a sequence-form representation of policies. From this formulation, we show how to derive Mixed Integer Linear Programming (MILP) problems that, once solved, give exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be derived either by using some combinatorial characteristics of the optimal solutions of the DEC-POMDPs or by using concepts borrowed from game theory. Through an experimental validation on classical test problems from the DEC-POMDP literature, we compare our approach to existing algorithms. Results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except for some particular problems. The main contributions of this work are the use of mathematical programming for DECPOMDPs and a better understanding of DEC-POMDPs and of their solutions. Besides, we argue that our alternate representation of DEC-POMDPs could be helpful for designing novel algorithms looking for approximate solutions to DEC-POMDPs.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}