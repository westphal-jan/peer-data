{"id": "1401.3832", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Constructing Reference Sets from Unstructured, Ungrammatical Text", "abstract": "Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text \"posts.\" Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called \"reference sets,\" by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data.", "histories": [["v1", "Thu, 16 Jan 2014 04:49:45 GMT  (444kb)", "http://arxiv.org/abs/1401.3832v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["matthew michelson", "craig a knoblock"], "accepted": false, "id": "1401.3832"}, "pdf": {"name": "1401.3832.pdf", "metadata": {"source": "CRF", "title": "Constructing Reference Sets from Unstructured, Ungrammatical Text", "authors": ["Matthew Michelson", "Craig A. Knoblock"], "emails": ["mmichelson@fetch.com", "knoblock@isi.edu"], "sections": [{"heading": "1. Introduction", "text": "There is an enormous amount of unstructured, ungrammatic data for the sale of cars, etc. The source code of this data includes classifieds, auction platforms and forum postings to which we refer over and over again. There are a variety of reference points to which we refer in order to create a series of classifieds for cars from the Craigslist site. We consider posts to be unstructured because we cannot assume that the arrangement of terms will be consistent from post to post. For example, we cannot assume that the car (e.g. Audi) will precede the car model (e.g. A4). Moreover, we consider posts to be ungrammatical because they would not yield any meaningful grammatical yield).Although posts are unstructured and ungrammatical, they are full of useful information."}, {"heading": "2. Previous Work using Reference Sets for Information Extraction", "text": "We motivate our work on the construction of reference sets by placing them in the context of the discovery of reference sets for use in information extraction 2001. Specifically, we focus on the task of extracting information from contributions using reference sets, as previously shown to support this task elsewhere (Michelson & Knoblock, 2005, 2007). Information extraction is the task of analyzing the desired attributes from any text (contributions in this case). Using the example passages of Figure 1, after extraction, we would like to reference the contributions of Figure 1 into an attribute, a model attribute, and a trim attribute attribute. Specifically, the third post could be commented with extractions such as MAKE (whichis implied), MODEL = {Accord}, and TRIM = {EX}. Once done, this would allow structured queries and integration across contributions by using the extracted attributes of the complete text."}, {"heading": "3. Reference Set Construction", "text": "The intuition for constructing reference systems from posts is that reference sets often form hierarchical structures, which we call \"entity trees.\" Consider the simple reference set of three cars in Figure 3. Each tuple in the reference set of cars has two attributes, a brand and a model. The model is a more specific version of the brand (i.e., an Accord is a Honda), and the attribute value is considered a child of the Honda value. The same applies to the Civic value, and so we can create an entity tree rooted in the value of two children."}, {"heading": "3.1 Creating Entity Trees", "text": "In fact, most people who live in the US also live in the US."}, {"heading": "3.2 Discovering General Tokens", "text": "This year it is more than ever before."}, {"heading": "3.3 Seed-Based Reference Set Construction", "text": "Our first approach to dealing with noise takes advantage of a small amount of background knowledge, called \"seeds,\" to focus on the construction of entity trees. Specifically, we use the Figure 4 method to build entity trees, with the additional caveat that each entity tree must be rooted to a certain seed value. Specifically, if we specify only \"Honda\" as seed, only one entity tree rooted on Honda would be constructed. Even if other entity trees are discovered, they are discarded. It is easy to discover a complete list of seeds on the web and include too many seeds, as our algorithm simply removes any entity tree that consists solely of a root from the constructed reference set (i.e., a singleton set). One of the key intuitions behind our approach is that the set of entity tree root nodes is generally much easier to detect than the node further down in the trees."}, {"heading": "3.4 Locking-Based Reference Set Construction", "text": "This year it is more than ever before."}, {"heading": "4. Experiments", "text": "The aim of this research is to construct reference sets for tasks such as information extraction from contributions. However, direct comparison of reference sets constructed in different ways has a number of problems. Firstly, it is unclear how reference sets can be directly compared quantitatively; this problem has also been noted elsewhere in the context of the hierarchy comparison (e.g. Bast, Dupret, Majumdar, & Piwowarski, 2006). For example, there is no clear way to define similarities when comparing hierarchies. Secondly, without context, it is difficult to judge the reference sets. That is, a reference set can be huge and comprehensive, which is of great use for a task such as ontology construction, but due to coverage differences for extraction may be of little use. Secondly, another reference set may be quite loud (hence bad for ontology construction), but in fact for extraction we will use the reference sets based on the extraction, as its coverage is better, instead of \"the quality of the sets we will directly measure the extraction set the reference sets,\" of the ones we use in the extraction."}, {"heading": "4.1 Experiments: Seed-based Approach", "text": "For our first experiment, we are testing the effectiveness of using the seed approach to build reference sets. For this experiment, we are comparing our seed approach with both complete, manually constructed reference sets extracted from online sources (what we call the \"manual\" approach) and a version of the seed approach that does not restrict the reference trees to be rooted in seed values (called \"no seed\"). By comparing the seed-based method with a manually constructed reference set, we are able to test for cover problems that arise from collecting reference sets online (compared to the contributions themselves). In addition, this comparison allows us to analyze the trade-off between the high cost of building a manual reference set (compared to the low cost of finding seed products) and the gains in accuracy by using the manual reference sets that derive attributes from the (the) themselves."}, {"heading": "4.1.1 Data for Seed-based Experiments", "text": "We used three sets of data from the real world as our experimental data. The first set contains used car classifieds for sale on the Craigslist.org classifieds page. The second set consists of used laptop classifieds from Craigslist.org. Again, we labeled 1,000 entries to extract the manufacturer (e.g. IBM), the model (e.g. Thinkpad), and the model number (e.g. T41). Our last set of data includes entries about skis for sale on eBay. We designated 1,000 entries to extract the make (e.g. Rossignol), the model (e.g. Bandit), and the model specification (e.g. B3, which is referred to as \"Spec.\" The data is in Table 6.We need full, constructed reference sets for comparain manain. For cars collected on the Eddomel.com website, we have 27,000 cars at our disposal."}, {"heading": "4.1.2 Results for Seed-based Experiments", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "4.1.3 Results for Iterating for General Tokens", "text": "We also tested the effect of the iteration to capture \"general\" symbols, as opposed to a simple pause after the first pass over the trees. We are again using the extraction results as a substitute for comparing these reference sets. In this case, it is assumed that the iterative method will capture \"general\" symbols and thus construct a fuller reference set that delivers better extraction results than when the algorithm stops after the first pass over the trees. Table 9 shows the comparable results of the F1 measurement for extraction compared between the \"single pass\" approach and the \"iterative\" approach. As expected, the iterative method delivers better results. The iterative method exceeds the single pass approach for every attribute except one (laptop model number, where the F1 measurement decreases by -0.33%). Interestingly, the algorithm improves more for attributes at deeper levels of the ski unit's component when iteration is applied, meaning that the extraction result for the trees only appears 2.5% higher when compared."}, {"heading": "4.1.4 Entity Tree Analysis", "text": "Although the extraction experiments serve as the best measure of the actual utility of the seed reference sets, we have also conducted experiments on the generated entity trees themselves. We have investigated whether the attribute values are consistent in their placement in the entity trees (the homogeneity column). However, given well-known car models such as Civic, we measure whether the model values are predominantly as car model attributes (second level in the tree) or incorrectly placed because they have the same attribute (third level). However, direct measurement without domain expertise is difficult. Instead, we compare the attribute values in the seed reference set with those in the manually constructed reference sets and for those values that match, we measure if they have the same attribute (i.e. their columns match) or not. This results in a measurement of the homogeneity column for the seed reference homogeneity for the seed-based reference sets, based on the manual one."}, {"heading": "4.1.5 Comparison Against Supervised Methods", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "4.2 Experiments: Locking Approach", "text": "The next series of experiments analyzes our locking-based approach to building reference sets. As mentioned above, the locking-based technique is appropriate for the specific case where seed is too expensive or impossible to find. Therefore, our locking-based approach is an alternative to the \"no-seed\" methods described in our previous experiments. However, the experimental procedure for these experiments is exactly the same as above. We use the same data sets and compare the reference sets constructed in different ways (seed-based, \"no-seed\" and \"locked\") by passing them on to the same extraction mechanism and using the extraction results as a proxy for the reference set. For the locking algorithms, we need to specify the number of posts to be added to each locking iteration. We set this value to 200, which is large enough to limit the total number of items."}, {"heading": "4.3 Experiments: Assumptions for Constructing Reference Sets", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5. Related Work", "text": "The focus of this research is on the creation of reference sentences from the contributions, and the number of contributions is generally limited to the number of entities. As the reference sentences are flattened, the work that resembles us most is the exploration of conceptual hierarchies (insinuations) from the text. There are alternative methods of building conceptual hierarchies, which are essentially not as well suited as the Sanderson and Croft Method (1999) that we have chosen for our problem. First, because our data is ungrammatic, we cannot use subsummation methods that rely on formal concept analysis that refer to subsumptions in the text to detect subsumptions (Cimiano, Hotho, & Staab, 2005). We have a lot of nouns in the contributions, but almost no verbs, because our algorithm runs iteratively due to the \"general token\" problem, we need a method that runs efficiently, the crop runs efficiently with the algorithm and our algorithm runs efficiently."}, {"heading": "6. Conclusion", "text": "This paper introduces a method for creating reference sets from unstructured, un-mmatized text on the Web. Once discovered, these reference sets can be used for tasks such as ontological maintenance, query formulation, and information extraction. We demonstrate the utility of machine-built reference sets by comparing them to manually created reference sets in an information extraction task, and we show that machine-built reference sets provide better extraction outcomes. In the future, we plan to study synonym recognition and its relationship to the automatic creation of reference sets. For example, we could automatically merge branches of the hierarchy if they are synonyms referring to the same object (such as \"Lenovo\" and \"IBM\" laptops). We also plan to examine the issue of dynamic data integration from automatically generated reference sets."}, {"heading": "Acknowledgments", "text": "This research is based on work supported in part by the National Science Foundation under premium number CMMI-0753124, in part by the Air Force Office of Scientific Research under premium number FA9550-07-1-0416, and in part by the Defense Advanced Research Projects Agency (DARPA) through the Department of the Interior, NBC, Acquisition Services Division under contract number NBCHD030010. The U.S. government is authorized to reproduce and distribute reports for government purposes, notwithstanding the copyright notices contained therein. The views and conclusions contained herein are those of the authors and should not necessarily be interpreted as representing the official policies or notices of any of any of the above organizations or persons affiliated with them."}], "references": [{"title": "Discovering a term taxonomy from term similarities using principal component analysis", "author": ["H. Bast", "G. Dupret", "D. Majumdar", "B. Piwowarski"], "venue": "In Semantics, Web and Mining.,", "citeRegEx": "Bast et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2006}, {"title": "Knowitnow: fast, scalable information extraction from the web", "author": ["M.J. Cafarella", "D. Downey", "S. Soderland", "O. Etzioni"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP),", "citeRegEx": "Cafarella et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cafarella et al\\.", "year": 2005}, {"title": "Guiding semi-supervision with constraintdriven learning", "author": ["Chang", "M.-W", "L. Ratinov", "D. Roth"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Chang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2007}, {"title": "Learning concept hierarchies from text corpora using formal concept analysis", "author": ["P. Cimiano", "A. Hotho", "S. Staab"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cimiano et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2005}, {"title": "Adaptive information extraction from text by rule induction and generalisation", "author": ["F. Ciravegna"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Ciravegna,? \\Q2001\\E", "shortCiteRegEx": "Ciravegna", "year": 2001}, {"title": "Exploiting dictionaries in named entity extraction: combining semi-markov extraction processes and data integration methods", "author": ["W. Cohen", "S. Sarawagi"], "venue": "In Proceedings of the 10th ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Cohen and Sarawagi,? \\Q2004\\E", "shortCiteRegEx": "Cohen and Sarawagi", "year": 2004}, {"title": "Roadrunner: Towards automatic data extraction from large web sites", "author": ["V. Crescenzi", "G. Mecca", "P. Merialdo"], "venue": "In Proceedings of 27th International Conference on Very Large Data Bases,", "citeRegEx": "Crescenzi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Crescenzi et al\\.", "year": 2001}, {"title": "Automatic creation and simplified querying of semantic web content: An approach based on information-extraction ontologies", "author": ["Y. Ding", "D.W. Embley", "S.W. Liddle"], "venue": "In ASWC,", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Principal components for automatic term hierarchy building", "author": ["G. Dupret", "B. Piwowarski"], "venue": "In SPIRE,", "citeRegEx": "Dupret and Piwowarski,? \\Q2006\\E", "shortCiteRegEx": "Dupret and Piwowarski", "year": 2006}, {"title": "Conceptual-model-based data extraction from multiple-record web pages", "author": ["D.W. Embley", "D.M. Campbell", "Y.S. Jiang", "S.W. Liddle", "D.W. Lonsdale", "Y.K. Ng", "R.D. Smith"], "venue": "Data Knowl. Eng.,", "citeRegEx": "Embley et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Embley et al\\.", "year": 1999}, {"title": "Unsupervised learning of field segmentation models for information extraction", "author": ["T. Grenager", "D. Klein", "C.D. Manning"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Grenager et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grenager et al\\.", "year": 2005}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Unsupervised information extraction approach using graph mutual reinforcement", "author": ["H. Hassan", "A. Hassan", "O. Emam"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hassan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2006}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proceedings of the 10th ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["J. Kazama", "K. Torisawa"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Kazama and Torisawa,? \\Q2007\\E", "shortCiteRegEx": "Kazama and Torisawa", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Automatic taxonomy extraction using google and term dependency", "author": ["M. Makrehchi", "M.S. Kamel"], "venue": "In Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence,", "citeRegEx": "Makrehchi and Kamel,? \\Q2007\\E", "shortCiteRegEx": "Makrehchi and Kamel", "year": 2007}, {"title": "Integrating unstructured data into relational databases", "author": ["I.R. Mansuri", "S. Sarawagi"], "venue": "In Proceedings of the International Conference on Data Engineering,", "citeRegEx": "Mansuri and Sarawagi,? \\Q2006\\E", "shortCiteRegEx": "Mansuri and Sarawagi", "year": 2006}, {"title": "Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu", "author": ["A. McCallum"], "venue": null, "citeRegEx": "McCallum,? \\Q2002\\E", "shortCiteRegEx": "McCallum", "year": 2002}, {"title": "Semantic annotation of unstructured and ungrammatical text", "author": ["M. Michelson", "C.A. Knoblock"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Michelson and Knoblock,? \\Q2005\\E", "shortCiteRegEx": "Michelson and Knoblock", "year": 2005}, {"title": "Unsupervised information extraction from unstructured, ungrammatical data sources on the world wide web. International Journal of Document Analysis and Recognition (IJDAR)", "author": ["M. Michelson", "C.A. Knoblock"], "venue": "Special Issue on Noisy Text Analytics,", "citeRegEx": "Michelson and Knoblock,? \\Q2007\\E", "shortCiteRegEx": "Michelson and Knoblock", "year": 2007}, {"title": "Creating relational data from unstructured and ungrammatical data sources", "author": ["M. Michelson", "C.A. Knoblock"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Michelson and Knoblock,? \\Q2008\\E", "shortCiteRegEx": "Michelson and Knoblock", "year": 2008}, {"title": "Exploiting background knowledge to build reference sets for information extraction", "author": ["M. Michelson", "C.A. Knoblock"], "venue": "In Proceedings of the 21st international jont conference on Artifical intelligence,", "citeRegEx": "Michelson and Knoblock,? \\Q2009\\E", "shortCiteRegEx": "Michelson and Knoblock", "year": 2009}, {"title": "Hierarchical wrapper induction for semistructured information sources", "author": ["I. Muslea", "S. Minton", "C.A. Knoblock"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Muslea et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Muslea et al\\.", "year": 2001}, {"title": "Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge", "author": ["M. Pasca", "D. Lin", "J. Bigham", "A. Lifchits", "A. Jain"], "venue": "In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pasca et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pasca et al\\.", "year": 2006}, {"title": "Extracting product features and opinions from reviews", "author": ["Popescu", "A.-M", "O. Etzioni"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Popescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2005}, {"title": "Deriving concept hierarchies from text", "author": ["M. Sanderson", "B. Croft"], "venue": "In Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval,", "citeRegEx": "Sanderson and Croft,? \\Q1999\\E", "shortCiteRegEx": "Sanderson and Croft", "year": 1999}, {"title": "Inducing ontology from flickr tags", "author": ["P. Schmitz"], "venue": "In Proceedings of the Workshop on Collaborative Web Tagging", "citeRegEx": "Schmitz,? \\Q2006\\E", "shortCiteRegEx": "Schmitz", "year": 2006}, {"title": "Information extraction from wikipedia: moving down the long tail", "author": ["F. Wu", "R. Hoffmann", "D.S. Weld"], "venue": "In Proceedings of the 14th ACM international conference on Knowledge discovery and data mining,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Given that posts do not conform to English grammar, Natural Language Processing approaches to extraction would not work on posts (Ciravegna, 2001).", "startOffset": 129, "endOffset": 146}, {"referenceID": 23, "context": "Further, since the posts are unstructured, wrapper methods would not work either (Muslea et al., 2001; Crescenzi et al., 2001).", "startOffset": 81, "endOffset": 126}, {"referenceID": 6, "context": "Further, since the posts are unstructured, wrapper methods would not work either (Muslea et al., 2001; Crescenzi et al., 2001).", "startOffset": 81, "endOffset": 126}, {"referenceID": 26, "context": "To build entity trees from the bigrams we use a modified version of the Sanderson and Croft (1999) heuristic for finding token subsumptions, with the notion that if token x subsumes y, then y is a child node under x in the entity tree that contains x.", "startOffset": 72, "endOffset": 99}, {"referenceID": 18, "context": "For this we used MALLET (McCallum, 2002) to implement two different CRF extractors.", "startOffset": 24, "endOffset": 40}, {"referenceID": 0, "context": "For example, some previous work discards concepts from the topic hierarchy if they consist of multiple terms (Bast et al., 2006).", "startOffset": 109, "endOffset": 128}, {"referenceID": 0, "context": "This is in contrast to other methods for term subsumption that use Principle Component Analysis (PCA) (Dupret & Piwowarski, 2006; Bast et al., 2006) and run in O(n3) time with respect to the token-by-token matrix (which may be sparse).", "startOffset": 102, "endOffset": 148}, {"referenceID": 27, "context": "There is also previous work that uses outside information sources, such as links between image tags and the users who supply them on Flickr, to aid in the building of term hierarchies (Schmitz, 2006).", "startOffset": 184, "endOffset": 199}, {"referenceID": 25, "context": "However, these methods are not as well suited as the Sanderson and Croft method (1999) that we chose for our problem.", "startOffset": 53, "endOffset": 87}], "year": 2010, "abstractText": "Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text \u201cposts.\u201d Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called \u201creference sets,\u201d by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machineconstructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data.", "creator": "TeX"}}}