{"id": "1401.3836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data", "abstract": "Crowdsourcing platforms offer a practical solution to the problem of affordably annotating large datasets for training supervised classifiers. Unfortunately, poor worker performance frequently threatens to compromise annotation reliability, and requesting multiple labels for every instance can lead to large cost increases without guaranteeing good results. Minimizing the required training samples using an active learning selection procedure reduces the labeling requirement but can jeopardize classifier training by focusing on erroneous annotations. This paper presents an active learning approach in which worker performance, task difficulty, and annotation reliability are jointly estimated and used to compute the risk function guiding the sample selection procedure. We demonstrate that the proposed approach, which employs active learning with Bayesian networks, significantly improves training accuracy and correctly ranks the expertise of unknown labelers in the presence of annotation noise.", "histories": [["v1", "Thu, 16 Jan 2014 04:51:19 GMT  (1248kb,D)", "http://arxiv.org/abs/1401.3836v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["liyue zhao", "yu zhang", "gita sukthankar"], "accepted": false, "id": "1401.3836"}, "pdf": {"name": "1401.3836.pdf", "metadata": {"source": "CRF", "title": "An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data", "authors": ["Liyue Zhao", "Yu Zhang", "Gita Sukthankar"], "emails": ["lyzhao@cs.ucf.edu", "yuz1988@iastate.edu", "gitars@eecs.ucf.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they are able to"}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that the number of unemployed able to do their jobs continues to increase, while the number of unemployed able to do their jobs continues to increase. In recent years, the number of unemployed able to do their jobs has multiplied in recent years, and the number of unemployed able to do their jobs has multiplied. In the last ten years, the number of unemployed able to do their jobs has multiplied. In the last ten years, the number of unemployed able to do their jobs has tripled, in the last ten years, the number of unemployed able to do their jobs has tripled, and in the last ten years, the number of unemployed able to do their jobs has tripled. In the last ten years, the number of unemployed among the unemployed has tripled, and the number of unemployed has tripled."}, {"heading": "3 Method", "text": "In this section, we describe our active learning approach to jointly evaluating work performance and annotation reliability. In the first part of the section, the probabilistic graphical model is defined to assess the expertise of labels and the difficulty of annotation tasks, before describing how EM is used to estimate model parameters. In the second section, our active learning approach to sampling workers and tasks using an entropy-based risk function is presented."}, {"heading": "3.1 Probabilistic Graphical Model", "text": "The structure of the graphical model is shown in Figure 1. The same model is used to estimate the expertise of employees, the difficulty of annotation tasks and the true names. However, the expertise of the employee i is defined as a procedure that corresponds to the level of annotation accuracy of the employee. As a procedure, the employee i becomes an increasingly capable oracle that almost always indicates the correct designation and procedure - the employee almost always provides the wrong designation. This means that the employee has no ability to distinguish the difference between the two classes and only randomly guess a designation. The difficulty of the task j is parameterized by \u03b2j, where 1 \u03b2j (0, + 1). For simple tasks, 1\u03b2j approaches become zero, and it is assumed that practically every employee can give the correct designation for the task."}, {"heading": "3.2 Query-by-Uncertainty", "text": "The key question in active learning is which criterion performs best in identifying the most valuable unlabeled sample among the current parameters. We assume that the class of labels is defined as C = {\u2212 1, + 1}, and the risk function is algorithm 1 Active Learning Algorithm Require: Input: A Set of data along with a matrix of sub-labels L The initial set of parameters is defined as B: Labeling Budget. Output: Sampling Venture Capital = {\u03b1, + 1}: The set of parameters representing the expertise of the labelers and the difficulty of the tasks is 1: while B > 0 do 2: Use the EM algorithm to update the parameters."}, {"heading": "4 Worker Selection", "text": "The selection of the task to be labeled could easily be solved by using active learning strategies to identify the most insecure or informative tasks for the workers. However, how to select the right worker to comment on the task is another problem that lies outside the scope of active learning. Selecting the \"most insecure\" worker can improve the appraisal of the worker's expertise, but at the same time reduce the speed to improve training accuracy, since such a strategy inevitably selects the workers who know the system least and not the best workers. The easiest way we have used in Algorithm 1 is to ask the worker with the highest expertise to provide the labeling. Experimental results show that this strategy works well. However, there are some arguments that challenge the strategy, as 1)."}, {"heading": "5 Experiments", "text": "The aim of our experiments is to show that in cases where the tasks are not labeled by all labels, the actual results based on our proposed sampling strategy are compared with the random sampling strategy when selecting the samples to be labeled. Although there are three sets of parameters in the model, which is the expertise of the label i, \u03b2j, which is the difficulty of the task j and zj, which is the true label of the task, our proposed method proves that the focus on estimating a better labeling expertise is more important than the difficulty of the task \u03b2 in predicting the real labeling of the task z, \u03b2j which is the difficulty of the task, given the same number of training labels, active sampling has advantages in 1) more accurate labels and 2) the determination of a more correlated ranking of labelers.In the following experiments, we test the performance of our proposed active learning algorithm on 1) a pool of sources of workers and 2) a crowd of tasks."}, {"heading": "6 Discussion", "text": "For real-world crowdsourcing applications, annotation accuracy and budget constraints are obviously the most important immediate criteria for evaluating the performance of a learning model. However, identifying knowledgeable and reliable workers is potentially useful because these workers could be used in future annotation tasks. The difficulty of the tasks mainly serves as a differentiator for distinguishing between good and bad labels, rather than as a scale for evaluating the performance of the learning model. In active learning in particular, aggressive sampling is unable to meet all the criteria well, which is why task difficulties should be sacrificed for performance gains on the other metrics. Our algorithm aggressively chooses to use the best possible labor force that will produce improvements in labeling, but makes it difficult to accurately estimate the relative rank of poor labelers. We believe that a simple analysis of ranking correlation may not be the best way to evaluate real-world estimation of labels because certain labels should be eliminated early from a label level."}, {"heading": "7 Conclusion", "text": "Although crowd-sourced active learning annotations are an attractive and affordable idea for large-scale data labeling, the approach poses significant difficulties. Several studies in various research areas show that active learning approaches developed for noise-free annotations do not perform well with crowd-sourced data. This approach provides a practical approach to leveraging active learning in conjunction with Bayesian networks to model both the expertise of unknown labeling specialists and the difficulty of annotation tasks. Our work provides two contributions that allow us to robustly train a probabilistic graphical model under these challenging conditions. First, we propose original and efficient sampling criteria that iteratively assign the tasks with the highest labeling risk to the most reliable labelers. Second, we present comprehensive assessments of both simulated and real data sets that demonstrate the strength of our proposed approach to significantly reduce the number of labels required for the training model."}], "references": [{"title": "Crowdsourcing systems on the world-wide web", "author": ["A. Doan", "R. Ramakrishnan", "A.Y. Halevy"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A probabilistic framework to learn from multiple annotators with time-varying accuracy", "author": ["P. Donmez", "J.G. Carbonell", "J. Schneider"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "The rise of crowdsourcing", "author": ["J. Howe"], "venue": "Wired Magazine,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Algorithms for the multi-armed bandit problem", "author": ["Volodymyr Kuleshov", "Doina Precup"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Supervised learning from multiple experts: Whom to trust when everyone lies a bit", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "A. Jerebko", "C. Florin", "G.H. Valadez", "L. Bogoni", "L. Moy"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Get another label? improving data quality and data mining using multiple, noisy labelers", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": "In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Active learning for parameter estimation in Bayesian networks", "author": ["S. Tong", "D. Koller"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Far-sighted active learning on a budget for image and video recognition", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J. Movellan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Active learning from crowds", "author": ["Y. Yan", "R. Rosales", "G. Fung", "J. Dy"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, Washington,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Incremental relabeling for active learning with noisy crowdsourced annotations", "author": ["L. Zhao", "G. Sukthankar", "R. Sukthankar"], "venue": "In IEEE International Conference on Social Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Active learning from multiple noisy labelers with varied costs", "author": ["Y. Zheng", "S. Scott", "K. Deng"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Our work is motivated by the recent interest in the use of crowdsourcing [3] as a source of annotations from which to train machine learning systems.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "This has stimulated the use of approaches such as active learning [6] that aim to minimize the amount of data required to learn a highquality classifier.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Second, the quality of crowdsourced annotations has been found to be poor [7], with causes ranging from workers who overstate their qualifications, lack of motivation among labelers, haste and deliberate vandalism.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 11, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 12, "context": "Better results can be obtained by applying weighted voting which assigns different weights to labelers based on their previous performance [7, 12, 13].", "startOffset": 139, "endOffset": 150}, {"referenceID": 9, "context": "In this paper, we use the Generative model of Labels, Abilities, and Difficulties [10] in which labeler expertise and the task difficulty are simultaneously estimated using EM (ExpectationMaximization) to learn the parameters of a probabilistic graphical model which represents the relationship between labelers, tasks and annotation predictions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "[3] coined the phrase \u201ccrowdsourcing\u201d to describe the concept of outsourcing work to a cheap labor pool composed of everyday people who use their spare time to create content and solve problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] define crowdsourcing as enlisting a crowd of humans to help solve a problem defined by the system owners.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Crowdsourcing annotation services, such as Amazon\u2019s Mechanical Turk, have become an effective way to distribute annotation tasks over multiple workers [9]; however, Sheng et al.", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "[7] noted the problem that crowdsourcing annotation tasks may generate unreliable labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 10, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 4, "context": "Several works [2, 11, 5] propose different approaches to model the annotation accuracy of workers; all of these approaches assume there are multiple experts/annotators providing labels but that no oracle exists.", "startOffset": 14, "endOffset": 24}, {"referenceID": 1, "context": "[2] propose a framework to learn the expected accuracy at each time step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] focus on the multiple annotator scenario where multiple labelers with varying expertise are available for querying.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] use a probabilistic model both to evaluate the different experts and also to provide an estimate of the actual hidden labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Our work is strongly influenced by GLAD [10], which uses a probabilistic model to simultaneously estimate the labels, the labeler expertise, and the task difficulty which are represented as latent variables in the Bayesian network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "Tong and Koller[8] proposed a way to implement active learning in Bayesian networks with a simple structure.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "An alternative worker selection strategy is the -greedy selection algorithm, which has been used in studying the exploration-exploitation tradeoff in reinforcement learning [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "[10] while attempting to reduce the labeling budget required to reach the desired accuracy level using three different active learning strategies:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] asked 20 real human workers on Mechanical Turk to annotate 160 facial images by labeling them as either Duchenne or Non-Duchenne.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Crowdsourcing platforms offer a practical solution to the problem of affordably annotating large datasets for training supervised classifiers. Unfortunately, poor worker performance frequently threatens to compromise annotation reliability, and requesting multiple labels for every instance can lead to large cost increases without guaranteeing good results. Minimizing the required training samples using an active learning selection procedure reduces the labeling requirement but can jeopardize classifier training by focusing on erroneous annotations. This paper presents an active learning approach in which worker performance, task difficulty, and annotation reliability are jointly estimated and used to compute the risk function guiding the sample selection procedure. We demonstrate that the proposed approach, which employs active learning with Bayesian networks, significantly improves training accuracy and correctly ranks the expertise of unknown labelers in the presence of annotation noise.", "creator": "LaTeX with hyperref package"}}}