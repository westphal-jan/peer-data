{"id": "1401.3840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Grounding FO and FO(ID) with Bounds", "abstract": "Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounders output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems. We first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.", "histories": [["v1", "Thu, 16 Jan 2014 04:53:20 GMT  (430kb)", "http://arxiv.org/abs/1401.3840v1", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI", "authors": ["johan wittocx", "maarten mari\\\"en", "marc denecker"], "accepted": false, "id": "1401.3840"}, "pdf": {"name": "1401.3840.pdf", "metadata": {"source": "CRF", "title": "Grounding FO and FO(ID) with Bounds", "authors": ["Johan Wittocx", "Maarten Mari\u00ebn", "Marc Denecker"], "emails": ["johan.wittocx@cs.kuleuven.be", "maarten.marien@cs.kuleuven.be", "marc.denecker@cs.kuleuven.be"], "sections": [{"heading": null, "text": "These systems provide the user with a rich first-order input language and can rely on efficient propositional solvers to perform the actual argumentation.In addition to a first-order theory and a finite domain, input for terrestrial citizens in many applications also includes additional data. By using this data, the size of the output of terrestrial citizens can often be significantly reduced. In this paper, we present a method for automatically calculating and supplementing this redundant information. Our method therefore simplifies the task of writing input theories that can be efficiently grounded by current systems. First, we present our method for classical first-order logic (FO) and then we expand it to FO (ID), the extension of FO to include inductive theories that allow our practical applications and more comprehensive validation theories."}, {"heading": "1. Introduction", "text": "Grounding, or propositionalization, is the task of reducing a first-order theory and a finite domain to an equivalent propositional theory called grounding. (It is) grounding can be used as a pre-processing stage in many logic-based reasoning systems. (It serves to provide the user with a rich input language while allowing the system to rely on efficient propositional approaches to perform actual reasoning.) Examples of systems based on grounding can be found in the finite first-order model generation (Claessen & So \ufffd rensson, 2003; East, Iakhiaev, Mikitiuk, & Truszczyn, 2006; Mitchell, Ternovska, Hach & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Marie \u00bc n, & Denecker, 2008d). Such systems are in turn used as part of Theorem Clavers (2003, Jackson & Leigh, 2006)."}, {"heading": "2. Preliminaries", "text": "In this section, we present the conventions and notations used in this essay. We assume that the reader is familiar with FO."}, {"heading": "2.1 First-Order Logic", "text": "It is not the first time that we have had to deal with the question whether it is a formal formula that represents a formula of nullarity, if we abuse the formula of denomination, we will often ignore it and simply write it < amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp"}, {"heading": "2.2 Rewriting and Term Normal Form", "text": "In this paper we will use the following well-known equivalents to rewrite formulas to logically equivalent formulas: 1. Shifting quantifiers (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying, (quantifying), (quantifying), (quantifying), (quantifying), (quantifying), (quantifying, (quantifying), (quantifying), (quantifying, (quantifying), (quantifying,), (quantifying, (quantifying), (quantifying,), (quantifying, (quantifying), (quantifying,), (quantifying, (quantifying,), (quantifying, (quantifying), (quantifying,), (quantifying, (quantifying), (quantifying), (quantifying, (quantifying), (quantifying, (quantifying,), (quantifying), (quantifying, (quantifying,), (quantifying, (quantifying,), (quantifying, (quantifying), (quantifying,), (quantifying, (quantifying,), (quantifying, (quantifying,), (quantifying, (quantifying,), (quantifying,), (quantifying,), (quantifying, (quantifying,), (quantifying,), (quantifying, (quantifying,), (quantifying, (quantifying,), (quantifying,), (quantifying), (, (quantifying, (,),"}, {"heading": "2.3 SAT", "text": "A statement theory (PC theory) is a theory of a statement vocabulary. A statement clause is a separation of statement words. A PC theory is in conjunctural normal form (CNF) when all sentences are sentences. Boolean satisfaction problem (SAT) is the NP-complete problem when deciding on a PC theory whether it is satisfactory. The NP search problem that corresponds to a SAT problem is the problem of calculating a witness of the decision problem in the form of a theory model. SAT solvers typically operate by constructing such a model. Contemporary SAT solvers perform impressively, so many NP problems can be efficiently solved by translating them into SAT. For example, this happens in the areas of model generation (Claessen & Sun rensson, 2003), planning (Kautz & Krotz, 2003), when they expect a general testimony 2003 as a buildable model (SAT), 1996 (SAT) and an answer."}, {"heading": "3. Model Generation and Grounding", "text": "Model generation is the problem of calculating a model of a logic theory T, usually in the context of a given finite domain, typically the Herbrand universe. A model generator makes it possible to determine the satisfaction of the theory in the context of that fixed domain. This is useful, for example, in the context of lightweight verification (Jackson, 2006). Beyond determining satisfaction, there is a broad class of problems, the answers to which are naturally given by models of a declarative domain theory. Thus, the model of a theory specifying a termination range typically contains a (correct) timetable. Thus, a model generator applied to this theory becomes the termination problem for that domain.1 This idea of model generation as an explanatory problem-solving paradigm was advanced in the field of ASP (Marek & Truszczyn'ski, 1999; Niemela \ufffd, 1999). In this area, answers to the problem of model generation can be presented as an additional problem by ASP-leading ASP theory problems."}, {"heading": "3.1 The Model Expansion Search Problem", "text": "The model of expansion theory is referred to as the model of expansion. < V = > Model expansion for a logic L, referred to as MX (L), is defined as follows. < T \u00b7 \u00b7 \u00b7 Let T be an L theory of a vocabulary problem that is, in fact, a subvocabulary of the vocabulary structure of the problem, the vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary-vocabulary"}, {"heading": "3.2 Reducing MX(FO) to SAT", "text": "For the rest of this paper, T is a theory between the models of T and T, namely a theory between the models of Tprop. (Marek & Truszczyn, 1999; Niemela, 1999).Let's have a theory between the models of T and T. (Marek & Truszczyn, 1999; Niemela, 1999).Let's have a theory between the models of Tprop. (D) Let's have a theory between the models of T and T. (Marek & Truszczyn, 1999; Niemela, 1999).Let's have the vocabulary of Tprop. (D) Let's have a theory between the models of T and T. (D) Let's have a theory between the models of T and T. (Marek & Truszczyn, 1999; Niemela, 1999).Let's have the vocabulary of Tprop. (D) Let's have a theory between the models of T and T."}, {"heading": "3.2.1 Grounding Algorithms", "text": "For the rest of this section, we assume that T is a theory in TNF. (As explained in Section 2.2, we can make this assumption without loss of universality.) We assume that the complete grounding of T with respect to the na\u00efve grounding mentioned in the introduction is in fact a complete grounding and definition of the complete grounding. (...) We assume that the complete grounding of T with respect to the na\u00efve grounding defined in the introduction is actually a complete grounding. (...) The complete grounding of T with respect to the complete grounding and definition. (...) We assume that the complete grounding of T is with respect to the complete grounding and definition. (...) The complete grounding of T is with respect to the complete grounding. (...) The complete grounding of T is with respect to the complete grounding. (...) The complete grounding of T is with respect to the complete grounding and definition. (...) The complete grounding of T is with respect to the complete grounding."}, {"heading": "4. Grounding with Bounds", "text": "In this section, we will present our method for reducing the grounding quantity. As mentioned in the introduction, it is based on calculation limits for subformulas of Input Theory T. Each limit for a subformula [x] is a formula above the input vocabulary \u03c3. It describes a series of tuples d for which any boundary applies (incorrectly) in any model of T expansion. The larger the quantity described by a boundary, the more precise the boundary is. Note that the fact that boundaries are formulas above \u03c3 means that they apply to the given structure I\u03c3.In Section 4.1, we will formally define boundaries. Then, we will show how boundaries can be inserted into T to obtain a new theory T. The reduced grounding of T is often much smaller than the reduced grounding of T. The more precise the inserted boundaries are, the smaller the grounding of T."}, {"heading": "4.1 Bounds", "text": "We distinguish between two kinds of boundary.Definition 6. Definition 6: Definition of a definite boundary (ct-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c) with respect to a formula (c-bound) with respect to a formula (c) with respect to a formula (c-bound) with respect to a formula (c) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound (c) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound (c) with respect to a formula (c-bound) with respect to a formula (c with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to a formula (c-bound) with respect to"}, {"heading": "4.2 C-Transformation", "text": "For the rest of this section, fix a c-map C for T is essentially equivalent. < c-map C for T is essentially equivalent. < c-map C for T is essentially equivalent. < c-map T is essentially equivalent. < c-map T is essentially equivalent. < c-map T is essentially equivalent. < c-map T is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent. < c is essentially equivalent."}, {"heading": "4.3 Atom-Based and Atom-Equal C-Maps", "text": "This means that we can find a basic formula for T in relation to the Izquier substitution. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p is the basic formula for C < p > p > p > p > p > p is the reduced basic formula for the reduced basic formula of p > p > p > p > p; p > p > p > p > p > p > p > p is the basic formula for the reduced basic formula of &ltC; p > p > p > p > p > p > p is the basic formula for the reduced basic formula of p > p > p > p; p > p > p > p > p > p > p is the basic formula for the reduced basic formula of &ltC; p > p > p > p is the basic formula for the reduced basic formula of p > p > p > p; p > p > p > p > p is the basic formula for the basic formula of p > p > p > p > p > p."}, {"heading": "4.4 Computing Bounds", "text": "We now present an algorithm for calculating a (non-trivial) c-map C. It is based on our work on the approximate reasoning for FO (Wittocx, Marie \ufffd n, & Denecker, 2008a). Generally, the resulting cmap is neither atom-based nor atom-equal, but an atom-based, atom-like c-map can be derived from it."}, {"heading": "4.4.1 Refining C-Maps", "text": "It is not only a question of time, but also a question of time, in which we have to deal with the question of how this situation is to come about. (...) It is a question of time in which a solution can come about. (...) It is a question of time in which a solution must come about. (...) It is a question of time in which a solution will come about. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...) It is a question of time in which a solution will come. (...)"}, {"heading": "4.4.2 Extracting an Atom-Based and Atom-Equal C-Map", "text": "The algorithms obtained from the refinement are generally neither atom-based nor atom-equal. To derive from any c-map C an atomic-like c-map, which is at least as precise as C, we first collect for each predicate P all boundaries associated with the events of P in theory. Then, the disjunction of these boundaries is mapped as a new map for each occurrence of P. Since all boundaries associated with the atoms via P are essentially the same, we have an atomic-like c-map. We now formally present this method: Definition 28. Let C be a c-map for a TNF theory T and P / n be a prediction. Let P (x11,., x1n),., P (xm1,. xmn) all occurrences of P in T and let y1,."}, {"heading": "5. Inductive Definitions", "text": "Although all NP problems can be discarded as MX (FO) problems, modeling such problems with pure FO can be extremely complex. In practice, modeling is often greatly improved by using FO extensions with constructs such as inductive definitions, subgrades, aggregates, subfunctions, and arithmetics.For this enriched language, we have implemented the model generator idp (Wittocx et al., 2008b; Wittocx & Marie \ufffd n, 2008).2In this paper, we focus on grounding the FO extension with inductive definitions. It is well known that in arbitrary areas, inductively definable concepts such as \"accessibility\" are not to be regarded as FO expressible. In finite domains, however, they can be coded (e.g. by coding the fixed-point construction), but the process is lengthy and leads to large theories. In this section, we will expand the mono algorithm to FO problems."}, {"heading": "5.1 Three-Valued Structures", "text": "While FO (ID) has a standard two-value semantics, three-value structures are used in the formal semantics of definitions. In fact, an inductive definition defines a series by describing how to construct it. In semantics, the intermediate phases of the construction of three-value sets are recorded, representing for each object whether it belongs to the set or not, or whether it has not yet been derived. Therefore, we remember the basic concepts of three-value logic. We designate the truth values true, false, and unknown by t, f, and u. A three-value interpretation I consists of a domain D and \u2022 a domain element xI."}, {"heading": "5.2 Inductive Definitions", "text": "A FO (ID) theory is a set of FO propositions and definitions. A definition is only a finite set of rules of Form3 (P) x (P) \u2190), 3. Normally, nested terms are allowed as arguments of P, but to facilitate representation, we only allow variables as arguments in this paper.Where P is a predicate and a FO formula, the free variables of P (x) are referred to as the head of the rule, of the body. Predictions that occur in the head of a rule are defined as predicates. The set of all defined predicates of the definition is denoted definition (ells). All other symbols are openly referred to as the structure of the body. The set of open symbols of the open symbols of the body is denoted by Open (D). Observe that an FO (ID) theory augments the occurrence of an FO theory with a collection of logical programs."}, {"heading": "5.3 Grounding Inductive Definitions", "text": "Like MX (FO) problems, MX (FO (ID) problems can be reduced by grounding to SAT (ID) problems. In this section, we expand grounding and the refinement algorithm from Section 4 to FO (ID). Without loss of generality (Marie \ufffd n, Gilis, & Denecker, 2004), we assume that none of the predicates of the input vocabulary is defined by a definition in T, and no predicate is defined by more than one definition. Furthermore, we assume that every rule body is in TNF."}, {"heading": "5.3.1 Full and Reduced Grounding", "text": "Let T be a FO (ID) theory. As for FO, grounding Tg for T in relation to I\u03c3 is a propositional FO (ID) theory, which is I\u03c3 equivalent to T. We expand the concept of complete and reduced grounding to definitions. Definition 37. The complete grounding of a rule \u0435x P (x) \u2190 in relation to I\u03c3 is the set {P (d) \u2190 Grvoll (B (x / d]) | d Dn}, where n is the number of variables in x. Likewise, the reduced grounding of x (P (x) \u2190) is the set {P (d) \u2190 Grred (B (X / d) | d (D) Dn}. The complete (reduced) grounding of a definition \u0445 is the union of the complete (reduced) grounding of all rules in T. The complete (reduced) grounding of an FO (ID) theory T is the set of complete (reduced) grounding of all definitions in T."}, {"heading": "5.3.2 Definitions Depending Only on \u03c3", "text": "We say that a definition that depends on the expansion symbols, if Open (\u2206) 6 \u03c3. If \u2206 does not depend on the expansion symbols, then the interpretation of each predicate in Def (\u0445) is the same in each model M of T. In fact, several algorithms are described in the deductive database literature to calculate wfm (M) for a definition that does not depend on expansion symbols. Most of them are defined only for definitions where each rule body is a conjunction of atoms, but some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technology (Ullman, 1988) can be easily adapted to the definition where each rule body is a conjunction of atoms."}, {"heading": "5.3.3 Bounds for Definitions", "text": "Definition 38. Definition is a subformula of a FO (ID) theory T, if there is a subformula of a sentence in T or a subformula of a rule body in a definition of T. A c-map for a FO (ID) theory T is a mapping of a ct- and cf-bound overshaping to each subformula of T. Note that a c-map has no boundaries for the heads of rules in a definition. Our strategy to calculate a c-map for a FO (ID) theory T is simple: construct the completion of T and apply the refinement algorithms to Comp (T) to get a c-map C for Comp (T). Restricting C to the subforms of T is a c-map for T. In fact, every subformula of T is in Comp (T) and since T."}, {"heading": "6. Implementation and Experiments", "text": "In this section, we will discuss the efficiency and practical implementation of grounding with boundaries. A first problem was mentioned at the end of Section 4.4.2: an atom-based c-map C, calculated by the refinement algorithm, contains many repeated constraints on variables. To ground C < T > efficiently, such repetitions should be avoided as much as possible. Second, an efficient grounding algorithm should consult boundaries as soon as possible. Specifically, it should use boundaries to avoid unnecessary instances of variables, rather than subsequently removing those instances. As a case study, we will show in detail how to customize a basic top-down-style grounding algorithm to efficiently exploit boundaries. We will outline how the same principles for a bottom-up style can be applied as a basis. In the second part of this section, we will discuss some aspects of implementing this grounding algorithm to efficiently exploit those boundaries."}, {"heading": "6.1 Case Study: Top-Down Grounding with Bounds", "text": "For the rest of this section, it is assumed that T in TNF [x / d] is similarly wrong as it is in other areas. (...) [...] [...] [...]. [...]. [...]. [...]. [...]. [...]. [...]. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...).). (...).). (...).). (...).). (...).). (...). (...). (...).).). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...).). (...). (...). (...).). (...). (...).). (...). (...).). (...). (...).).). (...).). (...). (...). (...). (...).). (...). (...). (...).).). (...).).). (...)."}, {"heading": "6.2 Implementing the Refinement Algorithm and Querying Bounds", "text": "In this section we will discuss some aspects of the implementation of the refinement algorithm. As mentioned above, the application of a simplification method for first-order formulas to simplify boundaries at regular intervals is essential for good implementation, using Goubault's (1995) method. To this end, the boundaries must be represented by first-order binary decision diagrams. In this section, we will show that such representation can be applied without too much effort in the application of one-step refinements. In addition, the use of binary decision diagrams brings additional benefits: we will get a cheap equivalence check for boundaries and an elegant algorithm to query boundaries required for the implementation of algorithm 1. At the end of this section, we will discuss a stop criterion for the refinement algorithm and discuss implementation."}, {"heading": "6.2.1 First-Order Binary Decision Trees and Diagrams", "text": "We borrow the definition of EEEE EEE EEE EEE EEE EEE EE EEE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EEE EE EE EE EE EE EE EE EE EEE EE EE EE EE EE EE EE EE EE EE EE EEE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE EE E"}, {"heading": "6.2.2 Querying a Bound", "text": "In algorithm 1, the main operation performed on a bound hand ends in the replacement of a response by a response. In algorithm 1, it ends in a bound response. In algorithm 1, it ends in a bound response. In algorithm 2, it ends in a bound response. In algorithm 2, it ends in a bound response. In algorithm 2, it ends in a bound response. In algorithm 2, it ends in a bound response. In algorithm 2, it ends in a bound response. In algorithm 2, it ends in a divided response. In algorithm 2, it ends in a divided response. In algorithm 2, it ends in a divided response. In algorithm 2, it ends in a divided response. In algorithm 3, it ends in a divided response."}, {"heading": "6.2.3 A Stop Criterion for the Refinement Algorithm", "text": "As shown in Section 4.4, the c-map refinement would lead to a new approach that does not exceed these limits. Again, a fixed point can be found whose calculation can take a long time, and the boundaries assigned by the fixed point can be so complex that the query becomes very inefficient, indicating the need for a good stop criterion. (Simple stop criteria A very simple stop criterion for the number of one-stage refinements that can be performed to a given maximum number of m.) This may depend on the theory T. (Number of subforms in T), where C is somewhat less naive technique that can be combined with the previous one, limits the \"complexity\" of the boundaries by setting a fixed upper limit N to the number of nodes BDD."}, {"heading": "6.2.4 Implementation of the Refinement Algorithm", "text": "It is not only a question of time, but also of the time, in which it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what extent it is a question, to what is a question, to what is a question, to what extent it is a question, to what extent it is a question to what is a question, to what is a question, to what extent it is a question to what is a question, to what is a question to what is a question, to what is a question to what is a question, to what extent it is a question to what extent, to what is a question to what extent, to what extent it is a question to what extent it is a question, to what extent it is a question, to what is a question to what is a question, to what extent it is a question to what extent it is a question, to what is a question to what extent, to what is a question to what is a question, to what is a question to what is a question, to what extent, to what is a question to what is a question to what extent, to what is a question to what is a question, to what is a question"}, {"heading": "6.3 Experiments", "text": "This year it has come to the point where it has never been able to get to the top."}, {"heading": "7. Related Work", "text": "In the previous sections, we have described a method for achieving fast and compact grounding. In the literature, several such methods have been described, some of which - like ours - are pre-processing techniques that rewrite the input theory. Other techniques include argument at the statement level. In this section, we will give an overview of what methods can be used to improve GidL. We will also give an overview of existing grounding methods."}, {"heading": "7.1 Methods to Optimize Grounding", "text": "To our knowledge, the methods proposed in the literature for deriving boundaries are less general than those we have presented in this paper, illustrated by Table 5, in which we show the effects of adding redundant information manually for several terrestrials. For all terrestrials in this table except GidL, the manual addition of redundancy can have serious implications. For some terrestrials, the need to add redundancy can sometimes be avoided by writing the input theory in a particular format. For example, the terrestrial gringo (Gebser et al., 2007) uses a syntactical check to derive boundaries: it derives that predicate q of the input vocabulary is bound to predicate p when p7. The precise formulation and proof of the property lie outside the scope of this work."}, {"heading": "37 Wire routing 0.92 0.99", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "36 Weight bounded dominating set 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "35 Waterbucket 0.36 0.36", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "34 Train scheduling 0.25 0.25", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "33 Toughnut 0.00 0.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "32 Tarski 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "31 Sudoku 0.75 0.75", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 Spanningtree 0.06 0.06", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "29 Solitaire 1.00 0.73", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "28 Sokoban 0.59 0.59", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "27 Social golfer 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "26 Slitherlink 0.04 0.04", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "25 Disjunctive scheduling 0.83 0.83", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "24 Pigeonhole 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "23 N-queens 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "22 Missionaries 0.03 0.03", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "21 Mirror puzzle 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "20 Maze generation 0.90 0.90", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "19 Magic series 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "18 Labyrinth 0.99 0.99", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "17 Knighttour 0.00 0.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "16 Tower of Hanoi 1.00 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "15 Hamiltonian circuit 0.01 0.01", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "14 Algebraic groups 0.99 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "13 Graph partitioning 0.94 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12 Golomb ruler 0.54 1.00", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 FO-hamcircuit 0.94 0.99", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 Fastfood 1.00 1.00", "text": "It is not as if it could be a matter of the way it is applied in practice, as it is applied in practice. (It is not as if it is a matter of the way it is applied in practice. (It is not as if there could be such a method.) (It is not as if there were such a method.) (It is not as if there were such a method.) (It is not as if there were such a method.) Since dlv applies the rules before coercion, as if there were a restriction, as if there were a limit for p, not improved. (It is not as if there were such a method.) As we explain below, the order of the rules, the meaning of the rules, and the limitations are decisive for such a method. (It is not as if there were such a method.) It is not as if there were such a method, as if there were such a method, as if there were such a method. (It is not as if there were such a method, as if there were such a method.)"}, {"heading": "7.2 Grounders", "text": "It is a question of whether and in what form people in Turkey, in which they live, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey, in Turkey"}, {"heading": "8. Conclusions", "text": "We presented a calculation method for a given theory, upper and lower limits for all subformulas of that theory. We showed how these limits can be used efficiently in the context of model extension for FO and FO (ID). Our method frees the user from having to manually find boundaries and add them to a theory. We introduced a top-down grounding algorithm that incorporates boundaries. We discussed implementation problems and demonstrated through experiments that our method works in practice: for many benchmark problems it leads to significant reductions in grounding size and time.Future work includes extending our algorithm to calculate boundaries for richer logic, such as extensions of FO with aggregates and arithmetic. On the implementation side, we plan to use more complex estimators to assess whether a calculated boundary is beneficial for grounding."}, {"heading": "Acknowledgments", "text": "Research supported by the Research Foundation Flanders (FWO-Vlaanderen) and the GOA 2003 / 08 \"Inductive Knowledge Bases.\" Johan Wittocx is a research associate of the Research Foundation Flanders (FWO-Vlaanderen)."}], "references": [{"title": "Logic Programming and Nonmonotonic Reasoning, 9th International Conference, LPNMR 2007, Tempe", "author": ["C. Baral", "G. Brewka", "J.S. Schlipf"], "venue": "AZ, USA, May 15-17,", "citeRegEx": "Baral et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baral et al\\.", "year": 2007}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "Bryant,? \\Q1986\\E", "shortCiteRegEx": "Bryant", "year": 1986}, {"title": "Compiling problem specifications into SAT", "author": ["M. Cadoli", "A. Schaerf"], "venue": "Artificial Intelligence,", "citeRegEx": "Cadoli and Schaerf,? \\Q2005\\E", "shortCiteRegEx": "Cadoli and Schaerf", "year": 2005}, {"title": "On demand indexing for the DLV instantiator", "author": ["G. Catalano", "N. Leone", "S. Perri"], "venue": "Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP)", "citeRegEx": "Catalano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Catalano et al\\.", "year": 2008}, {"title": "New techniques that improve MACE-style model finding", "author": ["K. Claessen", "N. S\u00f6rensson"], "venue": "In Workshop on Model Computation (MODEL)", "citeRegEx": "Claessen and S\u00f6rensson,? \\Q2003\\E", "shortCiteRegEx": "Claessen and S\u00f6rensson", "year": 2003}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "Journal of the ACM,", "citeRegEx": "Davis and Putnam,? \\Q1960\\E", "shortCiteRegEx": "Davis and Putnam", "year": 1960}, {"title": "Estimation of the number of tuples satisfying a query expressed in predicate calculus language", "author": ["R. Demolombe"], "venue": "In International Conference on Very Large Data Bases (VLDB),", "citeRegEx": "Demolombe,? \\Q1980\\E", "shortCiteRegEx": "Demolombe", "year": 1980}, {"title": "Extending classical logic with inductive definitions", "author": ["M. Denecker"], "venue": "J. (Eds.), International Conference on Computational Logic (CL),", "citeRegEx": "Denecker,? \\Q2000\\E", "shortCiteRegEx": "Denecker", "year": 2000}, {"title": "Inductive situation calculus", "author": ["M. Denecker", "E. Ternovska"], "venue": "Artificial Intelligence,", "citeRegEx": "Denecker and Ternovska,? \\Q2007\\E", "shortCiteRegEx": "Denecker and Ternovska", "year": 2007}, {"title": "A logic of nonmonotone inductive definitions", "author": ["M. Denecker", "E. Ternovska"], "venue": "ACM Transactions on Computational Logic (TOCL),", "citeRegEx": "Denecker and Ternovska,? \\Q2008\\E", "shortCiteRegEx": "Denecker and Ternovska", "year": 2008}, {"title": "Well-founded semantics and the algebraic theory of nonmonotone inductive definitions", "author": ["M. Denecker", "J. Vennekens"], "venue": null, "citeRegEx": "Denecker and Vennekens,? \\Q2007\\E", "shortCiteRegEx": "Denecker and Vennekens", "year": 2007}, {"title": "The second answer set programming competition", "author": ["M. Denecker", "J. Vennekens", "S. Bond", "M. Gebser", "M. Truszczy\u0144ski"], "venue": "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),", "citeRegEx": "Denecker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Denecker et al\\.", "year": 2009}, {"title": "Tools for modeling and solving search problems", "author": ["D. East", "M. Iakhiaev", "A. Mikitiuk", "M. Truszczy\u0144ski"], "venue": "AI Communications,", "citeRegEx": "East et al\\.,? \\Q2006\\E", "shortCiteRegEx": "East et al\\.", "year": 2006}, {"title": "A Mathematical Introduction To Logic (Second edition)", "author": ["H.B. Enderton"], "venue": null, "citeRegEx": "Enderton,? \\Q2001\\E", "shortCiteRegEx": "Enderton", "year": 2001}, {"title": "Generalized first-order spectra and polynomial-time recognizable sets", "author": ["R. Fagin"], "venue": "Complexity of Computation,", "citeRegEx": "Fagin,? \\Q1974\\E", "shortCiteRegEx": "Fagin", "year": 1974}, {"title": "Rete: A fast algorithm for the many patterns/many objects match problem", "author": ["C. Forgy"], "venue": "Artificial Intelligence,", "citeRegEx": "Forgy,? \\Q1982\\E", "shortCiteRegEx": "Forgy", "year": 1982}, {"title": "Database System Implementation. PrenticeHall", "author": ["H. Garcia-Molina", "J.D. Ullman", "J. Widom"], "venue": null, "citeRegEx": "Garcia.Molina et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Garcia.Molina et al\\.", "year": 2000}, {"title": "GrinGo : A new grounder for answer set programming", "author": ["M. Gebser", "T. Schaub", "S. Thiele"], "venue": "In Baral et al. (Baral et al.,", "citeRegEx": "Gebser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gebser et al\\.", "year": 2007}, {"title": "A BDD-based simplification and skolemization procedure", "author": ["J. Goubault"], "venue": "Logic Journal of IGPL,", "citeRegEx": "Goubault,? \\Q1995\\E", "shortCiteRegEx": "Goubault", "year": 1995}, {"title": "Software Abstractions: Logic, Language, and Analysis", "author": ["D. Jackson"], "venue": null, "citeRegEx": "Jackson,? \\Q2006\\E", "shortCiteRegEx": "Jackson", "year": 2006}, {"title": "Pushing the envelope: Planning, propositional logic and stochastic search", "author": ["H.A. Kautz", "B. Selman"], "venue": "In National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence (AAAI/IAAI),", "citeRegEx": "Kautz and Selman,? \\Q1996\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1996}, {"title": "Comparative evaluation of approaches to propositionalization", "author": ["Krogel", "M.-A", "S. Rawles", "F. Zelezn\u00fd", "P.A. Flach", "N. Lavrac", "S. Wrobel"], "venue": "International Conference on Inductive Logic Programming (ILP),", "citeRegEx": "Krogel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Krogel et al\\.", "year": 2003}, {"title": "Improving ASP instantiators by join-ordering methods", "author": ["N. Leone", "S. Perri", "F. Scarcello"], "venue": "International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR),", "citeRegEx": "Leone et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Leone et al\\.", "year": 2001}, {"title": "Heuristics based on unit propagation for satisfiability problems", "author": ["C.M. Li"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Li,? \\Q1997\\E", "shortCiteRegEx": "Li", "year": 1997}, {"title": "Stable models and an alternative logic programming paradigm", "author": ["V.W. Marek", "M. Truszczy\u0144ski"], "venue": null, "citeRegEx": "Marek and Truszczy\u0144ski,? \\Q1999\\E", "shortCiteRegEx": "Marek and Truszczy\u0144ski", "year": 1999}, {"title": "Model Generation for ID-Logic", "author": ["M. Mari\u00ebn"], "venue": "Ph.D. thesis,", "citeRegEx": "Mari\u00ebn,? \\Q2009\\E", "shortCiteRegEx": "Mari\u00ebn", "year": 2009}, {"title": "On the relation between ID-Logic and Answer Set Programming", "author": ["M. Mari\u00ebn", "D. Gilis", "M. Denecker"], "venue": "European Conference on Logics in Artificial Intelligence (JELIA),", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2004}, {"title": "The IDP framework for declarative problem solving", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker"], "venue": "In Search and Logic: Answer Set Programming and SAT,", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2006}, {"title": "MidL: a SAT(ID) solver", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker"], "venue": "In 4th Workshop on Answer Set Programming: Advances in Theory and Implementation,", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2007}, {"title": "SAT(ID): Satisfiability of propositional logic extended with inductive definitions", "author": ["M. Mari\u00ebn", "J. Wittocx", "M. Denecker", "M. Bruynooghe"], "venue": "International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Mari\u00ebn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mari\u00ebn et al\\.", "year": 2008}, {"title": "Mace4 reference manual and guide. CoRR, cs.SC/0310055", "author": ["W. McCune"], "venue": null, "citeRegEx": "McCune,? \\Q2003\\E", "shortCiteRegEx": "McCune", "year": 2003}, {"title": "A framework for representing and solving NP search problems", "author": ["D.G. Mitchell", "E. Ternovska"], "venue": null, "citeRegEx": "Mitchell and Ternovska,? \\Q2005\\E", "shortCiteRegEx": "Mitchell and Ternovska", "year": 2005}, {"title": "Model expansion as a framework for modelling and solving search problems", "author": ["D.G. Mitchell", "E. Ternovska", "F. Hach", "R. Mohebali"], "venue": "Tech. rep. TR 2006-24,", "citeRegEx": "Mitchell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2006}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["I. Niemel\u00e4"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Niemel\u00e4,? \\Q1999\\E", "shortCiteRegEx": "Niemel\u00e4", "year": 1999}, {"title": "Grounding for model expansion in k-guarded formulas with inductive definitions", "author": ["M. Patterson", "Y. Liu", "E. Ternovska", "A. Gupta"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Patterson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2007}, {"title": "Reducing inductive definitions to propositional satisfiability", "author": ["N. Pelov", "E. Ternovska"], "venue": "International Conference on Logic Programming (ICLP),", "citeRegEx": "Pelov and Ternovska,? \\Q2005\\E", "shortCiteRegEx": "Pelov and Ternovska", "year": 2005}, {"title": "Enhancing DLV instantiator by backjumping techniques", "author": ["S. Perri", "F. Scarcello", "G. Catalano", "N. Leone"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Perri et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Perri et al\\.", "year": 2007}, {"title": "Compact propositional encodings of first-order theories", "author": ["D. Ramachandran", "E. Amir"], "venue": null, "citeRegEx": "Ramachandran and Amir,? \\Q2005\\E", "shortCiteRegEx": "Ramachandran and Amir", "year": 2005}, {"title": "A comparison of different techniques for grounding near-propositional cnf formulae", "author": ["S. Schulz"], "venue": "International Florida Artificial Intelligence Research Society Conference (FLAIRS),", "citeRegEx": "Schulz,? \\Q2002\\E", "shortCiteRegEx": "Schulz", "year": 2002}, {"title": "Exploiting subformula sharing in automatic analysis of quantified formulas", "author": ["I. Shlyakhter", "M. Sridharan", "R. Seater", "D. Jackson"], "venue": "Poster presented at Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Shlyakhter et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shlyakhter et al\\.", "year": 2003}, {"title": "The complexity of decision problems in automata and logic", "author": ["L.J. Stockmeyer"], "venue": "Ph.D. thesis,", "citeRegEx": "Stockmeyer,? \\Q1974\\E", "shortCiteRegEx": "Stockmeyer", "year": 1974}, {"title": "Implementation of local grounding for logic programs with stable model semantics", "author": ["T. Syrj\u00e4nen"], "venue": "Tech. rep", "citeRegEx": "Syrj\u00e4nen,? \\Q1998\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 1998}, {"title": "Lparse 1.0 user\u2019s manual. http://www.tcs.hut.fi/Software/smodels/ lparse.ps.gz", "author": ["T. Syrj\u00e4nen"], "venue": null, "citeRegEx": "Syrj\u00e4nen,? \\Q2000\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 2000}, {"title": "Logic Programs and Cardinality Constraints: Theory and Practice. Doctoral dissertation, TKK Dissertations in Information and Computer Science TKK-ICS-D12, Helsinki University of Technology, Faculty of Information and Natural Sciences, Department of Information and Computer Science, Espoo, Finland", "author": ["T. Syrj\u00e4nen"], "venue": null, "citeRegEx": "Syrj\u00e4nen,? \\Q2009\\E", "shortCiteRegEx": "Syrj\u00e4nen", "year": 2009}, {"title": "Kodkod: A relational model finder", "author": ["E. Torlak", "D. Jackson"], "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS),", "citeRegEx": "Torlak and Jackson,? \\Q2007\\E", "shortCiteRegEx": "Torlak and Jackson", "year": 2007}, {"title": "Principles of database and knowledge-base systems, Vol. I", "author": ["J.D. Ullman"], "venue": null, "citeRegEx": "Ullman,? \\Q1988\\E", "shortCiteRegEx": "Ullman", "year": 1988}, {"title": "The well-founded semantics for general logic programs", "author": ["A. Van Gelder", "K.A. Ross", "J.S. Schlipf"], "venue": "Journal of the ACM,", "citeRegEx": "Gelder et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Gelder et al\\.", "year": 1991}, {"title": "The idp system. http://www.cs.kuleuven.be/~dtai/krr/ software/idpmanual.pdf", "author": ["J. Wittocx", "M. Mari\u00ebn"], "venue": null, "citeRegEx": "Wittocx and Mari\u00ebn,? \\Q2008\\E", "shortCiteRegEx": "Wittocx and Mari\u00ebn", "year": 2008}, {"title": "Approximate reasoning in first-order logic theories", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "International Conference on Knowledge Representation and Reasoning (KR),", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}, {"title": "Grounding with bounds", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}, {"title": "The idp system: a model expansion system for an extension of classical logic", "author": ["J. Wittocx", "M. Mari\u00ebn", "M. Denecker"], "venue": "In Workshop on Logic and Search (LaSh),", "citeRegEx": "Wittocx et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wittocx et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 30, "context": "Examples of systems that rely on grounding can be found in the area of finite first-order model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczy\u0144ski, 2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Mari\u00ebn, & Denecker, 2008d).", "startOffset": 107, "endOffset": 302}, {"referenceID": 19, "context": "Such systems are in turn used as part of theorem provers (Claessen & S\u00f6rensson, 2003) and for lightweight software verification (Jackson, 2006).", "startOffset": 128, "endOffset": 143}, {"referenceID": 42, "context": "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009).", "startOffset": 102, "endOffset": 209}, {"referenceID": 43, "context": "Currently, almost all Answer Set Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele, 2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009).", "startOffset": 102, "endOffset": 209}, {"referenceID": 38, "context": "Methods like clause splitting (Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.", "startOffset": 30, "endOffset": 44}, {"referenceID": 30, "context": "For instance, this is done in the areas of model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 21, "context": "For instance, this is done in the areas of model generation (Claessen & S\u00f6rensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and relational data mining (Krogel et al., 2003).", "startOffset": 163, "endOffset": 184}, {"referenceID": 19, "context": ", in the context of lightweight verification (Jackson, 2006).", "startOffset": 45, "endOffset": 60}, {"referenceID": 33, "context": "This idea of model generation as a declarative problem solving paradigm has been pioneered in the area of ASP (Marek & Truszczy\u0144ski, 1999; Niemel\u00e4, 1999).", "startOffset": 110, "endOffset": 153}, {"referenceID": 30, "context": "As shown by Mitchell and Ternovska (2005), it follows from Fagin\u2019s (1974) theorem that model expansion for FO captures NP, in the following sense:", "startOffset": 12, "endOffset": 42}, {"referenceID": 14, "context": "As shown by Mitchell and Ternovska (2005), it follows from Fagin\u2019s (1974) theorem that model expansion for FO captures NP, in the following sense:", "startOffset": 59, "endOffset": 74}, {"referenceID": 33, "context": "In this paper we focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek & Truszczy\u0144ski, 1999; Niemel\u00e4, 1999).", "startOffset": 104, "endOffset": 147}, {"referenceID": 36, "context": "The grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": ", 2007) and the grounders gringo (Gebser et al., 2007) and GidL (Wittocx, Mari\u00ebn, & Denecker, 2008b) take this approach.", "startOffset": 33, "endOffset": 54}, {"referenceID": 42, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.", "startOffset": 59, "endOffset": 91}, {"referenceID": 43, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al.", "startOffset": 59, "endOffset": 91}, {"referenceID": 32, "context": "Examples of grounders with a bottom-up approach are lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell et al., 2006).", "startOffset": 133, "endOffset": 156}, {"referenceID": 7, "context": "In this section we will extend the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008).", "startOffset": 66, "endOffset": 110}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc.", "startOffset": 18, "endOffset": 48}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics.", "startOffset": 18, "endOffset": 489}, {"referenceID": 7, "context": "As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)\u2019s definitions can not only be used to represent mathematical concepts, but also for the sort of common sense knowledge that is often represented by logic programs, such as (local forms of) the closed world assumption, inheritance, exceptions, defaults, causality, etc. The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf, 1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow the presentation of this semantics from Denecker and Vennekens (2007).", "startOffset": 18, "endOffset": 709}, {"referenceID": 7, "context": "Denecker and Vennekens (2007) show that each terminal well-founded induction for \u2206 above \u0128 has the same limit, which corresponds to the wellfounded model of \u2206 extending \u0128|Open(\u2206), and is denoted by wfm\u2206(\u0128).", "startOffset": 0, "endOffset": 30}, {"referenceID": 7, "context": "MidL (Mari\u00ebn, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Mari\u00ebn, Wittocx, Denecker, & Bruynooghe, 2008) take a native approach. Mari\u00ebn (2009) provides details on the specific form of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary propositional FO(ID) theories into this form.", "startOffset": 25, "endOffset": 143}, {"referenceID": 15, "context": "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.", "startOffset": 45, "endOffset": 58}, {"referenceID": 45, "context": "But some of them, such as the Rete algorithm (Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to handle full FO bodies.", "startOffset": 99, "endOffset": 113}, {"referenceID": 40, "context": "Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974).", "startOffset": 59, "endOffset": 77}, {"referenceID": 18, "context": "One can use Goubault\u2019s (1995) method for this purpose.", "startOffset": 12, "endOffset": 30}, {"referenceID": 18, "context": "We borrow the definition of first-order BDDs from Goubault (1995). Let \u03c6, \u03c81 and \u03c82 be three formulas.", "startOffset": 50, "endOffset": 66}, {"referenceID": 18, "context": "Definition 45 (Goubault, 1995).", "startOffset": 14, "endOffset": 30}, {"referenceID": 18, "context": "Goubault (1995) showed that for every FO formula \u03c6 there exists a BDT \u03c6\u2032 such that \u03c6 and \u03c6\u2032 are equivalent.", "startOffset": 0, "endOffset": 16}, {"referenceID": 18, "context": "\u2022 An implementation of the refinement algorithm using BDDs allows us to use the simplification algorithm for BDDs of Goubault (1995).", "startOffset": 117, "endOffset": 133}, {"referenceID": 1, "context": "If \u03c6, \u03c8 and \u03c7[x, y] are represented by BDDs, then a BDD representing \u00ac\u03c6, \u2203x \u03c6, \u2200x \u03c6, \u03c6 \u2227 \u03c8, \u03c6 \u2228 \u03c8 and \u03c7[x/x\u2032, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).", "startOffset": 141, "endOffset": 171}, {"referenceID": 18, "context": "If \u03c6, \u03c8 and \u03c7[x, y] are represented by BDDs, then a BDD representing \u00ac\u03c6, \u2203x \u03c6, \u2200x \u03c6, \u03c6 \u2227 \u03c8, \u03c6 \u2228 \u03c8 and \u03c7[x/x\u2032, y] can be computed efficiently (Bryant, 1986; Goubault, 1995).", "startOffset": 141, "endOffset": 171}, {"referenceID": 6, "context": "Given I\u03c3, it is possible to find a good estimate for the number of answers to \u03c8 in I\u03c3 (Demolombe, 1980), which is in turn a measure for the precision of \u03c8.", "startOffset": 86, "endOffset": 103}, {"referenceID": 18, "context": "If we use Goubault\u2019s simplification algorithm for BDDs for implementing line 9, the worst case complexity of this step is non-elementary in the size of C(\u03c6) \u2228 \u03c8 (Goubault, 1995).", "startOffset": 161, "endOffset": 177}, {"referenceID": 17, "context": "For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p", "startOffset": 33, "endOffset": 54}, {"referenceID": 36, "context": "The grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the propositional level.", "startOffset": 31, "endOffset": 51}, {"referenceID": 12, "context": "This technique is applied by the grounder psgrnd (East et al., 2006), which uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan, 1997) as propagation methods.", "startOffset": 49, "endOffset": 68}, {"referenceID": 36, "context": "In this case, a good grounding order can be derived from the dependency graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007).", "startOffset": 98, "endOffset": 148}, {"referenceID": 19, "context": "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.", "startOffset": 35, "endOffset": 50}, {"referenceID": 19, "context": "Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level.", "startOffset": 35, "endOffset": 155}, {"referenceID": 30, "context": "Clause splitting Clause splitting is a well-known rewriting technique applied in MACE style model generation (McCune, 2003).", "startOffset": 109, "endOffset": 123}, {"referenceID": 4, "context": "The simple heuristic to guide clause splitting described by Claessen and S\u00f6rensson (2003) can directly be applied to choose which quantifiers to move inside.", "startOffset": 60, "endOffset": 90}, {"referenceID": 36, "context": "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction \u03c61 \u2227 .", "startOffset": 87, "endOffset": 107}, {"referenceID": 36, "context": "One of the important methods in the dlv grounder is the use of a backjumping technique (Perri et al., 2007) to efficiently find all instances of a conjunction \u03c61 \u2227 . . . \u2227 \u03c6n that are possibly true, given (an overestimation of) the possibly true instances of each of the conjuncts \u03c6i. In GidL, this backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if \u03c6 is the formula \u03c61 \u2227 . . . \u2227 \u03c6n, then line 12 amounts to finding all possible instances of \u03c6, while the cf-bounds for \u03c61, . . . , \u03c6n provide an overestimation of the possibly true instances of these conjuncts. Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly false instances of a disjunction are calculated. Catalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.", "startOffset": 88, "endOffset": 783}, {"referenceID": 37, "context": "Partition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding technique that can reduce the grounding size of FO theories, depending on the availability of some", "startOffset": 26, "endOffset": 55}, {"referenceID": 30, "context": "mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO.", "startOffset": 5, "endOffset": 19}, {"referenceID": 42, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al.", "startOffset": 49, "endOffset": 81}, {"referenceID": 43, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al.", "startOffset": 49, "endOffset": 81}, {"referenceID": 17, "context": "Currently, there are three ASP grounders: lparse (Syrj\u00e4nen, 2000; Syrj\u00e4nen, 2009), gringo (Gebser et al., 2007) and the grounding component of dlv (Perri et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 36, "context": ", 2007) and the grounding component of dlv (Perri et al., 2007).", "startOffset": 43, "endOffset": 63}, {"referenceID": 20, "context": "This translation is described by Mari\u00ebn et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic program.", "startOffset": 33, "endOffset": 54}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.", "startOffset": 56, "endOffset": 197}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm.", "startOffset": 56, "endOffset": 566}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of \u22a5. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS).", "startOffset": 56, "endOffset": 2077}, {"referenceID": 7, "context": "This is the approach taken by MXidL (Mari\u00ebn, Wittocx, & Denecker, 2006). The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a \u201cbottom-up approach\u201d (see Section 3.2.1). To construct a grounding of a sentence \u03c6, it first creates all possible groundings of the atomic subformulas. Then it combines these groundings using relational algebra operations, working its way up the syntax tree. Finally, a grounding for \u03c6 is obtained. Mitchell et al. (2006) report on an implementation, called mxg, of the algorithm. kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg, it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One of the features of kodkod is that it allows a user to give part of a solution to an MX problem as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this information to produce smaller groundings. GidL also allows for a three-valued structure as input. When applying the refinement algorithm, the set of tuples d for which the user indicates that P should be true is then used as initial ct-bound for P instead of \u22a5. Similarly for the cf-bound. This leads to more efficient and compact groundings. mace (McCune, 2003) and paradox (Claessen & S\u00f6rensson, 2003) are finite model generators for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards. Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also methods that build the grounding incrementally are applied in these systems to avoid recomputing every grounding from scratch. East et al. (2006) developed the grounder psgrnd for MX(PS). PS is a fragment of FO(ID), extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the ground theory to reduce memory usage and grounding size. The experiments performed by East et al. (2006) show that carefully designed data structures are of key importance to build an efficient grounder.", "startOffset": 56, "endOffset": 2347}, {"referenceID": 12, "context": "Finally, because of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding (East et al., 2006).", "startOffset": 212, "endOffset": 231}], "year": 2010, "abstractText": "Grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounder\u2019s output can often be reduced significantly. A common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. In this paper we present a method to compute and add such redundant information automatically. Our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems. We first present our method for classical first-order logic (FO) theories. Then we extend it to FO(ID), the extension of FO with inductive definitions, which allows for more concise and comprehensive input theories. We discuss implementation issues and experimentally validate the practical applicability of our method.", "creator": "TeX"}}}