{"id": "1401.3845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Resource-Driven Mission-Phasing Techniques for Constrained Agents in Stochastic Environments", "abstract": "Because an agents resources dictate what actions it can possibly take, it should plan which resources it holds over time carefully, considering its inherent limitations (such as power or payload restrictions), the competing needs of other agents for the same resources, and the stochastic nature of the environment. Such agents can, in general, achieve more of their objectives if they can use --- and even create --- opportunities to change which resources they hold at various times. Driven by resource constraints, the agents could break their overall missions into an optimal series of phases, optimally reconfiguring their resources at each phase, and optimally using their assigned resources in each phase, given their knowledge of the stochastic environment. In this paper, we formally define and analyze this constrained, sequential optimization problem in both the single-agent and multi-agent contexts. We present a family of mixed integer linear programming (MILP) formulations of this problem that can optimally create phases (when phases are not predefined) accounting for costs and limitations in phase creation. Because our formulations multaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase, they exploit structure across these coupled problems. This allows them to find solutions significantly faster(orders of magnitude faster in larger problems) than alternative solution techniques, as we demonstrate empirically.", "histories": [["v1", "Thu, 16 Jan 2014 04:56:30 GMT  (888kb)", "http://arxiv.org/abs/1401.3845v1", null]], "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["jianhui wu", "edmund h durfee"], "accepted": false, "id": "1401.3845"}, "pdf": {"name": "1401.3845.pdf", "metadata": {"source": "CRF", "title": "Resource-Driven Mission-Phasing Techniques for Constrained Agents in Stochastic Environments", "authors": ["Jianhui Wu", "Edmund H. Durfee"], "emails": ["jianhuiw@umich.edu", "durfee@umich.edu"], "sections": [{"heading": null, "text": "In this paper, we formally define and analyze this limited sequential optimization problem in both a single and multi-agent context. We present a family of Mixed Integer Linear Programming (MILP) formulations of this problem that can optimally generate phases (if phases are not predefined) that take into account the costs and constraints of phase creation. As our formulations simultaneously find the optimal resource allocation in each phase and the optimal strategies for using the allocated resources in each phase, they exploit the structure of these coupled problems, enabling them to find solutions much faster (orders of magnitude for larger problems) than alternative solutions, as we empirically demonstrate."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.1 Simple Illustrating Single-Agent Example", "text": "This year it has come to the point where it is only a matter of time before it will happen, until it happens, until it happens."}, {"heading": "1.2 Paper Overview", "text": "While the idea of reconfiguring resources to improve agent performance is relatively simple, as the preceding example suggests, it can be a difficult problem to optimally reconfigure resources, and the primary goal of our study in this paper is to design computationally efficient algorithms to solve precisely this class of challenging problems. To this end, we are developing a series of algorithms that can turn complex resource-driven problems into compact mathematical formulations, after which these algorithms can fruitfully exploit the problem structure, often resulting in a significant reduction in computational costs.This paper is organized as follows. Section 2 introduces background techniques. Section 3 begins with a relatively simple resource-driven single method that determines the problem of resource-driven mission phase conversion states, known as phase conversion states."}, {"heading": "2. Background", "text": "We will formulate the problems of resource-driven single and multi-agent missions using the proven formalism of Markov decision-making processes (MDPs), with extensions to restricted MDPs. This section summarizes the relevant aspects of these previously developed formalities and illustrates them using the example previously discussed in Section 1.1."}, {"heading": "2.1 Markov Decision Processes", "text": "In general, a classic discrete time, fully observable Markov Decision Process with a endite state space = 1 endite action space = 1 endite function = 1 endite action space = 1 endite function = 1 endite function = 1 finite action space = 1 finite action space {1, i,... n}.A is a finite action space. For a state i, A, P, R > (Puterman, 1994), Ai A represents the series of actions that can be performed in the state i.P = {pi, a, j} represents state transition probability where pi, a, j} is the probability that the agent will reach state j if it executes action a in state i.For any state i and action a, j pi, a, j must be no greater than one. DP, a, j = 1 means j = 1 the agent will always stay in the system when executing a in state i, a, a, a, a, a, a. a."}, {"heading": "2.2 Linear Programming", "text": "Value iteration and policy iteration algorithms are widely used in solving classical MDPs (Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998), but it is surprisingly difficult to expand these algorithms to incorporate additional constraints without significantly increasing rewards 2. In this paper, (a) b represents an exponent, while ab represents an alternative solution based on mathematical programming (Altman, 1998; Feinberg, 2000; Dolgov & Durfee, 2006), a method of forming an MDP into a linear program (the solution of which results in optimal political maximization of the expected reward)."}, {"heading": "2.3 Constrained MDPs", "text": "The formulation of unrestricted MDPs as linear programs makes it easy to consider additional constraints, including capacity constraints and resource constraints. Several of these limited optimization problems have been examined by Dolgov and Durfee (2006). In what follows, we summarize that the limited capacity constraints of models can be represented as < M, \u03b1, C >, where: M is the classic MDP (Section 2.1), represented as < S, P, R >.\u03b1 = {\u03b1i} on the probability distribution over initial states.C is the capacity constraint of agents, represented as < O, U, in which there is a finite amount of non-consumable execution resources, e.g. O = {camera, spectrometer, gripper, etc.}.C = {c} is a limited amount of agent capacity."}, {"heading": "3. Resource Reconfiguration in Single-Agent Systems", "text": "We are addressing our new techniques and outcomes, which build on the work of others as summarized in the previous section. In particular, we are expanding the representations and techniques for resolving the limited MDPs, in which resources are allocated before execution, to sequential MDPs, in which resource allocations may change during execution. As mentioned above, we are referring to the intervals in which an agent's resources cannot change as a phase, and to the states that represent an opportunity, but no obligation, to change resource allocation."}, {"heading": "3.1 Problem Definition", "text": "Formally speaking, a resource-driven single-agent switching problem (S-RMP) > optimization problem is a generalization of the restricted MDP optimization problem presented in Section 2.3, consisting of a Markov decision-making process M, an initial probability distribution \u03b1, capacity constraint C of the agent, and a resource configuration Constraint5 R, where M is a classic MDP, as described in Section 2.1.\u03b1 = {\u03b1i}, a probability distribution across states where \u03b1i is the probability that the agent starts in state i.C, the capacity constraint of the agent is, as described in Section 2.3.R, the resource reconfiguration constraint (sometimes called phase switching constraint), which specifies constraints on the creation of phase switching states in which the configured agent can reconfigure its resources and adapt its use to its limited capacities."}, {"heading": "If S \u2282 S is predefined, then \u03bbi = 0 : \u2200i \u2208 S but \u03bbi > \u03bb\u0302 : \u2200i \u2208 S \u2212 S . This is the case where the phase-switching states are dictated to the agent.", "text": "If the agent can select any subset of n states to be phase switching states, but if the costs incurred are deducted from the agent's reward, he may choose to leave some (or most!) states as non-phase switching states. Given M, \u03b1, C and R inputs, the aim of the MP optimization problem is to maximize the total expected benefits of the capacity-constrained agent by selecting a number of phase switching states as non-phase switching states. Given M, C and R inputs, the MP optimization problem is to maximize the total expected benefits of the capacity-constrained agent by identifying a series of phase switching optimization phases."}, {"heading": "3.2 Computational Complexity Analysis", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "3.3 Exploiting Fixed Phase-Switching States", "text": "In fact, most of them will be able to abide by the rules in which they find themselves."}, {"heading": "3.4 Determining Optimal Phase-Switching States", "text": "In a general S-RMP optimization problem, phase change states are not fully predetermined. Instead, given a defined set of eligible phase change states S, a set of costs (where \u03bbi denotes the cost of creating a suitable state i into a phase change state), and a cost limit for the expected cumulative reward, the agent's goal is to find an optimal phase change set S that cannot be used directly for the general S-RMP optimization problem in Section 3.3. In this section, we construct a mixed integer linear program whose solution maximizes the optimal set of phase change states, as well as optimal resource configurations and executable strategies within each phase."}, {"heading": "X as in Section 1.1. The phase-switching costs \u03bbi and the cost limit \u03bb\u0302 are given above. The optimal integer solution to the mixed integer linear program Eq. 5 is:", "text": "[1, 2, 3, 4, 5, 6] = [1, 0, 1, 0, 1, 0] [1, 2, 3, 3, 3, 3, 4, 5, 21, 2, 2, 2, 2, 3, 4, 2, 1, 31, 3, 3, 3, 3, 3, 5, 4, 5, 5, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "3.4.1 Variation: Maximizing the Total Reward, Accounting for Cost", "text": "This subsection illustrates the expandability of our MILP-based algorithm by showing how easily it can be revised to work for another useful variant of the S-RMP optimization problem, where the phase switching states are not predetermined (Section 3.3), nor is the cost of creating phase switching states limited (Section 3.4). We now assume that any state could be a phase switching state that could create as many states as desired, and that (similar to Section 3.4) the costs are associated with treating a state as a phase switching state. However, instead of being subject to some cost limits, these costs are now calibrated with the rewards associated with implementing measures. Now, the optimization problem is to maximize the overall expected reward, taking into account the costs of creating phase switching states without first determining which phase switching states or how many phase states there will be."}, {"heading": "3.4.2 Variation: Cost Associated with State Features", "text": "A final variation that we will briefly describe, which is similar to the multi-agent approach that will be described later in Section 4, is the case where the conditions that enable resource reconfiguration (phase switching) are associated with a subset of the properties of the world and not with a fully grounded state. As a simple example, we will consider the situation in which resources (e.g. software packages, control of a satellite, etc.) can be licensed / leased at certain times and at certain intervals, e.g. l: Resource reconfiguration can take place at the beginning of each hour, identifying which resources (within their capacity constraints) to hold for the next hour. It could be located in any number of states (e.g. physical locations, pending task queue, etc.), but resource reconfiguration can take place in any of them. Similarly, in the example where a robot reconfigures resources on a toolbox, the critical state of a toolbox corresponds to the one that it is enclosed in."}, {"heading": "3.5 Experimental Evaluation", "text": "The importance of these techniques ultimately depends on their computational efficiency in solving problems that are more difficult. In this section, we provide an empirical evaluation of our techniques, which focus on problems in a more complex state space and with greater resources. Our experiments are conducted on a simplified Mars rover domain simulation in which an autonomous rover operates in a stochastic environment. According to much of the literature on similar problems (Bererton, Gordon, & Thrun, 2003; Dolgov & Durfee, 2006), the Mars rover domain is represented by a network world."}, {"heading": "3.5.1 Experimental Setup", "text": "It is indeed the case that we will be able to go in search of a solution that will enable us to move to another world in which we are able, in which we are able, in which we are in which we are, in which we are in."}, {"heading": "3.5.2 Improvements to Solution Quality", "text": "We start the evaluation by showing the improved reward from the use of the phase strategy via the approach that does not take into account the possibility of a resource change in the middle of execution = 46. Consider first the case where five supply stations are distributed in the vicinity (the first station is always located at the START site and the remaining four stations are randomly distributed when the problem is generated).Other parameters are set as follows: n = 8, i.e. the size of the network world is 8 by 8, and | O | = 9, i.e. there are nine different types of resources in the system. Figure 4 gives average rewards for these experiments, with error bars here and in the graphed results during this work showing the standard deviation. Obviously, the utilization of resource reconfiguration opportunities (using the abstract MDP solver represented in Section 3.3) can significantly improve the average performance of the robot, i.e. the performance of the robot is determined by the average of the resource rewards of the selected phase, not exceeding the rewards of the three stations on average, if the resource utilization is higher than 40% of the rewards of the three stations."}, {"heading": "3.5.3 Computational Efficiency", "text": "A key objective of the work presented in this section is to design a computationally efficient solution to the S-RMP optimization problem. Section 3.2 has given a theoretical analysis of the computational complexity of the S-RMP optimization problem. This subsection is designed to demonstrate empirically the efficiency of the solution approach presented in this section in solving complex S-RMP optimization problems. To make the presentation more precise, only the runtime performance of the MILP-based algorithm described in Section 3.4 is shown, i.e. The focus on the standard S-RMP optimization problems in Section 3.1.7The two previous simple algorithms described in Section 3.2, only the resource integration approaches based on enumeration and MDP-based strategies can also be used."}, {"heading": "3.6 Summary", "text": "Up to this point, we have analyzed several variations of a resource-driven mission phase problem that correspond to multiple cases of phase switching constraints, and presented a set of computationally efficient algorithms for identifying and utilizing mission phases. We have demonstrated through analysis and experimentation that our approach can significantly reduce the computational cost of accurately solving a complex S-RMP optimization problem compared to previous approaches. Later in this paper, we will extend such techniques to multilayer stochastic systems."}, {"heading": "4. Resource Reallocation in Multiagent Systems", "text": "Further complications arise when an agent decides which resources he can use for himself, due to the potential of resources he can use for himself. It may be that his capacity is not sufficient to obtain the required resources."}, {"heading": "4.1 A Multiagent Example", "text": "We describe a simple multi-agent resource allocation problem here, which we will use to illustrate the different approaches in this section. In this example, two cooperative agents try to maximize their total expected reward in a ten-step interval. Each agent has three tasks. At any point in time, an agent can choose to continue with the task he has already begun (if there is one and the required resources are still being allocated to that agent), start a new task (and abort his current task if there is one), or simply do nothing. In addition, we say that a task that was previously aborted (and thus failed) can be retried, but no task can be completed more than once. Figure 8 shows the detailed information of the tasks in the example problem, including release (RL) time (i.e., the earliest time the task can be successfully started)."}, {"heading": "4.2 Background: Integrated Resource Allocation and Policy Formulation", "text": "In fact, it is so that it is a way in which the individual plot lines and plot lines of the individual plot lines in the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines of the individual plot lines of the individual plot lines and plot lines of the individual plot lines of the individual plot lines of the individual plot lines of the individual plot lines of the individual plot lines,"}, {"heading": "4.3 Multiagent Resource-Driven Mission Phasing Problem Definition", "text": "The Multiagent Resource-Driven Mission Phasing (M-RMP) problem is the sequential version of Dolgov and Durfees (One-Shot) Multiagent Resource Allocation problem, as it has just been described. However, a general multiagent mission-phasing problem can be solved precisely by taking the approach presented in Section 3 of the Common State and Action Spaces of the Interacting Agents (provided that each actor has a complete overview of the Common State), but such a solution would suffer from the curse of dimensionality, as the sizes of the Common State and Action Spaces grow exponentially with the number of agents. Thus, we will exploit the loose coupling of agents: agents interact only through their (now potentially repeated) disputes for common resources, and are otherwise independent of each other. In the Multiagent case of sequential Resource (re) allocation, each phase corresponds to a certain distribution of common resources among agents."}, {"heading": "4.4 Computational Complexity Analysis", "text": "This section begins with the theoretical analysis of the computational complexity of the M-RMP problem, where the problem of optimizing the M-RMP problem in general cannot be solved (Theorem 4.1. M-RMP optimization is NP-hard).Since his particular case - a one-time resource allocation and policy formulation - is demonstrably covered by a reduction in the KNAPSACK problem (Dolgov, 2006), M-RMP optimization is NP-hard.Given the solution of the M-RMP problem, the satisfaction of resource allocation and resource reallocation constraints can be verified in linear time. Thereafter, for any agent that incorporates its policies into its MDP model, the M-RMP optimization problem becomes a Markov chain that can be solved in polynomial time. That is, M-RMP optimization is in both NMP-MP-MP-NP-MP-NP-MP-Optimization and NP-NP-MP-NP-NP-NP-NP-MP-Optimization."}, {"heading": "4.5 Exploiting a Fixed Resource Reallocation Schedule", "text": "As in Section 3.3, we start with a simple variant of the problem, where the schedule of resource reallocation is set, i.e. resource reallocation does not cost more than 0 if the time step t is set in a predefined schedule, and the cost limit is set no other than that the (one-line) integrated resource reallocation must be applied directly to each phase. Instead, our approach links the phases together by modelling the transition expectancy. The details are given in the following MILP: Note that to emphasize the M-RMP emphasis on resource allocation (re), the allocation of actions as a whole is impracticable; our approach links the phases together by modelling the transition expectancy. highlighting the M-RMP emphasis on resource allocation (re) is generally impracticable and in the entire remaining phase of this section to exceed capacity constraints."}, {"heading": "4.6 Determining an Optimal Resource Reallocation Schedule", "text": "Without a predefined resource reallocation, the cost of resource reallocation and resource reallocation in each phase, as well as the optimal resource reallocation for each phase of resource reallocation for each phase of resource reallocation, will be expanded in Section 4.4.We will instead extend the MILP in Eq.9 to extend the problem decomposition, and the extension will be shown in Eq.10, where (probability preservation) the limitations of resource reallocation will not be changed."}, {"heading": "4.6.1 Variation: Maximizing the Total Reward, Accounting for Cost", "text": "As in the S-RMP issue (Section 3.4.1), the problem of the changed approaches to such binary redistribution is relatively simple because the way resources are redistributed is relatively easy to characterize. As in the S-RMP issue (Section 3.4.1), we can consider a variation of the M-RMP issue in which neither resource redistribution nor the number of times resources are redistributed is limited (Section 4.6), but rather that costs are incurred per resource redistribution and these costs are calibrated with the benefits of MDP policy. Thus, the optimization issue will charge a constant fee regardless of which resources and how many of them are redistributed at that time."}, {"heading": "4.7 Experimental Evaluation", "text": "We analyzed the computational complexity of the M-RMP problem in Section 4.4; in this section we evaluate empirically the effectiveness and computational efficiency of the MILP-based solution algorithms developed in this section, using a grid-world environment similar to the one used for S-RMP evaluation in Section 3.5.14."}, {"heading": "4.7.1 Experimental Setup", "text": "The starting point of each agent is always in the center of their network.15 The goal of the group of agents is to maximize their total expected reward within T-time steps. Rewards for tasks are randomly determined, i.e. the task is awarded as a reward (in a random order), each task is limited by its release time and time.The release time is the time the task becomes available, i.e., the attempt to complete the task before it will return zero rewards. The time limit is if the task is not available (in a random order)."}, {"heading": "4.7.2 Improvements to Solution Quality", "text": "Figure 14 shows the improvement in our sequential resource allocation compared to the previous one-shot resource allocation approach. The X axis of the figure represents the number of agents in the world, and the Y axis specifies the total expected reward of the agents. Other parameters are set as follows: T = 10, n = 5, and the other four randomly selected resource allocation options during execution, the agents can receive a much higher reward. For example, that five fixed resources have (re) allocation times (one in the initial time and the other four randomly and uniformly when the test problem is defined), the agents are available in the middle of the execution phase."}, {"heading": "4.7.3 Computational Efficiency", "text": "In order to determine the number of phases in which a solution will be found, we must go in search of a solution that is capable of finding a solution, that is capable of finding a solution, and that is capable of finding a solution that is capable, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution."}, {"heading": "4.8 Summary", "text": "In this section, we have presented, analyzed and empirically evaluated a MILP-based approach that automates the process of finding and utilizing optimal resource reallocation17. Neither MILP nor WDP use parallel calculations for a group of actors operating in complex environments with resource constraints and uncertainties, and our analytical and experimental results have shown that the approach can significantly reduce computing costs compared to previous approaches."}, {"heading": "5. Related Work", "text": "The Resource-Driven Mission Phase Problem (RMP) includes three intertwined component problems: mission (problem) mining, resource configuration, and policy formulation; each of these component problems has been studied in a variety of research fields, and the combinations of the two issues have also received much attention in recent years. This section provides an overview of related work and discusses why these earlier approaches are not directly applicable to the RMP problem that is of interest in this paper.As outlined in Section 3.1 and Section 4.3, RMP problems are defined by extending an unrestricted MDP model to resource constraints and phase change constraints. Organizing this section follows the path of this definition. It begins with a discussion of policy formulation techniques, followed by a discussion of resource configuration techniques, and then problem decomposition techniques and their combinations with policy formulation and / or resource configuration modes, but does not fit the previous discussion in the transition section, \"but this section clearly concludes with a discussion of a review of the transition.\""}, {"heading": "5.1 Policy Formulation.", "text": "The well-known Markov decision-making process was described in Section 2.1. By formulating a sequential decision problem in an MDP model, a number of efficient (polynomial) solution approaches, such as value repetition and policy iteration algorithms, can be used to calculate an optimal policy (Puterman, 1994). However, the direct application of these algorithms in resource-limited systems, such as the resource-driven mission phase problem, typically involves the incorporation of resource characteristics into the representation of the MDP state (and thus measures can be made dependent on the availability of resources), which will lead to an exponential increase in the size of the state space (Meuleau et al., 1998), i.e. the well-known challenge of the \"curse of dimensionality.\" Empirical results (Section 3.5.3) showed that exponentially large state space can lead to computational inefficiency."}, {"heading": "5.2 Resource Configuration.", "text": "Since in some areas it is impossible (or expensive) to solve resource constraints by modifying the physical architecture of the agent (for example, adding another battery to a robot already deployed on Mars), improving the performance of a limited agent under its limited architecture has been an active theme in recent years, i.e., a class of problems with \"limited optimism\" (Russell, 2002). The Cooperative Intelligent Real-Time Control Architecture (CIRCA) is one such research effort (Musliner, Durfee, & Shin, 1993, 1995). However, CIRCA uses a simple greedy, short-sighted approach to calculate viable strategies, starting with the creation of an optimal, unrestricted strategy without worrying about its real-time requirements, and then greedily fixing policies until they are executable in the real-time system. Unsurprisingly, the (fast) greedy approach adopted by CIRCA can result in suboptimal strategies that cannot fully exploit the capacity of the agent."}, {"heading": "5.3 Problem Decomposition.", "text": "In the literature of stochastic planning, of course, a number of decomposition problems are not possible in the MDP. Discovering the \"recurring classes\" of MDPs is such a fragmentation strategy that expresses an exact fragmentation of public space in an environment of uncertainty (Puterman, 1994; Boutilier, Dean, & Hanks, 1999) as it represents a specific subset of state space, which means that an actor enters a recursive class in which it does not matter what policies he adopts forever. Puterman (1994) proposed a modification of the Fox-Landi algorithms (Fox & Landi, 1968) to discover recursive classes. By discovering recursive classes, the MDP solver can derive an optimal overall policy by building an optimal policy in each recurrent class and then the construction and solution of a reduced MDP consisting of transient states."}, {"heading": "5.4 Mode Transition.", "text": "Finally, it is important to shift resource-oriented research from the \"transition phase\" to the \"final phase\" (Schrage & Vachtsevanos, 1999; Wills, Kannan, Heck, Schrage, & Vachtsevanos, 2001; Grupen, Hanson, & Riseman, 2005)."}, {"heading": "6. Conclusion", "text": "The work in this paper designed, analyzed, and evaluated a set of computationally efficient algorithms that can automatically identify and leverage resource reconfiguration opportunities in resource-constrained environments. Analytical and experimental results showed and emphasized that the mission phase approach, which includes problem solving, resource allocation, and policy formulation, can help limited stakeholders use resource reconfiguration opportunities prudently and effectively to improve their performance.This section concludes the paper with a summary of the main contributions of this work and a discussion of several promising future research directions."}, {"heading": "6.1 Summary of Contributions", "text": "This work took into account explicitly known opportunities in the middle of execution to reconfigure resources and change strategies, and designed computationally efficient algorithms (including an abstract MDP algorithm for resource reconfiguration problems of individual agents and a MILP-based algorithm for resource reallocation problems of multiple agents) to optimize the use of these fixed opportunities in complex stochastic systems.The empirical results (Figure 4 and Figure 14) confirmed that the utilization of such phase switching opportunities can significantly improve performance, especially in tightly confined systems (reward doubles in some test cases).As an extension of the utilization of fixed phase switching capabilities, Section 3.4 (for single layer systems) and Section 4.6 (for multi-layer systems) presents MILP-based algo rithms capable of automating the process of identifying and using mission-hash systems."}, {"heading": "6.2 Future Work", "text": "rteeteeteeteeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "Acknowledgments", "text": "This material is based on work partially supported by the DARPA / IPTO COORDINATORs Program and the Air Force Research Laboratory under Contract Number FA8750-05-C-0030 and by the Air Force Office of Scientific Research under Contract Number FA9550-07-1-0262. The views and conclusions contained in this document are those of the authors and should not be interpreted to represent the official policy of the Defense Advanced Research Projects Agency, the Air Force or the U.S. Government, neither explicitly nor implicitly. The authors thank Dmitri Dolgov and the three anonymous critics for their very helpful suggestions and comments, as well as Stefan Witwicki and Jim Boerkoel for their help in proofreading this article."}], "references": [{"title": "Constrained Markov decision processes with total cost criteria: Lagrange approach and dual LP", "author": ["E. Altman"], "venue": "Methods and Models in Operations Research, 48, 387\u2013417.", "citeRegEx": "Altman,? 1998", "shortCiteRegEx": "Altman", "year": 1998}, {"title": "A survey of model reduction methods for large-scale systems", "author": ["A.C. Antoulas", "D.C. Sorensen", "S. Gugercin"], "venue": "Contemporary Mathematics,", "citeRegEx": "Antoulas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Antoulas et al\\.", "year": 2001}, {"title": "Planning and resource allocation for hard real-time, fault-tolerant plan execution", "author": ["E.M. Atkins", "T.F. Abdelzaher", "K.G. Shin", "E.H. Durfee"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Atkins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Atkins et al\\.", "year": 2001}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V.R. Lesser", "C.V. Goldman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Becker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "A Markov decision process", "author": ["R. Bellman"], "venue": "Journal of Mathematical Mechanics, 6, 679\u2013684.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Auction mechanism design for multi-robot coordination", "author": ["C.A. Bererton", "G.J. Gordon", "S. Thrun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bererton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bererton et al\\.", "year": 2003}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "In Proceedings of the 16th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "Bernstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2000}, {"title": "Multiagent control of self-reconfigurable robots", "author": ["H. Bojinov", "A. Casal", "T. Hogg"], "venue": "Artificial Intelligence,", "citeRegEx": "Bojinov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bojinov et al\\.", "year": 2002}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Combinatorial auctions: A survey", "author": ["S. De Vries", "R. Vohra"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "Vries and Vohra,? \\Q2003\\E", "shortCiteRegEx": "Vries and Vohra", "year": 2003}, {"title": "Decomposition techniques for planning in stochastic domains", "author": ["T. Dean", "S.H. Lin"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dean and Lin,? \\Q1995\\E", "shortCiteRegEx": "Dean and Lin", "year": 1995}, {"title": "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments", "author": ["D.A. Dolgov"], "venue": "Ph.D. thesis, Computer Science Department, University of Michigan.", "citeRegEx": "Dolgov,? 2006", "shortCiteRegEx": "Dolgov", "year": 2006}, {"title": "Computationally-efficient combinatorial auctions for resource allocation in weakly-coupled MDPs", "author": ["D.A. Dolgov", "E.H. Durfee"], "venue": "In Proceedings of the 4th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Dolgov and Durfee,? \\Q2005\\E", "shortCiteRegEx": "Dolgov and Durfee", "year": 2005}, {"title": "Resource allocation among agents with MDP-induced preferences", "author": ["D.A. Dolgov", "E.H. Durfee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dolgov and Durfee,? \\Q2006\\E", "shortCiteRegEx": "Dolgov and Durfee", "year": 2006}, {"title": "A Fault-Tolerant Control Architecture for Unmanned Aerial Vehicles", "author": ["G.R. Drozeski"], "venue": "Ph.D. thesis, Georgia Institute of Technology.", "citeRegEx": "Drozeski,? 2005", "shortCiteRegEx": "Drozeski", "year": 2005}, {"title": "Iterative MILP methods for vehicle control problems", "author": ["M. Earl", "R. D\u2019Andrea"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Earl and D.Andrea,? \\Q2005\\E", "shortCiteRegEx": "Earl and D.Andrea", "year": 2005}, {"title": "Constrained discounted Markov decision processes and Hamiltonian cycles", "author": ["E. Feinberg"], "venue": "Mathematics of Operations Research, 25, 130\u2013140.", "citeRegEx": "Feinberg,? 2000", "shortCiteRegEx": "Feinberg", "year": 2000}, {"title": "An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix", "author": ["B. Fox", "D.M. Landi"], "venue": "Communications of the ACM,", "citeRegEx": "Fox and Landi,? \\Q1968\\E", "shortCiteRegEx": "Fox and Landi", "year": 1968}, {"title": "Decentralized control of cooperative systems: Categorization and complexity analysis", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldman and Zilberstein,? \\Q2004\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2004}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Linear Programming and Finite Markovian Control Problems", "author": ["L. Kallenberg"], "venue": "Mathematisch Centrum, Amsterdam.", "citeRegEx": "Kallenberg,? 1983", "shortCiteRegEx": "Kallenberg", "year": 1983}, {"title": "Smart resource reconfiguration by exploiting dynamics in perceptual tasks", "author": ["D. Karuppiah", "R. Grupen", "A. Hanson", "E. Riseman"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Karuppiah et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Karuppiah et al\\.", "year": 2005}, {"title": "Integer optimization models of AI planning problems", "author": ["H. Kautz", "J. Walser"], "venue": "Knowledge Engineering Review,", "citeRegEx": "Kautz and Walser,? \\Q2000\\E", "shortCiteRegEx": "Kautz and Walser", "year": 2000}, {"title": "Solving factored MDPs via non-homogeneous partitioning", "author": ["Kim", "K.-E", "T. Dean"], "venue": "Proceedings of the 17th international joint conference on Artificial intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2001}, {"title": "Toward hierarchical decomposition for planning in uncertain environments", "author": ["T. Lane", "L.P. Kaelbling"], "venue": "In Proceedings of the 2001 IJCAI Workshop on Planning under Uncertainty and Incomplete Information,", "citeRegEx": "Lane and Kaelbling,? \\Q2001\\E", "shortCiteRegEx": "Lane and Kaelbling", "year": 2001}, {"title": "A study of index structures for main memory database management systems", "author": ["T.J. Lehman", "M.J. Carey"], "venue": "In Proceedings of the 12th International Conference on Very Large Data Bases,", "citeRegEx": "Lehman and Carey,? \\Q1986\\E", "shortCiteRegEx": "Lehman and Carey", "year": 1986}, {"title": "Evolution of the GPGP/TAEMS domain-independent coordination framework", "author": ["V. Lesser", "K. Decker", "T. Wagner", "N. Carver", "A. Garvey", "B. Horling", "D. Neiman", "R. Podorozhny", "M.N. Prasad", "A. Raja", "R. Vincent", "P. Xuan", "X. Zhang"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Lesser et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lesser et al\\.", "year": 2004}, {"title": "Solving very large weakly coupled Markov decision processes", "author": ["N. Meuleau", "M. Hauskrecht", "Kim", "K.-E", "L. Peshkin", "L.P. Kaelbling", "T. Dean", "C. Boutilier"], "venue": "In Proceedings of the 15th National Conference on Artificial Intelligence,", "citeRegEx": "Meuleau et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1998}, {"title": "CIRCA: A cooperative intelligent real time control architecture", "author": ["D.J. Musliner", "E.H. Durfee", "K.G. Shin"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Musliner et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Musliner et al\\.", "year": 1993}, {"title": "World modeling for the dynamic construction of real-time control plans", "author": ["D.J. Musliner", "E.H. Durfee", "K.G. Shin"], "venue": "Artificial Intelligence,", "citeRegEx": "Musliner et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Musliner et al\\.", "year": 1995}, {"title": "Deliberation scheduling strategies for adaptive mission planning in real-time environments", "author": ["D.J. Musliner", "R.P. Goldman", "K.D. Krebsbach"], "venue": null, "citeRegEx": "Musliner et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Musliner et al\\.", "year": 2005}, {"title": "A tutorial on decomposition methods for network utility maximization", "author": ["D. Palomar", "M. Chiang"], "venue": "IEEE Journal on Communications,", "citeRegEx": "Palomar and Chiang,? \\Q2006\\E", "shortCiteRegEx": "Palomar and Chiang", "year": 2006}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["R. Parr"], "venue": "Proceedings of the 14th Conference in Uncertainty in Artificial Intelligence, pp. 422\u2013430.", "citeRegEx": "Parr,? 1998", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Combinatorial auction design", "author": ["A. Pekec", "M. Rothkopf"], "venue": "Management Science,", "citeRegEx": "Pekec and Rothkopf,? \\Q2003\\E", "shortCiteRegEx": "Pekec and Rothkopf", "year": 2003}, {"title": "Functional decomposition in a vehicle control system", "author": ["A. Phillips"], "venue": "Proceedings of the 2002 American Control Conference, pp. 3713\u20133718.", "citeRegEx": "Phillips,? 2002", "shortCiteRegEx": "Phillips", "year": 2002}, {"title": "Multi-time models for temporally abstract planning", "author": ["D. Precup", "R. Sutton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Precup and Sutton,? \\Q1998\\E", "shortCiteRegEx": "Precup and Sutton", "year": 1998}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Real-Time Management of Resource Allocation Systems: A Discrete Event Systems Approach", "author": ["S.A. Reveliotis"], "venue": "Springer-Verlag New York.", "citeRegEx": "Reveliotis,? 2005", "shortCiteRegEx": "Reveliotis", "year": 2005}, {"title": "Crystalline robots: Self-reconfiguration with compressible unit modules", "author": ["D. Rus", "M. Vona"], "venue": "Autonomous Robots,", "citeRegEx": "Rus and Vona,? \\Q2001\\E", "shortCiteRegEx": "Rus and Vona", "year": 2001}, {"title": "Rationality and intelligence", "author": ["S. Russell"], "venue": "Elio, R. (Ed.), Common Sense, Reasoning, and Rationality. Oxford University Press, USA.", "citeRegEx": "Russell,? 2002", "shortCiteRegEx": "Russell", "year": 2002}, {"title": "A mechanism for managing the buffer pool in a relational database system using the hot set model", "author": ["G. Sacco", "M. Schkolnick"], "venue": "Proceedings of the 8th International Conference on Very Large Data Bases,", "citeRegEx": "Sacco and Schkolnick,? \\Q1982\\E", "shortCiteRegEx": "Sacco and Schkolnick", "year": 1982}, {"title": "Software-enabled control for intelligent UAVs", "author": ["D. Schrage", "G. Vachtsevanos"], "venue": "In Proceedings of 1999 International Symposium on Computer Aided Control System Design,", "citeRegEx": "Schrage and Vachtsevanos,? \\Q1999\\E", "shortCiteRegEx": "Schrage and Vachtsevanos", "year": 1999}, {"title": "Optimal and hierarchical controls in dynamic stochastic manufacturing systems: A survey", "author": ["S.P. Sethi", "H. Yan", "H. Zhang", "Q. Zhang"], "venue": "Manufacturing & Service Operations Management,", "citeRegEx": "Sethi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sethi et al\\.", "year": 2002}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence Journal,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Control of weakly-coupled Markov chains", "author": ["D. Teneketzis", "S.H. Javid", "B. Sridhar"], "venue": "In Proceedings of the 1980 19th IEEE Conference on Decision and Control including the Symposium on Adaptive Processes,", "citeRegEx": "Teneketzis et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Teneketzis et al\\.", "year": 1980}, {"title": "CPlan: A constraint programming approach to planning", "author": ["P. van Beek", "X. Chen"], "venue": "In Proceedings of the 16th National Conference on Artificial Intelligence,", "citeRegEx": "Beek and Chen,? \\Q1999\\E", "shortCiteRegEx": "Beek and Chen", "year": 1999}, {"title": "On the use of integer programming models in AI planning", "author": ["T. Vossen", "M. Ball", "A. Lotem", "D. Nau"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Vossen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Vossen et al\\.", "year": 1999}, {"title": "An application view of COORDINATORS: Coordination managers for first responders", "author": ["T. Wagner", "J. Phelps", "V. Guralnik", "R. VanRiper"], "venue": "In AAAI,", "citeRegEx": "Wagner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 2004}, {"title": "Modeling uncertainty and its implications to sophisticated control in TAEMS agents", "author": ["T. Wagner", "A. Raja", "V. Lesser"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Wagner et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 2006}, {"title": "An open platform for reconfigurable control", "author": ["L. Wills", "S. Kannan", "S. Sander", "M. Guler", "B. Heck", "J. Prasad", "D. Schrage", "G. Vachtsevanos"], "venue": "Control Systems Magazine, IEEE,", "citeRegEx": "Wills et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wills et al\\.", "year": 2001}, {"title": "Integer Programming", "author": ["L.A. Wolsey"], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Wolsey,? 1998", "shortCiteRegEx": "Wolsey", "year": 1998}, {"title": "Decomposition techniques for temporal resource allocation", "author": ["C. Wu", "D. Castanon"], "venue": "In IEEE Conference on Decision and Control,", "citeRegEx": "Wu and Castanon,? \\Q2004\\E", "shortCiteRegEx": "Wu and Castanon", "year": 2004}, {"title": "Mission-Phasing Techniques for Constrained Agents in Stochastic Environments", "author": ["J. Wu"], "venue": "Ph.D. thesis, University of Michigan.", "citeRegEx": "Wu,? 2008", "shortCiteRegEx": "Wu", "year": 2008}, {"title": "Automated resource-driven mission phasing techniques for constrained agents", "author": ["J. Wu", "E.H. Durfee"], "venue": "In Proceedings of the 4th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Wu and Durfee,? \\Q2005\\E", "shortCiteRegEx": "Wu and Durfee", "year": 2005}, {"title": "Mixed-integer linear programming for transition-independent decentralized MDPs", "author": ["J. Wu", "E.H. Durfee"], "venue": "In Proceedings of the 5th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Wu and Durfee,? \\Q2006\\E", "shortCiteRegEx": "Wu and Durfee", "year": 2006}, {"title": "Sequential resource allocation in multi-agent systems with uncertainties", "author": ["J. Wu", "E.H. Durfee"], "venue": "In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Wu and Durfee,? \\Q2007\\E", "shortCiteRegEx": "Wu and Durfee", "year": 2007}, {"title": "Solving large TAEMS problems efficiently by selective exploration and decomposition", "author": ["J. Wu", "E.H. Durfee"], "venue": "In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Wu and Durfee,? \\Q2007\\E", "shortCiteRegEx": "Wu and Durfee", "year": 2007}, {"title": "Simultaneous routing and resource allocation via dual decomposition", "author": ["L. Xiao", "M. Johansson", "S.P. Boyd"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Xiao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "Dolgov and Durfee (2006) looked at these kinds of problems, studying efficient techniques by which agents can assess the value of alternative resource combination (bundle) assignments in terms of the execution policies (and the expected utilities of those policies) that the resources enable.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "instance of the type of problem solved by Dolgov and Durfee (2006), and we will show their solution shortly as a stepping stone to our algorithm.", "startOffset": 42, "endOffset": 67}, {"referenceID": 36, "context": "1 Markov Decision Processes In general, a classical discrete-time, fully-observable Markov Decision Process with a finite state space and a finite action space can be defined as a four-tuple \u3008S,A, P,R\u3009 (Puterman, 1994), where: S is a finite state space, represented as a set of n states {1, .", "startOffset": 202, "endOffset": 218}, {"referenceID": 20, "context": "j pi,a,j < 1 means that there is some probability of the agent being out of the system (which can be equivalently interpreted as the agent entering a sink state where the agent would stay forever) when executing action a in state i (Kallenberg, 1983).", "startOffset": 232, "endOffset": 250}, {"referenceID": 4, "context": "The main property of a MDP is that it possesses the Markov property (Bellman, 1957): if the current state of a MDP at time t is known, transitions to a new state at time t + 1 only depend on the current state and the action chosen at it, but are independent of the previous history of states.", "startOffset": 68, "endOffset": 83}, {"referenceID": 20, "context": "Although in general the mission-phasing techniques in this paper will also apply to discounted MDPs and other contracting MDPs (Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998), we illustrate them in this paper using transient, non-discounted MDPs.", "startOffset": 127, "endOffset": 183}, {"referenceID": 36, "context": "Although in general the mission-phasing techniques in this paper will also apply to discounted MDPs and other contracting MDPs (Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998), we illustrate them in this paper using transient, non-discounted MDPs.", "startOffset": 127, "endOffset": 183}, {"referenceID": 20, "context": "j pi,a,j < 1 at some states), an agent will eventually leave the corresponding Markov chain, after running a policy for a finite number of steps (Kallenberg, 1983).", "startOffset": 145, "endOffset": 163}, {"referenceID": 20, "context": "2 Linear Programming The value iteration and policy iteration algorithms are widely used in solving classical MDPs (Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998).", "startOffset": 115, "endOffset": 171}, {"referenceID": 36, "context": "2 Linear Programming The value iteration and policy iteration algorithms are widely used in solving classical MDPs (Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998).", "startOffset": 115, "endOffset": 171}, {"referenceID": 0, "context": "For that reason, a number of researchers have proposed and utilized an alternative solution approach, which is based upon mathematical programming (Altman, 1998; Feinberg, 2000; Dolgov & Durfee, 2006).", "startOffset": 147, "endOffset": 200}, {"referenceID": 16, "context": "For that reason, a number of researchers have proposed and utilized an alternative solution approach, which is based upon mathematical programming (Altman, 1998; Feinberg, 2000; Dolgov & Durfee, 2006).", "startOffset": 147, "endOffset": 200}, {"referenceID": 11, "context": "Several of such constrained optimization problems have been investigated by Dolgov and Durfee (2006). In what follows, we summarize that work.", "startOffset": 76, "endOffset": 101}, {"referenceID": 51, "context": "Although MILPs are NPhard in the number of integer variables, they can be solved by a variety of highly optimized algorithms and tools (Cook, Cunningham, Pulleyblank, & Schrijver, 1998; Wolsey, 1998).", "startOffset": 135, "endOffset": 199}, {"referenceID": 11, "context": ", the agent can only configure its resources at the beginning of mission execution), has been proven to be NP-hard through a reduction from the well-known KNAPSACK problem (Dolgov, 2006; Dolgov & Durfee, 2006).", "startOffset": 172, "endOffset": 209}, {"referenceID": 53, "context": "In previous work (Wu, 2008), we compare our solution method to both a brute-force search algorithm and a MDP-expansion-based approach.", "startOffset": 17, "endOffset": 27}, {"referenceID": 32, "context": "In those approaches, states are partitioned into small regions, a policy is computed for each region, and then these local policies are pieced together to obtain an overall policy (Parr, 1998; Precup & Sutton, 1998; Lane & Kaelbling, 2001).", "startOffset": 180, "endOffset": 239}, {"referenceID": 36, "context": "For larger state spaces, a simplified value iteration algorithm might be preferable (simplified because the policy in each phase is fixed) (Puterman, 1994).", "startOffset": 139, "endOffset": 155}, {"referenceID": 11, "context": "4) has been shown to be efficient in solving large constrained MDPs (Dolgov, 2006), and so this work uses it for solving such remodeled constrained MDPs.", "startOffset": 68, "endOffset": 82}, {"referenceID": 53, "context": "Our experiments (Wu, 2008) also show that the trends of results for other variations of the S-RMP optimization problem are similar to those described in this subsection.", "startOffset": 16, "endOffset": 26}, {"referenceID": 11, "context": "The algorithm devised by Dolgov and Durfee (2006) is presented below in Eq.", "startOffset": 25, "endOffset": 50}, {"referenceID": 11, "context": "Given that its special case \u2014 one-shot resource allocation and policy formulation \u2014 can be proven to be NP-complete through a reduction from the KNAPSACK problem (Dolgov, 2006), M-RMP optimization is NP-hard.", "startOffset": 162, "endOffset": 176}, {"referenceID": 6, "context": "A general Dec-MDP is NEXP-complete (Bernstein et al., 2000), meaning the M-RMP involves solving a NEXP-complete problem with an input exponential in the number of resources.", "startOffset": 35, "endOffset": 59}, {"referenceID": 53, "context": "An empirical evaluation in the domain with problems similar to (but more complex than) the running example used in this section (Figure 8) can be found in the work of Wu and Durfee (2007a). 15.", "startOffset": 167, "endOffset": 189}, {"referenceID": 36, "context": "By formulating a sequential decision-making problem into a MDP model, a number of efficient (polynomialtime) solvers, such as the value iteration and policy iteration algorithms, can be used to compute an optimal policy (Puterman, 1994).", "startOffset": 220, "endOffset": 236}, {"referenceID": 27, "context": "However, directly applying these algorithms in resource-constrained systems, such as the resource-driven mission-phasing problem, typically involves incorporating resource features in the MDP state representation (and so actions can be conditioned on resource availability), which will result in an exponential increase in the size of the state space (Meuleau et al., 1998), i.", "startOffset": 351, "endOffset": 373}, {"referenceID": 39, "context": ", a class of \u201cbounded optimality\u201d problems (Russell, 2002).", "startOffset": 43, "endOffset": 58}, {"referenceID": 0, "context": "For example, Altman (1998) adopted a Lagrangian and dual LP approach to solve constrained MDPs with total cost criteria.", "startOffset": 13, "endOffset": 27}, {"referenceID": 0, "context": "For example, Altman (1998) adopted a Lagrangian and dual LP approach to solve constrained MDPs with total cost criteria. Feinberg (2000) analyzed the complexity of constrained discounted MDPs.", "startOffset": 13, "endOffset": 137}, {"referenceID": 0, "context": "For example, Altman (1998) adopted a Lagrangian and dual LP approach to solve constrained MDPs with total cost criteria. Feinberg (2000) analyzed the complexity of constrained discounted MDPs. Of particular relevance to the work in this paper is the study of strongly-coupled resource allocation and policy formulation problems by Dolgov and Durfee (2006). Their approach implements simultaneous combinatorial optimization and stochastic optimization via reduction to mixed integer linear programming, which has been recapped in Section 2.", "startOffset": 13, "endOffset": 356}, {"referenceID": 36, "context": "The discovery of \u201crecurrent classes\u201d of MDPs is one such decomposition strategy, which can find an exact state space decomposition in an environment with uncertainties (Puterman, 1994; Boutilier, Dean, & Hanks, 1999).", "startOffset": 168, "endOffset": 216}, {"referenceID": 32, "context": "As an example, in the robot navigation domain (Parr, 1998; Precup & Sutton, 1998; Lane & Kaelbling, 2001), doorways (or similar connection structures, such as bridges) can be used to break a large environment into blocks of states, e.", "startOffset": 46, "endOffset": 105}, {"referenceID": 37, "context": "Several resource allocation algorithms have been developed for the problem of allocating a set of heterogeneous resources with availability constraints to maximize a given utility function (Wu & Castanon, 2004; Palomar & Chiang, 2006; Reveliotis, 2005).", "startOffset": 189, "endOffset": 252}, {"referenceID": 35, "context": "The discovery of \u201crecurrent classes\u201d of MDPs is one such decomposition strategy, which can find an exact state space decomposition in an environment with uncertainties (Puterman, 1994; Boutilier, Dean, & Hanks, 1999). A recurrent class represents a special absorbing subset of the state space, which means that once an agent enters a recurrent class it remains there forever no matter what policy it adopts. Puterman (1994) has suggested a variation of the Fox-Landi algorithm (Fox & Landi, 1968) to discover recurrent classes.", "startOffset": 169, "endOffset": 424}, {"referenceID": 34, "context": "In addition to the Artificial Intelligence (AI) techniques discussed above, decomposition techniques, which are often integrated with hierarchical control (also called multilevel control in some literature), have received much attention in recent years in Operations Research, Operations Management, Systems Theory, Control Theory, and several other fields (Sethi, Yan, Zhang, & Zhang, 2002; Antoulas, Sorensen, & Gugercin, 2001; Xiao, Johansson, & Boyd, 2004; Phillips, 2002; Teneketzis, Javid, & Sridhar, 1980).", "startOffset": 357, "endOffset": 512}, {"referenceID": 42, "context": "The fundamental ideas are to reduce the overall complex problem into multiple smaller, manageable sub-problems, to solve these sub-problems, and to coordinate solutions to the sub-problems so that overall system objectives and constraints are satisfied (Sethi et al., 2002).", "startOffset": 253, "endOffset": 273}, {"referenceID": 14, "context": ", responding to an unexpected disastrous event and reconfiguring resources for fault toleration (Drozeski, 2005).", "startOffset": 96, "endOffset": 112}, {"referenceID": 53, "context": "Our preliminary investigations into developing anytime algorithms for solving problems with both resource constraints and time limitations has shown promise (Wu, 2008) but more work remains in this area, including comparing methods grounded in the RMP concepts with heuristic and greedy techniques for allocating resources to agents.", "startOffset": 157, "endOffset": 167}], "year": 2010, "abstractText": "Because an agent\u2019s resources dictate what actions it can possibly take, it should plan which resources it holds over time carefully, considering its inherent limitations (such as power or payload restrictions), the competing needs of other agents for the same resources, and the stochastic nature of the environment. Such agents can, in general, achieve more of their objectives if they can use \u2014 and even create \u2014 opportunities to change which resources they hold at various times. Driven by resource constraints, the agents could break their overall missions into an optimal series of phases, optimally reconfiguring their resources at each phase, and optimally using their assigned resources in each phase, given their knowledge of the stochastic environment. In this paper, we formally define and analyze this constrained, sequential optimization problem in both the single-agent and multi-agent contexts. We present a family of mixed integer linear programming (MILP) formulations of this problem that can optimally create phases (when phases are not predefined) accounting for costs and limitations in phase creation. Because our formulations simultaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase, they exploit structure across these coupled problems. This allows them to find solutions significantly faster (orders of magnitude faster in larger problems) than alternative solution techniques, as we demonstrate empirically.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}