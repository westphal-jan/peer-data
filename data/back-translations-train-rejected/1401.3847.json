{"id": "1401.3847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Automatic Induction of Bellman-Error Features for Probabilistic Planning", "abstract": "Domain-specific features are important in representing problem structure throughout machine learning and decision-theoretic planning. In planning, once state features are provided, domain-independent algorithms such as approximate value iteration can learn weighted combinations of those features that often perform well as heuristic estimates of state value (e.g., distance to the goal). Successful applications in real-world domains often require features crafted by human experts. Here, we propose automatic processes for learning useful domain-specific feature sets with little or no human intervention. Our methods select and add features that describe state-space regions of high inconsistency in the Bellman equation (statewise Bellman error) during approximate value iteration. Our method can be applied using any real-valued-feature hypothesis space and corresponding learning method for selecting features from training sets of state-value pairs. We evaluate the method with hypothesis spaces defined by both relational and propositional feature languages, using nine probabilistic planning domains. We show that approximate value iteration using a relational feature space performs at the state-of-the-art in domain-independent stochastic relational planning. Our method provides the first domain-independent approach that plays Tetris successfully (without human-engineered features).", "histories": [["v1", "Thu, 16 Jan 2014 04:57:22 GMT  (71kb)", "http://arxiv.org/abs/1401.3847v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jia-hong wu", "robert givan"], "accepted": false, "id": "1401.3847"}, "pdf": {"name": "1401.3847.pdf", "metadata": {"source": "CRF", "title": "Automatic Induction of Bellman-Error Features for Probabilistic Planning Online Appendix 1", "authors": ["Jia-Hong Wu", "Robert Givan"], "emails": ["JW@ALUMNI.PURDUE.EDU", "GIVAN@PURDUE.EDU"], "sections": [{"heading": null, "text": "Automatic Induction of Bellman Error Functions for Probabilistic PlanningOnline Appendix 1Jia-Hong Wu JW @ ALUMNI.PURDUE.EDU Robert Givan GIVAN @ PURDUE.EDU Electrical and Computer Engineering, Purdue University, W. Lafayette, IN 47907 USA"}, {"heading": "1. Additional Pseudo-code and Grammar", "text": "In this section, we introduce additional pseudo-codes and feature grammar for our feature learning framework as follows: 1. Pseudo-codes for our trajector-based approximate value iteration approach (AVI) are shown in Figure 1 on page 2.2. Pseudo-codes for drawing training sets according to a policy are shown in Figure 2 on page 2.3. Our relational feature grammar is shown in Figure 3 on page 3."}, {"heading": "2. Details on the Selection and Modification of Competition Domains", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3. Parameterization of Our Methods", "text": "Here we describe our selection of parameters for our methods. Where possible, parameterization is done once to apply to all experiments in the same way as described here. There are some decisions that are made once for each area, and these are described in the subsection for each area. Primary decisions that have to be made in a domain-specific way control learning from small problems: We must specify for each area the power threshold above which the difficulty is increased (as shown in Figure 1 of the essay), as well as the order of difficulties to be taken into account (in cases where there is more than one parameter that controls the problem size). We postpone the topic of automated control of problem difficulties when learning from small problems of future research. We are currently making these decisions by experimenting with the area; our experience with such experiments suggests that these decisions can be successfully automated in the future."}, {"heading": "3.1 Trajectory Termination", "text": "It is an important and somewhat independent research topic to automatically detect when such a development does not make progress, e.g. by detecting dead ends in state regions and / or lack of progress towards the goal. Such research results can be inserted directly into our methods by terminating all training courses if they do not pass an appropriate test. Here, we do not address this problem in a sophisticated way, but terminate the development courses whenever one of three conditions applies: 1. a goal state is reached, 2. a dead end state is reached, 3. the development comprises 1,000 steps."}, {"heading": "3.2 Training Set Sizes", "text": "Each feature learning training covering all of our relational learning experiments is set to 20,000 states using the method described in Section 3. Since the propositional feature learning is faster than the relational feature learning, we are able to allow 200,000 states in the propositional feature learning sets in the TETRIS and SYSADMIN experiments, but still only 20,000 states in the planning areas."}, {"heading": "3.3 Learning Rate for Weight Updates in AVI", "text": "As described in Section 2.5 of the paper, we adjust the weights of our approximate value functions using AVI. We use a search-then-convergence plan for the learning rate of this iterative gradient descent method during our experiments (see Darken & Moody, 1992); specifically, we set the learning rate \u03b1 in AVI to 31 + k / 100, where k is the number of AVI iterations already performed."}, {"heading": "3.4 Parametrization of the Relational Algorithm", "text": "There are various parameters in the design process of the characteristics described in this section, including beam width W, beam depth d, regularization parameter \u03bb, and limitation to the maximum number of quantifiers to the extent q. Changes to these parameters affect the quality of the constructed characteristics by changing the desired characteristic space regions and the number of characteristics considered, as well as the preferences expressed in the evaluation of the characteristics. The selection of these parameters further influences the choice of the size of the characteristic training set, since in practice fewer training examples can be taken into account when the number of characteristics grows. In all of our experiments we select W to 60, d to 5, and \u03bb to 0.03 for all domains. We set q to 1 for the domains of the planning competition (setting q to 2 does not result in a remarkable improvement in performance in these domains if we use the above parameters, but lead to a prediction and sometimes unbearable runtime), and we set q to a strict control that is required for the area 2."}, {"heading": "3.5 Parametrization of the Propositional Algorithm", "text": "Our predictive feature learning algorithm is already well defined in Section 4.4 of the essay, except for how to set up the underlying C4.5 learner (Quinlan, 1993). We use the default parameters for C4.5, except for the following: We use the reinforcement criterion instead of the reinforcement criterion. We allow the trees to grow from a node without limiting the minimum number of objects in the resulting branch1."}, {"heading": "4. PPDDL Source for Lifted-Fileworld3", "text": "The PPDDL source for LIFTED-FILEWORLD3 with a problem size of 10 files. (: Define (Domain File World) (: Requirements: Input: disjunctive prerequisites: negative prerequisites: conditional-effects: probabilistic-effects: universal-prerequisites) (: Types file folder) (: Predictions (hat-type? p - file) (goes-in? p - file? f - folder) (submitted? p - file) (have? f - folder) (: constants F0 F1 F2 - folder) (: Action get-type: parameter (? p - file): prerequisite (and (hat-type? p))))): Effect (and (hat-type? p) (hat-type? p): Effect (goes-in? p F0) and (goes-in? p) effect (goes-in? p) (goes-in? p file) (goes-in? p file): f-type: f-parameter require at least 2 branches of an arbitrary 3.03p, goes to at least 33 nodes of 3p?"}, {"heading": "5. Modifications to the Weight Update Rule in AVI", "text": "(It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It. (.). (It.). (It.). (It.). (It.). (It.). (It.). (It. (.). (It.). (It.). (It. (.). (It.). (It. (.). (.). (It. (.). (It. (.). (It. (.). (.). (It. (.). (.). (It. (.). (It.). (.). (It. (.). (It. (.). (.).). (It. (.). (.). (It. (.). (.). (It. (.). (It. (.). (It.). (.). (.). (It. (. (.). (.). (It. (It. (.). (It. (.). (.). (. (.).). (It. (. (It.).). (It. (. (It. (. (.). (.). (It.). (It.). (. (. (It.). (It. (. (.). (.). (It. (.). (It. (. (.).). (.).).). (. (It. (It. (. (.). (.). (.). (It. (.). (It. (.). (.). (.). (It."}], "references": [{"title": "Non-deterministic planning track of the 2006 international planning competition", "author": ["B. Bonet", "R. Givan"], "venue": "Website. http://www.ldc.usb.ve/ bonet/ipc5/", "citeRegEx": "Bonet and Givan,? \\Q2006\\E", "shortCiteRegEx": "Bonet and Givan", "year": 2006}, {"title": "Towards faster stochastic gradient search", "author": ["C. Darken", "J. Moody"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Darken and Moody,? \\Q1992\\E", "shortCiteRegEx": "Darken and Moody", "year": 1992}, {"title": "A variable step (VS) adaptive filter algorithm", "author": ["R. Harris", "D. Chabries", "F. Bishop"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Harris et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Harris et al\\.", "year": 1986}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["R. Jacobs"], "venue": "Neural Networks,", "citeRegEx": "Jacobs,? \\Q1988\\E", "shortCiteRegEx": "Jacobs", "year": 1988}, {"title": "A variable step size LMS algorithm", "author": ["R. Kwong", "E. Johnston"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Kwong and Johnston,? \\Q1992\\E", "shortCiteRegEx": "Kwong and Johnston", "year": 1992}, {"title": "A stochastic gradient adaptive filter with gradient adaptive step size", "author": ["V. Mathews", "Z. Xie"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mathews and Xie,? \\Q1993\\E", "shortCiteRegEx": "Mathews and Xie", "year": 1993}], "referenceMentions": [{"referenceID": 3, "context": "There is a substantial literature on dynamically adjusting step size during gradient descent (Jacobs, 1988; Kwong & Johnston, 1992; Harris, Chabries, & Bishop, 1986; Mathews & Xie, 1993); however, gradient descent is not the main topic of this paper and so we resort only to a simple work-around for arbitrarily large gradients: rather than step proportional to the gradient, we compress the unbounded space of possible step sizes to a finite interval using a sigmoidal function, as described next.", "startOffset": 93, "endOffset": 186}], "year": 2010, "abstractText": "Every goal-oriented domain with a problem generator from the first or second IPPC (Younes, Littman, Weissman, & Asmuth, 2005; Bonet & Givan, 2006) was considered for inclusion in our experiments. For inclusion, we require a planning domain with a fixed action definition, as defined in Section 2.4 in the paper, that in addition has only ground conjunctive goal regions. Four domains have these properties directly, and we have adapted three more of the domains to have these properties as we describe in the next paragraph. The resulting selection provides seven IPPC planning domains for our empirical study. Figure 4 lists the reasons for the exclusion of the other six goal-oriented domains. In addition, four of the domains that we use in evaluation occur in both competitions in slightly different forms and we evaluate on one version of each of these four, as described in Figure 5. The three domains we adapted for inclusion are as follows. We created our own problem generators for the first IPPC domains TOWERS OF HANOI and FILEWORLD, as none were provided in the competition. For both these domains, there is only one instance of each size. In Towers of Hanoi, all instances share the same action set and state predicates, so that a suitable problem generator is straightforward. In Fileworld, a planning domain with a fixed action definition results if we consider the collection of instances that share the same fixed number of folders, but varying the number of files. When the number of folders varies, the state predicates and actions change, so that instances with varying numbers of folders cannot be in the same fixed-action-definition planning do-", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}