{"id": "1401.3860", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Planning with Noisy Probabilistic Relational Rules", "abstract": "Noisy probabilistic relational rules are a promising world model representation for several reasons. They are compact and generalize over world instantiations. They are usually interpretable and they can be learned effectively from the action experiences in complex worlds. We investigate reasoning with such rules in grounded relational domains. Our algorithms exploit the compactness of rules for efficient and flexible decision-theoretic planning. As a first approach, we combine these rules with the Upper Confidence Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second approach converts these rules into a structured dynamic Bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states. We evaluate the effectiveness of our approaches for planning in a simulated complex 3D robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition. Empirical results show that our methods can solve problems where existing methods fail.", "histories": [["v1", "Thu, 16 Jan 2014 05:03:40 GMT  (775kb)", "http://arxiv.org/abs/1401.3860v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tobias lang", "marc toussaint"], "accepted": false, "id": "1401.3860"}, "pdf": {"name": "1401.3860.pdf", "metadata": {"source": "CRF", "title": "Planning with Noisy Probabilistic Relational Rules", "authors": ["Tobias Lang", "Marc Toussaint"], "emails": ["tobias.lang@tu-berlin.de", "mtoussai@cs.tu-berlin.de"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do, and they are able to determine for themselves what they want to do."}, {"heading": "2. Related Work", "text": "The problem of decision-making and planning in stochastic relationship domains has been approached in different ways. (RRL) The field of relational reinforcement learning (RRL) (Dz, eroski, de Raedt, & Driessens, 2001; van Otterlo, 2009) investigates value functions and Q functions defined by all possible soil states and actions of a relational domain. The key idea is to describe important global features in terms of abstract logical formulas that allow a generalization of objects and situations. Model-free RRL approaches learn value functions for states and actions directly from experience. Q function estimators include relational regression trees (Dz, eroski et al, 2001) and instance-based regression using distance metrics between relational states such as graphical kernels (Driessens, Ramon, & Ga, 2006). Model-free approaches allow planning for specific problem types that can be quickly adapted to the Y situations (e.g. in training examples), which may be unsuitable."}, {"heading": "3. Background", "text": "In this section, we present the theoretical background to the planning algorithms that we will present in the following sections. First, we describe relational representations to define world states and actions, then we present noisy indeterministic deictic (NID) rules in detail, and then we define the problem of decision-making planning in stochastic relational areas. Finally, we briefly consider dynamic Bayesian networks."}, {"heading": "3.1 State and Action Representation", "text": "A relational domain is represented by a language of relational logic L. The set of logical predicates P and the set of logical functions F contain the relationships and properties that can apply to domain objects. The set of logical predicates A encompasses the possible actions in the domain. A concrete instantiation of a relational domain consists of a finite set of objects O. If the arguments of a predicate or function are all concrete, i.e. taken from O, we call them grounded. A concrete world state s is fully described as a conjunction of all grounded (potentially negated) predicates and functional values. Concrete actions a are described by positively grounded predicates of A. The arguments of predicates and functions can also be abstract logical variables that can represent any object. If a prediction or function has only abstract arguments, we call it abstract. Abstract actions and functions enable generalization over objects, and situations that we apply to a substance of an object that we refer to when a formula is an object."}, {"heading": "3.2 Noisy Indeterministic Deictic Rules", "text": "We want to learn a relationship model of a stochastic world and use it for planning. (Pasula et al.) We have recently introduced an appealing action model based on noisy indeterministic deictic (NID) rules that combine several advantages: \u2022 a relational representation that allows a generalization of objects and situations; \u2022 indeterministic action outcomes with probabilities to take stochastic action areas into account; \u2022 deictic references for actions to reduce action space; \u2022 noise results to avoid explicit modeling of rare and excessively complex results; and \u2022 the existence of an effective learning algorithm.Table 1 shows an exemplary NID rule for our complex robot domain. Fig. 1 represents a situation in which this rule can be used for predicting. Formally, an NID rule r is given that is asar (X)."}, {"heading": "3.3 Decision-Theoretic Planning", "text": "The problem of decision theory is to find measures in a particular state that are expected to maximize future rewards for states and actions (Boutilier et al., 1999). In classical planning, this reward is usually defined in terms of a clearly defined goal that is either fulfilled or not fulfilled in a state. This can be expressed by a logical formula: This formula is a partial description of the state, so that there is more than one state in which there is a logical reward. The goal might be to put all our romantic books on a particular shelf, no matter where the remaining books are. In this case, it is about finding a sequence of actions that are carried out in a world state. However, in stochastic domains, we assume that the results of actions are uncertain. Probabilistic planning is inherently tougher than its deterministic counterpart (Littman, Mundhenk, 1997)."}, {"heading": "3.4 Dynamic Bayesian Networks", "text": "Dynamic Bayesian networks (DBNs) model the development of stochastic systems over time. Therefore, the PRADA planning algorithm we present in Section 5 uses this type of graphical model to evaluate the stochastic effects of action in factored grounded relational world states. Therefore, we will briefly review the Bayesian networks and their dynamic expansion.A Bayesian network (BN) (Jensen, 1996) is a compact representation of the common probability distribution across a series of random variables X using a directed acyclic graph G. The nodes in G represent the random variables, while the edges define their dependencies and thus express conditional assumptions of independence. The value x of a variable X-X depends only on the values of its immediate ancestors in G, which are called parent Pa (X) of the bnods. Conditional probability functions define each node (X | X) (P)."}, {"heading": "4. Planning with Look-Ahead Trees", "text": "To plan with NID rules, the area described by the vocabulary of relational logic can be treated as a relational Markov decision-making process, as described in paragraph 3.3. Below, we present two value-oriented learning algorithms for reinforcement that use NID rules as a generative model for creating predictive trees that are used from the initial state to estimate the values of actions and states."}, {"heading": "4.1 Sparse Sampling Trees", "text": "The Sparse Sampling Tree (SST) algorithm (Kearns et al., 2002) for MDP planning samples happens to be sparse, but full-grown predictive trees of states starting with the given state as the root are sufficient to calculate nearly optimal measures for each state of an MDP. Given a planning horizon d and a branching factor b, SST works as follows (see Fig. 2): In each tree node (representing a state), (i) SST takes into account all possible measures, and (ii) for each action, it takes samples from the successor state distribution using a generative model for the transitions, e.g. the transition model T of the MDP, to build tree nodes on the next level. Values of tree nodes are calculated recursively from the leaves to the root using the Bellman equation: in a given node, the Q value of each possible action is estimated by averaging the children for this action."}, {"heading": "4.2 Sampling Trees with Upper Confidence Bounds", "text": "The Upper Confidence Limits applied to Trees (UCT) algorithm (Kocsis & Szepesvari, 2006) is updated online with the results of the simulation. (Kocsis & Szepesvari, 2006) Samples of a search tree of subsequent states, starting with the current state as root, are also included. In contrast to SST, which generates b-successor states for each action in a state, the idea of the UCT is to selectively select actions in a particular state and thus selectively stomp from the successor state distribution. UCT tries to identify large suboptimal actions early in the sampling process and to focus on promising parts of the predictive tree instead.UCT builds its predictive tree by repeatedly evaluating simulated sequences from the initial state using a generative model, e.g. the transition model T of the UCT of the MDP. An episode is a sequence of states, rewards and actions, up to a limited horizon of each srd, srd, 1, srd, 1, srd, 1, srd, 1, d."}, {"heading": "5. Planning with Approximate Inference", "text": "The sample-based approaches discussed in the previous section solve this problem by repeatedly generating samples from the outcome distribution of an action using the transition probabilities of an MDP. This results in forward-looking trees that easily fly in the air with the planning horizon. Instead of sampling successor states, one can maintain a distribution across states, a so-called \"belief.\" In the following, we present an approach to planning in grounded stochastic relationship areas that propagates beliefs about states in the sense of government surveillance. First, we show how to create compact graphical models for NID rules, then we develop an approximate method of consequence to efficiently disseminate beliefs. With this approach, we describe our probabilistic Relational Action-Sampling in DBN's Planning Algorithm (PRADA), which tests actions in an informed manner and evaluates them using approximate conclusions in DBNs. Then, we present the PRADA as an example to discuss the previous extensions of the PRADA."}, {"heading": "5.1 Graphical Models for NID Rules", "text": "Subsequently, we will discuss how to convert the NID rules to DBNs, which the PRADA algorithms will use to create a plan with probable conclusions. We will name random variables by uppercase letters (e.g. S), their values by corresponding lowercase letters (e.g. S), variable vectors by uppercase letters (e.g. S = (S1, S2, S3), and value vectors by uppercase letters (e.g. s = (s1, s3). We will also use column notation, e.g. s 2: 4 = (s3, s4).A naive way to convert the NID rules to DBNs is shown in Fig."}, {"heading": "5.2 Approximate Inference", "text": "In the following, we present an efficient method for approximate inference in the previously proposed DBNs, which evaluates the factorization of the NID rules. We focus on the mathematical derivatives, an illustrative example is given in Sec. 5.4.We follow the idea of factored frontier (FF) algorithms (Murphy & Weiss, 2001) and approach the belief in a product of marginalities: P (st: 0: t \u2212 1). We follow the idea of factored frontier (FF) algorithms (sti): = P (s: t i: t \u2212 1) and (22) \u03b1 (st: p: 0 \u2212 1)."}, {"heading": "5.3 Planning", "text": "The DBN representation in Fig. 3 (b) together with the approximate inference method described in the last subsection enables us to derive a novel planning algorithm for stochastic relationship domains. (b) This approximate inference method described in the last subsection enables us to derive a novel planning algorithm for stochastic relationship domains. (b) The Probabilistic Relational Action-sampling in DBN's Planning Algorithm (PRADA) plans by sampling action sequences in an informed manner based on predicted beliefs about states and evaluating these action sequences using approximate inferences.Specifically, we are calculated sample sequences of actions a0: 1 of length T. For 0 < t \u2264 T, we subtract the action sequences over states P (st). (a0, s0: 0) and rewards action factor < < &p < Then we compute the action factor < &p < &p < or < 1. < < &p < then."}, {"heading": "5.4 Illustrative Example", "text": "Let's look at the small planning problem in Table 2 to illustrate PRADA's reasoning process. Our domain is a noisy Cubeworld, represented by predicate table (X), Cube (X), on (X, Y), and Clear (X), in which a robot can perform two types of actions: it can either pick up a cube X by means of Action Grab (X), or take hold of the cube it holds in its hand on another object. Starting state s0, shown in 2 (a), contains three types of actions a, b, and c stacked in a stack on the table. The goal shown in Table t is to get the middle cube at the top of the top cube a. Our world model provides three abstract NID rules to predict action effects shown in Table 2 (c). Only the first rule has uncertain results: it models to grab an object that lies beneath another object."}, {"heading": "5.5 Comparison of the Planning Approaches", "text": "The most prominent difference between the presented planning approaches is not the way in which they get a grip on the stochasticity of the action, but the way in which they control the individual action lines and action lines in the individual action lines of the individual action lines and action lines of the individual action lines in the individual action lines of the individual action lines and action lines in the individual action lines of the individual action lines and action lines in the individual action lines of the individual action lines and action lines of the individual action lines of the individual action lines in the individual action lines of the individual action lines of the individual action lines of the individual action lines and action lines of the individual action lines of the individual action lines and action lines of the individual action lines of the individual action lines of the individual action lines of the individual action lines and action lines of the individual action lines of the individual action lines of the individual action lines of the individual action lines, action lines of the individual action lines of the individual action lines of the individual action lines of the individual action lines and action lines of the individual action lines of the individual action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines and action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines and action lines of the action lines of the individual action lines of the action lines of the individual action lines of the individual action lines of the action lines of the individual action lines of the individual action lines of the action lines of the action lines of the individual action lines of the action lines of the action lines of the individual action lines, action lines and action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the individual action lines of the action lines of the individual action lines of the individual action lines of the individual action lines and action lines of the action lines of the individual action lines of the individual action lines of the action lines of the action lines of the individual action lines of the action lines of the individual action lines of the action lines of the individual action lines of the individual action lines of the action lines of the action lines of"}, {"heading": "5.6 An Extension: Adaptive PRADA", "text": "We present a very simple extension of the PRADA to increase its planning accuracy, taking into account in particular the fact that PRADA evaluates the complete sequence of actions - unlike SST and UCT, where the actions taken at t > 0 depend on the results of the sample. Adaptive PRADA (A-PRADA) investigates the best sequence of actions found by PRADA. While PRADA selects the first action of this sequence without further justification, A-PRADA reviews each action in this sequence and decides by simulation whether it can be deleted. The resulting shortened sequence could lead to an increased expected reward, which is when actions do not have a significant impact on the achievement of the target or if they reduce the likelihood of success. If such actions are not taken, the states with high rewards are reached earlier and their rewards are taken less into account. Consider, for example, the goal of grasping a blue ball: a sequence of actions that grabs a red cube grasped and then grabs the blue ball."}, {"heading": "6. Evaluation", "text": "We have implemented all the presented planning algorithms and the learning algorithm for NID rules in C + +. Our code is available at www.user.tu-berlin.de / lang / prada /. We evaluate our approaches in two different scenarios. The first is a noisy, complex simulated environment in which we learn NID rules from experience and use them for planning. Secondly, we apply our algorithms to the benchmarks of the uncertainty part of the 2008 International Planning Competition."}, {"heading": "6.1 Simulated Robot Manipulation Environment", "text": "In fact, it's not that we're able to change the world, and that we're able to change the world, \"he said in an interview with The New York Times.\" It's not that we're able to change the world, \"he said.\" But it's not that we're able to change the world. \""}, {"heading": "6.1.1 High Towers", "text": "In our first series of experiments, we designed the construction of skyscrapers, which is the planning task in the work of Pasula et al. (2007). Specifically, the reward in a state is defined as the average height of objects. This is a simple planning problem, as many different measures can increase the reward (object identities do not matter) and a small planning horizon d is sufficient, so that the reward for performing actions is 0."}, {"heading": "6.1.2 Desktop Clearance", "text": "The task in our second series of experiments is to clarify the desktop. Objects are distributed over the whole table at the beginning. An object is deleted if it is part of a tower that contains all the other objects of the same class. An object class is simply defined in terms of color, which is provided in addition to the state representation of the robot. The reward of the robot is defined as the number of objects released. In our experiments, classes contain 2-4 objects with at most 1 ball (to allow successful piling). Our initial situations contain some stacks, but only with objects of different classes. Thus, the reward for performing any actions is 0. Desktop release is more difficult than building high towers, because the number of good plans yield high rewards. We set the planning horizon d = 6 optimally for SST, which is required to clarify a class of 4 objects, namely classification and placement of three objects."}, {"heading": "6.1.3 Reverse Tower", "text": "In order to explore the limits of UCT, PRADA and A-PRADA, we have conducted a series of experiments in which the task is to reactivate the towers of C-cubes, which require at least 2C actions (each of which must be taken at least once).Apart from the fact that this is difficult due to the noise in the simulated world, we can engage in a limited number of actions that are not able to do so."}, {"heading": "6.1.4 Summary", "text": "Our results show that successful planning with Learned World models (here in the form of rules) must explicitly take into account the quantification of predictable uncertainties. Specifically, methods that use predictive trees (UCT) and approximate conclusions (A-) PRADA outperform FF-Replan for various tasks of varying difficulty. In addition, (A-) PRADA can solve long-horizon planning tasks where UCT fails. Only if the learned rules are reworked manually to clarify their application contexts, and the planning problem uses a conjunctive target structure and requires few and long plans, does FF-Replan perform better than UCT and (A-) PRADA."}, {"heading": "6.2 IPPC 2008 Benchmarks", "text": "In fact, in the second half of the last decade, we are dealing with a problem that affects many people: the number of people who are in the city and the number of people who are in the city, who are in the city, the number of people who are in the city, the number of people who are in the city, the number of people who are in the city, the number of people who are in the city, the number of people who are in the city."}, {"heading": "6.2.1 Summary", "text": "The majority of PPDDL descriptions of IPPC benchmarks can be converted into NID rules, which show the wide range of planning problems that can be covered by NID rules. Our results show that our approaches perform comparably or better than modern planners in many traditional craft planning problems. This points to the universality of our methods for probabilistic planning beyond the type of robotic manipulation areas covered in paragraph 6.1. Our methods perform particularly well in areas where probabilities of success need to be carefully considered. They face problems when the required planning horizons are very large, while the number of plans with a probability that is not zero is small, which can be avoided by interim rewards."}, {"heading": "7. Discussion", "text": "Our methods are designed to be based on learned rules that provide approximate submodels of noisy worlds. Our first approach is an adaptation of the UCT algorithm, which examines predictive trees to cope with action stochasticity. Our second approach, called PRADA, explicitly models uncertainty about states of belief and derives approximate conclusions from it in graphical planning models. By combining our planning algorithms with an existing ruler algorithm, an intelligent agent (i) can learn a compact model of the dynamics of a complex noisy environment and (ii) quickly derive appropriate measures for different objectives. Results in a complex simulated robotics domain show that our methods exceed state-of-the-art by applying our planners FF-Replan to a number of different planning tasks. Unlike FF-Replan, our methods are constructive enough to exceed the probabilities of trading."}, {"heading": "7.1 Outlook", "text": "In its current form, PRADA's approximate inference process is based on the specific compact DBNs compiled from rules. Similarly, the development of similar factored frontier filters for arbitrary DBNs, derived from more general PPDDL descriptions, is promising. Likewise, it is worthwhile adapting PRADA's factored frontier techniques to existing probabilistic planners. The use of probabilistic relational rules for backward planning appears attractive. It is easy to learn NID rules to undo the measures by providing reverse triples (s \u2032, a, s) to the rule-making algorithm by specifying the previous states for a state s \"if an action has been previously applied. Backward planning, which can be combined with forward planning, has received a lot of attention in classical planning and can be useful both for planning with anticipatory trees and for planning with endangered torture."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their careful and thorough comments, which have significantly improved this work. We thank Sungwook Yoon for implementing FF-Replan. We thank Olivier Buffet for answering our questions about the probable 2008 planning competition, which was supported by the German Research Foundation (DFG), Emmy Noether Fellowship TO 409 / 1-3."}, {"heading": "Appendix A. Proof of Proposition 1", "text": "Proposition 1 (paragraph 5.3) The set of action sequences PRADA samples with a probability of not zero is a super-set of those of SST and UCT.Proof: Let a0: T -1 be an action sequence sampled by SST (or UCT). So there is a state sequence s0: T and a rule sequence r0: T -1, so that in each state (t < T) the action is based on a unique coverage rule rt that predicts the follow-up state st + 1 with a probability pt > 0. Because if this is the case, st + 1 would never be sampled by SST (or UCT). We must show that the action is based on a unique coverage rule rt, 0 \u2264 t < T: P (st | a0, s0) > 0. If this is the case, then P tsample (a) > 0 is like a unique coverage rule st."}, {"heading": "Appendix B. Relation between NID rules and PPDDL", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "Noisy probabilistic relational rules are a promising world model representation for several reasons. They are compact and generalize over world instantiations. They are usually interpretable and they can be learned effectively from the action experiences in complex worlds. We investigate reasoning with such rules in grounded relational domains. Our algorithms exploit the compactness of rules for efficient and flexible decision-theoretic planning. As a first approach, we combine these rules with the Upper Confidence Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second approach converts these rules into a structured dynamic Bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states. We evaluate the effectiveness of our approaches for planning in a simulated complex 3D robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition. Empirical results show that our methods can solve problems where existing methods fail.", "creator": "LaTeX with hyperref package"}}}