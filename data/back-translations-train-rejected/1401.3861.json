{"id": "1401.3861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Best-First Heuristic Search for Multicore Machines", "abstract": "To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. PBNF allows speculative expansions when necessary to keep threads busy. We identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. Our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than improved versions of previous parallel search proposals.", "histories": [["v1", "Thu, 16 Jan 2014 05:04:02 GMT  (805kb)", "http://arxiv.org/abs/1401.3861v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["ethan burns", "sofia lemons", "wheeler ruml", "rong zhou"], "accepted": false, "id": "1401.3861"}, "pdf": {"name": "1401.3861.pdf", "metadata": {"source": "CRF", "title": "Best-First Heuristic Search for Multicore Machines", "authors": ["Ethan Burns", "EABURNS AT CS.UNH.EDU", "Sofia Lemons", "Wheeler Ruml", "RUML AT CS.UNH.EDU", "Rong Zhou", "RZHOU AT PARC.COM"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "It is widely expected that future microprocessors will have faster clock frequencies, but more computing cores per chip will be implemented instead."}, {"heading": "2. Previous Approaches", "text": "There has been a lot of previous work on the parallel search. We will briefly summarize selected proposals before turning to the basics of our work, the PRA * and PSDD algorithms."}, {"heading": "2.1 Depth- and Breadth-first Approaches", "text": "Early work on parallel heuristic search explored approaches based on depth search, two examples being the distributed tree search (Ferguson & Korf, 1988) and parallel window search (Powley & Korf, 1991). Distributed tree search begins with a single thread that maintains the initial state to expand; each time a node is created, an unused thread is assigned to the node; the threads are routed down the tree in the deepest way until there are no free threads to assign; when this happens, each thread will continue to search its own children with the deepest search; when the solution for a subtree is found, it will move the tree up the parent thread and the child thread will be freed to be re-assigned elsewhere in the tree."}, {"heading": "2.2 Simple Parallel Best-first Search", "text": "The idea behind this is that people live in the city in which they live, move into another world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live."}, {"heading": "2.3 Parallel Retracting A*", "text": "PRA * (Evett et al., 1995) tries to avoid disputes by assigning separate open and closed lists to each thread. A state representation hash is used to assign nodes to the corresponding thread when they are generated. (Full PRA * also includes a return scheme that reduces memory usage in exchange for increased computing time; we do not consider this feature in this essay.) Choosing the hash function affects the performance of the algorithm as it determines how the work is distributed. Note that with standard PRA * each thread can communicate with each of its peers, so each thread needs a synchronized message queue to which peers can add nodes. In a multicore setting, this is implemented by requiring a thread to insert a lock on the message queue. Typically, this requires a thread that sends (or receives) a message before it duplicates the thread before it may open."}, {"heading": "2.3.1 IMPROVEMENTS", "text": "In fact, we will be able to get to grips with the problems mentioned, \"he said in an interview with the Deutsche Presse-Agentur.\" We have never made as many mistakes as this year, \"he said.\" We have never made as many mistakes, \"he said.\" We have never made as many mistakes as this year. \""}, {"heading": "2.4 Parallel Structured Duplicate Detection", "text": "The purpose of PSDD is to avoid the need to lock each generation of nodes and to prevent individual nodes from being explicitly passed between the threads. It builds on the idea of structured double detection, originally developed for external memory search (Zhou & Hansen, 2004). SDD uses an abstraction function that has the same image in abstract space; the abstraction function generates an abstract graph of nodes that are images in abstract space; the abstract node that a state is associated with is called its image; a block of nodes is the set of nodes in state space that have the same image in abstract space; Figure 1 shows one of 36 nodes and one abstract graph in state space; if two states are successors in state space, then their images are successors in the abstract graph."}, {"heading": "2.4.1 IMPROVEMENTS", "text": "While PSDD can be considered a general framework for parallel search, in our terminology 3., PSDD cannot guarantee that operators will first find two types of SDD in a parallel environment that uses layer-based synchronization and breadthfirst search. (In this subsection, we introduce two algorithms that use the PSDD framework and try to improve the PSDD algorithm in specific ways.) To cope with situations where a good boundary is not available, we have implemented a novel algorithm that uses iterative indentation (IDPSDD) to increase the boundary. As we report below, this approach is not effective in domains that do not have a geometrically increasing number of nodes."}, {"heading": "3.1 Livelock", "text": "The greedy nblocks, in which the PBNF implements not only the acquired, but also the acquired nBNF orders. (That is, it is only a block in which the nBlock blockers can block each other.) (That is, we have developed a method called \"hot nblocks,\" where they altruistically block their nblock. We call this advanced algorithm \"Safe PBNF.\" We use the terms of b \"to refer to the set of nblocks when they are free. (That is, the\" hot nblocks \"when interfered with a better nblock.\" We call this advanced algorithm \"Safe PBNF.\" We use the term \"to refer to the set of nblocks that are acquired when they are.\""}, {"heading": "3.2 Correctness of PBNF", "text": "Given the complexity of parallel shared memory algorithms, it can be reassuring to have proof of correctness. In this section, we will check whether PBNF has several desirable properties:"}, {"heading": "3.2.1 SOUNDNESS", "text": "Solidity holds trivial, because no solution is returned that does not pass the target test."}, {"heading": "3.2.2 DEADLOCK", "text": "There is only one lock in PBNF and the thread that currently holds it never tries to buy it a second time, so there can be no blockage."}, {"heading": "3.2.3 LIVELOCK", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3.2.4 COMPLETENESS", "text": "This can easily be derived from Liveness: Episode 1 If heuristics are permissible or the search space is finite, a destination is returned when one is attainable.Proof: If heuristics are permissible, we inherit the completeness of the series A * (Nilsson, 1980) of Theorem 2. Nodes are expanded only when their g-value has improved, and this can only be done finite times, so that a finite number of extensions is sufficient to exhaust the search space. 2"}, {"heading": "3.2.5 OPTIMALITY", "text": "Since the expansion sequence of PBNF is not necessarily the first, it works like an algorithm at all times, and its optimality follows the same argument as that of algorithms such as Anytime A * (Hansen & Zhou, 2007).Theorem 3 PBNF only provides optimal solutions.Proof: Once an established solution is found, the search expands the nodes further until the minimum f-value among all boundary nodes is greater or equal to the resolution costs of the established ones, which means that the search ends only with the optimal solution.2Before discussing how to adapt PBNF to suboptimal and readily available searches, we first evaluate its performance in terms of optimal problem solving."}, {"heading": "4. Empirical Evaluation: Optimal Search", "text": "We have implemented and tested the parallel heuristic search algorithms described above on three different benchmark domains: Grid path finding, the sliding tile puzzle, and STRIPS planning. We will discuss each domain in turn. Except for the planning domain, the algorithms were programmed in C + + using the POSIX threading library, and executed on dual quad-core Intel Xeon E5320 1.86GHz processors with 16Gb RAM. For the planning results, the algorithms were written independently in C from the pseudo-code in Appendix A. This gives us additional confidence in the accuracy of the pseudo-code and our performance claims. The planning experiments were executed on dual quad-core Intel Xeon X5450 3.0GHz processors, which are limited to approximately 2GB RAM. All open lists and free lists were used as binary heaps, except for SDD and SDID, and an SDD."}, {"heading": "4.1 Tuning PBNF", "text": "This year it is more than ever before."}, {"heading": "4.2 Tuning PRA*", "text": "In fact, most of us are able to play by the rules we have set ourselves to play by, \"he said in an interview with The New York Times."}, {"heading": "4.3 Grid Pathfinding", "text": "In this section we analyze the parallel algorithms in the area of network path finding. The goal of this domain is to navigate a grid from a starting point to a destination while avoiding obstacles. We used two cost models (discussed below) and both four-way and eight-way motion. In the four-way networks cells were blocked with a probability of 0.35 and in the eight-way networks cells with a probability of 0.45. The abstraction function used maps blocks of adjacent cells to the same abstract state and forms a larger abstract grid covering the original space. Heuristics was the distance from Manhattan to the destination. The hash values for states (which are used to distribute nodes in PRA * and HDA *) are calculated as follows: x \u00b7 ymax + y of the location. This results in a minimal perfect hash value for each state. For this domain we were able to adjust the size of the abstraction and show the best abstraction where our results are."}, {"heading": "4.3.1 FOUR-WAY UNIT COST", "text": "In the second half of the last decade, the number of unemployed in the US tripled, and the number of unemployed in the US tripled. In the second half of the last decade, the number of unemployed in the US tripled. In the second half of the last decade, the number of unemployed in the US tripled. In the second half of the last decade, the number of unemployed in the US tripled. In the second half of the last decade, the number of unemployed in the US multiplied. In the second half of the last decade, the number of unemployed in the US multiplied. In the second half of the last decade, the number of unemployed in the US multiplied. In the second half of the last decade, the number of unemployed in the US multiplied. In the second half of the last decade, the number of unemployed in the US multiplied."}, {"heading": "4.3.2 FOUR-WAY LIFE COST", "text": "Movements in the life-cost model cost the line number of the state in which the train was executed - trains at the top of the grid are free, trains at the bottom cost 4999 (Ruml & Thu, 2007) This distinguishes between the shortest and cheapest paths, which has proven to be a very important distinction (Richter & Westphal, 2010; Cushing, Bentor, & Kambhampati, 2010).The middle left chart in Figure 9 shows these results in the same format as the unit-cost variant - number of threads on the x-axis and acceleration via the serial A * on the y-axis. On average, Safe PBNF provided better acceleration than AHDA *, but AHDA * outperformed the PBNF at six and seven threads. However, for eight threads, APRA * did not perform better than for seven threads. Both algorithms achieve accelerations that are very close to \"achievable acceleration\" in this domain, as the FPD performance was only boosted by upp - 3p."}, {"heading": "4.3.3 EIGHT-WAY UNIT COST", "text": "For problems planning eight-way movements, horizontal and vertical movements cost 1, while diagonal movements cost 2. These real costs differentiate the area from the previous two areas of path planning. The upper right panel in Figure 9 shows the number of threads on the X axis and acceleration via the serial A * on the Y axis for the unit cost the eight-way motion domain. We see that Safe PBNF provided the best average performance, achieving almost 6 times the acceleration on eight threads. AHDA * did no better on average than Safe PBNF, but achieved slightly more than 6 times the acceleration on seven threads over the serial A *. Again, however, we see that AHDA * did not deliver very stable performance gains with more threads. BFPSDD improved when the threads were added to eight, but never achieved more than 3 times the acceleration."}, {"heading": "4.3.4 EIGHT-WAY LIFE COST", "text": "This model combines the eight-way movement with the cost-of-living models; it tends to be the most difficult path planning area represented in this essay. Figure 9 \"s right center shows threads on the X-axis and acceleration on the Y-axis compared to the serial A * axis. AHDA * delivered the best average acceleration over the serial A * search, peaking at just under 6x acceleration on seven threads. Although it exceeded Safe PBNF on average on eight threads, AHDA * exhibits a steep performance drop that goes back to almost 5x acceleration, where Safe PBNF had about 6x acceleration on seven threads. BFPSDD again peaks at just under 3x acceleration on eight threads. 15 puzzles 250 simple AHDA * compared to Safe PBNF paired difference."}, {"heading": "4.4 Sliding Tile Puzzle", "text": "For these results, we use 250 randomly generated 15 puzzles that serial A * could solve within 3 million expansions. To get the maximum amount of expansions going back into the expanding thread of Figure 9, the abstraction used for the sliding puzzles ignores the numbers on a row of tiles. To get AHDA * the maximum amount of expansions going back into the expanding thread of Figure 9, we use an abstraction that looks at the position of the blank, one and two tiles. This abstraction gives 3360 nblocks. To get the maximum amount of expansions, the map back into the expanding thread (as described above), the abstraction uses the one, two, and three tiles. Since the position of the blank is ignored, each state generation that does not move generates two or three tiles, a child is generated in the same row of tiles."}, {"heading": "4.5 STRIPS Planning", "text": "Unlike the previous two domains, where the expansion of the node is very fast, the abstraction in this domain is relatively slow. In these experiments, the planner uses the regression and the max pair allowed for these algorithms. Abstraction is dynamically generated in this domain on the basis of a pro-problem and, following Zhou and Hansen, this time is not taken into account."}, {"heading": "4.6 Understanding Search Performance", "text": "We have seen that the PBNF algorithm for optimal search tends to perform better than the AHDA * algorithm. In this section, we show the results of a series of experiments that attempt to determine which factors allow PBNF to perform better in these areas. We looked at three hypotheses: First, PBNF can perform better because it expands fewer nodes with f-values that are larger than the optimal solution cost; second, PBNF can perform better because it tends to have many fewer nodes in each priority list than AHDA *. Finally, PBNF can perform better because it spends less time coordinating between topics. In the following subsections, we show the results of experiments that we have performed to test our three hypotheses."}, {"heading": "4.6.1 NODE QUALITY", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "4.6.2 OPEN LIST SIZES", "text": "We have found that because PBNF splits the search space into many different blocks, it tends to have data structures with much fewer entries than AHDA * that split the search space based on the number of threads. Because we are interested in general-purpose algorithms that can handle domains with real-value costs (such as eight-way network analysis), both PBNF and AHDA * use binary heaps to implement their open lists. PBNF has one pile per Nblock (which is one per abstract state), whereas AHDA * has one pile per thread. Since the number of Nblocks is greater than the number of threads, the AHDA * has many more nodes than PBNF in each of its heaps. This results in the heap operations in AHDA * taking longer than the heap operations in PBNF. The cost of operations on large heaps has proved to be strong."}, {"heading": "4.6.3 COOORDINATION OVERHEAD", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.7 Summary", "text": "In this section, we have shown that several simple parallel algorithms can be slower than a serial A * search, even if they offer more computing power. In addition, we have shown empirical results for a number of algorithms that make good use of parallelism and outperform the serial A * search. Overall, the safe PBNF algorithm delivered the best and most consistent performance of these latter algorithms. Our AHDA * variant of PRA * had the second fastest average performance in all areas. We have also shown that using abstraction in a PRA * -style search to distribute nodes across the various threads can give a significant speed boost by reducing the volume of communication. This change in PRA * seems to be much more helpful than using asynchronous communication."}, {"heading": "5. Bounded Suboptimal Search", "text": "Suboptimal solutions can often be found much faster and with lower memory requirements than optimal solutions. In this section, we show how to create limited-suboptimal variants of some of the best optimal parallel search algorithms. Weighted A * (Pohl, 1970), a variant of A * that orders its search for f \u2032 (n) = g (n) + w \u00b7 h (n), with w > 1, is probably the most popular suboptimal search. It guarantees that for a permissible heuristic h and a weight w the returned solution is w -permissible (within a W factor of optimal solution costs) (Davis, Bramanti-Gregor, & Wang, 1988).It is possible to modify AHDA *, BFPSDD, and PBNF to use weights to find suboptimal solutions, we call these algorithms wAHDA *, wBFPSDD, and wPBNF."}, {"heading": "5.1 Pruning Poor Nodes", "text": "A node n can clearly be truncated if f (n) \u2265 g (s), but according to the following theorem we need to keep n only if it is on the optimal path to a solution that is a factor of w better than s. This is a much stronger rule. Theorem 4 We can truncate a node n if w \u00b7 f (n) \u2265 g (s) without sacrificing w -admissibility. Promise: If the incumbent w -is allowed, we can safely truncate any node, so let's consider the case where g (s) > w \u00b7 g (opt) where opt is an optimal target. Note that without truncation, there is always a node p in any open list (or is created) that is well on its way to decide."}, {"heading": "5.2 Pruning Duplicate Nodes", "text": "When searching with an inconsistent heuristic, as in weighted A *, it is possible for the search to find a better path to an already extended state. Likhachev, Gordon and Thrun (2003) noted that, provided that the underlying heuristic function h is consistent, weighted A * nonetheless provides a w - valid solution when these duplicated states are truncated during the search. This ensures that each state is expanded at most once during the search. Unfortunately, their evidence depends on expansion in exactly the first order violated by several of the parallel search algorithms we are considering here. However, we can still prove that some duplicates may fall. Consider the expansion of a node n that generates a duplicated state d that has already been expanded. We suggest the following weak duplicates that we drop: the new copy of d can be truncated when the old g (d) is reversed."}, {"heading": "5.3 Optimistic Search", "text": "To take advantage of this, Thayer and Ruml (2008) take an optimistic approach to a limited suboptimal search that works in two phases: aggressive search with a weight greater than the desired optimality that is bound to find an established solution, and then a cleanup phase to prove that the incumbent is actually within the given value. The intuition behind this approach is that wA * can find a solution within a very narrow limit (much tighter than the acquired optimality), then the search on nodes can be continued in sequence until the limit can be proven. Thayer and Ruml show that this approach can actually exceed the speed of wA * for a given optimism. We have implemented an optimistic version of PBNF (oPBNF)."}, {"heading": "6. Empirical Evaluation: Bounded Suboptimal Search", "text": "We implemented and tested weighted versions of the parallel search algorithms discussed above: wAHDA *, wAPRA *, wBFPSDD, wPBNF, and oPBNF. All algorithms trim nodes based on the w \u00b7 f criterion set forth in Theorem 4, and trim entire open lists to f \u2032 as in episode 2. The search ends when all nodes have been trimmed by the established solution. Our experiments were conducted on the same three benchmark domains as for optimal search: network path finding, glide puzzle, and STRIPS planning."}, {"heading": "6.1 Grid Pathfinding", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6.2 Sliding Tile Puzzles", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6.3 STRIPS Planning", "text": "Table 5 shows the performance of parallel search algorithms in STRIPS planning problems, also in terms of acceleration compared to serially weighted A *. In this table, the columns represent different weights and the rows represent different planning problems with two and seven threads. Bold values represent table entries that are within 10% of the best performance for each domain. All algorithms had better acceleration at seven threads than at two. wPBNF reported the best acceleration for most domains, followed by wAHDA *, which was the fastest for three of the domains at seven threads. In two threads, there were a few domains (Satellit-6 and freecell-3) where wBFPSDD gave the highest acceleration, but not for seven threads. wAPRA * was always slower than the three remaining algorithms. In one problem, the serially weighted A * works much worse than the seven threads."}, {"heading": "6.4 Summary", "text": "In this section we have seen that limited suboptimal variants of the parallel search can perform better than their serial ancestors. We have also shown that the parallel search in the sliding puzzle offers a greater advantage over the serial search, as the difficulty of the problem increases, and we suspect that this result also applies to other areas. We suspect that the expense of using the parallelism is not amortized by the search time for very simple problems."}, {"heading": "7. Anytime Search", "text": "A popular alternative to the limited sub-optimal search is to search at any time where a highly sub-optimal solution is returned quickly and then improved solutions are returned over time until the algorithm is finished (or the established solution proves optimal).The two most popular heuristic search algorithms at all times are A * (AwA *) weighted (Hansen & Zhou, 2007) and repairable at any time A * (ARA *) (Likhachev, Gordon, & Thrun, 2003).In AwA *, a weighted A * search is allowed to continue after its initial solution if the unweighted f (n) g (s) \u2265 g (s is an established solution and n is a node that is considered for expansion. ARA * uses a weighted search that lowers the weight when a solution that reaches the current sub-optimal limit is found."}, {"heading": "7.1 Anytime Repairing PBNF", "text": "ARPBNF is a parallel, always available search algorithm based on ARA * (Likhachev et al., 2003). In ARPBNF, open lists and the pile of nblocks are sorted to f \u2032 as in AwPBNF, but instead of simply continuing the search until the incumbent proves optimal, ARPBNF uses a weight calendar. Each time an incumbent is found, the weight is lowered to the heuristic value by a certain amount, all open lists are withdrawn and the search continues. On the final iteration, the weight will be 1.0 and the optimal solution is found. The following procedure is used to deploy the nblocks in parallel between the established solutions: 1. The thread calling for a resort (which has found a destination) becomes the leader by placing the lock on the nblock and setting the resort flag. (If the flag has already been set, then another thread is used.)"}, {"heading": "7.2 Multi-weighted A*", "text": "In this section, we introduce a new and simple parallel algorithm called multi-weighted A *. PBNF and PRA * frameworks for parallelizing algorithms available at any time can be considered an end of a spectrum of parallel algorithms. In PBNF and PRA *, all threads work in search of a single solution of a certain quality; at the other end of the spectrum, each thread works to find its own solution. To compare an algorithm at that end of the spectrum, we implement an algorithm we call multi-weighted A * that assigns its available threads to their own weighted A *. The thread that ends first will generally be the thread that has been most heavily weighted and will therefore be the solution of the worst quality. The next thread to end the thread will have the next largest thread, and so on."}, {"heading": "8. Empirical Evaluation: Anytime Search", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "8.1 Four-Way Unit Cost Grids", "text": "AwAHDA * and AwPBNF found the best solutions faster than the other algorithms. Even with two threads, AwPBNF was the first algorithm to approach the optimal solution in 60% of the time from series A *. The next two algorithms are multi-wA * and always repair PBNF (ARPBNF). Multi-wA * converged faster than threads were added, but its performance in finding interim solutions did not change too much for different thread numbers. ARPBNF, on the other hand, took longer to find good solutions for low thread numbers, but when threads were added to provide better performance."}, {"heading": "8.2 Sliding Tile Puzzles", "text": "Figure 17 shows lower hulls for the always available algorithms on Korf's 100 instances of the 15-piece puzzle. In this figure, the X-axes show the total wall clock time in seconds. These times are not normalized to A * because they cannot solve all instances. In these panels, we see that AwAHDA * tends to find good solutions faster than all other algorithms. AwA * and AwPBNF behaved very similarly for two threads, and as the number of threads increased, AwPBNF began to find better solutions faster than AwA *. ARPBNF took longer to find good solutions than AwPBNF and AwAHDA *, but it was able to find better solutions faster than its serial counterpart. The simple multi-wA * algorithm provided the worst performance among parallel algorithms."}, {"heading": "8.3 STRIPS Planning", "text": "Table 6 shows the acceleration of the parallel algorithms at any time via serial at any time A *. All algorithms were executed until an optimal solution was proven. (At a weight of 5, AwA * ran out of memory at block-14, so our acceleration values at this weight are lower limits for this instance.) The bold entries in the table represent values within 10% of the best performance for the respective domain. In these two ranges, the AwAHDA * provided the best performance by at least a factor of 10x over AwPBNF.Hansen and Zhou (2007) show that AwA * can lead to an acceleration above A * at some weights."}, {"heading": "8.4 Summary", "text": "In this part of the paper, we demonstrated how to create some new parallel search algorithms based on the frameworks introduced in the previous sections. We also developed a new parallel algorithm that simply performs many weighted A * searches with different weights. In our experiments, we saw that AwPBNF and AwAHDA * found higher quality solutions faster than other algorithms, and that both performed better as more threads were added. In addition, ARPBNF, a parallel algorithm based on ARA * improved with more threads, and tended to lead to a smoother increase in solution quality than the previous two algorithms, although it did not find solutions as quickly and was unable to converge within the prescribed time to the optimal solution in the sliding tiles area."}, {"heading": "9. Discussion", "text": "We researched a number of best-first search algorithms that weighted the parallel capabilities of modern CPUs *. First, we looked at the parallel optimal search with (Safe) PBNF, several variants of PRA * and aset of simpler, previously proposed algorithms. Overall, Safe PBNF * gave the best performance for optimal search. Next, we created a series of limited-suboptimal search algorithms based on PBNF, the successful variants of PRA *, and the BFPSDD algorithm. PBNF and PRA * with asynchronous communication and abstraction (AHDA *) gave the best performance over all, with PBNF performing slightly better on average. In addition, we showed some results suggesting that boundary-suboptimal PBNF search has a greater advantage over serial weighted A * search as a problem difficulty. Finally, we turned NPF and PRA algorithms at all times into PRA *."}, {"heading": "9.1 Possible Extensions", "text": "While the basic guideline for creating good abstraction in SDD (and PBNF) is to minimize connectivity between abstract states, there are other aspects of abstraction that could be explored, such as discovering which features are good to include, or abstraction in detail. Although we always use a constant abstraction quantity in our current work, it seems likely that the abstraction size should change when the number of threads changes or perhaps even falls back on the characteristics of the domain or problem instance. Although a guideline could be developed, such as a ratio between the number of nblocks to threads or the value of the start state, it seems that the abstraction size should change."}, {"heading": "10. Conclusions", "text": "We have shown that a number of previously proposed parallel best-time initial search algorithms can be much slower than performing A * serial. We have introduced a novel hashing feature for PRA * that exploits the locality of a search space and provides superior performance. In addition, we have confirmed results presented by Kishimoto et al. (2009) that the use of asynchronous communication in PRA * enables better performance than the use of synchronous communication. We have presented a new algorithm, PBNF, that approaches a first-best search sequence while trying to occupy all threads. We have proven the correctness of the PBNF search framework and used it to derive new suboptimal and always new algorithms. We have performed a comprehensive empirical comparison with optimal, suboptimal and always available BNF search algorithms."}, {"heading": "Acknowledgments", "text": "We thank NSF (grant IIS-0812141), DARPA CSSG (grant HR0011-09-1-0021) and helpful suggestions from Jordan Thayer. Some of these results have already been reported by Burns, Lemons, Zhou and Ruml (2009b) and Burns, Lemons, Ruml and Zhou (2009a)."}, {"heading": "Appendix A. Pseudo-code for Safe PBNF", "text": "The first is a pointer to the current solution, the second is an open flag set to true when a thread recognizes that the search is complete, and the third is the list of free blocks, which then contain the list of free blocks containing the open node along with the lowest f-values. () The second is a single lock for access to both structures. () Each thread then also has a local Exp-number. () The best function on a series of nodes containing the open node with the lowest f-values. () SEARCH (INITIAL NODE) 1. Start node for each p-number. () 3. while threads are still running. () 4. Return incumbentTHREADSEARCH () 1. b."}], "references": [{"title": "Suboptimal and anytime heuristic search on multi-core machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS-09)", "citeRegEx": "Burns et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2009}, {"title": "Best-first heuristic search for multi-core machines", "author": ["E. Burns", "S. Lemons", "R. Zhou", "W. Ruml"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-09)", "citeRegEx": "Burns et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burns et al\\.", "year": 2009}, {"title": "Cost based search considered harmful", "author": ["W. Cushing", "J. Bentor", "S. Kambhampati"], "venue": "In The 2010 International Symposium on Combinatorial Search (SOCS-10)", "citeRegEx": "Cushing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cushing et al\\.", "year": 2010}, {"title": "Prioritizing bellman backups without a priority queue", "author": ["P. Dai", "E.A. Hansen"], "venue": "In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS-09)", "citeRegEx": "Dai and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Dai and Hansen", "year": 2007}, {"title": "The advantages of using depth and breadth components in heuristic search", "author": ["H.W. Davis", "A. Bramanti-Gregor", "J. Wang"], "venue": "In Methodologies for Intelligent Systems", "citeRegEx": "Davis et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1988}, {"title": "Localizing A", "author": ["S. Edelkamp", "S. Schr\u00f6dl"], "venue": "In Proceedings of the Seventeenth National Conference on Artificial Intelligence", "citeRegEx": "Edelkamp and Schr\u00f6dl,? \\Q2000\\E", "shortCiteRegEx": "Edelkamp and Schr\u00f6dl", "year": 2000}, {"title": "GPU exploration of two-player games with perfect hash functions", "author": ["S. Edelkamp", "D. Sulewski"], "venue": "In The 2010 International Symposium on Combinatorial Search (SOCS-10)", "citeRegEx": "Edelkamp and Sulewski,? \\Q2010\\E", "shortCiteRegEx": "Edelkamp and Sulewski", "year": 2010}, {"title": "A scalable concurrent malloc(3) implementation for FreeBSD", "author": ["J. Evans"], "venue": "In Proceedings of BSDCan", "citeRegEx": "Evans,? \\Q2006\\E", "shortCiteRegEx": "Evans", "year": 2006}, {"title": "PRA* - massively-parallel heuristic-search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Evett et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Evett et al\\.", "year": 1995}, {"title": "KBFS: K-best-first search", "author": ["A. Felner", "S. Kraus", "R. Korf"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Felner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Felner et al\\.", "year": 2003}, {"title": "Distributed tree search and its applications to alpha-beta pruning", "author": ["C. Ferguson", "R.E. Korf"], "venue": "In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88)", "citeRegEx": "Ferguson and Korf,? \\Q1988\\E", "shortCiteRegEx": "Ferguson and Korf", "year": 1988}, {"title": "Anytime heuristic search", "author": ["E.A. Hansen", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hansen and Zhou,? \\Q2007\\E", "shortCiteRegEx": "Hansen and Zhou", "year": 2007}, {"title": "A pragmatic implementation of non-blocking linked-lists", "author": ["T.L. Harris"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Harris,? \\Q2001\\E", "shortCiteRegEx": "Harris", "year": 2001}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P.E. Hart", "N.J. Nilsson", "B. Raphael"], "venue": "IEEE Transactions of Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "In Proceedings of the Fifth Internationas Conference on Artificial Intelligence Planning and Scheduling Systems", "citeRegEx": "Haslum and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Haslum and Geffner", "year": 2000}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bosnacki"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "Holzmann and Bosnacki,? \\Q2007\\E", "shortCiteRegEx": "Holzmann and Bosnacki", "year": 2007}, {"title": "Parallel external directed model checking with linear I/O", "author": ["S. Jabbar", "S. Edelkamp"], "venue": "Verification, Model Checking, and Abstract Interpretation,", "citeRegEx": "Jabbar and Edelkamp,? \\Q2006\\E", "shortCiteRegEx": "Jabbar and Edelkamp", "year": 2006}, {"title": "Scalable, parallel best-first search for optimal sequential planning", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS-09)", "citeRegEx": "Kishimoto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kishimoto et al\\.", "year": 2009}, {"title": "Iterative-deepening-A*: An optimal admissible tree search", "author": ["R.E. Korf"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Linear-space best-first search", "author": ["R.E. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1993\\E", "shortCiteRegEx": "Korf", "year": 1993}, {"title": "Delayed duplicate detection: extended abstract", "author": ["R.E. Korf"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Articial Intelligence", "citeRegEx": "Korf,? \\Q2003\\E", "shortCiteRegEx": "Korf", "year": 2003}, {"title": "Large-scale parallel breadth-first search", "author": ["R.E. Korf", "P. Schultze"], "venue": "In Proceedings of the Twentieth National Conference on Articial Intelligence", "citeRegEx": "Korf and Schultze,? \\Q2005\\E", "shortCiteRegEx": "Korf and Schultze", "year": 2005}, {"title": "Parallel best-first search of state-space graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "In Proceedings of the Seventh National Conference on Artificial Intelligence", "citeRegEx": "Kumar et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 1988}, {"title": "ARA*: Anytime A* with provable bounds on sub-optimality", "author": ["M. Likhachev", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the Seventeenth Annual Conference on Neural Information Porcessing Systems (NIPS-03)", "citeRegEx": "Likhachev et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Likhachev et al\\.", "year": 2003}, {"title": "ARA*: Formal analysis", "author": ["M. Likhachev", "G. Gordon", "S. Thrun"], "venue": "Tech. rep. CMU-CS-03148,", "citeRegEx": "Likhachev et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Likhachev et al\\.", "year": 2003}, {"title": "A parallel external-memory frontier breadthfirst traversal algorithm for clusters of workstations", "author": ["R. Niewiadomski", "J. Amaral", "R. Holte"], "venue": "In Proceedings of the 2006 International Conference on Parallel Processing", "citeRegEx": "Niewiadomski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Niewiadomski et al\\.", "year": 2006}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "In Proceedings of the 21st national conference on Artificial intelligence", "citeRegEx": "Niewiadomski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Niewiadomski et al\\.", "year": 2006}, {"title": "Principles of Artificial Intelligence", "author": ["N.J. Nilsson"], "venue": "Tioga Publishing Co", "citeRegEx": "Nilsson,? \\Q1980\\E", "shortCiteRegEx": "Nilsson", "year": 1980}, {"title": "Heuristic search viewed as path finding in a graph", "author": ["I. Pohl"], "venue": "Artificial Intelligence,", "citeRegEx": "Pohl,? \\Q1970\\E", "shortCiteRegEx": "Pohl", "year": 1970}, {"title": "Single-agent parallel window search", "author": ["C. Powley", "R.E. Korf"], "venue": "IEEE Transactions Pattern Analysis Machine Intelligence,", "citeRegEx": "Powley and Korf,? \\Q1991\\E", "shortCiteRegEx": "Powley and Korf", "year": 1991}, {"title": "The LAMA planner: Guiding cost-based anytime planning with landmarks", "author": ["S. Richter", "M. Westphal"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Richter and Westphal,? \\Q2010\\E", "shortCiteRegEx": "Richter and Westphal", "year": 2010}, {"title": "Best-first utility-guided search", "author": ["W. Ruml", "M.B. Do"], "venue": "In Proceedings of IJCAI-07,", "citeRegEx": "Ruml and Do,? \\Q2007\\E", "shortCiteRegEx": "Ruml and Do", "year": 2007}, {"title": "MPI-The Complete Reference: The MPI Core", "author": ["M. Snir", "S. Otto"], "venue": null, "citeRegEx": "Snir and Otto,? \\Q1998\\E", "shortCiteRegEx": "Snir and Otto", "year": 1998}, {"title": "Using magnetic disk instead of main memory in the mur \u03c6 verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "In Computer Aided Verification,", "citeRegEx": "Stern and Dill,? \\Q1998\\E", "shortCiteRegEx": "Stern and Dill", "year": 1998}, {"title": "Fast and lock-free concurrent priority queues for multi-thread systems", "author": ["H. Sundell", "P. Tsigas"], "venue": "Parallel and Distributed Processing Symposium, International,", "citeRegEx": "Sundell and Tsigas,? \\Q2005\\E", "shortCiteRegEx": "Sundell and Tsigas", "year": 2005}, {"title": "Faster than weighted A*: An optimistic approach to bounded suboptimal search", "author": ["J.T. Thayer", "W. Ruml"], "venue": "In Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling (ICAPS-08)", "citeRegEx": "Thayer and Ruml,? \\Q2008\\E", "shortCiteRegEx": "Thayer and Ruml", "year": 2008}, {"title": "Lock-Free Data Structures", "author": ["J.D. Valois"], "venue": "In Correct Hardware Design and Verification Methods,", "citeRegEx": "Valois,? \\Q1995\\E", "shortCiteRegEx": "Valois", "year": 1995}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Edge partitioning in external-memory graph search", "author": ["R. Zhou", "E. Hansen"], "venue": "In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-07)", "citeRegEx": "Zhou and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2007}, {"title": "Dynamic state-space partitioning in external-memory graph search", "author": ["R. Zhou", "E. Hansen"], "venue": "In The 2009 International Symposium on Combinatorial Search (SOCS-09)", "citeRegEx": "Zhou and Hansen,? \\Q2009\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2009}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04)", "citeRegEx": "Zhou and Hansen,? \\Q2004\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2004}, {"title": "Breadth-first heuristic search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Artificial Intelligence,", "citeRegEx": "Zhou and Hansen,? \\Q2006\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2006}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the Twenty-Second Conference on Artificial Intelligence", "citeRegEx": "Zhou and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2007}], "referenceMentions": [{"referenceID": 37, "context": "Parallel structured duplicate detection (PSDD) was originally developed by Zhou and Hansen (2007) for parallel breadth-first search, in order to reduce contention on shared data structures by allowing threads to enjoy periods of synchronization-free search.", "startOffset": 75, "endOffset": 98}, {"referenceID": 37, "context": "Parallel structured duplicate detection (PSDD) was originally developed by Zhou and Hansen (2007) for parallel breadth-first search, in order to reduce contention on shared data structures by allowing threads to enjoy periods of synchronization-free search. PSDD requires the user to supply an abstraction function that maps multiple states, called an nblock, to a single abstract state. We present a new algorithm based on PSDD called Parallel Best-NBlock-First (PBNF1). Unlike PSDD, PBNF extends easily to domains with non-uniform and non-integer move costs and inadmissible heuristics. Using PBNF in an infinite search space can give rise to livelock, where threads continue to search but a goal is never expanded. We will discuss how this condition can be avoided in PBNF using a method we call hot nblocks, as well as our use of bounded model checking to test its effectiveness. In addition, we provide a proof of correctness for the PBNF framework, showing its liveness and completeness in the general case. Parallel retracting A* (PRA*) was created by Evett, Hendler, Mahanti, and Nau (1995). PRA* distributes the search space among threads by using a hash of a node\u2019s state.", "startOffset": 75, "endOffset": 1099}, {"referenceID": 37, "context": "Parallel structured duplicate detection (PSDD) was originally developed by Zhou and Hansen (2007) for parallel breadth-first search, in order to reduce contention on shared data structures by allowing threads to enjoy periods of synchronization-free search. PSDD requires the user to supply an abstraction function that maps multiple states, called an nblock, to a single abstract state. We present a new algorithm based on PSDD called Parallel Best-NBlock-First (PBNF1). Unlike PSDD, PBNF extends easily to domains with non-uniform and non-integer move costs and inadmissible heuristics. Using PBNF in an infinite search space can give rise to livelock, where threads continue to search but a goal is never expanded. We will discuss how this condition can be avoided in PBNF using a method we call hot nblocks, as well as our use of bounded model checking to test its effectiveness. In addition, we provide a proof of correctness for the PBNF framework, showing its liveness and completeness in the general case. Parallel retracting A* (PRA*) was created by Evett, Hendler, Mahanti, and Nau (1995). PRA* distributes the search space among threads by using a hash of a node\u2019s state. In PRA*, duplicate detection is performed locally; communication with peers is only required to transfer generated search-nodes to their home processor. PRA* is sensitive to the choice of hashing function used to distribute the search space. We show a new hashing function, based on the same state space abstraction used in PSDD, that can give PRA* significantly better performance in some domains. Additionally, we show that the communication cost incurred in a naive implementation of PRA* can be prohibitively expensive. Kishimoto, Fukunaga, and Botea (2009) present a method that helps to alleviate the cost of communication in PRA* by using asynchronous message passing primitives.", "startOffset": 75, "endOffset": 1745}, {"referenceID": 15, "context": "Holzmann and Bosnacki (2007) have been able to successfully parallelize depth-first search for model checking.", "startOffset": 0, "endOffset": 29}, {"referenceID": 36, "context": "These lock-free data structures used to implement LPA* require a special lock-free memory manager that uses reference counting and a compare-and-swap based stack to implement a free list (Valois, 1995).", "startOffset": 187, "endOffset": 201}, {"referenceID": 20, "context": "A technique called delayed duplicate detection (DDD) (Korf, 2003), originally developed for externalmemory search, can be used to temporarily delay access to the a closed list.", "startOffset": 53, "endOffset": 65}, {"referenceID": 29, "context": "We implemented the concurrent priority queue data structure of Sundell and Tsigas (2005). For the closed list, we used a concurrent hash table which is implemented as an array of buckets, each of which is a concurrent ordered list as developed by Harris (2001).", "startOffset": 63, "endOffset": 89}, {"referenceID": 12, "context": "For the closed list, we used a concurrent hash table which is implemented as an array of buckets, each of which is a concurrent ordered list as developed by Harris (2001). These lock-free data structures used to implement LPA* require a special lock-free memory manager that uses reference counting and a compare-and-swap based stack to implement a free list (Valois, 1995).", "startOffset": 157, "endOffset": 171}, {"referenceID": 12, "context": "For the closed list, we used a concurrent hash table which is implemented as an array of buckets, each of which is a concurrent ordered list as developed by Harris (2001). These lock-free data structures used to implement LPA* require a special lock-free memory manager that uses reference counting and a compare-and-swap based stack to implement a free list (Valois, 1995). We will see that, even with these sophistocated structures, a straightforward parallel implementation of A* does not give competitive performance. One way of avoiding contention altogether is to allow one thread to handle synchronization of the work done by the other threads. K -Best-First Search (Felner, Kraus, & Korf, 2003) expands the best k nodes at once, each of which can be handled by a different thread. In our implementation, a master thread takes the k best nodes from open and gives one to each worker. The workers expand their nodes and the master checks the children for duplicates and inserts them into the open list. This allows open and closed to be used without locking, however, in order to adhere to a strict k -best-first ordering this approach requires the master thread to wait for all workers to finish their expansions before handing out new nodes. In the domains used in this paper, where node expansion is not particularly slow, we show that this method does not scale well. One way to reduce contention during search is to access the closed list less frequently. A technique called delayed duplicate detection (DDD) (Korf, 2003), originally developed for externalmemory search, can be used to temporarily delay access to the a closed list. While several variations have been proposed, the basic principle behind DDD is that generated nodes are added to a single list until a certain condition is met (a depth level is fully expanded, some maximum list size is reached (Stern & Dill, 1998), etc.) Once this condition has been met, the list is sorted to draw duplicate nodes together. All nodes in the list are then checked against the closed list, with only the best version being kept and inserted onto the open list. The initial DDD algorithm used a breadth-first frontier search and therefore only the previous depth-layer was required for duplicate detection. A parallel version was later presented by Niewiadomski, Amaral, and Holte (2006a), which split each depth layer into sections and maintained separate input and output lists for each.", "startOffset": 157, "endOffset": 2349}, {"referenceID": 12, "context": "For the closed list, we used a concurrent hash table which is implemented as an array of buckets, each of which is a concurrent ordered list as developed by Harris (2001). These lock-free data structures used to implement LPA* require a special lock-free memory manager that uses reference counting and a compare-and-swap based stack to implement a free list (Valois, 1995). We will see that, even with these sophistocated structures, a straightforward parallel implementation of A* does not give competitive performance. One way of avoiding contention altogether is to allow one thread to handle synchronization of the work done by the other threads. K -Best-First Search (Felner, Kraus, & Korf, 2003) expands the best k nodes at once, each of which can be handled by a different thread. In our implementation, a master thread takes the k best nodes from open and gives one to each worker. The workers expand their nodes and the master checks the children for duplicates and inserts them into the open list. This allows open and closed to be used without locking, however, in order to adhere to a strict k -best-first ordering this approach requires the master thread to wait for all workers to finish their expansions before handing out new nodes. In the domains used in this paper, where node expansion is not particularly slow, we show that this method does not scale well. One way to reduce contention during search is to access the closed list less frequently. A technique called delayed duplicate detection (DDD) (Korf, 2003), originally developed for externalmemory search, can be used to temporarily delay access to the a closed list. While several variations have been proposed, the basic principle behind DDD is that generated nodes are added to a single list until a certain condition is met (a depth level is fully expanded, some maximum list size is reached (Stern & Dill, 1998), etc.) Once this condition has been met, the list is sorted to draw duplicate nodes together. All nodes in the list are then checked against the closed list, with only the best version being kept and inserted onto the open list. The initial DDD algorithm used a breadth-first frontier search and therefore only the previous depth-layer was required for duplicate detection. A parallel version was later presented by Niewiadomski, Amaral, and Holte (2006a), which split each depth layer into sections and maintained separate input and output lists for each. These were later merged in order to perform the usual sorting and duplicate detection methods. This large synchronization step, however, will incur costs similar to KBFS. It also depends upon an expensive workload distribution scheme to ensure that all processors have work to do, decreasing the bottleneck effect of nodes being distributed unevenly, but further increasing the algorithm\u2019s overhead. A later parallel best-first frontier search based on DDD was presented (Niewiadomski, Amaral, & Holte, 2006b), but incurs even further overhead by requiring synchronization between all threads to maintain a strict best-first ordering. Jabbar and Edelkamp (2006) present an algorithm called parallel external A* (PEA*) that uses distributed computing nodes and external memory to perform a best-first search.", "startOffset": 157, "endOffset": 3112}, {"referenceID": 8, "context": "3 Parallel Retracting A* PRA* (Evett et al., 1995) attempts to avoid contention by assigning separate open and closed lists to each thread.", "startOffset": 30, "endOffset": 50}, {"referenceID": 17, "context": "1 IMPROVEMENTS Kishimoto et al. (2009) note that the original PRA* implementation can be improved by removing the synchronization requirement on the message queues between nodes.", "startOffset": 15, "endOffset": 39}, {"referenceID": 17, "context": "Using this simple and efficient implementation, we confirmed the results of Kishimoto et al. (2009) that show that the asynchronous version of PRA* (called HDA*) outperforms the standard synchronous version.", "startOffset": 76, "endOffset": 100}, {"referenceID": 37, "context": "Zhou and Hansen (2007) used PSDD to parallelize breadth-first heuristic search (Zhou & Hansen, 2006).", "startOffset": 0, "endOffset": 23}, {"referenceID": 37, "context": "As implemented by Zhou and Hansen (2007), the PSDD algorithm uses the heuristic estimate of a node only for pruning; this is only effective if a tight upper bound is already available.", "startOffset": 18, "endOffset": 41}, {"referenceID": 37, "context": "As implemented by Zhou and Hansen (2007), the PSDD algorithm uses the heuristic estimate of a node only for pruning; this is only effective if a tight upper bound is already available. To cope with situations where a good bound is not available, we have implemented a novel algorithm using the PSDD framework that uses iterative deepening (IDPSDD) to increase the bound. As we report below, this approach is not effective in domains such as grid pathfinding that do not have a geometrically increasing number of nodes within successive f bounds. Another drawback of PSDD is that breadth-first search cannot guarantee optimality in domains where operators have differing costs. In anticipation of these problems, Zhou and Hansen (2004) suggest two possible extensions to their work, best-first search and a speculative best-first layering approach that allows for larger layers in the cases where there are few nodes (or nblocks) with the same f value.", "startOffset": 18, "endOffset": 735}, {"referenceID": 5, "context": "To approximate this, we combine PSDD\u2019s duplicate detection scopes with an idea from the Localized A* algorithm of Edelkamp and Schr\u00f6dl (2000). Localized A*, which was designed to improve the locality of external memory search, maintains sets of nodes that reside on the same memory page.", "startOffset": 114, "endOffset": 142}, {"referenceID": 27, "context": "Proof: If the heuristic is admissible, we inherit the completeness of serial A* (Nilsson, 1980) by Theorem 2.", "startOffset": 80, "endOffset": 95}, {"referenceID": 7, "context": "we used the jemalloc library (Evans, 2006), a special multi-thread-aware malloc implementation, instead of the standard glibc (version 2.", "startOffset": 29, "endOffset": 42}, {"referenceID": 18, "context": "The hash value used for tiles states was a perfect hash value based on the techniques presented by Korf and Schultze (2005). The bottom panel of Figure 9 shows the results for AHDA*, and Safe PBNF on these sliding tiles puzzle instances.", "startOffset": 99, "endOffset": 124}, {"referenceID": 14, "context": "The planner used in these experiments uses regression and the max-pair admissible heuristic of Haslum and Geffner (2000). The abstraction function used in this domain is generated dynamically on a per-problem basis and, following Zhou and Hansen (2007), this time was not taken into account in the solution times presented for these algorithms.", "startOffset": 95, "endOffset": 121}, {"referenceID": 14, "context": "The planner used in these experiments uses regression and the max-pair admissible heuristic of Haslum and Geffner (2000). The abstraction function used in this domain is generated dynamically on a per-problem basis and, following Zhou and Hansen (2007), this time was not taken into account in the solution times presented for these algorithms.", "startOffset": 95, "endOffset": 253}, {"referenceID": 11, "context": "Finally, as is described by Hansen and Zhou (2007), the reduction in open list sizes can also explain the good single thread performance that PBNF experiences on STRIPS planning (see Table 1).", "startOffset": 28, "endOffset": 51}, {"referenceID": 28, "context": "Weighted A* (Pohl, 1970), a variant of A* that orders its search on f (n) = g(n) + w \u00b7 h(n), with w > 1, is probably the most popular suboptimal search.", "startOffset": 12, "endOffset": 24}, {"referenceID": 18, "context": "3 Optimistic Search Korf (1993) showed that weighted A* typically returns solutions that are better than the bound, w , would suggest.", "startOffset": 20, "endOffset": 32}, {"referenceID": 18, "context": "3 Optimistic Search Korf (1993) showed that weighted A* typically returns solutions that are better than the bound, w , would suggest. To take advantage of this, Thayer and Ruml (2008) use an optimistic approach to bounded suboptimal search that works in two stages: aggressive search using a weight that is greater than the desired optimality bound to find an incumbent solution and then a cleanup phase to prove that the incumbent is indeed within the bound.", "startOffset": 20, "endOffset": 185}, {"referenceID": 23, "context": "Duplicate states that have already been expanded are dropped in the serial wA* algorithm, as discussed by Likhachev et al. (2003). The rows of this table show the number of threads and different algorithms whereas the columns are the weights used for various domains.", "startOffset": 106, "endOffset": 130}, {"referenceID": 18, "context": "For the sliding tiles domain, we used the standard Korf 100 15-puzzles (Korf, 1985).", "startOffset": 71, "endOffset": 83}, {"referenceID": 23, "context": "1 Anytime Repairing PBNF ARPBNF is a parallel anytime search algorithm based on ARA* (Likhachev et al., 2003).", "startOffset": 85, "endOffset": 109}, {"referenceID": 11, "context": "Hansen and Zhou (2007) show that AwA* can lead to speedup over A* for some weight values in certain domains.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "This technique has been used to improve the performance of parallel delayed duplicate detection (Korf, 1993; Korf & Schultze, 2005) which is heavily I/O intensive.", "startOffset": 96, "endOffset": 131}, {"referenceID": 17, "context": "Additionally, we have verified results presented by Kishimoto et al. (2009) that using asynchronous communication in PRA* allows it to perform better than using synchronous communication.", "startOffset": 52, "endOffset": 76}], "year": 2010, "abstractText": "To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. PBNF allows speculative expansions when necessary to keep threads busy. We identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. Our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than improved versions of previous parallel search proposals.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}