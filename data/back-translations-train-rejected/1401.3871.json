{"id": "1401.3871", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Non-Deterministic Policies in Markovian Decision Processes", "abstract": "Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the users input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. This paper introduces the new concept of non-deterministic policies to allow more flexibility in the users decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.", "histories": [["v1", "Thu, 16 Jan 2014 05:09:10 GMT  (363kb)", "http://arxiv.org/abs/1401.3871v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["mahdi milani fard", "joelle pineau"], "accepted": false, "id": "1401.3871"}, "pdf": {"name": "1401.3871.pdf", "metadata": {"source": "CRF", "title": "Non-Deterministic Policies in Markovian Decision Processes", "authors": ["Mahdi Milani Fard", "Joelle Pineau", "Milani Fard"], "emails": ["mmilan1@cs.mcgill.ca", "jpineau@cs.mcgill.ca"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "2. Definitions and Notations", "text": "This section presents the main concepts behind sequential decision-making and the mathematical formulas used in the Directive."}, {"heading": "2.1 Markov Decision Processes", "text": "A Markov Decision Process (MDP) is a model of system dynamics in sequential decision-making processes that involve a probable uncertainty about future states of the system (Bellman, 1957). MDPs are used to model the interactions between an agent and an observable Markovian environment, but the system is assumed to be in a state at a given time. The agent observes the state and performs a corresponding action. The system then makes a transition to the next state and the agent receives some reward. Formally, an MDP is defined by the 5-tuple (S, A, T, R, R): \u2022 States: S is the set of states that normally covers the complete configuration of the system. Once the state of the system is known, the future of the system is independent of all previous system transitions. This means that the state of the system is a sufficient statistic of the history of the system. \u2022 Actions: A \u2192 2A is the set of actions that is allowed in each state of the system, A is set in each state of the system."}, {"heading": "2.2 Policy and Value Function", "text": "A policy is a method of defining the choice of actions of the actor in relation to changes in the environment. (5) However, a deterministic policy is a policy that defines a single action per state, which is the value function of politics, which is defined as the expectation of returns, since the actor acts according to this policy: V \u03c0 (s) def = E [D) = E [D) = E [D). The actor interacts with the environment and acts according to politics. The value function of politics is defined as the expectation of returns, since the actor acts according to this policy. (5) The policy of the actor interacts with the environment and takes action according to politics. (5) The value function of politics is defined as the expectation of returns. (6) Using the linearity of expectation, we can write the above term in a recursive form known as Belli."}, {"heading": "2.3 Planning Algorithms and Optimality", "text": "Optimal politics, referred to as \u03c0 \u043c, is defined as the policy that maximizes the value function in the initial state. (11) It has been shown (Bellman, 1957) that for each MDP there is an optimal deterministic policy that is no worse than any other policy for this MDP. (12) The value of optimal politics V \u0445 fulfills the Bellman optimality equation: V \u0445 (s) = max a [R, a) + x s \"S T (s, a, s\") V \"(s\"). (12) The deterministic optimum politics results from this equation. (s) = argmax a \"A\" (s, a) + \"s\" S \"T\" (s, a \"s\") S \"T\" (s \") V\" (s \"s\" s \"s\" s \"s.\" (13) Alternatively, we can write these equations with the Q function: Q \"s.\""}, {"heading": "3. Non-Deterministic Policies: Definition and Motivation", "text": "Recently, MDPs have proven to be useful frameworks for optimizing options for action in the context of medical decision-making support systems (Schaefer, Bailey, Shechter, & Roberts, 2004; Hauskrecht & Fraser, 2000; Magni, Quaglini, Marchetti, & Barosi, 2000; Ernst, Stan, Concalves, & Wehenkel, 2006) Given an adequate MDP model (or source of data), many methods can be applied to find a good policy for action, which is usually a deterministic or stochastic function, but strategies of this kind face significant obstacles to gaining acceptance in the medical community because they are highly prescriptive and leave little room for maneuver in the use of the physician. Of course, these problems are not specific to the medical field and are present in any application in which the actions are carried out by a human being."}, {"heading": "3.1 Providing Choice to the Acting Agent", "text": "Even in cases where we have complete knowledge of the dynamics of the present planning problem, and where we can accurately calculate the usefulness of the measures, it may not be desirable to provide the user with only the optimal course of action at each step. In some areas, the difference between the benefits of the top few measures may not be significant. In medical decision-making, for example, this difference may not be medically significant due to the given state variables. In such cases, it seems natural to let the user decide between the top few measures, using his own expertise in this area. This leads to a further injection of expertise into the decision-making process, making it more robust and practicable. Such decisions may be based on facts that do not factor the user into the automated planning system. They may also be based on preferences that could change on a case-by-case basis. Thus, for example, a physician may receive several recommendations on how to treat a patient in order to maximize the chance of a decision, but then have to choose which drugs to use independently of the outcome."}, {"heading": "3.2 Handling Model Uncertainty", "text": "In many practical cases, we do not have complete knowledge of the system at hand. Instead, we may receive a set of data collected from the system in accordance with a specific policy. In some cases, we are given the opportunity to choose this policy (online and active), and in other cases, we may only have access to data from a specific policy. In medical studies in particular, the data is usually collected in accordance with a randomized policy determined in advance by consultation with clinical researchers. In the light of a number of examples, we can either create a model of the area (in model-based approaches) or directly estimate the benefits of various measures (with model-free approaches). However, these models and estimates are not always accurate when we look at only a limited amount of data. In many cases, the data may be too sparse and incomplete to clearly identify the best option. That is, the difference in measuring the performance of different measures is not statistically significant."}, {"heading": "4. Near-Optimal Non-Deterministic Policies", "text": "Often it is beneficial to provide the user of a decision support system with a set of near-optimal solutions (MDPs), which would mean proposing a set of near-optimal measures (MDPs) to the user and letting the user choose between the proposed measures. Therefore, the concept of near-optimality should be based on all possible measures consistent with the proposed measures. Regardless of which action is chosen among the proposed options in each state, the final performance should be close to the choice of optimal measures. Such limitations suggest a worst-case analysis of the decision-making process. Therefore, we choose to guarantee the performance of any action policy consistent with a non-deterministic policy by imposing near-optimum limitation of actions by the user. Definition 3. The (worst-case) value of a government policy (s-case) is equivalent."}, {"heading": "4.1 -Optimal Non-Deterministic Policies", "text": "Definition 7: A non-deterministic policy on an MDP M is defined as -optimal (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative)) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) (conservative) ("}, {"heading": "4.2 Optimization Criteria", "text": "We formalize the problem of finding an -optimal, non-deterministic policy on an optimization problem. \u2022 There are several optimization criteria that can be formulated while maintaining the -optimal constraint. \u2022 Maximizing the size of the policy: According to this criterion, we are looking for non-magnifiable -optimal strategies that have the largest overall size (Def 2), which gives the agent more options while retaining the -optimal guarantees. The algorithms proposed in later sections use this optimization criterion. Note that the solution to this optimization problem is not augmentable according to the -optimal constraint because it maximizes the overall size of the policy."}, {"heading": "4.3 Maximal -Optimal Policy", "text": "The problem is certainly NP, since one can find the value of non-deterministic politics in polynomial time by solving the evaluation MDP with a linear program. We suspect that the problem NP-complete, but we still have to find a reduction from a known NP-complete problem. To find the greatest possible -optimal policy, we present two algorithms. We first present a formulation of the problem in the Mixed Integer Program (MIP) and then present a search algorithm that uses the monotonous property of the -optimal constraint. While the MIP method is useful as a general and theoretical formulation of the problem, the search algorithm has potential for further extensions with heuristics."}, {"heading": "4.3.1 Mixed Integer Programming Solution", "text": "Remember that we can formulate the problem of searching for the optimal deterministic politics of an MDP as a simple linear program (Bertsekas, 1995): minV \u00b5 TV, subjected to V (s) \u2265 R (s, a) + \u03b3 \"s\" T (s, s \") V (s\") \"s, a,\" (31), where \u00b5 can be thought of as an initial distribution across states. The solution to the above problem is the optimal value function (V \"s) eTs.\" Similarly, the problem of searching for an optimal, non-deterministic policy can be rewritten according to the size criterion, which is called the Mixed Integer Program: 2maxV \"s.\" (Vmin) eTs \"ea,\" subjected to toV \"(s)\" V \"s\" (s) \"s\" s. \""}, {"heading": "4.3.2 Heuristic Search", "text": "In fact, it is not the case that we would be able to pursue a policy in which the state is not able to take the necessary measures to take them. Indeed, the algorithm presented in Table 1 is a one-sided recursive depth search algorithm that seeks in the space of plausible, non-deterministic politics to maximize the function g (i.e. the search tree). Here, we assume that there is an order of state action packages, the {pi} = (sj, ak)}. This order can be sought in the space of plausible, non-deterministic politics to maximize the function g (i.e.). We assume that the order of state action pacts {pi} = (sj, ak)}}."}, {"heading": "4.3.3 Directed Acyclic Transition Graphs", "text": "One way to restrict the search is to add only the action that has the maximum value for each state, and ignore the rest of the actions if the top action results in values outside the optimum limit. (33) The modified algorithm in Table 2 results in a runtime of O (| S | d (tm + tg), but this does not guarantee that we will see all non-augmentable policies, due to the fact that the order of values may change if the transition structure of the MDP does not contain a loop with a non-zero probability."}, {"heading": "5. Empirical Results", "text": "To evaluate our framework and proposed algorithms, we first test the MIP and search formulations of randomly generated MDPs, and then test the search algorithm against a real-world treatment design scenario. Finally, we conduct an experiment on a computer-aided web navigation task with human subjects to assess the usefulness of non-deterministic strategies to support human decision-making."}, {"heading": "5.1 Random MDPs", "text": "In the first experiment, we aim to investigate how non-deterministic strategies compare with the value of and how the two algorithms compare in terms of runtime. In the beginning, we generated random MDPs with 5 states and 4 actions, the transitions are deterministic (evenly chosen randomly) and the rewards are random values between 0 and 1, except for one of the states with reward 10 for one of its actions. The MIP method was used with MATLAB and CPLEX.Figure 5 shows the solution of the MIP defined in Eqn 32 for a certain randomly generated MDP. We see that the size of non-deterministic policies increases with performance limits. We can see that even with small values in policy, several actions are included for each state. This is, of course, a result of the Q values that are close together."}, {"heading": "5.2 Medical Decision-making", "text": "To demonstrate how non-deterministic strategies can be used and presented in a medical setting, we tested the full search algorithm on an MDP designed for a medical decision-making task with real patient data. Data was collected in a large (4000 + patients) multi-stage randomized clinical trial designed to sequentially examine the comparative efficacy of different treatments (Fava et al., 2003) with the goal of finding a treatment plan that maximizes the chance of release. The data set includes a large number of measured outcomes. For the current experiment, we are focusing on a numerical score called Quick Inventory of Depressive Symptomatology (QIDS) used in the study to assess the level of depression (including patients who have achieved remission). For the purposes of our experiment, we are discrediting the results of the quantitative assessment (ranging from 5 to 27)."}, {"heading": "5.3 Human Subject Interaction", "text": "Finally, we conduct an experiment to assess the usefulness of non-deterministic strategies with human subjects. Ideally, we would conduct such experiments in medicine and with doctors, but such studies are costly and difficult to perform as they require the participation of many medical professionals. Therefore, we study non-deterministic strategies in a simpler domain by building a web-based game that can be played by any computer and people (either jointly or separately).The game is defined as follows: A user is asked to navigate the pages of Wikipedia and visit pages that contain that target, the user can click on each word in a page, and the system then uses a Google search on the wiki website with the word clicked and a keyword in the current page. It then randomly becomes one of the eight search results and moves to that page."}, {"heading": "6. Discussion", "text": "This year it is more than ever before."}, {"heading": "Acknowledgments", "text": "The authors thank A. John Rush (Duke-NUS Graduate Medical School), Susan A. Murphy (University of Michigan) and Doina Precup (McGill University) for their helpful discussions on this work, which was funded by the National Institutes of Health (grant R21 DA019800) and the NSERC Discovery Grant program."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Dynamic Programming and Optimal Control, Vol 2", "author": ["D. Bertsekas"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas,? 1995", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Clinical data based optimal STI strategies for HIV: a reinforcement learning approach", "author": ["D. Ernst", "G.B. Stan", "J. Concalves", "L. Wehenkel"], "venue": "In Proceedings of the Fifteenth Machine Learning conference of Belgium and The Netherlands (Benelearn),", "citeRegEx": "Ernst et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2006}, {"title": "Background and rationale for the sequenced treatment alternatives to relieve depression (STAR* D) study", "author": ["M. Fava", "A. Rush", "M. Trivedi", "A. Nierenberg", "M. Thase", "H. Sackeim", "F. Quitkin", "S. Wisniewski", "P. Lavori", "J. Rosenbaum", "D. Kupfer"], "venue": "Psychiatric Clinics of North America,", "citeRegEx": "Fava et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fava et al\\.", "year": 2003}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Planning treatment of ischemic heart disease with partially observable Markov decision processes", "author": ["M. Hauskrecht", "H. Fraser"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Hauskrecht and Fraser,? \\Q2000\\E", "shortCiteRegEx": "Hauskrecht and Fraser", "year": 2000}, {"title": "Consideration of risk in reinforcement learning", "author": ["M. Heger"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning (ICML), pp. 105\u2013111.", "citeRegEx": "Heger,? 1994", "shortCiteRegEx": "Heger", "year": 1994}, {"title": "A new polynomial-time algorithm for linear programming", "author": ["N. Karmarkar"], "venue": "Combinatorica, 4 (4), 373\u2013395.", "citeRegEx": "Karmarkar,? 1984", "shortCiteRegEx": "Karmarkar", "year": 1984}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh", "year": 2002}, {"title": "Deciding when to intervene: a Markov decision process approach", "author": ["P. Magni", "S. Quaglini", "M. Marchetti", "G. Barosi"], "venue": "International Journal of Medical Informatics,", "citeRegEx": "Magni et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Magni et al\\.", "year": 2000}, {"title": "Bias and variance in value function estimation", "author": ["S. Mannor", "D. Simester", "P. Sun", "J.N. Tsitsiklis"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning (ICML),", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Bias and variance approximation in value function estimates", "author": ["S. Mannor", "D. Simester", "P. Sun", "J.N. Tsitsiklis"], "venue": "Management Science,", "citeRegEx": "Mannor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2007}, {"title": "Amazon mechanical turk", "author": ["MTurk"], "venue": "http://www.mturk.com/.", "citeRegEx": "MTurk,? 2010", "shortCiteRegEx": "MTurk", "year": 2010}, {"title": "An experimental design for the development of adaptive treatment strategies", "author": ["S.A. Murphy"], "venue": "Statistics in Medicine, 24 (10), 1455\u20131481.", "citeRegEx": "Murphy,? 2005", "shortCiteRegEx": "Murphy", "year": 2005}, {"title": "Constructing evidence-based treatment strategies using methods from computer science", "author": ["J. Pineau", "M.G. Bellemare", "A.J. Rush", "A. Ghizaru", "S.A. Murphy"], "venue": "Drug and Alcohol Dependence,", "citeRegEx": "Pineau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2007}, {"title": "Artificial Intelligence: A Modern Approach (Second Edition)", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2003\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2003}, {"title": "Variance-penalized reinforcement learning for risk-averse asset allocation", "author": ["M. Sato", "S. Kobayashi"], "venue": "In Proceedings of the Second International Conference on Intelligent Data Engineering and Automated Learning,", "citeRegEx": "Sato and Kobayashi,? \\Q2000\\E", "shortCiteRegEx": "Sato and Kobayashi", "year": 2000}, {"title": "Handbook of Operations Research / Management Science Applications in Health Care, chap. Medical decisions using Markov decision processes", "author": ["A. Schaefer", "M. Bailey", "S. Shechter", "M. Roberts"], "venue": null, "citeRegEx": "Schaefer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schaefer et al\\.", "year": 2004}, {"title": "2008/9 wikipedia selection for schools", "author": ["Schools-Wikipedia"], "venue": "http://schoolswikipedia.org/.", "citeRegEx": "Schools.Wikipedia,? 2009", "shortCiteRegEx": "Schools.Wikipedia", "year": 2009}, {"title": "Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Agent based decision support system using reinforcement learning under emergency circumstances", "author": ["D. Thapa", "I. Jung", "G. Wang"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Thapa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thapa et al\\.", "year": 2005}, {"title": "Wikispeedia: an online game for inferring semantic distances between concepts", "author": ["R. West", "J. Pineau", "D. Precup"], "venue": "In Proceedings of the Twenty-First International Jont Conference on Artifical Intelligence (IJCAI),", "citeRegEx": "West et al\\.,? \\Q2009\\E", "shortCiteRegEx": "West et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "In the past few years, methods developed by the RL community have started to be used in sequential decision support systems (Murphy, 2005; Pineau, Bellemare, Rush, Ghizaru, & Murphy, 2007; Thapa, Jung, & Wang, 2005; Hauskrecht & Fraser, 2000).", "startOffset": 124, "endOffset": 242}, {"referenceID": 0, "context": "A Markov Decision Process (MDP) is a model of system dynamics in sequential decision problems that involves probabilistic uncertainty about future states of the system (Bellman, 1957).", "startOffset": 168, "endOffset": 183}, {"referenceID": 0, "context": "Using the linearity of the expectation, we can write the above expression in a recursive form, known as the Bellman equation (Bellman, 1957):", "startOffset": 125, "endOffset": 140}, {"referenceID": 6, "context": "There are, however, some ideas that take the risk or the variance of the return into account as a measure of optimality (Heger, 1994; Sato & Kobayashi, 2000).", "startOffset": 120, "endOffset": 157}, {"referenceID": 0, "context": "It has been shown (Bellman, 1957) that for any MDP, there exists an optimal deterministic policy that is no worse than any other policy for that MDP.", "startOffset": 18, "endOffset": 33}, {"referenceID": 1, "context": "The Bellman optimality equation can be formulated as a simple linear program (Bertsekas, 1995):", "startOffset": 77, "endOffset": 94}, {"referenceID": 7, "context": "It is known that linear programs can be solved in polynomial time (Karmarkar, 1984).", "startOffset": 66, "endOffset": 83}, {"referenceID": 1, "context": "1 Mixed Integer Programming Solution Recall that we can formulate the problem of finding the optimal deterministic policy of an MDP as a simple linear program (Bertsekas, 1995):", "startOffset": 159, "endOffset": 176}, {"referenceID": 3, "context": "The data was collected as part of a large (4000+ patients) multi-step randomized clinical trial, designed to investigate the comparative effectiveness of different treatments provided sequentially for patients suffering from depression (Fava et al., 2003).", "startOffset": 236, "endOffset": 255}, {"referenceID": 18, "context": "We construct this task on the CD version of Wikipedia (Schools-Wikipedia, 2009), which is a structured and manageable version of Wikipedia intended for use in schools.", "startOffset": 54, "endOffset": 79}, {"referenceID": 12, "context": "Using Amazon Mechanical Turk (MTurk, 2010), we consider three experimental conditions with this task.", "startOffset": 29, "endOffset": 42}], "year": 2011, "abstractText": "Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the user\u2019s input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. This paper introduces the new concept of non-deterministic policies to allow more flexibility in the user\u2019s decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.", "creator": "TeX"}}}