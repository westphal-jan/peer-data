{"id": "1401.3880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Regression Conformal Prediction with Nearest Neighbours", "abstract": "In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours Regression (k-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures.", "histories": [["v1", "Thu, 16 Jan 2014 05:12:21 GMT  (331kb)", "http://arxiv.org/abs/1401.3880v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harris papadopoulos", "vladimir vovk", "alex gammerman"], "accepted": false, "id": "1401.3880"}, "pdf": {"name": "1401.3880.pdf", "metadata": {"source": "CRF", "title": "Regression Conformal Prediction with Nearest Neighbours", "authors": ["Harris Papadopoulos", "Vladimir Vovk", "Alex Gammerman"], "emails": ["h.papadopoulos@frederick.ac.cy", "vovk@cs.rhul.ac.uk", "alex@cs.rhul.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Conformal Prediction", "text": "In this section we briefly describe the idea behind the formal prediction; for a more detailed description, the interested reader is referred to the book of Vovk et al. (2005). (1) + 1) + 1 (1) + 1 (1) + 1 (1) + 1). (1) + 1. \"(1) + 1.\" (1). \"1\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). \"(1).\" (1). (1). \"(1). (1). (1). (1). (). (1). (). (1). (). (1). (). (1). (). (1). (). (1). (1). (). (1). ().). (1). (1). ().). (1.). (1).). (1. (). (1.).). (1. (1.).). (1.). (1.).). (1.). (1. (1. ().). (1.).). (1.). (1.). (1. (1.). (1.). (1.). (1.). (1.). (1. (1.). (1.). (1.). (1. (1. (1.).). (1.). (1. (1. (1.).). (1.). (1.). (1. (1.). (1.). (1.). (1. (1.). (1. (1.). (1.). (1. (1.). (1. (1.). (1. (1.)."}, {"heading": "5. Normalized Nonconformity Measures", "text": "The main objective of this work was to improve the typical regression non-conformity measurement (8) by normalizing it with the expected accuracy of the underlying method (1). The intuition behind this is that if two examples have the same non-conformity measurement as defined by (8) and the prediction y for one of them, the first measure will be more accurate than the other, then the first measure will actually be weirder than the second. Since the next training examples are the ones actually used to derive the prediction of our underlying method for an example, the more accurate measurements of the expected accuracy we use are based on the distance of the example from its nearest neighbors."}, {"heading": "6. Theoretical Analysis of Nonconformity Measure (29)", "text": "In this section, we examine k-NNR ICP with non-conformity measure (29) under some specific assumptions and show that under these assumptions, the predictive ranges required for our conclusions are asymptotically optimal; it is important to note that these assumptions are not necessary for the validity of the resulting q ranges. We have chosen not to formalize all the conditions required for our conclusions, as this would have made our statement much too complicated. Suppose that each label yi is generated by a normal distribution N (\u00b5xi, \u03c32xi), which is a smooth function of x, that each xi is generated by a probability distribution focused on a compact quantity and whose density is always greater than any constant > 0, and that k 1, m qqqk and q k. In this case, non-conformity measure (29) with a normal measurement (\u00b5x) is close to the normalization (29), which is oriented to the objectivity (0)."}, {"heading": "7. Experimental Results", "text": "Our methods were tested on six benchmark datasets from the UCI (Frank & Asuncion, 2010) and DELVE (Rasmussen et al., 1996) was only a limited amount of data: \u2022 Boston Housing, which describes the median house prices for 506 different areas of Boston MA in $1000s. Each area is described by 13 attributes such as pollution and crime rate. \u2022 Abalone, which concerns the prediction of the age of abalone of physical measurements. The dataset consists of 4177 examples described by 8 attributes such as diameter, height and shell weight. \u2022 Computer activity, which is a collection of computer system activity measurements from a Sun SPARCstation 20 / 712 with 128 Mbytes of memory runtime. It consists of 8192 examples of 12 measured values such as the number of system buffers reading per second and the number of system calls per second predicting random points in time."}, {"heading": "8. Comparison with Gaussian Process Regression", "text": "In this context, it should be noted that they are two groups that are able to outdo each other and outdo each other."}, {"heading": "9. Conclusions", "text": "We presented the Transductive and Inductive Conformal Predictors based on the k-Nearest Neighbors Regression Algorithm. In addition to typical regression nonconformity, we have developed six new definitions that take into account the expected accuracy of the k-NNR algorithm using the example in question. Our definitions evaluate the expected accuracy of the k-NNR based on its distances from its nearest examples (24) and (25), on the standard deviation of its labels (29) or on a combination of the two (31) and (32). The experimental results obtained by applying our methods show that in all cases reliable prediction intervals are narrow enough to be useful in practice."}, {"heading": "Acknowledgments", "text": "We would like to thank Savvas Pericleous and Haris Haralambous for their useful discussions, as well as the anonymous reviewers for their insightful and constructive comments. This work was supported in part by the Cyprus Research Promotion Foundation through a research grant PLHRO / 0506 / 22 (\"Development of New Conformal Prediction Methods with Applications in Medical Diagnosis\")."}], "references": [{"title": "Qualified predictions for microarray and proteomics pattern diagnostics with confidence machines", "author": ["T. Bellotti", "Z. Luo", "A. Gammerman", "F.W.V. Delft", "V. Saha"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Bellotti et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bellotti et al\\.", "year": 2005}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2000}, {"title": "Network traffic demand prediction with confidence", "author": ["M. Dashevskiy", "Z. Luo"], "venue": "In Proceedings of the IEEE Global Telecommunications Conference 2008 (GLOBECOM", "citeRegEx": "Dashevskiy and Luo,? \\Q2008\\E", "shortCiteRegEx": "Dashevskiy and Luo", "year": 2008}, {"title": "Learning by transduction", "author": ["A. Gammerman", "V. Vapnik", "V. Vovk"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Gammerman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 1998}, {"title": "Serum proteomic abnormality predating screen detection of ovarian cancer", "author": ["A. Gammerman", "V. Vovk", "B. Burford", "I. Nouretdinov", "Z. Luo", "A. Chervonenkis", "M. Waterfield", "R. Cramer", "P. Tempst", "J. Villanueva", "M. Kabir", "S. Camuzeaux", "J. Timms", "U. Menon", "I. Jacobs"], "venue": "The Computer Journal,", "citeRegEx": "Gammerman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 2009}, {"title": "Hedging predictions in machine learning: The second computer journal lecture", "author": ["A. Gammerman", "V. Vovk"], "venue": "The Computer Journal,", "citeRegEx": "Gammerman and Vovk,? \\Q2007\\E", "shortCiteRegEx": "Gammerman and Vovk", "year": 2007}, {"title": "Intelligent computer reporting \u2018lack of experience\u2019: a confidence measure for decision support systems", "author": ["H. Holst", "M. Ohlsson", "C. Peterson", "L. Edenbrandt"], "venue": "Clinical Physiology,", "citeRegEx": "Holst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Holst et al\\.", "year": 1998}, {"title": "Comparing the Bayes and Typicalness frameworks", "author": ["T. Melluish", "C. Saunders", "I. Nouretdinov", "V. Vovk"], "venue": "In Proceedings of the 12th European Conference on Machine Learning (ECML\u201901),", "citeRegEx": "Melluish et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melluish et al\\.", "year": 2001}, {"title": "Ridge regression confidence machine", "author": ["I. Nouretdinov", "T. Melluish", "V. Vovk"], "venue": "In Proceedings of the 18th International Conference on Machine Learning", "citeRegEx": "Nouretdinov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Nouretdinov et al\\.", "year": 2001}, {"title": "Pattern recognition and density estimation under the general i.i.d. assumption", "author": ["I. Nouretdinov", "V. Vovk", "M.V. Vyugin", "A. Gammerman"], "venue": "In Proceedings of the 14th Annual Conference on Computational Learning Theory and 5th European Conference on Computational Learning Theory, Vol. 2111 of Lecture Notes in Computer Science,", "citeRegEx": "Nouretdinov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Nouretdinov et al\\.", "year": 2001}, {"title": "Inductive Conformal Prediction: Theory and application to neural networks", "author": ["H. Papadopoulos"], "venue": "Tools in Artificial Intelligence,", "citeRegEx": "Papadopoulos,? \\Q2008\\E", "shortCiteRegEx": "Papadopoulos", "year": 2008}, {"title": "Normalized nonconformity measures for regression conformal prediction", "author": ["H. Papadopoulos", "A. Gammerman", "V. Vovk"], "venue": "In Proceedings of the IASTED International Conference on Artificial Intelligence and Applications (AIA", "citeRegEx": "Papadopoulos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2008}, {"title": "Confidence predictions for the diagnosis of acute abdominal pain", "author": ["H. Papadopoulos", "A. Gammerman", "V. Vovk"], "venue": "Artificial Intelligence Applications & Innovations III,", "citeRegEx": "Papadopoulos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2009}, {"title": "Reliable confidence intervals for software effort estimation", "author": ["H. Papadopoulos", "E. Papatheocharous", "A.S. Andreou"], "venue": "In Proceedings of the 2nd Workshop on Artificial Intelligence Techniques in Software Engineering (AISEW 2009),", "citeRegEx": "Papadopoulos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2009}, {"title": "Inductive confidence machines for regression", "author": ["H. Papadopoulos", "K. Proedrou", "V. Vovk", "A. Gammerman"], "venue": "In Proceedings of the 13th European Conference on Machine Learning (ECML\u201902),", "citeRegEx": "Papadopoulos et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2002}, {"title": "Qualified predictions for large data sets in the case of pattern recognition", "author": ["H. Papadopoulos", "V. Vovk", "A. Gammerman"], "venue": "In Proceedings of the 2002 International Conference on Machine Learning and Applications (ICMLA\u201902),", "citeRegEx": "Papadopoulos et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2002}, {"title": "Conformal prediction with neural networks", "author": ["H. Papadopoulos", "V. Vovk", "A. Gammerman"], "venue": "In Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence (ICTAI\u201907),", "citeRegEx": "Papadopoulos et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2007}, {"title": "Transductive confidence machines for pattern recognition", "author": ["K. Proedrou", "I. Nouretdinov", "V. Vovk", "A. Gammerman"], "venue": "In Proceedings of the 13th European Conference on Machine Learning (ECML\u201902),", "citeRegEx": "Proedrou et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Proedrou et al\\.", "year": 2002}, {"title": "DELVE: Data for evaluating learning in valid experiments. URL http://www.cs.toronto.edu/\u223cdelve", "author": ["C.E. Rasmussen", "R.M. Neal", "G.E. Hinton", "D. Van Camp", "M. Revow", "Z. Ghahramani", "R. Kustra", "R. Tibshirani"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 1996}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Transduction with confidence and credibility", "author": ["C. Saunders", "A. Gammerman", "V. Vovk"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Saunders et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 1999}, {"title": "Computationally efficient transductive machines", "author": ["C. Saunders", "A. Gammerman", "V. Vovk"], "venue": "In Proceedings of the Eleventh International Conference on Algorithmic Learning Theory (ALT\u201900), Vol. 1968 of Lecture Notes in Artificial Intelligence,", "citeRegEx": "Saunders et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 2000}, {"title": "Plant promoter prediction with confidence estimation", "author": ["I.A. Shahmuradov", "V.V. Solovyev", "A.J. Gammerman"], "venue": "Nucleic Acids Research,", "citeRegEx": "Shahmuradov et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shahmuradov et al\\.", "year": 2005}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant,? \\Q1984\\E", "shortCiteRegEx": "Valiant", "year": 1984}, {"title": "Algorithmic Learning in a Random World", "author": ["V. Vovk", "A. Gammerman", "G. Shafer"], "venue": null, "citeRegEx": "Vovk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vovk et al\\.", "year": 2005}, {"title": "Recognition of hypoxia EEG with a preset confidence level based on EEG analysis", "author": ["J. Zhang", "G. Li", "M. Hu", "J. Li", "Z. Luo"], "venue": "In Proceedings of the International Joint Conference on Neural Networks (IJCNN 2008), part of the IEEE World Congress on Computational Intelligence (WCCI", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "One can apply the theory of Probably Approximately Correct learning (PAC theory, Valiant, 1984) to an algorithm in order to obtain upper bounds on the probability of its error with respect to some confidence level. The bounds produced by PAC theory though, will be very weak unless the data set to which the algorithm is being applied is particularly clean, which is rarely the case. Nouretdinov, Vovk, Vyugin, and Gammerman (2001b) demonstrated the crudeness of PAC bounds by applying one of the best bounds, by Littlestone and Warmuth (Cristianini & Shawe-Taylor, 2000, Thm.", "startOffset": 81, "endOffset": 433}, {"referenceID": 20, "context": "Different variants of CPs have been developed based on Support Vector Machines (Saunders et al., 1999; Saunders, Gammerman, & Vovk, 2000), Ridge Regression (Nouretdinov, Melluish, & Vovk, 2001a; Papadopoulos, Proedrou, Vovk, & Gammerman, 2002a), k-Nearest Neighbours for classification (Proedrou, Nouretdinov, Vovk, & Gammerman, 2002; Papadopoulos, Vovk, & Gammerman, 2002b) and Neural Networks (Papadopoulos, Vovk, & Gammerman, 2007), all of which have been shown to give reliable and high quality confidence measures.", "startOffset": 79, "endOffset": 137}, {"referenceID": 4, "context": "Moreover, CP has been applied successfully to many problems such as the early detection of ovarian cancer (Gammerman et al., 2009), the classification of leukaemia subtypes (Bellotti, Luo, Gammerman, Delft, & Saha, 2005), the diagnosis of acute abdominal pain (Papadopoulos, Gammerman, & Vovk, 2009a), the prediction of plant promoters (Shahmuradov, Solovyev, & Gammerman, 2005), the recognition of hypoxia electroencephalograms (EEGs) (Zhang, Li, Hu, Li, & Luo, 2008), the prediction of network traffic demand (Dashevskiy & Luo, 2008) and the estimation of effort for software projects (Papadopoulos, Papatheocharous, & Andreou, 2009b).", "startOffset": 106, "endOffset": 130}, {"referenceID": 3, "context": "A thorough analysis of CP was given by Vovk, Gammerman, and Shafer (2005), while an overview was presented by Gammerman and Vovk (2007). Conformal Predictors are built on top of traditional machine learning algorithms and accompany each of their predictions with valid measures of confidence.", "startOffset": 110, "endOffset": 136}, {"referenceID": 8, "context": "For this reason a modification of the original CP approach, called Inductive Conformal Prediction (ICP), was proposed by Papadopoulos et al. (2002a) for regression and by Papadopoulos et al.", "startOffset": 121, "endOffset": 149}, {"referenceID": 8, "context": "For this reason a modification of the original CP approach, called Inductive Conformal Prediction (ICP), was proposed by Papadopoulos et al. (2002a) for regression and by Papadopoulos et al. (2002b) for classification.", "startOffset": 121, "endOffset": 199}, {"referenceID": 8, "context": "For this reason a modification of the original CP approach, called Inductive Conformal Prediction (ICP), was proposed by Papadopoulos et al. (2002a) for regression and by Papadopoulos et al. (2002b) for classification. As suggested by its name, ICP replaces the transductive inference followed in the original approach with inductive inference. Consequently, ICPs are almost as computationally efficient as their underlying algorithms. This is achieved at the cost of some loss in the quality of the produced confidence measures, but this loss is negligible, especially when the data set in question is large, whereas the improvement in computational efficiency is significant. A computational complexity comparison between the original CP and ICP approaches was performed by Papadopoulos (2008). From now on, in order to differentiate clearly between the original CP and ICP approaches the former will be called Transductive Conformal Prediction (TCP).", "startOffset": 121, "endOffset": 796}, {"referenceID": 8, "context": "The first regression CPs were proposed by Nouretdinov et al. (2001a) following the TCP approach and by Papadopoulos et al.", "startOffset": 42, "endOffset": 69}, {"referenceID": 8, "context": "The first regression CPs were proposed by Nouretdinov et al. (2001a) following the TCP approach and by Papadopoulos et al. (2002a) following the ICP approach, both based on the Ridge Regression algorithm.", "startOffset": 42, "endOffset": 131}, {"referenceID": 8, "context": "The first regression CPs were proposed by Nouretdinov et al. (2001a) following the TCP approach and by Papadopoulos et al. (2002a) following the ICP approach, both based on the Ridge Regression algorithm. As opposed to the conventional point predictions, the output of regression CPs is a predictive region that satisfies a given confidence level. The typical nonconformity measure used so far in the case of regression is the absolute difference |yi\u2212 \u0177i|, between the actual label yi of the example i and the predicted label \u0177i of the underlying algorithm for that example, given the old examples as training set. Here we propose six extensions to this nonconformity measure for k -Nearest Neighbours Regression and develop the corresponding Inductive and Transductive CPs; unfortunately although all six new measures can be used with the ICP approach, only two of them can be used with TCP. Our definitions normalize the standard measure based on the expected accuracy of the underlying algorithm for each example, which makes the width of the resulting predictive regions vary accordingly. As a result, the predictive regions produced by our measures are in general much tighter than those produced by the standard regression measure. This paper extends our previous work (Papadopoulos, Gammerman, & Vovk, 2008) where the k -Nearest Neighbours Regression TCP was developed using two normalized nonconformity measures. It is also worth mentioning that one other such nonconformity measure definition was presented by Papadopoulos et al. (2002a) for the Ridge Regression ICP.", "startOffset": 42, "endOffset": 1547}, {"referenceID": 24, "context": "In this section we briefly describe the idea behind Conformal Prediction; for a more detailed description the interested reader is referred to the book by Vovk et al. (2005). We are given a training set {z1, .", "startOffset": 155, "endOffset": 174}, {"referenceID": 8, "context": "a proof was given by Nouretdinov et al. (2001b). As a result, if the p-value of a given label is below some very low threshold, say 0.", "startOffset": 21, "endOffset": 48}, {"referenceID": 8, "context": "Following Nouretdinov et al. (2001a) and Vovk et al.", "startOffset": 10, "endOffset": 37}, {"referenceID": 8, "context": "Following Nouretdinov et al. (2001a) and Vovk et al. (2005) we express the nonconformity score \u03b1i of each example i = 1, .", "startOffset": 10, "endOffset": 60}, {"referenceID": 11, "context": "We could use di as a measure of accuracy, in fact it was used successfully in our previous work (Papadopoulos et al., 2008).", "startOffset": 96, "endOffset": 123}, {"referenceID": 18, "context": "Our methods were tested on six benchmark data sets from the UCI (Frank & Asuncion, 2010) and DELVE (Rasmussen et al., 1996) repositories:", "startOffset": 99, "endOffset": 123}, {"referenceID": 19, "context": "All hyperparameters were adapted by maximizing marginal likelihood on each training set as suggested by Rasmussen and Williams (2006); the adaptation of hyperparameters using leave-one-out cross-validation produces more or less the same results.", "startOffset": 104, "endOffset": 134}], "year": 2011, "abstractText": "In this paper we apply Conformal Prediction (CP) to the k -Nearest Neighbours Regression (k -NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k -Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures.", "creator": "TeX"}}}