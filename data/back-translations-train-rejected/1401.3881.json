{"id": "1401.3881", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Value of Information Lattice: Exploiting Probabilistic Independence for Effective Feature Subset Acquisition", "abstract": "We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. We describe the Value of Information Lattice (VOILA), an optimal and efficient feature subset acquisition framework. Unlike the common practice, which is to acquire features greedily, VOILA can reason with subsets of features. VOILA efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination.", "histories": [["v1", "Thu, 16 Jan 2014 05:12:42 GMT  (1680kb)", "http://arxiv.org/abs/1401.3881v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mustafa bilgic", "lise getoor"], "accepted": false, "id": "1401.3881"}, "pdf": {"name": "1401.3881.pdf", "metadata": {"source": "CRF", "title": "Value of Information Lattice: Exploiting Probabilistic Independence for Effective Feature Subset Acquisition", "authors": ["Mustafa Bilgic", "Lise Getoor"], "emails": ["mbilgic@iit.edu", "getoor@cs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "The question is whether and in what form people will be able to survive themselves, and the question is to what extent they see themselves able to survive themselves, and to what extent they are able to survive themselves, and to what extent they are able to survive themselves, and to what extent they are able to survive themselves, and whether they are able to survive themselves, to survive themselves - and to survive themselves - and to survive themselves - and to survive themselves - and to survive themselves - and to survive themselves."}, {"heading": "2. Notation and Problem Formulation", "text": "Our main task is to classify a particular instance that represents a random variable and causes minimal acquisition and misclassification costs. Let the instance be described by a set of attributes X = {X1, X2,.., Xn} and let Y be the random variable that represents its class. However, we assume that the common probability distribution P (Y, X) depends on the costs acquired and we deal with any common probabilistic model that allows us to efficiently answer conditional independence questions. In notation, a set of random variables and non-bold response letters represents a single random variable. For example, X represents the set of attributes, while Xi X represents a feature of the class variable."}, {"heading": "3.1 Reducing the Space of Possible Sets", "text": "In most cases, these are complex interactions between the characteristics and the class label (Y), which we assume to represent a particular network. Contrary to naive bayes, characteristics are often not unconditionally independent from the class label. Some characteristics are useless once some other characteristics have already been acquired. For example, a breast X-ray is typically more specific than a skin test for tuberculosis. Likewise, some characteristics alone are useless if they are not accompanied by other characteristics. For example, chest pain alone could be due to a variety of diseases; if it is accompanied by high cholesterol, it could indicate heart disease, whereas combination with fever is more likely. These types of interactions between the characteristics allow us to reduce the space of candidate characteristics. As we have mentioned in the problem formulation, we assume that we already have a common probability model of the characteristics and class variables."}, {"heading": "3.2.1 Subset Relationships", "text": "VOILA uses the subset relationships between different sets of characteristics to avoid the calculation of EVI for some nodes (B). First, if there is a directed path from nodes S1 to S2 in VOILA, then S1, S2 and hence EV I (S1, E) \u2265 EV I (S2, E) 1. Now we assume that there is a directed path from Si to Sj and EV I (Si, e) = EV I (Sj, e), while all nodes on this path also have the same EVI, so we do not need to perform the calculation for these subsets. An algorithm that makes use of this observation is given in Algorithm 1: Efficient EVI calculation with VOILA. Input: VOILA V and Current Evidence E-Output: VOILA updates with correct EVI values (S)."}, {"heading": "3.2.2 Information Pathways at the Underlying Bayesian Network", "text": "The second mechanism that VOILA uses to divide EVI calculations is by the edges of the underlying Bayesian network. We specifically use the following fact: Proposition 2 For all S1 and S2, if S1 d-separates Y from S2 with respect to e, then EV I (S1 | e) \u2265 EV I (S2 | e).Proof: Consider S12 = S1 | e. From the partial relationship, we know that EV I (S12 | e) \u2265 EV I (S1 | e) and EV I (S12 | e) \u2265 EV I (S2 | e).EV I (S12 | e) \u2212, s12 P (s12 | e) \u2265 EV I (Y | e, s12) = EMY (Y | e) = EMC (Y | e) that (1, s2) \u2212 s2 P (s2, s2 | e) EMC."}, {"heading": "3.2.3 Incremental Inference", "text": "The third and final mechanism that VOILA uses to integrate the additional evidence for the VOILA nodes is the. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D\" D. \"D\" D \"D\" D. \"D\" D \"D\" D. \"D\" D \"D\" D. \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D"}, {"heading": "4. Experiments", "text": "We experimented with five real-world medical datasets that Turney (1995) described and used in his paper: Bupa liver disease, heart disease, hepatitis, Pima Indian diabetes, and thyroid disease, all of which are available through the UCI Machine Learning Repository (Frank & Asuncion, 2010). These datasets had a varying number of characteristics ranging from five to 20. Four of the five datasets had binary labels, whereas the thyroid dataset had three labels. For each dataset, we first learned a Bayesian network that provides both the common probability distribution P (Y, X) and thoroughly answers conditional independence queries (Pearl, 1988)."}, {"heading": "4.1 Search Space Reduction", "text": "Table 1 shows aggregated statistics for each data set and describes the number of characteristics, the number of all possible subsets, the number of subsets in VOILA and the percentage reduction of the search space. As this table shows, the number of irreducible subsets is much smaller than all possible subsets. For the thyroid disease data set, for example, the number of possible subsets is over one million, while the number of irreducible subsets is less than thirty thousand. This enormous reduction in the search space makes the search by the possible characteristics comprehensible in practice."}, {"heading": "4.2 Expected Total Cost Comparisons", "text": "We have compared the expected total costs. (...) We have to deal with the other countries. (...) We have to deal with other countries. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" We have the same problems. \"(...)\" (...) \"We have the same problems.\" (...) \"We.\" (...) \"We.\" (\"We.\" (\"We.\") \"(\" We. \"(\" We. \")\" (\"We.\") \"(\" We. \"(\" We. \")\" (\"We.\" (\"We.\") \"(\" We. \"\" We. \"(\" We. \")\" (\"We.\" We. \"(\" We. \")\" (\"We.\" We. \")\" (\"We.\" We. \"(\" We. \")\" (\"We.\" We. \"(\") \"We.\" (\"We.\") \"(\" We. \"We.\" (\")\" (\"We.\" We. \"(\") \"(\" We. \"We.\" (\")\" (\"We.\") \"(\" (\"We.\") \"(\" We. \"(\" We. \")\" (\"We.\" (\")\" (\"We.\" (\"We.\") \"(\" We. \")\" (\"We.\" (\"We.\") \"(\" We. \")\" (\"(\") \"We.\" (\"(\" We. \")\" (\"We.\") \"(\" We. \"(\" We. \")\" (\"We.\" (\"We.\") \"(\" We. \"(\" We. \")\" (\"We.\" We. \"(\" We. \")\" (\"We.\" We. \"(\" We. \")\" (\"We.\") \"(\" We. \"We.\" We. \"(\") \"(\" We."}, {"heading": "5. Related Work", "text": "Deciding on the theoretical value of information computations is a fundamental methodology for collecting information in general (Howard, 1966; Lindley, 1956).Influence charts, for example, are popular tools for displaying decisions and utility functions (Howard & Matheson, 1984).However, since the development of optimal genetic acquisition policies (i.e., the construction of the optimal decision tree) is generally unfeasible, most approaches to acquiring functionality are short-sighted (Dittmer & Jensen, 1997), greedily grasping a trait at a given time. Greedy approaches typically differ in the way the traits are evaluated, and iiii) the classification model is learned. Here, we review the differences between the different techniques in these three dimensions. Gaag and Wessels (1993) consider the problem of \"evidence\" using a Bayesian network, collecting evidence (i.e."}, {"heading": "6. Future Work", "text": "In this article, we have identified two types of constraints (features that render other features useless, and features that are useless without other features) purely on the basis of the underlying probability distribution. We have shown that these automatically discovered constraints have helped to drastically reduce the search space. In practice, it is possible to discover additional types of constraints that can potentially be used to further reduce the search space (e.g., restrictions on ordering certain procedures always precede other procedures). Restrictions can also be defined on the basis of observed characteristic values; for example, a treadmill test for patients in old age cannot be performed. Patients may reject certain procedures and medications. Adopting these restrictions by the domain experts and using them to further reduce the search space is a promising future direction. Most of the existing feature acquisition frameworks, including this one, are a significant simplification of what is happening in practice."}, {"heading": "7. Conclusion", "text": "We have described a general technique that can optimally restrict the search space by exploiting the conditional independence relationships between traits and class variables. We have demonstrated empirically that the use of conditional independence relationships can significantly reduce the number of possible subsets of traits. We have also introduced a new data structure called Value of Information Lattice (VOILA), which can both efficiently reduce the search space using conditional independence relationships and share probable conclusions between different subsets of traits. By using VOILA, we have been able to add to greedy acquisition policies a complete predictive capability that would otherwise be impracticable. We have experimentally demonstrated in five real medical datasets that the greedy strategy often stopped the acquisition of traits prematurely, performing worse than even a policy that acquires all traits."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their helpful and constructive feedback. This material is based on work supported by the National Science Foundation under grant number 0746930."}], "references": [{"title": "Learning diagnostic policies from examples by systematic search", "author": ["V. Bayer-Zubek"], "venue": "In Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Bayer.Zubek,? \\Q2004\\E", "shortCiteRegEx": "Bayer.Zubek", "year": 2004}, {"title": "VOILA: Efficient feature-value acquisition for classification", "author": ["M. Bilgic", "L. Getoor"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bilgic and Getoor,? \\Q2007\\E", "shortCiteRegEx": "Bilgic and Getoor", "year": 2007}, {"title": "Myopic value of information in influence diagrams", "author": ["S. Dittmer", "F. Jensen"], "venue": "In Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dittmer and Jensen,? \\Q1997\\E", "shortCiteRegEx": "Dittmer and Jensen", "year": 1997}, {"title": "UCI machine learning repository", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "Frank and Asuncion,? \\Q2010\\E", "shortCiteRegEx": "Frank and Asuncion", "year": 2010}, {"title": "Selective evidence gathering for diagnostic belief networks", "author": ["L. Gaag", "M. Wessels"], "venue": "AISB Quarterly,", "citeRegEx": "Gaag and Wessels,? \\Q1993\\E", "shortCiteRegEx": "Gaag and Wessels", "year": 1993}, {"title": "Optimization of control parameters for genetic algorithms", "author": ["J. Grefenstette"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Grefenstette,? \\Q1986\\E", "shortCiteRegEx": "Grefenstette", "year": 1986}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A.J. Grove", "D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Greiner et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Greiner et al\\.", "year": 2002}, {"title": "An approximate nonmyopic computation for value of information", "author": ["D. Heckerman", "E. Horvitz", "B. Middleton"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Heckerman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1993}, {"title": "Readings on the Principles and Applications of Decision Analysis, chap. Influence Diagrams", "author": ["R.A. Howard", "J.E. Matheson"], "venue": null, "citeRegEx": "Howard and Matheson,? \\Q1984\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Information value theory", "author": ["R.A. Howard"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Howard,? \\Q1966\\E", "shortCiteRegEx": "Howard", "year": 1966}, {"title": "Constructing optimal binary decision trees is NPComplete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Letters,", "citeRegEx": "Hyafil and Rivest,? \\Q1976\\E", "shortCiteRegEx": "Hyafil and Rivest", "year": 1976}, {"title": "On a measure of the information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Lindley,? \\Q1956\\E", "shortCiteRegEx": "Lindley", "year": 1956}, {"title": "The use of background knowledge in decision tree induction", "author": ["M. N\u00fa\u00f1ez"], "venue": "Machine Learning,", "citeRegEx": "N\u00fa\u00f1ez,? \\Q1991\\E", "shortCiteRegEx": "N\u00fa\u00f1ez", "year": 1991}, {"title": "Schedule of benefits: Physician services under the health insurance act", "author": ["O.M. Health"], "venue": null, "citeRegEx": "Health,? \\Q1992\\E", "shortCiteRegEx": "Health", "year": 1992}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "C4.5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "Enhancing automated test selection in probabilistic networks", "author": ["D. Sent", "L.C. Gaag"], "venue": "In Proceedings of the 11th conference on Artificial Intelligence in Medicine,", "citeRegEx": "Sent and Gaag,? \\Q2007\\E", "shortCiteRegEx": "Sent and Gaag", "year": 2007}, {"title": "CSL: A cost-sensitive learning system for sensing and grasping objects", "author": ["M. Tan"], "venue": "In IEEE International Conference on Robotics and Automation", "citeRegEx": "Tan,? \\Q1990\\E", "shortCiteRegEx": "Tan", "year": 1990}, {"title": "Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney,? \\Q1995\\E", "shortCiteRegEx": "Turney", "year": 1995}, {"title": "Test-cost sensitive classification on data with missing values", "author": ["Q. Yang", "C. Ling", "X. Chai", "R. Pan"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "Because devising the optimal policy is intractable in general, previous work has been greedy (Gaag & Wessels, 1993; Yang, Ling, Chai, & Pan, 2006), has approximated value of information calculations (Heckerman, Horvitz, & Middleton, 1993), and has developed heuristic feature scoring techniques (N\u00fa\u00f1ez, 1991; Turney, 1995).", "startOffset": 295, "endOffset": 322}, {"referenceID": 19, "context": "Because devising the optimal policy is intractable in general, previous work has been greedy (Gaag & Wessels, 1993; Yang, Ling, Chai, & Pan, 2006), has approximated value of information calculations (Heckerman, Horvitz, & Middleton, 1993), and has developed heuristic feature scoring techniques (N\u00fa\u00f1ez, 1991; Turney, 1995).", "startOffset": 295, "endOffset": 322}, {"referenceID": 19, "context": "\u2022 In addition to the feature acquisition costs defined by Turney (1995), we generate and experiment with synthetic feature costs.", "startOffset": 58, "endOffset": 72}, {"referenceID": 18, "context": ", Gaag & Wessels, 1993; Dittmer & Jensen, 1997) or have developed heuristic feature scoring techniques (e.g., N\u00fa\u00f1ez, 1991; Tan, 1990).", "startOffset": 103, "endOffset": 133}, {"referenceID": 9, "context": "Note that, the last two terms are equivalent to the definition of expected value of information, EVI, (Howard, 1966):", "startOffset": 102, "endOffset": 116}, {"referenceID": 14, "context": "Given a Bayesian network over X and Y , it is straightforward to check irreducibility through d-separation (Pearl, 1988).", "startOffset": 107, "endOffset": 120}, {"referenceID": 14, "context": "For each dataset, we first learned a Bayesian Network that both provides the joint probability distribution P (Y,X) and efficiently answers conditional independence queries thorough d-separation (Pearl, 1988).", "startOffset": 195, "endOffset": 208}, {"referenceID": 18, "context": "We experimented with five real-world medical datasets that Turney (1995) described and used in his paper.", "startOffset": 59, "endOffset": 73}, {"referenceID": 14, "context": "The Market Blanket of Y in a Bayesian network is defined as Y \u2019s parents, children, and its children\u2019s other parents (Pearl, 1988).", "startOffset": 117, "endOffset": 130}, {"referenceID": 18, "context": "The feature costs for each dataset are described in detail by Turney (1995). In summary, each feature can either have an independent cost, or can belong to a group of features, where the first feature in that group incurs an additional cost.", "startOffset": 62, "endOffset": 76}, {"referenceID": 13, "context": "The feature costs are based on the data from Ontario Ministry of Health (1992).", "startOffset": 65, "endOffset": 79}, {"referenceID": 19, "context": "The misclassification costs were not defined in the paper by Turney (1995). One reason could be that it is easier to define the feature costs, but defining the cost of a misclassification can be non-trivial.", "startOffset": 61, "endOffset": 75}, {"referenceID": 9, "context": "Decision theoretic value of information calculations provide a principled methodology for information gathering in general (Howard, 1966; Lindley, 1956).", "startOffset": 123, "endOffset": 152}, {"referenceID": 11, "context": "Decision theoretic value of information calculations provide a principled methodology for information gathering in general (Howard, 1966; Lindley, 1956).", "startOffset": 123, "endOffset": 152}, {"referenceID": 4, "context": "Gaag and Wessels (1993) consider the problem of \u201cevidence\u201d gathering for diagnosis using a Bayesian Network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 17, "context": "In more recent work, Sent and Gaag (2007) consider the problem of acquiring more than a single feature at each step.", "startOffset": 21, "endOffset": 42}, {"referenceID": 15, "context": "Similarly, Tan (1990) modifies the ID3 algorithm (Quinlan, 1986) to account for feature costs.", "startOffset": 49, "endOffset": 64}, {"referenceID": 5, "context": "Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using N\u00fa\u00f1ez\u2019s (1991) criteria to build C4.", "startOffset": 145, "endOffset": 165}, {"referenceID": 16, "context": "5 decision trees (Quinlan, 1993).", "startOffset": 17, "endOffset": 32}, {"referenceID": 10, "context": "N\u00fa\u00f1ez (1991) introduces a decision tree algorithm called EG2 that is sensitive to the feature costs.", "startOffset": 0, "endOffset": 13}, {"referenceID": 10, "context": "N\u00fa\u00f1ez (1991) introduces a decision tree algorithm called EG2 that is sensitive to the feature costs. Rather than splitting the decision tree at a feature that has high information gain, EG2 chooses a feature that has least \u201cinformation cost function,\u201d which is defined as the ratio of a feature\u2019s cost to its discriminative efficiency. EG2 is, however, is not directly optimized to balance the misclassification cost and feature acquisition cost; rather it is optimized for 0/1 loss while taking the feature costs into account. Similarly, Tan (1990) modifies the ID3 algorithm (Quinlan, 1986) to account for feature costs.", "startOffset": 0, "endOffset": 550}, {"referenceID": 10, "context": "N\u00fa\u00f1ez (1991) introduces a decision tree algorithm called EG2 that is sensitive to the feature costs. Rather than splitting the decision tree at a feature that has high information gain, EG2 chooses a feature that has least \u201cinformation cost function,\u201d which is defined as the ratio of a feature\u2019s cost to its discriminative efficiency. EG2 is, however, is not directly optimized to balance the misclassification cost and feature acquisition cost; rather it is optimized for 0/1 loss while taking the feature costs into account. Similarly, Tan (1990) modifies the ID3 algorithm (Quinlan, 1986) to account for feature costs. Tan considers the domain where a robot needs to sense, recognize, and act, and the number of features is very large. For the robot to act efficiently, it needs to trade-off accuracy for efficiency. Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using N\u00fa\u00f1ez\u2019s (1991) criteria to build C4.", "startOffset": 0, "endOffset": 835}, {"referenceID": 4, "context": "Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using N\u00fa\u00f1ez\u2019s (1991) criteria to build C4.", "startOffset": 146, "endOffset": 191}, {"referenceID": 4, "context": "Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using N\u00fa\u00f1ez\u2019s (1991) criteria to build C4.5 decision trees (Quinlan, 1993). Unlike N\u00fa\u00f1ez, Turney takes misclassification costs into account (in addition to the feature costs) to evaluate a given decision tree and looks for a good decision tree using genetic search algorithms. Yang et al. (2006) build cost-sensitive decision trees and Naive Bayes classifiers that take both feature costs and misclassification costs into account.", "startOffset": 146, "endOffset": 466}, {"referenceID": 4, "context": "Turney (1995) builds a decision tree called ICET (standing for Inexpensive Classification with Expensive Tests) using a genetic search algorithm (Grefenstette, 1986) and using N\u00fa\u00f1ez\u2019s (1991) criteria to build C4.5 decision trees (Quinlan, 1993). Unlike N\u00fa\u00f1ez, Turney takes misclassification costs into account (in addition to the feature costs) to evaluate a given decision tree and looks for a good decision tree using genetic search algorithms. Yang et al. (2006) build cost-sensitive decision trees and Naive Bayes classifiers that take both feature costs and misclassification costs into account. Unlike N\u00fa\u00f1ez (1991), who scores features based on information gain and cost ratio, Yang et al.", "startOffset": 146, "endOffset": 621}, {"referenceID": 0, "context": "Bayer-Zubek (2004) formulates the feature acquisition problem as a Markov Decision Process and provides both greedy and systematic search algorithms to develop diagnostic policies.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Bayer-Zubek (2004) formulates the feature acquisition problem as a Markov Decision Process and provides both greedy and systematic search algorithms to develop diagnostic policies. Bayer-Zubek takes both feature cost and misclassification costs into account and automatically finds an acquisition plan that balances the two costs. She introduces an admissible heuristic for AO* search and describes regularization techniques to reduce overfitting to the training data. Saar-Tsechansky, Melville, and Provost (2009) consider active feature acquisition for classifier induction.", "startOffset": 0, "endOffset": 515}, {"referenceID": 0, "context": "Bayer-Zubek (2004) formulates the feature acquisition problem as a Markov Decision Process and provides both greedy and systematic search algorithms to develop diagnostic policies. Bayer-Zubek takes both feature cost and misclassification costs into account and automatically finds an acquisition plan that balances the two costs. She introduces an admissible heuristic for AO* search and describes regularization techniques to reduce overfitting to the training data. Saar-Tsechansky, Melville, and Provost (2009) consider active feature acquisition for classifier induction. Specifically, they are given a training data with missing feature values, and a cost matrix that defines the cost of acquiring each feature value, they describe an incremental algorithm that can select the best feature to acquire iteratively to build a model that is expected to have high future performance. The utility of acquiring a feature is estimated in terms of expected performance improvement per unit cost. The two characteristics that make this work different from most of the previous work is that i) the authors do not assume a fixed budget a priori; rather they build the model incrementally, ii) each feature can have a different cost for each instance. Finally, Greiner, Grove, and Roth (2002) analyze the sample complexity of dynamic programming algorithms that performs value iteration to search for the best diagnostic policies.", "startOffset": 0, "endOffset": 1287}], "year": 2011, "abstractText": "We address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. Because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. We describe the Value of Information Lattice (VOILA), an optimal and efficient feature subset acquisition framework. Unlike the common practice, which is to acquire features greedily, VOILA can reason with subsets of features. VOILA efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. Through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination.", "creator": "TeX"}}}