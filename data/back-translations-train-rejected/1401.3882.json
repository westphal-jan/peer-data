{"id": "1401.3882", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Probabilistic Relational Planning with First Order Decision Diagrams", "abstract": "Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision diagrams, to capture domain dynamics and value functions. Work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes. Recent work introduced a first order variant of decision diagrams (FODD) and developed a value iteration algorithm for this representation. This paper develops several improvements to the FODD algorithm that make the approach practical. These include, new reduction operators that decrease the size of the representation, several speedup techniques, and techniques for value approximation. Incorporating these, the paper presents a planning system, FODD-Planner, for solving relational stochastic planning problems. The system is evaluated on several domains, including problems from the recent international planning competition, and shows competitive performance with top ranking systems. This is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning.", "histories": [["v1", "Thu, 16 Jan 2014 05:13:02 GMT  (362kb)", "http://arxiv.org/abs/1401.3882v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["saket joshi", "roni khardon"], "accepted": false, "id": "1401.3882"}, "pdf": {"name": "1401.3882.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Relational Planning with First Order Decision Diagrams", "authors": ["Saket Joshi", "Roni Khardon"], "emails": ["joshi@eecs.oregonstate.edu", "roni@cs.tufts.edu"], "sections": [{"heading": null, "text": "Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, particularly algebraic decision diagrams, to capture domain dynamics and value functions. Working on symbolic dynamic programming brought these ideas to first-order logic using multiple representation schemes. Recent work introduced a first-order variant of decision diagrams (FODD) and developed a value repetition algorithm for this representation. In this paper, several enhancements to the FODD algorithm are developed that make the approach practicable, including new reduction operators that reduce the size of the representation, several specidup techniques and value approximation techniques. Taking these factors into account, the paper presents a planning system, FODD Planner, for solving relational stochastic planning problems. The system is evaluated in several areas, including problems from the recent international planning competition, and demonstrates a competitive performance with top-tier anchoring systems, making this the first demonstration of feasibility."}, {"heading": "1. Introduction", "text": "The STRIPS Planning System (Fikes & Nilsson, 1971) spearheaded a generation of automated planning research, which produced a series of successful deterministic planning systems using various paradigms such as Partial Order Planning (Penberthy & Weld, 1992), Planning based on Planning Graphs (Blum & Furst, 1997), Planning by Satisfaction (Kautz & Selman, 1996) and heuristic Search (Bonet & Geffner, 2001), which were later used in solving the planning problem using uncertainty (Blum & Langford, 1998), Anderson, & Smith, 1998; Majercik & Littman, 2003; Yoon, Fern & Givan, 2007; TeichteilKoenigsbuch, Infantes & Kuter, 2008)."}, {"heading": "2. Preliminaries", "text": "This section provides an overview of Relational MDPs, first order decision diagrams, and the symbolic dynamic programming algorithm."}, {"heading": "2.1 Relational Markov Decision Processes", "text": "A Markov Decision Process (MDP) is a mathematical model of the interaction between an agent and his environment (Puterman, 1994). Formally, an MDP is a 5-tuple < S, A, T, R, \u03b3 > definition \u2022 A set of fully observable states S. \u2022 A set of actions available to the agent. \u2022 A transitional state function T that defines the probability that the reward is independent of a particular state (s, a). However, the general case can be handled in the same way. \u2022 A discount factor 0 that defines the relative value of immediate actions over future actions. <"}, {"heading": "2.2 First Order Decision Diagrams", "text": "This section briefly describes the previous work on FODDs and their use for relational MDPs (Wang et al., 2008). We use standard terms from First Order logic (Lloyd, 1987). A First Order Decision Diagram is a labeled directed acyclic graph, where each non-leade node has just 2 outgoing edges with true and false labels. Non-leaf nodes is also labeled with atoms that are generated from a predetermined signature of predicates, constants, and an enumerable set of variables. Leaf nodes has non-negative numerical values. The signature also defines a total order on atoms, and the FODD is ordered according to this order with each parent smaller than the child. Two examples of FODDs are given in Figure 1; in these and all the diagrams in the papers, the true branches and right edges are represented."}, {"heading": "2.3 VI with FODDs", "text": "In previous work (Wang et al., 2008) we have shown how to capture the reward and value functions directly with FODDs. However, the dynamics of the domains are captured by FODDs which describe the probabilities of the action variants used by Boutilier et al. (2001) prob (Aj (~ a) and by special FODDs, Truth Value Diagrams (TVD) which capture the deterministic effects of each action variant, similar to the successor axioms used by Boutilier et al. (2001). For each action variant Aj (~ a) and each predicate scheme p (~ x) of the TVD T (A), p (~ x))) is a FODD with {0, 1} leaves. TVD indicates the truth value of p (~ x) in the next state when A (~ a) has been performed. Therefore, the TVD structures are sufficient to capture the above described actionisms."}, {"heading": "3. Speedup Techniques", "text": "In this section, two techniques are presented to accelerate the VI algorithm of Wang et al. (2008) while maintaining an exact solution."}, {"heading": "3.1 Subtracting Apart - Improving Applicability of R7", "text": "The applicability of R7 can be increased if certain branches have variables that are standardized apart from a way that maintains the rating of FODD below the maximum aggregation semantics. Consider FODD B in Figure 3. Intuitively, a weak reduction is applicable to this chart based on the following argument. Consider an evaluation criterion for both (and any value for y) that traverses edge e2 under any interpretation. Then, I can | = B \u2192 \u00ac p (1) p (2). Therefore, there must be an evaluation: p = {x\\ 2, z\\ 3} (and any value for y) that traverses edge e1. Now, depending on the truth value of I | = B \u2192 q (2), we have four possibilities where, after passing the nodes, we would achieve the goal (e2) or goal (e1)."}, {"heading": "3.2 Not Standardizing Apart", "text": "Remember that the FODD-based VI algorithm must add functions represented by FODDs (in steps 2 and 4) and take the maximum over functions represented by FODDs (in step 4). Since the individual functions are independent functions of the state, the variables of different functions are not related to each other. Therefore, before the various diagrams are added or maximized, the algorithm of Wang et al. (2008) separates the diagrams, that is, all the variables in the diagrams are given new names so that they do not restrict each other. On the other hand, since the different diagrams are structurally related, this often introduces redundancies (in the form of renamed copies of the same atoms) that need to be removed by reduction operators. However, our reduction operators are not ideal and avoiding this step can lead to significant accelerations in the system."}, {"heading": "4. Additional Reduction Operators", "text": "In this section we present two new reduction operators that improve the efficiency of the VI algorithm. The following definitions are important in developing these reductions and in understanding potential margins for reduction diagrams. Definition 1 A descending path order (DPO) is an ordered list of all paths from the root to a leaf in FODD B, sorted in descending order according to the value of the leaf reached by the path. The relative sequence of paths reaching the same leaf can be specified at will. Definition 2 If B is a FODD, and P is the DPO for B, then a path is pj, then P is instrumental with respect to P iff. There is an interpretation I and evaluation so that PathB (I) = pj, and2."}, {"heading": "4.1 The R10 Reduction", "text": "This year it is more than ever before."}, {"heading": "4.2 The R11 Reduction", "text": "This happens, of course, without the artificial background knowledge being used for our example, but the corresponding diagrams are too large to be included in the text. The main reason for such redundancies is that they are often discussed in rates of value titeration algorithms. (This, of course, happens without the artificial background knowledge being used for our example, but the corresponding diagrams are too large to be included in the text.) The main reason for such redundancies is that they are often included in rates of value titeration algorithms in rates of value titeration algorithms. (This happens, of course, without the artificial background knowledge being used for our example, but the corresponding diagrams are too large to be included in the text.) The main reason for such redundancies is often discussed in rates of value titeration algorithms. (This happens, of course, without the artificial background knowledge being used for our example, but the corresponding diagrams are too large to be included."}, {"heading": "5. FODD-Planner", "text": "In this section we will discuss the system FODD-Planner, which implements the VI algorithm with FODDs. FODD-Planner uses a number of approximation techniques, which result in further acceleration, and also implements enhancements to the basic VI algorithm, which allow it to handle action costs and universal goals. In the following sections, these details are described."}, {"heading": "5.1 Value Approximation", "text": "Reductions help to keep the diagrams small by eliminating redundancies, but if the actual n-step-to-step function itself is large, legal reductions cannot help. There are areas where the true value function is unlimited, for example in the tire world in international planning competition, where the goal is always to get the vehicle to a destination city, there may be a chain of cities that are connected to each other to the destination. That chain can be any length, so if the value function is represented by government abstraction, it must be unlimited. Consequently, SDP-like algorithms are less effective in areas where the dynamics lead to such a transitive structure and each iteration of value repetition increases the size of the n-step-to-value function (Kersting et al., 2004; Sanner & Boutilier, 2009). In other cases, the value function is not infinite, but simply too large to be manipulated efficiently."}, {"heading": "5.1.1 Not Standardizing Apart Action Variants", "text": "The standardization of the diagrams of the action variants prior to their addition is necessary for the accuracy of the FODD-based VI algorithm, that is, if we do not standardize the action variant diagrams apart prior to their addition, the value given to some states may be lower than the true value (Wang et al., 2008). Intuitively, this is true because different paths in the value function variants divide atoms and variables. Now, in a fixed action, the best value for binding the variables and the corresponding value for the different action variants may be different. Thus, if the variables are forced to be the same for the variants, we exclude practicable value combinations. On the other hand, the value obtained if we do not standardize apart is a lower limit for the true value. This is because each path in the diagram resulting from the non-standardization of the variables is present in the diagram resulting from the standardization of the value, although the standardization of the value cannot be considered to be exactly separated, and therefore the method cannot be considered to be separated."}, {"heading": "5.1.2 Merging Leaves", "text": "The use of FODDs also allows us to approach the value function in a simple and controlled way. Here, we are following the approximation techniques of APRICODD (St-Aubin et al., 2000), where they were used for proposition problems. The idea is to reduce the size of the chart by merging substructures with similar values. One way to do this is to reduce the precision of the sheet values. That is, at a certain precision value, we join sheets whose value is within it. This, in turn, leads to a reduction in the chart because sub-sections of the chart that previously pointed to different sheets now point to the same sheet. However, the granularity of the approximation becomes an additional parameter for the system and must be carefully selected. Details can be found in the following experiments."}, {"heading": "5.1.3 Domain Determinization", "text": "Previous work on stochastic planning has found that in some areas, good results can be achieved by pretending that the domain is deterministic and rescheduled when unexpected results are achieved (Yoon et al., 2007). Here, we use a similar idea and determine the domain in the process of policy generation, saving considerable computing power and avoiding the typical increase in value function that occurs in step 2 of the VI algorithm. Domains can be determined in many ways. We opt for determination by replacing each stochastic action with its most likely deterministic alternative, which occurs only once before executing VI. Although this method of determination is suboptimal for many domains, it is useful for areas where the most likely outcome is equivalent to the successful execution of an action (Little & Thibaux, 2007), as is the case in those areas where we have experimented."}, {"heading": "5.2 Extensions of the VI Algorithm", "text": "FODD-Planner extends the basic algorithm by two extensions. This enables the handling of action costs, arbitrary subjunctive and universal objectives."}, {"heading": "5.2.1 Handling Action Costs", "text": "To avoid this difficulty, we point out that action costs can be supported as long as there is at least one zero-cost action. To see this, remember the VI algorithm. The appropriate place to add action costs is just before the object maximization step. However, since this step is followed by the maximization of action plans, if at least one action has 0 costs (if not, we can create a no-op action), the resulting chart will never have negative hands after maximization. Therefore, we safely convert negative hands to 0 before the maximization step, avoiding conflicts with reduction procedures."}, {"heading": "5.2.2 Handling Universal Goals", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6. Experimental Results", "text": "The probabilistic track of the IPC provides domain descriptions in the language of the PPDDL (Younes, Littman, Weissman, & Asmuth, 2005).3 All experiments were conducted on a Linux machine with an Intel Pentium D processor with 3 GHz and 2 GB of memory. In accordance with the IPC standards, all timings, rewards, and plan lengths we report are averages over 30 laps. For each domain, we manually construct background knowledge that limits arguments of predicates (e.g., a box can only be in one city at a time so that Bin (b, c1), Bin (b, c2) \u2192 (c1 = c2)."}, {"heading": "6.1 Merits of Reduction Operators", "text": "The following subsections show our performance in solving the problems of IPC. Before discussing them, we examine and illustrate the merits of the various reduction operators in terms of their impact on offline planning time. Experiments are conducted on the tire world and Boxworld domains, which are described in more detail below. For this section, it is sufficient to consider the domains as typical cases that we address in solving planning problems and focus on the differences between the reductions. In the first series of experiments, we compare runtime with R7 and R10 in the context of other reductions. Since R10 and R7 are the edge removal of reductions, R7-drop is used in conjunction with both, we compare R7 to R7 - directly replace the configurations of R9 and R11."}, {"heading": "6.2 The Logistics Benchmark Problem", "text": "This is the boxworld problem introduced by Boutilier et al. (2001), which has been used as a standard example of exact solution methods for relational MDPs. The domain consists of crates, cities and trucks, the goal being to bring certain crates to specific cities by loading, unloading and driving. In the benchmark problem, the goal is the existence of a crate in Paris. Loading and unloading actions are likely and the probability of success of the unloading depends on whether it rains or not. In this area, all cities are reachable from each other. As a result, the domain has a compact abstract optimal value function. Note that for this challenge domain there are no concrete planning cases to solve. Instead, the goal is to solve the offline problem and efficiently produce the (abstract) optimal solution. The domain description shows 3 predicates of difficulty 2 and 3 actions, each with 2 arguments."}, {"heading": "6.3 The Fileworld Domain", "text": "This domain was part of the probabilistic track of IPC-4 (2004) (information about the contests is available at http: / / ipc.icaps-conference.org /).The domain consists of files and folders. Each file is randomly assigned to a folder at execution time and the goal is to place each file in the folder assigned to it. There are costs of 100 for editing a folder and costs of 1 for placing a file in a folder. The optimal procedure for this domain is to first get the assignment of files to folders and then edit each folder once and all the files assigned to it. The domain description has 8 predictions of activity from 0 to 2 and 16 actions with 0 to 1 argument.The results were published for a problem instance consisting of thirty files and five folders. As the goal is conjunctive, we used the aforementioned addictive target splitting."}, {"heading": "6.4 The Tireworld Domain", "text": "This domain was part of the probabilistic track of IPC-5 (2006).The domain consists of a network of locations (or cities).A vehicle starts from a city and moves from city to city with the goal of reaching a destination city.Removals can only be made between cities that are directly connected by a road.In addition, the vehicle may lose a tire with a 40% probability of each move. Some cities have a spare tire that can be loaded onto the vehicle. If the vehicle contains a replacement tire, the plate tire can be replaced with a 50% probability of success.This domain is simple but not trivial due to the possibility of complex network topology and high probability of failure.The IPC description of this domain has 5 predicates of activity from 0 to 2 and 3 actions with a probability of 0 to 2 arguments.Participants in IPC-5 competed over 15 problem cases on this domain with varying degrees of difficulty."}, {"heading": "6.5 Value Approximation by Merging Leaves", "text": "Although the tire world domain as described above can be resolved within the IPC timeframe, one could wish for an even faster execution. As we will show next, the heuristics of blade fusion provide such a tool that potentially replaces the quality of the cover and the plan length with faster planning and execution times. Table 2 shows the average reduction in planning time, coverage and planning length achieved when using the approximate blade fusion. The highest reward achieved in a state is 500. We experimented with reducing the precision of the blades with values between 50.0 and 150.0. As the results show, the system can gain a certain loss in coverage and planning length in terms of execution time and planning time. For example, at a blade precision of 50.0 (10% of the total value) we get a 95.53% reduction in planning time (22-fold acceleration), but we lose 15.29% of coverage."}, {"heading": "6.6 Boxworld", "text": "The goal is to turn the boxes from the source to the target; the only likely course of action is to make them work as expected (and lead the problems from the source city to the target)."}, {"heading": "7. Related Work", "text": "The Introduction briefly reviews previous work on MDPs, propositional MDPs and RMDPs that focus on the work directly related to the ideas used in this paper. There have been several other solution formalities for RMDPs that combine dynamic programming with other ideas to achieve successful systems, including approaches that combine dynamic programming with linear function approximation (Sanner & Boutilier, 2009), forward search (Ho \ufffd lldobler et al). Other work does not directly use dynamic programming. For example, Guestrin, Gearhart and Kanodia (2003a) present an approach with additive value functions and employ linear programming to solve the RMDP."}, {"heading": "8. Conclusion and Future Work", "text": "The most important contribution of this paper is the introduction of the FODD Planner, a relational planning system based on first-order decision diagrams. This is the first planning system to use algebraic decision diagrams as a representation language and successfully solve planning problems from the international planning competition. FODD Planner offers several improvements over previous work on FODDs (Wang et al., 2008), including the reduction operators R10, R11 the sub-apart operator and several acceleration and value approximation techniques. Taken together, these improvements result in significant accelerations that make the approach more workable. Therefore, the results show that abstraction through compact representation is a promising approach to stochastic planning.Our work raises many questions about the fundamentals of FODDs and their application to solving RMDPs. The first is the question of reductions. Our package of measures of reductions is still heuristic and does not guarantee a canonical form of diagrams that is instrumental to the efficiency of the process."}, {"heading": "Acknowledgments", "text": "Saket Joshi was also supported by a Computing Innovation Postdoctoral Fellowship. Some of the experiments reported in this paper were conducted in the Tufts Linux Research Cluster, which is supported by Tufts UIT Research Computing. We thank Kristian Kersting for valuable input into the system and insightful discussions."}], "references": [{"title": "Algebraic decision diagrams and their applications", "author": ["R. Bahar", "E. Frohm", "C. Gaona", "G. Hachtel", "E. Macii", "A. Pardo", "F. Somenzi"], "venue": "In IEEE /ACM ICCAD,", "citeRegEx": "Bahar et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bahar et al\\.", "year": 1993}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Fast planning through planning graph analysis", "author": ["A. Blum", "M. Furst"], "venue": "Artificial Intelligence,", "citeRegEx": "Blum and Furst,? \\Q1997\\E", "shortCiteRegEx": "Blum and Furst", "year": 1997}, {"title": "Probabilistic planning in the graphplan framework", "author": ["A. Blum", "J. Langford"], "venue": "In Proceedings of the Fifth European Conference on Planning,", "citeRegEx": "Blum and Langford,? \\Q1998\\E", "shortCiteRegEx": "Blum and Langford", "year": 1998}, {"title": "Planning as heuristic search", "author": ["B. Bonet", "H. Geffner"], "venue": "Artificial Intelligence,", "citeRegEx": "Bonet and Geffner,? \\Q2001\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2001}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Symbolic dynamic programming for First-Order MDPs", "author": ["C. Boutilier", "R. Reiter", "B. Price"], "venue": "In Proceedings of the International Joint Conference of Artificial Intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2001}, {"title": "Online learning and exploiting relational models in reinforcement learning", "author": ["T. Croonenborghs", "J. Ramon", "H. Blockeel", "M. Bruynooghe"], "venue": "In Proceedings of the International Joint Conference of Artificial Intelligence,", "citeRegEx": "Croonenborghs et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Croonenborghs et al\\.", "year": 2007}, {"title": "Integrating guidance into relational reinforcement learning", "author": ["K. Driessens", "S. Dzeroski"], "venue": "Machine Learning,", "citeRegEx": "Driessens and Dzeroski,? \\Q2004\\E", "shortCiteRegEx": "Driessens and Dzeroski", "year": 2004}, {"title": "Relational reinforcement learning", "author": ["S. Dzeroski", "L. De Raedt", "K. Driessens"], "venue": "Machine Learning,", "citeRegEx": "Dzeroski et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dzeroski et al\\.", "year": 2001}, {"title": "Approximate policy iteration with a policy language bias", "author": ["A. Fern", "S. Yoon", "R. Givan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fern et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fern et al\\.", "year": 2006}, {"title": "STRIPS: A new approach to the application of theorem proving to problem solving", "author": ["R. Fikes", "N. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "Envelope-based planning in relational MDPs", "author": ["N. Gardiol", "L. Kaelbling"], "venue": "In Proceedings of the International Conference on Neural Information Processing Systems,", "citeRegEx": "Gardiol and Kaelbling,? \\Q2003\\E", "shortCiteRegEx": "Gardiol and Kaelbling", "year": 2003}, {"title": "Exploiting First-Order regression in inductive policy selection", "author": ["C. Gretton", "S. Thiebaux"], "venue": "In Proceedings of the Workshop on Uncertainty in Artificial Intelligence", "citeRegEx": "Gretton and Thiebaux,? \\Q2004\\E", "shortCiteRegEx": "Gretton and Thiebaux", "year": 2004}, {"title": "Binary decision diagrams for First-Order predicate logic", "author": ["J. Groote", "O. Tveretina"], "venue": "Journal of Logic and Algebraic Programming,", "citeRegEx": "Groote and Tveretina,? \\Q2003\\E", "shortCiteRegEx": "Groote and Tveretina", "year": 2003}, {"title": "Generalizing plans to new environments in relational MDPs", "author": ["C. Guestrin", "D. Koller", "C. Gearhart", "N. Kanodia"], "venue": "In Proceedings of the International Joint Conference of Artificial Intelligence,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "SPUDD: Stochastic planning using decision diagrams", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": "In Proceedings of the Workshop on Uncertainty in Artificial Intelligence,", "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "FluCaP: a heuristic search planner for First-Order MDPs", "author": ["S. H\u00f6lldobler", "E. Karabaev", "O. Skvortsova"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "H\u00f6lldobler et al\\.,? \\Q2006\\E", "shortCiteRegEx": "H\u00f6lldobler et al\\.", "year": 2006}, {"title": "Dynamic Programming and Markov Processes", "author": ["R. Howard"], "venue": null, "citeRegEx": "Howard,? \\Q1960\\E", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Generalized First-Order decision diagrams for First-Order Markov decision processes", "author": ["S. Joshi", "K. Kersting", "R. Khardon"], "venue": "In Proceedings of the International Joint Conference of Artificial Intelligence,", "citeRegEx": "Joshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2009}, {"title": "Self-Taught decision theoretic planning with First-Order decision diagrams", "author": ["S. Joshi", "K. Kersting", "R. Khardon"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling,", "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Pushing the envelope: Planning, propositional logic, and stochastic search", "author": ["H. Kautz", "B. Selman"], "venue": "In Proceedings of the National Conference of the American Association for Artificial Intelligence,", "citeRegEx": "Kautz and Selman,? \\Q1996\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1996}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["M. Kearns", "D. Koller"], "venue": "In Proceedings of the International Joint Conference of Artificial Intelligence,", "citeRegEx": "Kearns and Koller,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Koller", "year": 1999}, {"title": "Logical Markov decision programs and the convergence of logical TD(\u03bb)", "author": ["K. Kersting", "L. De Raedt"], "venue": "In Proceedings of Inductive Logic Programming,", "citeRegEx": "Kersting and Raedt,? \\Q2004\\E", "shortCiteRegEx": "Kersting and Raedt", "year": 2004}, {"title": "Learning function free Horn expressions", "author": ["R. Khardon"], "venue": "Machine Learning,", "citeRegEx": "Khardon,? \\Q1999\\E", "shortCiteRegEx": "Khardon", "year": 1999}, {"title": "Probabilistic planning vs. replanning", "author": ["I. Little", "S. Thibaux"], "venue": "In Proceedings of the ICAPS Workshop on IPC: Past, Present and Future", "citeRegEx": "Little and Thibaux,? \\Q2007\\E", "shortCiteRegEx": "Little and Thibaux", "year": 2007}, {"title": "Foundations of Logic Programming", "author": ["J. Lloyd"], "venue": null, "citeRegEx": "Lloyd,? \\Q1987\\E", "shortCiteRegEx": "Lloyd", "year": 1987}, {"title": "Contingent planning under uncertainty via stochastic satisfiability", "author": ["S. Majercik", "M. Littman"], "venue": "Artificial Intelligence,", "citeRegEx": "Majercik and Littman,? \\Q2003\\E", "shortCiteRegEx": "Majercik and Littman", "year": 2003}, {"title": "Solving relational MDPs with First-Order machine learning", "author": ["Mausam", "D. Weld"], "venue": "In Proceedings of the ICAPS Workshop on Planning under Uncertainty and Incomplete Information", "citeRegEx": "Mausam and Weld,? \\Q2003\\E", "shortCiteRegEx": "Mausam and Weld", "year": 2003}, {"title": "UCPOP: A sound, complete, partial order planner for ADL", "author": ["J. Penberthy", "D. Weld"], "venue": "In Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Penberthy and Weld,? \\Q1992\\E", "shortCiteRegEx": "Penberthy and Weld", "year": 1992}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Practical solution techniques for First-Order MDPs", "author": ["S. Sanner", "C. Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "Sanner and Boutilier,? \\Q2009\\E", "shortCiteRegEx": "Sanner and Boutilier", "year": 2009}, {"title": "APRICODD: Approximate policy construction using decision diagrams", "author": ["R. St-Aubin", "J. Hoey", "C. Boutilier"], "venue": "In Proceedings of the International Conference on Neural Information Processing Systems,", "citeRegEx": "St.Aubin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "St.Aubin et al\\.", "year": 2000}, {"title": "Relational reinforcement learning: An overview", "author": ["P. Tadepalli", "R. Givan", "K. Driessens"], "venue": "In Proceedings of the International Conference on Machine Learning \u201904 Workshop on Relational Reinforcement Learning", "citeRegEx": "Tadepalli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tadepalli et al\\.", "year": 2004}, {"title": "RFF: A robust FF-based MDP planning algorithm for generating policies with low probability of failure", "author": ["F. Teichteil-Koenigsbuch", "G. Infantes", "U. Kuter"], "venue": null, "citeRegEx": "Teichteil.Koenigsbuch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teichteil.Koenigsbuch et al\\.", "year": 2008}, {"title": "The logic of Adaptive behavior: Knowledge representation and algorithms for adaptive sequential decision making under uncertainty in First-Order and relational domains", "author": ["M. van Otterlo"], "venue": null, "citeRegEx": "Otterlo,? \\Q2008\\E", "shortCiteRegEx": "Otterlo", "year": 2008}, {"title": "Building relational world models for reinforcement learning", "author": ["T. Walker", "L. Torrey", "J. Shavlik", "R. Maclin"], "venue": "In Proceedings of Inductive Logic Programming,", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "First-Order Markov decision processes", "author": ["C. Wang"], "venue": "Ph.D. thesis,", "citeRegEx": "Wang,? \\Q2007\\E", "shortCiteRegEx": "Wang", "year": 2007}, {"title": "First-Order decision diagrams for relational MDPs", "author": ["C. Wang", "S. Joshi", "R. Khardon"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Policy iteration for relational MDPs", "author": ["C. Wang", "R. Khardon"], "venue": "In Proceedings of the Workshop on Uncertainty in Artificial Intelligence,", "citeRegEx": "Wang and Khardon,? \\Q2007\\E", "shortCiteRegEx": "Wang and Khardon", "year": 2007}, {"title": "Extending graphplan to handle uncertainty and sensing actions", "author": ["D. Weld", "C. Anderson", "D. Smith"], "venue": "In Proceedings of the National Conference on Artificial Intelligence", "citeRegEx": "Weld et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Weld et al\\.", "year": 1998}, {"title": "FF-Replan: A baseline for probabilistic planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling,", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}, {"title": "The first probabilistic track of the international planning competition", "author": ["H. Younes", "M. Littman", "D. Weissman", "J. Asmuth"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Younes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Younes et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 43, "context": "Of these, approaches using forward heuristic search related to the planning graph (Blum & Furst, 1997) have been very successful at the recent international planning competitions (Yoon et al., 2007; Teichteil-Koenigsbuch et al., 2008).", "startOffset": 179, "endOffset": 234}, {"referenceID": 36, "context": "Of these, approaches using forward heuristic search related to the planning graph (Blum & Furst, 1997) have been very successful at the recent international planning competitions (Yoon et al., 2007; Teichteil-Koenigsbuch et al., 2008).", "startOffset": 179, "endOffset": 234}, {"referenceID": 1, "context": "Classical solution techniques for MDPs, like value iteration (VI) (Bellman, 1957) and policy iteration (PI) (Howard, 1960), are based on dynamic programming.", "startOffset": 66, "endOffset": 81}, {"referenceID": 20, "context": "Classical solution techniques for MDPs, like value iteration (VI) (Bellman, 1957) and policy iteration (PI) (Howard, 1960), are based on dynamic programming.", "startOffset": 108, "endOffset": 122}, {"referenceID": 1, "context": "Owing to the curse of dimensionality (Bellman, 1957), even for reasonably small problems, the state space can be very large.", "startOffset": 37, "endOffset": 52}, {"referenceID": 18, "context": "One of the most successful, SPUDD (Hoey et al., 1999), demonstrated that if the MDP can be represented using algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993), then VI can be performed entirely using the ADD representation thereby avoiding the need to enumerate the state space.", "startOffset": 34, "endOffset": 53}, {"referenceID": 18, "context": "An alternative representation is motivated by the success of algebraic decision diagrams in solving propositional MDPs (Hoey et al., 1999; St-Aubin, Hoey, & Boutilier, 2000).", "startOffset": 119, "endOffset": 173}, {"referenceID": 40, "context": "Following this work, relational variants of decision diagrams have been defined and used for VI algorithms (Wang et al., 2008; Sanner & Boutilier, 2009).", "startOffset": 107, "endOffset": 152}, {"referenceID": 40, "context": "Our previous work (Wang et al., 2008) introduced First Order Decision Diagrams (FODD), and developed algorithms and reduction operators for them.", "startOffset": 18, "endOffset": 37}, {"referenceID": 1, "context": "Classical solution techniques for MDPs, like value iteration (VI) (Bellman, 1957) and policy iteration (PI) (Howard, 1960), are based on dynamic programming. These early solutions, however, require enumeration of the state space. Owing to the curse of dimensionality (Bellman, 1957), even for reasonably small problems, the state space can be very large. This can be seen easily for propositionally factored domains where the state is defined by N binary variables and the number of possible states is 2 . Several approaches were developed to handle such propositionally factored domains (Boutilier, Dearden, & Goldszmidt, 1999b; Kearns & Koller, 1999; Guestrin, Koller, Parr, & Venkataraman, 2003b; Hoey, St-Aubin, Hu, & Boutilier, 1999). One of the most successful, SPUDD (Hoey et al., 1999), demonstrated that if the MDP can be represented using algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993), then VI can be performed entirely using the ADD representation thereby avoiding the need to enumerate the state space. Propositionally factored representations show an impressive speedup by taking advantage of the propositional domain structure. However, they do not benefit from the structure that exists with objects and relations. Boutilier, Reiter, and Price (2001) developed the foundations for provably optimal solutions of relational problems and provided the Symbolic Dynamic Programming (SDP) algorithm in the context of situation calculus.", "startOffset": 67, "endOffset": 1317}, {"referenceID": 1, "context": "Classical solution techniques for MDPs, like value iteration (VI) (Bellman, 1957) and policy iteration (PI) (Howard, 1960), are based on dynamic programming. These early solutions, however, require enumeration of the state space. Owing to the curse of dimensionality (Bellman, 1957), even for reasonably small problems, the state space can be very large. This can be seen easily for propositionally factored domains where the state is defined by N binary variables and the number of possible states is 2 . Several approaches were developed to handle such propositionally factored domains (Boutilier, Dearden, & Goldszmidt, 1999b; Kearns & Koller, 1999; Guestrin, Koller, Parr, & Venkataraman, 2003b; Hoey, St-Aubin, Hu, & Boutilier, 1999). One of the most successful, SPUDD (Hoey et al., 1999), demonstrated that if the MDP can be represented using algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993), then VI can be performed entirely using the ADD representation thereby avoiding the need to enumerate the state space. Propositionally factored representations show an impressive speedup by taking advantage of the propositional domain structure. However, they do not benefit from the structure that exists with objects and relations. Boutilier, Reiter, and Price (2001) developed the foundations for provably optimal solutions of relational problems and provided the Symbolic Dynamic Programming (SDP) algorithm in the context of situation calculus. This algorithm provided a framework for dynamic programming solutions of Relational MDPs that was later employed in several formalisms and systems (Kersting, van Otterlo, & De Raedt, 2004; H\u00f6lldobler, Karabaev, & Skvortsova, 2006; Sanner & Boutilier, 2009; Wang, Joshi, & Khardon, 2008). The advantage of the relational representation is abstraction. One can plan at the abstract level without grounding the domain, potentially leading to more efficient algorithms. In addition, the solution at the abstract level is optimal for every instantiation of the domain and can be reused for multiple problems. However, this approach raises some difficult computational issues because one must use theorem proving to reason at the abstract level, and because for some problems optimal solutions at the abstract level can be infinite in size. Following Boutilier et al. (2001) several abstract versions of the value iteration (VI) algorithm have been developed using different representation schemes.", "startOffset": 67, "endOffset": 2366}, {"referenceID": 40, "context": "This new operation simultaneously expands the applicability of the R7 reduction (Wang et al., 2008) to cover more situations and simplifies the test for its applicability, that must be implemented in our system.", "startOffset": 80, "endOffset": 99}, {"referenceID": 32, "context": "1 Relational Markov Decision Processes A Markov decision process (MDP) is a mathematical model of the interaction between an agent and its environment (Puterman, 1994).", "startOffset": 151, "endOffset": 167}, {"referenceID": 40, "context": "The variant of SDP developed in our previous work (Wang et al., 2008) employed First Order Decision Diagrams to represent the intermediate constructs.", "startOffset": 50, "endOffset": 69}, {"referenceID": 40, "context": "This section briefly reviews previous work on FODDs and their use for relational MDPs (Wang et al., 2008).", "startOffset": 86, "endOffset": 105}, {"referenceID": 28, "context": "We use standard terminology from First-Order logic (Lloyd, 1987).", "startOffset": 51, "endOffset": 64}, {"referenceID": 15, "context": "Following Groote and Tveretina (2003), the semantics of FODDs are defined as follows.", "startOffset": 10, "endOffset": 38}, {"referenceID": 15, "context": "Groote and Tveretina (2003) introduced four strong reduction operators (R1 \u00b7 \u00b7 \u00b7 R4).", "startOffset": 0, "endOffset": 28}, {"referenceID": 15, "context": "Groote and Tveretina (2003) introduced four strong reduction operators (R1 \u00b7 \u00b7 \u00b7 R4). Wang et al. (2008) added the strong reduction operator R5.", "startOffset": 0, "endOffset": 105}, {"referenceID": 39, "context": "This can be seen by translating the formula f into a disjunctive normal form f = \u2228fi, representing every conjunct fi as a FODD, and calculating their disjunction using the apply procedure of Wang et al. (2008).", "startOffset": 191, "endOffset": 210}, {"referenceID": 40, "context": "Corresponding to these, the R7 reduction operator (Wang et al., 2008) has two variants - R7-replace (for removing redundant edges) and R7-drop (for removing redundant nodes).", "startOffset": 50, "endOffset": 69}, {"referenceID": 39, "context": "Wang et al. (2008) provide an algorithm for calculating a FODD representation of B.", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "With these definitions Wang et al. (2008) show that it is safe to perform R7-replace when the conditions P7.", "startOffset": 23, "endOffset": 42}, {"referenceID": 40, "context": "Lemma 1 ((Wang et al., 2008)) Let B be a FODD, e1 and e2 edges for which conditions P7.", "startOffset": 9, "endOffset": 28}, {"referenceID": 39, "context": "Several alternative conditions for the applicability of R7 (R7-replace and R7-drop) are given by Wang et al. (2008). This provided a set of alternative conditions for applicability of R7 none of which dominates the others, with the result that effectively one has to check all the conditions when reducing a diagram.", "startOffset": 97, "endOffset": 116}, {"referenceID": 40, "context": "In previous work (Wang et al., 2008) we showed how to capture the reward function and the dynamics of the domain using FODDs and presented a value iteration algorithm along the lines described in the last section.", "startOffset": 17, "endOffset": 36}, {"referenceID": 40, "context": "The details of these representations and algorithms were previously described (Wang et al., 2008) and they are not directly needed for the discussion in this paper and thus omitted here.", "startOffset": 78, "endOffset": 97}, {"referenceID": 40, "context": "The reductions R1-R9 (Groote & Tveretina, 2003; Wang et al., 2008) provide a first step towards an efficient FODD system.", "startOffset": 21, "endOffset": 66}, {"referenceID": 5, "context": "Domains dynamics are captured by FODDs describing the probabilities of action variants prob(Aj(~a)), and by special FODDs, Truth Value Diagrams (TVD), that capture the deterministic effects of each action variant, similar to the successor state axioms used by Boutilier et al. (2001). For every action variant Aj(~a) and each predicate schema p(~x) the TVD T (A(~a), p(~x)) is a FODD with {0, 1} leaves.", "startOffset": 260, "endOffset": 284}, {"referenceID": 39, "context": "This section presents two techniques to speed up the VI algorithm of Wang et al. (2008) while maintaining an exact solution.", "startOffset": 69, "endOffset": 88}, {"referenceID": 0, "context": "The algorithm uses the standard recursive template for combining ADDs and FODDs (Bahar et al., 1993; Wang et al., 2008) where a root node is chosen from the root of the two diagrams and the operation is recursively performed on the corresponding sub-diagrams.", "startOffset": 80, "endOffset": 119}, {"referenceID": 40, "context": "The algorithm uses the standard recursive template for combining ADDs and FODDs (Bahar et al., 1993; Wang et al., 2008) where a root node is chosen from the root of the two diagrams and the operation is recursively performed on the corresponding sub-diagrams.", "startOffset": 80, "endOffset": 119}, {"referenceID": 40, "context": "In fact, the smallest set of variables ~ w for B ~ w to have no negative leaves may not be unique (Wang et al., 2008).", "startOffset": 98, "endOffset": 117}, {"referenceID": 39, "context": "Using exactly the same proof as Lemma 1 given by Wang et al. (2008), we can show the following:", "startOffset": 49, "endOffset": 68}, {"referenceID": 40, "context": "3S) subsume all the previous conditions for applicability and safety of R7-replace that were previously given (Wang et al., 2008).", "startOffset": 110, "endOffset": 129}, {"referenceID": 39, "context": "Therefore, before adding or maximizing, the algorithm by Wang et al. (2008) standardizes apart the diagrams.", "startOffset": 57, "endOffset": 76}, {"referenceID": 40, "context": "That is, if we do not standardize apart action variant diagrams before adding them, the value given to some states may be lower than the true value (Wang et al., 2008).", "startOffset": 148, "endOffset": 167}, {"referenceID": 34, "context": "Here we follow the approximation techniques of APRICODD (St-Aubin et al., 2000) where they were used for propositional problems.", "startOffset": 56, "endOffset": 79}, {"referenceID": 43, "context": "Previous work on stochastic planning has discovered that for some domains one can get good performance by pretending that the domain is deterministic and re-planning if unexpected outcomes are reached (Yoon et al., 2007).", "startOffset": 201, "endOffset": 220}, {"referenceID": 21, "context": "Therefore our VI algorithm cannot handle universal goals at the abstract level (though see Joshi et al. (2009) for a formalism that does accept arbitrary quantifiers).", "startOffset": 91, "endOffset": 111}, {"referenceID": 21, "context": "Therefore our VI algorithm cannot handle universal goals at the abstract level (though see Joshi et al. (2009) for a formalism that does accept arbitrary quantifiers). For a concrete planning problem with a known set of objects we can instantiate the universal goal to get a large conjunctive goal. In principle we can run VI and policy generation for this large conjunctive goal. However, this would mean that we cannot plan off-line to get a generic policy and must replan for each problem instance from scratch. Here we follow an alternative heuristic approach previously introduced by Sanner and Boutilier (2009) and use an approximation of the true value function, that results from a simple additive decomposition of the goal predicates.", "startOffset": 91, "endOffset": 617}, {"referenceID": 40, "context": "In the FODD-Planner strong reductions are automatically applied every time two diagrams are combined (using the apply algorithm (Wang et al., 2008)) and weak reductions are applied every time two diagrams are combined except during regression by block combination.", "startOffset": 128, "endOffset": 147}, {"referenceID": 39, "context": "Wang (2007) provides an algorithm and a detailed discussion of translation from PPDDL to FODDs.", "startOffset": 0, "endOffset": 12}, {"referenceID": 5, "context": "This is the boxworld problem introduced by Boutilier et al. (2001) that has been used as a standard example for exact solution methods for relational MDPs.", "startOffset": 43, "endOffset": 67}, {"referenceID": 36, "context": "Competition results show that RFF (Teichteil-Koenigsbuch et al., 2008) was the only system that solved any of the 15 problems.", "startOffset": 34, "endOffset": 70}, {"referenceID": 19, "context": "These include approaches that combine dynamic programming with linear function approximation (Sanner & Boutilier, 2009), forward search (H\u00f6lldobler et al., 2006) and machine learning (Fern, Yoon, & Givan, 2006; Gretton & Thiebaux, 2004).", "startOffset": 136, "endOffset": 161}, {"referenceID": 18, "context": "Mausam and Weld (2003) employ SPUDD (Hoey et al., 1999) to solve ground instances of an RMDP, generate training data from the solutions and learn a lifted value", "startOffset": 36, "endOffset": 55}, {"referenceID": 18, "context": "These include approaches that combine dynamic programming with linear function approximation (Sanner & Boutilier, 2009), forward search (H\u00f6lldobler et al., 2006) and machine learning (Fern, Yoon, & Givan, 2006; Gretton & Thiebaux, 2004). All of these yielded strong implementations that participated in some planning competitions. Other works do not directly use dynamic programming. For instance Guestrin, Koller, Gearhart, and Kanodia (2003a) present an approach using additive value functions based on object classes and employ linear programming to solve the RMDP.", "startOffset": 137, "endOffset": 445}, {"referenceID": 18, "context": "These include approaches that combine dynamic programming with linear function approximation (Sanner & Boutilier, 2009), forward search (H\u00f6lldobler et al., 2006) and machine learning (Fern, Yoon, & Givan, 2006; Gretton & Thiebaux, 2004). All of these yielded strong implementations that participated in some planning competitions. Other works do not directly use dynamic programming. For instance Guestrin, Koller, Gearhart, and Kanodia (2003a) present an approach using additive value functions based on object classes and employ linear programming to solve the RMDP. Mausam and Weld (2003) employ SPUDD (Hoey et al.", "startOffset": 137, "endOffset": 592}, {"referenceID": 18, "context": "In a similar view the work on FODD-Planner is a relational extension of the work on ADD based solvers for propositionally factored MDPs (Hoey et al., 1999).", "startOffset": 136, "endOffset": 155}, {"referenceID": 40, "context": "We have previously argued (Wang et al., 2008) that the FODD semantics are much better suited for dynamic programming solutions.", "startOffset": 26, "endOffset": 45}, {"referenceID": 12, "context": "Gardiol and Kaelbling (2003) apply methods from probabilistic planning to solve RMDPs.", "startOffset": 0, "endOffset": 29}, {"referenceID": 12, "context": "Gardiol and Kaelbling (2003) apply methods from probabilistic planning to solve RMDPs. In the most closely related work that preceded our effort, Sanner and Boutilier (2009) developed a relational extension of linear function approximation techniques for factored MDPs.", "startOffset": 0, "endOffset": 174}, {"referenceID": 12, "context": "Gardiol and Kaelbling (2003) apply methods from probabilistic planning to solve RMDPs. In the most closely related work that preceded our effort, Sanner and Boutilier (2009) developed a relational extension of linear function approximation techniques for factored MDPs. The value function is represented as a weighted sum of basis functions, each denoting a partition of the state space. The difference from the work on factored MDPs is that these basis functions are First-Order formulas and thus the value function is valid for any domain size (this is the same fundamental advantage that RMDP solvers have over ground MDP solvers). They develop methods for automatic generation of First-Order constraints in a linear program and automatic generation of basis functions that show promise in solving some domains from the IPC. The work of Sanner and Boutilier is thus an extension of the work on linear representations for propositionally factored MDPs (e.g., Guestrin et al., 2003b) to capture relational structure. In a similar view the work on FODD-Planner is a relational extension of the work on ADD based solvers for propositionally factored MDPs (Hoey et al., 1999). In this context it is interesting to note that Sanner and Boutilier also developed a relational extension of ADDs they call FOADDs. In contrast with FODDs, nodes in FOADDs are labeled with closed First-Order formulas.5 Sanner and Boutilier report on an implementation that was able to provide exact solutions for simple problems, but they developed and applied the approach using linear function approximation for more complex problems. Our experiments do use approximation and they demonstrate that FODDs can be used to solve problems at least of the complexity currently employed in the IPC. Another important body of work is pursued by Relational Reinforcement Learning (RRL) (Tadepalli, Givan, & Driessens, 2004) where techniques from reinforcement learning are used to learn or construct value functions and policies for relational domains. RRL followed from the seminal work of Dzeroski, De Raedt, and Driessens (2001) whose algorithm involved generating state-value pairs by state space exploration (biased in favor of state-action pairs with high estimated value) and learning a relational value function tree from the collected data.", "startOffset": 0, "endOffset": 2100}, {"referenceID": 10, "context": "In a sense the First-Order decision trees used by Dzeroski et al. (2001) are similar to FODDs.", "startOffset": 50, "endOffset": 73}, {"referenceID": 33, "context": "As discussed by Sanner and Boutilier (2009) it is hard to characterize the exact relationship between FOADDs and FODDs in terms of representation and computational properties.", "startOffset": 16, "endOffset": 44}, {"referenceID": 37, "context": "An excellent overview of the various solutions methods for RMDPs is provided by van Otterlo (2008).", "startOffset": 84, "endOffset": 99}, {"referenceID": 40, "context": "FODD-Planner provides several improvements over previous work on FODDs (Wang et al., 2008).", "startOffset": 71, "endOffset": 90}, {"referenceID": 21, "context": "In recent work (Joshi, Kersting, & Khardon, 2010) we developed practical variants of model-checking reductions (Joshi et al., 2009) demonstrating significant speedup over the system presented here.", "startOffset": 111, "endOffset": 131}, {"referenceID": 21, "context": "Another direction is the use of the more expressive GFODDs (Joshi et al., 2009) that can handle arbitrary quantification and can therefore be applied more widely.", "startOffset": 59, "endOffset": 79}], "year": 2011, "abstractText": "Dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision diagrams, to capture domain dynamics and value functions. Work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes. Recent work introduced a first order variant of decision diagrams (FODD) and developed a value iteration algorithm for this representation. This paper develops several improvements to the FODD algorithm that make the approach practical. These include, new reduction operators that decrease the size of the representation, several speedup techniques, and techniques for value approximation. Incorporating these, the paper presents a planning system, FODD-Planner, for solving relational stochastic planning problems. The system is evaluated on several domains, including problems from the recent international planning competition, and shows competitive performance with top ranking systems. This is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning.", "creator": "gnuplot 4.2 patchlevel 4 "}}}