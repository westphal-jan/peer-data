{"id": "1401.3885", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Scaling up Heuristic Planning with Relational Decision Trees", "abstract": "Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.", "histories": [["v1", "Thu, 16 Jan 2014 05:14:42 GMT  (351kb)", "http://arxiv.org/abs/1401.3885v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomas de la rosa", "sergio jimenez", "raquel fuentetaja", "daniel borrajo"], "accepted": false, "id": "1401.3885"}, "pdf": {"name": "1401.3885.pdf", "metadata": {"source": "CRF", "title": "Scaling up Heuristic Planning with Relational Decision Trees", "authors": ["Tom\u00e1s de la Rosa", "Sergio Jim\u00e9nez", "Raquel Fuentetaja", "Daniel Borrajo"], "emails": ["TROSA@INF.UC3M.ES", "SJIMENEZ@INF.UC3M.ES", "RFUENTET@INF.UC3M.ES", "DBORRAJO@IA.UC3M.ES"], "sections": [{"heading": null, "text": "However, if evaluation functions are misleading or planning problems are large enough, many node evaluations need to be calculated, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. In particular, we define the task of heuristic planning search control as a task of relational classification, and we use a standard relational classification tool to solve this learning task. Our task of relational classification captures the preferred action to be selected in the different planning contexts of a specific planning area. These planning contexts are defined by the number of helpful measures of the current state, the goals yet to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first is to consider the resulting problems as larger states than those generated in advance by a multitude of hayristic planners."}, {"heading": "1. Introduction", "text": "Over the past few years, state-spatial search planning has achieved significant results and has become one of the most popular paradigms for automated planning. However, heuristic search planners suffer from severe scalability limitations. Even well-studied areas such as Blocksworld become a challenge for these planners when the number of blocks is relatively large. In addition, domain-independent heuristic search planners are usually action-based, spending most of their planning time calculating useless node valuations when the number of objects and / or action parameters is large enough. Furthermore, domain-independent heuristics are expensive to calculate. In domains where these heuristic planners are more misleading, they spend much of their planning time calculating useless node valuations. Even with the best current domain-independent heuristic functions in the literature, forward chaining heuristic planners are currently visiting too many nodes, which is especially necessary due to the time needed to compile them."}, {"heading": "2. Common Issues in Learning Domain-specific Control Knowledge", "text": "In fact, it is so that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, to change it, to change it, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change. \""}, {"heading": "3.2 The Learning Algorithm: Learning Generalized Policies with Relational Decision Trees", "text": "ROLLER implements a two-step learning process for building DCK from a collection of examples from various helpful contexts: 1. Learning the operator classifier. ROLLER builds a classifier to select the best operator in the various helpful contexts. 2. Learning the binding classifiers. For each operator in the domain, ROLLER builds a classifier to select the best binding (operator instantiation) in the various helpful contexts.The learning process is divided into these two steps to create DCK with standard learning tools.Each planning action can have different arguments and arguments (e.g. turn on actions (instrument, satellite) and focus on (satellite, direction, direction) from the satellite domain), which impedes the definition of the target classes. This two-step decision process is also clearer from the point of view of decision making. It helps the user to better understand the generated decision to use the algorithm, either by focusing on which one of the two CK steps is applicable to the other."}, {"heading": "3.2.1 LEARNING RELATIONAL DECISION TRESS", "text": "A classic approach to supporting decision-making is to collect a significant number of previous decisions and build a decision tree that generalizes them; the leaves of the resulting tree contain the classes (making decisions), and the internal nodes contain the conditions that lead to these decisions; the most common way to build these trees is the Top-Down Induction of Decision Trees (TDIDT) algorithm (Quinlan, 1986), which builds the tree by repeatedly splitting the set of training examples according to the conditions that minimize entropy in the examples. Traditionally, training examples are described in an attribute-value representation; therefore, the conditions of the decision trees are tests of the value of a given attribute of the examples. However, this attribute-value approach is not suitable to represent decisions if we want to maintain the predicatalog representation of logical representation."}, {"heading": "3.2.2 LEARNING THE OPERATOR CLASSIFIER", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3.2.3 LEARNING THE BINDING CLASSIFIERS", "text": "In the second half of the year, the number of newcomers to the US will increase by 20%, while the number of newcomers to the US will increase by 20%."}, {"heading": "3.3 Generation of Training Examples", "text": "ROLLER training examples are instances that are made when solving training problems. In order to characterize a variety of good solutions, these decisions should be considered to solve each individual problem."}, {"heading": "3.3.1 GENERATION OF SOLUTIONS", "text": "ROLLER solves each education problem with a best-first branch and ground (BFS-BnB) algorithm that extracts several high-quality solutions. If the search space has not been fully explored within a certain period of time, the problem is discarded and no examples are generated from it. Therefore, the education problems must be sufficiently small. Furthermore, the education problems must be representative enough to generalize the DCK that ROLLER supports in solving future problems in the same domain. BFS-BnB searches are completed without pruning repeated states. In practice, many repeated states are generated by changing the order between actions of different solutions. Thus, pruning repeated states actions would lead to these solutions, which is indeed not true. Furthermore, the BFS-BnB algorithm leads to an erroneous search in which is unable to find a faulty solution."}, {"heading": "3.3.2 SELECTING SOLUTIONS", "text": "Since it is difficult to develop domain-independent criteria for systematically selecting solutions that reflect the same selection of the operator in a particular context, we have defined an approach that heuristically favors some measures over others. These preferences are: \u2022 Minimal Commitment Preferences: Prefer measures that generate more alternatives of different solutions. \u2022 Difficulty Ranking: Prefer measures that achieve the goals or partial goals that are most difficult to achieve. In the example of Figure 9, in which Instrument T is turned on, only one measure is achievable. On the other hand, pointing toward D1 is considered easier because it can be achieved with measures that achieve goals or partial goals that are most difficult to achieve."}, {"heading": "3.3.3 EXTRACTING EXAMPLES FROM SOLUTIONS", "text": "ROLLER takes the subset of solutions selected in the previous step and generates training examples. When generating operator classification examples, ROLLER adopts solution plans \u03c0 \u2032 = {a1, a2,...,} that correspond to the sequence of state transitions {s0, s1,..., sn} and generates a learning example for each pair < si, ai + 1 > consisting of H (si) and the class (i.e. the operator name ai + 1). See learning example in Figure 2. When generating binding operator classification examples o, ROLLER only takes into account pairs < si, ai + 1 > where ai + 1 coincides with operator o. A learning example generated from the pair < si, ai + 1 > for binding operator o selection consists of H (si) and the classes of all applicable actions in si that match o, including ai + 1, does not include all actions other than the selected class 5 in the solution list, although all actions that can be applied to the top of the solution list are not."}, {"heading": "3.4 Use of the Learned Knowledge: Planning with Relational Decision Trees", "text": "This section describes how heuristic planning can benefit from our DCK, starting with how we build action orders with the learned DCK. Subsequently, two different search strategies are explained to exploit these orders: (1) the application of the DCK as a generalized action guideline (DepthFirst H-Context Policy Algorithm) and (2) the use of the DCK to generate predictive states within a best-first search (BFS) led by the FF heuristic (H-Context Policy Lookahead-BFS Algorithm)."}, {"heading": "3.4.1 ORDERING ACTIONS WITH RELATIONAL DECISION TREES", "text": "In view of the fact that the expression app (s) occupies the primacy of the individual measures in the individual countries, it represents an arrangement of the individual measures. The algorithm divides the list of applicable measures into two subareas: the helpful measures and the unhelpful measures. Then, it fits into the helpful context of the state, i.e. it is connected to the tree for the classification of the operator. This matching offers the list of applicable measures in two subareas."}, {"heading": "3.4.2 THE H-CONTEXT POLICY ALGORITHM", "text": "The helpful Context Action Policy algorithm moves forward and performs the best action according to the DCK in each state. The pseudo-code of the algorithm is displayed in Figure 11. The algorithm maintains an ordered open list. The open list contains the states to be expanded that are extracted in order. Once extracted, each state is evaluated using FF heuristics. Thus, we evaluate during extraction and not when nodes are included in the open list. The evaluation returns the heuristic value for the state, h, and the amount of helpful actions HA that are needed to generate the helpful context. The heuristic value is only generated for: (1) continuing the search when the state is a recognized dead end (h =), and (2) the target check (h = 0) the helpful context is generated."}, {"heading": "3.4.3 THE H-CONTEXT POLICY IN A LOOKAHEAD STRATEGY", "text": "In many cases, however, a direct application of the learned DCK system (without traceability) is not possible for the planner to achieve the objectives of the problem. Poor quality in the learned DCK list can be balanced with a leader of another kind, such as a domain-independent heuristic approach. A successful example is the ObtuseWedge system (without traceability), which combines a learned generalized policy with the FF heuristic approach. ObtuseWedge has used the learned policy to synthesize synthetic approaches. Lookahead states1. With perfect policy, we point to a policy that leads directly to a target state. Our policy is not guaranteed to be perfect, as it is generated through inductive learning processes."}, {"heading": "4. Experimental Results", "text": "In this section, we evaluate the performance of the ROLLER system. The evaluation is made using a variety of domains belonging to different IPCs: Four domains were selected from the IPC-2008 learning course (Gold-miner, Matching Blocksworld, Parking and Thoughtful); the remaining domains (Blocksworld, Depots, Satellite, Rover, Storage and TPP) were selected from the domains of IPC's sequential routes between 2000 and 2008 because they had different structures and levels of difficulty and because they have random generators so we can automatically create training kits for learning DCK. For each domain, we complete a training phase in which ROLLER learns the appropriate DCK and a test phase in which we evaluate the scalability and quality of the solutions ROLER learns with the DCK learnt. Next, we explain the experimental results achieved in each of these two phases. In addition, we give the details of each domain to the ROLLER learners and the SERs learnt."}, {"heading": "4.1 Training Phase", "text": "As explained in Section 3.2, ROLLER generates its training examples for solving the problems from the training set using a BFS-BnB search. However, we specify a 60-second time span to solve each training problem, discarding those that are not fully investigated during this time span. Afterwards, ROLLER calculates the training examples from the solutions found and creates the corresponding decision trees using the TILDE system (Blockeel & De Raedt, 1998). To evaluate the efficiency of ROLLER in the training phase, we calculated the following indicators: the time required to solve the training problems, the number of training examples generated in this process, the time TILDE spends learning the decision trees, and the number of leaves on the control tree. This last number indicates the size of the learning processes learned."}, {"heading": "4.2 Testing Phase", "text": "In the test phase, ROLLER tries to solve one set of thirty test problems for each domain, taken from the evaluation set of the corresponding IPC. If this evaluation set contains more problems, these thirty problems are the thirty most difficult ones. The repository domain is an exception with twenty-two problems, since the evaluation set for this domain on IPC-2002 contained only these twenty-two problems. In the test phase, three experiments are conducted: the first evaluates the performance of ROLLER if DCK is learned with all the solutions of the training problems or with the ranking solution approach; the second evaluates the usefulness of the learned DCK; and the third compares ROLLER with state-of-the-art planners. For each experiment, we evaluate two different dimensions of the solutions found by ROLLER: scalability and quality. All test experiments were performed with a 2.4 GHz processor with a time-bound 900-second and 6b-bound memory."}, {"heading": "4.2.1 SOLUTION RANKING EVALUATION", "text": "This experiment evaluates the effect of solution selection according to the approach described in Section 3.3. The ROLLER configurations for this assessment are: \u2022 Top-Ranked Solutions: The Depth-First H-Context Policy algorithm using DCK learned with the sub-set of the top ranking solutions. We use this search algorithm because its performance depends more on the quality of the DCK learned than that of the other algorithms using DCK. \u2022 All Solutions: The Depth-First H-Context Policy algorithm using DCK learned with all solutions by the BFS-BnB algorithm.Table 2 shows the number of problems solved by each configuration, including the time and plan length calculated on average over the problems solved by both configurations. The number in brackets in the first column is the number of jointly solved problems LFS-BnB algorithm. The Top-Ranking Solutions configuration Lain solved problem Lain's most important problem to constitute the solutions, 21 is the block-BnB algorithm."}, {"heading": "4.2.2 DCK USEFULNESS EVALUATION", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.2.3 TIME PERFORMANCE COMPARISON", "text": "This experiment evaluates the scalability of the ROLLER system compared to the state of the art planner. For comparison, we have LAMA (Richter & Westphal, 2010), the winner of the sequential track of the past IPC, and FF, which in the last IPC showed that they are still competitive. We have explained the three ROLLER configurations in the previous evaluation. Configuration for other planners are: \u2022 FF. Running the Enforced Hill-Climbing (EHC) algorithms along with a full BFS configuration failed. Although this planner dates back to 2001, we include them in the evaluation, as the results of the IPC-2008 show, it is still competitive."}, {"heading": "4.2.4 QUALITY PERFORMANCE COMPARISON", "text": "This experiment compares the quality of the first solutions found and the solutions found by the respective behavior. In the current configuration, the planners exhaust themselves in an attempt to incrementally improve the best solution found. Three ROLLER algorithms are modified to a configuration in which the best solution found so far is used as the top limit to cut all nodes exceeding this plan length. Anytime behavior is the regular configuration for LAMA. FF has no permanent behavior, but it will be included in the quality of other planners comparing at any time. Table 7 shows the quality scores for the first solution and for the last solution found by the respective configurations. The column for each planner shows the score variation and shows whether the planner was able to make relative improvements to the first solutions."}, {"heading": "4.2.5 BLOCKSWORLD DETAILS", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "4.2.6 DEPOTS DETAILS", "text": "In this area, the 30 training problems are different combinations of 2 or 3 sites (depots and distributors), 1 or 2 pallets per site, 1 lever per site and 5 boxes placed in different constellations. For the test phase, we have set the problems of the IPC-2002. The hardest problem is 12 sites (1 or 2 pallets per site), 1 lever per site and 5 boxes placed in different constellations."}, {"heading": "4.2.7 GOLD-MINER DETAILS", "text": "The goal of this domain is to navigate in a grid of cells until it comes to a cell that contains gold. Some of the cells are occupied by rocks that can be erased with bombs or a laser. In this area, the training program consists of: 10 problems with 3 x 3 cells, 10 problems with 4 x 4 cells and 10 problems with 5 x 5 cells. This domain was part of the learning course in IPC-2008, so we used the same test kit that is used in the competition. This set has problems ranging from 5 x 5 to 7 x 7 cells. Problems in the gold-miner domain are not solvable with helpful actions alone. This explains the difference in the number of solved problems between ROLLER and BFS-HA."}, {"heading": "4.2.8 MATCHING BLOCKSWORLD DETAILS", "text": "This domain is a version of Blocksworld designed to analyze the limitations of the relaxed plan heuristically. In this version, blocks are polarized, either positive or negative. There are also two polarized robotic arms. In addition, when a block is placed (stack or put-down actions) with an arm of different polarity, the block is damaged and no block can be placed on it. However, lifting or unstacking a block with the wrong polarity appears to be harmless. This fact makes finding dead ends a difficult task for FF heuristically. Especially in the relaxed task blocks, no blocks are damaged. Thus, both the relaxed plan (and consequently the set of helpful measures) and the heuristic estimate are wrong. The training set used in this area consists of fifteen 6-block problems and fifteen 8-block problems. We have even used a number of blocks to balance the problems (i.e., half of each block)."}, {"heading": "4.2.9 PARKING DETAILS", "text": "In this area, the training set consists of: fifteen problems with six cars and four kerbs and fifteen problems with eight cars and five kerbs. For testing, we used the test set from the IPC-2008 learning track. The most difficult problem in this set consists of 38 cars and 20 kerbs. The three ROLLER configurations solve all problems and perform significantly better than non-learning strategies. In addition, the three ROLLER configurations perform better than FF and LAMA with a difference of more than one order of magnitude. This is the reason for LAMA and FF low speed values. ROLLER configurations are consistently better than systematic and empty configurations. Figure 18 shows the percentage of problems solved while increasing the CPU time. On the other hand, the three ROLER configurations are not perfect in terms of quality."}, {"heading": "4.2.10 ROVERS DETAILS", "text": "In this area, the training kit consists of: ten problems with a rover, four waypoints, two targets and one camera; ten problems with an additional camera; and ten problems with an additional rover. Problems in the test kit are the thirty biggest problems from the IPC 2006 set (i.e. problems 11 to 40). The biggest problem in this set has 14 rovers and 100 waypoints. DCK strategies are faster than systematic and empty strategies, but the differences are not significant, as all configurations have solved most problems. On the one hand, helpful measures in the rover area are quite good, but on the other hand, the test kit does not show any problems large enough to create differences between approaches. In terms of planner comparison, ROLLER achieves the top performance score and scales significantly better than the CER problems and solves fewer problems than the CMA-19."}, {"heading": "4.2.11 SATELLITE DETAILS", "text": "This domain encompasses a number of satellites with different instruments, which can operate in different formats (modes). The tasks are to manage the instruments for capturing images of specific targets in certain modes. In this area, the training consists of thirty problems with one satellite, two instruments, five modes and five observations. Problems in the test set are the thirty biggest problems from the IPC-2004 (i.e. problems 7 to 36). The biggest problem in this set has 10 satellites, 5 modes and 174 observations. The three ROLLER configurations improved the number of solved problems of their non-learning counterpart. In addition, ROLLER and ROLER-BFS-HA have solved the 30 problems in the set, two more than LAMA and eight more than FF. Figure 20 shows the percentage of solved problems in increasing the CPU time."}, {"heading": "4.2.12 STORAGE DETAILS", "text": "The domain tasks include the use of hoists to transport crates of containers to a specific area in the depot. The training set consists of 30 problems with 1 depot, 1 container, 1 hoist and various combinations of 2 or 3 crates and from 2 to 6 areas within the depot. For the test set we used the 30 problems from the IPC 2006 set. The biggest problem in this area consists of 4 depots with 8 areas each, 5 hoists and 20 crates.The first 12 problems are solved trivially by all configurations, then the problem difficulty increases rapidly as the number of problem objects increases. The BFS solves 20 problems, one more than any DCK strategy, which means that DCK strategies do not pay off. This area is also difficult for FF and LAMA."}, {"heading": "4.2.13 THOUGHTFUL DETAILS", "text": "This domain models a version of the solitaire card game where all cards are visible and you can spin each card from the talon instead of 3 at the same time. As in the original version, the aim of the game is to place all cards in ascending order in their respective home deck. There is no random problem generator available for this domain. Therefore, we used the bootstrap problem distribution specified in the IPC-2008 learning path. This set contains problems for the four suits with up to seven cards for each suit. For the test phase, we used the 30 problems from the test distribution of learning from IPC-2008. The biggest problem in this domain is the complete set of a standard card game. ROLLER solves only 12 problems, three less than GR-HA. However, ROLER-BFS and ROLERBFS-HA are better in terms of the number of problems solved than the unlearned approaches."}, {"heading": "4.2.14 TPP DETAILS", "text": "TPP stands for Traveling Purchase Problem, which is a generalization of the Traveling Salesman Problem. Tasks in the STRIPS version consist of selecting a subset of markets to satisfy the demand for a number of goods. The selection of markets should try to optimize the routing and purchasing costs of goods. In the STRIPS version, the graphics connecting markets has the same cost for all bows. Nevertheless, the domain is still interesting because it is difficult for planners to increase the number of goods, markets and trucks. The training set consists of thirty problems with a number of goods, trucks and depots that vary from one to three and with load levels of five and six. The test set consists of the thirty problems used for the planner evaluation at IPC-2006. The biggest problem in this group are 20 goods, 8 trucks, 8 markets with load levels of sixty. ROLLLER, GR-HA and HER-LFF forms the two LER-HER-30 problem solved."}, {"heading": "5. Lessons Learned from the IPC", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times. \"\" I don't think they are able to survive me, \"he said in an interview with the\" New York Times. \"\" I don't think they are able to survive me, \"he said.\" I don't think they are able to survive me. \""}, {"heading": "6. Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "7. Conclusions and Future Work", "text": "This year, the time has come for us to find a solution that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us, that will enable us to be as we are."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Spanish MICIIN project TIN2008-06701-C03-03 and the regional CAM-UC3M project CCG08-UC3M / TIC-4141."}, {"heading": "Appendix A. DCK Usefulness Results", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Using temporal logics to express search control knowledge for planning", "author": ["F. Bacchus", "F. Kabanza"], "venue": "Artificial Intelligence,", "citeRegEx": "Bacchus and Kabanza,? \\Q2000\\E", "shortCiteRegEx": "Bacchus and Kabanza", "year": 2000}, {"title": "An evolutionary metaheuristic based on state decomposition for domain-independent satisficing planning", "author": ["J. Biba\u0131", "P. Sav\u00e9ant", "M. Schoenauer", "V. Vidal"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling", "citeRegEx": "Biba\u0131\u0308 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Biba\u0131\u0308 et al\\.", "year": 2010}, {"title": "Top-down induction of first-order logical decision trees", "author": ["H. Blockeel", "L. De Raedt"], "venue": "Artificial Intelligence,", "citeRegEx": "Blockeel and Raedt,? \\Q1998\\E", "shortCiteRegEx": "Blockeel and Raedt", "year": 1998}, {"title": "A robust and fast action selection mechanism for planning", "author": ["B. Bonet", "G. Loerincs", "H. Geffner"], "venue": "In Proceedings of the American Association for the Advancement of Artificial Intelligence Conference (AAAI),", "citeRegEx": "Bonet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 1997}, {"title": "Macro-FF: Improving AI planning with automatically learned macro-operators", "author": ["A. Botea", "M. Enzenberger", "M. M\u00fcller", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Botea et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Botea et al\\.", "year": 2005}, {"title": "Marvin: A heuristic search planner with online macro-action learning", "author": ["A. Coles", "A. Smith"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Coles and Smith,? \\Q2007\\E", "shortCiteRegEx": "Coles and Smith", "year": 2007}, {"title": "Learning relational decision trees for guiding heuristic planning", "author": ["T. De la Rosa", "S. Jim\u00e9nez", "D. Borrajo"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Rosa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rosa et al\\.", "year": 2008}, {"title": "Three relational learning approaches for lookahead heuristic planning", "author": ["T. De la Rosa", "S. Jim\u00e9nez", "R. Garc\u0131\u0301a-Dur\u00e1n", "F. Fern\u00e1ndez", "A. Garc\u0131\u0301a-Olaya", "D. Borrajo"], "venue": "In Working Notes of ICAPS 2009 Workshop on Planning and Learning,", "citeRegEx": "Rosa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rosa et al\\.", "year": 2009}, {"title": "Logical and Relational Learning", "author": ["L. De Raedt"], "venue": null, "citeRegEx": "Raedt,? \\Q2008\\E", "shortCiteRegEx": "Raedt", "year": 2008}, {"title": "Talplanner: A temporal logic based planner", "author": ["P. Doherty", "J. Kvarnstr\u00f6m"], "venue": "AI Magazine,", "citeRegEx": "Doherty and Kvarnstr\u00f6m,? \\Q2001\\E", "shortCiteRegEx": "Doherty and Kvarnstr\u00f6m", "year": 2001}, {"title": "Relational reinforcement learning", "author": ["S. Dzeroski", "L. De Raedt", "H. Blockeel"], "venue": "In International Workshop on ILP,", "citeRegEx": "Dzeroski et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dzeroski et al\\.", "year": 1998}, {"title": "Relational instance-based learning", "author": ["W. Emde", "D. Wettschereck"], "venue": "In Proceedings of the 13th Conference on Machine Learning,", "citeRegEx": "Emde and Wettschereck,? \\Q1996\\E", "shortCiteRegEx": "Emde and Wettschereck", "year": 1996}, {"title": "Timiplan: An application to solve multimodal transportation problems", "author": ["J.E. Fl\u00f3rez", "J. Garc\u0131\u0301a", "A. Torralba", "C. Linares", "A. Garc\u0131\u0301a-Olaya", "D. Borrajo"], "venue": "In Proceedings of SPARK, Scheduling and Planning Applications workshop,", "citeRegEx": "Fl\u00f3rez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fl\u00f3rez et al\\.", "year": 2010}, {"title": "Improving control-knowledge acquisition for planning by active learning", "author": ["R. Fuentetaja", "D. Borrajo"], "venue": "In ECML, Berlin, Germany,", "citeRegEx": "Fuentetaja and Borrajo,? \\Q2006\\E", "shortCiteRegEx": "Fuentetaja and Borrajo", "year": 2006}, {"title": "An automatically configurable portfolio-based planner with macro-actions: Pbp", "author": ["A. Gerevini", "A. Saetti", "M. Vallati"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling,", "citeRegEx": "Gerevini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gerevini et al\\.", "year": 2009}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hoffmann and Nebel,? \\Q2001\\E", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "Ordered landmarks in planning", "author": ["J. Hoffmann", "J. Porteous", "L. Sebastia"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hoffmann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2004}, {"title": "Learning action strategies for planning domains", "author": ["R. Khardon"], "venue": "Artificial Intelligence,", "citeRegEx": "Khardon,? \\Q1999\\E", "shortCiteRegEx": "Khardon", "year": 1999}, {"title": "Inductive learning of search control rules for planning", "author": ["C. Leckie"], "venue": "Artificial Intelligence,", "citeRegEx": "Leckie,? \\Q1998\\E", "shortCiteRegEx": "Leckie", "year": 1998}, {"title": "Learning generalized policies in planning using concept languages", "author": ["M. Martin", "H. Geffner"], "venue": "In International Conference on Artificial Intelligence Planning Systems,", "citeRegEx": "Martin and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Martin and Geffner", "year": 2000}, {"title": "Learning generalized policies from planning examples using concept languages", "author": ["M. Martin", "H. Geffner"], "venue": "Appl. Intell,", "citeRegEx": "Martin and Geffner,? \\Q2004\\E", "shortCiteRegEx": "Martin and Geffner", "year": 2004}, {"title": "Taxonomic syntax for first order inference", "author": ["D. Mcallester", "R. Givan"], "venue": "Journal of the ACM,", "citeRegEx": "Mcallester and Givan,? \\Q1989\\E", "shortCiteRegEx": "Mcallester and Givan", "year": 1989}, {"title": "A heuristic estimator for means-ends analysis in planning", "author": ["D. McDermott"], "venue": "In Proceedings of the 3rd Conference on Artificial Intelligence Planning Systems (AIPS),", "citeRegEx": "McDermott,? \\Q1996\\E", "shortCiteRegEx": "McDermott", "year": 1996}, {"title": "Quantitative results concerning the utility of explanation-based learning", "author": ["S. Minton"], "venue": "Artif. Intell.,", "citeRegEx": "Minton,? \\Q1990\\E", "shortCiteRegEx": "Minton", "year": 1990}, {"title": "Inverse entailment and progol", "author": ["S. Muggleton"], "venue": "New Generation Computing,", "citeRegEx": "Muggleton,? \\Q1995\\E", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L. De Raedt"], "venue": "Journal of Logic Programming,", "citeRegEx": "Muggleton and Raedt,? \\Q1994\\E", "shortCiteRegEx": "Muggleton and Raedt", "year": 1994}, {"title": "SHOP2: An HTN planning system", "author": ["D. Nau", "Au", "T.-C", "O. Ilghami", "U. Kuter", "W. Murdock", "D. Wu", "F. Yaman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Nau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nau et al\\.", "year": 2003}, {"title": "Learning macro-actions for arbitrary planners and domains", "author": ["M.A.H. Newton", "J. Levine", "M. Fox", "D. Long"], "venue": "In Proceedings of the 17th International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Newton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newton et al\\.", "year": 2007}, {"title": "Induction of decision trees", "author": ["J. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "The LAMA planner: Guiding cost-based anytime planning with landmarks", "author": ["S. Richter", "M. Westphal"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Richter and Westphal,? \\Q2010\\E", "shortCiteRegEx": "Richter and Westphal", "year": 2010}, {"title": "The more, the merrier: Combining heuristic estimators for satisficing planning", "author": ["G. R\u00f6ger", "M. Helmert"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "R\u00f6ger and Helmert,? \\Q2010\\E", "shortCiteRegEx": "R\u00f6ger and Helmert", "year": 2010}, {"title": "Integrating planning and learning: The PRODIGY", "author": ["M. Veloso", "J. Carbonell", "A. P\u00e9rez", "D. Borrajo", "E. Fink", "J. Blythe"], "venue": "architecture. JETAI,", "citeRegEx": "Veloso et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Veloso et al\\.", "year": 1995}, {"title": "A lookahead strategy for heuristic search planning", "author": ["V. Vidal"], "venue": "In Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Vidal,? \\Q2004\\E", "shortCiteRegEx": "Vidal", "year": 2004}, {"title": "Discriminative learning of beam-search heuristics for planning", "author": ["Y. Xu", "A. Fern", "S.W. Yoon"], "venue": "IJCAI", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Learning linear ranking functions for beam search with application to planning", "author": ["Y. Xu", "A. Fern", "S. Yoon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2009}, {"title": "Iterative learning of weighted rule sets for greedy search", "author": ["Y. Xu", "A. Fern", "S. Yoon"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Learning heuristic functions from relaxed plans", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "In Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "Yoon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2006}, {"title": "Using learned policies in heuristic-search planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "In Proceedings of the 20th IJCAI", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}, {"title": "Learning control knowledge for forward search planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Yoon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2008}, {"title": "Learning-assisted automated planning: looking back, taking stock, going forward", "author": ["T. Zimmerman", "S. Kambhampati"], "venue": "AI Magazine, 24,", "citeRegEx": "Zimmerman and Kambhampati,? \\Q2003\\E", "shortCiteRegEx": "Zimmerman and Kambhampati", "year": 2003}], "referenceMentions": [{"referenceID": 39, "context": "Zimmerman and Kambhampati (2003) made a comprehensive survey of ML for defining DCK.", "startOffset": 0, "endOffset": 33}, {"referenceID": 38, "context": "Consequently, ILP algorithms have been used by heuristic planners to capture DCK in different forms such as decision rules to select actions in the different planning context or regression rules to obtain better node evaluations (Yoon et al., 2008).", "startOffset": 229, "endOffset": 248}, {"referenceID": 38, "context": "An effective way of dealing with this problem in heuristic planners is integrating the learned DCK within robust strategies such as a Best-First Search (Yoon et al., 2008) or combining it with domain-independent heuristic functions (R\u00f6ger & Helmert, 2010).", "startOffset": 152, "endOffset": 171}, {"referenceID": 22, "context": "The idea of delete-relaxation for computing heuristics in planning was first introduced by McDermott (1996) and by Bonet, Loerincs and Geffner (1997).", "startOffset": 91, "endOffset": 108}, {"referenceID": 22, "context": "The idea of delete-relaxation for computing heuristics in planning was first introduced by McDermott (1996) and by Bonet, Loerincs and Geffner (1997). The relaxed plan is extracted from a relaxed planning graph, which is a sequence of facts and actions layers (F0, A0, .", "startOffset": 91, "endOffset": 150}, {"referenceID": 28, "context": "The most common way to build these trees is following the Top-Down Induction of Decision Trees (TDIDT) algorithm (Quinlan, 1986).", "startOffset": 113, "endOffset": 128}, {"referenceID": 24, "context": "This tool implements a relational version of the TDIDT algorithm, although any other off-the-shelf tool for learning relational classifiers could have been used, such as PROGOL (Muggleton, 1995) or RIBL (Emde & Wettschereck, 1996).", "startOffset": 177, "endOffset": 194}, {"referenceID": 8, "context": "In this paper we use the tool TILDE (Blockeel & De Raedt, 1998) for learning the operator and binding classifiers. This tool implements a relational version of the TDIDT algorithm, although any other off-the-shelf tool for learning relational classifiers could have been used, such as PROGOL (Muggleton, 1995) or RIBL (Emde & Wettschereck, 1996). Each of these different learning algorithms would provide different results, since they explore the classifiers space differently. The study of the pros and cons of the different algorithms is beyond the scope of the paper. For a comprehensive explanation of current relational learning approaches please refer to the work by De Raedt (2008).", "startOffset": 51, "endOffset": 689}, {"referenceID": 38, "context": "A successful example is the ObtuseWedge system (Yoon et al., 2008) that combined a learned generalized policy with the FF heuristic.", "startOffset": 47, "endOffset": 66}, {"referenceID": 32, "context": "were first applied in heuristic planning by the YAHSP planner (Vidal, 2004).", "startOffset": 62, "endOffset": 75}, {"referenceID": 32, "context": "In general, the use of lookahead states in a forward state-space search slightly increases the branching factor of the search process, but overall, as shown by the YAHSP planner at IPC-2004 and in the experiments included in the YAHSP paper (Vidal, 2004), the approach seems to improve the performance significantly.", "startOffset": 241, "endOffset": 254}, {"referenceID": 32, "context": "Another technique for prioritizing helpful actions in BFS was implemented in YAHSP (Vidal, 2004) which inserts two consecutive instances of each node in the open list.", "startOffset": 83, "endOffset": 96}, {"referenceID": 38, "context": "ROLLER achieves shorter Learning Times, fourth column in Table 1, than the state-of-the-art systems for learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008).", "startOffset": 134, "endOffset": 177}, {"referenceID": 17, "context": "One approach consisted of including recursive definitions of new predicates, such as the support predicates above(X,Y) and inplace(X) (Khardon, 1999).", "startOffset": 134, "endOffset": 149}, {"referenceID": 38, "context": ", the operator for defining recursion) was discarded in a subsequent work (Yoon et al., 2008) and the above predicate was used instead.", "startOffset": 74, "endOffset": 93}, {"referenceID": 38, "context": "A similar approach was followed by the winner of the best learner award, OBTUSE WEDGE (Yoon et al., 2008).", "startOffset": 86, "endOffset": 105}, {"referenceID": 31, "context": "Our approach is strongly inspired by the way Prodigy (Veloso et al., 1995) models DCK.", "startOffset": 53, "endOffset": 74}, {"referenceID": 23, "context": "Both selections can be guided by DCK in the form of control rules (Leckie & Zukerman, 1998; Minton, 1990).", "startOffset": 66, "endOffset": 105}, {"referenceID": 38, "context": "Previous works on learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008) succeed in addressing these two limitations of RRL.", "startOffset": 48, "endOffset": 91}, {"referenceID": 23, "context": "However, this benefit decreases with the number of new macro-actions added because they enlarge the branching factor of the search tree causing the utility problem (Minton, 1990).", "startOffset": 164, "endOffset": 178}, {"referenceID": 32, "context": "Vidal et al. (2010) consider this as an optimization problem and use a specialized optimization algorithm to discover good decompositions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 33, "context": "A different approach is followed by Xu et al. (2010) who generate training examples from partially ordered plans.", "startOffset": 36, "endOffset": 53}, {"referenceID": 17, "context": "As opposed to previous works that represent generalized policies in predicate logic (Khardon, 1999), our representation does not need extra background knowledge (support predicates) to learn efficient policies for the Blocksworld domain.", "startOffset": 84, "endOffset": 99}, {"referenceID": 16, "context": "One possible direction is extending the helpful context with subgoal information such as landmarks (Hoffmann et al., 2004) of the relaxed plan.", "startOffset": 99, "endOffset": 122}, {"referenceID": 13, "context": "In the near future, we plan to explore how the learner can generate the most convenient distribution of training examples according to a target planning task as proposed by Fuentetaja and Borrajo (2006).", "startOffset": 173, "endOffset": 203}], "year": 2011, "abstractText": "Current evaluation functions for heuristic planning are expensive to compute. In numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. However, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. Particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. These planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. This paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead states within a Best First Search algorithm. Experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.", "creator": "TeX"}}}