{"id": "1401.3886", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference", "abstract": "Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations---two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisy-OR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.", "histories": [["v1", "Thu, 16 Jan 2014 05:15:08 GMT  (291kb)", "http://arxiv.org/abs/1401.3886v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wei li", "pascal poupart", "peter van beek"], "accepted": false, "id": "1401.3886"}, "pdf": {"name": "1401.3886.pdf", "metadata": {"source": "CRF", "title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference", "authors": ["Wei Li", "Pascal Poupart", "Peter van Beek"], "emails": ["wei.li@autodesk.com", "ppoupart@cs.uwaterloo.ca", "vanbeek@cs.uwaterloo.ca"], "sections": [{"heading": "1. Introduction", "text": "It is a basic building block of many AI applications. A Bayesian network consists of a controlled acyclic graph in which the nodes represent the random variables and each node is labeled with a conditional probability table (CPT) representing the strengths of the parent node networks on the child node (Pearl, 1988). Generally, random variables with domain size d, the CPT of a child node with n-parents are used, which is a practical difficulty and has led to the introduction of patterns for CPTs that require specification of many fewer parameters (e.g. Good, 1961; Pearl, 1988; D-Iez and Druzdzel, 2006).Perhaps the most common patterns in practice are the noisy-OR relationship and its generalization, the noisy-MAX-MAX relationship (Good, 1961; Pearl, 1988)."}, {"heading": "2. Background", "text": "In this section, we examine the noisy-OR / MAX relationships and the background required for weighted model counting approaches for exact inference in Bayesian networks (for more on these topics, see for example Koller & Friedman, 2009; Darwiche, 2009; Chavira & Darwiche, 2008)."}, {"heading": "2.1 Patterns for CPTs: Noisy-OR and Noisy-MAX", "text": "With the noisy-OR relationship, one assumes that there are different causes X1, = 1., Xn leads to an effect Y = 1. (see Figure 1), assuming all random variables as booleanrated domains. Any cause Xi is either present or not present, and any Xi in isolation is likely to cause Y, and the probability is not reduced if more than one cause is present. Furthermore, one assumes that all possible causes are given, and if all causes are missing, the effect is absent. Finally, one assumes that the mechanism or reason why Xi prevents Y from causing Y is independent of the mechanism or reason that causes an Xj, j = i, causes X. A noisy-OR relationship specifies a CPT with n parameters, q1,., one for each parent, where qi is the probability that Y is wrong, because Xi is true and all other parents are wrong."}, {"heading": "2.2 Weighted Model Counting for Probabilistic Inference", "text": "In fact, most of them will be able to retaliate, and they will be able to retaliate and retaliate."}, {"heading": "3. Related Work", "text": "In this section, we refer to previously proposed methods for accurate conclusions in Bayesian networks, which include a variety of probability calculations. However, one method of solving such networks is to replace each noisy-OR / MAX with its full CPT representation, and then use any of the known methods to answer probable questions such as variable elimination or tree clustering / joint tree. However, in general - and especially for the networks we use in our experimental evaluation - this method is impractical. A more fruitful approach to solving such networks is to use the structure or semantics of noisy-OR / MAX relationships to improve both time and space efficiency (e.g. Heckerman, 1989; Olesen et al., 1989; D'Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996."}, {"heading": "4. Efficient Encodings of Noisy-OR into CNF", "text": "In this section, we present techniques to improve the weighted model counting approach for Bayesian networks with noisy-OR relationships. In particular, we present two CNF encodings for noisy-OR relationships that use their structure or semantics. For the noisy-OR relationship, we use the Boolean domains to simplify encodings."}, {"heading": "4.1 Weighted CNF Encoding 1: An Additive Encoding", "text": "s introduce a relationship to node Y (see Figure 1) in which all random variables for each parent of the Y formula are assumed. (WMC1) In our first weighted model, we will introduce an indicator IY for each parent of the Y formula. (WMC1) The weights of these variables are as follows: Weight (IXi) = Weight (IY) = Weight (Pqi) = 1 Weight (Pqi) = 1 Weight (Pqi) = 1 Weight (see Equation 1).The weights of these variables are as follows: Weight (IXi) = Weight (IY) = Weight (Pqi) = 1 \u2212 Qi (Qi) = Weight (Qi).The noisy-OR relationship can then be encoded as a formula. (IX1)"}, {"heading": "4.2 Weighted CNF Encoding 2: A Multiplicative Encoding", "text": "Again, there are causes X1,. Xn leads to an effect Y = Y and creates a Y relationship (see Figure 1) in which all random variables are assumed to have a Y value domain. Our second weighted model encoding method (WMC2) takes as its starting point D weight and Gala weight n's (2003) directed auxiliary diagram transformation of a Xi-Xi network with a noisy-OR / MAX relationship. D-ez and Gala n note that for the noisy-OR relationship equation (6) can be presented as a product of the matrices. (P-Y = 0-1) (P-1) (Y-X) -X) P (Y-X-X) P (Y-O) relationship. Based on this factorization, a noisy-OR network can be integrated into a regular Bayesian network by introducing a hidden node for each Y conversion."}, {"heading": "5. Efficient Encodings of Noisy-MAX into CNF", "text": "In this section we present techniques to improve weighted model calculation for Bayesian networks with multiple values."}, {"heading": "5.1 Weighted CNF Encoding 1 for Noisy-MAX", "text": "Our first weighted model count for noisy-MAX relationships (MAX1) is based on an additive definition of noisy-MAX = 14. Remember the dissected probability model for the noisy-MAX relationship, which is discussed at the end of Section 2.1. It can be shown that for noisy-MAX, P (Y-MAX) = IC (Y-MAX) = ICs (Y-X1) = ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs, ICs"}, {"heading": "5.2 Weighted CNF Encoding 2 for Noisy-MAX", "text": "Our second weighted model for encoding the factorization table MY has been generally explained and is based on a multiplicative definition of noisy-MAX. Equation 5 states that P (Y \u2264 y-X1,. Xn) can be used as multiplicative encoding, P (Y \u2264 y-X) as multiplicative encoding. Substitution of the above equation results in P (Y = y-X1,. Xn) as outer operator being multiplication, so we refer to MAX2 as multiplicative encoding. The above equation results in P (Y = y-X1,.) It is this equation that we encode in CNF. The coding of the factorization table MY is generally explained."}, {"heading": "6. Experimental Evaluation", "text": "In this section we evaluate empirically the effectiveness of our encodings. We use the cachet solver2 in our experiments, as it is one of the fastest weighted models to count the solvers. We compare it against Ace (version 2) (Chavira et al., 2005) and D \"iez\" n \"s (2003), as we decided against Ace for two reasons. Firstly, Ace prevailed in the 2008 exact inference competition (no winner was declared, but Ace has better classes of problems than all other interventions). Secondly, other methods that are publicly available or that participated well in the competition, such as Smile / GeNIe, 2005) or Cachet with a general encoding on the full CPT representation, we currently take no compatible advantages of noisy-OR and noisy-MAX algorithms."}, {"heading": "6.1 Experiment 1: Random Two-Layer Networks", "text": "In our first set of experiments, we used randomly generated two-layer networks to compare the time and space efficiency of the WMC1 and WMC2 encodings, and both the WMC1 and WMC2 encodings can answer probable queries with Equation 7. Both encodings result in fast factorization based on evidence during encoding, and the pauses from negative evidence can be represented compactly in the resulting CNF, even for a large number of parents. In the WMC2 encoding, positive evidence can be represented by only three Boolean variables (see Example 9 for an illustration of which variables will be deleted and which will be retained in the event of positive evidence), while the WMC1 encoding requires n Boolean variables, one for each parent (see Example 7). In the WMC2 encoding, we use two parameter variables (P 0Xi, Y, and Xi \u2032 1 \u2032 1, Xi \u2032 1 \u2032, to provide each arc, during WC1)."}, {"heading": "30 3686 10 0.2 30 6590 11 0.1 30 31.7 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "35 3716 11 0.6 30 6605 11 0.2 30 32.5 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "40 3746 13 21.4 30 6620 11 0.5 30 32.7 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "45 3776 14 38.8 30 6635 13 2.0 30 35.7 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "50 3806 19 75.3 30 6650 13 6.1 30 40.9 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "55 3836 22 175.2 30 6665 16 71.0 30 166.0 30", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "60 3916 24 17 6680 16 27 21", "text": "The first column shows the amount of positive evidence in the symptom variables, while the other evidence variables are negative symptoms. It can be seen that while the WMC1 encoding generates fewer variables than the WMC2 encoding, the evidence generated by the WMC2 encoding has a smaller breadth. Evidence probability (PE) is calculated using the tree decomposition variable order (Huang & Darwiche, 2003) and the results are compared with Ace3 (a more detailed experimental analysis will be given in the next experiments)."}, {"heading": "6.2 Experiment 2: QMR-DT", "text": "In the second half of the year, the number of unemployed in the US will increase by more than 20%, while the number of unemployed in the US will decrease by 20%."}, {"heading": "6.3 Experiment 3: Random Multi-Layer Networks", "text": "In fact, most of us are able to play by the rules we set ourselves, \"he said in an interview with the German Press Agency.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules we set ourselves. \""}, {"heading": "6.4 Discussion", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on ourselves. (...) It is not as if they are able to understand the rules. (...) It is not as if they are abiding by the rules. (...) It is not as if they are abiding by the rules. (...) It is not as if they are abiding by the rules. (...) It is not as if they are abiding by the rules. (...) It is as if they are abiding by the rules. (...) It is as if they are abiding by the rules. \"(...) It is as if they are abiding by the rules.\" (...) It is as if they are abiding by the rules. (...) It is as if they are abiding by the rules. (...)"}, {"heading": "7. Conclusions and Future Work", "text": "Large graphical models, such as QMR-DT, can often not be accurately derived when there is a large amount of positive evidence. We presented time and space-efficient CNF encodings for noisy-OR / MAX relationships. We also explored alternative search series heuristics for the DPLL-based backtracking algorithm on these encodings. In our experiments, we demonstrated that our techniques collectively extend the model-counting approach for accurate conclusions to networks that were previously insoluble for the approach. While our experimental results must be interpreted with some care, as we compare not only our encodings but also implementations of systems with conflicting design goals, our techniques based on these benchmarks gave accelerations of up to three orders of magnitude over the best earlier approaches and scaled to larger instances."}, {"heading": "Acknowledgments", "text": "A preliminary version of this article was published as: Wei Li, Pascal Poupart, and Peter van Beek. Exploiting Causal Independence Using Weighted Model Counting. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pp. 337-343, 2008. The authors would like to thank the anonymous speakers for their helpful comments."}], "references": [{"title": "DPLL with caching: A new algorithm for #SAT and Bayesian inference", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Electronic Colloquium on Computational Complexity,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Algebraic decision diagrams and their applications", "author": ["R.I. Bahar", "E.A. Frohm", "C.M. Gaona", "G.D. Hachtel", "E. Macii", "A. Pardo", "F. Somenzi"], "venue": "In Proceedings of the 1993 IEEE/ACM International Conference on Computer-Aided Design (ICCAD-93),", "citeRegEx": "Bahar et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bahar et al\\.", "year": 1993}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Exploiting evidence in probabilistic inference", "author": ["M. Chavira", "D. Allen", "A. Darwiche"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Chavira et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chavira et al\\.", "year": 2005}, {"title": "Compiling Bayesian networks with local structure", "author": ["M. Chavira", "A. Darwiche"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Chavira and Darwiche,? \\Q2005\\E", "shortCiteRegEx": "Chavira and Darwiche", "year": 2005}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chavira and Darwiche,? \\Q2008\\E", "shortCiteRegEx": "Chavira and Darwiche", "year": 2008}, {"title": "Symbolic probabilistic inference in large BN2O networks", "author": ["B. D\u2019Ambrosio"], "venue": "In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "D.Ambrosio,? \\Q1994\\E", "shortCiteRegEx": "D.Ambrosio", "year": 1994}, {"title": "Modeling and Reasoning with Bayesian Networks. Cambridge", "author": ["A. Darwiche"], "venue": null, "citeRegEx": "Darwiche,? \\Q2009\\E", "shortCiteRegEx": "Darwiche", "year": 2009}, {"title": "A logical approach to factoring belief networks", "author": ["A. Darwiche"], "venue": "In Proceedings of the Eighth International Conference on Principles of Knowledge Representation and Reasoning", "citeRegEx": "Darwiche,? \\Q2002\\E", "shortCiteRegEx": "Darwiche", "year": 2002}, {"title": "A machine program for theorem proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "Davis et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1962}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "J. ACM,", "citeRegEx": "Davis and Putnam,? \\Q1960\\E", "shortCiteRegEx": "Davis and Putnam", "year": 1960}, {"title": "Parameter adjustement in Bayes networks. The generalized noisy ORgate", "author": ["F.J. D\u0301\u0131ez"], "venue": "In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "D\u0301\u0131ez,? \\Q1993\\E", "shortCiteRegEx": "D\u0301\u0131ez", "year": 1993}, {"title": "Canonical probabilistic models for knowledge engineering", "author": ["F.J. D\u0301\u0131ez", "M.J. Druzdzel"], "venue": "Tech. rep. CISIAD-06-01,", "citeRegEx": "D\u0301\u0131ez and Druzdzel,? \\Q2006\\E", "shortCiteRegEx": "D\u0301\u0131ez and Druzdzel", "year": 2006}, {"title": "Efficient computation for the noisy MAX", "author": ["F.J. D\u0301\u0131ez", "S.F. Gal\u00e1n"], "venue": "International J. of Intelligent Systems,", "citeRegEx": "D\u0301\u0131ez and Gal\u00e1n,? \\Q2003\\E", "shortCiteRegEx": "D\u0301\u0131ez and Gal\u00e1n", "year": 2003}, {"title": "Intelligent decision support systems based on SMILE", "author": ["M.J. Druzdzel"], "venue": "Software 2.0,", "citeRegEx": "Druzdzel,? \\Q2005\\E", "shortCiteRegEx": "Druzdzel", "year": 2005}, {"title": "A causal calculus", "author": ["I.J. Good"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Good,? \\Q1961\\E", "shortCiteRegEx": "Good", "year": 1961}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Heckerman,? \\Q1989\\E", "shortCiteRegEx": "Heckerman", "year": 1989}, {"title": "Causal independence for probability assessment and inference using Bayesian networks", "author": ["D. Heckerman", "J. Breese"], "venue": "IEEE, Systems, Man, and Cyber.,", "citeRegEx": "Heckerman and Breese,? \\Q1996\\E", "shortCiteRegEx": "Heckerman and Breese", "year": 1996}, {"title": "Causal independence for knowledge acquisition and inference", "author": ["D. Heckerman"], "venue": "In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI93)", "citeRegEx": "Heckerman,? \\Q1993\\E", "shortCiteRegEx": "Heckerman", "year": 1993}, {"title": "Some practical issues in constructing belief networks", "author": ["M. Henrion"], "venue": "In Proceedings of the Third Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Henrion,? \\Q1987\\E", "shortCiteRegEx": "Henrion", "year": 1987}, {"title": "A structure-based variable ordering heuristic for SAT", "author": ["J. Huang", "A. Darwiche"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Huang and Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Huang and Darwiche", "year": 2003}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Initial experiments in stochastic satisfiability", "author": ["M.L. Littman"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence", "citeRegEx": "Littman,? \\Q1999\\E", "shortCiteRegEx": "Littman", "year": 1999}, {"title": "Quick medical reference for diagnostic assistance", "author": ["R.A. Miller", "F.E. Masarie", "J.D. Myers"], "venue": "Medical Computing,", "citeRegEx": "Miller et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1986}, {"title": "A MUNIN network for the median nerve: A case study on", "author": ["K.G. Olesen", "U. Kjaerulff", "F. Jensen", "F.V. Jensen", "B. Falck", "S. Andreassen", "S.K. Andersen"], "venue": "loops. Appl. Artificial Intelligence,", "citeRegEx": "Olesen et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Olesen et al\\.", "year": 1989}, {"title": "Using causal knowledge to create simulated patient cases: The CPCS project as an extension of INTERNIST-1", "author": ["R. Parker", "R. Miller"], "venue": "In The 11th Symposium Computer Applications in Medical Care,", "citeRegEx": "Parker and Miller,? \\Q1987\\E", "shortCiteRegEx": "Parker and Miller", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Combining component caching and clause learning for effective model counting", "author": ["T. Sang", "F. Bacchus", "P. Beame", "H. Kautz", "T. Pitassi"], "venue": "In Proceedings of the 7th International Conference on Theory and Applications of Satisfiability Testing (SAT04)", "citeRegEx": "Sang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2004}, {"title": "Solving Bayesian networks by weighted model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Heuristics for fast exact model counting", "author": ["T. Sang", "P. Beame", "H.A. Kautz"], "venue": "In Proceedings of the 8th International Conference on Theory and Applications of Satisfiability Testing", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Efficient ADD operations for point-based algorithms", "author": ["G. Shani", "R.I. Brafman", "S.E. Shimony", "P. Poupart"], "venue": "In Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling", "citeRegEx": "Shani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2008}, {"title": "Multiplicative factorization of noisy-MAX", "author": ["M. Takikawa", "B. D\u2019Ambrosio"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Takikawa and D.Ambrosio,? \\Q1999\\E", "shortCiteRegEx": "Takikawa and D.Ambrosio", "year": 1999}, {"title": "Knowledge engineering for Bayesian networks: How common are noisy-MAX distributions in practice", "author": ["A. Zagorecki", "M.J. Druzdzel"], "venue": "In Proceedings of the 10th European Conference on Artificial Intelligence", "citeRegEx": "Zagorecki and Druzdzel,? \\Q1992\\E", "shortCiteRegEx": "Zagorecki and Druzdzel", "year": 1992}, {"title": "Exploiting causal independence in Bayesian network inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "J. of Artificial Intelligence Research,", "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 26, "context": "A Bayesian network consists of a directed acyclic graph where the nodes represent the random variables and each node is labeled with a conditional probability table (CPT) that represents the strengths of the influences of the parent nodes on the child node (Pearl, 1988).", "startOffset": 257, "endOffset": 270}, {"referenceID": 26, "context": "This presents a practical difficulty and has led to the introduction of patterns for CPTs that require one to specify many fewer parameters (e.g., Good, 1961; Pearl, 1988; D\u0301\u0131ez & Druzdzel, 2006).", "startOffset": 140, "endOffset": 195}, {"referenceID": 15, "context": "Perhaps the most widely used patterns in practice are the noisy-OR relation and its generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988).", "startOffset": 123, "endOffset": 148}, {"referenceID": 26, "context": "Perhaps the most widely used patterns in practice are the noisy-OR relation and its generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988).", "startOffset": 123, "endOffset": 148}, {"referenceID": 6, "context": "A more fruitful approach for solving such networks is to take advantage of the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen, Kjaerulff, Jensen, Jensen, Falck, Andreassen, & Andersen, 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira, Allen, & Darwiche, 2005).", "startOffset": 161, "endOffset": 405}, {"referenceID": 10, "context": "As well, Zagorecki and Druzdzel (1992) show that in three real-world Bayesian networks, noisy-OR/MAX relations were a good fit for up to 50% of the CPTs in these networks and that converting some CPTs to noisy-OR/MAX relations gave good approximations when answering probabilistic queries.", "startOffset": 23, "endOffset": 39}, {"referenceID": 7, "context": "In this section, we review noisy-OR/MAX relations and the needed background on weighted model counting approaches to exact inference in Bayesian networks (for more on these topics see, for example, Koller & Friedman, 2009; Darwiche, 2009; Chavira & Darwiche, 2008).", "startOffset": 154, "endOffset": 264}, {"referenceID": 26, "context": "Similarly, in Pearl\u2019s (1988) decomposed model, one only has to specify n probabilities to fully specify the model (see Figure 4); i.", "startOffset": 14, "endOffset": 29}, {"referenceID": 26, "context": "Figure 4: Pearl\u2019s (1988) decomposed form of the noisy-OR relation.", "startOffset": 10, "endOffset": 25}, {"referenceID": 15, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 19, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 11, "context": "The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; D\u0301\u0131ez, 1993) is a generalization of the noisy-OR to non-Boolean domains.", "startOffset": 23, "endOffset": 80}, {"referenceID": 8, "context": "In particular, exact inference in Bayesian networks can be reduced to the weighted model counting of CNFs (Darwiche, 2002; Littman, 1999; Sang et al., 2005a).", "startOffset": 106, "endOffset": 157}, {"referenceID": 22, "context": "In particular, exact inference in Bayesian networks can be reduced to the weighted model counting of CNFs (Darwiche, 2002; Littman, 1999; Sang et al., 2005a).", "startOffset": 106, "endOffset": 157}, {"referenceID": 24, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 6, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 3, "context": "A more fruitful approach for solving such networks is to take advantage of the structure or the semantics of the noisy-OR/MAX relations to improve both time and space efficiency (e.g., Heckerman, 1989; Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005).", "startOffset": 178, "endOffset": 359}, {"referenceID": 16, "context": "Quickscore (Heckerman, 1989) was the first efficient exact inference algorithm for Booleanvalued two-layer noisy-OR networks.", "startOffset": 11, "endOffset": 28}, {"referenceID": 3, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Zhang & Poole, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003; Chavira et al., 2005). Quickscore (Heckerman, 1989) was the first efficient exact inference algorithm for Booleanvalued two-layer noisy-OR networks. Chavira, Allen and Darwiche (2005) present a method for multi-layer noisy-OR networks and show that their approach is significantly faster than Quickscore on randomly generated two-layer networks.", "startOffset": 123, "endOffset": 307}, {"referenceID": 6, "context": "Many alternative methods have been proposed to decompose a noisy-OR/MAX by adding hidden or auxiliary nodes and then solving using adaptations of variable elimination or tree clustering (e.g., Olesen et al., 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003).", "startOffset": 186, "endOffset": 307}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators.", "startOffset": 8, "endOffset": 124}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators.", "startOffset": 8, "endOffset": 470}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering.", "startOffset": 8, "endOffset": 772}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR.", "startOffset": 8, "endOffset": 1189}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach.", "startOffset": 8, "endOffset": 1559}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach. The complexity of their approach is O(max(2d, nd2)). However, Takikawa and D\u2019Ambrosio\u2019s approach allows more efficient elimination orderings in the variable elimination algorithm, while D\u0301\u0131ez\u2019s method enforces more restrictions on the orderings. More recently, D\u0301\u0131ez and Gal\u00e1n (2003) proposed a multiplicative factorization that improves on this previous work, as it has the advantages of both methods.", "startOffset": 8, "endOffset": 1901}, {"referenceID": 6, "context": ", 1989; D\u2019Ambrosio, 1994; Heckerman & Breese, 1996; Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003). Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two parents. Heckerman (1993) presented a sequential decomposition method again based on adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary nodes and similarly sized CPTs. However, as Takikawa and D\u2019Ambrosio (1999) note, using either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network\u2014depending on how the causes are ordered\u2014and the efficiency of query answering can vary exponentially when using variable elimination or tree clustering, depending on the particular query and the choice of ordering. To take advantage of causal independence models, D\u0301\u0131ez (1993) proposed an algorithm for the noisy-MAX/OR. By introducing one auxiliary variable Y \u2032, D\u0301\u0131ez\u2019s method leads to a complexity of O(nd2) for singly connected networks, where n is the number of causes and d is the size of the domains of the random variables. However, for networks with loops it needs to be integrated with local conditioning. Takikawa and D\u2019Ambrosio (1999) proposed a similar multiplicative factorization approach. The complexity of their approach is O(max(2d, nd2)). However, Takikawa and D\u2019Ambrosio\u2019s approach allows more efficient elimination orderings in the variable elimination algorithm, while D\u0301\u0131ez\u2019s method enforces more restrictions on the orderings. More recently, D\u0301\u0131ez and Gal\u00e1n (2003) proposed a multiplicative factorization that improves on this previous work, as it has the advantages of both methods. We use their auxiliary graph as the starting point for the remaining three of our CNF encodings (WMC2, MAX1, and MAX2). In our experiments, we perform a detailed empirical comparison of their approach using variable elimination against our proposals on large Bayesian networks. In our work, we build upon the DPLL-based weighted model counting approach of Sang, Beame, and Kautz (2005a). Their general encoding assumes full CPTs and yields a parameter clause for each CPT parameter.", "startOffset": 8, "endOffset": 2407}, {"referenceID": 27, "context": "For those nodes with full CPTs, s determines the correct entry in each CPT by Lemma 2 in Sang et al. (2005a) and for those nodes with noisy-ORs, s determines the correct probability by Lemma 1 above.", "startOffset": 89, "endOffset": 109}, {"referenceID": 22, "context": "As Sang et al. (2005a) note, the weighted model counting approach supports queries and evidence in arbitrary propositional form and such queries are not supported by any other exact inference method.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": "Our WMC1 encoding for noisy-OR is essentially similar to a more indirect but also more general proposal by Chavira and Darwiche (2005) (see Darwiche, 2009, pp.", "startOffset": 107, "endOffset": 135}, {"referenceID": 3, "context": "Third, in our encoding there are a linear number of clauses conditioned on IY whereas in the Chavira et al. encoding there are 2n\u22121 clauses. We note, however, that Chavira, Allen, and Darwiche (2005) discuss a direct translation of a noisy-OR to CNF based on Pearl\u2019s decomposition that is said to compactly represent the noisy-OR (i.", "startOffset": 93, "endOffset": 200}, {"referenceID": 11, "context": "Our second weighted model encoding method (WMC2) takes as its starting point D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with a noisy-OR/MAX relation.", "startOffset": 77, "endOffset": 102}, {"referenceID": 11, "context": "Figure 5: D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) transformation of a noisy-OR relation applied to the Bayesian network shown in Figure 2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 11, "context": "As with WMC2, these two noisy-MAX encodings take as their starting point D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) directed auxiliary graph transformation of a Bayesian network with noisy-OR/MAX.", "startOffset": 73, "endOffset": 98}, {"referenceID": 3, "context": "We compare against Ace (version 2) (Chavira et al., 2005) and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 35, "endOffset": 57}, {"referenceID": 14, "context": "Second, other methods that are publicly available or that did well at the competition, such as Smile/GeNIe (Druzdzel, 2005) or Cachet using a general encoding on the full CPT representation, currently do not take any computational advantage of noisy-OR and noisy-MAX and thus would be \u201cstraw\u201d algorithms.", "startOffset": 107, "endOffset": 123}, {"referenceID": 3, "context": "We compare against Ace (version 2) (Chavira et al., 2005) and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 36, "endOffset": 87}, {"referenceID": 9, "context": "We chose to compare against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach, which consists of variable elimination applied to an auxiliary network that permits exploitation of causal independence, as they show that the approach is more efficient than previous proposals for noisy-MAX.", "startOffset": 28, "endOffset": 53}, {"referenceID": 11, "context": "Effect of amount of positive symptoms on the time to answer probability of evidence queries, for the WMC1 encoding and the DTree variable ordering heuristic, the WMC1 encoding and the VSADS variable ordering heuristic, the WMC2 encoding and the DTree variable ordering heuristic, and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 284, "endOffset": 309}, {"referenceID": 11, "context": "To test randomly generated multi-layer networks, we constructed a set of acyclic Bayesian networks using the same method as D\u0301\u0131ez and Gal\u00e1n (2003): create n binary variables; randomly select m pairs of nodes and add arcs between them, where an arc is added from Xi to Xj if i < j; and assign a noisy-OR distribution or a noisy-MAX distribution to each node with parents.", "startOffset": 124, "endOffset": 147}, {"referenceID": 11, "context": "Effect of number of hidden variables on average time to answer probability of evidence queries, for the WMC1 encoding and the VSADS variable ordering heuristic, the WMC1 encoding and the DTree variable ordering heuristic, and D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination.", "startOffset": 226, "endOffset": 251}, {"referenceID": 27, "context": "Sang et al.\u2019s (2005b) VSADS heuristic, which combines both conflict analysis and literal counting, avoids this pitfall and can be seen to work very well on these large Bayesian networks with large amounts of evidence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Effect of number of arcs on average time to answer probability of evidence queries, for the MAX1 encoding for noisy-MAX, the MAX2 encoding for noisy-MAX, and Chavira, Allen, and Darwiche\u2019s Ace (2005).", "startOffset": 178, "endOffset": 200}, {"referenceID": 6, "context": "It can be seen that on these instances our CNF encoding MAX2 out performs our encoding MAX1 and significantly outperforms Chavira, Allen, and Darwiche\u2019s Ace (2005). It has been recognized that for noisy-MAX relations, the multiplicative factorization has significant advantages over the additive factorization (Takikawa & D\u2019Ambrosio, 1999; D\u0301\u0131ez & Gal\u00e1n, 2003).", "startOffset": 142, "endOffset": 164}, {"referenceID": 3, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al., 2005).", "startOffset": 119, "endOffset": 141}, {"referenceID": 6, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 3, "context": "We also compared against D\u0301\u0131ez and Gal\u00e1n\u2019s (2003) approach using variable elimination (hereafter, D&G) and against Ace (Chavira et al., 2005). In our experiments, our approach dominated D&G and Ace with speedups of up to three orders of magnitude. As well, our approach could solve many instances which D&G and Ace could not solve within the resource limits. However, our results should be interpreted with some care for at least three reasons. First, it is well known that the efficiency of variable elimination is sensitive to the variable elimination heuristic that is used and to how it is implemented. While we were careful to optimize our implementation and to use a high-quality heuristic, there is still the possibility that a different implementation or a different heuristic would lead to different results. Second, Cachet, which is based on search, is designed to answer a single query and our experiments are based on answering a single query. However, Ace uses a compilation strategy which is designed to answer multiple queries efficiently. The compilation step can take a considerable number of resources (both time and space) which does not payoff in our experimental design. Third, although Ace can be viewed as a weighted model counting solver, we are not comparing just encodings in our experiments. As Chavira and Darwiche (2008) note, Cachet and Ace differ in many ways including using different methods for decomposition, variable splitting, and caching.", "startOffset": 120, "endOffset": 1350}], "year": 2011, "abstractText": "Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations\u2014 two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisyOR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.", "creator": "gnuplot 4.4 patchlevel 2"}}}