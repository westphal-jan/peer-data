{"id": "1401.3907", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Policy Invariance under Reward Transformations for General-Sum Stochastic Games", "abstract": "We extend the potential-based shaping method from Markov decision processes to multi-player general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. The property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game.", "histories": [["v1", "Thu, 16 Jan 2014 05:22:56 GMT  (89kb)", "http://arxiv.org/abs/1401.3907v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["xiaosong lu", "howard m schwartz", "sidney n givigi jr"], "accepted": false, "id": "1401.3907"}, "pdf": {"name": "1401.3907.pdf", "metadata": {"source": "CRF", "title": "Policy Invariance under Reward Transformations for General-Sum Stochastic Games", "authors": ["Xiaosong Lu", "Howard M. Schwartz"], "emails": ["LUXIAOS@SCE.CARLETON.CA", "SCHWARTZ@SCE.CARLETON.CA", "SIDNEY.GIVIGI@RMC.CA"], "sections": [{"heading": "1. Introduction", "text": "Delayed reward will lead to difficulties in distributing credits or penalties for each action from a long sequence of actions, and this will cause the algorithm to learn slowly. An example of this problem can be found in some episodic tasks, such as a football game in which the player receives only one credit or penalty after scoring a goal. If the number of states in the football game is large, it will take a long time for a player to learn his equilibrium policy. Further development is a technique to improve the learning performance of an enhancement teacher by introducing environmental rewards (Gullapalli & Barto, 1992; Mataric, 1994). If the state space is large, the reward slows down learning dramatically. To accelerate learning, the learner can apply rewards to the environment as a complement to delayed reward."}, {"heading": "2. Framework of Stochastic Games", "text": "Stochastic games were first introduced by Shapley (1953). In a stochastic game, players select the common action and move from one state to another based on the common action they choose. In this section, within the framework of stochastic games, we present Markov decision-making processes, matrix games and stochastic games."}, {"heading": "2.1 Markov Decision Processes", "text": "A Markov decision-making process is a tuple (S, A, T, \u03b3, R) in which S = S = S = S = S = J = J = J = J = J = J = J = J J = J = J = J = J = J J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J ="}, {"heading": "2.2 Matrix Games", "text": "A matrix game is a tuple (n, A1,.., An, R1,., Rn) in which n is the number of players, Ai (i = 1,.., n) is the game strategy for player i and Ri: A1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Ash (A), where n is the number of players, Ai (i = 1,.., n) is the game strategy for player i and Ri: A1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Ash (A) is the payout function for player i. A matrix game is a game in which several players and a single state are involved. Each player i (i = 1,.,.,., n) selects an action from his Ai action set and receives a reward. The payout function of player i is determined by all players who are involved in the common action space A1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 A game with two players, we can set up a matrix, each element of which becomes a payoff function for each player (If we have one of the two action pairs)."}, {"heading": "2.3 Stochastic Games", "text": "For a game with more than one player and several states, we define a stochastic game (or Markov game) as the combination of Markov decision-making processes and matrix games. A stochastic game is a tuple (n, S, A1,.., Rn) in which n is the number of players, T: S \u00d7 A1 \u00b7 \u00b7 \u00b7 An \u00d7 S \u2192 [0,1] is the transition function, Ai (i = 1,.., n) is the action set for the player i, [0,1] is the discount factor, and Ri: S \u00d7 A1 \u00b7 \u00b7 \u00b7 \u00b7 R is the reward function for players i. The transition function in a stochastic game is a probability distribution in the next states."}, {"heading": "2.4 Multi-Player General-Sum Stochastic Games", "text": "If we know the reward function and transition function in the game, we may consider the balance in a stochastic game as a tuple of n strategies (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes\") (\"ashes) (\" ashes) (\") (\" ashes) (\") (\" ashes) (\") (\" ashes) (\") (\" ashes) (\") (\" (\"ashes) (\") (\"ashes) (\") (\"ashes) (\") (\"ashes) (\") (\"(\" ashes) (\") (\" ashes \"(\") \"(\") \"(\") \"(\" (\"ashes\") \"(\") \"(\" (\")\" (\")\" (\")\" (\"(\") \"(\") \"(\") \"(\" (\")\" (\")\" (\")\" (\"(\") \"(\") \"(\") \"(\" (\")\" (\"(\" (\")\" (\")\" (\"(\") \"(\") \"(\" (\")\" (\"(\") \"(\" (\")\" (\"(\") \"(\") \"(\" (\")\" (\")\" (\"(\" (\")\" (\"(\") \"(\" (\")\" (\")\" (\")\" (\"(\") \"(\") \"(\" (\")\" (\"(\" (\"(\") \")\" (\"(\" (\")\" (\"(\") \")\" (\")\" (\"(\" (\")\" (\"(\") \"(\" (\")\" (\"(\") (\")\" (\"(\") (\")\" (\""}, {"heading": "3. Potential-Based Shaping in General-Sum Stochastic Games", "text": "Combining the creative reward with the original reward can improve the learning performance of an enhanced learning algorithm and accelerate convergence to the optimal policy. In our research, we extend the potential-based design method of Markov decision-making processes to multiplayer games. We prove that the Nash equilibria under the potential-based design method will be the Nash equilibria transformation for the original game within the overall sum of stochastic games. We define a potential-based design method."}, {"heading": "4. Conclusion", "text": "In this article, we extend the potential-based formation method to general-sum stochastic games. We prove that the proposed potential-based formation reward applied to a general-sum stochastic game will not alter the original Nash balance of the game. Analysis in this article has the potential to improve player learning performance in a stochastic game."}], "references": [{"title": "Potential-based shaping in model-based reinforcement learning", "author": ["J. Asmuth", "M.L. Littman", "R. Zinkov"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,", "citeRegEx": "Asmuth et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2008}, {"title": "Social reward shaping in the prisoner\u2019s dilemma", "author": ["M. Babes", "E. Munoz de Cote", "M.L. Littman"], "venue": "In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "Babes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2008}, {"title": "Dynamic Noncooperative Game Theory", "author": ["T. Ba\u015far", "G.J. Olsder"], "venue": "SIAM Series in Classics in Applied Mathematics 2nd,", "citeRegEx": "Ba\u015far and Olsder,? \\Q1999\\E", "shortCiteRegEx": "Ba\u015far and Olsder", "year": 1999}, {"title": "Dynamic Programming: Deterministic and Stochastic Models", "author": ["D.P. Bertsekas"], "venue": "PrenticeHall, Englewood Cliffs, NJ.", "citeRegEx": "Bertsekas,? 1987", "shortCiteRegEx": "Bertsekas", "year": 1987}, {"title": "Multiagent Learning in the Presence of Agents with Limitations", "author": ["M. Bowling"], "venue": "Ph.D. thesis, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.", "citeRegEx": "Bowling,? 2003", "shortCiteRegEx": "Bowling", "year": 2003}, {"title": "Theoretical considerations of potential-based reward shaping for multi-agent systems", "author": ["S. Devlin", "D. Kudenko"], "venue": "In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Devlin and Kudenko,? \\Q2011\\E", "shortCiteRegEx": "Devlin and Kudenko", "year": 2011}, {"title": "Robot shaping: developing autonomous agents through learning", "author": ["M. Dorigo", "M. Colombetti"], "venue": "Artificial Intelligence,", "citeRegEx": "Dorigo and Colombetti,? \\Q1994\\E", "shortCiteRegEx": "Dorigo and Colombetti", "year": 1994}, {"title": "Shaping as a method for accelerating reinforcement learning", "author": ["V. Gullapalli", "A. Barto"], "venue": "In Proceedings of the 1992 IEEE International Symposium on Intelligent Control,", "citeRegEx": "Gullapalli and Barto,? \\Q1992\\E", "shortCiteRegEx": "Gullapalli and Barto", "year": 1992}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the 11th International Conference on Machine Learning, pp. 157\u2013163.", "citeRegEx": "Littman,? 1994", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Reward functions for accelerated learning", "author": ["M.J. Mataric"], "venue": "Proceedings of the 11th International Conference on Machine Learning.", "citeRegEx": "Mataric,? 1994", "shortCiteRegEx": "Mataric", "year": 1994}, {"title": "Policy invariance under reward transformations: theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "In Proceedings of the 16th International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": "In Proceedings of the 15th International Conference on Machine Learning", "citeRegEx": "Randl\u00f8v and Alstr\u00f8m,? \\Q1998\\E", "shortCiteRegEx": "Randl\u00f8v and Alstr\u00f8m", "year": 1998}, {"title": "Stochastic games", "author": ["L.S. Shapley"], "venue": "Proceedings of the National Academy of Sciences, Vol. 39, pp. 1095\u20131100.", "citeRegEx": "Shapley,? 1953", "shortCiteRegEx": "Shapley", "year": 1953}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Potential-based shaping and Q-value initialization are equivalent", "author": ["E. Wiewiora"], "venue": "Journal of Artificial Intelligence Research, 19, 205\u2013208.", "citeRegEx": "Wiewiora,? 2003", "shortCiteRegEx": "Wiewiora", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Reward shaping is a technique to improve the learning performance of a reinforcement learner by introducing shaping rewards to the environment (Gullapalli & Barto, 1992; Mataric, 1994).", "startOffset": 143, "endOffset": 184}, {"referenceID": 9, "context": "The applications of reward shaping can be found in the literature (Gullapalli & Barto, 1992; Dorigo & Colombetti, 1994; Mataric, 1994; Randl\u00f8v & Alstr\u00f8m, 1998).", "startOffset": 66, "endOffset": 159}, {"referenceID": 6, "context": "Gullapalli and Barto (1992) demonstrated the application of shaping to a key-press task where a robot was trained to press keys on a keyboard.", "startOffset": 0, "endOffset": 28}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior.", "startOffset": 0, "endOffset": 29}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function for a group of mobile robots to learn a foraging task.", "startOffset": 0, "endOffset": 128}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function for a group of mobile robots to learn a foraging task. Randl\u00f8v and Alstr\u00f8m (1998) combined reinforcement learning with shaping to make an agent learn to drive a bicycle to a goal.", "startOffset": 0, "endOffset": 259}, {"referenceID": 14, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008).", "startOffset": 58, "endOffset": 136}, {"referenceID": 10, "context": "In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games.", "startOffset": 107, "endOffset": 124}, {"referenceID": 6, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping reward that can guarantee the policy invariance for a single agent in a Markov decision process (MDP).", "startOffset": 112, "endOffset": 155}, {"referenceID": 6, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping reward that can guarantee the policy invariance for a single agent in a Markov decision process (MDP). Ng et al. proved that the optimal policy keeps unchanged after adding the potential-based shaping reward to an MDP environment. Following Ng et al., Wiewiora (2003) showed that the effects of potential-based shaping can be achieved by a particular initialization of Q-values for agents using Q-learning.", "startOffset": 112, "endOffset": 459}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game.", "startOffset": 0, "endOffset": 304}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game. But there is no theoretical proof of policy invariance under the reward transformation. In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko (2011) was published while this article was under review.", "startOffset": 0, "endOffset": 805}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game. But there is no theoretical proof of policy invariance under the reward transformation. In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko (2011) was published while this article was under review. But Devlin and Kudenko only proved sufficiency based on a proof technique introduced by Asmuth et al. (2008), while we prove both sufficiency and necessity using a different proof technique in this article.", "startOffset": 0, "endOffset": 965}, {"referenceID": 12, "context": "Framework of Stochastic Games Stochastic games were first introduced by Shapley (1953). In a stochastic game, players choose the joint action and move from one state to another state based on the joint action they choose.", "startOffset": 72, "endOffset": 87}, {"referenceID": 3, "context": "For any MDP, there exists a deterministic optimal policy for the player (Bertsekas, 1987).", "startOffset": 72, "endOffset": 89}, {"referenceID": 8, "context": "An example is the soccer game introduced by Littman (Littman, 1994) where an agent on the offensive side must use a probabilistic policy to pass an unknown defender.", "startOffset": 52, "endOffset": 67}, {"referenceID": 4, "context": "In the literature, a solution to a stochastic game can be described as Nash equilibrium strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994).", "startOffset": 150, "endOffset": 180}, {"referenceID": 8, "context": "In the literature, a solution to a stochastic game can be described as Nash equilibrium strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994).", "startOffset": 150, "endOffset": 180}, {"referenceID": 4, "context": ",an) is known for all the states, we can find player i\u2019s Nash equilibrium policy by solving the associated state-specific matrix game (Bowling, 2003).", "startOffset": 134, "endOffset": 149}, {"referenceID": 10, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 14, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 0, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 9, "context": "Potential-Based Shaping in General-Sum Stochastic Games Ng et al. (1999) presented a reward shaping method to deal with the credit assignment problem by adding a potential-based shaping reward to the environment.", "startOffset": 56, "endOffset": 73}, {"referenceID": 0, "context": ", 1999; Wiewiora, 2003; Asmuth et al., 2008). In our research, we extend the potential-based shaping method from Markov decision processes to multi-player stochastic games. We prove that the Nash equilibria under the potential-based shaping reward transformation will be the Nash equilibria for the original game under the framework of general-sum stochastic games. We define a potential-based shaping reward Fi(s,s) for player i as Fi(s,s ) = \u03b3\u03a6i(s)\u2212\u03a6i(s), (9) where \u03a6 : S \u2192 R is a real-valued shaping function and \u03a6(sT ) = 0 for any terminal state sT . We define a multi-player stochastic game as a tuple M = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where S is a set of states, A1, . . . ,An are players\u2019 action sets, T is the transition function, \u03b3 is the discount factor, and Ri(s,a1, . . . ,an,s)(i = 1, . . . ,n) is the reward function for player i. After adding the shaping reward function Fi(s,s) to the reward function Ri(s,a1, . . . ,an,s), we define a transformed multi-player stochastic game as a tuple M\u2032 = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where Ri(i = 1, . . . ,n) is the new reward function given by Ri(s,a1, . . . ,an,s \u2032) = Fi(s,s)+Ri(s,a1, . . . ,an,s). Inspired by Ng et al. (1999)\u2019s proof of policy invariance in an MDP, we prove the policy invariance in a multi-player general-sum stochastic game as follows.", "startOffset": 24, "endOffset": 1201}, {"referenceID": 10, "context": "Similar to Ng et al. (1999)\u2019s proof of necessity, we define \u2206 = Fi(s,s)\u2212 [\u03b3\u03a6i(s)\u2212\u03a6i(s)].", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "Ng et al. (1999) showed that \u03a6i(s) = V \u2217 Mi(s) is a good candidate for improving the player\u2019s learning", "startOffset": 0, "endOffset": 17}], "year": 2011, "abstractText": "We extend the potential-based shaping method from Markov decision processes to multi-player general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. The property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}