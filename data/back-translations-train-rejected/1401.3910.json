{"id": "1401.3910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Topological Value Iteration Algorithms", "abstract": "Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods use reachability analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm, topological value iteration (TVI), detects the structure of MDPs and backs up states based on topological sequences. It (1) divides an MDP into strongly-connected components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm, focused topological value iteration (FTVI), is an extension of TVI. FTVI restricts its attention to connected components that are relevant for solving the MDP. Specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows FTVI to find smaller connected components, thus running faster. We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI also significantly outperforms popular heuristically-informed MDP algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of magnitude. Finally, we characterize the type of domains where FTVI excels --- suggesting a way to an informed choice of solver.", "histories": [["v1", "Thu, 16 Jan 2014 05:24:38 GMT  (3426kb)", "http://arxiv.org/abs/1401.3910v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peng dai", "mausam", "daniel sabby weld", "judy goldsmith"], "accepted": false, "id": "1401.3910"}, "pdf": {"name": "1401.3910.pdf", "metadata": {"source": "CRF", "title": "Topological Value Iteration Algorithms", "authors": ["Peng Dai", "Daniel S. Weld", "Judy Goldsmith"], "emails": ["DAIPENG@CS.WASHINGTON.EDU", "MAUSAM@CS.WASHINGTON.EDU", "WELD@CS.WASHINGTON.EDU", "GOLDSMIT@CS.UKY.EDU"], "sections": [{"heading": null, "text": "To solve this problem, many approaches have been proposed, including ILAO * and variants of RTDP. These methods use accessibility analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches builds the graphical structure of state transitions in a pre-processing step or uses the structural information to systematically decompose a problem, creating an intelligent backup sequence of state space. In this paper, we present two optimal MDP algorithms. The first algorithm, topological value teration (TVI), recognizes the structure of MDPs and secures states based on topological sequences. It (1) splits an MDP into strongly connected components (SCCs) and solves these components sequentially. TVI surpasses VI and other state-of-the-art attention algorithms that focus on multiple components."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Background", "text": "We provide an overview of the Markov Decision Process (MDP) and dynamic programming algorithms that solve an MDP."}, {"heading": "2.1 Markov Decision Processes for Planning", "text": "An MDP is defined as a fivefold reward < S, A, Ap, T, C >, where \"S\" is a finite set of implicit states. \"A\" is a finite set of all applicable measures. \"A\" is a finite set of measures. \"T: S\" P \"(A) is the transitional function that describes the effects of executing measures.\" C: S \"S\" R + is the cost of executing a measure in a state. \"A\" is the amount of power of action. \"T: S\" A \"S\" S \"S\" S \"S\" S \"S\" is the transitional function that describes the effects of executing a measure in a state. \"C\" S \"R + is the cost of executing a measure in a state. The agent executes its actions in discrete time increments. At each step the system is in a particular state s\" S \"S. The agent can execute any measure from a set of applicable measures.\" A. \""}, {"heading": "2.2 Dynamic Programming", "text": "Most optimal MDP algorithms are based on dynamic programming, the usefulness of which was first proven by a simple but powerful algorithm called Value Iteration (Bellman, 1957). Value Iteration initially arbitrarily initializes the value function, for example, all zero. Then, the values are updated iteratively using an operator called Bellman Backup (line 7 of algorithm 1) to create successively better approximations for each state per iteration. We define the Bellman remainder of a state as the absolute difference of a state value before and after a Bellman backup. Value Iteration stops when the value resistation converts. In implementation, it is typically signaled by converting the Bellman error, algorithm 1 (Gauss Seidel) Value Iteration 1: Input: an MDP M = < S, A, Ap, T, C, T: the resistance function."}, {"heading": "2.2.1 HEURISTIC SEARCH", "text": "To improve the effectiveness of dynamic programming, researchers have examined various ideas from traditional heuristic approaches and their usefulness for MDPs (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet & Geffner, 2003b, 2006; McMahan et al., 2005; Smith & Simmons, 2006; Sanner et al., 2009; The basic idea of heuristic search is to consider an action only when it is necessary, which leads to a more conservative backup strategy that helps to a lot of unnecessary backups.We define a heuristic function h: S \u2192 R +, where an estimate of V (s) is possible if it never overestimates the value of a state (s)."}, {"heading": "2.3 A Limitation of Previous Solvers", "text": "A state can be backed up in the pre-order (if it is visited for the first time, the variants of RTDP are), or in the post-order (if it traces the order, it is ILAO *). None of the algorithms uses the graphical structure of an MDP that regulates the complexity of solving a problem (Littman et al., 1995) to determine the order in which the states are solved. Consider a PhD program in any finance department. Figure 1 shows the progress of a doctoral student who regulates the complexity of the solution (Littman et al., 1995)."}, {"heading": "3. Topological Value Iteration", "text": "It is a question of whether and how such a process can occur. (...) It is a question of whether and how such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur. (...) It is a question of whether such a process can occur."}, {"heading": "3.1 Convergence", "text": "If the Bellman operator is a contraction process (Bertsekas, 2001), we have: Theorem 2 Topological Value Iteration guarantees that it converges to a value function with a Bellman error no greater than \u03b4.Proof We first prove that TVI is guaranteed to end in finite time. Since each MDP contains a finite number of states, it contains a finite number of associated states. In solving each of these states, TVI uses a value titeration. Since the value titeration is guaranteed to converge in finite time (given a finite number of states), TVI, which is essentially a finite number of value titerations, ends in finite time. We then prove that TVI is guaranteed to converge to an optimal value function with Bellman errors in most cases. We prove by induction that an MDP contains only one SCC point, then TVI falls with VI, an optimal algorithm."}, {"heading": "3.2 Implementation", "text": "We have made two optimizations in the implementation of TVI: The first is an uninformed accessibility analysis. TVI is not dependent on initial state information, but once this information is given, TVI is able to mark the attainable components and later ignore the unattainable ones in the dynamic programming step. TVI can find the attainable state space by an in-depth search starting at s0, where the overhead is in | S | and | A | linear. It is extremely useful if only a small part of the state space is attainable (e.g. most domains of the 2006 International Planning Competition, see Bonet, 2006). The second optimization consists in using heuristic values Vl (\u00b7) as a starting point. We used the hmin (Bonet & Geffner, 2003b), a permissible heuristic: hmin (s) = 0, if s-G, otherwise hmin (s) = mina-Ap (s) [s, C \u2032 a), + T3mins, T3b (first measure)."}, {"heading": "3.3 Experiments", "text": "We addressed the following questions in our experiments: (1) How does TVI compare with VI and heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most beneficial problem features for TVI? We compared TVI with several other optimal algorithms, including DP (Bellman, 1957), ILAO * (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003a), BRTDP (McMahan et al., 2005), RTDP (Sanner et al., 2009) (BaRTDP), and HDP (Bonet & Geffner, 2003a) 1. We used the fully optimized C code from ILAO * provided by Eric A. Hansen and additionally implemented the rest of the algorithms over the same framework. We ran all experiments on a 2.5 GHz dual-core AMD opteron (tm)."}, {"heading": "4. Focused Topological Value Iteration", "text": "Topological values iteration improves the performance of value iteration most noticeably when an MDP has many equally large strongly connected components. However, we also observe that many MDPs do not have evenly distributed connected components. This is due to the following reason: a state can have many actions, most of which are suboptimal. These suboptimal actions, although not part of an optimal policy, can lead to connectivity between many states. For example, domains such as Blocksworld have reversible actions. Because of these actions, most states are causally dependent on each other. As a result, states that are connected by reversible actions end up forming a large connected component, causing TVI to slow down. On the other hand, heuristic search is a powerful solution technology that successfully focuses on the calculation, in the form of backups, on states and transitions that are more likely part of an optimal policy."}, {"heading": "4.1 The FTVI Algorithm", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able to be able to hide, and that we are able to hide ourselves, \"he said."}, {"heading": "4.2 Implementation", "text": "There are several interesting questions to answer in the implementation. How to calculate the initial upper and lower limit? How many search criteria do we then have to perform automatically in the search? Is it possible that FTVI converges in the search? What if there is still a large component, even after the removal of errors? We used the same lower search criteria level as in TVI (see Section 3.2). For the upper search criteria level we start with a simple upper limit: algorithm 3 focused topological value iteration 1: input: an MDP < S, A, Ap, T, C >, x: the number of search criteria in a row, y: the lower limit of the percentage change in the initial state value: the threshold values-2: {Step 1: Search} 3: {Step 1: do true 4: old values-Vl (s0) 5: do iter-1 to 6-8: do marker state for each."}, {"heading": "4.3 Experiments", "text": "In our experiments, we deal with the following two questions: (1) How does FTVI compare with other algorithms for a wide range of domain problems? (2) What are the specific domains where FTVI should be preferred over heuristic search? We implemented FTVI in the same framework as in Section 3.3, using the same cut-off time of 5 minutes for each algorithm per problem. To test the helpfulness of action management, we also implemented a VI variant that applies action control in backups. We used the same threshold \u03b4 = 10 \u2212 6, and performed BRTDP and BaRTDP on the same upper limit as FTVI."}, {"heading": "4.3.1 RELATIVE SPEED OF FTVI", "text": "We have the possibility that there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process, in which there will be a process. \""}, {"heading": "4.3.2 FACTORS DETERMINING PERFORMANCE", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4.3.3 DISCUSSION", "text": "From the experiments, we learn that FTVI is much better in areas where problems have a low number of target states and a long search depth from the initial state to a destination (such as MCar, SAP, and Drive), but FTVI's Convergence Control Module helps to successfully align the performance of FTVI with the fastest heuristic search algorithm. Furthermore, FTVI has a limited advantage over heuristic search in the two incidents where a problem (1) has many target states, but a long search depth (elevator) or (2) a short depth, but less target states (DAP). In summary, FTVI is our algorithm of choice when a problem has either a small number of target states or a long search depth."}, {"heading": "5. Related Work", "text": "In addition to TVI, several other researchers have proposed splitting an MDP into sub-problems and combining their solutions for the final policy (e.g. the work of Hauskrecht et al. (1998) and Parr (1998), but these approaches typically assume an additional structure of the problem, either known hierarchies or known decomposition into weakly coupled sub-MDPs, etc., whereas FTVI does not assume an additional structure (McMahan et al., 2005), Bayesian RTDP (Sanner et al., 2009), and Focused RTDP (Smith & Simmons, 2006) (FRTDP) also maintain an upper limit for the value function. However, all algorithms use the upper limit to assess how close a state is to convergence by comparing the difference between the upper and lower limits. For example, BRTDP tries to focus more on states whose two limits are less comparable."}, {"heading": "6. Conclusions", "text": "This paper makes several contributions. First, we present two new optimal algorithms for solving MDPs, topological value iteration (TVI) and focused topological value iteration (FTVI). TVI investigates the graphical structure of an MDP by splitting it into strongly connected components and solves the MDP based on the topological order of the components. FTVI expands the topological value iteration algorithm by focusing the construction of strongly connected components on transitions that are likely to belong to an optimal policy. FTVI does this by using a small amount of heuristic searching to eliminate demonstrably suboptimal actions. In contrast to TVI, which does not care about target-state information, FTVI removes transitions that it deems irrelevant to an optimal policy for achieving the goal. In this sense, FTVI builds a much more informative topological structure than TVI. Second, we empirically show that TVI meets state-VI and other MVI state-MDP algorithms when other algorithms meet MVI and MDP-VI are a more complex algorithm."}, {"heading": "Acknowledgments", "text": "This work was done when Peng Dai was a student at the University of Washington. This work was supported by the Office of Naval Research Scholarship N00014-06-1-0147, the National Science Foundation IIS-1016465, ITR-0325063, and the WRF / TJ Cable Professorship. We thank Eric A. Hansen for sharing his code for ILAO * and anonymous reviewers for excellent suggestions to improve the manuscript."}], "references": [{"title": "Decision-Theoretic Military Operations Planning", "author": ["D. Aberdeen", "S. Thi\u00e9baux", "L. Zhang"], "venue": "In Proc. of the 14th International Conference on Automated Planning and Scheduling", "citeRegEx": "Aberdeen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Aberdeen et al\\.", "year": 2004}, {"title": "Learning to act using real-time dynamic programming", "author": ["A. Barto", "S. Bradtke", "S. Singh"], "venue": "Artificial Intelligence J.,", "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "The Complexity of Decentralized Control of Markov Decision Processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Opererations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Faster Heuristic Search Algorithms for Planning with Uncertainty and Full Feedback", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proc. of 18th International Joint Conf. on Artificial Intelligence", "citeRegEx": "Bonet and Geffner,? \\Q2003\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2003}, {"title": "Labeled RTDP: Improving the Convergence of Real-time Dynamic Programming", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proc. 13th International Conference on Automated Planning and Scheduling", "citeRegEx": "Bonet and Geffner,? \\Q2003\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2003}, {"title": "Non-Deterministic Planning Track of the 2006 International Planning Competition.. http://www.ldc.usb.ve/ \u0303bonet/ipc5", "author": ["B. Bonet"], "venue": null, "citeRegEx": "Bonet,? \\Q2006\\E", "shortCiteRegEx": "Bonet", "year": 2006}, {"title": "On the Speed of Convergence of Value Iteration on Stochastic Shortest-Path Problems", "author": ["B. Bonet"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bonet,? \\Q2007\\E", "shortCiteRegEx": "Bonet", "year": 2007}, {"title": "Learning in Depth-First Search: A Unified Approach to Heuristic Search in Deterministic Non-deterministic Settings, and Its Applications to MDPs", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proc. of the 16th International Conference on Automated Planning and Scheduling", "citeRegEx": "Bonet and Geffner,? \\Q2006\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2006}, {"title": "Planning under Continuous Time and Resource Uncertainty: A Challenge for AI", "author": ["J.L. Bresina", "R. Dearden", "N. Meuleau", "S. Ramkrishnan", "D.E. Smith", "R. Washington"], "venue": "In Proc. of 18th Conf. in Uncertainty in AI (UAI-02),", "citeRegEx": "Bresina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bresina et al\\.", "year": 2002}, {"title": "Introduction to Algorithms, Second Edition", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2001}, {"title": "Topological Value Iteration Algorithm for Markov Decision Processes", "author": ["P. Dai", "J. Goldsmith"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Dai and Goldsmith,? \\Q2007\\E", "shortCiteRegEx": "Dai and Goldsmith", "year": 2007}, {"title": "Prioritizing Bellman Backups Without a Priority Queue", "author": ["P. Dai", "E.A. Hansen"], "venue": "In Proc. of the 17th International Conference on Automated Planning and Scheduling", "citeRegEx": "Dai and Hansen,? \\Q2007\\E", "shortCiteRegEx": "Dai and Hansen", "year": 2007}, {"title": "Partitioned External-Memory Value Iteration", "author": ["P. Dai", "Mausam", "D.S. Weld"], "venue": "In AAAI,", "citeRegEx": "Dai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2008}, {"title": "Domain-Independent, Automatic Partitioning for Probabilistic Planning", "author": ["P. Dai", "Mausam", "D.S. Weld"], "venue": "In IJCAI,", "citeRegEx": "Dai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2009}, {"title": "Focused Topological Value Iteration", "author": ["P. Dai", "Mausam", "D.S. Weld"], "venue": "In Proc. of ICAPS,", "citeRegEx": "Dai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2009}, {"title": "Topological Order Planner for POMDPs", "author": ["J.S. Dibangoye", "G. Shani", "B. Chaib-draa", "Mouaddib", "A.-I"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Dibangoye et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2009}, {"title": "Symbolic Heuristic Search for Factored Markov Decision Processes", "author": ["Z. Feng", "E.A. Hansen"], "venue": "In Proc. of the 17th National Conference on Artificial Intelligence (AAAI-05)", "citeRegEx": "Feng and Hansen,? \\Q2002\\E", "shortCiteRegEx": "Feng and Hansen", "year": 2002}, {"title": "Symbolic Generalization for On-line Planning", "author": ["Z. Feng", "E.A. Hansen", "S. Zilberstein"], "venue": "In Proc. of the 19th Conference in Uncertainty in Artificial Intelligence", "citeRegEx": "Feng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2003}, {"title": "Region-Based Incremental Pruning for POMDPs", "author": ["Z. Feng", "S. Zilberstein"], "venue": "In Proc. of UAI,", "citeRegEx": "Feng and Zilberstein,? \\Q2004\\E", "shortCiteRegEx": "Feng and Zilberstein", "year": 2004}, {"title": "Focussed Dynamic Programming: Extensive Comparative Results", "author": ["D. Ferguson", "A. Stentz"], "venue": "Tech. rep. CMU-RI-TR-04-13,", "citeRegEx": "Ferguson and Stentz,? \\Q2004\\E", "shortCiteRegEx": "Ferguson and Stentz", "year": 2004}, {"title": "Efficient Solution Algorithms for Factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "J. of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "LAO*: A heuristic search algorithm that finds solutions with loops", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence J.,", "citeRegEx": "Hansen and Zilberstein,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 2001}, {"title": "Hierarchical Solution of Markov Decision Processes using Macro-actions", "author": ["M. Hauskrecht", "N. Meuleau", "L.P. Kaelbling", "T. Dean", "C. Boutilier"], "venue": "In Proc. of UAI,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "How Good is Almost Perfect", "author": ["M. Helmert", "G. R\u00f6ger"], "venue": "In Proc. of AAAI,", "citeRegEx": "Helmert and R\u00f6ger,? \\Q2008\\E", "shortCiteRegEx": "Helmert and R\u00f6ger", "year": 2008}, {"title": "SPUDD: Stochastic Planning using Decision Diagrams", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": "In Proc. of the 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "ReTrASE: Intergating Paradigms for Approximate Probabilistic Planning", "author": ["A. Kolobov", "Mausam", "D.S. Weld"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Kolobov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2009}, {"title": "Classical Planning in MDP Heuristics: With a Little Help from Generalization", "author": ["A. Kolobov", "Mausam", "D.S. Weld"], "venue": "In Proc. of ICAPS,", "citeRegEx": "Kolobov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2010}, {"title": "SixthSense: Fast and Reliable Recognition of Dead Ends in MDPs", "author": ["A. Kolobov", "Mausam", "D.S. Weld"], "venue": "In Proc. of AAAI", "citeRegEx": "Kolobov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2010}, {"title": "Computing and Using Lower and Upper Bounds for Action Elimination in MDP Planning", "author": ["U. Kuter", "J. Hu"], "venue": "In SARA,", "citeRegEx": "Kuter and Hu,? \\Q2007\\E", "shortCiteRegEx": "Kuter and Hu", "year": 2007}, {"title": "On the Complexity of Solving Markov Decision Problems", "author": ["M.L. Littman", "T. Dean", "L.P. Kaelbling"], "venue": "In Proc. of the 11th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Planning with Continuous Resources in Stochastic Domains", "author": ["Mausam", "E. Benazera", "R.I. Brafman", "N. Meuleau", "E.A. Hansen"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Mausam et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mausam et al\\.", "year": 2005}, {"title": "Planning with Durative Actions in Stochastic Domains", "author": ["Mausam", "D.S. Weld"], "venue": "J. of Artificial Intelligence Research (JAIR),", "citeRegEx": "Mausam and Weld,? \\Q2008\\E", "shortCiteRegEx": "Mausam and Weld", "year": 2008}, {"title": "Fast Exact Planning in Markov Decision Processes", "author": ["H.B. McMahan", "G.J. Gordon"], "venue": "In Proc. of the 15th International Conference on Automated Planning and Scheduling (ICAPS05)", "citeRegEx": "McMahan and Gordon,? \\Q2005\\E", "shortCiteRegEx": "McMahan and Gordon", "year": 2005}, {"title": "Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees", "author": ["H.B. McMahan", "M. Likhachev", "G.J. Gordon"], "venue": "In Proceedings of the 22nd international conference on Machine learning", "citeRegEx": "McMahan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2005}, {"title": "A Heuristic Search Approach to Planning with Continuous Resources in Stochastic Domains", "author": ["N. Meuleau", "E. Benazera", "R.I. Brafman", "E.A. Hansen", "Mausam"], "venue": "J. of Artificial Intellegence Research (JAIR),", "citeRegEx": "Meuleau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 2009}, {"title": "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time", "author": ["A. Moore", "C. Atkeson"], "venue": "Machine Learning,", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Flexibly Integrating Deliberation and Execution in Decision-Theoretic Agents. In ICAPS Workshop on Planning and Plan-Execution for Real-World Systems", "author": ["D.J. Musliner", "J. Carciofini", "R.P. Goldman", "J.W.E.H. Durfee", "M.S. Boddy"], "venue": null, "citeRegEx": "Musliner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Musliner et al\\.", "year": 2007}, {"title": "Principles of Artificial Intelligence", "author": ["N.J. Nilson"], "venue": "Tioga Publishing", "citeRegEx": "Nilson,? \\Q1980\\E", "shortCiteRegEx": "Nilson", "year": 1980}, {"title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems", "author": ["R. Parr"], "venue": "In Proc. of UAI,", "citeRegEx": "Parr,? \\Q1998\\E", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Greedy Linear Value-Approximation for Factored Markov Decision Processes", "author": ["R. Patrascu", "P. Poupart", "D. Schuurmans", "C. Boutilier", "C. Guestrin"], "venue": "In Proc. of the 17th National Conference on Artificial Intelligence", "citeRegEx": "Patrascu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Patrascu et al\\.", "year": 2002}, {"title": "Piecewise Linear Value Function Approximation for Factored MDPs", "author": ["P. Poupart", "C. Boutilier", "R. Patrascu", "D. Schuurmans"], "venue": "In Proc. of the 18th National Conference on Artificial Intelligence", "citeRegEx": "Poupart et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2002}, {"title": "Bayesian Real-Time Dynamic Programming", "author": ["S. Sanner", "R. Goetschalckx", "K. Driessens", "G. Shani"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Sanner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sanner et al\\.", "year": 2009}, {"title": "Focused Real-Time Dynamic Programming for MDPs: Squeezing More Out of a Heuristic", "author": ["T. Smith", "R.G. Simmons"], "venue": "In Proc. of the 21th National Conference on Artificial Intelligence (AAAI-06)", "citeRegEx": "Smith and Simmons,? \\Q2006\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2006}, {"title": "Prioritization Methods for Accelerating MDP Solvers", "author": ["D. Wingate", "K.D. Seppi"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Wingate and Seppi,? \\Q2005\\E", "shortCiteRegEx": "Wingate and Seppi", "year": 2005}, {"title": "FF-Replan: A Baseline for Probabilistic Planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "In Proc. of the 17th International Conference on Automated Planning and Scheduling", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "Markov Decision Processes (MDPs) (Bellman, 1957) are a powerful and widely-adopted formulation for modeling autonomous decision making under uncertainty.", "startOffset": 33, "endOffset": 48}, {"referenceID": 6, "context": ", Mountain car (Wingate & Seppi, 2005) and Drive (Bonet, 2006).", "startOffset": 49, "endOffset": 62}, {"referenceID": 2, "context": "2 Dynamic Programming Most optimal MDP algorithms are based on dynamic programming, whose utility was first proved by a simple yet powerful algorithm named value iteration (Bellman, 1957).", "startOffset": 172, "endOffset": 187}, {"referenceID": 7, "context": "Value iteration converges to the optimal value function in time polynomial in |S| (Littman, Dean, & Kaelbling, 1995; Bonet, 2007), yet in practice it is usually inefficient, since it blindly performs backups over the state space iteratively, often introducing many unnecessary backups.", "startOffset": 82, "endOffset": 129}, {"referenceID": 34, "context": "1 HEURISTIC SEARCH To improve the efficiency of dynamic programming, researchers have explored various ideas from traditional heuristic-guided search, and have consistently demonstrated their usefulness for MDPs (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet & Geffner, 2003b, 2006; McMahan et al., 2005; Smith & Simmons, 2006; Sanner et al., 2009).", "startOffset": 212, "endOffset": 367}, {"referenceID": 42, "context": "1 HEURISTIC SEARCH To improve the efficiency of dynamic programming, researchers have explored various ideas from traditional heuristic-guided search, and have consistently demonstrated their usefulness for MDPs (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet & Geffner, 2003b, 2006; McMahan et al., 2005; Smith & Simmons, 2006; Sanner et al., 2009).", "startOffset": 212, "endOffset": 367}, {"referenceID": 38, "context": "The AO* algorithm (Nilson, 1980) solves acyclic MDPs, so it is not applicable to general MDPs.", "startOffset": 18, "endOffset": 32}, {"referenceID": 1, "context": "Real-time dynamic programming (RTDP) (Barto et al., 1995) is another popular algorithm for MDPs.", "startOffset": 37, "endOffset": 57}, {"referenceID": 1, "context": "Real-time dynamic programming (RTDP) (Barto et al., 1995) is another popular algorithm for MDPs. It interleaves dynamic programming with search through plan execution trials. An execution trial is a path that originates from s0 and ends at any goal state or by a bounded-step cutoff. Each execution step simulates the result of one-step plan execution. The agent greedily picks an action a of the current state s, and mimics the state transition to a new current state s\u2032, chosen stochastically based on the transition probabilities of the action, i.e., s\u2032 \u223c Ta(s|s). Dynamic programming happens when states are backed up immediately when they are visited. RTDP is good at finding a good sub-optimal policy relatively quickly. However, in order for RTDP to converge, states on the optimal policy have to be backed up sufficiently, so its convergence is usually slow. To overcome the slow convergence problem of RTDP, researchers later proposed several heuristic search variants of the algorithm. Bonet and Geffner (2003b) introduced a smart labeling technique in a RTDP extension named labeled RTDP (LRTDP).", "startOffset": 38, "endOffset": 1022}, {"referenceID": 34, "context": "McMahan et al. (2005) proposed another extension named bounded RTDP (BRTDP), which not only uses a lower bound heuristic of the value function Vl, but also an upper bound Vu.", "startOffset": 0, "endOffset": 22}, {"referenceID": 34, "context": "McMahan et al. (2005) proposed another extension named bounded RTDP (BRTDP), which not only uses a lower bound heuristic of the value function Vl, but also an upper bound Vu. BRTDP has two key differences from the original RTDP algorithm. First, once BRTDP backs up a state s, it updates both the lower bound and the upper bound. Second, when choosing the next state s\u2032, the difference of its two bounds, Vu(s)\u2212 Vl(s), is also taken into consideration. More concretely, s\u2032 \u223c Ta(s|s)[Vu(s) \u2212 Vl(s)], which focuses search on states that are less likely to be converged. One feature of BRTDP is its adaptive trial termination criterion, which is very helpful in practice. Smith and Simmons (2006) introduced a similar algorithm named focused RTDP (FRTDP).", "startOffset": 0, "endOffset": 694}, {"referenceID": 34, "context": "McMahan et al. (2005) proposed another extension named bounded RTDP (BRTDP), which not only uses a lower bound heuristic of the value function Vl, but also an upper bound Vu. BRTDP has two key differences from the original RTDP algorithm. First, once BRTDP backs up a state s, it updates both the lower bound and the upper bound. Second, when choosing the next state s\u2032, the difference of its two bounds, Vu(s)\u2212 Vl(s), is also taken into consideration. More concretely, s\u2032 \u223c Ta(s|s)[Vu(s) \u2212 Vl(s)], which focuses search on states that are less likely to be converged. One feature of BRTDP is its adaptive trial termination criterion, which is very helpful in practice. Smith and Simmons (2006) introduced a similar algorithm named focused RTDP (FRTDP). They define occupancy as an intuitive measure of the expected number of times a state is visited before execution termination. Therefore occupancy of a state indicates its relevance to a policy. Similar to BRTDP, FRTDP also keeps two bounds for a state. FRTDP uses the product of a state\u2019s occupancy and the difference of its bounds for picking the next state. Also, FRTDP assumes a discounted cost setting, so it is not immediately applicable to SSP problems. Recently Sanner et al. (2009) described another advanced RTDP variant named Bayesian RTDP, which also uses two value bounds.", "startOffset": 0, "endOffset": 1244}, {"referenceID": 30, "context": "None of the algorithms use an MDP\u2019s graphical structure, an intrinsic property that governs the complexity of solving a problem (Littman et al., 1995), in a way to decide the order in which states are solved.", "startOffset": 128, "endOffset": 150}, {"referenceID": 10, "context": "Algorithm 2 Topological Value Iteration 1: Input: an MDP M = \u3008S,A, Ap, T, C\u3009, \u03b4: the threshold value 2: SCC(M ) 3: for i\u2190 1 to cpntnum do 4: S \u2032 \u2190 the set of states s where id[s] = i 5: M \u2032 \u2190 \u3008S \u2032,A, Ap, T, C\u3009 6: VI(M \u2032, \u03b4) 7: 8: Function SCC(M) 9: construct GR of M 10: construct a graph GR which reverses the head and tail vertices of every edge in GR 11: {call Kosaraju\u2019s algorithm (Cormen et al., 2001).", "startOffset": 385, "endOffset": 406}, {"referenceID": 2, "context": "3 Experiments We address the following questions in our experiments: (1) How does TVI compare with VI and heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most favorable problem features for TVI? We compared TVI with several other optimal algorithms, including VI (Bellman, 1957), ILAO* (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b), BRTDP (McMahan et al.", "startOffset": 296, "endOffset": 311}, {"referenceID": 34, "context": "3 Experiments We address the following questions in our experiments: (1) How does TVI compare with VI and heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most favorable problem features for TVI? We compared TVI with several other optimal algorithms, including VI (Bellman, 1957), ILAO* (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b), BRTDP (McMahan et al., 2005), Bayesian RTDP (Sanner et al.", "startOffset": 387, "endOffset": 409}, {"referenceID": 42, "context": ", 2005), Bayesian RTDP (Sanner et al., 2009) (BaRTDP), and HDP (Bonet & Geffner, 2003a)1.", "startOffset": 23, "endOffset": 44}, {"referenceID": 2, "context": "3 Experiments We address the following questions in our experiments: (1) How does TVI compare with VI and heuristic search algorithms on MDPs that contain multiple SCCs? (2) What are the most favorable problem features for TVI? We compared TVI with several other optimal algorithms, including VI (Bellman, 1957), ILAO* (Hansen & Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003b), BRTDP (McMahan et al., 2005), Bayesian RTDP (Sanner et al., 2009) (BaRTDP), and HDP (Bonet & Geffner, 2003a)1. We used the fully optimized C code of ILAO* provided by Eric A. Hansen and additionally implemented the rest of the algorithms over the same framework. We performed all experiments on a 2.5GHz Dual-Core AMD Opteron(tm) Processor with 2GB memory. Recall that BRTDP and BaRTDP use upper bounds. We used upper bounds as described in Section 4.2. We used \u03b1 = 2 \u00d7 10\u22126 and \u03c4 = 10 for BRTDP and BaRTDP.2 For BaRTDP, we used the probabilistic termination condition in Algorithm 3 of Sanner et al. (2009). 3 We compared all algorithms on running time, time between an algorithm starts solving a problem until generating a policy with a Bellman error of at most \u03b4(= 10\u22126).", "startOffset": 297, "endOffset": 989}, {"referenceID": 34, "context": "This is reminiscent of backups in BRTDP (McMahan et al., 2005).", "startOffset": 40, "endOffset": 62}, {"referenceID": 6, "context": "Single-arm pendulum (Wingate & Seppi, 2005), Drive, and Elevator (Bonet, 2006).", "startOffset": 65, "endOffset": 78}, {"referenceID": 34, "context": "BRTDP (McMahan et al., 2005), Bayesian RTDP (Sanner et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 42, "context": ", 2005), Bayesian RTDP (Sanner et al., 2009) and Focused RTDP (Smith & Simmons, 2006) (FRTDP) also keep an upper bound for the value function.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": ", the work of Hauskrecht et al. (1998) and Parr (1998).", "startOffset": 14, "endOffset": 39}, {"referenceID": 22, "context": ", the work of Hauskrecht et al. (1998) and Parr (1998). However, these approaches typically assume some additional structure in the problem, either known hierarchies, or known decomposition into weakly coupled sub-MDPs, etc.", "startOffset": 14, "endOffset": 55}, {"referenceID": 12, "context": "Dai and Hansen (2007) demonstrate that these algorithms have large overhead in maintaining a priority queue so they are outperformed by a simple backward search algorithm, which implicitly prioritizes backups without a priority queue.", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "A POMDP problem is typically much harder than an MDP problem since the decision agent only has partial information of the current state (Littman et al., 1995).", "startOffset": 136, "endOffset": 158}, {"referenceID": 38, "context": "functions (Guestrin, Koller, Parr, & Venkataraman, 2003; Poupart, Boutilier, Patrascu, & Schuurmans, 2002; Patrascu, Poupart, Schuurmans, Boutilier, & Guestrin, 2002; Yoon, Fern, & Givan, 2007; Kolobov, Mausam, & Weld, 2009, 2010a, 2010b). The techniques of these algorithms are orthogonal to the ones by FTVI, and an interesting future direction is to approximate FTVI by applying basis functions. When an MDP maintains a logical representation, another type of algorithm aggregates groups of states of an MDP by features, represents them as a factored MDP using algebraic and Boolean decision diagrams (ADDs and BDDs) and solves the factored MDP using ADD and BDD operations; SPUDD (Hoey, St-Aubin, Hu, & Boutilier, 1999), sLAO* (Feng & Hansen, 2002), sRTDP (Feng, Hansen, & Zilberstein, 2003) are examples. The factored representation can be exponentially simpler than a flat MDP, but the computation efficiency is problem-dependent. The idea of these algorithms are orthogonal to those of (F)TVI. Exploring ways of combining the ideas of (F)TVI with compact logical representation to achieve further performance improvements remains future work. Action elimination was originally proposed by Bertsekas (2001). It has been proved to be helpful for RTDP in the factored MDP setting (Kuter & Hu, 2007), when the cost of an action depends on only a few state variables.", "startOffset": 29, "endOffset": 1213}], "year": 2011, "abstractText": "Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods use reachability analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm, topological value iteration (TVI), detects the structure of MDPs and backs up states based on topological sequences. It (1) divides an MDP into strongly-connected components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm, focused topological value iteration (FTVI), is an extension of TVI. FTVI restricts its attention to connected components that are relevant for solving the MDP. Specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows FTVI to find smaller connected components, thus running faster. We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI also significantly outperforms popular \u2018heuristically-informed\u2019 MDP algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of magnitude. Finally, we characterize the type of domains where FTVI excels \u2014 suggesting a way to an informed choice of solver.", "creator": "TeX"}}}