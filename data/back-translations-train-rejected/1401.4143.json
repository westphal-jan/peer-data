{"id": "1401.4143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Convex Optimization for Binary Classifier Aggregation in Multiclass Problems", "abstract": "Multiclass problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC), have been studied, to decompose multiclass problems into binary problems. However, little study has been made to optimally aggregate binary problems to determine a final answer to the multiclass problem. In this paper we present a convex optimization method for an optimal aggregation of binary classifiers to estimate class membership probabilities in multiclass problems. We model the class membership probability as a softmax function which takes a conic combination of discrepancies induced by individual binary classifiers, as an input. With this model, we formulate the regularized maximum likelihood estimation as a convex optimization problem, which is solved by the primal-dual interior point method. Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Numerical experiments on synthetic and real-world data sets demonstrate that our method outperforms existing aggregation methods as well as direct methods, in terms of the classification accuracy and the quality of class membership probability estimates.", "histories": [["v1", "Thu, 16 Jan 2014 19:49:02 GMT  (413kb)", "http://arxiv.org/abs/1401.4143v1", "Appeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014)"]], "COMMENTS": "Appeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunho park", "taehyun hwang", "seungjin choi"], "accepted": false, "id": "1401.4143"}, "pdf": {"name": "1401.4143.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sunho Park", "TaeHyun Hwang", "Seungjin Choi"], "emails": ["taehyun.hwang}@utsouthwestern.edu", "seungjin@postech.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 140 1.aPublished in the papers of the 2014 SIAM International Conference on Data Mining (SDM 2014).2 S. Park et al. CONTENTSContents"}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Preliminaries 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Convex Optimization for Binary Classifier Aggregation 6", "text": "3.1 Convex formulation..................................... 6 3.2 Primal-Dual-Interior-Point method.........................."}, {"heading": "4 Connections to Large Margin Classifiers 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Experiments 15", "text": "5.1 Field of experimentation......................................................................................................................"}, {"heading": "6 Conclusions 27", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Appendix 27", "text": "7.1 Derivatives of gradient and Hessian of objective function (10)...... 27 7.2 Proof of Proposition 1............................................... 28 7.3 Proof of Proposition 2..........................................................................."}, {"heading": "1 Introduction", "text": "It is much easier and easier to learn a unique classification, the aim of which is to assign data to a finite set of K-classes, which is solved by one of two different approaches (direct and indirect methods). Direct approach involves constructing a discrimination function directly for the problem of the multi-class list SVM [1, 2] models of a K-way classifier, which directly distinguishes the correct class name from the rest of the class names on a large marginal scale. Alternatively, the problem of multi-class differentiation is broken down into several binary classification problems, which are solved by individual binary classifiers, the results of which are integrated into a definitive solution. All-pairs (APs) and man-versus-all (OVA) are well-known methods for decomposing multicultural problems into binary problems."}, {"heading": "2 Preliminaries", "text": "We assume that both are data vectors and not data vectors and vectors. (1) For each data vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector class vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "3 Convex Optimization for Binary Classifier Aggrega-", "text": "In this section, we present our main contribution, the convex formulation, which formulates an optimal aggregation of binary classifiers to a definitive answer to multi-class problems as convex optimization, which is solved by the primary-dual method of the inner dot. We use the Softmax function to relate the probabilities of class belonging to the probabilities of binary probabilities. Softmax model takes a conical combination of the discrepancies between code words and the probability estimates of binary classifiers as input arguments to represent the probabilities of class belonging. This approach provides a simple model for evaluating the probabilities of class belonging compared to the (generalized) Bradley-Terry method, which is based on the model [16]."}, {"heading": "3.1 Convex Formulation", "text": "Considering the probability probability for membership in a class, Cj, k, while the model is the probability for membership in another class. We evaluate which code word ck = j = j is closest to qi in the sense of a given discrepancy. (3) We define the discrepancy (3) as an opposite combination of errors caused by M binary classifiers. (1 \u2212 Cj) Log (1 \u2212 Cj) Log (1 \u2212 Qj) Log (4), i), (4) is the cross-entropy error function for two classes in which the model probability for membership in a class is Qj, i and the corresponding true probability is Cj."}, {"heading": "3.2 Primal-Dual Interior Point Method", "text": "We will apply the Primal Dual Method [22, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector. We will briefly explain the Primal Dual Interior Method in this section to make our paper independent and sketch the algorithm in most application areas such as linear, square, geometric and semi-definite. Readers familiar with the Primal Dual Interior Method can skip this method and find more details in this section."}, {"heading": "4 Connections to Large Margin Classifiers", "text": "In this section, we will show the relationships of our convex formula to the large margin classifiers, in which the discrepancy between the differences (K = 1) and the logistic regression (K = 1) by binary classifiers (instead of training examples xi) is considered inputs to a large margin classifier. Following work in [28], which shows a close relationship between large margins and logistic regression formulas, we note that the large margin formulation can be understood as a limiting case of our convex formula. We assume that we assign data points xi to class y, i ify, i ify, i = argmin k, w), where the large margin formulation can be understood as a limiting case of our convex formula."}, {"heading": "5 Experiments", "text": "We evaluated the performance of our method using several sets of data in terms of classification accuracy and quality of class probability estimates compared to existing methods of multi-class classification, including two direct methods for multi-class problems, {Multiclass SVM (M-SVM) [2] and Lasso multinomial regression (LMR) [32], and three aggregation methods, {loss-based decoding [7], GBTM in [15] (a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].} We conducted numerical experiments on two synthetic and eight real data sets to demonstrate the usefulness and high performance of our convex aggregation method."}, {"heading": "5.1 Experimental Setting", "text": "It is not the way in which we are in the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world"}, {"heading": "5.2 Synthetic Data", "text": "In this section we show that our method the overall classification of Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi"}, {"heading": "5.3 Real-World Data", "text": "This year, it is more than ever before in the history of mankind."}, {"heading": "6 Conclusions", "text": "The softmax function was used to model the probability of class membership, using a conical combination of discrepancies induced by binary classifiers and a guess of class membership, and the corresponding log probability method was a convex function in the form of log-sum-exp, resulting in a convex formulation for optimal binary classifier aggregation. To solve the convex optimization problem, the primary-dual internal point methodology was used. Our method has several advantages over an existing optimal aggregation method, WMAP [16], which optimally combines binary class membership probability estimates to form a common probability estimate for all K classes that corresponds to the generalized Bradley-Terry model. In WMAP, both aggregation weights and class membership probabilities are treated as parameters to be estimated."}, {"heading": "7 Appendix", "text": "7.1 Derivatives of the gradient and the Hessian of the objective function (10) In this section we include the course and the Hessian of the objective function (10), which can easily be calculated on the basis of the derivatives of the gradient and the Hessian of the log sum Exp function, which is described in the appendix in [22]. We first calculate \"j,\" \"yii\" for \"j = 1,\"..., K \"and\" i = 1,..., \"\" N \"for (8). Then we define\" i \"R\" K \"M\" and \"ui\" R \"K\" 1 for the ith data point: \"i...\" (K \"yii\").ui = \"exp\" w \"1,\" yii,..., \"exp\" M, \"yi\"). (44) The objective function is defined by (1) \"f0 (w) = 1NN.\""}, {"heading": "7.2 Proof of Proposition 1", "text": "The proof. Let's define the l function as a modified log-sum-Exp function with a log function with a log function (log function i = log function i = log function i = log function i = log function i = log function i (log function i = log function i = log function i). Let's first show that the sequence of functions (K, yi, yi, yi, yi,..., K, yi, w) for log function i (K, yi, w) for log function i (K, k, yi). Let's first show that the sequence of functions (K, yi, yi, yi,..., yi, w) for log function i (K, yi, w) for log function i (K, k, yi)."}, {"heading": "7.3 Proof of Proposition 2", "text": "s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. We can consider the margin of each individual example (xi, yi) as a decision function for multiclass-specific problems. (Example: The data point xi is incorrectly classified if and only if it is the 0-1 loss function, which equals 1 if the predicate \"p\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\""}], "references": [{"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "Proceedings of the Seventh European Symposium On Artificial Neural Networks", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "On the algorithmic implementation of multiclass kernelbased vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "A review on the combination of binary classifiers in multiclass problems", "author": ["A.C. Lorena", "A.C. Carvalho", "J.M. Gama"], "venue": "Artificial Intelligence Review, 30:19\u201337", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Handwritten digit recognition by neural networks with single-layer training", "author": ["S. Knerr", "L. Personnaz", "G. Dreyfus"], "venue": "IEEE Transactions on Neural Networks, 3:962\u2013 968", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["C.W. Hsu", "C.J. Lin"], "venue": "IEEE Transactions on Neural Networks, 13:415\u2013425", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research, 2:263\u2013286", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research, 1:113\u2013141", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "MetaCost: A general method for making classifiers cost-sensitive", "author": ["P. Domingos"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 155\u2013164", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Class probability estimation and cost-sensitive classification decisions", "author": ["D.D. Margineantu"], "venue": "Proceedings of the European Conference on Machine Learning (ECML), pages 270\u2013281", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), Williams Town, MA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiely & Sons", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics, 26(2):451\u2013471", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Rank analysis of incomplete block designs: The method of paired comparisons", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "Biometrika, 39:324\u2013345", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1952}, {"title": "Reducing multiclass to binary by coupling probability estimates", "author": ["B. Zadrozny"], "venue": "Advances in Neural Information Processing Systems (NIPS), volume 14. MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Generalized Bradley-Terry models and multiclass probability estimates", "author": ["T.K. Huang", "R.C. Weng", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 7:85\u2013115", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal aggregation of binary classifiers for multiclass cancer diagnosis using gene expression profiles", "author": ["N. Yukinawa", "S. Oba", "K. Kato", "S. Ishii"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, 6(2):333\u2013343", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A multiclass classification method based on decoding of binary classifiers", "author": ["T. Takenouchi", "S. Ishii"], "venue": "Neural Computation, 21(7):2049\u20132081", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian aggregation of binary classifiers", "author": ["S. Park", "S. Choi"], "venue": "Proceedings of the IEEE International Conference on Data Mining (ICDM)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Geometric programming for classifier aggregation", "author": ["S. Park", "S. Choi"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J.C. Platt"], "venue": "Advances in Large Margin Classifiers, pages 61\u201374. MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on geometric programming", "author": ["S. Boyd", "S.J. Kim", "L. Vandenberghe", "A. Hassibi"], "venue": "Optimization and Engineering, 8(1):67\u2013127", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear Programming: Sequential Unconstrained Minimization Techniques", "author": ["Anthony V. Fiacco", "Garth P. McCormick"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1968}, {"title": "A new polynomial-time algorithm for linear programming", "author": ["N. Karmarkar"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1984}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "An interior-point method for large-scale l1regularized logistic regression", "author": ["K.M. Koh", "S.J. Kim", "S. Boyd"], "venue": "Journal of Machine Learning Research, 8:1519\u20131555", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Text categorization based on regularized linear classification methods", "author": ["T. Zhang", "F.J. Oles"], "venue": "Information Retrieval, 4:5\u201331", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Large scale transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research, 7:1687\u20131712", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hestie", "R. Tibshirani"], "venue": "Journal of Statistical Software, 33(1)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized linear models", "author": ["J. Nelder", "W.M. Wedderburn"], "venue": "Journal of the Royal Statistical Society A, 135:370\u2013384", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1972}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u2013 1874", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A note on Platt\u2019s probabilistic outputs for support vector machine", "author": ["H.T. Lin", "C.J. Lin", "R.C. Weng"], "venue": "Machine Learning, 68(3):267\u2013276", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Classification", "author": ["E.J. Yeoh", "M.E. Ross", "S.A. Shurtleff", "W.K. Williams", "D. Patel", "R. Mahfouz", "F.G. Behm", "S.C. Raimondi", "M.V. Relling", "A. Patel", "C. Cheng", "D. Campana", "D. Wilkins", "X. Zhou", "J. Li", "H. Liu", "C.H. Pui", "W.E. Evans", "C. Naeve", "L. Wong", "J.R. Downing"], "venue": "subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling. Cancer Cell, 1(2):133\u2013143", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Multiclass cancer diagnosis using tumor gene expression signatures", "author": ["S. Ramaswamy", "P. Tamayo", "R. Rifkin", "S. Mukherjee", "C.H. Yeang", "M. Angelo", "C. Ladd", "M. Reich", "E. Latulippe", "J.P. Mesirov", "T. Poggio", "W. Gerald", "M. Loda", "E.S. Lander", "T.R. Golub"], "venue": "Proceedings of the National Academy of Sciences, USA, 98(26):15149\u201315154", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), Edmonton, Canada", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Verification of forecasts expressed in terms of probability", "author": ["G.W. Brier"], "venue": "Monthly Weather Review, 78:1\u20133", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1950}, {"title": "and N", "author": ["P. Carbonetto", "M. Schmidt"], "venue": "de Freitas. An interior-point stochastic approximation method and an L1-regularized delta rule. In Advances in Neural Information Processing Systems (NIPS), volume 21. MIT Press", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Modified logistic regression: An approximation to SVM and its applications in large-scale text categorization", "author": ["J. Zhang", "R. Jin", "Y. Yang", "A.G. Hauptmann"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), Washington, DC", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "For example, the multiclass SVM [1, 2] models a K-way classifier which directly separates the correct class label from the rest of class labels in the large margin framework.", "startOffset": 32, "endOffset": 38}, {"referenceID": 1, "context": "For example, the multiclass SVM [1, 2] models a K-way classifier which directly separates the correct class label from the rest of class labels in the large margin framework.", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "It is much easier and simpler to learn a set of binary classifiers than to train one unique classifier which separates all classes simultaneously [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "For example, a digit recognition problem can be decomposed into a set of simpler sub-problems, which can be easily solved by linear classifiers [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "A comparison study [5] observed that the direct methods, such as multiclass SVM [1, 2], generally require more training time than binary-decomposition methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 4, "context": "It was also observed in [5] that APs-based decomposition methods show higher predictive accuracy than the multiclass SVM for most of cases.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Several encoding methods are widely used, including APs, OVA, and error correcting output code (ECOC) [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "Various loss functions (such as exponential loss and logistic loss) are considered in the case where binary classifiers yield a score whose magnitude is a measure of confidence in the prediction, referred to be as loss-based decoding [7].", "startOffset": 234, "endOffset": 237}, {"referenceID": 7, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 8, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 9, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 53, "endOffset": 63}, {"referenceID": 10, "context": "For instance, in the case of cost-sensitive decision [8, 9, 10], the Bayes optimal prediction is to assign an example to the class label that has a lowest expected cost (which is also called conditional risk [11]).", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "In probabilistic decoding, we are given binary class membership probability estimates (scores in [0,1]) determined by binary classifiers.", "startOffset": 97, "endOffset": 102}, {"referenceID": 11, "context": "In the case of APs, Hastie and Tibshirani [12] developed a method, pairwise coupling, in which pairwise class membership probability estimates are combined to form a joint probability estimates for all K classes, fitting the Bradley-Terry model [13] by minimizing a KL-divergence criterion.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "In the case of APs, Hastie and Tibshirani [12] developed a method, pairwise coupling, in which pairwise class membership probability estimates are combined to form a joint probability estimates for all K classes, fitting the Bradley-Terry model [13] by minimizing a KL-divergence criterion.", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 78, "endOffset": 86}, {"referenceID": 14, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 78, "endOffset": 86}, {"referenceID": 14, "context": "This was extended for arbitrary code matrix (OVA and ECOC in addition to APs) [14, 15], where a generalized Bradley-Terry model [15] was considered to relate probability estimates obtained by binary classifiers to class membership probability estimates.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "This problem is alleviated by introducing confidence weights placed on individual binary classifiers that are optimally tuned based on training data [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "However, the method in [16] involves a huge number of parameters, NK +M , where N is the number of training data points and M is the number of binary classifiers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Takenouchi and Ishii [17] proposed a different type of decoding method in which misclassification in binary classifier is formulated as a bit inversion error problem, as in information transmission theory.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "Recently, we have developed a Bayesian aggregation method [18] for probabilistic decoding.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "In this way, aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M).", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "To solve these problems, one can consider the maximum likelihood estimation instead of full Bayesian learning [18], in which class membership probabilities can be easily computed by evaluating the softmax function with the learned aggregation weights.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "In our previous work [19], we proposed the l1 norm regularized maximum likelihood method to determine the optimal aggregation weights, which is a convex problem.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "However, our previous method [19] still has several limitations: (1) the optimization problem can be directly solved by the standard convex optimization algorithms without transforming it to geometric programming; (2) it only allows l1 norm regularization.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "In contrast to [19] where the problem was converted to geometric programming, we directly solve the optimization problem using primal-dual interior point method that is an efficient solver for convex optimization problems, which allows us to use various types of regularization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "In our formulation, the aggregation weights are the only parameters to be tuned (M), while the existing method [16] scales linearly with the number of samples (NK +M).", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Moreover, we present data-dependent generalization error bound, based on margins and Rademacher complexity, extending existing work on binary problems [20] to our multiclass problems which are solved by aggregating binary solutions.", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "For example, we can use probabilistic SVM [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "This approach provides a simple model to evaluate class membership probabilities, compared to the (generalized) Bradley-Terry model-based method [16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "For instance, we can also choose the exponential loss function that was used in loss-based decoding [7]", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The prediction based on the loss-based decoding [7] is a special case of our model.", "startOffset": 48, "endOffset": 51}, {"referenceID": 21, "context": "associated with data point xi in the objective function (11) is called log-sum-exp which is a well known convex function used for geometric programming [22, 23].", "startOffset": 152, "endOffset": 160}, {"referenceID": 22, "context": "associated with data point xi in the objective function (11) is called log-sum-exp which is a well known convex function used for geometric programming [22, 23].", "startOffset": 152, "endOffset": 160}, {"referenceID": 21, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 23, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 24, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 25, "context": "2 Primal-Dual Interior Point Method We make use of the primal-dual interior point method [22, 24, 25, 26] to solve the convex optimization problem (10) to estimate the optimal aggregation weight vector w.", "startOffset": 89, "endOffset": 105}, {"referenceID": 21, "context": "The primal-dual interior point method exhibits better than linear convergence and outperforms the standard interior point methods in most of applications such as linear, quadratic, geometric and semidefinite programming [22].", "startOffset": 220, "endOffset": 224}, {"referenceID": 21, "context": "Readers who are familiar with the primal-dual interior point method can skip this section and more details can be found in [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "One can easily see that Slater\u2019s constraint qualification holds for the problem (10), since any point on the positive orthant (w \u2208 R++) could be a strictly feasible solution to the problem [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 23, "context": "We augment the objective function f0(w) by a logarithmic barrier [24] such that the constrained optimization problem (10) is converted to an unconstrained optimization:", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "The residual is not necessary 0 at each iteration, except in the limits as the algorithm converges [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "As proposed in [27], we construct P as a diagonal matrix in which the diagonal entries are set to that of H .", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "The step length s can be computed by a backtracking line search as described in [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "To do this, we first compute smax to ensure z(\u03bc) 0: smax = sup{s \u2208 [0, 1]|z + s\u2206z 0} = min{1,min{\u2212zj/\u2206zj |\u2206zj < 0}}.", "startOffset": 67, "endOffset": 73}, {"referenceID": 26, "context": "In order to update the barrier parameter \u03bc, we use an adaptively strategies that determines it according to the reduction of the duality gap as in [27]:", "startOffset": 147, "endOffset": 151}, {"referenceID": 27, "context": "Following the work in [28] where a close relation between large margin and logistic regression formulations is shown in the case of binary classification, we provide its multiclass extension in this section.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "The multiclass hinge loss function, which is a convex upper bound on the 0-1 loss 1 ( \u0177i 6= yi ) [2], was considered as a surrogate function:", "startOffset": 97, "endOffset": 100}, {"referenceID": 28, "context": "The projected subgradient methods [29] can be applied to directly solve the primal form (36).", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "Note that the projected subgradient method is a first-order method which exploits the gradient only, thus its performance much depends on the problem scaling or conditioning [26, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 19, "context": "In addition, we present a data-dependent generalization error bound, based on the large margin formulation (36) using the Rademacher complexity [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "Our result is an extension of existing work on binary problems [20] to the multiclass problems solved by aggregating binary solutions.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Applying Theorem 7 in [20] to our problem, with the empirical Rademacher complexity, yields the following proposition.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "where \u03bdw(xi, yi) is the margin (50) and \u03c6(z) = min(1,max(0, 1\u2212 z)), for z \u2208 R, is the ramp loss that is a clipped version of hinge loss [31].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "Lemma 22 in [20] was used to compute the empirical Rademacher complexity in our problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "Note that our generalization error bound is similar to the ones for boosting with loss-based decoding [7], while the error bound in [7] is based on VC dimension which does not depend on sample distribution in contrast to Rademacher complexity.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "Note that our generalization error bound is similar to the ones for boosting with loss-based decoding [7], while the error bound in [7] is based on VC dimension which does not depend on sample distribution in contrast to Rademacher complexity.", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 81, "endOffset": 84}, {"referenceID": 30, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 183, "endOffset": 186}, {"referenceID": 14, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "They include two direct methods for multiclass problems, {multiclass SVM (M-SVM) [2] and lasso multinomial regression (LMR) [32]}, and three aggregation methods, {loss-based decoding [7], GBTM in [15] (which is a probabilistic decoding method based on the generalized Bradley-Terry models) and WMAP [16].", "startOffset": 299, "endOffset": 303}, {"referenceID": 1, "context": "1 Experimental Setting M-SVM [2] aims to directly construct a K-way classifier which separates the correct class labels from the rest of class labels by maximizing the margin defined as gyi(xi)\u2212maxk 6=yi gk(xi), where {gk} K k=1 are classification functions for each class.", "startOffset": 29, "endOffset": 32}, {"referenceID": 30, "context": "LMR [32] is a variant of generalized liner model (GML) [33] that generalizes linear regression by allowing a linear model to be related with the response variables (characterized by exponential family distribution) through the response function.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "LMR [32] is a variant of generalized liner model (GML) [33] that generalizes linear regression by allowing a linear model to be related with the response variables (characterized by exponential family distribution) through the response function.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "LMR [32] determines the parameters by maximum likelihood with the l1 norm (lasso) regularization.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "In the case of ECOC encoding, we used two strategies to generate the code matrix C: complete code and sparse random code [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "For K \u2265 8, we generated a spare random code matrix as in [7], in which M = \u230815 log2 K\u2309, and entries of the code matrix are chosen as \u25b3 with probability 1/2 and 0 or 1 with probability 1/4 for each.", "startOffset": 57, "endOffset": 60}, {"referenceID": 32, "context": "We used two linear SVMs to implement the base binary classifier, LibSVM and Liblinear [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "In the case of LibSVM, Platt\u2019s sigmoid model [21] is used to calculate the binary class membership probability:", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "where gj is the function learned by the jth SVM and A,B \u2208 R are parameters are tuned by the regularized maximum likelihood framework [21, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 33, "context": "where gj is the function learned by the jth SVM and A,B \u2208 R are parameters are tuned by the regularized maximum likelihood framework [21, 35].", "startOffset": 133, "endOffset": 141}, {"referenceID": 6, "context": "1, the loss-based decoding [7] can be implemented as a special case of our aggregation method, where the aggregation weights are set to w\u03031 = .", "startOffset": 27, "endOffset": 30}, {"referenceID": 15, "context": "WMAP [16] is an existing optimal aggregation method, which also tunes aggregation weights based on training data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "The aggregation weights are assigned to each classifier an learned by maximizing the objective function which represents the concordance between the class membership probability estimates and the target labels [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "GBTM is also a probabilistic decoding method based on generalized Bradely-Terry models [15], which can be understood as a special case of WMAP with the uniform aggregation weighs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "We implemented the method according to Algorithm 2 in [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Here, uj,i and vj,i were generated from the uniform distribution, uj,i, vj,i \u223c U[0 1] and r(a) is a binary function that is 1 if a > 0.", "startOffset": 80, "endOffset": 85}, {"referenceID": 0, "context": "For example, in the case of BC1, Q1,i = u1,i for i \u2208 I3, where u1,i \u223c U[0 1].", "startOffset": 71, "endOffset": 76}, {"referenceID": 19, "context": "The data instances evenly sampled from K number of 2-dimensional Gaussian distributions, the mean vectors of which are chosen as D independent uniform [0, 20] random variables.", "startOffset": 151, "endOffset": 158}, {"referenceID": 34, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "The cancer data sets are acute lymphoblastic leukemia (ALL) [37] and global cancer map (GCM) [38], which were used to evaluate the performance of WMAP [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "As in [10, 39], we can assume that the class membership probabilities are given by to = [Tj,o] \u2208 R K where Tj,o is defined to be 1 if the label of xo is j and 0 otherwise.", "startOffset": 6, "endOffset": 14}, {"referenceID": 36, "context": "As in [10, 39], we can assume that the class membership probabilities are given by to = [Tj,o] \u2208 R K where Tj,o is defined to be 1 if the label of xo is j and 0 otherwise.", "startOffset": 6, "endOffset": 14}, {"referenceID": 37, "context": "Note that MSE is also called Brier score [40], and the lower value the better performance.", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "For binary-decomposition methods, the base binary classifier was chosen according to the scale of dataset: LibSVM for the datasets {GCM, ALL}, and Liblinear [34] for other datasets.", "startOffset": 157, "endOffset": 161}, {"referenceID": 4, "context": "This result is consistent with the comparison study in [5] which reported that APs-based decomposition methods generally show higher predictive accuracy than the direct methods for multiclass problems, such as M-SVM.", "startOffset": 55, "endOffset": 58}, {"referenceID": 15, "context": "Our method has several advantages over an existing optimal aggregationmethod, WMAP [16] which optimally combines binary class membership probability estimates to form a joint probability estimates for all K classes, fitting the generalized Bradley-Terry model.", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "We may use a stochastic approximation of interior point methods [41] to improve the scalability.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "1 Derivations of gradient and Hessian of the objective function (10) In this section, we include the gradient and Hessian of the objective function (10), which can be easily calculated based on the derivations of gradient and Hessian of the log-sum-exp function, described in Appendix in [22].", "startOffset": 288, "endOffset": 292}, {"referenceID": 39, "context": "This proposition is an extension of the results in [42] which provide a connection between the hinge loss function and logistic loss function (that is a special case of the log-sum-exp function) in the case of binary problems.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "For all \u03be \u2208 R , we are given the following inequalities for the log-sum-exp function [22]:", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "We instead consider a ramp loss \u03c6 : R \u2192 [0, 1], which is a continuous upper bound on 0-1 loss function:", "startOffset": 40, "endOffset": 46}, {"referenceID": 29, "context": "Note that, \u03c6 is a clipped version of hinge loss [31].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "Finally, we define the empirical Rademacher complexity [20] of a class of functions we are interested in.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "Let G be a class of functions mapping from X \u00d7 Y to R and given samples {(xi, yi)} N i=1, the empirical Rademacher complexity of the class G is given by [20]", "startOffset": 153, "endOffset": 157}, {"referenceID": 19, "context": "In our case, the empirical Rademacher can be calculated based on Lemma 22 in [20].", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "Multiclass problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC), have been studied, to decompose multiclass problems into binary problems. However, little study has been made to optimally aggregate binary problems to determine a final answer to the multiclass problem. In this paper we present a convex optimization method for an optimal aggregation of binary classifiers to estimate class membership probabilities in multiclass problems. We model the class membership probability as a softmax function which takes a conic combination of discrepancies induced by individual binary classifiers, as an input. With this model, we formulate the regularized maximum likelihood estimation as a convex optimization problem, which is solved by the primal-dual interior point method. Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Numerical experiments on synthetic and real-world data sets demonstrate that our method outperforms existing aggregation methods as well as direct methods, in terms of the classification accuracy and the quality of class membership probability estimates. aAppeared in Proceedings of the 2014 SIAM International Conference on Data Mining (SDM 2014). 2 S. Park et al. CONTENTS", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}