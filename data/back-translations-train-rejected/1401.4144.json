{"id": "1401.4144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Arguments using ontological and causal knowledge", "abstract": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A_1, ... ,A_n} causes B) and ontological links (o_1 is_a o_2). We present a system that provides a candidate explanation ({A_1, ... ,A_n} explains {B_1, ... ,B_m}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed: a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties.", "histories": [["v1", "Thu, 16 Jan 2014 19:49:42 GMT  (64kb,D)", "http://arxiv.org/abs/1401.4144v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philippe besnard", "marie-odile cordier", "yves moinard"], "accepted": false, "id": "1401.4144"}, "pdf": {"name": "1401.4144.pdf", "metadata": {"source": "CRF", "title": "Arguments using ontological and causal knowledge", "authors": ["Philippe Besnard", "Marie-Odile Cordier", "Yves Moinard"], "emails": ["besnard@irit.fr,", "cordier@irisa.fr,", "yves.moinard@inria.fr"], "sections": [{"heading": null, "text": "We will examine an approach of arguing about causes through reasoning. We will look at a causal model for a physical system and look for arguments about facts. Some arguments are intended to provide explanations for facts, while some question these explanations, etc. The reasoning is based on causal connections ({A1, \u00b7 \u00b7 \u00b7, A} causes B) and ontological connections (o1 is _ a o2). We will present a system that provides an explanation for candidates ({A1, \u00b7 \u00b7, An} explains {B1, \u00b7 \u00b7, Bm}) by relying on an underlying causal connection supported by appropriate ontological connections. We will then argue from these various explanatory connections. A case study will be developed: a severe storm Xynthia that devastated a part of France in 2010 with an inexplicably high number of deaths."}, {"heading": "1 Introduction and Motivation", "text": "Looking for explanations is a common operation, in different areas, from justice to mechanical fields. We look at the case where we have some precise (not necessarily exhaustive) description of a mechanism or situation, and we look for explanations for some facts. The description contains logical formulas, plus some causal and ontological formulas (or linkages). In fact, it is known that although there are similarities between causal connections and implications, the causal connections cannot be reproduced by a simple logical implication. Moreover, the confusion of causal connections could lead to undesirable relationships. Therefore, we use a causal formalism here, in which some causal connections and ontological linkages are added to classical logical formulas. Then, causal formalism will produce different explanatory connections [1]. However, if the situation described is complex enough, this will lead to a large number of possible explanations, and some arguments are involved to choose between all these explanations."}, {"heading": "2 Enriched causal model = Causal model + ontological model", "text": "The model used to provide preliminary explanations and support reasoning is called an enriched causal model, and is based on a causal model that relates dictionaries in causal linkages, and an ontological model in which classes of objects are interrelated through specialization / generalization linkages."}, {"heading": "2.1 The causal model", "text": "By a causal model [8] we mean the representation of a body of causal relationships used to generate arguments that provide explanations for a given set of facts. Basically, the case is that of a causal relationship \"\u03b1 causes \u03b2,\" where \u03b1 and \u03b2 are literals. In this simple case, \u03b1 stands for the singleton {\u03b1}, since the general case of a causal relationship is the form {\u03b11, \u03b12, \u00b7 \u00b7, \u03b1n} causes \u03b2, where {\u03b11, \u03b12, \u00b7 \u00b7 \u03b1n} is a set of words (interpreted conjunctively). Part of the causal model for our Xynthia example is shown in Fig. 2 (each simple black arrow represents a causal relationship)."}, {"heading": "2.2 The ontological model", "text": "The letters P (o1, o2, \u00b7 \u00b7 \u00b7, ok), which occur in the causal model, use some predicates P, which are applied to object classes oi. The ontological model consists of specialization / generalization connections between objects1 is-a \u2212 \u2192 o2, where is-a \u2212 \u2192 denotes the usual specialization connection between classes. For example, we have Hurri is-a \u2212 \u2192 SWind, House1FPA is-a \u2212 \u2192 HouseFPA is-a \u2212 \u2192 BFPA: A \"hurricane\" (hurricane) is a specialization of a \"strong wing\" (SWind), and the class of \"low houses with one level only in flood-prone area\" (House1FPA) is a specialization of the class of \"houses in flood-prone area\" (HouseFPA), which itself represents a specialization of the class of \"buildings in this area\" (BFPA). A part of the house model for our white Fyrow is an example of a XPA."}, {"heading": "2.3 The enriched causal model", "text": "The causal model is extended by recourse to the ontological model, and the result is referred to as the enriched causal model. Enrichment lies in the so-called ontological deduction (referred to as predicate) between words. Such a linkage simply means that \u03b2 can be derived from specialization / generalization, which represents a kind of linkage in the ontological model that relates the classes of objects mentioned in \u03b1 and \u03b2. Note that the relationship between font \u2212 \u2192 transitive and reflexive.Here is a simple illustration. A sedan is a type of car represented by sedan - a \u2212 \u2192 car in the ontological model. Then, Iown (sedan) ded ont \u2212 \u2192 Iown (car) is an ontological linkage in the extended class."}, {"heading": "3 The Xynthia example", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4 Explanations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Explaining a literal from a [set of] literal[s]", "text": "Causal and ontological links allow us to draw conclusions from explanatory links. We want to show explanatory approaches that can explain a fact by at least one causal link (\u03b2). We ignore explanations that are only implicit links. Here is how causal and ontological links are used in our formalism: Let us designate a set of words and \u03b2 be a dictionary. The basic case is that links that \u03b2 causes lead to a result that \u03b2 can be explained by links. (2) The general initial case comprises two dictionaries \u03b2 = Prop (cl2) and Prop = Prop (cl1), which is built on the same predicate Prop (ultimately other parameters that are the same in these two dictionaries): {?? Causes that give links that \u03b2} a link can be explained by Proclat Proclat (1) and (p) (if) (p)."}, {"heading": "4.2 Explaining a set of literals from a set of literals", "text": "Definition 3 Let us be a natural integer number, and if we can declare for each i-number (1, 2, \u00b7 \u00b7 \u00b7, n) that i-number is possible, then we will define the following explanation links, which can explain i-number (1, 2, \u00b7 \u00b7, n), provided that i-number (n) is possible. Again, such an explanation link only applies if its (complete) justification set (here: 1, 2, \u00b7, n) is possible (not disproved by the data). Again, we will point out that we do not want to explain this restriction by ourselves, and extend this restriction to \"set links\": We do not want to explain that the (complete) justification of the set link (here: 0, the set link is all we know). In fact, this seems to be fraud, which is an explanation (we want some causal links to be explained)."}, {"heading": "4.3 About explanation links and arguments", "text": "As a small example, we assume that in the causal model the link is Cause Prop (Class2) and that in ontology the links (Class1) Prop (Class2) Prop (Class1) (Class3) are the next two links in the enriched model: Prop (Class1) Prop (Class2) Prop (Class1). \u2212 Prop (Class3) By matching patterns via the diagrams in Fig. 4 and 5 [CF patterns (3) and (5], we can explain Prop (Class3) throughProp (Class1) (\"Justification\"). It is assumed that the causal link is expressed at the appropriate level: In other words, if there is some doubt about the type of objects (here Class2) that Prop enjoys due to reason, the causal link is expected."}, {"heading": "4.4 An example of compound explanations", "text": "Figure 6 shows an example from Xynthia (cf. also Figure 3) for some possible explanations, represented by dashed lines with their labels such as 1 or 1a. The sentences of literals from which the explanatory links start are framed and numbered from (1) to (5), showing the transitivity of explanations at work: For example, sentence 1 can explain Victims _ 1 (BFPA) (explanatory link with labels 1 + 1a + 1b) used explanatory links 1, 1a and 1b. Another example is sentence 5 can explain Victims _ 3 (House1FPA) (explanatory link 1 + 1a + 2 + 3): explanatory links 1, 1a, 2 and 3 are at work here, and link 2 uses links 1 and 1a together with the flooded (BFPA) font \u2212 \u2192 flooded (House1FPA) St."}, {"heading": "5 Argumentation", "text": "As we have just seen, the enriched causal graph allows us to derive explanations for assertions and these explanations could be used in an argumentative context [2, 3]. Let us first provide some motivations from our example. A possible set of explanations for the flooded buildings is formed by the bad weather conditions (\"very low pressure\" and \"strong wind\") together with \"high spring tide\" (see Fig. 2). In view of this explanation (argument) it is possible to attack them by noting: a strong wind is supposed to trigger the red alarm of my anemometer and I have not received an alarm. However, this counter-argument itself can be attacked by pointing out that in the case of a hurricane, which is a kind of strong wind, an anemometer is no longer in operation, which may explain that a red alarm cannot be observed."}, {"heading": "5.1 Counter-arguments", "text": "In general, an argument that \"can be explained by a strong wind\" is challenged on the grounds that \"my explanation was not explained by red light.\" This is called into question by a statement that is \"either\" (e.g., an argument that has an explanation for the negation of red light) or \"mo\" (e.g., an argument that has an explanation for the negation of red light) or \"mo\" (e.g., an argument that has an explanation for the negation of red light) 4. Or by \"mo\" (e.g., an argument that has an explanation for the negation of red light). Such objections are counter-arguments (they take the form of an argument: they explain something - but what they explain contradicts something in the contested argument). Consider the presentation at the beginning of this section: The argument (that buildings in the flood-prone area are flooded) is wrong."}, {"heading": "6 Conclusion", "text": "The aim of this paper is to investigate the link between causes and explanations [5, 6] and to rely on explanations in an argumentative context [2]. In a first part, we define explanations resulting from causal and ontological connections. However, an enriched causal model is based on a causal model and an ontology from which the explanatory links are derived (see e.g. Fig4 and (2). Our work differs from other approaches in the literature in that it strictly separates causality, ontology and explanations, while taking into account that ontology is the key to generating reasonable explanations from causal statements. Note, however, that some authors have already introduced ontology to solve tasks as planning. [7, Chapter 2] and more recently for diagnosis and repair [9]. We then argue that these causal explanations are interesting building blocks that have been identified as different processes in an argumentative context."}], "references": [{"title": "Ontology-based inference for causal explanation", "author": ["Philippe Besnard", "Marie-Odile Cordier", "Yves Moinard"], "venue": "Integrated Computer-Aided Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Elements of Argumentation", "author": ["Philippe Besnard", "Anthony Hunter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "author": ["Phan Minh Dung"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "User acceptance of knowledge-based system recommendations : Explanations, arguments, and fit", "author": ["Justin Scott Giboney", "Susan Brown", "Jay F. Nunamaker Jr."], "venue": "In 45th Annual Hawaii International Conference on System Sciences", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Causes and Explanations : A Structural-Model Approach", "author": ["Joseph Halpern", "Judea Pearl"], "venue": "Part I : Causes", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Causes and Explanations : A Structural-Model Approach", "author": ["Joseph Y. Halpern", "Judea Pearl"], "venue": "Part II : Explanations", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Reasoning about plans", "author": ["Henry Kautz"], "venue": "Reasoning About Plans, chapter A Formal Theory of Plan Recognition and its Implementation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "The Facts of Causation", "author": ["Dov Hugh Mellor"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Hypothesis discrimination with abstractions based on observation and action costs", "author": ["Gianluca Torta", "Daniele Theseider Dupr\u00e9", "Luca Anselma"], "venue": "In Alban Grastien and Markus Strumpter, editors, 19th Workshop on Principles of Diagnosis", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Explanations and arguments based on practical reasoning", "author": ["Douglas Walton"], "venue": "Workshop on Explanation-Aware Computing at IJCAI\u201909,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Then the causal formalism will produce various explanation links [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "By a causal model [8], we mean a representation of a body of causal relationships to be used to generate arguments that display explanations for a given set of facts.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "As just seen, the enriched causal graph allows us to infer explanations for assertions and these explanations might be used in an argumentative context [2, 3].", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "As just seen, the enriched causal graph allows us to infer explanations for assertions and these explanations might be used in an argumentative context [2, 3].", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 74, "endOffset": 80}, {"referenceID": 5, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 74, "endOffset": 80}, {"referenceID": 1, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 8, "context": "Note however that some authors have already introduced ontology to be used for problem solving tasks as planning [7, Chapter 2] and more recently for diagnosis and repair [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "Although explanation and argumentation have long been identified as distinct processes [10], it is also recognized that the distinction is a matter of context, hence they both play a role [4] when it comes to eliciting an answer to a \u201cwhy\u201d question.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "Although explanation and argumentation have long been identified as distinct processes [10], it is also recognized that the distinction is a matter of context, hence they both play a role [4] when it comes to eliciting an answer to a \u201cwhy\u201d question.", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "[1] Philippe Besnard, Marie-Odile Cordier, and Yves Moinard.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Philippe Besnard and Anthony Hunter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Phan Minh Dung.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Justin Scott Giboney, Susan Brown, and Jay F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joseph Halpern and Judea Pearl.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Joseph Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Henry Kautz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Dov Hugh Mellor.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Gianluca Torta, Daniele Theseider Dupr\u00e9, and Luca Anselma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Douglas Walton.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A1, \u00b7 \u00b7 \u00b7 , An} causes B) and ontological links (o1 is_a o2). We present a system that provides a candidate explanation ({A1, \u00b7 \u00b7 \u00b7 , An} explains {B1, \u00b7 \u00b7 \u00b7 , Bm}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed : a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties.", "creator": "LaTeX with hyperref package"}}}