{"id": "1401.4436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction", "abstract": "The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.", "histories": [["v1", "Thu, 16 Jan 2014 04:53:44 GMT  (541kb)", "http://arxiv.org/abs/1401.4436v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["muhammad arshad ul abedin", "vincent ng", "latifur khan"], "accepted": false, "id": "1401.4436"}, "pdf": {"name": "1401.4436.pdf", "metadata": {"source": "CRF", "title": "Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction", "authors": ["Muhammad Arshad Ul Abedin", "Vincent Ng", "Latifur Khan"], "emails": ["arshad@student.utdallas.edu", "vince@hlt.utdallas.edu", "lkhan@utdallas.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2. Dataset", "text": "We used all 140,599 reports collected between January 1998 and December 2007. Each report contains a free-text statement written by the reporter and several specified fields about the incident such as time and place of the incident, environmental information, details about the aircraft involved, references of the reporting persons, details such as anomaly, detector, resolution and consequence of the incident itself, and a description of the situation. In other words, the specified fields in a report contain different information about what happened and under what physical circumstances, but do not cover the reasons for the incident. As discussed by Posse et al. (2005) and Ferryman, Posse, Rosenthal, Srivastava and Statler (2006), only the presentation of a report contains information about the shaping factors of the incident. For5. Available at http: / / asrs.arc.nasa.gov / search / data."}, {"heading": "2.1 Shaping Factors", "text": "The incidents described in the ASRS reports occur for a variety of reasons. Posse et al. (2005) focus on the 14 formation factors, or simply shapers. The following is a brief description of these formation factors, taken literally from the work of Posse et al. 1. Attitude: Any indication of an unprofessional or antagonistic attitude of an air traffic controller or flight crew. 2. Communication environment: interference with cockpit communications such as noise, auditory disturbances, radio traffic jams or language barriers. 3. Mandatory cycle: A strong indication of unusual working hours, e.g. a long day that is very late at night, exceeding service time allowances, short and insufficient rest periods."}, {"heading": "2.2 Preprocessing", "text": "For our semantic lexicon learning approach to creating identification, we need to (1) identify the part of the language (POS) of each word in the text, (2) the phrases or chunks in sentences, and (3) the grammatical roles of the words and their relevant words. Ideally, however, we would comment a section of the ASRS corpus manually to highlight the rest of the corpus. However, this in itself is a labor-intensive task and goes beyond the scope of this paper. Therefore, we have used publicly available tools trained on standard corpora for these three tasks. It is inevitable that this will not produce the most accurate automatic annotations of our corpus, but as we will see, this has no problem in this task.From our corpus, we first identify sentence boundaries that we use the XCRINGER 2000 tool."}, {"heading": "NO SIGN TO INDICATE WHICH RWY I WAS XING. I CLRED BOTH DIRECTIONS BEFORE XING. WE WERE THE ONLY ACFT ON THE FIELD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "AT THE TIME. NO MENTION ON THE ATIS OF SIGNS BEING OUT OR CONSTRUCTION ON THE RAMP AREA. THE CTLR DIDN\u2019T QUESTION", "text": "In fact, most of them are able to move to another world, where they can move to another world, where they can find their way to another world."}, {"heading": "2.3 Human Annotation Procedure", "text": "Remember that we need reports labeled with formation factors to train the classifiers for cause identification and test the performance of our two approaches to cause identification. In addition, in order to learn a semantic lexicon using bootstrapping, we need as a starting point a small number of seeds and phrases related to each formation factor. As a result, after language normalization, we performed two types of annotations: (1) the labeling of a series of reports with formation factors, and (2) the identification of a set of seeds and phrases from the reports. The annotation process is described in more detail in the following sections."}, {"heading": "2.3.1 Annotating Reports with Shaping Factors", "text": "While NASA has previously developed a heuristic approach to the task of determining the cause (Posse et al., 2005), this approach has been evaluated on only 20 manually commented reports, which is far from satisfactory in terms of establishing a strong base methodology. So we decided to comment on a series of reports ourselves to evaluate our methods of automatically finding the cause. From the complete set of 140,599 reports, we selected a random set of 1,333 reports for commenting, which subset was split into two parts, consisting of 233 reports, commented by two people (a student and a doctoral student).For each report, they were asked to answer the following question: Which design factor (s) was responsible for the incident described in the report? Our commentators were trained in a similar way to those who identified the 20 reports used in the assessment by NASA researchers (see Posse al, 2005)."}, {"heading": "2.3.2 Extracting Seed Words and Phrases", "text": "In a separate process, the first author went through the first 233 reports that both commentators have edited and selected words and phrases that are relevant to each of the form factors. His assessment of whether a word or phrase is relevant to a form factor was based on a careful reading of the description of the form factors in the works of Posse et al. (2005) and Ferryman et al. (2006), as well as the seed words selected by NASA experts shown in these two papers. In this case, the specific task was: Is there a word or phrase in each report indicative of any of the form factors? If there is, identify it and assign it to the corresponding form factors. Note that these seeds and phrases were selected without considering the form factor annotation of the document; they were selected for the possibility of their relevance to the respective form factors."}, {"heading": "3. Baseline System For Cause Identification", "text": "As discussed in the introduction, the goal of our research is to provide the incident reports with the formation factors that caused the incidents. In order to evaluate the performance of our cause-finding methods, we need a baseline that uses the same amount of training data as all the methods described in this paper and performs reasonably well in the test kit. In view of the fact that cause-finding is a relatively new and studied task, no standard basis was adopted for this task. In fact, the only related work to cause-finding in the field of aviation safety has been carried out by NASA researchers (see Posse et al., 2005; Ferryman et al., 2006). As a result, we construct a base system motivated by Posse et al. \"s work. Specifically, the baseline takes as input a set of seeds and phrases collected manually for each of the formation factors (see Section 2.3.2), and label a report with the precursor hay factor for each section of the form factor (see section 11)."}, {"heading": "4. Semantic Lexicon Learning", "text": "As described in Section 3, the baseline uses the seeds and phrases manually extracted from 233 reports, in combination with the occurrence heuristic, to mark the reports with formation factors. However, the reports may not contain exactly the same words and phrases, but they may contain various variations, synonyms, or words and phrases that are semantically similar to the seeds and phrases in the 140,599 report corpus. To address this potential problem, we suggest using semantic lexicon learning algorithms to semantically learn more words and phrases that would have required enormous amounts of human effort."}, {"heading": "4.1 Weakly Supervised Lexicon Learning", "text": "As mentioned above, we use a weakly supervised bootstrapping approach to build the semantic lexicon, and then select words and phrases from the uncommented reports that are semantically similar to the words already present in the semantic lexicon. Reports in the corpus do not need to be labeled with form factors; the semantic similarity between two words is measured by features that are extracted from the corpus for each word. This process is repeated iteratively: in each iteration, a certain number of words are added to the semantic lexicon, and the words in this extended lexicon are used as seeds for the following iteration. This process is illustrated in Figure 2."}, {"heading": "4.2 Basilisk Framework", "text": "In fact, the fact is that most of them will be able to be in a position to be in a position to be able to move, to be able to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "4.3 Modifications to the Basilisk Framework", "text": "As we will see later in this section, an analysis of the framework shows that in some cases, the words selected by Basilisk may not be the most relevant. Therefore, we propose three algorithmic modifications to the Basilisk framework: (1) use of a new semantic measure of similarity, (2) merge the word pools into a single pool for mapping words to the semantic categories, and (3) introduce a minimum of support and maximum generality criteria for patterns and words to be added to the pattern pools and word pools. In addition, we propose a linguistic modification in which we use a type of trait that can be robustly calculated from the words and phrases in the corpus, namely the N-gram characteristics."}, {"heading": "4.3.1 Modification 1: New Semantic Similarity Measure", "text": "Let's take a look at section 4.2, where the AvgLog score function is used to measure the semantic similarity between the words."}, {"heading": "4.3.2 Modification 2: Common Word Pool", "text": "Since we have to calculate Eqn (4) for each word in the word pool for each of the categories and assign the word to the semantic category for which the probability is highest, we modify the framework so that we have only one common word pool for each of the semantic categories. There are still separate pattern pools for different semantic categories, but the words referring to patterns in the pattern pools are placed in the same common word pool and from there placed in the most likely semantic category. If there are separate word pools for each semantic category, we must add a fixed number of words to each category in each iteration, and such a constraint can undesirably cause a word to be added to a category that is not the most likely. However, since we have only one word pool after our modification, we do not have the constraint that we have to add a fixed number of words to each category, and we can assign each word to its most likely category."}, {"heading": "4.3.3 Modification 3: Minimum Support and Maximum Generality", "text": "There are some scenarios in which SemProb metrics can lead to undesirable results. For example, consider a very rare word Wi, which occurs exactly once in the entire corpus. Suppose that the pattern Pj, which extracts Wi, extracts words of the semantic category Sk with a probability of 70%. Thus, according to SemProb, the probability that Wi belongs to Sk is 70%. However, this is not sufficient proof that Wi belongs to Sk. Such cases are not too rare, we have imposed a minimal word frequency restriction on the words that are placed in the word pool, so that words that appear less than a certain number of times are not taken into account. A pattern that appears too rarely in the corpus can also lead to such a problem. Let's consider a very rare pattern, Pj, which appears exactly twice in the corpus and extracts two words."}, {"heading": "4.3.4 Modification 4: N-gram Patterns", "text": "< < < < > > > > Patterns already used by Basilisk, we also employ N-gram-based patterns, with the aim of capturing the context in which the words appear more robustly. We construct N-gram patterns as follows. \u2022 For each noun and adjective, X, in the corpus, we create two N-gram patterns to extract X: (a) the preceding N-words + < X >, and (b) the following N-words. For example, in the sentence... \"a solid line of thunderstorms has been detected...,\" the Bigram patterns for \"thunderstorms\" would be \"and\" X. \""}, {"heading": "5. Semantic Lexicon-Based Approaches to Cause Identification From", "text": "ASRS ReportsWe examine a heuristic and a learning-based approach to inducing identifications, both of which use the information of an automatically acquired semantic lexicon. In this section, the details of these two approaches are described."}, {"heading": "5.1 Heuristic-Based Approach", "text": "The heuristic approach works essentially in the same way as the cause identification system described in Section 3, where the heuristics of occurrence are used to mark a report with formation factors, the only difference being that the words and phrases used by the heuristics of occurrence in the baseline are manually identified, while the words and phrases of our heuristic approach are acquired through our Basilisk Modified Procedure."}, {"heading": "5.2 Learning-Based Approach", "text": "Note that we have a multi-level, multi-level classification task: there are 14 classes and each report can be labeled with more than one class. A number of approaches have been proposed to tackle multi-level, multi-level, multi-level classification tasks. Later in this section, we will describe the three existing approaches to multi-level, multi-level text classification that we are exploring in our experiments (Section 5.2.1) and give an overview of the theory of Support Vector Machines (SVMs), the underlying learning algorithm we use to train classifiers working with these three approaches (Section 5.2.2)."}, {"heading": "5.2.1 Three Approaches to Multi-Class Multi-Labeled Text Classification", "text": "In fact, most of them are able to survive on their own, most of them are able to survive on their own, without being able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, and most of them are able to survive on their own."}, {"heading": "5.2.2 An Overview of Support Vector Machines", "text": "In the following, we describe two versions of SVMs: (1) inductive SVMs that learn a classifier solely from marked data, and (2) transductive SVMs that learn a classifier from the two marked and unmarked data. (1) Inductive SVMs that contain a set of data points belonging to two classes, an inductive SVM aims to find a separating hyperplane that maximizes the distance from the separating hyperplane to the next data points. These closest data points act as support vectors for the plane.Formally, we let D be the data set with m points, whereas the hyperplane n, ci) xi-xi-R n, ci-1), 1}, 1 \u2264 i \u2264 m} (5) Each xi point is represented as an n-dimensional vector and is associated with a class tag."}, {"heading": "6. Evaluation", "text": "The aim of our evaluation is to examine the effectiveness of our two approaches to causing the identification, namely the semantic lexicon learning approach and the classification approach. To this end, we test the performance of the approaches using randomly selected reports that have been manually commented on with the formation factors that caused the incidents described in them (Section 2.3.1). We begin by describing the setup of the experiment (Section 6.1), followed by the baseline results (Section 6.2) and the performance of our two approaches (Section 6.3 and 6.4). We then describe the experiment in which we increase the amount of training data available to the classification approach and examine how this affects performance (Section 6.5). We then conduct an analysis of the flaws of the best performance approach (Section 6.6) and conduct additional experiments to gain a better understanding of the root cause problem that may help us in future research (Section 6.7)."}, {"heading": "6.1 Experimental Setup", "text": "As described in Section 2.3, of the 140,599 reports throughout the corpus, we have commented on 1333 incident reports with formation factors manually, the first 233 of which we have used to (1) manually extract the initial words and phrases for the semantic lexicon learning process, and (2) train classifiers to identify formation factors associated with a report. Of the remaining reports, we have used 1000 reports as test data and 100 reports as development data (for parameter optimization)."}, {"heading": "6.1.1 Evaluation Metrics", "text": "As mentioned in Section 2.1, there are 14 shaping factors, and a report can be labeled with one or more of these shaping factors. We evaluate the performance of our approaches to finding the cause based on how well the automatic annotations match the human annotations of the reports in the test set. To evaluate, we use precision, callback, and F-measure, which are calculated as described by Sebastiani (2002). Specifically, for each shaping factor Si, i = 1, 2,.. 14, let ni be the number of reports in the test set that the human marker labeled Si, i.e. the number of real Si-labeled reports in the test set. Further, let pi be the number of reports labeled Si by an automatic labeling scheme labeling Ci with Si, i.e. the number of reports labeled Si in the test set."}, {"heading": "6.1.2 Statistical Significance Tests", "text": "To determine whether one labeling scheme is better than another, we use two statistical significance tests - the McNemar test (Everitt, 1977; Dietterich, 1998) and the stratified approximate randomization test (Noreen, 1989) - to test whether the difference in performance is really statistically significant. McNemar's test compares two labeling schemes based on errors (i.e., whether both labeling schemes make the same errors), and the stratified approximate randomization test compares the labeling schemes in the F scale. Both tests have been widely applied in machine learning and in the NLP literature. Specifically, stratified approximate randomization is the standard significance test used by the organizers of the Measure Understanding Conferences to determine whether the difference in F measurements obtained by two information extraction systems is significant to us. Specifically, the stratified approximate randomization is the standard significance test used by the organizers of the Measure to determine whether the difference in F measurements obtained by two information extraction systems is significant to us."}, {"heading": "6.2 Baseline System", "text": "Let's remember that starting point is the heuristic method described in Section 3, where occurrence heuristics are used to label a report with the seed words and phrases manually extracted from the 233 training reports. Results presented in Section Experiment 1 of Table 4 are reported in terms of precision (P), recall (R), and F measure (F. The last two columns show whether a particular automatic labeling scheme is significantly better than the basic scheme with respect to the McNemar test (MN) and stratification randomization test (AR). [Statistical significance and insignificance are indicated by an X and an X, respectively] When evaluating the 1000 reports in the test set, the baseline achieves a precision of 56.48%, a recall of 40.47%, and an F measure of 47.15%."}, {"heading": "6.3 Experiments with Semantic Lexicon Approach", "text": "Let us remember that in the semantic lexicon learning approach we label a report in a test sentence with the occurrence heuristic in combination with the semantic lexicon learned through the modified Basilisk framework described in Section 4.3. Before we show the results of this approach, we first describe how we adjust the parameters of the modified Basilisk framework."}, {"heading": "6.3.1 Parameters", "text": "The first four are the thresholds resulting from the four frequency-based constraints that provide minimal support and maximum generality (see Modification 3 in Section 4.3.3). Specifically, the four \"threshold pattern\" parameters are (1) the minimum frequency of a word (MinW), (2) the maximum frequency of a word (MaxW), (3) the minimum frequency of a pattern (MinP), and (4) the maximum number of words extracted by a pattern (MaxP). In addition, we have three types of patterns (namely subject-verb-object patterns, bigram patterns to extract words). Our fifth parameter is the \"pattern\" that determines what subset of these three types of patterns is used."}, {"heading": "6.3.2 Results", "text": "The semantic lexicon, which was learned using the best combination of parameters (based on performance on the development set), is used to identify the reports of the test set. As we can see from line 1 of Experiment 2 of Table 4, the Modified Basilisk Approach achieves an accuracy of 53.15%, a recall of 47.57% and an F-measure of 50.21%. Compared to the baseline, this method has a lower accuracy and a higher recall. The increased recall shows that morereports are covered by the extended lexicon. However, the lexicon learned also contains some general words that have led to a decrease in precision. Overall, it has a higher measure that is statistically significantly better than that of the baseline after both significance tests. This confirms our premise that learning more words and phrases relevant to the formation factors helps us identify the formation factors from more reports."}, {"heading": "6.3.3 Results Using Original Basilisk", "text": "In order to better understand whether our proposed linguistic and algorithmic modifications of the Basilisk framework (see section 4.3) are actually advantageous for our task, we repeated the experiment described above, except that we replaced the lexicon generated with the modified Basilisk framework with a lexicon generated with the original Basilisk framework. Specifically, we implemented the original Basilisk framework as described by Thelen and Riloff (2002), but with one slight difference: in the case of Bigram patterns that extract phrases, the word pools described in section 4.2 were populated with whole phrases instead of just headers. This was because the seed word list extracted in section 2.3.2 contains both words and phrases, and therefore we would like to learn whole phrases. The only parameter that can be tuned for the original Basilisk framework is the pattern set determined by the types mentioned above."}, {"heading": "6.4 Experiments with Classification Approach", "text": "Remember that in the causation classification approach, we train an SVM classifier for each form factor Sk to determine whether a report should be referred to as Sk. As desired, this approach allows a report in the test set to potentially have multiple names, as the resulting 14 SVM classifiers are applied independently to each report. To investigate the effect of different sets of characteristics on causation performance, we use five sets of characteristics in our experiments: (1) unigrams only; (2) unigrams and bigrams; (3) lexicon words only; (4) unigrams and lexicon words; and (5) unigrams, bigrams and lexicon words. The unigrams and bigrams were generated from the reports in the training set by first removing stopwords and ignoring case information, while the semantic lexicon constructed by our modified Basilisk framework. Before describing the results of our supervised and transductive experiments, we first demonstrated the parameters."}, {"heading": "6.4.1 Parameters", "text": "The first parameter is the percentage of attributes to be used. It has been shown that performance in text classification is improved (Yang & Pedersen, 1997), so we do not apply the attributes to the lexicon words, but apply the attributes only to the unigrams and bigrams."}, {"heading": "6.4.2 Supervised One-Versus-All Classifiers: Results and Discussions", "text": "The results of the reviewed One Versus All classification approach with the five features described above are shown in Experiment 3 of Table 4.14, as we can see when using features 1 (uniques only) and 4 (unigrams and lexicon words), we achieve the best results - F readings of 47.46% and 47.45%, even these best results being statistically indistinguishable from the base result (according to the approximate randomization test), and are significantly worse than the result generated by the reviewed approach."}, {"heading": "6.4.3 Transductive One-Versus-All Classifiers: Results and Discussions", "text": "To investigate whether it makes sense to use unlabeled data, we use transductive SVM to combine labeled and unlabeled data. In essence, we repeated the experiments in the monitored One Versus All classification approach, except that we trained each transductive SVM classifier using both the (labeled) reports in the training set and the (unlabeled) reports in the test set in Section 5.2.2. The two parameters - the percentage of features used and the classification threshold - are jointly tuned to maximize the F measure on the development set, as described in the monitored approach, except that the transductive SVMs used in the parameter tuning step are trained on the training set as labeled data and the development set as unlabeled data. The results of these transductive SVM classifiers are shown in Experiment 4, section of Table 4. Overall, the transductive results are significantly worse than the corresponding results in Experiment 3."}, {"heading": "6.4.4 Results From Additional Supervised Approaches", "text": "Next, we present the results of the two other monitored approaches, namely MetaLabeler and ensembles of truncated groups (Section 5.2.1). The characteristics used by both approaches are the same as those used by the One Versus All method. As in the OneVersus All method, both approaches use SVM as the underlying learning algorithm. The only parameters that need to be adjusted for the MetaLabeler approach are the proportion of characteristics (N) to be used that were selected based on the classification (F measure). The results of the MetaLabeler approaches are shown in Table 5."}, {"heading": "6.5 Experiments Using Additional Training Data", "text": "This year it is more than ever before."}, {"heading": "6.6 Error Analysis and Lessons Learned", "text": "In fact, it is the case that most of us are able to abide by the rules that we have imposed on ourselves. (...) In fact, it is the case that we are able to change the rules. (...) It is not the case that we are able to change the rules. (...) It is not the case that we are able to change the rules. (...) \"\" It is not the case that we have to abide by the rules. (...) \"\" It is the case that we are not able to change the rules. \"(...). (...)\" (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (). \"(...).\" (). \"(...).\" (...). (). \"(...).\" (). \"(...).\" (). \"(...).\" (). \"().\" (). \"(...).\" (). \"().\" (...). \"().\" (). \"().\" (). \"().\" (...). \"().\" (). \"().\" (). \"().\" (). \").\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\"). \"().\" (). \"().\" (). \"().\" (). \"(.\" ().). \"().\" (. \").\" (. \").\"). \"(.\"). \"().\" (). \"().\" (. \").\" (). \"(.\"). \"(.\"). \"().\" (). \").\" (. \"().\"). \"().\" (). \"().\"). \"().\" (). \"(.\" (). \"().\" (). \"().\" (). \").\" (). \"().).\" ()."}, {"heading": "6.7 Additional Analyses", "text": "In this section, we present the results of a series of additional analyses that we have carried out in relation to our cause-finding task and our approaches to this task. In Section 6.7.1, we will examine the relative difficulties in classifying the different formation factors. In Section 6.7.2 and 6.7.3, we will show the learning curves of the semantic lexicon-based approach and the learning-based approach respectively, i.e. how the performance of these two approaches differs since they are supplied with different amounts of training data. Finally, in Section 6.7.4, we will discuss the results of the experiment to determine whether our modifications of the Basilisk framework are useful for learning general semantic categories."}, {"heading": "6.7.1 Per-Class Results", "text": "To get an insight into which of the classes are difficult to classify, we perform an analysis of the class performance of two identification schemes: the best heuristic method (i.e. the heuristics of occurrence using the lexicon learned through the modified Basilisk framework) [see the first part of Table 7] and the best learning-based method (i.e. the 5x SVM classifiers that use unigrams, bigrams and the words of the lexicon as characteristics) [see the second part of Table 7]. In conjunction with Table 1, two classes stand out as the most difficult to classify - illusion and task setting. Both classes are very little represented in the training, testing and development sets, have a very small number of seed words and result in a very poor performance through each of the approaches. The more easily identifiable classes were Physical Environment, Physical Factors, Resource Deficiency and Preoccupation, in which both classification schemes are very little represented, and we believe that these identification numbers are more useful than the general indicators in education, and that the differences in the classes were more reasonable."}, {"heading": "6.7.2 Lexicon Learning Curve", "text": "As mentioned in Section 2.3.2, we used a total of 177 seed words and phrases for each formation factor. At first glance, this number of seeds may seem large in terms of bootstrapping experiments, but, given that these 177 seeds are distributed across 14 formation factors, we only have an average of 12.6 words and phrases per formation factor. Nevertheless, it would still be interesting to investigate how the cause identification performance is affected if we reduce the number of seeds for each formation factor used by Modified Basilisk in the bootstrapping process. As a result, we conducted a series of experiments to measure the cause identification performance used by the semantic lexicon learned from Modified Basilisk when there are different seed words where the specific parameters of Modified Basilisk are set as described in Section 6.3.1. More specifically, we chose the top 3, 4, 7, 7, and 7 seed factors for each formation factor."}, {"heading": "6.7.3 SVM Learning Curve", "text": "As discussed in Section 6.5, we assume that the failure of the SVM classifiers to perform better than the baseline is due to the scarcity of training instances available to the learner. To answer this question, we draw a learning curve for the One Versus All Classification approach, using as characteristics a combination of unigrams, bigrams and lexicon word features before we can see a statistically significant decrease in causation, which is the best performance in Table 4. Specifically, we generated random subsets of training sets of sizes 50, 100,... parameters, namely the percentage of features and the classification threshold, were selected in the same way as the original experiment was described in Section 6.5, and the F measurement was evaluated using the entire test system."}, {"heading": "6.7.4 General Usefulness of Our Modifications to Basilisk", "text": "To test whether the modifications we have made in the Basilica are useful to improve reading in general, we added two general categories to the design factors, namely people and equipment. These two categories were added because firstly, words and sentences that have been included in these categories are easy to check (i.e., whether they are words or phrases that represent people or devices), and secondly, because they are similar to the original context in which the framework of Basilisk was originally evaluated (i.e., learning terms in the categories construction, EVENT, LOCATION, TIME and WEAPON arise from the terrorism reports. These two additional categories were in which the Samen-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading-Reading"}, {"heading": "6.8 Summary of Conclusions", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Related Work", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "8. Conclusions", "text": "This year it is more than ever before."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers who have provided us with comments that have been invaluable in improving the quality of the essay. This research has been partially supported by NASA funding NNX08AC35A. Any opinions, findings, conclusions or recommendations expressed in this essay are those of the authors and do not necessarily reflect the views or official guidelines of NASA, either express or imply."}, {"heading": "Appendix A: Seed Words", "text": "In the following, the seed words are taken manually from the 233 reports in training (see section 2.3.2).Shaping Factor Seed wordsAttitude get HOMEITIS, attitude, inattentiveness, get THEREITIS, complacency, arrogance, inattentiveness, familiarity with the environment, constipation, uncertainty Duty Cycle 11 hours day, insufficient rest, last of four legs, diminished hibernation, full-day flight, all night familiarity, unfamiliar, first exit, uncertainty, low susceptibility, low time, bright lights, confused glances, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused, confused."}, {"heading": "Appendix B: Sample Lexicon Words Learned by Modified Basilisk", "text": "Below are the semantic words that were learned through the modified Basilisk framework in the first two iterations.Shaping Factor New wordsAttitude Communication Environment Duty Cycle Familiarity aligned, relatively new, familiar illusion Other initial confusion, minimal fuel distress, misunderstandings, weather emergency Physical EnvironmentTRWS, conflict message, cumulonimbus, large cells, numerous thunderstorms, occasional severe thunderstorm cells, weather superstructures, weather cell, weather en routePhysical Factors first factor Preoccupation adequate attention, as much attention, attention, attention close enough, attention of the crew, sufficient attention, much attention, adequate attention, strict attention, very precise attention Pressure competence resource shortage different, bad, wrong, obviously wrong, resulting loss, seriously wrong, minor loss, temporary loss, terribly wrong, very wrong burden of duty unexpected"}, {"heading": "Appendix C: Sample Lexicon Words Learned by Original Basilisk", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "Appendix D: Additional Stratified Approximate Randomization Tests", "text": "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X"}, {"heading": "Appendix E: Sample Preprocessed Reports", "text": "It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is."}], "references": [{"title": "Semantic lexicon construction: Learning from unlabeled data via spectral analysis", "author": ["R.K. Ando"], "venue": "Proceedings of the 8th Conference on Computational Natural Language Learning, pp. 9\u201316.", "citeRegEx": "Ando,? 2004", "shortCiteRegEx": "Ando", "year": 2004}, {"title": "Inter-coder agreement for computational linguistics", "author": ["R. Artstein", "M. Poesio"], "venue": "Computional Linguistics,", "citeRegEx": "Artstein and Poesio,? \\Q2008\\E", "shortCiteRegEx": "Artstein and Poesio", "year": 2008}, {"title": "Automatic expansion of domain-specific lexicons by term categorization", "author": ["H. Avancini", "A. Lavelli", "F. Sebastiani", "R. Zanoli"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Avancini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Avancini et al\\.", "year": 2006}, {"title": "Distributional clustering of words for text classification", "author": ["L.D. Baker", "A.K. McCallum"], "venue": "In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Baker and McCallum,? \\Q1998\\E", "shortCiteRegEx": "Baker and McCallum", "year": 1998}, {"title": "Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing", "author": ["M. Banko", "E. Brill"], "venue": "In Proceedings of the 1st International Conference on Human Language Technology Research", "citeRegEx": "Banko and Brill,? \\Q2001\\E", "shortCiteRegEx": "Banko and Brill", "year": 2001}, {"title": "Web 1T 5-gram Version 1. Linguistic Data", "author": ["T. Brants", "A. Franz"], "venue": null, "citeRegEx": "Brants and Franz,? \\Q2006\\E", "shortCiteRegEx": "Brants and Franz", "year": 2006}, {"title": "From covariation to causation: A causal power theory", "author": ["P.W. Cheng"], "venue": "Psychological Review, 104 (2), 367\u2013405.", "citeRegEx": "Cheng,? 1997", "shortCiteRegEx": "Cheng", "year": 1997}, {"title": "The statistical significance of the MUC-4 results", "author": ["N. Chinchor"], "venue": "Proceedings of the 4th Message Understanding Conference, pp. 30\u201350.", "citeRegEx": "Chinchor,? 1992", "shortCiteRegEx": "Chinchor", "year": 1992}, {"title": "Evaluating message understanding systems: An analysis of the Third Message Understanding Conference (MUC-3)", "author": ["N. Chinchor", "L. Hirschman", "D.D. Lewis"], "venue": "Computational Linguistics,", "citeRegEx": "Chinchor et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Chinchor et al\\.", "year": 1993}, {"title": "On the algorithmic implementation of multiclass kernelbased vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Improvements in automatic thesaurus extraction", "author": ["J.R. Curran", "M. Moens"], "venue": "In Proceedings of the ACL 2002 Workshop on Unsupervised Lexical Acquisition,", "citeRegEx": "Curran and Moens,? \\Q2002\\E", "shortCiteRegEx": "Curran and Moens", "year": 2002}, {"title": "Minimising semantic drift with mutual exclusion bootstrapping", "author": ["J.R. Curran", "T. Murphy", "B. Scholz"], "venue": "In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "Curran et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words", "author": ["D. Davidov", "A. Rappoport"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Davidov and Rappoport,? \\Q2006\\E", "shortCiteRegEx": "Davidov and Rappoport", "year": 2006}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["T.G. Dietterich"], "venue": "Neural Computation, 10 (7), 1895\u20131923.", "citeRegEx": "Dietterich,? 1998", "shortCiteRegEx": "Dietterich", "year": 1998}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["T. Dunning"], "venue": "Computational Linguistics, 19 (1), 61\u201374.", "citeRegEx": "Dunning,? 1993", "shortCiteRegEx": "Dunning", "year": 1993}, {"title": "The Analysis of Contingency Tables", "author": ["B.S. Everitt"], "venue": "Chapman and Hall.", "citeRegEx": "Everitt,? 1977", "shortCiteRegEx": "Everitt", "year": 1977}, {"title": "What happened, and why: Toward an understanding of human error based on automated analyses of incident reports", "author": ["T.A. Ferryman", "C. Posse", "L.J. Rosenthal", "A.N. Srivastava", "I.C. Statler"], "venue": "Volume II. Tech. rep. NASA/TP\u20132006-213490, National Aeronautics and Space Administration", "citeRegEx": "Ferryman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ferryman et al\\.", "year": 2006}, {"title": "COATIS, an NLP system to locate expressions of actions connected by causality links", "author": ["D. Garcia"], "venue": "Proceedings of the 10th European Workshop on Knowledge Acquisition, Modeling and Mangement, pp. 347\u2013352.", "citeRegEx": "Garcia,? 1997", "shortCiteRegEx": "Garcia", "year": 1997}, {"title": "Automatic detection of causal relations for question answering", "author": ["R. Girju"], "venue": "Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering, pp. 76\u201383.", "citeRegEx": "Girju,? 2003", "shortCiteRegEx": "Girju", "year": 2003}, {"title": "Discriminative methods for multi-labeled classification", "author": ["S. Godbole", "S. Sarawagi"], "venue": "In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Godbole and Sarawagi,? \\Q2004\\E", "shortCiteRegEx": "Godbole and Sarawagi", "year": 2004}, {"title": "Structure and strength in causal induction", "author": ["T.L. Griffiths", "J.B. Tenenbaum"], "venue": "Cognitive Psychology,", "citeRegEx": "Griffiths and Tenenbaum,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Tenenbaum", "year": 2005}, {"title": "Causal and temporal text analysis: The role of the domain model", "author": ["R. Grishman", "T. Ksiezyk"], "venue": "In Proceedings of the 13th International Conference on Computational Linguistics,", "citeRegEx": "Grishman and Ksiezyk,? \\Q1990\\E", "shortCiteRegEx": "Grishman and Ksiezyk", "year": 1990}, {"title": "Causes and explanations: A structural-model approach. Part I: Causes", "author": ["J.Y. Halpern", "J. Pearl"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Halpern and Pearl,? \\Q2005\\E", "shortCiteRegEx": "Halpern and Pearl", "year": 2005}, {"title": "Unsupervised information extraction approach using graph mutual reinforcement", "author": ["H. Hassan", "A. Hassan", "O. Emam"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hassan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2006}, {"title": "Original work published 1748))", "author": ["D. Hume"], "venue": "An Enquiry Concerning Human Understanding", "citeRegEx": "Hume,? \\Q1999\\E", "shortCiteRegEx": "Hume", "year": 1999}, {"title": "Original work published 1739))", "author": ["D. Hume"], "venue": "A Treatise of Human Nature", "citeRegEx": "Hume,? \\Q2000\\E", "shortCiteRegEx": "Hume", "year": 2000}, {"title": "Advances in Kernel Methods - Support Vector Learning, chap", "author": ["T. Joachims"], "venue": "Making large-Scale SVM Learning Practical. MIT-Press.", "citeRegEx": "Joachims,? 1999", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proceedings of the 10th European Conference on Machine Learning, pp. 137\u2013142.", "citeRegEx": "Joachims,? 1998", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proceedings of the 16th International Conference on Machine Learning, pp. 200\u2013209.", "citeRegEx": "Joachims,? 1999", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Knowledge-based acquisition of causal relationships in text", "author": ["R. Kaplan", "G. Berry-Rogghe"], "venue": "Knowledge Acquisition,", "citeRegEx": "Kaplan and Berry.Rogghe,? \\Q1991\\E", "shortCiteRegEx": "Kaplan and Berry.Rogghe", "year": 1991}, {"title": "KSC-PaL: A peer learning agent that encourages students to take the initiative", "author": ["C. Kersey", "B. Di Eugenio", "P. Jordan", "S. Katz"], "venue": "In Proceedings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Kersey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kersey et al\\.", "year": 2009}, {"title": "Extracting causal knowledge from a medical database using graphical patterns", "author": ["C.S.G. Khoo", "S. Chan", "Y. Niu"], "venue": "In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Khoo et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Khoo et al\\.", "year": 2000}, {"title": "Improving text categorization using the importance of sentences", "author": ["Y. Ko", "J. Park", "J. Seo"], "venue": "Information Processing and Management,", "citeRegEx": "Ko et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2004}, {"title": "Content analysis: An introduction to its methodology", "author": ["K. Krippendorff"], "venue": "Sage Publications, Inc.", "citeRegEx": "Krippendorff,? 2004", "shortCiteRegEx": "Krippendorff", "year": 2004}, {"title": "Causation", "author": ["D. Lewis"], "venue": "Journal of Philosophy, 70 (17), 556\u2013567.", "citeRegEx": "Lewis,? 1973", "shortCiteRegEx": "Lewis", "year": 1973}, {"title": "Dependency-based evaluation of MINIPAR", "author": ["D. Lin"], "venue": "Proceedings of the LREC Workshop on Evaluation of Parsing Systems, pp. 317\u2013329.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Induction of semantic classes from natural language text", "author": ["D. Lin", "P. Pantel"], "venue": "In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Text classification by bootstrapping with keywords, EM and shrinkage", "author": ["A. McCallum", "K. Nigam"], "venue": "In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,", "citeRegEx": "McCallum and Nigam,? \\Q1999\\E", "shortCiteRegEx": "McCallum and Nigam", "year": 1999}, {"title": "Computer-Intensive Methods for Testing Hypotheses : An Introduction", "author": ["E.W. Noreen"], "venue": "Wiley.", "citeRegEx": "Noreen,? 1989", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "Discovering word senses from text", "author": ["P. Pantel", "D. Lin"], "venue": "In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Pantel and Lin,? \\Q2002\\E", "shortCiteRegEx": "Pantel and Lin", "year": 2002}, {"title": "Automatically labeling semantic classes", "author": ["P. Pantel", "D. Ravichandran"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Pantel and Ravichandran,? \\Q2004\\E", "shortCiteRegEx": "Pantel and Ravichandran", "year": 2004}, {"title": "Computing reliability for coreference annotation", "author": ["R. Passonneau"], "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation, Vol. 4, pp. 1503\u20131506.", "citeRegEx": "Passonneau,? 2004", "shortCiteRegEx": "Passonneau", "year": 2004}, {"title": "Learning domain-specific information extraction patterns from the web", "author": ["S. Patwardhan", "E. Riloff"], "venue": "In Proceedings of the COLING/ACL Workshop on Information Extraction Beyond The Document,", "citeRegEx": "Patwardhan and Riloff,? \\Q2006\\E", "shortCiteRegEx": "Patwardhan and Riloff", "year": 2006}, {"title": "Effective information extraction with semantic affinity patterns and relevant regions", "author": ["S. Patwardhan", "E. Riloff"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Patwardhan and Riloff,? \\Q2007\\E", "shortCiteRegEx": "Patwardhan and Riloff", "year": 2007}, {"title": "Distributional clustering of English words", "author": ["F.C.N. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "CRFChunker: CRF English Phrase Chunker", "author": ["Phan", "X.-H."], "venue": "http://crfchunker. sourceforge.net/.", "citeRegEx": "Phan and X..H.,? 2006a", "shortCiteRegEx": "Phan and X..H.", "year": 2006}, {"title": "CRFTagger: CRF English POS Tagger", "author": ["Phan", "X.-H."], "venue": "http://crftagger. sourceforge.net/.", "citeRegEx": "Phan and X..H.,? 2006b", "shortCiteRegEx": "Phan and X..H.", "year": 2006}, {"title": "Exploiting role-identifying nouns and expressions for information extraction", "author": ["W. Phillips", "E. Riloff"], "venue": "In Proceedings of International Conference on Recent Advances in Natural Language Processing", "citeRegEx": "Phillips and Riloff,? \\Q2007\\E", "shortCiteRegEx": "Phillips and Riloff", "year": 2007}, {"title": "Contextual valence shifters", "author": ["L. Polanyi", "A. Zaenen"], "venue": "In Computing Attitude and Affect in Text: Theory and Applications,", "citeRegEx": "Polanyi and Zaenen,? \\Q2006\\E", "shortCiteRegEx": "Polanyi and Zaenen", "year": 2006}, {"title": "Extracting information from narratives: An application to aviation safety reports", "author": ["C. Posse", "B. Matzke", "C. Anderson", "A. Brothers", "M. Matzke", "T. Ferryman"], "venue": "In Proceedings of the 2005 IEEE Aerospace Conference,", "citeRegEx": "Posse et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Posse et al\\.", "year": 2005}, {"title": "Multi-label classification using ensembles of pruned sets", "author": ["J. Read", "B. Pfahringer", "G. Holmes"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining,", "citeRegEx": "Read et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Read et al\\.", "year": 2008}, {"title": "Automatically generating extraction patterns from untagged text", "author": ["E. Riloff"], "venue": "Proceedings of the 13th National Conference on Artificial Intelligence, pp. 1044\u20131049.", "citeRegEx": "Riloff,? 1996", "shortCiteRegEx": "Riloff", "year": 1996}, {"title": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction", "author": ["B. Roark", "E. Charniak"], "venue": "In Proceedings of the 17th International Conference on Computational Linguistics,", "citeRegEx": "Roark and Charniak,? \\Q1998\\E", "shortCiteRegEx": "Roark and Charniak", "year": 1998}, {"title": "Towards full automation of lexicon construction", "author": ["R. Rohwer", "D. Freitag"], "venue": "In Proceedings of the Computational Lexical Semantics Workshop at HLT-NAACL", "citeRegEx": "Rohwer and Freitag,? \\Q2004\\E", "shortCiteRegEx": "Rohwer and Freitag", "year": 2004}, {"title": "An efficient algorithm for building a distributional thesaurus (and other Sketch Engine developments)", "author": ["P. Rychl\u00fd", "A. Kilgarriff"], "venue": "In Proceedings of the ACL 2007 Demo and Poster Sessions,", "citeRegEx": "Rychl\u00fd and Kilgarriff,? \\Q2007\\E", "shortCiteRegEx": "Rychl\u00fd and Kilgarriff", "year": 2007}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys, 34 (1), 1\u201347.", "citeRegEx": "Sebastiani,? 2002", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "Overview of the fourth message understanding evaluation and conference", "author": ["B.M. Sundheim"], "venue": "Proceedings of the Fourth Message Understanding Conference, pp. 3\u2013", "citeRegEx": "Sundheim,? 1992", "shortCiteRegEx": "Sundheim", "year": 1992}, {"title": "Large scale multi-label classification via metalabeler", "author": ["L. Tang", "S. Rajan", "V.K. Narayanan"], "venue": "In Proceedings of International World Wide Web Conference,", "citeRegEx": "Tang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2009}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["M. Thelen", "E. Riloff"], "venue": "In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Thelen and Riloff,? \\Q2002\\E", "shortCiteRegEx": "Thelen and Riloff", "year": 2002}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "Tsoumakas and Katakis,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Katakis", "year": 2007}, {"title": "Random k -labelsets: An ensemble method for multilabel classification", "author": ["G. Tsoumakas", "I.P. Vlahavas"], "venue": "In Proceedings of the 18th European Conference on Machine Learning,", "citeRegEx": "Tsoumakas and Vlahavas,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Vlahavas", "year": 2007}, {"title": "Parametric mixture models for multi-labeled text", "author": ["N. Ueda", "K. Saito"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ueda and Saito,? \\Q2002\\E", "shortCiteRegEx": "Ueda and Saito", "year": 2002}, {"title": "Retrieving NASA problem reports: A case study in natural language information retrieval", "author": ["S. van Delden", "F. Gomez"], "venue": "Data and Knowledge Engineering,", "citeRegEx": "Delden and Gomez,? \\Q2004\\E", "shortCiteRegEx": "Delden and Gomez", "year": 2004}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In Proceedings of the 14th International Conference on Machine Learning,", "citeRegEx": "Yang and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}, {"title": "Counter-training in discovery of semantic patterns", "author": ["R. Yangarber"], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 343\u2013350.", "citeRegEx": "Yangarber,? 2003", "shortCiteRegEx": "Yangarber", "year": 2003}, {"title": "Using \u201cannotator rationales\u201d to improve machine learning for text categorization", "author": ["O.F. Zaidan", "J. Eisner", "C. Piatko"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Zaidan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zaidan et al\\.", "year": 2007}, {"title": "Graph mutual reinforcement based bootstrapping", "author": ["Q. Zhang", "Y. Zhou", "X. Huang", "L. Wu"], "venue": "Information Retrieval Technology,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "For instance, there has been some work on causal analysis for question answering, where a question may involve the cause(s) of an event (e.g., Kaplan & Berry-Rogghe, 1991; Garcia, 1997; Khoo, Chan, & Niu, 2000; Girju, 2003).", "startOffset": 136, "endOffset": 223}, {"referenceID": 18, "context": "For instance, there has been some work on causal analysis for question answering, where a question may involve the cause(s) of an event (e.g., Kaplan & Berry-Rogghe, 1991; Garcia, 1997; Khoo, Chan, & Niu, 2000; Girju, 2003).", "startOffset": 136, "endOffset": 223}, {"referenceID": 17, "context": ", Kaplan & Berry-Rogghe, 1991; Garcia, 1997; Khoo, Chan, & Niu, 2000; Girju, 2003). Here, the focus is on finding causal relationship between two sentence components. As another example, causal analysis on equipment malfunction reports have been attempted by Grishman and Ksiezyk (1990), whose work is restricted to the analysis of reports related to one specific piece of equipment they studied.", "startOffset": 31, "endOffset": 287}, {"referenceID": 51, "context": "Specifically, motivated by Thelen and Riloff\u2019s (2002) Basilisk framework, we learn a semantic lexicon, which consists of a set of words and phrases semantically related to each of the shaping factors, as follows.", "startOffset": 38, "endOffset": 54}, {"referenceID": 50, "context": "The first approach is a heuristic approach, which, motivated by Posse et al. (2005), labels a report with a shaping factor if it contains at least a word or a phrase that is relevant to the shaping factor.", "startOffset": 64, "endOffset": 84}, {"referenceID": 50, "context": "However, the use of these simple features is relevant for the task and is motivated by the work performed by the NASA researchers, who, as mentioned above, have manually identified seed words and phrases for each shaping factor (Posse et al., 2005).", "startOffset": 228, "endOffset": 248}, {"referenceID": 52, "context": "\u2022 We propose several modifications to Thelen and Riloff\u2019s (2002) semi-supervised lexicon learning framework, and show that our Modified Basilisk framework allows us to acquire a semantic lexicon that yields significantly better performance for cause identification than the original Basilisk framework.", "startOffset": 49, "endOffset": 65}, {"referenceID": 50, "context": "As discussed by Posse et al. (2005) and Ferryman, Posse, Rosenthal, Srivastava, and Statler (2006), only the narrative of a report contains information on the shaping factors of the incident.", "startOffset": 16, "endOffset": 36}, {"referenceID": 50, "context": "As discussed by Posse et al. (2005) and Ferryman, Posse, Rosenthal, Srivastava, and Statler (2006), only the narrative of a report contains information on the shaping factors of the incident.", "startOffset": 16, "endOffset": 99}, {"referenceID": 50, "context": "Posse et al. (2005) focus on the 14 shaping factors, or simply shapers.", "startOffset": 0, "endOffset": 20}, {"referenceID": 35, "context": "Also, the Minipar parser (Lin, 1998) is run on the sentences to identify the grammatical roles of the words.", "startOffset": 25, "endOffset": 36}, {"referenceID": 34, "context": "Also, the Minipar parser (Lin, 1998) is run on the sentences to identify the grammatical roles of the words. However, the report text has to be preprocessed before applying these tools for reasons described in the following paragraphs. The reports in the ASRS data set are usually informally written, using various domain specific abbreviations and acronyms. In general, as observed by van Delden and Gomez (2004), Posse et al.", "startOffset": 26, "endOffset": 414}, {"referenceID": 34, "context": "Also, the Minipar parser (Lin, 1998) is run on the sentences to identify the grammatical roles of the words. However, the report text has to be preprocessed before applying these tools for reasons described in the following paragraphs. The reports in the ASRS data set are usually informally written, using various domain specific abbreviations and acronyms. In general, as observed by van Delden and Gomez (2004), Posse et al. (2005) and Ferryman et al.", "startOffset": 26, "endOffset": 435}, {"referenceID": 16, "context": "(2005) and Ferryman et al. (2006), these narratives tend to be written in short, abbreviated manner, and tend to contain poor grammar.", "startOffset": 11, "endOffset": 34}, {"referenceID": 50, "context": "While NASA has previously developed a heuristic approach to tackle the cause identification task (Posse et al., 2005), this approach was evaluated on only 20 manually annotated reports, which is far from satisfactory as far as establishing a strong baseline method is concerned.", "startOffset": 97, "endOffset": 117}, {"referenceID": 42, "context": "After the annotations were completed, the inter-annotator agreement was computed using the Krippendorff\u2019s (2004) \u03b1 statistics as described by Artstein and Poesio (2008), using the Measuring Agreement on Set-valued Items (MASI) scoring metric (Passonneau, 2004).", "startOffset": 242, "endOffset": 260}, {"referenceID": 15, "context": "and Ferryman et al. (2006), both of which describe the shaping factors, and also give some examples of the words and phrases that indicate the influence of the shaping factors on the described incidents.", "startOffset": 4, "endOffset": 27}, {"referenceID": 15, "context": "and Ferryman et al. (2006), both of which describe the shaping factors, and also give some examples of the words and phrases that indicate the influence of the shaping factors on the described incidents. The definitions of the shapers are repeated in Section 2.1. Following Posse et al.\u2019s method, our annotators were explicitly instructed to adhere to these definitions as much as possible when annotating the reports with shaping factors. After the annotations were completed, the inter-annotator agreement was computed using the Krippendorff\u2019s (2004) \u03b1 statistics as described by Artstein and Poesio (2008), using the Measuring Agreement on Set-valued Items (MASI) scoring metric (Passonneau, 2004).", "startOffset": 4, "endOffset": 553}, {"referenceID": 1, "context": "After the annotations were completed, the inter-annotator agreement was computed using the Krippendorff\u2019s (2004) \u03b1 statistics as described by Artstein and Poesio (2008), using the Measuring Agreement on Set-valued Items (MASI) scoring metric (Passonneau, 2004).", "startOffset": 142, "endOffset": 169}, {"referenceID": 49, "context": "His judgment of whether a word or phrase is relevant to a shaping factor was based on a careful reading of the description of the shaping factors in the works of Posse et al. (2005) and Ferryman et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 16, "context": "(2005) and Ferryman et al. (2006), as well as the example seed words selected by the NASA experts that were shown in these two papers.", "startOffset": 11, "endOffset": 34}, {"referenceID": 16, "context": "In fact, to our knowledge, the only related works on cause identification for the aviation safety domain were conducted by the researchers at NASA (see Posse et al., 2005; Ferryman et al., 2006).", "startOffset": 147, "endOffset": 194}, {"referenceID": 52, "context": "For multi-category learning, Thelen and Riloff (2002) experimented with different scoring metrics and reported that they achieved the best performance by calculating the diff score for each word.", "startOffset": 40, "endOffset": 54}, {"referenceID": 55, "context": "is the maximum pattern generality constraint: motivated by Rychl\u00fd and Kilgarriff (2007), we remove from consideration patterns that are too general (i.", "startOffset": 59, "endOffset": 88}, {"referenceID": 9, "context": "In our implementation of this approach, the first model is learned using SVM, which is an implementation of multi-class SVM described by Crammer and Singer (2002)13.", "startOffset": 137, "endOffset": 163}, {"referenceID": 26, "context": "As implemented in the SVM software package by Joachims (1999) 13.", "startOffset": 46, "endOffset": 62}, {"referenceID": 26, "context": "SVMs have been shown to be very effective in text classification (Joachims, 1999).", "startOffset": 65, "endOffset": 81}, {"referenceID": 64, "context": "More details can be found in Cortes and Vapnik (1995). In our experiments, we use the radial basis function (RBF) kernel, where every dot product is replaced by the function k (x,x\u2032) = exp (", "startOffset": 40, "endOffset": 54}, {"referenceID": 26, "context": "As described by Joachims (1999), the goal is then to minimize the expected number of classification errors over the test set.", "startOffset": 16, "endOffset": 32}, {"referenceID": 26, "context": "As described by Joachims (1999), the goal is then to minimize the expected number of classification errors over the test set. The expected error rate is defined in Vapnik (1998) as follows:", "startOffset": 16, "endOffset": 178}, {"referenceID": 55, "context": "For evaluation, we use precision, recall and F-measure, which are computed as described by Sebastiani (2002). Specifically, for each shaping factor Si, i = 1, 2, .", "startOffset": 91, "endOffset": 109}, {"referenceID": 15, "context": "To determine whether a labeling scheme is better than another, we apply two statistical significance tests \u2014 McNemar\u2019s test (Everitt, 1977; Dietterich, 1998) and the stratified approximate randomization test (Noreen, 1989) \u2014 to test whether the difference in their performances is really statistically significant.", "startOffset": 124, "endOffset": 157}, {"referenceID": 13, "context": "To determine whether a labeling scheme is better than another, we apply two statistical significance tests \u2014 McNemar\u2019s test (Everitt, 1977; Dietterich, 1998) and the stratified approximate randomization test (Noreen, 1989) \u2014 to test whether the difference in their performances is really statistically significant.", "startOffset": 124, "endOffset": 157}, {"referenceID": 39, "context": "To determine whether a labeling scheme is better than another, we apply two statistical significance tests \u2014 McNemar\u2019s test (Everitt, 1977; Dietterich, 1998) and the stratified approximate randomization test (Noreen, 1989) \u2014 to test whether the difference in their performances is really statistically significant.", "startOffset": 208, "endOffset": 222}, {"referenceID": 35, "context": "To better understand whether our proposed linguistic and algorithmic modifications to the Basilisk framework (see Section 4.3) are indeed beneficial to our cause identification task, we repeated the experiment described above, except that we replaced the lexicon generated using the modified Basilisk framework with one generated using the original Basilisk framework. More specifically, we implemented the original Basilisk framework as described by Thelen and Riloff (2002), but with one minor difference: in the case of the bigram patterns extracting phrases, the word pools described in Section 4.", "startOffset": 42, "endOffset": 476}, {"referenceID": 51, "context": "Among the parameters of the ensembles of pruned sets approach, the number of classifiers in the ensemble, M , and the size of the sample of the training data on which each classifier in the ensemble was trained, were chosen to be the same ones used by Read et al. (2008), namely 10 and 63% respectively.", "startOffset": 252, "endOffset": 271}, {"referenceID": 43, "context": "For example, Patwardhan and Riloff (2007) discuss a relevant sentence classifier that is trained on a small set of seed patterns and a set of documents marked as relevant and irrelevant that can be useful in this context.", "startOffset": 13, "endOffset": 42}, {"referenceID": 52, "context": "The seed words for these two categories were selected in the same manner as done by Thelen and Riloff (2002), i.", "startOffset": 95, "endOffset": 109}, {"referenceID": 50, "context": "As mentioned previously, their use is motivated by the labor-intensive procedure that the NASA researchers employed in manually identifying seed words and phrases for each shaping factor (Posse et al., 2005).", "startOffset": 187, "endOffset": 207}, {"referenceID": 5, "context": "Notable investigations on causation in the field of psychology include those by Cheng (1997), who defines causation in terms of the probabilistic contrast model ; Griffiths and Tenenbaum (2005), who discuss learning about cause and effect relationships using causal graphical models; and Halpern and Pearl (2005), who provide explanations of causality by means of structural equations governing random variables representing events.", "startOffset": 80, "endOffset": 93}, {"referenceID": 5, "context": "Notable investigations on causation in the field of psychology include those by Cheng (1997), who defines causation in terms of the probabilistic contrast model ; Griffiths and Tenenbaum (2005), who discuss learning about cause and effect relationships using causal graphical models; and Halpern and Pearl (2005), who provide explanations of causality by means of structural equations governing random variables representing events.", "startOffset": 80, "endOffset": 194}, {"referenceID": 5, "context": "Notable investigations on causation in the field of psychology include those by Cheng (1997), who defines causation in terms of the probabilistic contrast model ; Griffiths and Tenenbaum (2005), who discuss learning about cause and effect relationships using causal graphical models; and Halpern and Pearl (2005), who provide explanations of causality by means of structural equations governing random variables representing events.", "startOffset": 80, "endOffset": 313}, {"referenceID": 17, "context": "For instance, Girju (2003) describes a method for automatically discovering lexico-semantic patterns that refer to causation.", "startOffset": 14, "endOffset": 27}, {"referenceID": 17, "context": "For instance, Girju (2003) describes a method for automatically discovering lexico-semantic patterns that refer to causation. In particular, she focuses on the explicit intra-sentential pattern \u3008NP1 verb NP2\u3009, where verb is a simple causative. She also shows how these patterns can be used to improve the performance of a system for answering cause-effect type questions. Khoo et al. (2000) use graphical pattern matching to identify causal relations from medical article abstracts.", "startOffset": 14, "endOffset": 391}, {"referenceID": 17, "context": "Similarly, Garcia (1997) uses hand-crafted extraction patterns to identify causal relations from sentences in the French language.", "startOffset": 11, "endOffset": 25}, {"referenceID": 49, "context": "NASA\u2019s own research on identifying causes of incidents from the report narratives have been performed by Posse et al. (2005), who describe a specific experiment in which they brought together experts to manually analyze the report narratives and identify words, phrases and expressions related to each of the shaping factors, as mentioned earlier.", "startOffset": 105, "endOffset": 125}, {"referenceID": 16, "context": "Later work by Ferryman et al. (2006) take these manually extracted expressions as ground truth and compare the anomalies described in the reports to the shaping factors derived from applying these expressions to the same reports.", "startOffset": 14, "endOffset": 37}, {"referenceID": 53, "context": "Roark and Charniak (1998) propose a method of constructing semantic lexicons based on co-occurrence statistics of nouns in conjunctions, lists and appositives.", "startOffset": 0, "endOffset": 26}, {"referenceID": 14, "context": "After construction, they rank the words by a log-likelihood statistic (Dunning, 1993).", "startOffset": 70, "endOffset": 85}, {"referenceID": 14, "context": "After construction, they rank the words by a log-likelihood statistic (Dunning, 1993). However, due to the general brevity of the reports, such co-occurrences and lists are rather few in our corpus, and it is more useful for us to use context-based similarities like Thelen and Riloff (2002). They describe their Basilisk framework for learning semantic lexicon using extraction patterns as features.", "startOffset": 71, "endOffset": 292}, {"referenceID": 65, "context": "Rather than making an arbitrary choice, Yangarber (2003) proposes a method for detecting termination of unsupervised semantic pattern learning processes.", "startOffset": 40, "endOffset": 57}, {"referenceID": 65, "context": "Rather than making an arbitrary choice, Yangarber (2003) proposes a method for detecting termination of unsupervised semantic pattern learning processes. The method requires that the documents must be labeled as relevant or irrelevant. Since such information is not available for our corpus, it is not useful for us. Curran, Murphy, and Scholz (2007) suggest an improvement over traditional bootstrapping methods by discarding words and contexts that appear to be related to more than one category, in order to minimize semantic drift and enforce mutual exclusion.", "startOffset": 40, "endOffset": 351}, {"referenceID": 65, "context": "Rather than making an arbitrary choice, Yangarber (2003) proposes a method for detecting termination of unsupervised semantic pattern learning processes. The method requires that the documents must be labeled as relevant or irrelevant. Since such information is not available for our corpus, it is not useful for us. Curran, Murphy, and Scholz (2007) suggest an improvement over traditional bootstrapping methods by discarding words and contexts that appear to be related to more than one category, in order to minimize semantic drift and enforce mutual exclusion. On the other hand, we handle such cases by comparing the conditional probabilities for the different categories to which such words can belong. Zhang, Zhou, Huang, and Wu (2008) present bootstrapping with the graph mutual reinforcement-based bootstrapping (GMR) (Hassan, Hassan, & Emam, 2006), a modification of the Basilisk method.", "startOffset": 40, "endOffset": 743}, {"referenceID": 0, "context": "Among non-bootstrapping approaches, Ando (2004) presents a new method of constructing semantic lexicons from an unannotated corpus using a set of semantic classes and a set of seed words and phrases for each semantic class.", "startOffset": 36, "endOffset": 48}, {"referenceID": 0, "context": "Among non-bootstrapping approaches, Ando (2004) presents a new method of constructing semantic lexicons from an unannotated corpus using a set of semantic classes and a set of seed words and phrases for each semantic class. She uses spectral analysis to improve the feature vectors by projecting the useful portions of the vectors into a subspace and removing the \u201charmful\u201d portions of the vectors. The resultant feature vectors are then used by a centroid-based classifier using cosine similarity measure to label the words. Avancini, Lavelli, Sebastiani, and Zanoli (2006) take a classification approach to semantic lexicon construction.", "startOffset": 36, "endOffset": 575}, {"referenceID": 51, "context": "Riloff (1996) describes the AutoSlog-TS system that learns extraction patterns from untagged text.", "startOffset": 0, "endOffset": 14}, {"referenceID": 48, "context": "Phillips and Riloff (2007) present a method of boot-", "startOffset": 0, "endOffset": 27}, {"referenceID": 52, "context": "Patwardhan and Riloff (2006) use the AutoSlog-TS system (Riloff, 1996) to learn domain specific extraction patterns by processing documents retrieved by querying the web with selected domain-specific words.", "startOffset": 56, "endOffset": 70}, {"referenceID": 43, "context": "Patwardhan and Riloff (2006) use the AutoSlog-TS system (Riloff, 1996) to learn domain specific extraction patterns by processing documents retrieved by querying the web with selected domain-specific words.", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "Baker and McCallum (1998) use Pereira et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Baker and McCallum (1998) use Pereira et al.\u2019s distributional clustering technique to perform feature space reduction for supervised classification with n\u00e4\u0131ve Bayes by using the clusters as features. Lin and Pantel (2001) present their approach of generating a collection of sets of semantically similar words, or concepts, using their clustering method, UNICON, with dependency relations as features.", "startOffset": 0, "endOffset": 222}, {"referenceID": 3, "context": "Baker and McCallum (1998) use Pereira et al.\u2019s distributional clustering technique to perform feature space reduction for supervised classification with n\u00e4\u0131ve Bayes by using the clusters as features. Lin and Pantel (2001) present their approach of generating a collection of sets of semantically similar words, or concepts, using their clustering method, UNICON, with dependency relations as features. Pantel and Lin (2002) present another clustering approach, clustering by committee, using contextual features with point wise mutual information as feature values, that they compare as better than Lin and Pantel\u2019s results.", "startOffset": 0, "endOffset": 424}, {"referenceID": 3, "context": "Baker and McCallum (1998) use Pereira et al.\u2019s distributional clustering technique to perform feature space reduction for supervised classification with n\u00e4\u0131ve Bayes by using the clusters as features. Lin and Pantel (2001) present their approach of generating a collection of sets of semantically similar words, or concepts, using their clustering method, UNICON, with dependency relations as features. Pantel and Lin (2002) present another clustering approach, clustering by committee, using contextual features with point wise mutual information as feature values, that they compare as better than Lin and Pantel\u2019s results. Rohwer and Freitag (2004) present a clustering-based automatic thesaurus building process from an unannotated corpus.", "startOffset": 0, "endOffset": 651}, {"referenceID": 12, "context": "Among non-cluster-based methods, Davidov and Rappoport (2006) present an unsupervised method of discovering groups of words that have similar meanings.", "startOffset": 33, "endOffset": 62}, {"referenceID": 55, "context": "Concentrating on the performance issues that plague attempts to build thesaurus from a large corpus, Rychl\u00fd and Kilgarriff (2007) present two methods of improving performance of general context-based thesaurus building algorithms.", "startOffset": 101, "endOffset": 130}, {"referenceID": 27, "context": "In our case, the underlying learner is Support Vector Machines (Joachims, 1998).", "startOffset": 63, "endOffset": 79}, {"referenceID": 18, "context": "Godbole and Sarawagi (2004) suggest a number of improvements to this scheme, namely, including class labels suggested by a preliminary set of classifiers as features, removing negative examples too close to the classification hyperplane, and selectively removing some classes from the one-versus-others classifications scheme.", "startOffset": 0, "endOffset": 28}, {"referenceID": 18, "context": "Godbole and Sarawagi (2004) suggest a number of improvements to this scheme, namely, including class labels suggested by a preliminary set of classifiers as features, removing negative examples too close to the classification hyperplane, and selectively removing some classes from the one-versus-others classifications scheme. Another notable method, followed by Tsoumakas and Vlahavas (2007) and Read et al.", "startOffset": 0, "endOffset": 393}, {"referenceID": 18, "context": "Godbole and Sarawagi (2004) suggest a number of improvements to this scheme, namely, including class labels suggested by a preliminary set of classifiers as features, removing negative examples too close to the classification hyperplane, and selectively removing some classes from the one-versus-others classifications scheme. Another notable method, followed by Tsoumakas and Vlahavas (2007) and Read et al. (2008), is to treat each unique set of labels as a new label, thus converting the problem to a multi-class single-labeled one.", "startOffset": 0, "endOffset": 416}, {"referenceID": 0, "context": "The former, called RAndom k-LabELsets, or RAKEL, builds an ensemble of classifiers by randomly sampling label sets of size k; whereas the latter adopts the method of filtering observed label sets by minimum support. Tang et al. (2009), on the other hand, take a different approach: they train one classifier that predicts the number of labels that a test instance would have, and then choose that many labels for that instance based on output of another classifier that ranks the labels by likelihood for that instance.", "startOffset": 20, "endOffset": 235}, {"referenceID": 38, "context": "McCallum and Nigam (1999) propose a system that starts with a small set of keywords and unlabeled documents, and learns a n\u00e4\u0131ve Bayes classifier in a bootstrapping process from the keyword-induced labels by using hierarchical shrinkage and expectation maximization", "startOffset": 0, "endOffset": 26}, {"referenceID": 61, "context": "Ueda and Saito (2002) present another generative model called Parametric Mixture Models, which treats multi-labeled text as a parametric mixture of words relevant to each label.", "startOffset": 0, "endOffset": 22}, {"referenceID": 60, "context": "A more comprehensive review of different approaches to multi-class multi-label text classification can be found in the work of Tsoumakas and Katakis (2007).", "startOffset": 127, "endOffset": 156}, {"referenceID": 50, "context": "Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloff\u2019s (2002) Basilisk framework augmented with our three algorithmic modifications (namely, the use of a probabilistic similarity measure, the use of a common word pool, and the enforcement of minimum support and maximum generality constraints for words and extraction patterns) and one linguistic modification (the use of N-gram-based extraction patterns).", "startOffset": 118, "endOffset": 134}, {"referenceID": 10, "context": "For instance, we would like to study the performance of the semantic similarities and weighting functions suggested by Curran and Moens (2002) in our context.", "startOffset": 119, "endOffset": 143}], "year": 2010, "abstractText": "The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloff\u2019s Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}