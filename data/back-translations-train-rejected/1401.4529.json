{"id": "1401.4529", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "General factorization framework for context-aware recommendations", "abstract": "General feature based solutions are emerging in the field of recommender systems with the increased need to incorporate multiple sources of information into a single model. The common property of these approaches is that they use a fixed factorization model class that can be extended to handle arbitrary number of context dimensions; the model is then learnt using different optimization techniques. In this paper we propose a general framework in which arbitrary linear feature models can be learnt efficiently. Moreover, both the factorization model and the model members are unrestricted in the framework, thus it is more flexible than state-of-the-art general feature based solutions. The framework allows for both implicit feedback based item ranking and rating prediction from explicit feedback. The paper focuses on the implicit feedback based recommendation problems, due to its expansive use in practical systems compared to explicit feedback. Using the flexibility of the framework we (1) evaluate various factorization models using 5 implicit feedback data sets and injecting contextual information into the model; (2) identify models that can solve the implicit feedback based context-aware recommendation task better than previously proposed model classes. Advantages and drawbacks of various models and learning strategies are also discussed briefly.", "histories": [["v1", "Sat, 18 Jan 2014 11:13:26 GMT  (316kb)", "http://arxiv.org/abs/1401.4529v1", null], ["v2", "Tue, 19 May 2015 11:50:22 GMT  (593kb,D)", "http://arxiv.org/abs/1401.4529v2", "The final publication is available at Springer viathis http URLData Mining and Knowledge Discovery, 2015"]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["bal\\'azs hidasi", "domonkos tikk"], "accepted": false, "id": "1401.4529"}, "pdf": {"name": "1401.4529.pdf", "metadata": {"source": "CRF", "title": "General factorization framework for context-aware recommendations", "authors": ["Bal\u00e1zs Hidasi", "Domonkos Tikk"], "emails": ["balazs.hidasi@gravityrd.com", "domonkos.tikk@gravityrd.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 1.45 29v1 [cs.IR] 1 8Ja n"}, {"heading": "1 Introduction", "text": "The Recommender systems are information filtering tools that help users in the overload of information. To model user preferences, they use either metadata (content-based filtering, CBF) or interactions with the user (collaborative filtering, CF). The algorithms prove to be more accurate than the CBF methods when there is sufficient interaction data (or events) available. The latent factor CF methods are gaining popularity due to their attractive accuracy and scalability. They want to capture the user preferences by explaining the observed user events (ratings). The models are generated by factoring the user rating matrix."}, {"heading": "2 The basic general framework", "text": "In recommendation problems, the main target is the modeling of user preferences on items. In the traditional CF approach, the preference model is learned exclusively from sample events; an event describes a user interaction with an item. In the context-sensitive case, the model is extended to include contexts that determine the preference relationship. Context can be the place or the time of the interaction, the device on which the interaction was performed, or other parameters that can influence user preferences, including weather, social network, reference link, search term. Here, we present a general modeling framework - inspired by the latent factor CF approach - the context data is efficiently integrated into the preference model. Lassen dom (A) denote the domain of the categorical attribute A; the values of the categorical attribute (A) are denoted as entities. The descarted product product product product product product product product product product (A) is the domain of the categorical attribute A (A)."}, {"heading": "2.1 ALS-based training on implicit data", "text": "The learning method presented generalizes the work of [10] and the concept introduced in [11] is based on a preferred PR method. Weighted root means squared error (wRMSE) is used to optimize power vectors with ALS.3A weight is therefore associated with all possible combinations of attribute values: C: ti1,..., iD = < a (1) i1,., a (D) iD > RC (ti1,..., iD) = ci1,..., iD = {c \u2032 i1, if ti1,..., iD = 1, if ti1,..., iD = 1ci1, otherwise (1) A practical choice for weighting is c0 = 1 and c \u2032 i1,..., iD = 100.4The loss function is: 5L = S1,..., SD = 1, iD = 1ci1,..., iD = 11, ci1,..., iD =..., iD = 1ci."}, {"heading": "2.2 Complexity of training", "text": "In Equation (6), I \u2032 s and J \u2032 s can be calculated in O (N + 1 K 2 | O |) time; here, N + 1 is the number of training events in which the value of the A (1) attribute is one (1) 1, and | O | is the complexity of the model (i.e. the number of vector operations used to calculate r \u00b2). This is possible due to the definition of the c weights and r preferences, since most of the members in the sums of O, I \u2032 and J \u2032 are actually zeros. For each column of M (1), this number adds up to O (N + K2 | O), where N + is the number of training examples. Me and J are independent of the column actually calculated. Thus, they are calculated once during the recalculation of M (1), composed of members described in (9). C (j) and O (j) can each be changed in O (SjK2) time."}, {"heading": "3 Model comparison", "text": "The flexibility of the general framework allows for efficient learning of arbitrary linear models. Here, we try to (1) answer questions when there is a single model that works well on many different issues; (2) where scenarios can outperform one model over the other. Our experimental setup is as follows: We used five implicit datasets from different areas, two of which (Grocery and VoD) are aware that others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available. See also Table 1.9 The primary metric is a good proxy to evaluate Top-N recommendations. We estimate that in practice users are exposed to an average of 20 recommendations during a visit (4 page views, 5 items per recommendation) dictating a cutoff of 20. Mean Average Precision (MAP) is the secondary rating metric."}, {"heading": "3.1 Experiments with BPR+SGD learning", "text": "We have performed the same experiments with BPR + SGD as with wRMSE + ALS and show Recall @ 20 in Table 2 for the best models 11.BPR + SGD. Results suggest that BPR + SGD cannot learn the members of the UCI type. A full explanation of the failure of BPR + SGD in tripartite products requires further studies. Overall, wRMSE + ALS was better in 3 out of 5 cases when comparing the best models. We continue to argue for the use of wRMSE + ALS because (1) wRMSE + ALS is better than BPR + SGD for larger datasets (Grocery, LastFM, VoGD, the best model (SGD) (SALD) (SALS) in general not (SALS) because (1) wRMSE + ALS is better than BPR + SGD for larger datasets (Vocery, GD)."}, {"heading": "4 Extension of the framework", "text": "On the basis of the NSVD1 approach [19] let us use M (j) = X (j) W (j), where the columns of M (j), RK (j) and Sj are the primary characteristics, those of X (j), RK (Zj) are the secondary characteristics and W (j), RZj (j) is the sparse mixing matrix that combines the secondary and primary characteristics, i.e. a primary characteristic vector is the linear combination of some secondary characteristic vectors. This approach allows us to expand our framework in the desired way and has several advantages. We overcome the original problem by adding a secondary attribute to the entities of the primary characteristic, i.e. a primary characteristic vector is the linear combination of some secondary characteristic vectors."}, {"heading": "4.1 Metadata boosted predictions", "text": "For example, item metadata helps to overcome the item cold start problem in CF. Here we use a simple vector space approach to integrate metadata into the model. Each item was presented as a sparse, normalized metadata vector. Thus, we get a term item matrix with terms as attributes of the items, i.e. we have assigned a primary attribute to the items in our model and assigned metadata terms as secondary attributes to the entities of this primary attribute. Two models were examined: (i) use metadata instead of items (UM + USM + UQM); (ii) use of metadata and traditional item attributes in the same model (UI + USI + UQI + UQI + USM + UQM); where M are the metadata-based primary items (in table 3)."}, {"heading": "4.2 Incorporating session information", "text": "Different sessions of the same user are usually treated uniformly by recipient systems, provided that user preferences do not change from session to session. Although this is usually the case, there are some unusual scenarios: A user who prefers horror movies can watch a romantic comedy with his girlfriend. In general, some external factors may cause user preference to differ from the normal, and the traditional dualistic user-object model without knowledge of external factors cannot capture this. We treat irregular sessions by adding a session-dependent part to the model. The domain of the primary attribute in this setting is the event set, so each event has its own primary attribute vector. The domain of the secondary attribute is the set of elements. A secondary attribute value is assigned to a primary attribute value if (a) the element of the event has not been observed with the value of the secondary attribute of the session; thus, the secondary attribute has its own event."}, {"heading": "5 Related work", "text": "It is time for us to set out in search of new ways to travel the world."}, {"heading": "6 Discussion & future work", "text": "In this paper, we presented a general factorization framework, but compared to previous work, our approach is more flexible, as there are no limitations of the model as long as it is linear; the framework is able to evaluate predictions and implicit feedback-based item ranking through pointed preference estimates; the framework allows experimentation with different models; the in-depth model comparison (Section 3) suggests that there are model classes that usually exceed others: we found that the interaction of the basic CF model and additional context-dependent prediction models can be best; we note that this model is not a member of the model classes of previously suggested general frame words.We also proposed an extension of our framework that removes the restriction to the number of entities in a context dimension corresponding to an event, allowing us to integrate each context into a single factorization model."}], "references": [{"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "M. Gemmis", "G. Semeraro"], "venue": "Recommender Systems Handbook. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Recommending new movies: Even a few ratings are more valuable than metadata", "author": ["I. Pil\u00e1szy", "D. Tikk"], "venue": "Recsys\u201909: ACM Conf. on Recommender Systems.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Advances in collaborative filtering", "author": ["Y. Koren", "R. Bell"], "venue": "In Ricci, F., et al., eds.: Recommender Systems Handbook. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["R. Bell", "Y. Koren"], "venue": "ICDM\u201907: IEEE Int. Conf. on Data Mining.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Major components of the Gravity recommendation system", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "SIGKDD Explor. Newsl. 9", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Advances in Neural Information Processing Systems 20. MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Recsys\u201908: ACM Conf. on Recommender Systems.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "UAI \u201909: 25 Conf. on Uncertainty in Artificial Intelligence.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "Proc. of the ECML-PKDD, Part II. Number 7524 in LNCS. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM-08: IEEE Int. Conf. on Data Mining.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling and learning context-aware recommendation scenarios using tensor decomposition", "author": ["H. Wermser", "A. Rettinger", "V. Tresp"], "venue": "ASONAM\u201911: IEEE Int. Conf. on Advances in Social Networks Analysis and Mining.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Applications of the conjugate gradient method for implicit feedback collaborative filtering", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "D. Tikk"], "venue": "RecSys\u201911: ACM Conf. on Recommender Systems.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-aware recommendations from implicit data via scalable tensor factorization", "author": ["B. Hidasi", "D. Tikk"], "venue": "ArXiv e-prints", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of cold-start recommendations in iptv systems", "author": ["P. Cremonesi", "R. Turrin"], "venue": "Proc. of the 2009 ACM Conference on Recommender Systems.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Music Recommendation and Discovery in the Long Tail", "author": ["O. Celma"], "venue": "Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": "WSDM\u201910: ACM Int. Conf. on Web Search and Data Mining.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 3(3)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["A. Paterek"], "venue": "Proc. of KDD Cup and Workshop. Volume 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Incorporating contextual information in recommender systems using a multidimensional approach", "author": ["G. Adomavicius", "R. Sankaranarayanan", "S. Sen", "A. Tuzhilin"], "venue": "ACM Trans. Inf. Syst. 23(1)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Social network and click-through prediction with factorization machines", "author": ["S. Rendle"], "venue": "Proc. of the KDD Cup and Workshop.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature-based matrix factorization", "author": ["T. Chen", "Z. Zheng", "Q. Lu", "W. Zhang", "Y. Yu"], "venue": "CoRR abs/1109.2271", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "SVDFeature: A toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yu"], "venue": "Journal of Machine Learning Research 13", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For modeling user preferences, classical approaches either use item metadata (content based filtering, CBF; [1]), or user\u2013 item interactions (collaborative filtering, CF; [2]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "For modeling user preferences, classical approaches either use item metadata (content based filtering, CBF; [1]), or user\u2013 item interactions (collaborative filtering, CF; [2]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "CF algorithms proved to be more accurate than CBF methods, if sufficient interaction data (or events) is available [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Latent factor based CF methods gained popularity due to their attractive accuracy and scalability [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "The dualistic user\u2013item based modeling concept can be extended by considering additional information that may influence the user preferences at recommendation; such data are together termed contextual information, or briefly context [8].", "startOffset": 233, "endOffset": 236}, {"referenceID": 8, "context": "The contribution of this paper is threefold: (1) we show that arbitrary linear feature models can be learnt efficiently using an ALS based pointwise preference estimation; (2) we evaluate various factorization models in this flexible framework to identify those that solve the implicit feedback based context aware recommendation task accurately; (3) we compare the pointwise preference estimation, with the Bayesian Personalized Ranking (BPR; [9]); a pairwise learning strategy) and show that while BPR clearly outperforms pointwise preference estimation in the classical 2D MF setting, it seems to fall behind with more complex models on bigger data sets.", "startOffset": 444, "endOffset": 447}, {"referenceID": 9, "context": "iTALS, the context-aware tensor factorization model [10] predicts the preference", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "The presented learning method generalizes the works of [10] and the concept introduced in [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "The presented learning method generalizes the works of [10] and the concept introduced in [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "3 A pairwise ranking method, namely BPR based [9] optimization with SGD was also implemented, but the ALS method is preferred for training.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "The description of the BPR+SGD learning is omitted here, but can be reconstructed from [9,12].", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "The description of the BPR+SGD learning is omitted here, but can be reconstructed from [9,12].", "startOffset": 87, "endOffset": 93}, {"referenceID": 12, "context": "conjugate gradient, [13,14]) can be used to get the new value of the feature vector.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "conjugate gradient, [13,14]) can be used to get the new value of the feature vector.", "startOffset": 20, "endOffset": 27}, {"referenceID": 12, "context": "CG was used as an approximation for the naive LS solver in ALS based MF in [13] and for three way tensor factorization by [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "CG was used as an approximation for the naive LS solver in ALS based MF in [13] and for three way tensor factorization by [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Two of them (Grocery and VoD) are proprietary, the others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available; see also Table 1.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "Two of them (Grocery and VoD) are proprietary, the others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available; see also Table 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "Two context dimension are used: (1) seasonality and (2) sequentiality [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "Both models were already suggested for context-aware modeling (see [10,17]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "Both models were already suggested for context-aware modeling (see [10,17]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 17, "context": "The pairwise model (with a single context) can be further extended with additional contexts: either to a full pairwise model\u2014identical with the model of Factorization Machines (FM; [18])\u2014or to the same model without the SQ member.", "startOffset": 181, "endOffset": 185}, {"referenceID": 8, "context": "BPR+SGD clearly outperforms the wRMSE+ALS learning on the basic CF model (UI); stated also in [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "Based on the NSVD1 approach [19] let us use M (j) = XW (j) where the columns of M (j) \u2208 Rj are the primary features, those of X \u2208 Rj are the secondary features and W (j) \u2208 Rji is the sparse mixing matrix that connects the secondary and primary features, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Then one can apply two-phase learning of the secondary features [3] for efficiency.", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "The most extensive data space model is the multidimensional data model (MD) [20] and most of the other models are restricted MD variants.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "al proposed factorization machines (FM; [18]) as a general framework.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "It is suggested in [18] that the released libFM software is capable of handling implicit feedback through BPR+SGD.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "The modeling of insignificant interactions can slow down the learning process drastically[21] and can result in poorer predictive performance.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "al proposed another framework, coined SVDFeature, that uses a subset of the FM model [22,23].", "startOffset": 85, "endOffset": 92}, {"referenceID": 22, "context": "al proposed another framework, coined SVDFeature, that uses a subset of the FM model [22,23].", "startOffset": 85, "endOffset": 92}], "year": 2017, "abstractText": "General feature based solutions are emerging in the field of recommender systems with the increased need to incorporate multiple sources of information into a single model. The common property of these approaches is that they use a fixed factorization model class that can be extended to handle arbitrary number of context dimensions; the model is then learnt using different optimization techniques. In this paper we propose a general framework in which arbitrary linear feature models can be learnt efficiently. Moreover, both the factorization model and the model members are unrestricted in the framework, thus it is more flexible than state-of-the-art general feature based solutions. The framework allows for both implicit feedback based item ranking and rating prediction from explicit feedback. The paper focuses on the implicit feedback based recommendation problems, due to its expansive use in practical systems compared to explicit feedback. Using the flexibility of the framework we (1) evaluate various factorization models using 5 implicit feedback data sets and injecting contextual information into the model; (2) identify models that can solve the implicit feedback based context-aware recommendation task better than previously proposed model classes. Advantages and drawbacks of various models and learning strategies are also discussed briefly.", "creator": "LaTeX with hyperref package"}}}