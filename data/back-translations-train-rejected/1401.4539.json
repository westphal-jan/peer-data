{"id": "1401.4539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Solving the Minimum Common String Partition Problem with the Help of Ants", "abstract": "In this paper, we consider the problem of finding a minimum common partition of two strings (MCSP). The problem has its application in genome comparison. As it is an NP-hard, discrete combinatorial optimization problem, we employ a metaheuristic technique, namely, MAX-MIN ant system to solve this. The experimental results are found to be promising.", "histories": [["v1", "Sat, 18 Jan 2014 13:15:30 GMT  (93kb,D)", "http://arxiv.org/abs/1401.4539v1", null], ["v2", "Wed, 21 May 2014 06:35:41 GMT  (128kb,D)", "http://arxiv.org/abs/1401.4539v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["s m ferdous", "m sohel rahman"], "accepted": false, "id": "1401.4539"}, "pdf": {"name": "1401.4539.pdf", "metadata": {"source": "CRF", "title": "A MAX -MIN Ant Colony System for Minimum Common String Partition Problem", "authors": ["S. M. Ferdous", "M. Sohel Rahman"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we look at the problem of finding a minimum shared partition of two strings (MCSP), which has its application in genome comparison. As it is a NP-hard, discrete combinatorial optimization problem, we use a metaheuristic technique, the MAX-MIN ant system, to solve this problem. Experimental results are promising. Keywords: ant colony optimization, stringology, genome sequencing, combinatorial optimization, swarm intelligence, string partitioning"}, {"heading": "1. Introduction", "text": "String comparison is one of the major problems in computer science with various applications in different fields, including genome sequencing > Q \u00b7 Q = 1. Q \u00b7 Compressions. In this work we deal with the problem of finding a minimum common partition (MCSP) of two strings. MCSP is closely related to the genome arrangement, which is an important issue in computer biology. In the face of two DNA sequences, the MCSP asks for the least common group of common building blocks of the sequences. In the MCSP problem, we are given two related strings, which are an important issue in computer biology. Two strings are connected when each character appears the same number of times in each of them. Clearly, two strings have a common partition when and only when they are related to each other. The length of the two strings are also the same (say, n). Our goal is to partition each string into c-segments, which are referred to as blocks, so by the blocks of the X and the blocks in the partition."}, {"heading": "1.1. Our Contribution", "text": "In this paper, we look at metaheuristic approaches to solving the problem. To our knowledge, there is no attempt to solve the problem with metaheuristic approaches. In the literature, only theoretical work exists. In particular, we are interested in nature-inspired algorithms. As the problem is a discrete combinatorial optimization problem, the natural choice is ant colony optimization (ACO). Before using ACO, it is necessary to map the problem in a diagram. We developed this mapping. In this paper, we implement a variant of the ACO algorithm, the MAX MIN Ant System (MMAS), to solve the MCSP problem. We conduct experiments on both random and real data in order to compare our algorithm with the state of the art in literature and obtain promising results."}, {"heading": "2. Literature Review", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "3. Preliminaries", "text": "In this section, we will introduce some definitions and notations used throughout the work. Two strings (X, Y), each with the length n, are formally related if each letter contains the same number of times in each block. A block B = (id, i, j), 0 \u2264 i \u2264 j < n, a string S is a data structure with three fields: id is a string of S and the start and end positions of the block in S are represented by i and j respectively. Of course, the length of a block is [id, i, j] (j \u2212 i + 1), a string of three fields: id is a substring of S and is generated by the block [id, i, j]. Throughout the paper, we will use 0 and 1 as the identifiers of X (i.e., id) and Y (i.e., id) to indicate an empty blocking."}, {"heading": "3.1. Basics of ACO", "text": "In general, the ACO approach attempts to solve a combinatorial optimization problem (CO) by iterating the following two steps: First, solutions are constructed using a pheromone model, i.e. a parameterized probability distribution across the solution space. Then, the solutions constructed in previous iterations are used to modify pheromone values in a way that limits the search for high-quality solutions."}, {"heading": "3.2. Ant Based Solutions Construction", "text": "As mentioned above, the basic component of an ACO algorithm is constructive heuristics for the probable construction of solutions. A constructive heuristics assembles solutions as sequences of solution components from a finite amount of solution components C = {c1, c2,... cn}. A solution construction begins with an empty partial solution sp = \u2205. Then, at each construction step, the current partial solution sp is extended by a feasible solution component from solution space C. The process of constructing solutions can be considered as a walk (or path) on the so-called construction graph Gc = (C, E), the vertices of which are solution components C and the quantity E the connections (i.e. edges)."}, {"heading": "3.3. Heuristic Information", "text": "In most ACO algorithms, the transition probabilities, i.e. the probabilities for the choice of the next solution component, are defined as follows: p (ci | sp) = \u03c4i \u03b1 \u00b7 \u03b7 (ci) \u03b2 \u2211 cj \u0445 N (sp) \u03c4j \u03b1 \u00b7 \u03b7 (cj) \u03b2, \u0418ci \u0435N (sp) (3) This is a weight function containing heuristic information, and \u03b1, \u03b2 are positive parameters whose values determine the relationship between pheromone information and heuristic information."}, {"heading": "3.4. Pheromone Update", "text": "There are different types of pheromone updating. We use a pheromone updating process that is used by almost every ACO algorithm. This pheromone updating consists of two parts. First, pheromone evaporation is performed that evaporates all pheromone levels evenly. In practical terms, pheromone evaporation is required to prevent the algorithm from converging too quickly towards a suboptimal region. It helps to forget the local optimal solutions and therefore favors exploration of new areas in the search space. Then, one or more solutions from current or previous iterations are used to increase the values of pheromone parameters to solution components that are part of these solutions. As a prominent example, we describe the following pheromone updating rule used in the Ant System (AS) [8], which was the first ACO algorithm proposed."}, {"heading": "3.5. MAX-MIN Ant System (MMAS)", "text": "MMAS algorithms are characterized as follows: First, pheromone values are limited to an interval [\u03c4MIN, \u03c4MAX] of 0 < \u03c4MIN < \u03c4MAX. Pheromone paths are initialized to \u03c4max to facilitate diversification during early iterations, thus preventing premature convergence. Explicit pheromone limits ensure that the chance of finding a global optimum is never zero. Second, if the algorithm detects that the search is too limited to a specific area in the search space, a restart is performed by re-initializing all pheromone values. Third, the pheromone update is always performed with either the best iteration solution, the best restart solution (i.e. the best solution found since the last restart) or the best solution so far."}, {"heading": "4. Our Approach: MAX-MIN Ant System on the Common Substring Graph", "text": "\"We must adjust to the fact that we define the edge Y (V, E, id (X) of a string X in relation to Y as follows: V is the vertex set of the graph and E is the edge set. Vertices are the positions of the string X, i.e., for each v, V, X, 1, 2, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "4.2. Heuristics", "text": "Heuristics (\u03b7) contain the problem-specific information. We propose two different heuristics for MCSP. Firstly, we propose a static heuristics that does not change during the course of the algorithm. The other heuristics that we propose is dynamic in the sense that it changes between the gradients."}, {"heading": "4.2.1. The Static Heuristic for MCSP", "text": "To grasp this phenomenon, we assign a numerical value to each edge of the common sub-string graph proportional to the length of the sub-string corresponding to the edge block. Formally, the static heuristics (\u03b7s) of an edge block [id, i, j] are defined as \u03b7s ([id, i, j]) and length ([id, i, j]) (7)."}, {"heading": "4.2.2. The Dynamic Heuristic for MCSP", "text": "We observe that static heuristics can sometimes lead us to very bad solutions. E.g. if (X, Y) = {\"bceabcd,\" \"abcdbec\"}, then according to static heuristics the edge block [0, 0, 1] is assigned a much higher value than [0, 0, 0, 0]. But if we take [0, 0, 1, 1], we must assign it to the block [1, 1, 2], and we continue to miss the opportunity to take [0, 3, 6] later. The resulting partition will be {\"bc,\" \"e,\" \"a,\" \"b,\" \"\" c, \"\" \"d\"}, but if we take [0, 0, 0] in the first step one of the resulting blocks, then one of the resulting partitions would be {\"b,\" \"c,\" \"\" e, \"\" \"abcd.\" To overcome this lack of static heuristics, we define a dynamic heuristics as follows."}, {"heading": "4.3. Initialization and Configuration", "text": "Given two strings (X, Y), we first construct the common sub-string graph Gcs = (V, E, id (X). We use the following notations. Local best solution (LLB) is the best solution found in each iteration. Global best solution (LGB) is the best solution found so far among all iterations. The pheromone of the edge block is limited between \u03c4max and \u03c4min. Like [20], we use the following values for \u03c4max and \u03c4min: \u03c4max = 1\u03b5 \u00b7 cost (LGB) and \u03c4min = \u03c4min (1 \u2212 n \u221a pbest) (avg \u2212 1) n \u221a pbest. Here, avg is the average number of choices an ant has in the construction phase; n is the length of the string; pbest is the probability of finding the best solution when the system converts, and \u03b5 is the evaporation rate. Initially, the pheromone values of all the edge blocks (sub-string) will represent the beginning of the pheromone that is at the beginning of the Iteration."}, {"heading": "4.4. Construction of a Solution", "text": "A solution for an ant starting at a vertex vs is constructed by the following steps: Step 1: Leave vi = vs Select an available edge block based on vi using the discrete probability distribution defined below. An edge block is available if its matlist is not empty and its inclusion in the partition list and the mappedList of Equation 11. The probability for selecting the edge block [0, vi, vj] is: p ([0, vi, vj]) = bar ([0, vi, vj]) \u03b2."}, {"heading": "4.5. Intelligent Positioning", "text": "For each edge block of Gcs in X, we have a MatchList that contains the matching block of string Y. In the construction (step 1), if an edge block is selected based on the probability distribution, we take a block from the MatchList of the selected edge block. We can select the matching block at random, but we observe that random selection can lead to a very bad partition. For example, if we select (X, Y) = {\"abababc,\" \"abcab,\" \"c\"} then the MatchList ([0, 0, 1] = {1, 0, 1], [1, 3, 4], we eventually get the partition as {\"ab,\" ab, \"\" c \"}, but there is a smaller partition and that is {\" ab, \"\" abc \"}. To solve this problem, we have set a rule for selecting the matching block. So we will select a block from the MatchList that has the lowest possible range."}, {"heading": "4.6. Pheromone Update", "text": "When each of the ants in the colony constructs a solution (i.e. a common partition), an iteration is completed. We define the locally best solution as the best partition, which is the minimum length partition in an iteration. The world's best solution for n iterations is defined as the minimum length of the common partition over the entire iteration n. We define the fitness F (L) of a solution L as the flip side of the length L. The pheromone of each interval of each target string is calculated according to Equation 4 after each iteration. The pheromone values are limited within the range \u03c4MIN and \u03c4MAX. We update the pheromone values according to LLB or LGB. First, we update pheromone for the first 50 iterations only by LLB to facilitate the search. Afterwards, we develop a schedule in which the frequency of updating with LLB decreases and LGB increases to facilitate exploitation."}, {"heading": "4.7. The Pseudocode", "text": "The pseudo code of our approach to solving MCSP is given in Algorithm 1.Algorithm 1 MMAS for MCSP to calculate heuristic information () for run = 1 \u2192 MAXRUN doInitialize pheromone Initialize global best repeatInitialize local best for i = 1 \u2192 nAnts doConstruction for anti update local best for update global best update pheromone. Either by local or global records until the time maxAllowedT ime or no update for maxAllowedIteration end for reaches."}, {"heading": "5. Experiments", "text": "We carried out our experiments on a computer with Intel Core 2 Quad CPU 2.33 GHz, the available memory was 4.00 GB, the operating system Windows 7. The programming environment was Java. jre version is \"1.7.0 15.\" As an integrated development environment we used JCreator, the maximum allowed time for each instance was 120 minutes."}, {"heading": "5.1. Datasets", "text": "We conducted our experiments with two types of data: randomly generated DNA sequences and real gene sequences."}, {"heading": "5.1.1. Random DNA sequences:", "text": "The proportion of bases A, T, G and C is given as 0.25 each. We mix them for each DNA sequence to create a new DNA sequence. Shuffling is done using the online toolbox [21]. The original random DNA sequence and its mixed pair form a single input (X, Y) in our experiment. This data set is divided into 3 classes: the first 10 have lengths of less than or equal to 200 bps (base pairs), the next 10 lengths within [201, 400] and the remaining 10 lengths within [401, 600] bps."}, {"heading": "5.1.2. Real Gene Sequences:", "text": "We collected the real gene sequence data from the NCBI-GenBank1 and decided on the simulation for bacterial sequencing (part 14). We took the first 15 gene sequences whose lengths are within [200, 600]."}, {"heading": "5.2. Parameters", "text": "There are several parameters that need to be set carefully to get good results, and the parameters for which we obtained the results are described in Table 1."}, {"heading": "5.3. Results and Analysis", "text": "We compared our approach to the greedy algorithm of [4] because none of the other algorithms in the literature is suitable for general MCSP: each of the other approximation algorithms limits parameters.1http: / / www.ncbi.nlm.nih.gov"}, {"heading": "5.3.1. Random DNA sequence:", "text": "Table 2, Table 3 and Table 4 show the comparison between our approach and the greedy approach [4] for the random DNA sequences. For a particular DNA sequence, the experiment was performed four times and the average result is reported; the first column under each group indicates the partition size calculated by the greedy approach, the second column is the average partition size found by MMAS, the third column represents the difference between the two approaches, the fourth column indicates the standard deviation of 4 passes, and the fifth column is the average time in seconds in which the specified partition size is reached. A positive (negative) difference indicates that the greedy result by this amount is better (worse) than the MAS result. The table shows that in 30 cases, our approach achieves a better partition size for 28 cases. Table 7 shows the result of the student test of the difference values of each group. 95% of the constant of each group suggests that the mean interval and the actual interval is negative."}, {"heading": "5.3.2. Effects of Dynamic Heuristics:", "text": "In Section 4.2.2 we discussed the dynamic heuristics we use in our algorithm. We conducted experiments to test and verify the effect of this dynamic heuristics. We conducted experiments with two versions of our algorithm - with and without the use of dynamic heuristics. The effect is shown in Table 5, where for each group the average partition size is given with dynamic heuristics and without dynamic heuristics. The positive difference is the improvement with dynamic heuristics. In 30 cases we found positive differences in 27 cases. This clearly shows the significant improvement with dynamic heuristics. It can also be observed that the positive differences increase with the increase in length. Figures 1, 2 and 3 show the results on a case-by-case basis. The blue bars represent the partition size with dynamic heuristics and the red bars represent the partition size without dynamic heuristics."}, {"heading": "5.3.3. Real Gene Sequence:", "text": "Table 6 shows the minimum common partition size of our approach and the greedy approach to the real gene sequences. Of the 15 cases, we get better results in 11 instances. The result of the t-test for the real gene sequence is in Table 7. The 95% confidence interval indicates that the actual mean of the difference is negative and is on the interval. Here, the null hypothesis is that the actual mean of the difference is zero. The null hypothesis is rejected (Table 7), demonstrating the significant improvement over the greedy approach."}, {"heading": "6. Conclusion", "text": "In this paper, we have described a metaheuristic approach to solving the problem. In this approach, we have used static and dynamic heuristic information with intelligent positioning. Simulation is performed on random DNA sequences and real gene sequences. Results are significantly better than previous results. The result of the t-test also shows significant improvements. As a future work, other metaheuristic techniques can be applied to present better solutions to the problem."}], "references": [{"title": "Beam-aco for the longest common subsequence problem", "author": ["C. Blum"], "venue": "in: IEEE Congress on Evolutionary Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "An ant colony optimization algorithm for dna sequencing by hybridization", "author": ["C. Blum", "M.Y. Vall\u00e8s", "M.J. Blesa"], "venue": "Comput. Oper. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Assignment of orthologous genes via genome rearrangement", "author": ["X. Chen", "J. Zheng", "Z. Fu", "P. Nan", "Y. Zhong", "S. Lonardi", "T. Jiang"], "venue": "IEEE/ACM Trans. Comput. Biol. Bioinformatics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "The greedy algorithm for the minimum common string partition problem", "author": ["M. Chrobak", "P. Kolman", "J. Sgall"], "venue": "ACM Trans. Algorithms", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Minimum common string partition parameterized", "author": ["P. Damaschke"], "venue": "Algorithms in Bioinformatics. Springer Berlin Heidelberg. volume 5251 of Lecture Notes in Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Optimization, Learning and Natural Algorithms", "author": ["M. Dorigo"], "venue": "Ph.D. thesis. Politecnico di Milano, Italy", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Positive feedback as a search strategy", "author": ["M. Dorigo", "A. Colorni", "V. Maniezzo"], "venue": "Technical Report 91-016. Dipartimento di Elettronica, Politecnico di Milano. Milan, Italy. URL: http://citeseerx.ist.psu. edu/viewdoc/summary?doi=10.1.1.52.6342", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Ant algorithms for discrete optimization", "author": ["M. Dorigo", "G. Di Caro", "L.M. Gambardella"], "venue": "Artif. Life", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Ant colony system: A cooperative learning approach to the traveling salesman problem", "author": ["M. Dorigo", "L.M. Gambardella"], "venue": "Trans. Evol. Comp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "The ant system: Optimization by a colony of cooperating agents", "author": ["M. Dorigo", "V. Maniezzo", "A. Colorni"], "venue": "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Ant colony optimization: Overview and recent advances", "author": ["M. Dorigo", "T. Sttzle"], "venue": "Handbook of Metaheuristics. Springer US. volume 146 of International Series in Operations Research & Management Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Ant Colony Optimization", "author": ["M. Dorigo", "T. St\u00fctzle"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Ant colony optimization approach to solve the minimum string cover problem", "author": ["S. Ferdous", "A. Das", "R.M.S", "R.M.M"], "venue": "in: International Conference on Informatics, Electronics & Vision (ICIEV),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Ant-q: A reinforcement learning approach to the traveling salesman", "author": ["L. Gambardella", "M. Dorigo"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Minimum common string partitioning problem: Hardness and approximations", "author": ["A. Goldstein", "P. Kolman", "J. Zheng"], "venue": "The Electronic Journal of Combinatorics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Minimum common string partition revisited", "author": ["H. Jiang", "B. Zhu", "D. Zhu", "H. Zhu"], "venue": "in: Proceedings of the 4th International Conference on Frontiers in Algorithmics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Finding the longest common subsequence for multiple biological sequences by ant colony optimization", "author": ["S.J. Shyu", "C.Y. Tsai"], "venue": "Comput. Oper. Res", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Improving the Ant System: A Detailed Report on the MAX-MIN Ant System", "author": ["T. St\u00fctzle", "H. Hoos"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Max-min ant system and local search for the traveling salesman problem, in: IEEE INTERNATIONAL CON- FERENCE ON EVOLUTIONARY COMPUTATION (ICEC\u201997)", "author": ["T. St\u00fctzle", "H. Hoos"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Max-min ant system", "author": ["T. St\u00fctzle", "H.H. Hoos"], "venue": "Future Gener. Comput. Syst", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Fabox: An online fasta sequence toolbox. URL: http://www.birc.au.dk/software/fabox", "author": ["P. Villesen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "The chromosome inversion problem", "author": ["G. Watterson", "W. Ewens", "T. Hall", "A. Morgan"], "venue": "Journal of Theoretical Biology", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1982}], "referenceMentions": [{"referenceID": 4, "context": "Given two DNA strings, MCSP answers the possibilities of re-arrangement of one DNA string to another [5].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "In[3], the authors present a new approach to ortholog assignment that takes into account both sequence similarity and evolutionary events at a genomic level.", "startOffset": 2, "endOffset": 5}, {"referenceID": 21, "context": "MCSP is essentially the breakpoint distance problem [22] between two permutations which is to count the number of ordered pairs of symbols that", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "are adjacent in the first string but not in the other; this problem is obviously solvable in polynomial time [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "The 2-MCSP is proved to be NPhard and moreover APX-hard in [15].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "The authors in [15] also presented several approximation algorithms.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "[3] studied the problem, Signed Reversal Distance with Duplicates (SRDD), which is a generalization of MCSP.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In [5], the author analyzed the fixed-parameter tractability of MCSP considering different parametrs.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [16], the authors investigated k-MCSP along with two other variants: MCSP , where the alphabet size is at most c; and xbalanced MCSP, which requires that the length of the blocks must be witnin the range (n/d\u2212 x, n/d+ x), where d is the number of blocks in the optimal common partition and x is a constant integer.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "[4] analyzed a natural greedy heuristic for MCSP: iteratively, at each step, it extracts a longest common substring from the input strings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.", "startOffset": 30, "endOffset": 40}, {"referenceID": 8, "context": "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.", "startOffset": 30, "endOffset": 40}, {"referenceID": 11, "context": "Ant colony optimization (ACO) [8, 9, 12] was introduced by M.", "startOffset": 30, "endOffset": 40}, {"referenceID": 6, "context": "The original algorithm is known as the Ant System(AS) [7, 6, 10].", "startOffset": 54, "endOffset": 64}, {"referenceID": 5, "context": "The original algorithm is known as the Ant System(AS) [7, 6, 10].", "startOffset": 54, "endOffset": 64}, {"referenceID": 9, "context": "The original algorithm is known as the Ant System(AS) [7, 6, 10].", "startOffset": 54, "endOffset": 64}, {"referenceID": 5, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 35, "endOffset": 42}, {"referenceID": 9, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 35, "endOffset": 42}, {"referenceID": 13, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 80, "endOffset": 83}, {"referenceID": 17, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 96, "endOffset": 108}, {"referenceID": 18, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 96, "endOffset": 108}, {"referenceID": 19, "context": "The other variants are, Elitist AS [6, 10], ANT-Q [14], Ant Colony System (ACS) [9], MAX-MIN AS [18, 19, 20] etc.", "startOffset": 96, "endOffset": 108}, {"referenceID": 7, "context": "In [8] the authors distinguished among two", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In 2010 a non-exhaustive list of applications of ACO algorithms grouped by problem types is presented in [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "In [2], the authors addressed the reconstruction of DNA sequences from DNA fragments by ACO.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "Several ACO algorithms have been proposed for the longest common subsequence (LCS) problem in [17, 1].", "startOffset": 94, "endOffset": 101}, {"referenceID": 0, "context": "Several ACO algorithms have been proposed for the longest common subsequence (LCS) problem in [17, 1].", "startOffset": 94, "endOffset": 101}, {"referenceID": 12, "context": "Recently minimum string cover problem is solved by ACO in [13].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "For example, if we have two strings (X, Y ) = {\u201cabcdab\u201d,\u201cbcdaba\u201d}, then [0, 0, 1] and [0, 4, 5] both represent the substring \u201cab\u201d of X.", "startOffset": 72, "endOffset": 81}, {"referenceID": 3, "context": "For example, if we have two strings (X, Y ) = {\u201cabcdab\u201d,\u201cbcdaba\u201d}, then [0, 0, 1] and [0, 4, 5] both represent the substring \u201cab\u201d of X.", "startOffset": 86, "endOffset": 95}, {"referenceID": 4, "context": "For example, if we have two strings (X, Y ) = {\u201cabcdab\u201d,\u201cbcdaba\u201d}, then [0, 0, 1] and [0, 4, 5] both represent the substring \u201cab\u201d of X.", "startOffset": 86, "endOffset": 95}, {"referenceID": 0, "context": "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = \u201cab\u201d.", "startOffset": 26, "endOffset": 35}, {"referenceID": 3, "context": "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = \u201cab\u201d.", "startOffset": 49, "endOffset": 58}, {"referenceID": 4, "context": "In other words, substring([0, 0, 1]) = substring([0, 4, 5]) = \u201cab\u201d.", "startOffset": 49, "endOffset": 58}, {"referenceID": 4, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 58, "endOffset": 67}, {"referenceID": 6, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 58, "endOffset": 67}, {"referenceID": 10, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 73, "endOffset": 84}, {"referenceID": 11, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 73, "endOffset": 84}, {"referenceID": 7, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 94, "endOffset": 104}, {"referenceID": 9, "context": "As an example, suppose we have three blocks, namely, B1 = [0, 5, 7],B2 = [0, 11, 12] and B3 = [0, 8, 10].", "startOffset": 94, "endOffset": 104}, {"referenceID": 4, "context": "Then B1 \u222a B2 = B\u2032 lst = {[0, 5, 7], [0, 11, 12]}.", "startOffset": 25, "endOffset": 34}, {"referenceID": 6, "context": "Then B1 \u222a B2 = B\u2032 lst = {[0, 5, 7], [0, 11, 12]}.", "startOffset": 25, "endOffset": 34}, {"referenceID": 10, "context": "Then B1 \u222a B2 = B\u2032 lst = {[0, 5, 7], [0, 11, 12]}.", "startOffset": 36, "endOffset": 47}, {"referenceID": 11, "context": "Then B1 \u222a B2 = B\u2032 lst = {[0, 5, 7], [0, 11, 12]}.", "startOffset": 36, "endOffset": 47}, {"referenceID": 4, "context": "On the other hand, B\u2032 lst\u222aB3 = [0, 5, 12], which is basically identical to B1\u222aB2\u222aB3.", "startOffset": 31, "endOffset": 41}, {"referenceID": 11, "context": "On the other hand, B\u2032 lst\u222aB3 = [0, 5, 12], which is basically identical to B1\u222aB2\u222aB3.", "startOffset": 31, "endOffset": 41}, {"referenceID": 0, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 36, "endOffset": 45}, {"referenceID": 1, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 47, "endOffset": 56}, {"referenceID": 3, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 58, "endOffset": 67}, {"referenceID": 4, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 58, "endOffset": 67}, {"referenceID": 0, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 97, "endOffset": 106}, {"referenceID": 1, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 145, "endOffset": 154}, {"referenceID": 4, "context": "For example, if listb = {[0, 0, 0], [0, 0, 1], [0, 0, 2], [0, 4, 5]} then span([0, 0, 0]) = span([0, 0, 1]) = span([0, 0, 2]) = 3 where as, span([0, 4, 5]) = 2.", "startOffset": 145, "endOffset": 154}, {"referenceID": 7, "context": "As a prominent example, we describe the following pheromone update rule that was used in Ant System (AS) [8], which was the first ACO algorithm proposed.", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "This also holds for the two currently bestperforming ACO variants in practice, namely, the Ant Colony System (ACS) [9] and the MAX-MIN Ant System (MMAS) [20].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "This also holds for the two currently bestperforming ACO variants in practice, namely, the Ant Colony System (ACS) [9] and the MAX-MIN Ant System (MMAS) [20].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 57, "endOffset": 66}, {"referenceID": 0, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 68, "endOffset": 77}, {"referenceID": 0, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 68, "endOffset": 77}, {"referenceID": 1, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 79, "endOffset": 88}, {"referenceID": 1, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 79, "endOffset": 88}, {"referenceID": 1, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 90, "endOffset": 99}, {"referenceID": 2, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 90, "endOffset": 99}, {"referenceID": 2, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 101, "endOffset": 110}, {"referenceID": 2, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 101, "endOffset": 110}, {"referenceID": 3, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 110, "endOffset": 119}, {"referenceID": 3, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 110, "endOffset": 119}, {"referenceID": 4, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 121, "endOffset": 130}, {"referenceID": 4, "context": "Then, we have V = {0, 1, 2, 3, 4, 5} and E = {[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 2, 2], [0, 2, 3], [0, 3, 3][0, 4, 4], [0, 5, 5]}.", "startOffset": 121, "endOffset": 130}, {"referenceID": 0, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 12, "endOffset": 21}, {"referenceID": 0, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 26, "endOffset": 35}, {"referenceID": 0, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 26, "endOffset": 35}, {"referenceID": 0, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 37, "endOffset": 46}, {"referenceID": 3, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 37, "endOffset": 46}, {"referenceID": 4, "context": ", matchList([0, 0, 1]) = {[1, 0, 1], [1, 4, 5]}.", "startOffset": 37, "endOffset": 46}, {"referenceID": 0, "context": "For example if (X, Y ) = {\u201cbceabcd\u201d,\u201cabcdbec\u201d} then according to the static heuristic much higher value will be assigned to edge block [0, 0, 1] than to [0, 0, 0].", "startOffset": 135, "endOffset": 144}, {"referenceID": 0, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 15, "endOffset": 24}, {"referenceID": 0, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 56, "endOffset": 65}, {"referenceID": 0, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 56, "endOffset": 65}, {"referenceID": 1, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 56, "endOffset": 65}, {"referenceID": 2, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 110, "endOffset": 119}, {"referenceID": 5, "context": "But if we take [0, 0, 1], we must match it to the block [1, 1, 2] and we further miss the opportunity to take [0, 3, 6] later.", "startOffset": 110, "endOffset": 119}, {"referenceID": 0, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 76, "endOffset": 85}, {"referenceID": 0, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 76, "endOffset": 85}, {"referenceID": 0, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 76, "endOffset": 85}, {"referenceID": 0, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "In the example, minSpan([0, 0, 0]) is 1 as follows: matchList([0, 0, 0]) = {[1, 1, 1], [1, 4, 4]}.", "startOffset": 87, "endOffset": 96}, {"referenceID": 0, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 5, "endOffset": 14}, {"referenceID": 0, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 5, "endOffset": 14}, {"referenceID": 0, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 5, "endOffset": 14}, {"referenceID": 0, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 29, "endOffset": 38}, {"referenceID": 3, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 29, "endOffset": 38}, {"referenceID": 3, "context": "span([1, 1, 1]) = 4 and span([1, 4, 4] = 1).", "startOffset": 29, "endOffset": 38}, {"referenceID": 0, "context": "On the other hand, minSpan([0, 0, 1]) is 4.", "startOffset": 27, "endOffset": 36}, {"referenceID": 0, "context": "So, according to the dynamic heuristic much higher numeral will be assigned to block [0, 0, 0] rather than to block [0, 0, 1].", "startOffset": 116, "endOffset": 125}, {"referenceID": 19, "context": "Like [20], we use the following values for \u03c4max and \u03c4min: \u03c4max = 1 \u03b5\u00b7cost(LGB) , and \u03c4min = \u03c4max(1\u2212 n \u221a pbest) (avg\u22121) n pbest .", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "Initially, the pheromone values of all edge blocks (substring) are initialized to initPheromone which is a large value to favor the exploration at the first iteration [20].", "startOffset": 167, "endOffset": 171}, {"referenceID": 0, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 63, "endOffset": 72}, {"referenceID": 0, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 77, "endOffset": 86}, {"referenceID": 0, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 77, "endOffset": 86}, {"referenceID": 0, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 88, "endOffset": 97}, {"referenceID": 2, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 88, "endOffset": 97}, {"referenceID": 3, "context": "For example, if (X, Y ) = {\u201cababc\u201d,\u201cabcab\u201d} then the matchList([0, 0, 1]) = {[1, 0, 1], [1, 3, 4]}.", "startOffset": 88, "endOffset": 97}, {"referenceID": 0, "context": "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.", "startOffset": 20, "endOffset": 29}, {"referenceID": 0, "context": "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.", "startOffset": 20, "endOffset": 29}, {"referenceID": 0, "context": "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.", "startOffset": 49, "endOffset": 58}, {"referenceID": 2, "context": "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.", "startOffset": 49, "endOffset": 58}, {"referenceID": 3, "context": "In our example span([1, 0, 1]) = 3 where as span([1, 3, 4]) = 2.", "startOffset": 49, "endOffset": 58}, {"referenceID": 20, "context": "The shuffling is done using the online toolbox [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "We have compared our approach with the greedy algorithm of [4] because none of the other algorithms in the literature are for general MCSP: each of the other approximation algorithms put some restrictions on the parameters.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "Table 2, Table 3 and Table 4 present the comparison between our approach and the greedy approach [4] for the random DNA sequences.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "Table 2: Comparison between Greedy approach [4] and MMAS on random DNA sequences (Group 1, 200 bps).", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Table 3: Comparison between Greedy approach [4] and MAX-MIN on random DNA sequences (Group 2, 400 bps).", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Table 4: Comparison between Greedy approach [4] and MAX-MIN on random DNA sequences (Group 3, 600 bps).", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Table 6: Comparison between Greedy approach [4] and MMAS on real gene sequence.", "startOffset": 44, "endOffset": 47}], "year": 2017, "abstractText": "In this paper, we consider the problem of finding a minimum common partition of two strings (MCSP). The problem has its application in genome comparison. As it is an NP-hard, discrete combinatorial optimization problem, we employ a metaheuristic technique, namely, MAX-MIN ant system to solve this. The experimental results are found to be promising.", "creator": "LaTeX with hyperref package"}}}