{"id": "1401.4590", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks", "abstract": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted.", "histories": [["v1", "Sat, 18 Jan 2014 21:03:23 GMT  (1181kb)", "http://arxiv.org/abs/1401.4590v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["enrique amig\\'o", "julio gonzalo", "javier artiles", "felisa verdejo"], "accepted": false, "id": "1401.4590"}, "pdf": {"name": "1401.4590.pdf", "metadata": {"source": "CRF", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks", "authors": ["Enrique Amig\u00f3", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo"], "emails": ["enrique@lsi.uned.es", "julio@lsi.uned.es", "javier.artiles@qc.cuny.edu", "felisa@lsi.uned.es"], "sections": [{"heading": null, "text": "In addition to discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the text clustering problem, where there is a trade-off between precision and recall-based metrics and the results are particularly sensitive to the weighting scheme used for their combination. Remarkably, our experiments show that UIR can be used as a predictor of how well the differences between systems measured on a particular test bench hold up on another test bench."}, {"heading": "1. Introduction", "text": "Many artificial intelligence tasks cannot be evaluated with a single quality criterion, and some kind of weighted combination is required to provide system rankings. Many problems, for example, require consideration of both precision (P) and recall (R) to compare the performance of systems. Perhaps the most common combination function is the F measurement (van Rijsbergen, 1974), which includes a parameter that determines the relative weight of the measurements; if \u03b1 = 0.5, both measurements have the same relative weight and F calculates their harmonious meaning. One problem with weighted combination quantities is that relative weights are intuitively set for a given task, but at the same time a slight change in relative weights can cause significant changes in system rankings. The reason for this behavior is that a general improvement in F is often derived from an improvement in individual systems."}, {"heading": "2. Combining Functions for Evaluation Metrics", "text": "In this section, we briefly discuss various combination criteria for metrics, explaining the reasons behind each metric weighting approach and its impact on system ranking."}, {"heading": "2.1 F-measure", "text": "The most common method of combining two evaluation variables is the F-Measure (van Rijsbergen, 1974). It was originally proposed for the evaluation of information retrieval systems (IR), but its application has expanded to many other tasks. Given two measurements P and R (e.g. precision and recall, purity and inverse purity, etc.), van Rijsbergen's F-Measure combines them into a single measure of efficiency as follows: F (R, P) = 1\u03b1 (1P) + (1 \u2212 \u03b1) (1 R) F assumes that the \u03b1-value is set for a particular evaluation scenario. This parameter represents the relative weight of the measurement variables. In some cases, the \u03b1-value is not decisive; especially when measurements are correlated, Figure 1 shows the precision and recall values achieved in the CoNLL-2004 task of evaluating semantic labeling systems, not precision, but precision."}, {"heading": "2.2 Precision and Recall Break-even Point", "text": "Another way to combine metrics is to evaluate the system at the point where one metric matches the other (Tao Li & Zhu, 2006).This method is applicable if each system is represented by a trade-off between the two metrics, such as a precision / callback curve. This method is based on the idea that an increase in both metrics necessarily implies an overall improvement in quality. It assumes, for example, that it is better to obtain 0.4 precision at the callback point than 0.3 precision at the callback point.In fact, the break-even point assumes the same relevance for both metrics, taking into account the precision / callback point at which the system distributes its efforts evenly between both metrics. In fact, we could change the relative relevance of the metrics if we calculate the break-even point. Figure 2 illustrates this idea. The continuous curve represents the target conflict between the precision and the backpoint for S1."}, {"heading": "2.3 Area Under the Precision/Recall Curve", "text": "Another example is the Receiver Operating Characteristic (ROC) function, which is used to evaluate binary classifiers (Cormack & Lynam, 2005).ROC calculates the likelihood that a positive sample, regardless of the threshold used to classify the samples, will obtain a confidence value higher than a negative sample.Both functions are related to the range AUC that exists under the precision / recall curve (Cormack & Lynam, 2005).In both MAP and ROC, the regions with low and high recall rates have the same relative relevance in calculating this range. Again, we could change the measures to assign different weights to high and low recall values. In fact, a weighted zone below the curve is proposed in (Weng & Poon, 2008)."}, {"heading": "3. Combining Metrics in Clustering Tasks", "text": "In this section, we present metric combination experiments for a specific cluster task. Our results confirm the importance of quantifying the robustness of systems across different weighting schemes."}, {"heading": "3.1 The Clustering Task", "text": "Clustering (grouping of similar objects) has applications in a wide range of Artificial Intelligence problems. Especially in the context of access to textual information, cluster algorithms are used for information retrieval (clustering of text documents according to their similarity in content), document summary (grouping of portions of text to detect redundant information), topic tracking, opinion forming (e.g. grouping of opinions on a particular topic), etc. In such scenarios, cluster distributions produced by systems are usually evaluated for their similarity to a manually generated gold standard (extrinsic evaluation).There is a wide range of metrics measuring this similarity (Amigo, Gonzalo, Artiles, & Verdejo, 2008), but all of them are based on two quality dimensions: (i) to what extent articles in the same cluster also belong to the same gold standard group (ii), to what extent articles in different metrics 2003 also belong to different groups (ypic, 2001, Pugiar, Entrtric, and Entrtrtrtrtrtrtrine)."}, {"heading": "3.2 Dataset", "text": "WePS (Web People Search) campaigns focus on the task of clearly naming people names in web search results (e.g. \"John Smith\").The challenge is to correctly estimate the number of different people sharing the name in search results and group documents that relate to the same person. For each person name, WePS records deliver about 100 websites from the top search results, using the person name cited as a query. To provide different ambiguity scenarios, U.S. Census, Wikipedia and lists of members of the Computer Science Conferences program committee were evaluated by comparing their results to a gold standard: a manual grouping of documents created by two human judges in two rounds (first they commented on the corpse independently and then they discussed the discrepancies together).WePS document is evaluated as a Wek.PS document, although these manual rounds of search documents are created by two judges, the two human search documents are grouped by Amazon."}, {"heading": "3.3 Thresholds and Stopping Criteria", "text": "The clustering task includes three main aspects that determine the output quality of the system: the first is the method of measuring similarity between documents; the second is the clustering algorithm (k-neighbors, Hierarchical Agglomerative Clustering, etc.); and the third aspect that usually needs to be taken into account consists of a few related variables that need to be fixed: a similarity threshold - above which two sides are considered related - and a stop criterion that determines when the clustering process stops and thus the number of clusters produced by the system. Figure 3 shows how purity and inverse purity values change for different clustering breakpoints, for one of the systems evaluated on the WePS-1b corpus 3. Purity focuses on the frequency of the most common category in each cluster (Amigo-et al., 2008)."}, {"heading": "3.4 Robustness Across \u03b1 Values", "text": "Determining the appropriate \u03b1 value for a given scenario is not trivial. For example, from the point of view of a user in the WePS task, it is easier to discard a few irrelevant documents from the good cluster (because its precision is not perfect, but it has a high recall) than to have to search for additional relevant documents in all clusters (because its precision is high, but its recall is not). Therefore, it seems that inverse purity should take precedence over purity, i.e., the value of \u03b1 should be below 0.5. However, from the point of view of a company offering a search service for web persons, the situation is quite different: their priority is very high precision, because mixing the profiles of, say, a criminal and a doctor can lead to the company being sued. From their point of view, \u03b1 should receive a high value."}, {"heading": "3.5 Robustness Across Test Beds", "text": "The average size of clusters in the gold standard may vary from one test rig to another. As this affects the purity and reverse purity compromise, the same cluster system may achieve a different balance between the two metrics in different corporas; and this can lead to conflicting evaluation results when comparing systems across corporas, even for the same \u03b1 value. In the WePS-1b test rig (Artiles et al., 2007), B1 performs much better than B100 (0.58 vs. 0.49 using F\u03b1 = 0.5). However, in the WePS-2 dataset (Artiles et al., 2009), B100 performs better than B1 (0.53 versus 0.34), due to the fact that singletons in WePS-2 occur less frequently. In other words, the comparison between B100 and B1 depends both on the \u03b1 value and the particular distribution of reference cluster sizes in the test.Our point is that system improvements that are not affected by this system, the phenomenon should not be S\u03b1 and the S1 should not be affected by this one."}, {"heading": "4. Proposal", "text": "Our primary motivation in this article is to quantify the robustness of \u03b1 values in order to supplement the information of the traditional system rankings. To this end, we present the unanimous improvement ratio in this section."}, {"heading": "4.1 Unanimous Improvements", "text": "Van Rijsbergen (1974) argued that it is not possible to determine empirically which metric combination function is most suitable in the context of information retrieval evaluation. (Based on the principles of measurement theory, however, van Rijsbergen described the set of properties that a metric combination function should fulfil, which includes the axiom of independence (also known as single cancellation) from which the monotonicity property is derived. (The monotonicity property states that the quality of a system that surpasses or equates another metric is necessarily equal or better than the other. In other words, one system is better than the other, without any dependence on how the relative importance of each metric is set. We will define a combination method for metrics, Unanimous improvement based on this property. (QX) This system is better than the other, without any dependence."}, {"heading": "4.2 Unanimous Improvement Ratio", "text": "In the limit condition in which all samples are determined, a three-step improvement (TB = B) is necessary (TB = B) = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B"}, {"heading": "5. Theoretical Foundations", "text": "In this section we discuss the theoretical foundations of the unanimous improvement rate in the context of the Conjoint Measurement Theory. We then describe the formal characteristics of the UIR and its implications from the point of view of the evaluation methodology. Readers who are only interested in the practical implications of using the UIR can proceed directly to Section 6."}, {"heading": "5.1 Conjoint Measurement Theory", "text": "The problem of the combination of measurement measures is closely related to the Conjoint Measurement Theory, which was established independently of the economist Debreu (1959) and the mathematical psychologist R. Duncan Luce and the statistician John Tukey (Luce & Tukey, 1964).The theory of measurement defines the conditions necessary to establish a homomorphism between an empirical relationship structure (e.g. \"John is taller than Bill\") and a numerical relationship structure (\"John's height is 1.79 meters and Bill's height is 1.56 meters\").In the case of Conjoint Measurement Theory, the relational structure is factored into two (or more) ordered substructures. \"In our context, the numerical structures are given by the evaluation of metric values (e.g. purity and reverse purity)."}, {"heading": "5.2 Formal Properties of the Unanimous Improvement", "text": "The unanimous improvement of connectivity (B) trivially fulfils most of the desirable characteristics proposed by van Rijsbergen (1974) in terms of metric combination functions: transitivity, independence, Thomsen condition, limited solvability, essential components and decreasing marginal effectiveness; the exception being the connectivity characteristic (4). Given that the non-comparability (biased improvements, see Section 4.1) is derived from the unanimous improvement, it is possible to find system pairs where neither QX (a) nor QX (b) or QX (b) are equally satisfactory. Therefore, let us not satisfy connectivity. Formally, the limitation of the unanimous improvement is that it does not represent a weak order because it cannot satisfy transitivity and connectivity (b)."}, {"heading": "5.2.1 Uniqueness of the Unanimous Improvement", "text": "The unanimous improvement has the interesting property that it does not contradict any rating given by the F measurement (regardless of the value used in F: QX (a) \u2265 \"QX.\" This property is crucial for the purpose of verifying the robustness of system improvements in terms of values. And crucially, unanimous improvement is the only function that fulfils this property. Specifically, unanimous improvement is the only relational structure that does not contradict the satisfaction of monotonicity (see Section 5.1). To prove this claim, we must define the concept of compatibility with an additive common structure."}, {"heading": "6. F versus UIR: Empirical Study", "text": "In this section, we will conduct a series of empirical studies on the WePS corpora to find out how UIR behaves in practice. First, we will focus on a set of empirical results that show how UIR rewards robustness beyond \u03b1 values and how this information complements the information provided by F. Second, we will examine to what extent - and why - F and UIR correlate."}, {"heading": "6.1 UIR: Rewarding Robustness", "text": "Figure 4 shows three examples of system comparisons in WePS-1b corpus. 3. Using the measurements of purity and inverse purity. Each curve represents the F\u03b1 value achieved for a system according to different \u03b1 values. System S6 (black curves) is compared to S10, S9 and S11 (grey curves) in each of the three diagrams. In all cases, there is a similar quality increase according to F\u03b1 = 0.5; however, UIR values are between 0.32 and 0.42, depending on how robust the difference to changes in \u03b1 is. The highest difference in UIR is for the (S6, S11) system pair (most rectangular diagram), because these systems do not exchange their F\u03b1 values for a specific value. The smallest UIR value is for (S6, S10), where S6 is better than S10 for values below 0.8, and worse if it is larger. This comparison illustrates how UIR values are captured, for similar increases are dependent in F."}, {"heading": "6.2 Correlation Between F and UIR", "text": "The fact that UIR and F provide different information about the outcome of an experiment does not imply that UIR and F are orthogonal; in fact, there is a certain correlation between the two values. Figure 5 represents F\u03b1 = 0.5 differences and UIR values for any system pair in the WePS-1. The general trends are (i) high UIR values imply a positive difference in F (ii) high | 4F0,.5 | values do not imply anything in relation to UIR values; (iii) low UIR does not seem to imply anything in relation to | 4F0,.5 | values. Overall, the figure suggests a triangular relationship that results in a Pearson correlation of 0.58."}, {"heading": "6.2.1 Reflecting improvement ranges", "text": "If there is a consistent difference between two systems for most \u03b1 values, UIR rewards larger areas of improvement. Let's illustrate this behavior using three sample system pairs from the WePS-1 test bed. Figure 6 represents the F\u03b1 values for three system pairs. In all cases, one system improves the other for all \u03b1 values. However, UIR shows greater improvements in F (greater distance between the black and gray curve), which is because a greater average improvement over test cases makes it less likely that individual test cases (which UIR considers as such) contradict the average result. Another interesting finding is that when both metrics improve, the metric with the weakest improvement determines the behavior of UIR. Figure 7 illustrates this relationship for the ten system pairs with the greatest improvement; the Pearson correlation in this graph is 0.94. In other words, when both metrics improve, UIR is sensitive to the weakest improvement."}, {"heading": "6.2.2 Analysis of boundary cases", "text": "In order to better understand the relationship between UIR and F, we will now examine in detail two cases of system improvements where UIR and F lead to drastically different results. These two cases are marked as A and B. The point marked as Case A in the figure corresponds to the comparison of systems S1 and S15. There is a significant (and statistically significant) difference between the two systems according to F\u03b1 = 0.5. However, UIR has a low value for each test case, i.e. the improvement is not robust to changes in \u03b1 according to UIR. A visual explanation of these results can be seen in Figure 8. It shows the purity and inverse purity results of systems S1, S15 for each test case. In most test cases, S1 has an important advantage in purity at the expense of a slight - but consistent - loss in inverse purity. Given that F\u03b1 = 0.5 purity ranges compares purity and inverse purity ranges ranges, it is noted that there is an important and statistically significant - loss of purity in most cases."}, {"heading": "6.3 A Significance Threshold for UIR", "text": "We already mentioned that UIR has parallelism with statistical significance tests, which are typically used in information retrieval to estimate the probability p that an observed difference between two systems is achieved by chance, i.e. the difference is an artifact of the test collection and not a real difference between systems. However, when calculating statistical significance, it is useful to set a threshold that enables a binary decision; for example, a result is often called statistically significant when p < 0.05, and not significantly different. However, the choice of the significance level is arbitrary, but it helps in reporting and summarizing significance tests. Stricter thresholds increase the confidence of the test, but lead to an increased risk of not detecting a significant result. The same situation applies to UIR: We would like to set a UIR threshold that determines whether a difference is relative and robust to changes in the system observed."}, {"heading": "6.4 UIR and System Rankings", "text": "We now turn to the question of how to use UIR as a component in the analysis of the results of an evaluation campaign. To answer this question, we applied UIR to the results of the evaluation campaign WePS-2 (Artiles et al., 2009). In this campaign, the best results for each system were evaluated according to Bcubed precision and recall metrics, combined with F\u03b1 = 0.5. In addition to all participating systems, three baseline approaches were included in the ranking: all documents in a cluster (B100), each document in a cluster (B1) and the combination of both (BCOMB) 5.Table 7 shows the results of the application of UIR to participating systems WePS-2. The robust improvements are presented in the third column (\"improved systems\") UIR."}, {"heading": "7. UIR as Predictor of the Stability of Results across Test Collections", "text": "A common problem when evaluating systems that deal with natural language is that the results on different test systems are often contradictory. In the particular case of text clustering, one factor contributing to this problem is that the average size of clusters can vary across different thresholds, and this variability modifies the optimal balance between precision and recall. A system that tends to favor precision can achieve good results in a dataset with a small average cluster size and worse results in a test collection with a larger average cluster size. Therefore, if we only use F to combine individual metrics, we can achieve conflicting results across different test beds. Since UIR does not depend on metric evaluation criteria, our hypothesis is that a high UIR value increases the robustness of evaluation results across test beds.5. See the work of Artiles et al. (2009) for an extended explanation."}, {"heading": "8. Parametric versus Non-Parametric UIR", "text": "According to our analysis (see Section 5.2), the only relational structure over pairs < Pi, Ri > that does not depend on the weight criteria for IR is the unanimous improvement: a) b) Pa \"Pb\" Ra \"Rb\" F. When comparing systems, our UIR measurement can take into account the unanimous improvement results in test cases: UIRX, T (a) - Prob (b) - Prob (b) - Prob (b) - Prob (b) - Prob (b) - Prob) - Prob (b) - Prop) - Prop (b) - Prop (a) - Prop. As we have already said, the main disadvantage of unanimous improvement is that it is a three-step function that does not take into account metric ranges; UIR - UIR - inherits this disadvantage."}, {"heading": "9. Conclusions", "text": "This year, it has never been as good as it has been this year."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the Spanish Government (Holopedia Scholarship, TIN2010-21128-C02) and the Madrid Regional Government in the framework of the MA2VICMR research network (S2009 / TIC-1542)."}], "references": [{"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["E. Amig\u00f3", "J. Gonzalo", "J. Artiles", "F. Verdejo"], "venue": "Information Retrieval,", "citeRegEx": "Amig\u00f3 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Amig\u00f3 et al\\.", "year": 2008}, {"title": "WePS-2 Evaluation Campaign: Overview of the Web People Search Clustering Task", "author": ["J. Artiles", "J. Gonzalo", "S. Sekine"], "venue": "In Proceedings Of The 2nd Web People Search Evaluation Workshop (WePS", "citeRegEx": "Artiles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2009}, {"title": "The SemEval-2007 WePS evaluation: Establishing a Benchmark for the Web People Search Task", "author": ["J. Artiles", "J. Gonzalo", "S. Sekine"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Artiles et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2007}, {"title": "Entity-Based Cross-Document Coreferencing Using the Vector Space Model", "author": ["A. Bagga", "B. Baldwin"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics", "citeRegEx": "Bagga and Baldwin,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin", "year": 1998}, {"title": "Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling", "author": ["X. Carreras", "L. M\u00e0rquez"], "venue": "HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning", "citeRegEx": "Carreras and M\u00e0rquez,? \\Q2004\\E", "shortCiteRegEx": "Carreras and M\u00e0rquez", "year": 2004}, {"title": "Topological methods in cardinal utility theory", "author": ["G. Debreu"], "venue": "Mathematical Methods in the Social Sciences, Stanford University Press,", "citeRegEx": "Debreu,? \\Q1959\\E", "shortCiteRegEx": "Debreu", "year": 1959}, {"title": "Scalable clustering methods for data mining", "author": ["J. Ghosh"], "venue": "Handbook of Data Mining. Lawrence Erlbaum", "citeRegEx": "Ghosh,? \\Q2003\\E", "shortCiteRegEx": "Ghosh", "year": 2003}, {"title": "On Clustering Validation Techniques", "author": ["M. Halkidi", "Y. Batistakis", "M. Vazirgiannis"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "Halkidi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Halkidi et al\\.", "year": 2001}, {"title": "Simultaneous conjoint measurement: a new scale type of fundamental measurement", "author": ["R. Luce", "J. Tukey"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Luce and Tukey,? \\Q1964\\E", "shortCiteRegEx": "Luce and Tukey", "year": 1964}, {"title": "Multi-Document Statistical Fact Extraction and Fusion", "author": ["G.S. Mann"], "venue": "Ph.D. thesis,", "citeRegEx": "Mann,? \\Q2006\\E", "shortCiteRegEx": "Mann", "year": 2006}, {"title": "Comparing clusterings", "author": ["M. Meila"], "venue": "In Proceedings of COLT", "citeRegEx": "Meila,? \\Q2003\\E", "shortCiteRegEx": "Meila", "year": 2003}, {"title": "Measurement: The theory of numerical assignments", "author": ["L. Narens", "R.D. Luce"], "venue": "Psychological Bulletin,", "citeRegEx": "Narens and Luce,? \\Q1986\\E", "shortCiteRegEx": "Narens and Luce", "year": 1986}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "In KDD Workshop on Text Mining,2000", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Empirical Studies on Multilabel Classification", "author": ["C.Z. Tao Li", "S. Zhu"], "venue": "In Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI", "citeRegEx": "Li and Zhu,? \\Q2006\\E", "shortCiteRegEx": "Li and Zhu", "year": 2006}, {"title": "Foundation of evaluation", "author": ["C.J. van Rijsbergen"], "venue": "Journal of Documentation,", "citeRegEx": "Rijsbergen,? \\Q1974\\E", "shortCiteRegEx": "Rijsbergen", "year": 1974}, {"title": "A New Evaluation Measure for Imbalanced Datasets", "author": ["C.G. Weng", "J. Poon"], "venue": "Seventh Australasian Data Mining Conference (AusDM 2008), Vol. 87 of CRPIT,", "citeRegEx": "Weng and Poon,? \\Q2008\\E", "shortCiteRegEx": "Weng and Poon", "year": 2008}, {"title": "Criterion functions for document clustering: Experiments and analysis", "author": ["Y. Zhao", "G. Karypis"], "venue": "Technical Report TR 01\u201340,", "citeRegEx": "Zhao and Karypis,? \\Q2001\\E", "shortCiteRegEx": "Zhao and Karypis", "year": 2001}], "referenceMentions": [{"referenceID": 6, "context": "A wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach, Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001), precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.", "startOffset": 77, "endOffset": 125}, {"referenceID": 10, "context": "A wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach, Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001), precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.", "startOffset": 270, "endOffset": 326}, {"referenceID": 0, "context": "See the work of Amig\u00f3 et al. (2008) for a detailed overview.", "startOffset": 16, "endOffset": 36}, {"referenceID": 1, "context": "For our experiments we have focused on the evaluation results obtained in the WePS-1 (Artiles, Gonzalo, & Sekine, 2007) and WePS-2 (Artiles et al., 2009) evaluation campaigns.", "startOffset": 131, "endOffset": 153}, {"referenceID": 9, "context": "The WePS-1 corpus also includes data from the Web03 test bed (Mann, 2006), which was used for trial purposes and follows similar annotation guidelines, although the number of document per ambiguous name is more variable.", "startOffset": 61, "endOffset": 73}, {"referenceID": 0, "context": "Purity focuses on the frequency of the most common category into each cluster (Amig\u00f3 et al., 2008).", "startOffset": 78, "endOffset": 98}, {"referenceID": 2, "context": "For instance, in the WePS-1b test bed (Artiles et al., 2007), B1 substantially outperforms B100 (0.", "startOffset": 38, "endOffset": 60}, {"referenceID": 1, "context": "In the WePS-2 data set (Artiles et al., 2009), however, B100 outperforms B1 (0.", "startOffset": 23, "endOffset": 45}, {"referenceID": 14, "context": "Van Rijsbergen (1974) argued that it is not possible to determine empirically which metric combining function is the most adequate in the context of Information Retrieval evaluation.", "startOffset": 4, "endOffset": 22}, {"referenceID": 5, "context": "The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the mathematical psychologist R.", "startOffset": 153, "endOffset": 167}, {"referenceID": 5, "context": "The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey, 1964). The Theory of Measurement defines the necessary conditions to state an homomorphism between an empirical relational structure (e.g. \u201cJohn is bigger than Bill\u201d) and a numeric relational structure (\u201cJohn\u2019s height is 1.79 meters and Bill\u2019s height is 1.56 meters\u201d). In the case of the Conjoint Measurement Theory, the relational structure is factored into two (or more) ordered substructures (e.g. \u201cheight and weight\u201d). In our context, the numerical structures are given by the evaluation metric scores (e.g. Purity and Inverse Purity). However, we do not have an empirical quality ordering for clustering systems. Different human assessors could assign more relevance to Purity than to Inverse Purity or viceversa. Nevertheless, the Conjoint Measurement Theory does provide mechanisms that state what kind of numerical structures can produce an homomorphism assuming that the empirical structure satisfies certain axioms. Van Rijsbergen (1974) used this idea to analyze the problem of combining evaluation metrics.", "startOffset": 153, "endOffset": 1207}, {"referenceID": 14, "context": "The F-measure proposed by van Rijsbergen (1974) and the arithmetic mean of P,R satisfy all these axioms.", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "2 Formal Properties of the Unanimous Improvement The Unanimous Improvement \u2265\u2200x trivially satisfies most of the desirable properties proposed by van Rijsbergen (1974) for metric combining functions: transitivity, independence, Thomsen condition, Restricted Solvability, Essential Components and Decreasing Marginal Effectiveness; the exception being the connectedness property4.", "startOffset": 148, "endOffset": 166}, {"referenceID": 1, "context": "In order to answer this question we have applied UIR to the results of the WePS-2 evaluation campaign (Artiles et al., 2009).", "startOffset": 102, "endOffset": 124}, {"referenceID": 1, "context": "See the work of Artiles et al. (2009) for an extended explanation.", "startOffset": 16, "endOffset": 38}], "year": 2011, "abstractText": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen\u2019s F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.", "creator": "TeX"}}}