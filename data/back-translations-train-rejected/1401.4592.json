{"id": "1401.4592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Proximity-Based Non-uniform Abstractions for Approximate Planning", "abstract": "In a deterministic world, a planning agent can be certain of the consequences of its planned sequence of actions. Not so, however, in dynamic, stochastic domains where Markov decision processes are commonly used. Unfortunately these suffer from the curse of dimensionality: if the state space is a Cartesian product of many small sets (dimensions), planning is exponential in the number of those dimensions.", "histories": [["v1", "Sat, 18 Jan 2014 21:03:58 GMT  (849kb)", "http://arxiv.org/abs/1401.4592v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jiri baum", "ann e nicholson", "trevor i dix"], "accepted": false, "id": "1401.4592"}, "pdf": {"name": "1401.4592.pdf", "metadata": {"source": "CRF", "title": "Proximity-Based Non-uniform Abstractions for Approximate Planning", "authors": ["J\u01d0\u0155\u0131 Baum", "Ann E. Nicholson", "Trevor I. Dix"], "emails": ["Jiri@baum.com.au", "Ann.Nicholson@monash.edu", "Trevor.Dix@monash.edu"], "sections": [{"heading": null, "text": "Our new technique uses the intuitive strategy of selectively ignoring different dimensions in different parts of the state space. The resulting inconsistency has a strong impact, as the approach is no longer Markovian and requires the use of a modified planner. We also use a spatial and temporal approximation that responds to the continued planning and movement of the actor through the state space to dynamically adapt abstraction as a planning progress. We present qualitative and quantitative results in a number of experimental areas that show that an actor using this novel approach successfully finds solutions to the planning problem by using much less than the entire state space. We evaluate and analyze the characteristics of areas that our method can exploit."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Planning under Non-uniform Abstractions", "text": "In a non-deterministic world, where a planning agent can only be sure of the consequences of his actions as probabilities, he cannot plan a simple sequence of actions to achieve his goals. To be able to plan in such dynamic, stochastic areas, he must adopt a more sophisticated approach. Markov decision-making processes are an appropriate and commonly used representation of this type of planning problem."}, {"heading": "2.1 Illustrative Problems", "text": "To help with the exposure, here are two example problems: the full set of experimental areas is presented in Section 8.1; the two illustrative problems both stem from a grid navigation area shown in Figure 1; both have integer X and Y coordinates from 0 to 9; three doors that can be either open or closed; and a damage indicator that can be either yes or no; the agent can move in one of the four directions, open a door if it is located next to it, or do nothing; the doors are relatively difficult to open, with a 10% probability of success per time step, while the movement has an 80% chance of success, with no effect in the event of failure; running into a wall or a closed door causes damage that cannot be repaired; the transitions are shown in Table 1; the agent starts at the place marked s0 in Figure 1 with the key closed and without damage, and the goal is to open the key with the problem marked with the keys, each of which are also required to reach the keys."}, {"heading": "2.2 Exact Planning", "text": "An approach to exact planning in stochastic areas involves the use of Markov Decision Processes (MDPs). An MDP is a tuple of S, A, T, R, s0, where S is the state space, A is the totality of available measures, T is the transition function, R is the reward function, and s0 \"S is the initial state. The agent begins in the state s0.\" In each time step, the agent selects an action that, together with the current state, distributes a distribution over S. The current state in the next time step is random and we write PrT (s, s \") for the probability that the action taken in the state s will explicitly take place in the next time step. The agent also receives a reward at each time step, calculated by R from the current state (and possibly also the selected action)."}, {"heading": "2.3 Uniform Abstraction", "text": "One method of approach is to exploit these dimensions by ignoring some of them - those that are irrelevant or only marginally relevant - in order to obtain an approximate solution. It is uniform in the sense that the same dimensions are ignored throughout the state. Since this approach attacks the curse of dimensionality where it originates, it should be very effective in counteracting it. Dearden and Boutilier use it to obtain an exact solution (Boutilier, 1997) or an approximate solution (Boutilier & Dearden, 1997). However, their abstractions are fixed throughout the execution, and dimensions are also erased from the problem in a predetermined sequence, making their approach somewhat inflexible. Likewise, Nicholson and Kaelbling (1994) propose this technique for approximate planning. They delete dimensions from the problem based on sensitivity analysis and refine their abstraction as execution time allowed."}, {"heading": "2.4 Non-uniform Abstraction", "text": "Our approach, non-uniform abstraction, replaces the state space S with W, a certain kind of division of S, as originally introduced by Baum and Nicholson (1998). We call W the world view, so that members of W are states of belief, while members of S. Non-uniform abstraction is based on the intuitive idea of ignoring some dimensions in some parts of the state space. For example, a door is of interest only when the actor is about to go through it, and can be ignored in those parts of the state space that are far from the door. In a certain member of the world view wi-W, each dimension is either fully taken into account (concrete, refined in) or ignored altogether (abstract, roughly expressed). wi = D = 1 w i d and each individual world is a subset of the corresponding Sd for concrete dimensions and equal to Sd for abstract dimensions."}, {"heading": "2.5 Comparison to Other Approaches", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2.6 Dynamic Approximate Planning", "text": "The top level algorithm is shown as algorithm 1. After some initialization, consisting of selecting the original abstraction and determining the policy, value, and proximity to a0, 0, and proportional to the size of each ideological state, or 5, the planner enters an infinite loop in which he stochastically switches between five possible calculations, each of which is described in the following sections. Here and elsewhere in the algorithm, we use stochastic choice as standard in the absence of a more focused methodology. The agent is assumed to have processing power at his disposal while acting so that he can continuously improve his policy, modify the approximation, and update the focus of his planning on the current state. This means that the agent does not have to plan so well for improbable possibilities, and can therefore extend more of his planning efforts to the most likely paths and to the nearest future, expecting that if and when he reaches other parts of the state space, he can improve approximation."}, {"heading": "3. Solving Non-uniformly Abstracted MDPs", "text": "Considering a non-uniform abstraction value (w), the easiest way to use it for planning is to take one of the standard MDP algorithms (such as the modified policies of Puterman and Shin (1978) to ignore the actual abstraction (such as the modified policies of Puterman and Shin (1978), and it is not possible to use the simple variant for the non-uniform abstraction). The possibilities of transition from one philosophical state to another are aligned with a uniform distribution across the concrete states (or possibly another distribution) when more information is available. Algorithm 2 Politics and value calculation for all philosophical states w do Update value for w do Update value for wh procedures when PrT (w), an updated value for the (w), an optimistic value for all philosophical states w)."}, {"heading": "3.1 Locally Uniform Abstraction", "text": "The ostrich effect occurs when states of different abstractions are considered, for example one in which a door is abstract and one in which the same door is concrete and closed. The solution is to make abstraction locally homogeneous and therefore locally Markovian iterative step for the duration of the political generation. By making abstraction locally, temporarily homogeneous, the iterative step of the political generation algorithm must never operate beyond the edge of an abstract region, and since the same information is available in all states that are considered at any point, there is no impetus for one of them to be favored or avoided on that basis (for example, by avoiding a state in which a door is concrete and closed in favor of an abstract region). The chosen action is chosen only on the basis of the information and not on the basis of its presence or absence. This is a modification of the updating policy for the algorithm 2 procedure: since the states are considered individually, the region will be encompassing the region, the region will be encompassing the region."}, {"heading": "4. Initial Abstraction", "text": "This year, it has come to the point where you see yourself able to live in a country where people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "5. Policy-Based Refinement", "text": "This section introduces the first method of modification of world view, policy-based refinement. This method modifies world view directly on the basis of current approximate policy. In particular, it refines states on the basis of differences between the measures envisaged in neighboring, differently abstract states. Where such differences indicate that one dimension can be important, all neighboring states that are abstract in this dimension are refined (i.e. this dimension is concretized in these states).The method was previously introduced by Baum and Nicholson (1998), who, using an example of a small navigation domain (the 3Doors problem of this paper), showed that this refinement method led to good politics, though not optimal."}, {"heading": "5.1 Motivation", "text": "The motivation for this method is twofold: firstly, as already indicated, the method recognizes areas where a certain dimension is important because it influences the planned measures and ensures that it is concrete in the adjacent states. Thus, regions where a dimension is taken into account will expand as long as the dimension is important, and then stop. Secondly, the method fulfils the requirements for choosing a world view in order to avoid a convergence in policy calculation, as mentioned above in Section 3.1. Dimensions are important where they influence policy, because the policy is the result of the planner. Secondly, they are less important in parts of the state space where they do not affect policy. Therefore, the dimensions that must be concrete and that can remain abstract can be determined for each part of the state space by comparing the optimal measures for the different states. Where the optimal measures are the same, the states can be abstract and where they differ from each other."}, {"heading": "5.2 Method", "text": "The method uses the transition function as the definition of neighboring states, so the transition function does not really seem useful, but that is no problem for this method as it can be seen below. (The algorithms are called algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms; algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms 4.algorithms; algorithms 4.algorithms; algorithms; algorithms; algorithms; algorithms; algorithms; algorithms; algorithms; algorithms 5.0 algorithms 6.0 algorith"}, {"heading": "5.3 Limitations", "text": "Political refinement can only deal with cases where a single dimension makes a difference. If two dimensions are needed in combination, they will often miss it. For example, in the case of the 3Keys problem, each key is quite far from the corresponding door, and political refinement will therefore never find the relationship between the two. In the case of the key, there seems to be no reason to pick it up, while there does not seem to be a means to unlock it at the door.Obviously, this can be fixed ad hoc by cancelling keys for their own savory.In fact, some domain formulations in the literature do just that, rewarding the agent for partially achieving the goal. However, this is not a clean solution. In fact, such domain specifications are \"scams\" by providing such references.Another problem is that political refinement does not provide for a solidification of the world view or for a modification in another way, such as as as executing progress, and the planner needs to update the plan. Political refinement is actually required in the initial state, which is in fact."}, {"heading": "6. A Proximity Measure", "text": "In this section, we describe a measure (originally in Baum & Nicholson, 1998) that realizes this concept, namely the convergence that diminishes the further the state is in the future and the less probable.7 This section extends the brief description of Baum and Nicholson (1998), and in the following section we present new methods for modifying the worldview that are directly based on the measure."}, {"heading": "6.1 Motivation", "text": "Proximity to the state is a realization of the intuitive concept of states that are close to the agent and likely to be visited, as opposed to those that are far from the agent and unlikely. It, of course, takes into account the current state of Scure in recurring planning, or the original state s0 in precursor planning, as opposed to policy-based subtleties that they completely ignore. Therefore, a planner who selects worldviews based on this proximity scale will produce solutions that are tailored to the respective scur or s0, and ignore portions of the MDP that are irrelevant or virtually irrelevant to the performance of that state, thus avoiding a compilation that would otherwise be knowledgeable. Baum and Nicholson (1998) used the word \"probability\" for this measure \"to avoid confusion with the other meanings of the word\" probability. \"Munos and Moore (1999) use the word\" influence \"for a somewhat similar measure in continuous areas."}, {"heading": "6.2 Calculation", "text": "There are therefore three differences: First, instead of starting from the reward function, it is based on the reward function that occurs in the future (after taking action), while the \"transition probability\" is based on the present before taking action. Since the sequence of action and the function on which the formula is based is reversed over time, a similar reversal must be applied to the transition probabilities. Third, an estimated future policy is applied instead of \"Prop.\" In this estimate, a stochastic policy is defined by distributing across actions a constant probability over the current probability (s) and distributing the remaining probability mass to the other actions equally."}, {"heading": "6.3 Discussion", "text": "An interesting feature of the resulting numbers is that they emphasize the absorbing and near-absorbing states somewhat more than might be expected intuitively. However, the consideration that absorbing states are generally important is a good feature, especially since normally the planner will try to minimize the likelihood of entering an absorbing state (unless it is the goal). This feature should help to keep absorbing states in mind as long as there is any chance of falling into them. (1995), for example, that in their algorithm such undesirable absorbing states along the path to the goal tend to come out of consideration as candidates for removal (due to the low probability of reaching them with current policies), and that they need to make special adjustments for them so that they are not removed from consideration. With the proximity measure that emphasizes these states, such special handling is not necessary. In contrast to this approach, Kirman does not use the probabilities of the previous steps of the policy (where steps were taken in 1994)."}, {"heading": "7. Proximity-Based Dynamic Abstraction", "text": "The proximity measurement described in the previous section serves to draw the planner's attention to areas that may be useful in the near future. First, this means that the world view should be adapted to the proximity by, if necessary, refining and coarsening it. Second, since the proximity measurement takes into account the current state, this method automatically updates the world view when the agent's circumstances change in recurring mode, i.e. when planning and execution take place simultaneously."}, {"heading": "7.1 Refinement", "text": "However, if they are abstract, this is a reason to refine them to allow for more detailed planning. Therefore, such states with high proximity are considered candidates for refinement, but high proximity is defined by a simple threshold, as shown in Algorithm 6. When refinement occurs, sometimes an anomaly occurs that has led to the policy-based refinement method, it arises from different levels of abstraction, but this is not an adjacent, more abstract state that causes the problem, but rather a refined one. If a state is refined, the values V of the new states are initially estimated from the previous value V. Typically, however, this means that some of them are overestimated and others are underestimated. If the policy is recalculated, the state with the overestimated value becomes active."}, {"heading": "7.2 Coarsening", "text": "This is especially useful if the agent finds himself in an unexpected part of the state, for example due to low probability, or if the agent has planned a path that only partially leads to the goal (perhaps up to a partial reward)."}, {"heading": "8. Results", "text": "We run the algorithm over different areas to demonstrate our approach, and the areas are divided into two large groups: the first group consists only of the area of grid navigation; this was the area where intuition was gathered and runs were carried out, but problems in this area show how well the approach works, but not its generality; and the second group consists of areas from the literature that show how well our approach generalises."}, {"heading": "8.1 Experimental Domains", "text": "The first five years after its assumption as president of the International Olympic Committee (IOC) it has moved in its role as head of the International Olympic Committee (IOC) in a role of the chief strategist from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the world leader from a role of the man from a role of the world leader from a role of the man from a role of the world leader from a woman from a role of the woman in a role of the world leader from a woman from a woman in a role of the world leader from a man from a role of the man in a role of the world leader from a woman in a role of the woman in a role of the world leader from a woman in a role of the woman in a role of the woman in a role of the world leader from a woman in a role of the woman in a role of the woman in a role of the world leader from a woman in a role of the world leader from a man in a woman in a role of the man in a role of the man in a role of the man in a role of the world leader from a woman in a role of the woman in a role of the world leader from a woman in a role of the world leader from a woman in a woman in a role of the woman in a role of the world leader of the world leader from a woman in a role of the world leader of the world leader from a woman"}, {"heading": "8.2 Direct Evaluation of Policies with Pre-cursor Deliberation", "text": "These problems are small enough that we can use an accurate algorithm to calculate the actual value functions that only conform to these approximate guidelines. As noted in Section 2.6, this can be useful because it contains fewer potentially focussed variables, but it does not exploit the full potential of our approach. 12 Table 4 (a) shows such results only for policy-based refinement - that is, with proximity-based methods (algorithms 5, 6, and 7) this is the case. For each problem, the table lists the size of the problem and the value of the initial State12. Proximity-based coarsening (algorithm 7) primarily targets regions of government space that the agent has already traversed, but with pre-cursor considerations there is no overlap."}, {"heading": "8.3 Evaluation by Simulation with Recurrent Deliberation", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "8.4 The Effect of the Discounting Factor", "text": "The Shuttlebot problem is similar to the 3Doors problem, but requires the agent to move back and forth repeatedly between two locations. It is interesting because it was not resolved at all for \u03b3 = 0.999 99 99 in pre-cursor mode heats, while it is optimally solved for \u03b3 = 0.95. Consideration was given to whether the \u03b3 = 0.999 99 99 case could behave better under simulation if the agent uses the option of planning only for the next reward and then rescheduling as soon as that reward is received. Finally, the agent could work well even if none of the measures alone would be a good solution. However, as shown in Figure 11, the agent behaves very similar to the pre-cursor case: for \u03b3 = 0.999 99 99 99, (a) only for refinement, run 1, and (b) if coarse, run 2, it would pick up the reward immediately next to s0, but no more than that."}, {"heading": "8.5 Initial Worldview", "text": "Even modified, smaller initial world views obtained by activating only the nexus step of algorithm 3 and deactivating the reward step are too big. However, deactivating both the reward and nexus steps leads to a singleton initial world view, W = {S}, which treats the entire state space S as a single (very) abstract world view state. Unfortunately, this means that the planner begins with very little indication of the direction in which he will refine and, at least initially, receives no other information on which this crucial decision is based. As a result, he does not collect any reward. In some cases, he remains in the initial state s0, in others he moves around the state space - sometimes from a distance, other times in a small loop - but he does not reach the goal or any of the goals."}, {"heading": "8.6 Worldview Size and Quality", "text": "Finally, we consider the effect of world view size and quality on the Robot4 domain, where the agent moves through a series of rooms with lights. This domain is an excellent example in which the simulated agent works well. In all runs of the robot 4-10 and Robot4-15 problems, which the agent thought for a small amount of time, then quickly moved to the target and stayed there, with only very small world views as seen in Figure 12 for the Robot4-15 problem. Four representative runs are shown, two with the two refinement methods only (runs 1 and 2) and two with all three methods (runs 3 and 4). In all four runs, the world view sizes of the world view are reasonable - consider that the whole state area contains almost half a million states, so a 1,000-state world view represents only one-fifth of one percent. However, despite this small world view size, the planner is effective."}, {"heading": "9. Discussion", "text": "This year it is more than ever before."}, {"heading": "10. Conclusions", "text": "The theory of Markov decision-making processes provides algorithms for optimal planning. However, these algorithms are insoluble in larger areas and approximate solutions are needed. Where state space is expressed in terms of dimensions, its size and the resulting computing costs are exponential in the number of dimensions. Fortunately, this also leads to a structured state space in which effective approximations are possible. Similarly, Kolobow, Mausam and Weld (2008) report on a variant of tireworld rather than on tireworld itself without providing an explanation. Our approach is based on selectively ignoring some of the dimensions in some parts of state space in order to obtain a safe solution at lower computing costs. This non-uniform abstraction is dynamically adapted as planning and (in online situations) as implementation progress, and various dimensions are ignored in different parts of state space."}, {"heading": "10.1 Future Work", "text": "One possible direction for future research would be the search for ideological initialization and modification methods that lead to smaller but useful worldviews that are likely domain-specific to expand the method to other areas, either larger or with different characteristics. For example, the factory and tire world domains are goal-oriented and predicate-based, and a belief selection or modification method based on a predicate-oriented approach could find possible ways to reach the goal and ensure that the relevant preconditions along this path are concrete. Interestingly, in the 10x10 problem, while the proximity-based methods did not keep the world view size small, they seemed to find a balance at a larger but still moderate size. Another possibility could be to coordinate the proximity-based methods or to develop self-conceived variations.For a number of points, for example phase selection, our algorithm may be used to replace hay-oriented learning as a standard or this could be standard."}], "references": [{"title": "Magnifying-lens abstraction for Markov decision processes", "author": ["L. de Alfaro", "P. Roy"], "venue": "In Proceedings of the 19th International Conference on Computer Aided Verification,", "citeRegEx": "Alfaro and Roy,? \\Q2007\\E", "shortCiteRegEx": "Alfaro and Roy", "year": 2007}, {"title": "Hierarchical solution of large Markov decision processes", "author": ["J. Barry", "L.P. Kaelbling", "T. Lozano-Prez"], "venue": "In Proceedings of the ICAPS Workshop on Planning and Scheduling in Uncertain Domains", "citeRegEx": "Barry et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barry et al\\.", "year": 2010}, {"title": "Fast approximate hierarchical solution of MDPs", "author": ["J.L. Barry"], "venue": null, "citeRegEx": "Barry,? \\Q2009\\E", "shortCiteRegEx": "Barry", "year": 2009}, {"title": "Learning to act using real-time dynamic programming", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": "Artificial Intelligence, Special Volume: Computational Research on Interaction and Agency,", "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Non-uniform Abstractions for Approximate Planning in Large Structured Stochastic Domains", "author": ["J. Baum"], "venue": "Ph.D. thesis, Clayton School of Information", "citeRegEx": "Baum,? \\Q2006\\E", "shortCiteRegEx": "Baum", "year": 2006}, {"title": "Dynamic non-uniform abstractions for approximate planning in large structured stochastic domains", "author": ["J. Baum", "A.E. Nicholson"], "venue": "Topics in Artificial Intelligence, Proceedings of the 5th Pacific Rim International Conference on Artificial Intelligence", "citeRegEx": "Baum and Nicholson,? \\Q1998\\E", "shortCiteRegEx": "Baum and Nicholson", "year": 1998}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Macro-FF: Improving AI planning with automatically learned macro-operators", "author": ["A. Botea", "M. Enzenberger", "M. Muller", "J. Schaeffer"], "venue": "Journal of Articial Intelligence Research,", "citeRegEx": "Botea et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Botea et al\\.", "year": 2005}, {"title": "Correlated action effects in decision theoretic regression", "author": ["C. Boutilier"], "venue": "Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Boutilier", "year": 1997}, {"title": "Approximating value trees in structured dynamic programming", "author": ["C. Boutilier", "R. Dearden"], "venue": "In Proceedings of the 13th International Conference on Machine Learning,", "citeRegEx": "Boutilier and Dearden,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1996}, {"title": "Exploiting structure in policy construction", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95),", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Continuous value function approximation for sequential bidding policies", "author": ["C. Boutilier", "M. Goldszmidt", "B. Sabata"], "venue": "Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation", "author": ["A.R. Cassandra", "L.P. Kaelbling", "J.A. Kurien"], "venue": "Tech. rep. TR CS-96-17,", "citeRegEx": "Cassandra et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1996}, {"title": "Exact decomposition approaches for Markov decision processes: A survey", "author": ["C. Daoui", "M. Abbad", "M. Tkiouat"], "venue": "Advances in Operations Research,", "citeRegEx": "Daoui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Daoui et al\\.", "year": 2010}, {"title": "Planning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A.E. Nicholson"], "venue": "Artificial Intelligence,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Abstraction and approximate decision theoretic planning", "author": ["R. Dearden", "C. Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "Dearden and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1997}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["T.G. Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich,? \\Q2000\\E", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "Anytime synthetic projection: Maximizing the probability of goal satisfaction", "author": ["M. Drummond", "J. Bresina"], "venue": "Proceedings of the 8th National Conference on Artificial Intelligence", "citeRegEx": "Drummond and Bresina,? \\Q1990\\E", "shortCiteRegEx": "Drummond and Bresina", "year": 1990}, {"title": "Envelope-based planning in relational MDPs", "author": ["N.H. Gardiol", "L.P. Kaelbling"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gardiol and Kaelbling,? \\Q2004\\E", "shortCiteRegEx": "Gardiol and Kaelbling", "year": 2004}, {"title": "Adaptive envelope MDPs for relational equivalence-based planning", "author": ["N.H. Gardiol", "L.P. Kaelbling"], "venue": "Tech. rep. MIT-CSAIL-TR-2008-050,", "citeRegEx": "Gardiol and Kaelbling,? \\Q2008\\E", "shortCiteRegEx": "Gardiol and Kaelbling", "year": 2008}, {"title": "Unrolling\u201d complex task models into MDPs", "author": ["R.P. Goldman", "D.J. Musliner", "M.S. Boddy", "E.H. Durfee", "J. Wu"], "venue": "In Proceedings of the 2007 AAAI Spring Symposium on Game Theoretic and Decision Theoretic Agents", "citeRegEx": "Goldman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 2007}, {"title": "Dynamic abstraction planning", "author": ["R.P. Goldman", "D.J. Musliner", "K.D. Krebsbach", "M.S. Boddy"], "venue": "Proceedings of the 14th National Conference on Artificial Intelligence and 9th Innovative Applications of Artificial Intelligence Conference", "citeRegEx": "Goldman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 1997}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "LAO*: A heuristic search algorithm that finds solutions with loops", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence,", "citeRegEx": "Hansen and Zilberstein,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 2001}, {"title": "Hierarchical solution of Markov decision processes using macro-actions", "author": ["M. Hauskrecht", "N. Meuleau", "L.P. Kaelbling", "T. Dean", "C. Boutilier"], "venue": "Proceedings of the 14th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "SPUDD: Stochastic planning using decision diagrams", "author": ["J. Hoey", "R. St.-Aubin", "A. Hu", "C. Boutilier"], "venue": "In Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "Howard,? \\Q1960\\E", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Representations and Algorithms for Large Stochastic Planning Problems", "author": ["Kim", "K.-E"], "venue": "Ph.D. thesis,", "citeRegEx": "Kim and K..E.,? \\Q2001\\E", "shortCiteRegEx": "Kim and K..E.", "year": 2001}, {"title": "Predicting Real-time Planner Performance by Domain Characterization", "author": ["J. Kirman"], "venue": "Ph.D. thesis,", "citeRegEx": "Kirman,? \\Q1994\\E", "shortCiteRegEx": "Kirman", "year": 1994}, {"title": "Regressing deterministic plans for MDP function approximation. InWorkshop on A Reality Check for Planning and Scheduling Under Uncertainty at ICAPS", "author": ["A. Kolobov", "Mausam", "D.S. Weld"], "venue": null, "citeRegEx": "Kolobov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2008}, {"title": "Macro-operators: A weak method for learning", "author": ["R. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Tireworld domain. The Fifth International Planning Competition (IPC-5) hosted at the International Conference on Automated Planning and Scheduling (ICAPS", "author": ["M. Littman", "D. Weissman", "B. Bonet"], "venue": null, "citeRegEx": "Littman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2006}, {"title": "Variable resolution discretization for high-accuracy solutions of optimal control problems", "author": ["R. Munos", "A. Moore"], "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelligence", "citeRegEx": "Munos and Moore,? \\Q1999\\E", "shortCiteRegEx": "Munos and Moore", "year": 1999}, {"title": "World modeling for the dynamic construction of real-time plans", "author": ["D.J. Musliner", "E.H. Durfee", "K.G. Shin"], "venue": "Artificial Intelligence,", "citeRegEx": "Musliner et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Musliner et al\\.", "year": 1995}, {"title": "Toward approximate planning in very large stochastic domains", "author": ["A.E. Nicholson", "L.P. Kaelbling"], "venue": "In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning,", "citeRegEx": "Nicholson and Kaelbling,? \\Q1994\\E", "shortCiteRegEx": "Nicholson and Kaelbling", "year": 1994}, {"title": "A unifying framework for temporal abstraction in stochastic processes", "author": ["R. Parr"], "venue": "In Proceedings of the Symposium on Abstraction Reformulation and Approximation", "citeRegEx": "Parr,? \\Q1998\\E", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Modified policy iteration algorithms for discounted Markov decision processes", "author": ["M.L. Puterman", "M.C. Shin"], "venue": "Management Science,", "citeRegEx": "Puterman and Shin,? \\Q1978\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "AsistO: A qualitative MDP-based recommender system for power plant operation", "author": ["A. Reyes", "L.E. Sucar", "E.F. Morales"], "venue": "Computacio\u0301n y Sistemas,", "citeRegEx": "Reyes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2009}, {"title": "Practical solution techniques for first-order MDPs", "author": ["S. Sanner", "C. Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "Sanner and Boutilier,? \\Q2009\\E", "shortCiteRegEx": "Sanner and Boutilier", "year": 2009}, {"title": "Abstract planning with unknown object quantities and properties", "author": ["S. Srivastava", "N. Immerman", "S. Zilberstein"], "venue": "In Proceedings of the Eighth Symposium on Abstraction, Reformulation and Approximation", "citeRegEx": "Srivastava et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2009}, {"title": "APRICODD: Approximate policy construction using decision diagrams", "author": ["R. St-Aubin", "J. Hoey", "C. Boutilier"], "venue": "In Proceedings of Conference on Neural Information Processing Systems,", "citeRegEx": "St.Aubin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "St.Aubin et al\\.", "year": 2000}, {"title": "Solving Large Stochastic Planning Problems using Multiple Dynamic Abstractions", "author": ["K.A. Steinkraus"], "venue": "Ph.D. thesis,", "citeRegEx": "Steinkraus,? \\Q2005\\E", "shortCiteRegEx": "Steinkraus", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "For small to medium-sized stochastic domains, the theory of Markov decision processes provides algorithms for generating the optimal plan (Bellman, 1957; Howard, 1960; Puterman & Shin, 1978).", "startOffset": 138, "endOffset": 190}, {"referenceID": 27, "context": "For small to medium-sized stochastic domains, the theory of Markov decision processes provides algorithms for generating the optimal plan (Bellman, 1957; Howard, 1960; Puterman & Shin, 1978).", "startOffset": 138, "endOffset": 190}, {"referenceID": 42, "context": "However, as the domain becomes larger, these algorithms become intractable and approximate solutions become necessary (for instance Drummond & Bresina, 1990; Dean, Kaelbling, Kirman, & Nicholson, 1995; Kim, 2001; Steinkraus, 2005).", "startOffset": 118, "endOffset": 230}, {"referenceID": 15, "context": "Our work is an extension and synthesis of two existing approaches to approximate planning: the locality-based approximation of envelope methods (Dean et al., 1995) and the structure-based approximation of uniform abstraction (Nicholson & Kaelbling, 1994; Dearden & Boutilier, 1997).", "startOffset": 144, "endOffset": 163}, {"referenceID": 4, "context": "Baum and Nicholson (1998) introduced the main concepts while full details of our algorithms and experimental results are presented in Baum\u2019s (2006) thesis.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Baum and Nicholson (1998) introduced the main concepts while full details of our algorithms and experimental results are presented in Baum\u2019s (2006) thesis.", "startOffset": 0, "endOffset": 148}, {"referenceID": 4, "context": "Baum and Nicholson (1998) introduced the main concepts while full details of our algorithms and experimental results are presented in Baum\u2019s (2006) thesis. There have been some studies of arbitrary abstraction, for instance by Bertsekas and Tsitsiklis (1996). However, these are generally theoretical and in any case they tended to treat the approximation as Markovian, which would have resulted in unacceptable performance in practice.", "startOffset": 0, "endOffset": 259}, {"referenceID": 4, "context": "Baum and Nicholson (1998) introduced the main concepts while full details of our algorithms and experimental results are presented in Baum\u2019s (2006) thesis. There have been some studies of arbitrary abstraction, for instance by Bertsekas and Tsitsiklis (1996). However, these are generally theoretical and in any case they tended to treat the approximation as Markovian, which would have resulted in unacceptable performance in practice. We improve on this by extending the planning algorithm to deal with the non-Markovian aspects of the approximation. Finally, we use a measure of locality, introduced by Baum and Nicholson (1998), that is similar to but more flexible than the influence measure of Munos and Moore (1999).", "startOffset": 0, "endOffset": 632}, {"referenceID": 4, "context": "Baum and Nicholson (1998) introduced the main concepts while full details of our algorithms and experimental results are presented in Baum\u2019s (2006) thesis. There have been some studies of arbitrary abstraction, for instance by Bertsekas and Tsitsiklis (1996). However, these are generally theoretical and in any case they tended to treat the approximation as Markovian, which would have resulted in unacceptable performance in practice. We improve on this by extending the planning algorithm to deal with the non-Markovian aspects of the approximation. Finally, we use a measure of locality, introduced by Baum and Nicholson (1998), that is similar to but more flexible than the influence measure of Munos and Moore (1999). We assume that the agent continues to improve its plan while it is acting and that planning failures are generally not fatal.", "startOffset": 0, "endOffset": 723}, {"referenceID": 6, "context": "There are well known iterative algorithms, Bellman\u2019s (1957) value iteration, Howard\u2019s (1960) policy iteration, and the modified policy iteration of Puterman and Shin (1978) for computing the optimal policy \u03c0\u2217.", "startOffset": 43, "endOffset": 60}, {"referenceID": 6, "context": "There are well known iterative algorithms, Bellman\u2019s (1957) value iteration, Howard\u2019s (1960) policy iteration, and the modified policy iteration of Puterman and Shin (1978) for computing the optimal policy \u03c0\u2217.", "startOffset": 43, "endOffset": 93}, {"referenceID": 6, "context": "There are well known iterative algorithms, Bellman\u2019s (1957) value iteration, Howard\u2019s (1960) policy iteration, and the modified policy iteration of Puterman and Shin (1978) for computing the optimal policy \u03c0\u2217.", "startOffset": 43, "endOffset": 173}, {"referenceID": 8, "context": "Dearden and Boutilier use this to obtain an exact solution (Boutilier, 1997) or an approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997).", "startOffset": 59, "endOffset": 76}, {"referenceID": 8, "context": "Dearden and Boutilier use this to obtain an exact solution (Boutilier, 1997) or an approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997). However, their abstractions are fixed throughout execution, and dimensions are also deleted from the problem in a pre-determined sequence. This makes their approach somewhat inflexible. Similarly, Nicholson and Kaelbling (1994) propose this technique for approximate planning.", "startOffset": 12, "endOffset": 383}, {"referenceID": 8, "context": "Dearden and Boutilier use this to obtain an exact solution (Boutilier, 1997) or an approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997). However, their abstractions are fixed throughout execution, and dimensions are also deleted from the problem in a pre-determined sequence. This makes their approach somewhat inflexible. Similarly, Nicholson and Kaelbling (1994) propose this technique for approximate planning. They delete dimensions from the problem based on sensitivity analysis, and refine their abstraction as execution time permits, but it is still uniform. Dietterich (2000) uses this kind of abstraction in combination with hierarchical planning to good effect: a subtask, such as \u2018Navigate to location t\u2019, can ignore irrelevant dimensions, such as location of items to be", "startOffset": 12, "endOffset": 602}, {"referenceID": 19, "context": "Gardiol and Kaelbling (2008) use it where some dimensions are relevant, but only marginally, so that ignoring them results in an approximate solution that can be improved as planning progresses.", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "Our approximation, non-uniform abstraction, replaces the state space S with W, a particular type of partition of S, as originally introduced by Baum and Nicholson (1998). We call W the worldview, so members of W are worldview states while members of S are specific states.", "startOffset": 144, "endOffset": 170}, {"referenceID": 0, "context": "A similar restriction affects de Alfaro and Roy\u2019s (2007) \u201cmagnifying-lens abstraction\u201d, with the refinement that multivalued dimensions are taken bit-by-bit and the bits interleaved, so that each level of the decision tree halves the space along a different dimension in a pre-determined order.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": "A similar restriction affects de Alfaro and Roy\u2019s (2007) \u201cmagnifying-lens abstraction\u201d, with the refinement that multivalued dimensions are taken bit-by-bit and the bits interleaved, so that each level of the decision tree halves the space along a different dimension in a pre-determined order. As they note, this would work well where these dimensions correspond to a more-or-less connected space, as in a gridworld, but it would do less well with features like the doors of our grid navigation domain. Magnifying-lens abstraction calculates upper and lower bounds to the value function, rather than a single approximation, which is an advantage for guiding abstraction selection and allows for a definite termination condition (which we lack). On the other hand, it always considers fully-concrete states in part of the algorithm, limiting its space savings to the square root of the state space, whereas our algorithm can work with a mixture of variously abstract states not necessarily including any fully concrete ones. Another related approach is variable grids used for discretisation, which can be indirectly used for some discrete domains, as Boutilier, Goldszmidt, and Sabata (1999) do, if dimensions can be reasonably approximated as continuous (for instance money).", "startOffset": 33, "endOffset": 1193}, {"referenceID": 0, "context": "A similar restriction affects de Alfaro and Roy\u2019s (2007) \u201cmagnifying-lens abstraction\u201d, with the refinement that multivalued dimensions are taken bit-by-bit and the bits interleaved, so that each level of the decision tree halves the space along a different dimension in a pre-determined order. As they note, this would work well where these dimensions correspond to a more-or-less connected space, as in a gridworld, but it would do less well with features like the doors of our grid navigation domain. Magnifying-lens abstraction calculates upper and lower bounds to the value function, rather than a single approximation, which is an advantage for guiding abstraction selection and allows for a definite termination condition (which we lack). On the other hand, it always considers fully-concrete states in part of the algorithm, limiting its space savings to the square root of the state space, whereas our algorithm can work with a mixture of variously abstract states not necessarily including any fully concrete ones. Another related approach is variable grids used for discretisation, which can be indirectly used for some discrete domains, as Boutilier, Goldszmidt, and Sabata (1999) do, if dimensions can be reasonably approximated as continuous (for instance money). Unlike our approach, variable grids are completely inapplicable to predicates and other binary or enumerated dimensions. Some, such as Reyes, Sucar, and Morales (2009), use techniques in some ways quite similar to ours for continuous MDPs, though they are quite different in other ways: they consider refinement only, not coarsening; they use sampling, rather than directly dealing with the domain model; and they use a different refinement method, where each refinement is evaluated after the fact and then either committed or rolled back.", "startOffset": 33, "endOffset": 1446}, {"referenceID": 0, "context": "A similar restriction affects de Alfaro and Roy\u2019s (2007) \u201cmagnifying-lens abstraction\u201d, with the refinement that multivalued dimensions are taken bit-by-bit and the bits interleaved, so that each level of the decision tree halves the space along a different dimension in a pre-determined order. As they note, this would work well where these dimensions correspond to a more-or-less connected space, as in a gridworld, but it would do less well with features like the doors of our grid navigation domain. Magnifying-lens abstraction calculates upper and lower bounds to the value function, rather than a single approximation, which is an advantage for guiding abstraction selection and allows for a definite termination condition (which we lack). On the other hand, it always considers fully-concrete states in part of the algorithm, limiting its space savings to the square root of the state space, whereas our algorithm can work with a mixture of variously abstract states not necessarily including any fully concrete ones. Another related approach is variable grids used for discretisation, which can be indirectly used for some discrete domains, as Boutilier, Goldszmidt, and Sabata (1999) do, if dimensions can be reasonably approximated as continuous (for instance money). Unlike our approach, variable grids are completely inapplicable to predicates and other binary or enumerated dimensions. Some, such as Reyes, Sucar, and Morales (2009), use techniques in some ways quite similar to ours for continuous MDPs, though they are quite different in other ways: they consider refinement only, not coarsening; they use sampling, rather than directly dealing with the domain model; and they use a different refinement method, where each refinement is evaluated after the fact and then either committed or rolled back. Perhaps the most similar to our approach is one of the modules of Steinkraus (2005), the ignore-state-variables module.", "startOffset": 33, "endOffset": 1903}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class.", "startOffset": 66, "endOffset": 424}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class.", "startOffset": 66, "endOffset": 569}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops.", "startOffset": 66, "endOffset": 1030}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops.", "startOffset": 66, "endOffset": 1103}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops.", "startOffset": 66, "endOffset": 1155}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops.", "startOffset": 66, "endOffset": 1193}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops.", "startOffset": 66, "endOffset": 1291}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops. Goldman, Musliner, Boddy, Durfee, and Wu (2007) reduce the state space while generating the (limited-horizon, undiscounted) MDP from a different, non-MDP representation by only including reachable states, pruning those which can be detected as being clearly and immediately poor, or inferior or equivalent to already-generated states.", "startOffset": 66, "endOffset": 1410}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops. Goldman, Musliner, Boddy, Durfee, and Wu (2007) reduce the state space while generating the (limited-horizon, undiscounted) MDP from a different, non-MDP representation by only including reachable states, pruning those which can be detected as being clearly and immediately poor, or inferior or equivalent to already-generated states. Naturally, many of these approaches can be combined. For instance, Gardiol and Kaelbling (2004, 2008) combine state space abstraction with the envelope work of Dean et al. (1995), while Steinkraus (2005) uses a modular planner with a view of combining as many approaches as may be appropriate for a given problem.", "startOffset": 66, "endOffset": 1876}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops. Goldman, Musliner, Boddy, Durfee, and Wu (2007) reduce the state space while generating the (limited-horizon, undiscounted) MDP from a different, non-MDP representation by only including reachable states, pruning those which can be detected as being clearly and immediately poor, or inferior or equivalent to already-generated states. Naturally, many of these approaches can be combined. For instance, Gardiol and Kaelbling (2004, 2008) combine state space abstraction with the envelope work of Dean et al. (1995), while Steinkraus (2005) uses a modular planner with a view of combining as many approaches as may be appropriate for a given problem.", "startOffset": 66, "endOffset": 1901}, {"referenceID": 8, "context": "For instance, the \u201cfactored MDP\u201d approach (used, for instance, by Boutilier et al., 2000, or Guestrin, Koller, Parr, & Venkataraman, 2003) is suitable for domains where parts of the state and action spaces can be grouped together so that within each group those actions or action dimensions affect the corresponding states or state dimensions but interaction between the groups is weak. St-Aubin, Hoey, and Boutilier (2000) iterate a symbolic representation in the form of algebraic decision diagrams to produce approximate solutions, while Sanner and Boutilier (2009) iterate a symbolic representation of a whole class of problems in a domain, using symbolic dynamic programming, first-order algebraic decision diagrams and linear value approximation, to pre-compute a generic solution which can then be used to quickly solve specific problems of that class. While we focus only on the state space, others approximate the action space, typically grouping actions (possibly hierarchically) into \u201cmacro actions\u201d, after Korf (1985). For instance Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) or Botea, Enzenberger, Muller, and Schaeffer (2005) take this approach, while Parr (1998) uses finite state automata for the macro actions and Srivastava, Immerman, and Zilberstein (2009) take it further by using algorithm-like plans with branches and loops. Goldman, Musliner, Boddy, Durfee, and Wu (2007) reduce the state space while generating the (limited-horizon, undiscounted) MDP from a different, non-MDP representation by only including reachable states, pruning those which can be detected as being clearly and immediately poor, or inferior or equivalent to already-generated states. Naturally, many of these approaches can be combined. For instance, Gardiol and Kaelbling (2004, 2008) combine state space abstraction with the envelope work of Dean et al. (1995), while Steinkraus (2005) uses a modular planner with a view of combining as many approaches as may be appropriate for a given problem. For more details and further approaches and variants we refer the reader to a recent survey of the field by Daoui, Abbad, and Tkiouat (2010).", "startOffset": 66, "endOffset": 2152}, {"referenceID": 15, "context": "Dean et al. (1995) call this recurrent deliberation, and use it with their locality-based approximation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 37, "context": "Given a non-uniform abstraction, the simplest way to use it for planning is to take one of the standard MDP algorithms, such as the modified policy iteration of Puterman and Shin (1978), and adapt it to the non-uniform abstraction minimally.", "startOffset": 161, "endOffset": 186}, {"referenceID": 4, "context": "The method was previously introduced by Baum and Nicholson (1998), who showed, using a small navigation domain example (the 3Doors problem of this paper), that this refinement method resulted in a good policy, though not optimal.", "startOffset": 40, "endOffset": 66}, {"referenceID": 4, "context": "Omitted here as they uninteresting, but presented by Baum (2006).", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "In this section we describe a measure (originally in Baum & Nicholson, 1998) which realises this concept, proximity \u03c4P , which decreases both as a state is further in the future and as it is less probable.7 This section extends the brief description of Baum and Nicholson (1998). In the following section we then present new worldview modification methods based directly on the measure.", "startOffset": 53, "endOffset": 279}, {"referenceID": 4, "context": "Baum and Nicholson (1998) used the word \u2018likelihood\u2019 for this measure.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Baum and Nicholson (1998) used the word \u2018likelihood\u2019 for this measure. We now prefer \u2018proximity\u2019 to avoid confusion with the other meanings of the word \u2018likelihood\u2019. Munos and Moore (1999) use the word \u2018influence\u2019 for a somewhat similar measure in continuous domains.", "startOffset": 0, "endOffset": 189}, {"referenceID": 15, "context": "Dean et al. (1995), for instance, note that in their algorithm such undesirable absorbing states along the path to the goal tend to come up as candidates for removal from consideration (due to the low probability of reaching them with the current policy), and have to make special accommodation for them so they are not removed from consideration.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Dean et al. (1995), for instance, note that in their algorithm such undesirable absorbing states along the path to the goal tend to come up as candidates for removal from consideration (due to the low probability of reaching them with the current policy), and have to make special accommodation for them so they are not removed from consideration. With the proximity measure emphasising these states, such special handling is not necessary. In contrast with this approach, Kirman (1994) uses the probabilities after Es steps, where Es is (an estimate of) the number of steps the agent will take before switching from the previous policy to the policy currently being calculated.", "startOffset": 0, "endOffset": 487}, {"referenceID": 33, "context": "8 Munos and Moore (1999) use an \u2018influence\u2019 measure on their deterministic continuous domains, which is very similar to \u03c4P .", "startOffset": 2, "endOffset": 25}, {"referenceID": 2, "context": "The remaining domains are based on domains from the literature, in particular as used by Kim (2001) and Barry (2009). As described in Section 2.", "startOffset": 104, "endOffset": 117}, {"referenceID": 2, "context": "The final domain is the tireworld domain from the 2006 ICAPS IPC competition (Littman, Weissman, & Bonet, 2006) as used by Barry (2009). In this domain, a robotic car is trying to drive from point A to point B.", "startOffset": 123, "endOffset": 136}, {"referenceID": 8, "context": "-Aubin, Hu, and Boutilier (1999) and is based on the builder domain of Dearden and Boutilier (1997) which was \u201cadapted from standard job-shop scheduling problems used to test partial-order planners\u201d.", "startOffset": 16, "endOffset": 33}, {"referenceID": 8, "context": "-Aubin, Hu, and Boutilier (1999) and is based on the builder domain of Dearden and Boutilier (1997) which was \u201cadapted from standard job-shop scheduling problems used to test partial-order planners\u201d.", "startOffset": 16, "endOffset": 100}, {"referenceID": 4, "context": "We omit the graphs here, but they are presented by Baum (2006).", "startOffset": 51, "endOffset": 63}, {"referenceID": 4, "context": "Further details of these unsuccessful runs, including the |W| behaviour, are given by Baum (2006).", "startOffset": 86, "endOffset": 98}, {"referenceID": 23, "context": "For instance, Kim (2001) obtains approximate solutions for the problems and for their larger variants, as do others who use this domain or a variant, including Hoey et al. (1999) and their originators, Dearden and Boutilier (1997).", "startOffset": 160, "endOffset": 179}, {"referenceID": 8, "context": "(1999) and their originators, Dearden and Boutilier (1997). It was the necessity of using a singleton initial worldview which understandably greatly hurt the performance.", "startOffset": 42, "endOffset": 59}, {"referenceID": 8, "context": "(1999) and their originators, Dearden and Boutilier (1997). It was the necessity of using a singleton initial worldview which understandably greatly hurt the performance. An infelicity at the worldview initialisation stage could impair the entire planning process, since the worldview was never completely discarded by the planner. The worldview-improvement algorithms could have made up for some amount of weakness in the initial worldview, but a singleton worldview was a poor starting point indeed. This is similar to an observation made by Dean et al. (1995) in their work using a reduced envelope of states (that is, a subset of the state space).", "startOffset": 42, "endOffset": 563}, {"referenceID": 7, "context": "It is interesting to compare this with the results of Sanner and Boutilier (2009) for tireworld, which they only describe in passing as \u201cextremely poorly approximated\u201d before going on to manually tweak the domain for their planner, adding the information that the locations are mutually exclusive.", "startOffset": 65, "endOffset": 82}, {"referenceID": 2, "context": "Some, such as Barry, Kaelbling, and Lozano-Prez (2010), generate the full policy, while others take advantage of the initial state, as our planner would do.", "startOffset": 14, "endOffset": 55}, {"referenceID": 4, "context": "This paper extends the preliminary work of Baum and Nicholson (1998) by modifying the worldview based on the proximity measure, both enlarging and reducing its size, and by evaluating the behaviour against a simulation.", "startOffset": 43, "endOffset": 69}, {"referenceID": 4, "context": "This paper extends the preliminary work of Baum and Nicholson (1998) by modifying the worldview based on the proximity measure, both enlarging and reducing its size, and by evaluating the behaviour against a simulation. This allows us to test the approach on larger problems, but more importantly demonstrates both the full strength of the approach and its limits in terms of the domain features that it can exploit and those that it can exploit only with adjustment or not at all. The abstraction becomes truly dynamic, reacting to changes in the agent\u2019s current state and enabling planning to be tailored to the agent\u2019s situation as it changes. As shown by the qualitative and quantitative results presented both here and by Baum (2006), the approach can be effective and efficient in calculating approximate policies to guide the agent in the simulated worlds.", "startOffset": 43, "endOffset": 739}, {"referenceID": 15, "context": "For example, as mentioned in that section, Gardiol and Kaelbling (2004, 2008) combine hierarchical state space abstraction somewhat similar to ours with the envelope work of Dean et al. (1995). Many other combinations would likely be fruitful for planning in domains which have features relevant to multiple methods.", "startOffset": 174, "endOffset": 193}, {"referenceID": 36, "context": "could be added, for instance as one based on the after-the-fact refinement criterion with roll-back of Reyes et al. (2009). On a more theoretical side, one could look for situations in which optimality can be guaranteed, as Hansen and Zilberstein (2001) do with their LAO* algorithm for the work of Dean et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 23, "context": "On a more theoretical side, one could look for situations in which optimality can be guaranteed, as Hansen and Zilberstein (2001) do with their LAO* algorithm for the work of Dean et al.", "startOffset": 100, "endOffset": 130}, {"referenceID": 15, "context": "On a more theoretical side, one could look for situations in which optimality can be guaranteed, as Hansen and Zilberstein (2001) do with their LAO* algorithm for the work of Dean et al. (1995), observing that if an admissible heuristic is used to evaluate fringe states, rather than a pragmatically chosen V (out), the algorithm can be related to heuristic search and acquires a stopping criterion with guaranteed optimality (or \u03b5-optimality).", "startOffset": 175, "endOffset": 194}], "year": 2012, "abstractText": "In a deterministic world, a planning agent can be certain of the consequences of its planned sequence of actions. Not so, however, in dynamic, stochastic domains where Markov decision processes are commonly used. Unfortunately these suffer from the \u2018curse of dimensionality\u2019: if the state space is a Cartesian product of many small sets (\u2018dimensions\u2019), planning is exponential in the number of those dimensions. Our new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space. The resulting non-uniformity has strong implications, since the approximation is no longer Markovian, requiring the use of a modified planner. We also use a spatial and temporal proximity measure, which responds to continued planning as well as movement of the agent through the state space, to dynamically adapt the abstraction as planning progresses. We present qualitative and quantitative results across a range of experimental domains showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space. We assess and analyse the features of domains which our method can exploit.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}