{"id": "1401.4597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs", "abstract": "We describe Dr.Fill, a program that solves American-style crossword puzzles. From a technical perspective, Dr.Fill works by converting crosswords to weighted CSPs, and then using a variety of novel techniques to find a solution. These techniques include generally applicable heuristics for variable and value selection, a variant of limited discrepancy search, and postprocessing and partitioning ideas. Branch and bound is not used, as it was incompatible with postprocessing and was determined experimentally to be of little practical value. Dr.Fillls performance on crosswords from the American Crossword Puzzle Tournament suggests that it ranks among the top fifty or so crossword solvers in the world.", "histories": [["v1", "Sat, 18 Jan 2014 21:05:30 GMT  (528kb)", "http://arxiv.org/abs/1401.4597v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["matthew l ginsberg"], "accepted": false, "id": "1401.4597"}, "pdf": {"name": "1401.4597.pdf", "metadata": {"source": "CRF", "title": "Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs", "authors": ["Matthew L. Ginsberg"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In recent years, interest in solving problems in the real world has increased."}, {"heading": "2. Preliminaries", "text": "In this section we give a brief overview of satisfaction with constraints, crossword puzzles and the relationship between the two."}, {"heading": "2.1 Constraint Satisfaction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2.2 Crosswords", "text": "Since the introduction of the first \"word cross\" in the Sunday New York World nearly a century ago (December 21, 1913), crossword puzzles have become one of the world's most popular intellectual pastimes. Will Shortz, editor of the crossword for The New York Times, estimates that about five million people solve the puzzle every day, including syndications.2"}, {"heading": "2.2.1 Features of Crosswords", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2.3 Crossword Puzzles as SWCSPs", "text": "In fact, the fact is that most of them are able to put themselves at the top of society, in the way that they have put themselves at the top of society: in the way that they have put themselves at the top of society, in the way that they are able to assert themselves, and in the way that they are pushed to the top of society, in the way that they see themselves able to take power, in the way that they have moved to the top of society, in the way that they are able to change and change the world, in the way that they are able to change the world. \""}, {"heading": "2.4 Data Resources Used by Dr.Fill", "text": "One of the most important resources available to Dr.Fill is access to a variety of databases composed of online information. We briefly describe each of the data sources here; a summary can be found in Table 1. The table also contains information on the size of the analog data source used by Littman's Proverb crossword solving program (Littman et al., 2002)."}, {"heading": "2.4.1 Puzzles", "text": "Dr.Fill has access to a library of over 47,000 published crossword puzzles, including virtually all major published sources, including the New York Times, the (now defunct) New York Sun, the Los Angeles Times, USA Today, the Washington Post, and many others. It contains most of the puzzles since the early 1990s. In total, these puzzles provide a database of just over 3.8 million clues, about half of which are unique, contrasted with the corresponding database in Proverb, which contains about 5,000 puzzles and 250,000 unique clues.The clue database is available at http: / / www.otsys.com / clue, a nonprofit clue database used by many crossword constructors. The underlying data is compressed, but the source code is also available and should allow interested parties to decompress the data in question."}, {"heading": "2.4.2 Dictionaries", "text": "As with Proverb, Dr.Fill uses two dictionaries. A small dictionary is supposed to contain \"ordinary\" words, and a larger dictionary is supposed to contain \"everything.\" The larger dictionary is a fusion of many sources, including Moby7 and other online dictionaries, Wikipedia titles, all the words ever used in crossword puzzles, and so on. The small dictionary is the \"basic English\" dictionary supplied with Crossword Compiler, an automated tool that can be used to help build crossword puzzles.The large dictionary is much larger. Each entry in the large dictionary is also marked with a score that is supposed to reflect its crossword \"merit.\" Some words are generally considered well filled, while others are bad. As an example, BUZZ LIGHTYEAR is the excellent filling. It is \"vivid\" and has positive connotations. The letters are highly interesting (Scrable) and the combination of letters is highly acceptable."}, {"heading": "2.4.3 Grammatical and Synonym Information", "text": "From the data provided by the WordNet project (Fellbaum, 1998; Miller, 1995), grammatical information is collected, including a list of 154,000 words along with language and root components (e.g. WALKED has WALK as its root), Proverb also cites WordNet as its source, and an online thesaurus has been turned into a list of 1.2 million synonyms."}, {"heading": "2.4.4 Wikipedia", "text": "Dr. Fill uses a list of all titles in Wikipedia entries as a source of useful names and phrases, and uses a list of all consecutive word pairs in Wikipedia to help develop phrases and fill in blank clues. There are approximately 8.5 million Wikipedia titles, and Wikipedia itself contains 77 million unique word pairs."}, {"heading": "3. Heuristics", "text": "We assume that this propagation leads to failure if we have discovered a problem that we cannot solve. (...) We assume that most csps solve a problem if we solve a problem that we cannot solve. (...) We assume that we seek a partial solution for a partial solution. (...) We assume that a partial solution for a partial solution is S: 1, if S: 1, if S: 1, 6, 6, 6, 7, 8, if Q: 3 for each d: Dv (...) 4, S: 1, 5 C: 1, 6, 6, 7, then Q: 7, 8, if Q: 9, back Q: 9, back Q: We select an unallocated variable for each possible value, and try the variable for the given value and propagate in some unspecified way. (...) We assume that this propagation leads to a misconception."}, {"heading": "3.1 Value Selection", "text": "This kind of reasoning will be valid even if we replace procedure 3.4 with other algorithms that are more effective in practice; it is always advantageous to design the search in such a way that the final solution is found earlier than it is applicable in relation to the individual elements. There are a multitude of elements that we really need to perform a function (v, n) that returns the next element to the real world. In any case, we will increase the value of the individual elements, while Macho-Gonz\u00e1lez (2005) takes a similar approach in her work on \"open.\" As in work on open conservation, we can deal with the fact that we are dealing with the fact that we are not being confronted with others."}, {"heading": "3.2 Variable Selection", "text": "In the oiaePnlrrrteeu nvo edm rf\u00fc ide eaePnln nvo rf\u00fc ide eaePnlrln ni rde eaePnlrln, nlrf\u00fc die sda rf\u00fc ide eaePnln nvo nlrf\u00fc ide eaePnlrln nvo rf\u00fc ide eaePnlrrrln ni der eaeePnlrrrrrrre\u00fcb zu.nreD \"iDe eSe\" n \"iSe nlrf\u00fc die eeirf\u00fc ide eaeaePnlrln\" s tis, nn os os rf\u00fc ide ePnlrlrln nr, nn so iwr, \"eos os os f\u00fc f\u00fc,\" eos ios ios, \"ios ios rrrrf\u00fc,\" eos \"ios,\" ios \"ios rrrrf\u00fc,\" ios \"ios\" ios rrf\u00fc."}, {"heading": "4. Limited Discrepancy Search", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}, {"heading": "5. Dr.Fill as a Crossword Solver", "text": "In this case, it is only a matter of time before it happens, until it happens."}, {"heading": "6. Postprocessing", "text": "An examination of Dr.Fill's finished puzzles based on the algorithms presented so far shows many cases where a single letter is wrong, and the problem lies in the search, not in heuristics. In other words, replacing the given letter with the \"right\" one reduces the total cost of filling the puzzle. This would probably have been found with a larger discrepancy limit, but has not been discovered in practice. This suggests that Dr.Fill would benefit from some kind of post-processing. The simplest approach is simply to remove each word from the filling and replace it with the best word for the slot in question."}, {"heading": "6.1 Formalization and Algorithmic Integration", "text": "We can formalize this process easily as follows: Method 6.1 Given a CSP-C and an optimal solution B to calculate post (C, B), the result of trying to improve B with post-processing may look like this: 1 Amendment \u2190 true 2, while Amendment 3 Amendment \u2190 false 4 for each v \u00b2 CV 5 B \u00b2 \u2190 B 6 yields the value of v in line 6. We then solve the puzzle (line 7) anew (line 7) so that if there is a better choice for that word in isolation, it is found. If this leads to an improvement, we put a flag in line 10 and repeat the entire process. Note that we only delete one word at a time, as we always start with the currently best solution in line 5.As with AC-3, Method 6.1 can be slightly improved by examining a variation in a certain way that we spend only one change."}, {"heading": "6.2 Interaction With Branch and Bound", "text": "Suppose that our original method 4.1 first produces a solution B1 and then produces an improvement B2, with c (B2) < c (B1). Suppose that post-processing also improves both solutions comparably, so that c (post (B2)) < c (post) < c (post (B1))). And finally, suppose that post-processing significantly improves the solutions, so much so that we can actually find a better solution, but only after post-processing. < c (B2). We are now in danger of missing out on B2 as it is truncated by the test in line 1 of procedural 6.3. B2 will allow us to find a better solution, but only after post-processing. If we prepare B2 early, we will never postpone it, and the improvement will not be found until a greater discrepancy limit is reached."}, {"heading": "6.3 Results", "text": "The results of the procedure 6.4 appear in Table 2. Dr.Fill's score improves to 11,210, which would have earned him a tie for 43rd place at the 2010 Tournament. However, there are certainly real problems where branch-and-boing makes sense, such as the use of MendelSoft to solve problems with cattle pedigree trees (Sanchez, de Givry, & Schiex, 2008)."}, {"heading": "7. AND/OR Search", "text": "There is another algorithmic improvement that applies directly to Dr.Fill because it needs to integrate itself appropriately with integration. As we observed Dr.Fill complete puzzles, there have been many cases where there would be enough of the puzzle that the remaining problem would be divided into two distinct sub-problems, and the search would often oscillate between these two sub-problems that could clearly introduce inefficiencies. This general observation was made by many others and probably comes from Freuder and Quinn (1985), who called the variables in independent sub-problems stable propositions. McAllester (1993) cites a technique that provides for a polynomial space of aggressive tracing when it solves disjunctural sub-problems in time that is the sum of the sub-problems independent of the sub-problems. Most recently, Marinescu and Dechter (2009) have specifically examined this notion in the context of propagation, using the structure of the associated search spaces as and / or graphs."}, {"heading": "8. Related and Future Work", "text": "There is a great deal of work on wcsps, for which there are reasonably precise heuristics, and the development and processing of results in the academic literature, and we will not repeat any particular element of this work here. What distinguishes our contribution is the fact that we were driven by the results of a naturally occurring problem: that solving crossword errors has led us to the following specific innovations in relation to previous work: \u2022 Developing a value selection heuristic way that compares the difference between the projected cost of assigning a value to both the currently selected variables and to all the variables with which these variables share a limitation \u2022 Developing a variable selection heuristic, which compares the difference between the projected cost and the second best values values values values values of the values, and the branches for which this difference is maximized. \u2022 A modification of the limited discrepancy search, which seems to work well, for weighted domains that do not work."}, {"heading": "Acknowledgments", "text": "I would like to thank my colleagues at On Time Systems for their useful technical advice and support, and also the crossword solvers and designers, especially Will Shortz, for their warm support over the years. Daphne Koller, Rich Korf, Michael Littman, Thomas Schiex, Bart Selman, and the anonymous reviewers of this paper provided me with valuable comments on earlier drafts, making the work itself much stronger. The work described in this paper relates to certain pending and issued U.S. patent applications, and the publication of these ideas is not intended to convey a license to use patented information or processes. On Time Systems is generally licensed royalty-free for-of-use purposes."}], "references": [{"title": "Semiring-based CSPs and valued CSPs: Frameworks, properties, and comparison", "author": ["S. Bistarelli", "U. Montanari", "F. Rossi", "T. Schiex", "G. Verfaillie", "H. Fargier"], "venue": null, "citeRegEx": "Bistarelli et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bistarelli et al\\.", "year": 1999}, {"title": "Boosting systematic search by weighting constraints", "author": ["F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais"], "venue": "In Proceedings of ECAI-2004,", "citeRegEx": "Boussemart et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boussemart et al\\.", "year": 2004}, {"title": "Soft arc consistency revisited", "author": ["M. Cooper", "S. de Givry", "M. Sanchez", "T. Schiex", "M. Zytnicki", "T. Werner"], "venue": "Artificial Intelligence,", "citeRegEx": "Cooper et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cooper et al\\.", "year": 2010}, {"title": "A truth maintenance system", "author": ["J. Doyle"], "venue": "Artificial Intelligence,", "citeRegEx": "Doyle,? \\Q1979\\E", "shortCiteRegEx": "Doyle", "year": 1979}, {"title": "WebCrow: a WEB-based system for CROssWord solving", "author": ["M. Ernandes", "G. Angelini", "M. Gori"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence,", "citeRegEx": "Ernandes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernandes et al\\.", "year": 2005}, {"title": "Open constraint programming", "author": ["B. Faltings", "S. Macho-Gonzalez"], "venue": "Artificial Intelligence,", "citeRegEx": "Faltings and Macho.Gonzalez,? \\Q2005\\E", "shortCiteRegEx": "Faltings and Macho.Gonzalez", "year": 2005}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Building Watson: An overview of the DeepQA poject", "author": ["D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A.A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J. Prager", "N. Schlaefer", "C. Welty"], "venue": "AI Magazine,", "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Taking advantage of stable sets of variables in constraint satisfaction problems", "author": ["E.C. Freuder", "M.J. Quinn"], "venue": "In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Freuder and Quinn,? \\Q1985\\E", "shortCiteRegEx": "Freuder and Quinn", "year": 1985}, {"title": "GIB: Steps toward an expert-level bridge-playing program", "author": ["M.L. Ginsberg"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ginsberg,? \\Q2001\\E", "shortCiteRegEx": "Ginsberg", "year": 2001}, {"title": "Search lessons learned from crossword puzzles", "author": ["M.L. Ginsberg", "M. Frank", "M.P. Halpin", "M.C. Torrance"], "venue": "In Proceedings of the Eighth National Conference on Artificial Intelligence,", "citeRegEx": "Ginsberg et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Ginsberg et al\\.", "year": 1990}, {"title": "Existential arc consistency: Getting closer to full arc consistency in weighted CSPs", "author": ["S.D. Givry", "M. Zytnicki"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Givry and Zytnicki,? \\Q2005\\E", "shortCiteRegEx": "Givry and Zytnicki", "year": 2005}, {"title": "Nonsystematic Backtracking Search", "author": ["W.D. Harvey"], "venue": "Ph.D. thesis,", "citeRegEx": "Harvey,? \\Q1995\\E", "shortCiteRegEx": "Harvey", "year": 1995}, {"title": "Limited discrepancy search", "author": ["W.D. Harvey", "M.L. Ginsberg"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Harvey and Ginsberg,? \\Q1995\\E", "shortCiteRegEx": "Harvey and Ginsberg", "year": 1995}, {"title": "Squeaky wheel optimization", "author": ["D.E. Joslin", "D.P. Clements"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Joslin and Clements,? \\Q1999\\E", "shortCiteRegEx": "Joslin and Clements", "year": 1999}, {"title": "Improved limited discrepancy search", "author": ["R.E. Korf"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1996\\E", "shortCiteRegEx": "Korf", "year": 1996}, {"title": "On the dual representation of non-binary semiring-based CSPs", "author": ["J. Larrosa", "R. Dechter"], "venue": "In Proceedings SOFT-2000", "citeRegEx": "Larrosa and Dechter,? \\Q2000\\E", "shortCiteRegEx": "Larrosa and Dechter", "year": 2000}, {"title": "Solving weighted CSP by maintaining arc consistency", "author": ["J. Larrosa", "T. Schiex"], "venue": "Artificial Intelligence,", "citeRegEx": "Larrosa and Schiex,? \\Q2004\\E", "shortCiteRegEx": "Larrosa and Schiex", "year": 2004}, {"title": "Reasoning from last conflict(s) in constraint programming", "author": ["C. Lecoutre", "L. S\u00e4\u0131s", "S. Tabary", "V. Vidal"], "venue": "Artificial Intelligence,", "citeRegEx": "Lecoutre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lecoutre et al\\.", "year": 2009}, {"title": "A probabilistic approach to solving crossword puzzles", "author": ["M.L. Littman", "G.A. Keim", "N. Shzaeer"], "venue": "Artificial Intelligence,", "citeRegEx": "Littman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "Consistency in networks of relations", "author": ["A.K. Mackworth"], "venue": "Artificial Intelligence,", "citeRegEx": "Mackworth,? \\Q1977\\E", "shortCiteRegEx": "Mackworth", "year": 1977}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schuetze"], "venue": null, "citeRegEx": "Manning and Schuetze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Schuetze", "year": 1999}, {"title": "AND/OR branch-and-bound search for combinatorial optimization in graphical models", "author": ["R. Marinescu", "R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Marinescu and Dechter,? \\Q2009\\E", "shortCiteRegEx": "Marinescu and Dechter", "year": 2009}, {"title": "Partial order backtracking", "author": ["D.A. McAllester"], "venue": "Unpublished technical report,", "citeRegEx": "McAllester,? \\Q1993\\E", "shortCiteRegEx": "McAllester", "year": 1993}, {"title": "WordNet: A lexical database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller,? \\Q1995\\E", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Mendelian error detection in complex pedigrees using weighted constraint satisfaction techniques", "author": ["M. Sanchez", "S. de Givry", "T. Schiex"], "venue": null, "citeRegEx": "Sanchez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sanchez et al\\.", "year": 2008}, {"title": "Man versus machine for the world checkers championship", "author": ["J. Schaeffer", "N. Treloar", "P. Lu", "R. Lake"], "venue": "AI Magazine,", "citeRegEx": "Schaeffer et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Schaeffer et al\\.", "year": 1993}, {"title": "Total-order multi-agent task-network planning for contract bridge", "author": ["S.J. Smith", "D.S. Nau", "T. Throop"], "venue": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence,", "citeRegEx": "Smith et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1996}, {"title": "Introduction to dual decomposition for inference", "author": ["D. Sontag", "A. Globerson", "T. Jaakkola"], "venue": "Optimization for Machine Learning,", "citeRegEx": "Sontag et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2011}, {"title": "Bounds arc consistency for weighted CSPs", "author": ["M. Zytnicki", "C. Gaspin", "S. de Givry", "T. Schiex"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zytnicki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zytnicki et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "Section 7 describes the utility of splitting a crossword into smaller problems when the associated constraint graph disconnects, an idea dating back to work of Freuder and Quinn (1985) but somewhat different in the setting provided by lds.", "startOffset": 160, "endOffset": 185}, {"referenceID": 20, "context": "Iterating this idea recursively until quiescence (Mackworth, 1977) leads to the well known AC-3 algorithm, which preserves arc consistency as the csp is solved.", "startOffset": 49, "endOffset": 66}, {"referenceID": 16, "context": "And in fact, Larrosa and Dechter (2000) have shown that all weighted csps can be recast similarly, into a form with only hard binary constraints and soft unary constraints.", "startOffset": 13, "endOffset": 40}, {"referenceID": 9, "context": "As with so many other automated game players (Campbell, Hoane, & Hsu, 2002; Ginsberg, 2001; Schaeffer, Treloar, Lu, & Lake, 1993), we will rely on search to replace understanding.", "startOffset": 45, "endOffset": 129}, {"referenceID": 13, "context": "This general idea underlies Joslin and Clements\u2019 (1999) \u201csqueaky wheel optimization\u201d and virtually every more recent variable selection heuristic, such as Boussemart et.", "startOffset": 28, "endOffset": 56}, {"referenceID": 13, "context": "This general idea underlies Joslin and Clements\u2019 (1999) \u201csqueaky wheel optimization\u201d and virtually every more recent variable selection heuristic, such as Boussemart et. al\u2019s (2004) notion of constraint weighting, and the dom/wdeg heuristic (Lecoutre, S\u00e4\u0131s, Tabary, & Vidal, 2009, and others).", "startOffset": 28, "endOffset": 182}, {"referenceID": 19, "context": "The table also includes information on the size of the analogous data source used by Littman\u2019s crossword solving program Proverb (Littman et al., 2002).", "startOffset": 129, "endOffset": 151}, {"referenceID": 6, "context": "Grammatical information is collected from the data provided as part of the WordNet project (Fellbaum, 1998; Miller, 1995).", "startOffset": 91, "endOffset": 121}, {"referenceID": 24, "context": "Grammatical information is collected from the data provided as part of the WordNet project (Fellbaum, 1998; Miller, 1995).", "startOffset": 91, "endOffset": 121}, {"referenceID": 5, "context": "Faltings and Macho-Gonzalez (2005) take a similar approach in their work on \u201copen\u201d constraint programming.", "startOffset": 0, "endOffset": 35}, {"referenceID": 9, "context": "This overall search-based approach underlies virtually all of the best computer game players (Campbell et al., 2002; Ginsberg, 2001; Schaeffer et al., 1993) and search-based algorithms have easily outperformed their knowledge-based counterparts (Smith, Nau, & Throop, 1996, for example) in games where direct comparisons can be made.", "startOffset": 93, "endOffset": 156}, {"referenceID": 26, "context": "This overall search-based approach underlies virtually all of the best computer game players (Campbell et al., 2002; Ginsberg, 2001; Schaeffer et al., 1993) and search-based algorithms have easily outperformed their knowledge-based counterparts (Smith, Nau, & Throop, 1996, for example) in games where direct comparisons can be made.", "startOffset": 93, "endOffset": 156}, {"referenceID": 6, "context": "The part of speech analysis is based on the WordNet dictionary (Fellbaum, 1998; Miller, 1995), which is then used to search for parse patterns in the clue database.", "startOffset": 63, "endOffset": 93}, {"referenceID": 24, "context": "The part of speech analysis is based on the WordNet dictionary (Fellbaum, 1998; Miller, 1995), which is then used to search for parse patterns in the clue database.", "startOffset": 63, "endOffset": 93}, {"referenceID": 19, "context": "Proverb has some thirty individual scoring modules (Littman et al., 2002), although Littman has suggested (personal communication) that most of the value comes from modules that are analogous to those used by Dr.", "startOffset": 51, "endOffset": 73}, {"referenceID": 11, "context": "The reason is that this solving procedure suffers from what Harvey (1995) has called the \u201cearly mistakes\u201d problem.", "startOffset": 60, "endOffset": 74}, {"referenceID": 15, "context": "Korf (1996) presents an algorithmic improvement that addresses this issue to some extent.", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "This general observation has been made by many others, and probably originates with Freuder and Quinn (1985), who called the variables in independent subproblems stable sets.", "startOffset": 84, "endOffset": 109}, {"referenceID": 8, "context": "This general observation has been made by many others, and probably originates with Freuder and Quinn (1985), who called the variables in independent subproblems stable sets. McAllester (1993) calls a solution technique a polynomial space aggressive backtracking procedure if it solves disjoint subproblems in time that is the sum of the times needed for the subproblems independently.", "startOffset": 84, "endOffset": 193}, {"referenceID": 8, "context": "This general observation has been made by many others, and probably originates with Freuder and Quinn (1985), who called the variables in independent subproblems stable sets. McAllester (1993) calls a solution technique a polynomial space aggressive backtracking procedure if it solves disjoint subproblems in time that is the sum of the times needed for the subproblems independently. Most recently, Marinescu and Dechter (2009) explore this notion in the context of constraint propagation specifically, exploiting the structure of the associated search spaces as and/or graphs.", "startOffset": 84, "endOffset": 430}, {"referenceID": 19, "context": "Fill are Proverb (Littman et al., 2002), the crossword solver developed by Littman et.", "startOffset": 17, "endOffset": 39}, {"referenceID": 7, "context": "al in 1999, and Watson (Ferrucci et al., 2010), the Jeopardy-playing robot developed by ibm in 2011.", "startOffset": 23, "endOffset": 46}], "year": 2011, "abstractText": "We describe Dr.Fill, a program that solves American-style crossword puzzles. From a technical perspective, Dr.Fill works by converting crosswords to weighted csps, and then using a variety of novel techniques to find a solution. These techniques include generally applicable heuristics for variable and value selection, a variant of limited discrepancy search, and postprocessing and partitioning ideas. Branch and bound is not used, as it was incompatible with postprocessing and was determined experimentally to be of little practical value. Dr.Fill\u2019s performance on crosswords from the American Crossword Puzzle Tournament suggests that it ranks among the top fifty or so crossword solvers in the world.", "creator": "TeX"}}}