{"id": "1401.4600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams", "abstract": "We focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others models, which changes as the agent acts and observes in the multiagent setting.", "histories": [["v1", "Sat, 18 Jan 2014 21:09:03 GMT  (1646kb)", "http://arxiv.org/abs/1401.4600v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yifeng zeng", "prashant doshi"], "accepted": false, "id": "1401.4600"}, "pdf": {"name": "1401.4600.pdf", "metadata": {"source": "CRF", "title": "Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams", "authors": ["Yifeng Zeng", "Prashant Doshi"], "emails": ["YFZENG@CS.AAU.DK", "PDOSHI@CS.UGA.EDU"], "sections": [{"heading": null, "text": "This problem has previously been formalized by several frameworks, one of which is the Interactive Dynamic Influence Diagram (I-DID), which generalizes the known Influence Diagram to the multi-agent situation. I-DIDs are graphical models and can be used to calculate an agent's policy based on his belief in the physical state and the models of other agents that change when the agent acts and observes in the multi-agent setting. Predictably, the solution of I-DIDs is mathematically difficult, primarily due to the large space of candidate models attributed to the other agents, and its exponential growth over time. We present two methods for reducing the size of the model space and containing its exponential growth. Both methods involve aggregating individual models into equivalence classes. Our first method collectively groups behavioral models and selects only those models for updating that approximate to the other model."}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to survive on their own, without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, and in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process."}, {"heading": "2. Related Work", "text": "In fact, it is so that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "3. Background", "text": "We briefly review the interactive influence diagrams (I-ID) for interactions with two agents, followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009).Both formalities allow to model the other agent and to use this information in the decision-making of the agent in question.We illustrate the formalities and our approaches in the context of the multi-agent tiger problem (Gmytrasiewicz & Doshi, 2005) - a generalization of the well-known tiger problem with two agents (Kaelbling et al., 1998).In this problem, two agents, i and j, stand in front of two closed doors, one of which hides a tiger while the other hides a pot of gold. An agent is rewarded for opening the door that hides the gold but is punished for opening the door that leads to the tiger. Each agent can open the left door (action referred to by OL), open the right door (L), (action) or (right door)."}, {"heading": "3.1 Interactive Dynamic Influence Diagrams", "text": "The question of whether this is a model that involves a new type of node called a model node is the hexagonal node that models the reward function of the agent, Ri. In addition to these nodes, it is the strategy level that allows the nested modeling of i by the other agent. Agent j's level is one less than that of i, which is consistent with previous hierarchical modeling in game theory (Aumann, 1999a; Brandenburger & Dekel, 1993) and decision theory (Gmysiewicz, 2005). Agent j's level is one less than that of i, which is consistent with previous hierarchical modeling in game theory."}, {"heading": "3.2 Behavioral Equivalence and Model Solution", "text": "Although the space of possible models is very large, not all models of agent i need to be considered in the model node. As we have already mentioned, models that are BE (Rathnasabapathy et al., 2006; Pynadath & Marsella, 2007) can be truncated and a single representative model can be considered, because the solution of the subject agent's I-DID is influenced by the predicted behavior of the other agent; therefore, we need not distinguish between behavioral equivalent models. We define BE more formally below: Definition 1 (Behavioral equivalence) Two models, mj, l \u2212 1 and m \u2032 j, l \u2212 1, of the other agent, j, are behavioral equivalent when OPT (mj, l \u2212 1) = OPT (m \u2212 1), where OPT (\u00b7) is the solution of the model that forms the reasoning."}, {"heading": "4. Discriminative Model Updates", "text": "Solving I-DIDs is mathematically difficult, not only because of the large space and complexity of the models attributed to j, but also because of the exponential growth of the candidate models of j over time. This growth leads to a disproportionate increase in the interactive state space over time. We begin with the introduction of a series of models that are, in a certain sense, minimal, and describe a method for generating this set. The minimum amount corresponds to one of the concepts of a minimum mental model space as described by Pynadath and Marsella (2007). We assume that the models of the other agent differ only in their beliefs and that the frame of the other agent is known."}, {"heading": "4.1 Behaviorally Minimal Model Set", "text": "In fact, it is true that it is a purely mental game, in which it is a matter of saving the world. (...) In fact, it is a matter of the world being able to save the world. (...) In fact, it is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world. (...) It is a matter of the world being able to save the world."}, {"heading": "4.2 Discrimination Using Policy Graphs", "text": "A simple way to get M-j, l-1 exactly at any point in time is to first determine the BE groups of models, which requires solving the I-DIDs or DIDs that represent the models, then selecting a representative model from each BE group that is included in M-j, l-1, and truncating all others that have the same solution as the representative."}, {"heading": "4.2.1 APPROACH", "text": "In view of the way in which we apply them, we believe that it is a way in which the individual models will in time lead to updated models that are identical to those of other existing models at the time in which they are not selected. Consequently, the resulting model is not selected at the time in which it will differ from other models. Consequently, the resulting model is at the time in which the individual models in Mtj, l \u2212 1. Solutions to DIDs or I-DIDs are political trees that are pulled up from the bottom in order to obtain a political graph."}, {"heading": "4.2.2 APPROXIMATION", "text": "One way to do this is by arbitrarily selecting K-models of j, so that K-M0j, l-1 |. Solving the models leads to K-strategy trees, which, as shown in Fig. 12, can be combined into a political diagram. This political diagram is used to distinguish between the model updates. Note that the approach becomes accurate when the optimal solution of each model in M0j, l-1 is identical to that of one of the K models. Since the K models are selected randomly, this assumption is implausible and the approach is likely to result in a significant loss of optimism mediated by K. We propose a simple but effective refinement that mitigates the loss. Let's remember that models whose beliefs are spatially close together are likely BE (Rathnasabapathy et al., 2006)."}, {"heading": "4.3 Transfer of Probability Mass", "text": "Note that one consequence of not updating models with an action-observation combination is that the probability mass that would have been assigned to the updated model in the model node at t + 1 is lost; this probability mass could cause an error in the optimality of the solution. We did not perform the update because a model assigned to the potentially updated model is already assigned to the model node at t + 1. We could avoid the error by transferring the probability mass that would have been assigned to the updated model at t + 1 to the BE model. As we have already mentioned, the CPT of the model [M t + 1j, l \u2212 1] implements the ability of the model at t + 1 j, l \u2212 1, as values the various models assigned to the agent j at t + 1, to the BE model. The CPT of the model [M t + 1j, l \u2212 1] implements the ability of the model at t = 1, tj, j = 1 \u2212 b \u2212 1, j \u2212 b \u2212 1 \u2212 b \u2212 1, \u2212 b \u2212 1 \u2212 b \u2212 b \u2212 b \u2212 b \u2212 1."}, {"heading": "4.4 Algorithm", "text": "We present the discriminatory update-based algorithm for solving a level l \u2265 1 I-DID (as well as a level 0 DID) in Fig. 13. The algorithm differs from the exact approach in the expansion phase (Fig. 10). In addition to two time intersection levels l I-DID and horizon T, the algorithm takes as input the number of random models K to be solved first and the distance. Following Section 4.2, we start with the random selection of K models to be solved (lines 2-5). For each of the remaining models, we identify one of the K models whose conviction is closest in space (randomly broken bonds). If the proximity is within it, the model is not solved - instead, the previously fulfilled solution is assigned to the corresponding action node of the model."}, {"heading": "4.5 Computational Savings and Prediction Error Bound", "text": "The primary complexity of the solution of I-DIDs is due to the large number of models that have to be solved via T-time steps. At some point, there may be | M0j, l-1 | (| Aj-j |) t many models of the other agent j, where | M0j, l-1 | the number of models considered at each level adds further to the complexity, since solutions for each model at level l-1 require the solution of the lower level l-2 models, and thus recursively up to level 0. In an N + 1 agent advocating for an agent at each level, the solution of an I-DID model at level l-2 is required, namely on recursively up to level 0."}, {"heading": "5. Grouping Models Using Action Equivalence", "text": "The grouping of BE models can significantly reduce the given space of models of other actors in the model node without loss of optimism. We can further compact the space of models in the model node by observing that behavior-specific models can prescribe identical actions in a single time step. We can then group these models into a single equivalence class. Compared to BE, the equivalence class includes those models whose prescribed actions are the same for that particular time step, and we call them shareholder equivalence. We formally define them next."}, {"heading": "5.1 Action Equivalence", "text": "For a general case, we define action equivalence (AE) below: Definition p p (action equivalence) p p p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence)) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence)) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence) p (action equivalence)) p (action equivalence (action equivalence)) p (action equivalence) p (action equivalence (action equivalence) p) p (action equivalence (action equivalence)) p (action equivalence (action equivalence)) p (action equivalence) p (action equivalence (action equivalence)) p (action equivalence) p (action equivalence (action equivalence)) p (action equivalence (action equivalence)) p (action equivalence ("}, {"heading": "5.3 Algorithm", "text": "In Fig. 16, we provide an algorithm to use AE to solve a level l \u2265 1 I-DID (as well as a level 0 DID). The algorithm begins by selectively solving subordinate I-DID or DID models at t = 0, resulting in a series of policy trees (line 2). We then create the policy graph by merging the policy trees as mentioned in lines 1-13 of Fig. 13. The algorithm is different from Fig. 13 in the expansion phase. Specifically, we start grouping AE models in the initial model node. This changes the value of the initial mod node with the AE classes (lines 3-9). Subsequently, updated models that are AE are aggregated in all time steps, and the CPTs of the mod nodes are revised to reflect the limitation regarding AE classes (lines 13-24)."}, {"heading": "5.4 Computational Savings", "text": "As already mentioned, the complexity of the exact solution of one level l I-DID is due in part to the solution of the subordinate models of the other actor and, in view of the solutions, to the exponentially growing space of models. In particular, at a certain point in time, there may be at most | M0j, l \u2212 1 | (| Aj | | HJ |) t many models in which M0j, l \u2212 1 is the set of initial models of the other actor. Whereas K \u0445 | M 0 j, l \u2212 1 | models are solved, the model space is limited to at most | Aj | different classes taking into account AU. Thus, the cardinality of the interactive state space in I-DID is limited by | S | | Aj | elements at any given time. This is a significant reduction in the size of the state space. In addition, we incur the computational costs for the merging of the political trees, which O ((((((| HJ \u2212 T \u2212 1) M0D, M0D, M0D, 1) \u2212 1), which is applied by all of the 2 levels \u2212 \u2212 \u2212 \u2212 \u2212 \u2212) \u2212 \u2212 \u2212 \u2212."}, {"heading": "6. Empirical Results", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6.1 Improved Efficiency Due to BE", "text": "We report on the performance of the exact methods (Exact-BE and Exact) when used to solve both Level 1 and 2 I-DIDs formulated for the small problem areas. As there are an infinite number of calculable models, we obtain the policy by precisely solving the I-DIDs for the two problem areas, as a function of the time allocated to their solutions. In Figure 17, we show the average rewards achieved by executing the policy trees obtained from the exact solution of Level 1 and 2 I-DIDs for the two problem areas, as a function of the time allocated to their solutions. Each data point is the average of policy execution, where the true model of the other actor, j, is randomly selected according to the faith distribution across the J models. The time consumed is a function of the original number of models and the horizon of the I-DID, both of which differ from each other."}, {"heading": "6.2 Comparative Performance of Approximation Methods", "text": "This year it is more than ever before."}, {"heading": "6.3 Runtime Comparison", "text": "We show the runtimes of the exact and approximate techniques for solving Level 1 and 2 I-DIDs with simultaneous scaling in horizons in Table 1. Note that a simple, accurate approach that does not exploit model equivalences goes poorly beyond small horizons. In contrast, the simple grouping of BE models and the reduction in exponential growth of models leads to significantly faster execution and better scaling. Runtimes are reported for both approaches and resolve exactly the same I-DID. In determining runtimes for the approximations, we adjusted the appropriate parameters so that the quality of the solution was similar for each approach. DMU and AE reported significantly lower execution times and better scaling compared to MC for both domains. However, the runtimes of DMU and AE are relatively similar for Level 1 I-DIDs. Although, as we have previously seen, there is a difference in the number of models that sometimes hold up the differences of the DIIN level I-DIDs as the number of the technicians are larger than the two DIDs."}, {"heading": "6.4 Scalable Testbed: GaTAC", "text": "This year, it will only be once before such a process takes place."}, {"heading": "7. Discussion", "text": "In fact, most of them will be able to move to another world in which they will be able to move."}, {"heading": "Acknowledgments", "text": "Yifeng Zeng thanks for the support of the Obel Family Foundation (Denmark) and NSFC (# 60974089 and # 60975052), Prashant Doshi thanks for the support of a CAREER grant from NSF (# IIS-0845036) and a grant from the US Air Force (# FA9550-08-1-0429), Ekhlas Sonu and Yingke Chen for their help in running the GaTAC-based simulations and all anonymous critics for their helpful comments."}, {"heading": "Appendix A. Proofs", "text": "The calculation in the I-DID could be designed using the standard dynamic approach in such a way that the solution of the I-DID is preserved. Let Qn (bi, l, ai) be the action value on the horizon n. Its calculation in the I-DID could be modelled using the standard dynamics. Let ERi (s, mj, l \u2212 1, ai) be the expected immediate reward for the actions predicted by j. Then, the calculation in the I-DID could be designed using the I-DID in such a way that it is, bi \u2212 1, bi \u2212 s, bi \u2212 2, bi \u2212 1, bi \u2212 1, l \u2212 s, i \u2212 s, i s, i s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, s \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i \u2212 s, i, i \u2212 s, i \u2212 s, i \u2212 s, i, i \u2212 s, i \u2212 s, i, i \u2212 s, i, i, i, i-s, i \u2212 s, i-s, i-s, i \u2212 s, i-s, i \u2212 s, i \u2212 s, i \u2212 s, i, i \u2212 s, i \u2212 s, i \u2212 s, i, i,"}, {"heading": "Appendix B. Problem Domains", "text": "In fact, most people are able to move to another world in which they are able to live, in which they want to live."}], "references": [{"title": "Hugin: A shell for building belief universes for expert systems", "author": ["S. Andersen", "F. Jensen"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Andersen and Jensen,? \\Q1989\\E", "shortCiteRegEx": "Andersen and Jensen", "year": 1989}, {"title": "Interactive epistemology i: Knowledge", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "Interactive epistemology ii: Probability", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory,", "citeRegEx": "Aumann,? \\Q1999\\E", "shortCiteRegEx": "Aumann", "year": 1999}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Hierarchies of beliefs and common knowledge", "author": ["A. Brandenburger", "E. Dekel"], "venue": "Journal of Economic Theory,", "citeRegEx": "Brandenburger and Dekel,? \\Q1993\\E", "shortCiteRegEx": "Brandenburger and Dekel", "year": 1993}, {"title": "Forest fire monitoring with multiple small uavs", "author": ["D. Casbeer", "R. Beard", "T. McLain", "L. Sai-Ming", "R. Mehra"], "venue": "In American Control Conference,", "citeRegEx": "Casbeer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Casbeer et al\\.", "year": 2005}, {"title": "Equivalence relations in fully and partially observable markov decision processes", "author": ["P. Castro", "P. Panangaden", "D. Precup"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Castro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2009}, {"title": "Refinement and coarsening of bayesian networks", "author": ["Chang", "K.-C", "R. Fung"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Chang et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Chang et al\\.", "year": 1991}, {"title": "Gatac: A scalable and realistic testbed for multiagent decision making)", "author": ["P. Doshi", "E. Sonu"], "venue": "In Fifth Workshop on Multiagent Sequential Decision Making in Uncertain Domains (MSDM),", "citeRegEx": "Doshi and Sonu,? \\Q2010\\E", "shortCiteRegEx": "Doshi and Sonu", "year": 2010}, {"title": "Graphical models for interactive pomdps: Representations and solutions", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS),", "citeRegEx": "Doshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2009}, {"title": "Knowledge combination in graphical multiagent models", "author": ["Q. Duong", "M. Wellman", "S. Singh"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Duong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2008}, {"title": "History-dependent graphical multiagent models", "author": ["Q. Duong", "M. Wellman", "S. Singh", "Y. Vorobeychik"], "venue": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Duong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2010}, {"title": "Game theoretic control for robot teams", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2005}, {"title": "Networks of influence diagrams: A formalism for representing agents\u2019 beliefs and decision-making processes", "author": ["Y. Gal", "A. Pfeffer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gal and Pfeffer,? \\Q2008\\E", "shortCiteRegEx": "Gal and Pfeffer", "year": 2008}, {"title": "Equivalence notions and model minimization in markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2003}, {"title": "A framework for sequential planning in multiagent settings", "author": ["P. Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Gmytrasiewicz and Doshi,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2005}, {"title": "Games with incomplete information played by bayesian players", "author": ["J.C. Harsanyi"], "venue": "Management Science,", "citeRegEx": "Harsanyi,? \\Q1967\\E", "shortCiteRegEx": "Harsanyi", "year": 1967}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence Journal,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Graphical models for game theory", "author": ["M. Kearns", "M. Littman", "S. Singh"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "Multi-agent influence diagrams for representing and solving games", "author": ["D. Koller", "B. Milch"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Koller and Milch,? \\Q2001\\E", "shortCiteRegEx": "Koller and Milch", "year": 2001}, {"title": "An influence diagram framework for acting under influence by agents with unknown goals", "author": ["N.S. Madsen", "F.V. Jensen"], "venue": "In Fourth European Workshop on Probabilistic Graphical Models (PGM),", "citeRegEx": "Madsen and Jensen,? \\Q2008\\E", "shortCiteRegEx": "Madsen and Jensen", "year": 2008}, {"title": "Formulation of bayesian analysis for games with incomplete information", "author": ["J. Mertens", "S. Zamir"], "venue": "International Journal of Game Theory,", "citeRegEx": "Mertens and Zamir,? \\Q1985\\E", "shortCiteRegEx": "Mertens and Zamir", "year": 1985}, {"title": "A Calculus of Communicating Systems", "author": ["R. Milner"], "venue": null, "citeRegEx": "Milner,? \\Q1980\\E", "shortCiteRegEx": "Milner", "year": 1980}, {"title": "Applications for mini vtol uav for law enforcement. In SPIE 3577:Sensors, C3I, Information, and Training Technologies for Law Enforcement", "author": ["D. Murphy", "J. Cycon"], "venue": null, "citeRegEx": "Murphy and Cycon,? \\Q1998\\E", "shortCiteRegEx": "Murphy and Cycon", "year": 1998}, {"title": "Taming decentralized pomdps : Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Evaluating influence diagrams using limids", "author": ["D. Nilsson", "S. Lauritzen"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Nilsson and Lauritzen,? \\Q2000\\E", "shortCiteRegEx": "Nilsson and Lauritzen", "year": 2000}, {"title": "Lossless clustering of histories in decentralized pomdps", "author": ["F.A. Oliehoek", "S. Whiteson", "M.T.J. Spaan"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "Oliehoek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2009}, {"title": "The flightgear flight simulator. In UseLinux", "author": ["A.R. Perry"], "venue": null, "citeRegEx": "Perry,? \\Q2004\\E", "shortCiteRegEx": "Perry", "year": 2004}, {"title": "Anytime point-based approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Minimal mental models", "author": ["D. Pynadath", "S. Marsella"], "venue": "In Twenty-Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Pynadath and Marsella,? \\Q2007\\E", "shortCiteRegEx": "Pynadath and Marsella", "year": 2007}, {"title": "Exact solutions to interactive pomdps using behavioral equivalence", "author": ["B. Rathnasabapathy", "P. Doshi", "P.J. Gmytrasiewicz"], "venue": "In Autonomous Agents and Multi-Agents Systems Conference (AAMAS),", "citeRegEx": "Rathnasabapathy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rathnasabapathy et al\\.", "year": 2006}, {"title": "Artificial Intelligence: A Modern Approach (Third Edition)", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Memory-bounded dynamic programming for dec-pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "The optimal control of partially observable markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research (OR),", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Learning models of other agents using influence diagrams", "author": ["D. Suryadi", "P. Gmytrasiewicz"], "venue": "In International Conference on User Modeling,", "citeRegEx": "Suryadi and Gmytrasiewicz,? \\Q1999\\E", "shortCiteRegEx": "Suryadi and Gmytrasiewicz", "year": 1999}, {"title": "Dynamic programming and influence diagrams", "author": ["J.A. Tatman", "R.D. Shachter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Tatman and Shachter,? \\Q1990\\E", "shortCiteRegEx": "Tatman and Shachter", "year": 1990}, {"title": "Influence-based policy abstraction for weakly-coupled dec-pomdps", "author": ["S.J. Witwicki", "E.H. Durfee"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Witwicki and Durfee,? \\Q2010\\E", "shortCiteRegEx": "Witwicki and Durfee", "year": 2010}, {"title": "Solving multistage influence diagrams using branch-andbound search", "author": ["C. Yuan", "X. Wu", "E. Hansen"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "Approximate solutions of interactive dynamic influence diagrams using model clustering", "author": ["Y. Zeng", "P. Doshi", "Q. Chen"], "venue": "In Twenty Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Zeng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "Not only is this representation more intuitive to use, it translates into computational benefits when compared to the enumerative representation as used in I-POMDPs (Doshi et al., 2009).", "startOffset": 165, "endOffset": 185}, {"referenceID": 34, "context": "Related Work Suryadi and Gmytrasiewicz (1999) in an early piece of related work, proposed modeling other agents using IDs.", "startOffset": 13, "endOffset": 46}, {"referenceID": 9, "context": "As detailed by Doshi et al. (2009), I-DIDs contribute to an emerging and promising line of research on graphical models for multiagent decision making.", "startOffset": 15, "endOffset": 35}, {"referenceID": 30, "context": "The concept of BE of models was proposed and initially used for solving I-POMDPs (Rathnasabapathy et al., 2006), and discussed generally by Pynadath and Marsella (2007).", "startOffset": 81, "endOffset": 111}, {"referenceID": 22, "context": "Along this direction, another type of equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009).", "startOffset": 145, "endOffset": 215}, {"referenceID": 14, "context": "Along this direction, another type of equivalence in probabilistic frameworks such as MDPs and POMDPs, sometimes also called BE, is bisimulation (Milner, 1980; Givan et al., 2003; Castro, Panangaden, & Precup, 2009).", "startOffset": 145, "endOffset": 215}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002).", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "Our agent models are analogous to types in game theory (Harsanyi, 1967), which are defined as attribute vectors that encompass all of an agent\u2019s private information.", "startOffset": 55, "endOffset": 71}, {"referenceID": 23, "context": ", 2006), and discussed generally by Pynadath and Marsella (2007). We contextualize BE within the framework of I-DIDs and seek further extensions.", "startOffset": 36, "endOffset": 65}, {"referenceID": 12, "context": "A somewhat related notion is that of state equivalence introduced by Givan et al. (2003) where the equivalence concept is exploited to factorize MDPs and gain computational benefit.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering.", "startOffset": 52, "endOffset": 236}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss.", "startOffset": 52, "endOffset": 389}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence.", "startOffset": 52, "endOffset": 564}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster the models of the other agent, we note that BE models when combined with the subject agent\u2019s policy induce identical distributions over the subject agent\u2019s action-observation history. More recently, Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agent\u2019s belief to the other agent\u2019s relevant information by focusing on mutually-modeled features only.", "startOffset": 52, "endOffset": 1031}, {"referenceID": 3, "context": "Most notable among them is the decentralized POMDP (Bernstein et al., 2002). This framework is suitable for cooperative settings only and focuses on computing the joint solution for all agents in the team. Seuken and Zilberstein (2008) provide a comprehensive survey of approaches related to decentralized POMDPs; we emphasize a few that exploit clustering. Emery-Montemerlo et al. (2005) propose iteratively merging action-observation histories of agents that lead to a small worst-case expected loss. While this clustering could be lossy, Oliehoek et al. (2009) losslessly cluster histories that exhibit probabilistic equivalence. Such histories generate an identical distribution over the histories of the other agents and lead to the same joint belief state. While we utilize BE to losslessly cluster the models of the other agent, we note that BE models when combined with the subject agent\u2019s policy induce identical distributions over the subject agent\u2019s action-observation history. More recently, Witwicki and Durfee (2010) use influence-based abstraction in order to limit an agent\u2019s belief to the other agent\u2019s relevant information by focusing on mutually-modeled features only. Our agent models are analogous to types in game theory (Harsanyi, 1967), which are defined as attribute vectors that encompass all of an agent\u2019s private information. In this context, Dekel et al. (2006) define a strategic topology on universal type spaces (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993) under which two types are close if their strategic behavior is similar in all strategic situations.", "startOffset": 52, "endOffset": 1391}, {"referenceID": 9, "context": "Background We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009).", "startOffset": 150, "endOffset": 170}, {"referenceID": 17, "context": "We illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005) \u2013 a two-agent generalization of the well-known single agent tiger problem (Kaelbling et al., 1998).", "startOffset": 199, "endOffset": 223}, {"referenceID": 9, "context": "Background We briefly review interactive influence diagrams (I-ID) for two-agent interactions followed by their extension to dynamic settings, I-DIDs (Doshi et al., 2009). Both these formalisms allow modeling the other agent and to use that information in the decision making of the subject agent. We illustrate the formalisms and our approaches in the context of the multiagent tiger problem (Gmytrasiewicz & Doshi, 2005) \u2013 a two-agent generalization of the well-known single agent tiger problem (Kaelbling et al., 1998). In this problem, two agents, i and j, face two closed doors one of which hides a tiger while the other hides a pot of gold. An agent gets rewarded for opening the door that hides the gold but gets penalized for opening the door leading to the tiger. Each agent may open the left door (action denoted by OL), open the right door (OR), or listen (L). On listening, an agent may hear the tiger growling either from the left (observation denoted by GL) or from the right (GR). Additionally, the agent hears creaks emanating from the direction of the door that was possibly opened by the other agent \u2013 creak from the left (CL) or creak from right (CR) \u2013 or silence (S) if no door was opened. All observations are assumed to be noisy. If any door is opened by an agent, the tiger appears behind any of the two doors randomly in the next time step. While the actions of the other agent do not directly affect the reward for an agent, they may potentially change the location of the tiger. This formulation of the problem differs from that of Nair et al. (2003) in the presence of door creaks and that it is not cooperative.", "startOffset": 151, "endOffset": 1577}, {"referenceID": 9, "context": "As an aside, Doshi et al. (2009) show how I-IDs relate to NIDs (Gal & Pfeffer, 2008).", "startOffset": 13, "endOffset": 33}, {"referenceID": 30, "context": "As we mentioned previously, models that are BE (Rathnasabapathy et al., 2006; Pynadath & Marsella, 2007) could be pruned and a single representative model considered.", "startOffset": 47, "endOffset": 104}, {"referenceID": 17, "context": "11 using the tiger problem (Kaelbling et al., 1998), the set M\u0302j,l\u22121 that minimizes Mj,l\u22121 comprises of all the behaviorally distinct representatives of the models in Mj,l\u22121 and only these models.", "startOffset": 27, "endOffset": 51}, {"referenceID": 32, "context": "Seuken and Zilberstein (2007) reuse subtrees of smaller horizon by linking to them using pointers while forming policy trees for the next horizon in the solution of decentralized POMDPs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 30, "context": "Recall that models whose beliefs are spatially close are likely to be BE (Rathnasabapathy et al., 2006).", "startOffset": 73, "endOffset": 103}, {"referenceID": 39, "context": "In addition to these, we utilize the previous approximation technique of k-means clustering (Zeng et al., 2007), referred to as MC, and the exact approach without exploiting BE, referred to as Exact, as baselines.", "startOffset": 92, "endOffset": 111}, {"referenceID": 9, "context": "Furthermore, they provide the advantage of facilitating detailed analysis of the solutions and uncovering interesting behaviors as previously demonstrated (Doshi et al., 2009).", "startOffset": 155, "endOffset": 175}, {"referenceID": 14, "context": "As we mentioned, our formulation of this problem (|S|=2, |Ai|=|Aj |=3, |\u03a9i|=6, |\u03a9j |=2) follows the one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair et al.", "startOffset": 118, "endOffset": 149}, {"referenceID": 14, "context": "As we mentioned, our formulation of this problem (|S|=2, |Ai|=|Aj |=3, |\u03a9i|=6, |\u03a9j |=2) follows the one introduced by Gmytrasiewicz and Doshi (2005), which differs from the formulation of Nair et al. (2003), in not being cooperative and having door creaks as additional observations.", "startOffset": 118, "endOffset": 207}, {"referenceID": 27, "context": "Briefly, GaTAC employs multiple instances of an opensource flight simulator, called FlightGear (Perry, 2004), possibly on different networked platforms that communicate with each other via external servers, and an autonomous control module that interacts with the simulator instances.", "startOffset": 95, "endOffset": 108}, {"referenceID": 16, "context": "I-DIDs are founded on the normative paradigm of decision theory as formalized by DIDs and augmented with aspects of Bayesian games (Harsanyi, 1967) and interactive epistemology (Aumann, 1999a, 1999b) to make them applicable to interactions.", "startOffset": 131, "endOffset": 147}, {"referenceID": 39, "context": "Hence, the cumulative error in j\u2019s predicted behavior over T steps is at most T \u00d7 \u03c1, which is similar to that of the previous k-means model clustering approach (Zeng et al., 2007): \u03c1 \u2264 (R j \u2212R min j )T \u01eb The second term, |(\u03b1\u2032\u2032 \u00b7 bj,l\u22121 \u2212\u03b1\u2032 \u00b7 bj,l\u22121)|, in Eq.", "startOffset": 160, "endOffset": 179}, {"referenceID": 17, "context": "1 Multiagent Tiger Problem As we mentioned previously, our multiagent tiger problem is a non-cooperative generalization of the well-known single agent tiger problem (Kaelbling et al., 1998) to the multiagent setting.", "startOffset": 165, "endOffset": 189}, {"referenceID": 24, "context": "It differs from other multiagent versions of the same problem (Nair et al., 2003) by assuming that the agents hear creaks as well as the growls and the reward function does not promote cooperation.", "startOffset": 62, "endOffset": 81}, {"referenceID": 34, "context": "Smallwood and Sondik (1973) described an MM problem involving a machine containing two internal components.", "startOffset": 0, "endOffset": 28}], "year": 2010, "abstractText": "We focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others\u2019 models, which changes as the agent acts and observes in the multiagent setting. As we may expect, solving I-DIDs is computationally hard. This is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. We present two methods for reducing the size of the model space and stemming its exponential growth. Both these methods involve aggregating individual models into equivalence classes. Our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. Exactly identifying the equivalences would require us to solve all models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving I-DIDs due to the equivalences.", "creator": "gnuplot 4.2 patchlevel 2 "}}}