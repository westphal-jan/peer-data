{"id": "1401.4601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems", "abstract": "Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our proposals.eWe then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. Experimental results show that counting-based search generally outperforms other generic heuristics.", "histories": [["v1", "Sat, 18 Jan 2014 21:09:25 GMT  (2994kb)", "http://arxiv.org/abs/1401.4601v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gilles pesant", "claude-guy quimper", "alessandro zanarini"], "accepted": false, "id": "1401.4601"}, "pdf": {"name": "1401.4601.pdf", "metadata": {"source": "CRF", "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems", "authors": ["Gilles Pesant", "Claude-Guy Quimper", "Alessandro Zanarini"], "emails": ["gilles.pesant@polymtl.ca", "claude-guy.quimper@ift.ulaval.ca", "alessandro.zanarini@dynadec.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves in the lead."}, {"heading": "2. Background and Related Work", "text": "We start with the usual general representation formalism for cp.Definition 1 (Constraint Satisfaction Problem (csp)). Given a finite set of variables X = {x1, x2,..}, a finite domain of possible values for each of these variables, D = {D1,..., D | X, C), requires a finite set of constraints (relationships) over subsets of X, C = {c1, c2,.}, the constraint satissfaction problem (X, D, C) an assignment of a value of Di to each variable xi of X, which (belongs to) each cj in C. And now recall some definitions and notations of Pesant (2005) and Zanarini and Pesant (2009)."}, {"heading": "2.1 Impact-Based Heuristics", "text": "Refalo (2004) proposed Impact Based Search (IBS), a heuristic that selects the variable whose instantiation triggers the greatest reduction in the search space (maximum impact), which is approximated as a decrease in the product of the variable domain cardinalities. Formally, the influence of a variable-value pair is as follows: I (xi = d) = 1 \u2212 Pafter and Pbefore are the products of the domain cardinalities or, respectively, gradually and before branching to xi = d (and dissemination of this decision). The influence is either calculated exactly to a particular node of the search (the exact calculation provides better information, but is more time-consuming) or approaches the average decrease observed during the search (therefore automatically collected on-the-go at almost no additional cost), i.e. I (xi = d) = K I k k k (xi = d) | K where K is the index set of the previously observed effect for xi = d."}, {"heading": "2.2 Conflict-Driven Heuristics", "text": "Boussemart, Hemery, Lecoutre and Sais (2004) proposed a conflict-driven variable order heuristically: They expanded the concept of variable grade by integrating a simple but effective learning method that takes failures into account. Basically, each restriction has an associated weight that is increased by one each time the restriction leads to a failure (i.e. a domain that is erased). A variable has a weighted degree - wdeg - this is the sum of the loads it is involved in. Formally, the weighted degree of a variable is \"c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "2.3 Approximated Counting-Based Heuristics", "text": "Kask, Dechter, and Gogate (2004) approach the total number of solutions that represent a partial solution to a problem as heuristics, and use them in a value selection heuristics, selecting the value whose association with the current variable yields the largest approximate number of solutions. An implementation optimized for binary constraints performs well compared to other popular strategies. Hsu, Kitching, Bacchus, and McIlraith (2007) and later Bras, Zanarini, and Pesant (2009) apply a belief propagation algorithm within an Expectation Maximization Framework (EMBP) to approximate variable distortions (or marginals), i.e. the likelihood that a variable in a solution takes a given value, while the resulting heuristics tend to be effective but quite time consuming. One way to differentiate our work from them is that we focus on overall grainy information across the constraints of individual problem information."}, {"heading": "3. Counting for Alldifferent Constraints", "text": "The alldifferent constraint limits a set of variables that must be different in pairs (Re \u0301 gin, 1994).Definition 4 (Alldifferent Constraint).Faced with a set of variables X = {x1,.., xn} with the respective domains D1,..., Dn, the set of tuples allowed by alldifferent (X) are: {(d1, d2,.., dn) | di, di 6 = dj, i 6 = j} We define the associated (0-1) square matrix A = (help) with | i = 1,..., nDi | rows and columns so that Help = 1 iff d, Di 2. If there are clearer values in the domains than there are variables, say p more, we add p rows with 1s to matrix A. An equivalent representation is given by the bipartite value curve with a vertex."}, {"heading": "3.1 Upper Bounds", "text": "In the following, for reasons of notation, we assume that matrix A has n rows and columns, and that ri denotes the sum of the elements in the first line of A (i.e. ri = \u2211 n d = 1 auxiliary). A first upper limit for the permanent was assumed by Minc (1963) and later proved by Bre'gman (1973): perm (A) \u2264 n-i = 1 (ri!) 1 / ri. (2) Recently, Liang and Bai (2004) proposed a second upper limit (with qi = min {d ri + 12 e, d i 2e}): perm (A) 2 \u2264 n-i = 1 qi (ri \u2212 qi + 1). (3) Neither of these two upper limits strictly dominates the other. In the following, we denote the Bre'gman-Minc upper limit by UBBM (A), and the Liang-Bai upper limit by UBLB (A). Jurkat and Ryser (1966) proposed another limit: However, it is regarded as integral (BBI = 1)."}, {"heading": "3.1.1 Algorithm", "text": "We have decided to adapt the UBBM and the UBLB in order to find an approximation of the solution factors to the respective factors. (...) We have decided to adapt the UBBM and the UBLB in order to achieve an approximation of the solutions to the respective factors. (...) We have decided to adapt the UBBM and the UBLB in order to achieve an approximation of the solutions to the other factors. (...) We have decided to put the UBLB and the UBLB on another level, as the UBxi. (...) We have no other condition than the UBxi and the UBLB. (...) The UBLB and the UBLB. (...) (...) The UBLB and the UBLB. (...) (...) The UBLB. (...) The UBLB. (...) The UBLB. (...) The UBLB. (....... (...) and the UBLB. (...). (.... (...) The UBLB. (...). (... The UBLB. (...). (....... The UBLB. (...). (... The UBLB. (...). (...). (... The UBLB. (...). (... The UBLB. (...) and the UBLB. (...). (...). (... The UBLB. (...). (... The UBLB. (...). (...). (... The UBLB. (...). (...). (... The UBLB. (...). (...). (... The UBLB. (...). (... The UBLB. (... The UBLB. (...) and the UBLB. (... and the UBLB. (...). (... and the UBLB. (...). (... and the UBLB. (...). (... and the UBLB. (.... (...). (...)."}, {"heading": "3.2 Symmetric Alldifferent", "text": "This corresponds to a traditional alldifferent with an additional set of constraints stating that variable i is assigned to a value j iff variable j that is assigned to the value i. This constraint is useful in many real-world problems where a number of entities must be combined; especially in sports schema problems, teams must form a series of pairs that define the games. A symmetrical alldifferent achieving domain consistency provides more force than the equivalent decomposition given by the alldifferent graph and the set of xi = y xj = i constraints (Re-in, 1999) Its filtering algorithm is inspired by the one-for-all difference, calculating the fit in a graph (not necessarily two-part value) where conversions and values representing the same entity are present."}, {"heading": "4. Counting for Global Cardinality Constraints", "text": "In this section, we will show how the results obtained in Section 3 are extended. (gcc), which is a generalization of all the different constraints, is each as: T (gcc, l, u) = {d1,. (dk), where there are a number of k variables, l and u), each as: T (gcc, l, u) = {d1,. (dk), D (di), D (di), D (di), D (di), D (e), D (e), D (e), D (e), D), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e), D (e)."}, {"heading": "5. Counting for Regular and Knapsack Constraints", "text": "Regular Language Membership Constraint is useful for expressing patterns that must be shown by sequences of variables. Definition 8 (Regular Language Membership Constraint). Regular (X) constraint holds if the values taken by the sequence of finite domain variables X = < x1, x2,., xk > A word belonging to regular language is defined by the deterministic finite automaton.,,,,, Q \"where Q is a finite set of states, is an alphabet that is a partial transition function, q0\" Q is the initial state, and F \"Q is the sequence of final (or accepting) states. Linear qualities and inequalities are expressed as crisp constraints., Definition 9 (Short Bag Constraint Constraint) is a terse transitional function."}, {"heading": "5.1 Domain Consistent Knapsacks", "text": "We start from the reduced graph described by Trick (2003), which is a stratified directed graph G (V, A) with a special vertex v0,0 and a vertex vi, b \u00b2 V for 1 \u2264 i \u2264 k and 0 \u2264 b \u2264 u whenever there is a solution [1, i], and an arc (vi, b, vi + 1, b \u00b2) each time there is a solution (i, n \u00b2, dj \u00b2 j so that \"\u2212 b \u2264 k \u00b2 j = i + 1cjdj \u2264 u \u2212 b, and an arc (vi, vi + 1, b \u00b2) each time di + 1 so that ci + 1d = b \u00b2 \u2212 b. We define the following two recursions by the number of incoming and outgoing paths at each node. For each vertex vi, b \u00b2 V, let # ip (i, b) we seal the number of paths from vertexts 0,0 # b \u00b2 to the number vi \u00b2 b \u00b2 n."}, {"heading": "5.2 Bounds Consistent Knapsacks", "text": "Concise constraints, in fact most arithmetic constraints, have traditionally been handled by enforcing the consistency of boundaries, a much cheaper form of inference. In some situations, we can't afford to enforce the consistency of the domain in order to get the solution we need to direct our search. Can we still retrieve such information, perhaps not as accurately, from the weaker boundaries of consistency? Consider the variable x with the domain D = [a, b]. Each value in D is equivalent. We associate x with the discrete random variable X, which follows a discrete uniform distribution with the probability mass function f (v), the mean \u00b5 = E [X], and the variance 2 = V ar [X]."}, {"heading": "6. Generic Constraint-Centered Counting-based Heuristics", "text": "The previous sections for which a change in the domain of the variables has occurred within its scope are used by many of the most commonly used data. (This information must then be evaluated directly to guide the search.) The approaches are then chosen by one of the most successful ones in Section 7, so that we present them in more detail. (In the following, C (xi) will list the number of constraints the scope of which contains variable xi. (All heuristic suggestions assume a lexicographic order as a tie break.) The enumeration of information is collected once in a search tree to reach a fixed point: only the domain of the variables is used."}, {"heading": "7. Experimental Analysis", "text": "We conducted a thorough experimental analysis to assess the performance of the proposed heuristics on eight different problems.7 All problems reveal substructures that can be encapsulated in global constraints for which counting algorithms are known. Count-based heuristics are useless for random problems, since this type of problem does not reveal a structure; nevertheless, real problems usually constitute a structure, so the performance of the proposed heuristics can have a positive impact on the quest to provide general and efficient heuristics for structured problems. The problems we experimented with have different structures and different limitations, with possibly different meanings.7 The examples we used are at www.crt.umontreal.ca / quosseca / fichiers / 20-JAIRbenchs.tar.gz.arieties that are linked in different ways can therefore be considered good representatives of the variety of problems that can occur in real life."}, {"heading": "7.1 Quasigroup Completion Problem with Holes (QWH)", "text": "Also known as the Latin square problem, the QWH is defined on an n \u00b7 n grid, the squares of which each contain an integer from 1 to n, so that each integer appears exactly once per row and column (problem 3 of the CSPLib in Ghent, Walsh, Hnich & Miguel, 2009). The most common model uses a matrix of integer variables and a very different constraint for each row and column, so each constraint is defined on n variables and is of the same type; each variable is involved in two constraints and has the same domain (without considering the clues), which is a very homogeneous problem. We tested the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of the holes (corresponding to the phase transition) created following Gomes and Shmoys (2002)."}, {"heading": "7.2 Magic Square Completion Problem", "text": "The magic square completion problem (CSPLib problem 19) is defined on an n \u00b7 n grid and requires that the square be filled with numbers from 1 to n2 so that each row, column, and main diagonal adds up to the same value. To make it more difficult, the problem instances were partially prefilled (half of the instances have 10% of the variable set and the other half, 50% of the variable set).The 40 instances (9 \u00d7 9) are taken from the work of Pesant and Quimper (2008), which is modeled with a matrix of integer variables, a single constraint that covers all variables, and a boundary condition for each row, column, and main diagonal."}, {"heading": "7.3 Nonograms", "text": "A nonogram (problem 12 of the CSPLib) is based on a rectangular n \u00d7 m grid and requires the filling in of some of the squares in a unique, practical way according to some clues given on each row and column. As a reward, you get a fairly monochrome image. Each clue indicates how many sequences of consecutive filled squares there are in the row (column), with their respective size in the order of their appearance. For example, \"2 1 5\" indicates that there are two consecutive filled squares, then one isolated and finally five consecutive. Each sequence is separated from the others by at least one empty square, but we know little about their actual position in the row (column). Such clues can be modeled with regular constraints. This is a very homogeneous problem, with constraints of identical type defined by m or n variables, and with each (binary) variable involving two constraints."}, {"heading": "7.4 Multi Dimensional Knapsack Problem", "text": "We took the same approach as Refalo (2004), when we turned the multidimensional knapsack problem into a feasibility problem by setting the objective function to its optimal value, thereby introducing a 0-1 equality for knapsacks; the other constraints are upper limited knapsacks constraints for the same variables; we tested on three different instances for a total of 25 instances: the first group corresponds to the six instances used by Refalo; the second group and third group come from the OR library (Weish [1-13] from Shi, 1979; PB [1,2,4] and HP [1,2] from Freville & Plateau, 1990); the first instance has n, i.e. the number of variables ranging from 6 to 50 and m, i.e. the number of constraints, from 5 to 10; in the second and third instance n variables from 27 to 60; and the number of variables from only one and the previous 5-m constraints respectively."}, {"heading": "7.5 Market Split Problem", "text": "The market-sharing problem was originally introduced by Cornue \u0301 jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There is both a feasibility and an optimization version. The feasibility problem consists of m 0-1 equality constraints defined on the same set of 10 (m \u2212 1) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly difficult to solve using standard methods. We used the 10 instances tested by Pesant and Quimper (2008), which were generated by Aquarius (2007)."}, {"heading": "7.6 Rostering Problem", "text": "The roster problem is inspired by a roster context, the goal of which is to schedule n employees over a period of n periods. In each period, n \u2212 1 tasks must be completed and an employee from n has a break. Tasks are fully ordered from 1 to n \u2212 1; for each employee, the schedule must comply with the following rules: Two consecutive periods must be assigned to either two consecutive tasks (in whatever order, i.e. (t, t + 1) or the same task (i.e. (t, t + 1)); an employee can take a break after which task; after a break, an employee cannot complete the task that precedes the break (i.e. (t, pause, t \u2212 1) is not allowed); the problem is modeled with a regular constraint per line and a different constraint per column."}, {"heading": "7.7 Cost-Constrained Rostering Problem", "text": "The cost-constrained roster problem was taken over by Pesant and Quimper (2008) and the 10 cases. It is inspired by a roster problem where m-employees (m = 4) have to complete a series of tasks within an n-day schedule (n = 25). No employee can perform the same task as another employee on the same day (any restrictions on each day). In addition, there are hourly costs for performing a job that vary by employee and day. For each employee, the total cost must correspond to a randomly generated value (equality requirement for each employee). Finally, each instance has about 10 prohibited shifts, i.e. there are some days on which an employee cannot perform a certain task. Below, we also call this problem KPRostering. This problem represents constraints of different kinds that vary widely."}, {"heading": "7.8 Traveling Tournament Problem with Predefined Venues (TTPPV)", "text": "The TTPPV was introduced by Melo, Urrutia and Ribeiro (2009) and consists in finding an optimal single game plan for a sporting event. In view of a number of teams, each team must play against each other, i.e. when one team competes against another, no team can be played at home or at home.The peculiarity of this problem lies in the fact that in most cases the number of home games is predetermined, i.e. that the team plays against another team, it is already known whether the team is playing at home or in another country.The TTPPV instance is balanced when the number of home games and the number of away games differ from each other."}, {"heading": "7.9 Comparing Counting-Based Search Heuristics", "text": "We first compare several of the suggested search heuristics, which are based on counting how well they conduct the search, measured as the number of traces required to find a solution.The important issue of total runtime will be addressed in the following paragraphs.9 Even for knapsack constraints, comparative experimental results point to the same benchmark instances originally reported by Pesant and Quimper (2008), suggesting that maxSD performs better with domain consistency and the related counting algorithm.Figure 6 records the number of solved instances against trace for our eight benchmark problems. On the nonogram, multi-crunch bag and market-split problems, maxSD, maxRelSD and maxRelRatio correspond to the same heuristics because domains are binary. Restricting the use of solution densities to the choice of a value once the variable is selected by the domain's general dynamic size over the domain's maxic size;"}, {"heading": "7.10 Comparing with Other Generic Search Heuristics", "text": "The experimental results of the previous section indicate that the relatively simple maxSD heuristic guidelines for searching are at least as good as any other (see section 2 for reference), which are good examples of state-of-the-art generic search heuristics: \u2022 dom - it selects evenly between the variables with the smallest remaining domain and then selects a value that is evenly random; \u2022 domWDeg - it selects the variables according to the domain / wdeg heuristic and then the first value in the lexicographic order; \u2022 IBS - Impact-based Search with full initialization of the impacts; it selects a subset of 5 variables with the best approximated effects and then breaks ties based on the node while further bindings are randomly broken; (ILOG, 2005) the number of instances dissolved against backtracks 7 and 8 plots the number of instances dissolved."}, {"heading": "7.11 Adding Randomized Restarts", "text": "It was noted that some combinatorial searches have a strictly positive probability of reaching a sub-tree that requires exponentially more time than the other sub-trees encountered so far (so-called \"heavy tail\" behavior). Nevertheless, heavy tails can be largely avoided by placing random restarts at the top of the search process (Gomes, Selman, & Kautz, 1998).This technique is applied orthogonally to the search heuristically, and it systematically restarts the search every time a limit (typically a limit on the number of restarts) is reached; obviously, randomized restarts need to be applied along with a heuristic method that represents a kind of randomization or learning, so that different parts of the search tree are examined each time a restart is made. We tested the same heuristics to evaluate their performance with randomized restarts. The maxSD and IBS heuristics were randomized: in particular, a variable value pair is randomly selected, with the probability of both problems being solved."}, {"heading": "7.12 Using Limited Discrepancy Search", "text": "Another way to avoid heavy tails is to change the order in which the search tree is traversed, reversing decisions made at the top of the search tree at an earlier stage. A popular method is the use of a Limited Discrepancy Search (LDS), which visits branches in an increasing order of the number of \"discrepancies\" that correspond to branched decisions that are heuristic to the search (Harvey & Ginsberg, 1995). As for restarts, it can be combined with any search heuristically, and in some cases can cause dramatic improvements, but this less natural overrun comes with a price. Figure 9 illustrates the impact of LDS on two of our benchmark problems by using maxSD as a search heuristic. Either the usual Depth First Search (\"maxSD\" curve) or a limited Discrepancy Search, grouping branches that have exactly the same number of discrepancies is used (\"2) (\" LDS)."}, {"heading": "7.13 Analyzing Variable and Value Selection Separately", "text": "To investigate this, we introduce some hybrid heuristics: \u2022 maxSD; random - selects a variable as in maxSD, but then randomly selects a value in its range; \u2022 IBS; maxSD - selects a variable as in IBS, but then selects a value in its range according to solution densities; \u2022 domWDeg; maxSD - selects a variable as in DomWDeg, but then selects a value in its range according to solution densities; Figure 10 and 11 records the number of solved instances against traceability and time for our eight benchmark problems. Comparing maxSD and maxSD randomly indicates that the majority of time value selection according to solution densities is decisive, with the rostering problem being an exception. Interestingly, the value selection by solution density is very favorable to the overall success of maxSD and maxSD; the comparison between maxSD and square SD indicates that the improvement in IBS is not necessarily the improvement in IBS performance."}, {"heading": "8. Conclusion", "text": "This paper describes and evaluates counting search for constraints to solve satisfaction problems. We presented some algorithms necessary to extract counting information from several major families of constraints in cp. We proposed a variety of heuristics based on this counting information and evaluated them. We then compared an outstanding representative, maxSD, with the state of the art on eight different problems from the literature and achieved very encouraging results. The next logical steps in this research include designing counting algorithms for some of the other common constraints and strengthening our empirical evaluation by taking into account new problems and comparing them with application-specific heuristics. The next two paragraphs describe less obvious steps. Users often need to introduce auxiliary variables or different views of the models that are linked by exercising constraints. It is very important to provide all available information at the level of branched variables or at least at a more direct level where a comparison is useful."}, {"heading": "Acknowledgments", "text": "Financial support for this research came in part from the Natural Sciences and Engineering Research Council of Canada and the fund que'be \u0301 cois de la recherche sur la nature et les technologies. We thank Tyrel Russell, who was involved in the implementation and experiments, and the anonymous arbitrators for their constructive comments, which allowed us to improve our work."}], "references": [{"title": "The Maximum Number of Perfect Matchings in Graphs with a Given Degree Sequence", "author": ["N. Alon", "S. Friedland"], "venue": "The Electronic Journal of Combinatorics,", "citeRegEx": "Alon and Friedland,? \\Q2008\\E", "shortCiteRegEx": "Alon and Friedland", "year": 2008}, {"title": "Experimental evaluation of modern variable selection strategies in Constraint Satisfaction Problems. In Proceedings of the Fifteenth Knowledge Representation and Automated Reasoning Workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion, RCRA-08", "author": ["T. Balafoutis", "K. Stergiou"], "venue": null, "citeRegEx": "Balafoutis and Stergiou,? \\Q2008\\E", "shortCiteRegEx": "Balafoutis and Stergiou", "year": 2008}, {"title": "On Conflict-driven variable ordering heuristics", "author": ["T. Balafoutis", "K. Stergiou"], "venue": "In Proceedings of Thirteenth Annual ERCIM International Workshop on Constraint Solving and Constraint Logic Programming,", "citeRegEx": "Balafoutis and Stergiou,? \\Q2008\\E", "shortCiteRegEx": "Balafoutis and Stergiou", "year": 2008}, {"title": "MAC and Combined Heuristics: Two Reasons to Forsake FC (and CBJ?) on Hard Problems", "author": ["C. Bessi\u00e8re", "R\u00e9gin", "J.-C"], "venue": "In Proceedings of the Second International Conference on Principles and Practice of Constraint Programming, CP-96,", "citeRegEx": "Bessi\u00e8re et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bessi\u00e8re et al\\.", "year": 1996}, {"title": "Boosting Systematic Search by Weighting Constraints", "author": ["F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais"], "venue": "In Proceedings of the Sixteenth Eureopean Conference on Artificial Intelligence,", "citeRegEx": "Boussemart et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boussemart et al\\.", "year": 2004}, {"title": "Efficient Generic Search Heuristics within the EMBP framework", "author": ["R.L. Bras", "A. Zanarini", "G. Pesant"], "venue": "In Proceedings of the Fifteenth International Conference on Principles and Practice of Constraint Programming, CP-04,", "citeRegEx": "Bras et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bras et al\\.", "year": 2009}, {"title": "Some Properties of Nonnegative Matrices and their Permanents", "author": ["L.M. Br\u00e9gman"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Br\u00e9gman,? \\Q1973\\E", "shortCiteRegEx": "Br\u00e9gman", "year": 1973}, {"title": "New Methods to Color the Vertices of a Graph", "author": ["D. Br\u00e9laz"], "venue": "Communications of the ACM,", "citeRegEx": "Br\u00e9laz,? \\Q1979\\E", "shortCiteRegEx": "Br\u00e9laz", "year": 1979}, {"title": "A Class of Hard Small 0-1 Programs", "author": ["G. Cornu\u00e9jols", "M. Dawande"], "venue": "INFORMS Journal of Computing,", "citeRegEx": "Cornu\u00e9jols and Dawande,? \\Q1999\\E", "shortCiteRegEx": "Cornu\u00e9jols and Dawande", "year": 1999}, {"title": "A Branch and Bound Method for the Multiconstraint Zero One Knapsack Problem", "author": ["A. Freville", "G. Plateau"], "venue": "Investigation Operativa,", "citeRegEx": "Freville and Plateau,? \\Q1990\\E", "shortCiteRegEx": "Freville and Plateau", "year": 1990}, {"title": "An Upper Bound for the Number of Perfect Matchings in Graphs. http://arxiv.org/abs/0803.0864", "author": ["S. Friedland"], "venue": null, "citeRegEx": "Friedland,? \\Q2008\\E", "shortCiteRegEx": "Friedland", "year": 2008}, {"title": "A Problem Library for Constraints. http://www.csplib.org", "author": ["I.P. Gent", "T. Walsh", "B. Hnich", "I. Miguel"], "venue": null, "citeRegEx": "Gent et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gent et al\\.", "year": 2009}, {"title": "Boosting Combinatorial Search Through Randomization", "author": ["C. Gomes", "B. Selman", "H. Kautz"], "venue": "In Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence,", "citeRegEx": "Gomes et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 1998}, {"title": "Completing Quasigroups or Latin Squares: A Structured Graph Coloring Problem", "author": ["C. Gomes", "D. Shmoys"], "venue": "In Proceedings of Computational Symposium on Graph Coloring and Generalizations,", "citeRegEx": "Gomes and Shmoys,? \\Q2002\\E", "shortCiteRegEx": "Gomes and Shmoys", "year": 2002}, {"title": "Learning to Identify Global Bottlenecks in Constraint Satisfaction Search. In Learning for Search: Papers from the AAAI-06", "author": ["D. Grimes", "R.J. Wallace"], "venue": null, "citeRegEx": "Grimes and Wallace,? \\Q2006\\E", "shortCiteRegEx": "Grimes and Wallace", "year": 2006}, {"title": "Sampling Strategies and Variable Selection in Weighted Degree Heuristics", "author": ["D. Grimes", "R.J. Wallace"], "venue": "In Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Grimes and Wallace,? \\Q2007\\E", "shortCiteRegEx": "Grimes and Wallace", "year": 2007}, {"title": "Increasing Tree Seach Efficiency for Constraint Satisfaction Problems", "author": ["R.M. Haralick", "G.L. Elliott"], "venue": "Artificial Intelligence,", "citeRegEx": "Haralick and Elliott,? \\Q1980\\E", "shortCiteRegEx": "Haralick and Elliott", "year": 1980}, {"title": "Limited Discrepancy Search", "author": ["W.D. Harvey", "M.L. Ginsberg"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Harvey and Ginsberg,? \\Q1995\\E", "shortCiteRegEx": "Harvey and Ginsberg", "year": 1995}, {"title": "Using Expectation Maximization to Find Likely Assignments for Solving CSP\u2019s", "author": ["E.I. Hsu", "M. Kitching", "F. Bacchus", "S.A. McIlraith"], "venue": "In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hsu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2007}, {"title": "The Impact of Search", "author": ["T. Hulubei", "B. O\u2019Sullivan"], "venue": "Heuristics on Heavy-Tailed Behaviour. Constraints,", "citeRegEx": "Hulubei and O.Sullivan,? \\Q2006\\E", "shortCiteRegEx": "Hulubei and O.Sullivan", "year": 2006}, {"title": "Matrix Factorizations of Determinants and Permanents", "author": ["W. Jurkat", "H.J. Ryser"], "venue": "Journal of Algebra,", "citeRegEx": "Jurkat and Ryser,? \\Q1966\\E", "shortCiteRegEx": "Jurkat and Ryser", "year": 1966}, {"title": "Counting-Based Look-Ahead Schemes for Constraint Satisfaction", "author": ["K. Kask", "R. Dechter", "W. Gogate"], "venue": "In Springer-Verlag (Ed.), Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming, CP-04,", "citeRegEx": "Kask et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kask et al\\.", "year": 2004}, {"title": "An Upper Bound for the Permanent of (0,1)-Matrices", "author": ["H. Liang", "F. Bai"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Liang and Bai,? \\Q2004\\E", "shortCiteRegEx": "Liang and Bai", "year": 2004}, {"title": "The traveling tournament problem with predefined venues", "author": ["R. Melo", "S. Urrutia", "C. Ribeiro"], "venue": "Journal of Scheduling,", "citeRegEx": "Melo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2009}, {"title": "Upper Bounds for Permanents of (0, 1)-matrices", "author": ["H. Minc"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Minc,? \\Q1963\\E", "shortCiteRegEx": "Minc", "year": 1963}, {"title": "A Regular Language Membership Constraint for Finite Sequences of Variables", "author": ["G. Pesant"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Pesant,? \\Q2004\\E", "shortCiteRegEx": "Pesant", "year": 2004}, {"title": "Counting solutions of csps: A structural approach", "author": ["G. Pesant"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Pesant,? \\Q2005\\E", "shortCiteRegEx": "Pesant", "year": 2005}, {"title": "Counting solutions of knapsack constraints", "author": ["G. Pesant", "Quimper", "C.-G"], "venue": "In Proceedings of the Fifth International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems,", "citeRegEx": "Pesant et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pesant et al\\.", "year": 2008}, {"title": "Recovering indirect solution densities for counting-based branching heuristics", "author": ["G. Pesant", "A. Zanarini"], "venue": "CPAIOR, Vol. 6697 of Lecture Notes in Computer Science,", "citeRegEx": "Pesant and Zanarini,? \\Q2011\\E", "shortCiteRegEx": "Pesant and Zanarini", "year": 2011}, {"title": "Improved algorithms for the global cardinality constraint", "author": ["C. Quimper", "A. Lopez-Ortiz", "P. van Beek", "A. Golynski"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Quimper et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quimper et al\\.", "year": 2004}, {"title": "Impact-Based Search Strategies for Constraint Programming", "author": ["P. Refalo"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Refalo,? \\Q2004\\E", "shortCiteRegEx": "Refalo", "year": 2004}, {"title": "A Filtering Algorithm for Constraints of Difference in CSPs", "author": ["R\u00e9gin", "J.-C"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence, AAAI-94,", "citeRegEx": "R\u00e9gin and J..C.,? \\Q1994\\E", "shortCiteRegEx": "R\u00e9gin and J..C.", "year": 1994}, {"title": "The Symmetric Alldiff Constraint", "author": ["J. R\u00e9gin"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "R\u00e9gin,? \\Q1999\\E", "shortCiteRegEx": "R\u00e9gin", "year": 1999}, {"title": "A Branch and Bound Method for the Multiconstraint Zero One Knapsack Problem", "author": ["W. Shi"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Shi,? \\Q1979\\E", "shortCiteRegEx": "Shi", "year": 1979}, {"title": "Trying Harder to Fail First", "author": ["B.M. Smith", "S.A. Grant"], "venue": "In Thirteenth European Conference on Artificial Intelligence,", "citeRegEx": "Smith and Grant,? \\Q1998\\E", "shortCiteRegEx": "Smith and Grant", "year": 1998}, {"title": "Permanental Bounds for Nonnegative Matrices via Decomposition", "author": ["G.W. Soules"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Soules,? \\Q2005\\E", "shortCiteRegEx": "Soules", "year": 2005}, {"title": "Guiding Search using Constraint-Level Advice", "author": ["R. Szymanek", "B. O\u2019Sullivan"], "venue": "In Proceeding of Seventeenth European Conference on Artificial Intelligence, ECAI06,", "citeRegEx": "Szymanek and O.Sullivan,? \\Q2006\\E", "shortCiteRegEx": "Szymanek and O.Sullivan", "year": 2006}, {"title": "A dynamic programming approach for consistency and propagation for knapsack constraints", "author": ["M.A. Trick"], "venue": "Annals of Operations Research,", "citeRegEx": "Trick,? \\Q2003\\E", "shortCiteRegEx": "Trick", "year": 2003}, {"title": "The Complexity of Computing the Permanent", "author": ["L. Valiant"], "venue": "Theoretical Computer Science,", "citeRegEx": "Valiant,? \\Q1979\\E", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "Search in a Small World", "author": ["T. Walsh"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Walsh,? \\Q1999\\E", "shortCiteRegEx": "Walsh", "year": 1999}, {"title": "The feasibility version of the market split problem. http://did.mat.uni-bayreuth.de/ alfred/marketsplit.html", "author": ["A. Wassermann"], "venue": null, "citeRegEx": "Wassermann,? \\Q2007\\E", "shortCiteRegEx": "Wassermann", "year": 2007}, {"title": "Improved algorithm for the soft global cardinality constraint", "author": ["A. Zanarini", "M. Milano", "G. Pesant"], "venue": "In Proceedings of the Third International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, CPAIOR-06,", "citeRegEx": "Zanarini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zanarini et al\\.", "year": 2006}, {"title": "Solution Counting Algorithms for Constraint-Centered", "author": ["A. Zanarini", "G. Pesant"], "venue": "Search Heuristics. Constraints,", "citeRegEx": "Zanarini and Pesant,? \\Q2009\\E", "shortCiteRegEx": "Zanarini and Pesant", "year": 2009}, {"title": "More robust counting-based search heuristics with alldifferent constraints", "author": ["A. Zanarini", "G. Pesant"], "venue": "CPAIOR, Vol. 6140 of Lecture Notes in Computer Science,", "citeRegEx": "Zanarini and Pesant,? \\Q2010\\E", "shortCiteRegEx": "Zanarini and Pesant", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "The concept of counting-based search heuristics has already been introduced, most recently by Zanarini and Pesant (2009). The specific contributions of this paper are: additional counting algorithms, including for other families of constraints, thus broadening the applicability of these heuristics; experiments that include the effect of some common features of search heuristics such as search tree traversal order, restarts and learning; considerable empirical evidence that counting-based search outperforms other generic heuristics.", "startOffset": 107, "endOffset": 121}, {"referenceID": 25, "context": "And now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant (2009).", "startOffset": 50, "endOffset": 64}, {"referenceID": 25, "context": "And now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant (2009).", "startOffset": 50, "endOffset": 95}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables).", "startOffset": 33, "endOffset": 47}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessi\u00e8re and R\u00e9gin (1996) and Smith and Grant (1998) combined the domain and degree information by minimizing the ratio dom/deg or dom/ddeg.", "startOffset": 33, "endOffset": 283}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessi\u00e8re and R\u00e9gin (1996) and Smith and Grant (1998) combined the domain and degree information by minimizing the ratio dom/deg or dom/ddeg.", "startOffset": 33, "endOffset": 310}, {"referenceID": 30, "context": "1 Impact-Based Heuristics Refalo (2004) proposed Impact Based Search (IBS), a heuristic that chooses the variable whose instantiation triggers the largest search space reduction (highest impact) that is approximated as the reduction of the product of the variable domain cardinalities.", "startOffset": 26, "endOffset": 40}, {"referenceID": 30, "context": "The variable impact is defined by Refalo (2004) as", "startOffset": 34, "endOffset": 48}, {"referenceID": 34, "context": "As an interesting connection with impact-based heuristics, Szymanek and O\u2019Sullivan (2006) proposed to query the model constraints to approximate the number of filtered values by each constraint individually.", "startOffset": 59, "endOffset": 90}, {"referenceID": 1, "context": "Balafoutis and Stergiou (2008b) proposed, among other improvements over the original dom/wdeg, weight aging, that is the constraint weights are periodically reduced.", "startOffset": 0, "endOffset": 32}, {"referenceID": 25, "context": "Hsu, Kitching, Bacchus, and McIlraith (2007) and later Bras, Zanarini, and Pesant (2009) apply a Belief Propagation algorithm within an Expectation Maximization framework (EMBP) in order to approximate variable biases (or marginals) i.", "startOffset": 75, "endOffset": 89}, {"referenceID": 25, "context": "Then as discussed by Zanarini and Pesant (2009), counting the number of solutions to an alldifferent constraint is equivalent to computing the permanent of A (or the number of maximum matchings in the value graph), formally defined as", "startOffset": 34, "endOffset": 48}, {"referenceID": 38, "context": "Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 65, "endOffset": 80}, {"referenceID": 25, "context": "If p extra rows were added, the result must be divided by p! as shown by Zanarini and Pesant (2010). Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 86, "endOffset": 100}, {"referenceID": 25, "context": "If p extra rows were added, the result must be divided by p! as shown by Zanarini and Pesant (2010). Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 86, "endOffset": 210}, {"referenceID": 25, "context": "This was originally introduced by Zanarini and Pesant (2010).", "startOffset": 47, "endOffset": 61}, {"referenceID": 23, "context": "A first upper bound for the permanent was conjectured by Minc (1963) and later proved by Br\u00e9gman (1973): perm(A) \u2264 n \u220f", "startOffset": 57, "endOffset": 69}, {"referenceID": 6, "context": "A first upper bound for the permanent was conjectured by Minc (1963) and later proved by Br\u00e9gman (1973): perm(A) \u2264 n \u220f", "startOffset": 89, "endOffset": 104}, {"referenceID": 22, "context": "Recently Liang and Bai (2004) proposed a second upper bound (with qi = min{d ri+1 2 e, d i 2e}): perm(A) \u2264 n \u220f", "startOffset": 9, "endOffset": 30}, {"referenceID": 6, "context": "In the following we denote by UBBM (A) the Br\u00e9gman-Minc upper bound and by UBLB(A) the Liang-Bai upper bound. Jurkat and Ryser (1966) proposed another bound:", "startOffset": 43, "endOffset": 134}, {"referenceID": 25, "context": "The sampling algorithm introduced by Zanarini and Pesant (2009) performed very well both in approximating the solution count and the solution densities, but this is not the case for upper bounds.", "startOffset": 50, "endOffset": 64}, {"referenceID": 32, "context": "A symmetric alldifferent achieving domain consistency provides more pruning power than the equivalent decomposition given by the alldifferent constraint and the set of xi = j \u21d0\u21d2 xj = i constraints (R\u00e9gin, 1999).", "startOffset": 197, "endOffset": 210}, {"referenceID": 32, "context": "2 Symmetric Alldifferent R\u00e9gin (1999) proposed the symmetric alldifferent constraint that is a special case of the alldifferent in which variables and values are defined from the same set.", "startOffset": 25, "endOffset": 38}, {"referenceID": 0, "context": "Friedland (2008) and Alon and Friedland (2008) extended the Br\u00e9gman-Minc upper bound to consider the number of matchings in general undirected graphs.", "startOffset": 21, "endOffset": 47}, {"referenceID": 25, "context": "Inspired by Quimper, Lopez-Ortiz, van Beek, and Golynski (2004) and Zanarini, Milano, and Pesant (2006), we define Gl the lower bound graph.", "startOffset": 90, "endOffset": 104}, {"referenceID": 25, "context": "The filtering algorithms for the regular constraint and the knapsack constraint (when domain consistency is enforced) are both based on the computation of paths in a layered acyclic directed graph (Pesant, 2004; Trick, 2003).", "startOffset": 197, "endOffset": 224}, {"referenceID": 37, "context": "The filtering algorithms for the regular constraint and the knapsack constraint (when domain consistency is enforced) are both based on the computation of paths in a layered acyclic directed graph (Pesant, 2004; Trick, 2003).", "startOffset": 197, "endOffset": 224}, {"referenceID": 37, "context": "1 Domain Consistent Knapsacks We start from the reduced graph described by Trick (2003), which is a layered directed graph G(V,A) with special vertex v0,0 and a vertex vi,b \u2208 V for 1 \u2264 i \u2264 k and 0 \u2264 b \u2264 u whenever \u2200 j \u2208 [1, i], \u2203 dj \u2208 Dj such that i \u2211", "startOffset": 75, "endOffset": 88}, {"referenceID": 25, "context": "This was originally introduced by Pesant and Quimper (2008).", "startOffset": 34, "endOffset": 60}, {"referenceID": 24, "context": "We tested on the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding to the phase transition), generated following Gomes and Shmoys (2002).", "startOffset": 56, "endOffset": 70}, {"referenceID": 13, "context": "We tested on the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding to the phase transition), generated following Gomes and Shmoys (2002).", "startOffset": 160, "endOffset": 184}, {"referenceID": 25, "context": "The 40 instances (9 \u00d7 9) are taken from the work of Pesant and Quimper (2008). This problem is modeled with a matrix of integer variables, a single alldifferent constraint spanning over all the variables and a knapsack constraint for each row, column and main diagonal.", "startOffset": 52, "endOffset": 78}, {"referenceID": 30, "context": "We followed the same approach as Refalo (2004) in transforming the optimization problem into a feasibility problem by fixing the objective function to its optimal value, thereby introducing a 0-1 equality knapsack constraint.", "startOffset": 33, "endOffset": 47}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches.", "startOffset": 77, "endOffset": 107}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility and optimization version. The feasibility problem consists of m 0-1 equality knapsack constraints defined on the same set of 10(m\u22121) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant and Quimper (2008) that were generated by Wassermann (2007).", "startOffset": 77, "endOffset": 483}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility and optimization version. The feasibility problem consists of m 0-1 equality knapsack constraints defined on the same set of 10(m\u22121) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant and Quimper (2008) that were generated by Wassermann (2007). The Market Split Problem shares some characteristics with the Multi Dimensional Knapsack problem: the constraints are of the same type and they are posted on the same set of variables.", "startOffset": 77, "endOffset": 524}, {"referenceID": 25, "context": "7 Cost-Constrained Rostering Problem The cost-constrained rostering problem was borrowed from Pesant and Quimper (2008) and the 10 instances as well.", "startOffset": 94, "endOffset": 120}, {"referenceID": 23, "context": "The TTPPV was originally introduced as an optimization problem where the sum of the traveling distance of each team has to be minimized, however Melo et al. (2009) show that it is particularly difficult to find a single feasible solution employing traditional integer linear programming methods.", "startOffset": 145, "endOffset": 164}, {"referenceID": 25, "context": "Even for knapsack constraints, comparative experimental results on the same benchmark instances, originally reported by Pesant and Quimper (2008), indicated that maxSD performed better with domain consistency and the associated counting algorithm.", "startOffset": 120, "endOffset": 146}, {"referenceID": 39, "context": "We implemented a slow geometric restart policy (Walsh, 1999) (that is 1, r, r2, .", "startOffset": 47, "endOffset": 60}, {"referenceID": 30, "context": "Note that, as pointed out by Refalo (2004), impact information is carried over different runs to improve the quality of the impact approximation.", "startOffset": 29, "endOffset": 43}], "year": 2012, "abstractText": "Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our proposals. We then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. Experimental results show that counting-based search generally outperforms other generic heuristics.", "creator": "TeX"}}}