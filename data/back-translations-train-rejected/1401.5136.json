{"id": "1401.5136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2014", "title": "A Unifying Framework for Typical Multi-Task Multiple Kernel Learning Problems", "abstract": "Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques in the context of kernel-based learning. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Solving different MKL formulations usually involves designing algorithms that are tailored to the problem at hand, which is, typically, a non-trivial accomplishment.", "histories": [["v1", "Tue, 21 Jan 2014 01:16:44 GMT  (51kb,D)", "http://arxiv.org/abs/1401.5136v1", "17 pages, 1 figure. Accepted by IEEE Transactions on Neural Networks and Learning Systems; currently published as Early Access Article"]], "COMMENTS": "17 pages, 1 figure. Accepted by IEEE Transactions on Neural Networks and Learning Systems; currently published as Early Access Article", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cong li", "michael georgiopoulos", "georgios c anagnostopoulos"], "accepted": false, "id": "1401.5136"}, "pdf": {"name": "1401.5136.pdf", "metadata": {"source": "META", "title": "A Unifying Framework for Typical Multi-Task Multiple Kernel Learning Problems", "authors": ["Cong Li", "Michael Georgiopoulos", "Georgios C. Anagnostopoulos"], "emails": ["congli@eecs.ucf.edu,", "michaelg@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "In this paper, we present a general Multi-Task Multi-Kernel Learning (Multi-Task MKL) framework, which includes well-known Multi-Task MKL formulations as well as several important MKL approaches to individual tasks. Afterwards, we derive a simple algorithm that can solve the unifying framework. To demonstrate the flexibility of the proposed framework, we formulate a new learning problem, namely the Partially Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its benefits through experiments.Keywords: Multiple Kernel Learning, Multi-Task Learning, Support Vector Machines"}, {"heading": "1 Introduction", "text": "This year, it is as far as it has ever been before, until it is able to retaliate."}, {"heading": "2 Problem Formulation", "text": "Consider a supervised learning task with the parameter \u03b1, which can be expressed in the Formmax functionalities, which can be expressed in the Formmax functionalities. (\u03b1) g (\u03b1, K) (1), where K is the core matrix of the training data sets, i.e., their (i, j) input is K (xi, xj), < \u03c6 (xi), \u03c6 (xj) > H, and H is the Hilbert space, which is implied by the core function k (i, j), by the core function k (\u00b7) and xi, xj (X), where X is an input set. Let us also assume that g) has a finite maximum of the local maxima in relation to the functional set implied by the core function k (\u00b7) and in relation to the individual entries of K. Some known superior learning tasks having these properties are considered in SVM, KRR, SVDD, and One-Class SVM."}, {"heading": "3 Exact Penalty Function Method", "text": "In general, the min-max problem (6) is not easy to solve, but it can be transformed into an equivalent epigraph problem in the following SIP form (8). (7) Before solving the problem (7), we will first show an equivalence between EPF-based problems [21] and general SIP problems, which will ultimately facilitate the development of an algorithm to solve the problem (7). (7) General SIP problem (7) we will look at the general SIP problem in x-based problems (a, x)."}, {"heading": "4 Algorithm", "text": "In this section, we focus on solving the EPF-based problem (10) for our framework. Consider problem (10), for which we define x (1), [2), (3), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (8), (8), (7), (8), (8), (8), (8), (8), (8), (8, 8, 8, 8, 8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, 8, 8, 8, 8, 8, 8, (8), 8, (8), 8, 8, 8, 8, 8 (8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8 (8, 8, 8, 8, 8, 8 (8), 8, 8, 8 (8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8 (8), 8, 8, 8, 8 (8, 8, 8, 8, 8, 8 (8), 8, 8 (8), 8, 8 (8, 8, 8, 8, 8, 8 (8), 8, 8 (8), 8, 8 (8, 8, 8, 8), 8 (8, 8, 8, 8, 8), 8 (8, 8, 8, 8, 8), 8 (8, 8, 8, 8, (8), 8, 8, 8, (8, 8, 8, 8,"}, {"heading": "4.1 Analysis", "text": "The advantages of our algorithm are multifaceted. First, the framework depends on relatively few mild constraints, which are typically met in practice. Specifically, we only assume that g \u00b2 is continuous and doubly differentiable in relation to a problem of limited second order and limited number of local maxims, the value of which is also limited, and g \u00b2 is affinity in relation to the elements of K. There is no need for g \u00b2 to be concave in relation to a first step of the algorithm. Note also that all these constraints are met for the 4 examples of problems 2 to 5. Therefore, the framework and its associated algorithm can enjoy broad applicability. Second, the maximization problem can be divided into T independent problems in relation to a first step of the algorithm, where the commonly encountered regions of support are mutually independent, as in the formulations of [18] and [15]."}, {"heading": "5 The Partially Shared Common Space Model", "text": "In order to demonstrate the flexibility of our framework and the ability of the associated algorithm, we present in this section the complicated q model of a Partial Shared Common Space (PSCS) for Multi-Task MKL as a novel concrete instance of our framework. As already mentioned, in MTL several tasks that share a common representation are trained simultaneously. However, for Multi-Task MKL it is a natural choice to have all tasks share a common core function by having all tasks fall back on a common core function (see Section 2). However, in practice there may be some problems for which sharing a common space is not the optimal choice. For example, an MTL problem may include a few complex tasks, but also some much simpler ones. In this situation, it can be difficult to find a common feature mapping so that all tasks run well. Therefore, it makes sense to leave complex tasks in their own task-specific position, while allowing the remaining tasks to share a common space."}, {"heading": "6 Experimental Results", "text": "In the following subsections, we experimentally evaluate our PSCS model on classification tasks. In order to apply our framework for classification tasks, the objective function g for SVM training is specified as a cross-domain objective function: T \u2211 t = 1 g (\u03b1t, M \u2211 m = 1 \u03b8tmK t m) = T \u2211 t = 1 (\u03b1t \u2032 1 \u2212 \u03b1t \u2032 Y t (M \u2211 m = 1 \u03b8tmK t m) Y t\u03b1t) (20) Note that all core functions in our experiments are used in their normalized form as k (x, y) \u221a k (x, x) k (y, y)."}, {"heading": "6.1 A qualitative case study", "text": "To illustrate the potential of our framework, in this section we apply our approach to the well-known iris flower classification problem, which we will reclassify as an MTL problem; the associated data sets comprise 150 patterns, each of which comes from one of three iris flower classes: Setosa, Versicolor and Virginica (each, classes 1, 2 and 3); each of these 3 classes is represented by 50 examples and each sample, the 4 attributes corresponding to the width and length of the flower's sepal and pedal. We chose only two attributes, namely sepal width and length, to form a two-dimensional dataset, so that the distribution of the patterns and the resulting decision limits can be visualized. Note that each attribute to [0, 1] we split the three-class problem into three binary classification tasks using a one-on-one strategy."}, {"heading": "6.2 Quantitative Analysis on Benchmark Problems", "text": "In this section, we will initially evaluate only our SVM-based PSCS method using 6 > Benchmark Multi-Class Data Sets, which we obtained from the UCI repository. [5] Each associated detection problem was considered a multi-task classification problem by using the one-against-all method, which is an effective method to compare multi-class models; for example, we will refer to [9] and [26]. Specifically, we will use the USPS Handwritten Task Sets (USPS), MNIST Handwritten Digit (MNIST), Wall-Following Robot Navigation (Robot), Statlog Shuttle (Shuttle), Statlog Vehicle Silhouettes (Vehicle Silhouettes), and Letter Recognition (Letter) datasets, which each class is represented by an equal number of samples. An exception is the original Shuttle Data Collection, which we select from seven sets of which we have very poorly represented four classes."}, {"heading": "7 Conclusions", "text": "In this work, we proposed a Multi-Task Multi-Kernel Learning (Multi-Task MKL) framework, which is formulated as a minimum-max problem and encompasses a broad class of kernel-based learning problems. We demonstrated that our formulation can be optimized by solving an Exact Penalty Function (EPF) optimization problem. Subsequently, we derived a simple algorithm to solve this problem, which is capable of benefiting from existing efficient solvers or closed-form solutions for some commonly used learning tasks. The availability of this algorithm eliminates the need to use existing, potentially very sophisticated algorithms or develop algorithms specifically tailored to the problem. In order to demonstrate the usefulness of this novel framework and associated algorithm, we developed the PartiallyShared Common Space (PSCS) Multi-Task MKL model as a special case of our framework, which allows some tasks to be shared with others while allowing them to share a proper feature."}, {"heading": "Acknowledgements", "text": "C. Li acknowledges partial support from National Science Foundation (NSF) grants no. 0806931 and no. 0963146. In addition, M. Georgiopoulos acknowledges partial support from NSF grants no. 0525429, no. 0963146, no. 1200566 and no. 1161228. Finally, G. C. Anagnostopoulos acknowledges partial support from NSF grants no. 0647018. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF. Finally, the authors would like to thank the anonymous reviewers of this manuscript for their time and helpful comments."}], "references": [{"title": "Variable sparsity kernel learning", "author": ["Jonathan Aflalo", "Aharon Ben-Tal", "Chiranjib Bhattacharyya", "Jagarlapudi Saketha Nath", "Sankaran Raman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "l2 regularization for learning kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In UAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Multiple kernel learning algorithms", "author": ["Mehmet Gonen", "Ethem Alpaydin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["Laurent Jacob", "Francis Bach", "Jean-Philippe Vert"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Multitask sparsity via maximum entropy discrimination", "author": ["Tony Jebara"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Non-sparse multiple kernel learning", "author": ["Marius Kloft", "Ulf Brefeld", "Pavel Laskov", "Soren Sonnenburg"], "venue": "In NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["Marius Kloft", "Ulf Brefeld", "Soren Sonnenburg", "Pavel Laskov", "Klaus-Robert Muller", "Alexander Zien"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert R.G. Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "lp\u2212lq penalty for sparse linear and sparse multiple kernel multitask learning", "author": ["Alain Rakotomamonjy", "Remi Flamary", "Gilles Gasso", "Stephane Canu"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Ridge regression learning algorithm in dual variables", "author": ["Craig Saunders", "Alexander Gammerman", "Volodya Vovk"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["Bernhard Sch\u00f6lkopf", "John C. Platt", "John Shawe-Taylor", "Alex J. Smola"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "On multiple kernel learning with multiple labels", "author": ["Lei Tang", "Jianhui Chen", "Jieping Ye"], "venue": "In IJCAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Support vector domain description", "author": ["David M.J. Tax", "Robert P.W. Duin"], "venue": "Pattern Recognition Letters,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Globally convergent methods for semi-infinite programming", "author": ["G.A. Watson"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1981}, {"title": "Multi-task multiple kernel learning (MT-MKL)", "author": ["Christian Widmer", "Nora C. Toussaint", "Yasemin Altun", "Gunnar Ratsch"], "venue": "In NIPS 2010 Workshop: New Directions in Multiple Kernel Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Soft margin multiple kernel learning", "author": ["Xinxing Xu", "I.W. Tsang", "Dong Xu"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Zenglin Xu", "Rong Jin", "Haiqin Yang", "Irwin King", "Michael R. Lyu"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Multi-task learning for classification with dirichlet process priors", "author": ["Ya Xue", "Xuejun Liao", "Laurence Carin", "Balaji Krishnapuram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "According to the Multi-Kernel Learning (MKL) approach [13], which is one of the most popular strategies for learning kernels, multiple predetermined kernels are, most commonly, linearly combined.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "A thorough review of MKL methods and associated algorithms is provided in [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 10, "context": "The earlier work in [13] suggests a MKL formulation with trace constraints over the linearly combined kernels, which is further transformed into a solvable Semi-Definite Programming problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "A similar algorithm is also applied in [23].", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "In [11], an L2-norm constraint is applied to the linear combination coefficients and the proposed min-max formulation is transformed into a Semi-Infinite Programming (SIP) problem, which is then solved via a cutting plane algorithm.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Moreover, two algorithms are proposed in [12] to solve the Lp-MKL formulation, where the coefficient constraint is generalized to an Lp-norm constraint.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "that entails a Group-Lasso regularizer is pointed out in [24], which utilizes a block coordinate descent algorithm to solve the problem in the primal domain.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "Besides MKL-based SVM models, a MKL formulation for Kernel Ridge Regression (KRR)[16] has been proposed in [3].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "Besides MKL-based SVM models, a MKL formulation for Kernel Ridge Regression (KRR)[16] has been proposed in [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "For example, a framework is formulated in [18], where individual tasks utilize their own task-specific space in conjunction with a shared space component, while the balance between these two components is controlled through weights.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "Additionally, in [1], a Group-Lasso regularizer on SVM weights is considered that yields coefficient sparsity within a group of tasks and non-sparse regularization across groups.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "Also, in [15], the Group-Lasso regularizer is generalized to Lp \u2212 Lq regularization and two optimization problems are addressed, one being convex and the other non-convex.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Furthermore, in [22], Multi-Task MKL is formulated in the primal domain by penalizing the SVM weight of each task to be close to a shared common weight.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Finally, maximum entropy discrimination is employed in [9] to construct a Multi-Task MKL framework.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Its unifying character stems from its applicability to prominent kernel-based tasks, such as SVM-based binary classification, KRRbased regression and outlier detection based on One-Class SVM [17] and Support Vector Domain Description (SVDD) [19], to name a few important ones.", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "Its unifying character stems from its applicability to prominent kernel-based tasks, such as SVM-based binary classification, KRRbased regression and outlier detection based on One-Class SVM [17] and Support Vector Domain Description (SVDD) [19], to name a few important ones.", "startOffset": 241, "endOffset": 245}, {"referenceID": 8, "context": "For example, it subsumes L2-MKL and Lp-MKL considered in [11] and [12], the KRR-based MKL in [3] and the Multi-Task MKL formulations in [18] and [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "For example, it subsumes L2-MKL and Lp-MKL considered in [11] and [12], the KRR-based MKL in [3] and the Multi-Task MKL formulations in [18] and [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "For example, it subsumes L2-MKL and Lp-MKL considered in [11] and [12], the KRR-based MKL in [3] and the Multi-Task MKL formulations in [18] and [15].", "startOffset": 93, "endOffset": 96}, {"referenceID": 14, "context": "For example, it subsumes L2-MKL and Lp-MKL considered in [11] and [12], the KRR-based MKL in [3] and the Multi-Task MKL formulations in [18] and [15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "For example, it subsumes L2-MKL and Lp-MKL considered in [11] and [12], the KRR-based MKL in [3] and the Multi-Task MKL formulations in [18] and [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Some examples include the works in [25], [8], [10] and [26], to name a few.", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "Some examples include the works in [25], [8], [10] and [26], to name a few.", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "Some examples include the works in [25], [8], [10] and [26], to name a few.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "For example, for T = 1 it can specialize to Lpnorm MKL [12], where p \u2265 1, by using \u03a8(\u03b8) = {\u03b8 : \u2016\u03b8\u2016p \u2264 1,\u03b8 0}.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Obviously, the L2-norm MKL [11] is also covered by our framework.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "Additionally, it can express the Multi-Task MKL model in [18] that allows individual tasks to utilize their own task-specific space in conjunction with a shared space component.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "To mention a final example, a Group-Lasso type regularizer is employed on the SVM primal-domain weights in [1], which leads to intricate optimization problems and algorithms.", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "Before solving Problem (7), we first show an equivalence between EPF-based problems [21] and general SIP problems.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Then, the EPF P (x) introduced in [21] is defined in a neighborhood of x as:", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "We refer the interested reader to [21] for more details about EPFs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "In order to solve Problem (10), a descent algorithm is suggested in [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "In [21] it is proven that, for sufficiently large \u03bd and sufficiently small step length > 0, the limit point of {xk} is a KKT point of Problem (8), if the sequence {dk} is bounded and the sequence {xk} remains in a bounded region, in which f and g are doubly-differentiable functions with bounded second-order derivatives.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "As suggested in [21], one possibility is to choose k as the largest element of the set {1, \u03b2, \u03b2, \u00b7 \u00b7 \u00b7 }, for some \u03b2, 0 < \u03b2 < 1, such that [P (xk + kdk)\u2212 P (xk)]/ kGk \u2265 \u03c3 (13) where P is the EPF given in Problem (10), \u03c3 is some constant that satisfies 0 < \u03c3 < 1, and Gk is the directional derivative of P with respect to x at the k-th step.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Secondly, the maximization problem with respect to a in the first step of the algorithm can be separated into T independent problems under the commonly-encountered setting, where the feasible regions of the \u03b1\u2019s are mutually independent, such as in the formulations of [18] and [15].", "startOffset": 268, "endOffset": 272}, {"referenceID": 11, "context": "Secondly, the maximization problem with respect to a in the first step of the algorithm can be separated into T independent problems under the commonly-encountered setting, where the feasible regions of the \u03b1\u2019s are mutually independent, such as in the formulations of [18] and [15].", "startOffset": 277, "endOffset": 281}, {"referenceID": 1, "context": "Furthermore, in most situations, solving each of the T problems can be addressed via readily-available efficient optimizers; for example, in the case of SVM problems, one could use LIBSVM [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 14, "context": "Another example is the setting considered in [18], which is of the form of Problem (12) and becomes", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "For example, if within-task sparsity is desired, an L1 \u2212 L2 norm with p = 1 and q = 2 can be utilized, as discussed in [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The time complexity of LIBSVM for solving a SVM or SVDD problem is given in [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 19, "context": "For example, for single-task MKL with an Lp-norm constraint as discussed in [24], a block coordinate descent algorithm is used, which is equivalent to our algorithm with step length equal to 1, when our algorithm is adapted to solve this model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Additionally, for solving the model that is proposed in [18], when 500 samples from the USPS data set are used for training, our algorithm takes on average 9 seconds to train the model, while the algorithm proposed in [18] takes 7 seconds.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Additionally, for solving the model that is proposed in [18], when 500 samples from the USPS data set are used for training, our algorithm takes on average 9 seconds to train the model, while the algorithm proposed in [18] takes 7 seconds.", "startOffset": 218, "endOffset": 222}, {"referenceID": 0, "context": "Note that each attribute is normalized to [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "This is an effective method to test multi-task models; for example, refer to [9] and [26].", "startOffset": 77, "endOffset": 80}, {"referenceID": 20, "context": "The 9 features include four moment-based features, three correlation-based features, one energy ration feature and one spatial variance feature [25].", "startOffset": 144, "endOffset": 148}], "year": 2014, "abstractText": "Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques in the context of kernel-based learning. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Solving different MKL formulations usually involves designing algorithms that are tailored to the problem at hand, which is, typically, a non-trivial accomplishment. In this paper we present a general Multi-Task Multi-Kernel Learning (Multi-Task MKL) framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To demonstrate the flexibility of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation.", "creator": "LaTeX with hyperref package"}}}