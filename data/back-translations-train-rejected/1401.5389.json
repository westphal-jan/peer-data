{"id": "1401.5389", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback", "abstract": "While traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment. Without knowing the users intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the users intention or having a human construct a feature space in an interactive manner during the clustering process. With the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. We demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets.", "histories": [["v1", "Thu, 16 Jan 2014 04:56:03 GMT  (410kb)", "http://arxiv.org/abs/1401.5389v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["sajib dasgupta", "vincent ng"], "accepted": false, "id": "1401.5389"}, "pdf": {"name": "1401.5389.pdf", "metadata": {"source": "CRF", "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback", "authors": ["Sajib Dasgupta", "Vincent Ng"], "emails": ["sajib@hlt.utdallas.edu", "vince@hlt.utdallas.edu"], "sections": [{"heading": "1. Introduction", "text": "It is about the question of whether this is a way in which a user can actually handle documents along other dimensions, such as the condition of the author, age or sensation. In other words, can a text clustering algorithm always create a clustering along the dimensions it wants? The answer to this question depends to a large extent on whether the user can communicate successfully. \"Their intention to cluster algorithms can be achieved by designing a good similarity function that can capture the similarity between a pair of documents so that their ideal clustering can be produced."}, {"heading": "2. Spectral Clustering", "text": "However, it is well known that k-means has the great disadvantage of not being able to separate data points that are not linearly separable in the given attribute space (see e.g. Dhillon, Guan, & Kulis, 2004; Cai, He, & Han, 2005).Since k-means clusters represent documents directly in the given attribute space, which typically comprises hundreds of thousands of features for text applications, their performance could be affected by the curse of dimensionality. Spectral cluster algorithms were developed in response to these problems with k-means. In this section, we will first introduce one of the most commonly used algorithms for spectral clustering (Section 2.1), and then provide the intuition behind spectral clusters (Section 2.2)."}, {"heading": "2.1 Algorithm", "text": "Let us find X = {x1,.., xn} a set of n data points that need to be bundled, s: X \u00b7 X \u2192 a similarity function defined by X, and S a similarity matrix that captures pairs of similarities (i.e., Si, j = s (xi, xj). Like many other cluster algorithms, a spectral cluster algorithm takes S as input and gives a k-way partition C = {C1, C2,.., Ck} (i.e., i = 1Ci = X and i, j: i 6 = j = \u21d2 Ci, Cj =.). Similarly, spectral clustering can be imagined as learning a partitioning function f, which is represented as a separate vector in the rest of this article, so that f (i) is the cluster form v1,."}, {"heading": "2.2 Intuition behind Spectral Clustering", "text": "It may be immediately clear why spectral clusters produce a \"meaningful\" partitioning of a number of points. (There are theoretical reasons behind spectral clustering, but since mathematics is very involved, we will provide only an intuitive rationale for this clustering technique in a way sufficient for the reader to understand our active clustering algorithms in Section 3, and refer the interested reader to Shi and Malik (2000), which can focus our discussion on 2-way clustering in this subsectionism. (Since we will only use spectral clustering to produce a certain number of data points in the rest of this article, we will focus our discussion on 2-way clustering in this subsectionism.) Specifically, a set of data points in an arbitrary feature is presented as an undirected graph where each node corresponds to a data point and the edge between two nodes and their similarity."}, {"heading": "2.3 Clustering with Eigenvectors", "text": "As Ng et al. (2001) points out, \"different authors still do not agree on which eigenvectors to use and how to derive clusters from them.\" In this section, we describe two common methods for determining the eigenvectors to use, and for each method, we show how to derive clusters from the selected eigenvectors, which will serve as the basis for our evaluation."}, {"heading": "2.3.1 Method 1: Using the Second Eigenvector Only", "text": "Since Shi and Malik (2000) show that the second eigenvector, e2, is the approximate solution to the problem of minimizing the normalized section, perhaps it should come as no surprise that e2 is commonly chosen as the only eigenvector to derive a partition. However, since e2 is a real solution to the problem of limited optimization, we must specify how to derive clusters from it. Clustering with e2 is trivial: since we have a linearization of points, one simple way is to determine the threshold for partitioning them. However, we follow Ng et al. (2001) and cluster the points with 2 means in this one-dimensional space."}, {"heading": "2.3.2 Method 2: Using the Top m Eigenvectors", "text": "Let us remember from Section 2.1 that after the self-decomposition of the laplac matrix, each data point is represented by m coordinates. In the second method, we use 2 means to cluster the data points in this m-dimensional space, effectively exploiting all uppermost m eigenvectors. 5. Since f = D is \u2212 1 / 2g, we must pre-multiply the second eigenvector of L by D \u2212 1 / 2 to get the solution to the problem (1), but after Ng et al. (2001) we use the second eigenvector of L directly for clustering, ignoring the term D \u2212 1 / 2."}, {"heading": "3. Our Active Clustering Algorithm", "text": "As mentioned before, sentiment-based clustering is a challenge, in part because ratings can be bundled along multiple dimensions. In this section, we describe our active clustering algorithm, which makes it easy for the user to determine that the dimension along which he wants to cluster the data points is mood. To motivate the importance of user feedback, it helps to understand why the two baseline clustering algorithms described in Section 2.3, which are also based on spectral methods but not on user feedback, do not always lead to sentiment-based clustering. First, consider the first method, which uses only the second eigenvector to induce the partition. Remember that the second eigenvector reveals the most prominent dimension of the data, so if the mood is not the most prominent dimension (though the review can always be a eigenvector line that does not contain sentiment words)."}, {"heading": "3.1 Step 1: Identify the Important Clustering Dimensions", "text": "We rely on a simple method of identifying the important cluster dimensions of a given collection of texts: we use the uppermost eigenvectors of laplazium as important cluster dimensions. This method is motivated by the fact that e2, the second eigenvector of laplazium, is the optimal real solution for the objective function that minimizes the spectral cluster (i.e., normalized section, Shi & Malik, 2000) and is therefore an optimal cluster dimension. More importantly, we use a rarely used observation discussed in Section 2.2: While the remaining eigenvectors are all suboptimal solutions (where one is more suboptimal than i increases), the uppermost eigenvectors (i.e. those with small i values), because they are less suboptimal, can still provide reasonably good (though not optimal) clustering of the data and thus serve as good cluster dimensions."}, {"heading": "3.2 Step 2: Identify the Relevant Features for Each Partition", "text": "The next question is: How can we determine what dimension of user interest is captured? One way to do this is that the user must read a large number of reviews in order to make a decision. To reduce human efforts, we use an alternative method: We (1) identify the most informative features for characterizing each partition, and (2) have users only reviewed the characteristics of the reviews in order to make a decision. To reduce human efforts, we employ an alternative method: We (1) identify the most informative features for characterizing each partition, and (2) have users only inspect the characteristics, instead of identifying the ratings, the characteristics should simply be identified for a person so that they are useful for distinguishing the ratings in the two clusters. To identify and rank the informative features, we use a method that we call upon."}, {"heading": "3.3 Step 3: Identify the Unambiguous Reviews", "text": "However, there is a caveat. As mentioned in the introduction, many ratings contain both positive and negative mood-enhancing words. These ambiguous ratings are more likely to be misgrouped than their unambiguous counterparts. As the character rankings are derived from each partition, the presence of these ambiguous ratings can negatively affect the identification of the informative characteristics using MMFR. As a result, we remove the ambiguous ratings before deriving informative characteristics from a partition. We use a simple method of identifying unambiguous ratings. In calculating the eigenvalues, all the data points point to the orthogonal predictions of the other data points with which they have an affinity. Ambiguous data points get the orthogonal predictions from both the positive and negative data points, and therefore they have almost zero values in the swivelling eigenvectors. In other words, the points with almost zero values in the eigenvectors are more ambiguous than those with large values, so we can sort the respective values upwards and only the corresponding eigenvectors."}, {"heading": "3.4 Step 4: Cluster Along the Selected Eigenvector", "text": "Finally, we use 2 means to group all ratings along the eigenvector selected by the user, regardless of whether a rating is ambiguous or not."}, {"heading": "4. Evaluation", "text": "In this section, we describe experiments aimed at evaluating the effectiveness of our active cluster algorithm and providing insights into it."}, {"heading": "4.1 Experimental Setup", "text": "9. Note that 25% is a somewhat arbitrary choice, based only on the assumption that a fraction of the ratings are unambiguous. As we will see in the evaluation section, these ratings can be classified with high accuracy based on their polarity; consequently, the characteristics resulting from the resulting clusters are also of high quality. Further experiments showed that the list of top ranking characteristics does not change significantly if they are derived from a smaller number of unambiguous ratings."}, {"heading": "4.1.1 Datasets", "text": "We use five sentiment datasets, including the widely used film review dataset [MOV] (Pang, Lee, & Vaithyanathan, 2002) and four datasets of reviews of four different product types from Amazon [Books (BOO), DVDs (DVD), Electronics (ELE), and Kitchen Appliances (KIT)] (Blitzer, Dredze, & Pereira, 2007)."}, {"heading": "4.1.2 Document Preprocessing", "text": "To pre-edit a document, we first sort it by the frequency of the document and then shrink it, and then render it as a vector of unoriginated unique items, each of which takes a value of 1 or 0 indicating its presence or absence in the document. In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in a single review. A preliminary examination of our evaluation records shows that these words typically make up 1-2% of a vocabulary. Deciding exactly how many terms to remove from each record is subjective: a large corpus typically requires more distances than a small corpus. To be consistent, we simply sort the vocabulary by the frequency of the document and remove the top 1.5%."}, {"heading": "4.1.3 Spectral Learning Setup", "text": "According to common practice in spectral learning for text domains (e.g. Kamvar, Klein, & Manning, 2003; Cai et al., 2005), we calculate the similarity of two reviews by taking the point product of their characteristic vectors. As in the spectral cluster algorithm of Ng et al. (2001), we set the diagonal entries of the similarity matrix to 0. Furthermore, we set m to 5. In other words, we consider the second to fifth eigenvectors, provided that they are sufficient to capture the desired clusters. 10"}, {"heading": "4.1.4 Evaluation Metrics", "text": "We use two evaluation yardsticks: First, we give the results for each data set in terms of accuracy, that is, the percentage of documents for which the label assigned by our system is identical to the gold standard label. Second, according to Kamvar et al. (2003), we evaluate the clusters produced by our approach using the Adjusted Rand Index (ARI), which is the randomized version of the Rand Index. Specifically, for a set of N data points and two clusters of these points, U and V, 10. Note that setting m to 5 is a somewhat arbitrary choice and that any number of eigenvectors can be used in our active clustering algorithm."}, {"heading": "4.2 Baseline Systems", "text": "In this section, we describe our baseline results: The first two baseline systems are those described in Section 2.3, and the last two are probably more complex clustering algorithms used to strengthen our baseline results."}, {"heading": "4.2.1 Clustering Using the Second Eigenvector Only", "text": "Since our first baseline follows the approach of Shi and Malik (2000) and summarizes the assessments using only the second eigenvector e2, as described in Section 2.3.Results on POL and sentiment datasets, expressed in terms of accuracy and ARI, are presented in row 1 of Tables 2a and 2b, respectively. Due to the randomness in selecting seeds for 2-means, these and all other experimental results with 2-means are viewed on average over ten independent valuations.11 As we can see, this baseline achieves an accuracy of 93.7% for POL, but much lower accuracy (of 50-70%) for sentiment datasets. The same performance trend can be observed in ARI. These results provide suggestive evidence that the production of sentimental clustering requires characteristics other than the production of theme-based clustering, and that in many cases the more striking features of data analysis are more thematic."}, {"heading": "4.2.2 Clustering Using the Top Five Eigenvectors", "text": "As our second baseline, we represent each data point using the five best eigenvectors (i.e., e1 to e5) and cluster them by two means in this five-dimensional space, as described in Section 2.3. Therefore, we can imagine this as an \"ensemble approach\" in which the cluster decision is made collectively by the five eigenvectors. Results are presented in row 2 of Tables 2a and 2b.13, with improvements in accuracy and ARI for POL and the three sentiment datasets where the first baseline performs poorly (i.e. BOO, DVD and ELE) compared to the first baseline, with the most drastic improvements to ELE observed. However, performance on the remaining two sentiment datasets deteriorates. These results can be attributed to the fact that for BOO, DVD and ELE, e2 the cluster dimensions do not capture the sentient dimension, but some other eigenvectors within the ensemble have improvements."}, {"heading": "4.2.3 Clustering Using the Interested Reader Model", "text": "Our third baseline is Kamvar et al.'s (2003), an unattended cluster algorithm that, according to the authors, is ideal for text clusters and has recently proven to be a special case of ratio-cut optimization (Kulis, Basu, Dhillon, & Mooney, 2009). Specifically, they are introducing a new laplace inspired by the \"Interested Reader Model.\" This laptop is calculated as (S + dmaxI \u2212 D) / dmax, where D and S are defined as in Section 2.1, except that Si, j = 0, if i is not one of j's closest neighbors and j is not one of i's closest neighbors to k; dmax is the maximum sum of S; and I is the identity matrix. As its performance is highly sensitive to k, we have tested values of 10, 15,... and reported the best results in row 3 of Tables 2a and 2b."}, {"heading": "4.2.4 Clustering Using Non-Negative Matrix Factorization", "text": "Non-negative matrix factorization (NMF) has recently been proven effective for document clustering by Xu, Liu, and Gong (2003). After re-implementing this algorithm, we evaluate it on our six datasets. 14 row 4 of Tables 2a and 2b shows the best results obtained after running the algorithm five times. Compared to the first baseline, NMF achieves better performance on ELE, comparable performance on MOV, and worse performance on the remaining datasets."}, {"heading": "4.3 Our Active Clustering Algorithm", "text": "In this section we describe human and automatic experiments to evaluate our active cluster algorithm."}, {"heading": "4.3.1 Human Experiments", "text": "Unlike the four baselines, our active cluster algorithm requires users to indicate which of the four dimensions (defined by the second to fifth eigenvectors) are most closely related to feeling, by examining a series of characteristics derived from the unique ratings for each dimension using MMFR. To better understand how easy it is for a person to select the desired dimension based on the characteristics, we conducted the experiment independently with five people (all of whom are computer science students not associated with this research) and calculated the match rate. Specifically, for each data set we showed the top 100 characteristics for each cluster according to MMFR (see Tables 3-8 for a subset of these 100 characteristics induced for each of the six datasets, with the slightly divided columns corresponding to the sensing dimensions chosen by the majority of human judges)."}, {"heading": "4.3.2 Clustering Results", "text": "The cluster results are presented in row 5 of tables 2a and 2b. Compared to the best starting position for each data set, we see that our algorithm performs much better on BOO, DVD, and ELE, at almost the same level on MOV and KIT, but slightly worse on POL. Note that the improvements observed for BOO, DVD, and ELE are due to the failure of e2 to capture the sentiment dimension."}, {"heading": "4.3.3 Identification of Unambiguous Documents", "text": "To get an idea of how accurate our algorithm for identifying unique documents is, we show in Table 10 the accuracy achieved when the unique documents in each data set were clustered using the eigenvector selected by the majority of judges. As we can see, the accuracy on each data set is higher than the corresponding accuracy shown in row 5 of Table 2a. In fact, an accuracy of more than 85% was achieved on All16. As in the first baseline, since we are in a one-dimensional space, the results are not sensitive to seed selection and result in zero deviation from the ten independent runs. But one data set. This indicates that our method of identifying unique documents is reasonably accurate. Note that it is crucial to achieve a high accuracy on the unique documents: if the accuracy is low, the characteristics generated from this part that assign to the human data may not be accurate enough to assess the fact."}, {"heading": "4.3.4 User Feedback Versus Labeled Data", "text": "Let's remember that our four baselines are unsupervised, while our algorithm can be characterized as semi-supervised because it relies on user feedback to select the desired dimension. Therefore, it should come as no surprise to see that the average cluster performance of our algorithm is better than that of the baselines. To make a fairer comparison, we are conducting another experiment in which we compare our algorithm to a semi-supervised mood classification system that uses a transductive SVM as the underlying semi-supervised learner. Specifically, the aim of this experiment is to determine how many labeled documents are needed for the transductive learner to achieve the same level of performance as our algorithm. To answer this question, we first give the transductive learner access to the 2000 documents for each set of data as unlabeled data. Next, we randomly sample 50 unlabeled documents and assign them the true label."}, {"heading": "4.3.5 Multiple Relevant Eigenvectors", "text": "As shown in Table 9, some human judges chose more than one eigenvector for some datasets (e.g. {2,3,4} for POL; {2,4} for MOV; and {3,4} for ELE). To better understand whether these additional eigenvectors can help improve accuracy and ARI, we conduct another experiment in which we use 2-means to group the documents in the space defined by all the selected eigenvectors. Table 12 shows the accuracy and ARI results averaged over ten independent gradients. As we can see, the results for POL are considerably better than those obtained when only the highest eigenvector is used, suggesting that the additional eigenvectors contain useful information. However, the results for MOV and ELE slightly decrease with the addition of the additional eigenvectors, indicating that the additional mood dimensions are not useful."}, {"heading": "4.3.6 Supervised Classification Results", "text": "Next, we present results for a supervised classification of our five sentiment datasets. Although we should not expect our largely unsupervised approach to perform as well as a fully supervised approach, we believe that fully supervised results give the reader a sense of where our work stands between the existing work to identify the mood in these datasets. Specifically, in Table 13, we report an average of ten-fold cross-validation accuracy, where an SVM classifier trains in nine folds and is tested for the remaining fold in each fold experiment. We see that our results lag behind monitored results by 8.1-15.2%."}, {"heading": "4.4 Alternative Document Representations", "text": "In fact, most of them are able to survive on their own."}, {"heading": "4.5 Domain Adaptation", "text": "As mentioned in the introduction, the majority of existing approaches to sentiment classification are monitored. A weakness of these monitored approaches is that when you specify a new domain, you have to go through the costly process of collecting a large amount of commented data in order to learn an accurate polarity classification, you can argue that our active clustering algorithm suffers from the same weakness: the user has to automatically identify the sensation dimension for each domain. One way to address this weakness is through domain adaptation. Specifically, we are investigating whether the sensation dimension can be manually identified for a domain (henceforth the source) in order to automatically identify the sensation dimension for a new domain (henceforth the target domain). We hypothesize that domain adaptation is feasible, especially if the two domains are sentimentally similar (i.e. there is a significant overlap between the characteristics that characterize the two dimensions)."}, {"heading": "4.6 Subjectivity Lexicon versus Human Feedback", "text": "It could be argued that if we had access to a subjectivity dictionary, we could use it to automatically identify the correct sensory dimension, eliminating the need for human feedback as a whole. (In this subsection, we will examine whether it is actually possible to use a hand-built general purpose dictionary to identify the eigenvector corresponding to the sensory dimension in a new domain. (For our experiment, we will use the subjectivity dictionary L, which is described in Section 4.4. As mentioned above, L contains all and only these words in Wilson et al.'s (2005) Subjectivity dictionary, which is characterized by a previous polarity of positive or negative. The method of automatically identifying the sensory dimension using L is similar to the icon described in the domain adaptation section: for each of the second to fifth eigenvectors, we first calculate the similarity between the eigenvector and L, and then choose the eigenvector, as the most similar to L."}, {"heading": "4.7 Single Data, Multiple Clusterings", "text": "This year, it has come to the point where it will be able to take the lead, at a time when it is not as far away as it has been in recent years, when it has never been so far."}, {"heading": "1 3 4,5 3 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.7.1 Human Experiments", "text": "This year, it is time to put yourself in a position to take the lead."}, {"heading": "4.7.2 Clustering Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "4.7.3 Multiple Relevant Eigenvectors", "text": "Let us remember from Table 18b that for each of the four extended data sets, there is at least one judge indicating that more than one eigenvector is relevant for the sentiment dimension. However, in creating the mood-based cluster results using our system in Table 23b, to use only the eigenvector most frequently evaluated by human judges, to better understand whether the use of more relevant eigenvectors can help improve the results for sentiment-based clusters, we repeat the experiment in which we use 2-means to cluster the documents in the room defined by all eigenvectors determined as relevant by at least one judge. Specifically, we cluster with the following set of eigenvectors: {3.4} for MOV-DVD, {4.5} for BOO-DVD, {3.5} for DVD-ELE and {3.5} for MOV-KIT. The accuracy and ARI results of this experiment are shown in Table 24, but in comparison to the last set of eigenvectors, we can only see that the BOO results for the last series of eigenvectors are selected from Table 23b Sentilators."}, {"heading": "5. Significance of Our Work", "text": "We believe that our approach is much better than would otherwise be possible. 1. The production of clustering according to the user interest 3. We have proposed a novel framework in which we have enabled a spectral cluster algorithm to take into account human feedback and produce clustering along the dimension of interest to the user. A particularly appealing aspect of our approach concerns the relatively minimal human feedback, in which the user only needs to take a cursory look at a small number of features that are representative of each induced topic. It is worth noting that human inspection and selection is a new form of interaction between a human and a cluster-like algorithm task. It allows a person to easily engage in different cluster tasks in order to improve their performance in a simple, subordinate way. We believe that our approach belongs to an emerging family of interactive algorithms that make small, conductive tweaks while achieving results that would be feasible in the future."}, {"heading": "6. Related Work", "text": "In this section, we focus on discussing related work on topic-based clustering and classification, sentiment classification, active learning, and the production of multiple clustering in computer-based styles. In this section, we fail to discuss the related work on topic-based clustering and classification methods, which focus primarily on topic-based clustering, largely due to DARPA's Topic Detection and Tracking initiative in the 1990s. Many different clustering algorithms have been used, including non-hierarchical algorithms such as k-means and Expectation-Maximization (EM) and hierarchical algorithms such as single-link, complete-link, and tracking-average, and Singlepass (Hatzivassiloglou, Gravano, & Maganti, 2000), which contain a set of documents in a feature space typically spanned from all sides."}, {"heading": "7. Conclusions and Future Work", "text": "This is why we are embarking on a search for a way that allows us to identify the dimension of data intended by users, possibly hidden, and produce the desired clustering, which differs from competing methods in that it requires very limited feedback: in order to select the intended dimension, the user only needs to examine a small number of characteristics. We demonstrated its viability through a series of human and automatic experiments with the challenging but not yet studied task of emotional clustering, which yields promising results. Further experiments provided suggestive evidence that (1) the adaptation can be successfully applied to identify the sensitivity of a new domain when viewed in a similar way."}, {"heading": "Acknowledgments", "text": "We thank the four anonymous reviewers for their helpful comments and for unanimously recommending this article for publication in JAIR. Any opinions, findings, conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views or official guidelines of NSF, neither express nor implied."}], "references": [{"title": "Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums", "author": ["A. Abbasi", "H. Chen", "A. Salem"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Abbasi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abbasi et al\\.", "year": 2008}, {"title": "Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses", "author": ["A. Andreevskaia", "S. Bergler"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Andreevskaia and Bergler,? \\Q2006\\E", "shortCiteRegEx": "Andreevskaia and Bergler", "year": 2006}, {"title": "Clustering with interactive feedback", "author": ["Balcan", "M.-F", "A. Blum"], "venue": "In Proceedings of the 19th International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Multilingual subjectivity analysis using machine translation", "author": ["C. Banea", "R. Mihalcea", "J. Wiebe", "S. Hassan"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Banea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2008}, {"title": "Interactive clustering of text collections according to a user-specified criterion", "author": ["R. Bekkerman", "H. Raghavan", "J. Allan", "K. Eguchi"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Bekkerman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bekkerman et al\\.", "year": 2007}, {"title": "Towards a taxonomy of web registers and text types: A multidimensional analysis", "author": ["D. Biber", "J. Kurjian"], "venue": "Language and Computers,", "citeRegEx": "Biber and Kurjian,? \\Q2006\\E", "shortCiteRegEx": "Biber and Kurjian", "year": 2006}, {"title": "Integrating constraints and machine learning in semi-supervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Bilenko et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bilenko et al\\.", "year": 2004}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Document clustering using locality preserving indexing", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Cai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2005}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["P.K. Chan", "D.F. Schlag", "J.Y. Zien"], "venue": "IEEE Transactions on Computer-Aided Design,", "citeRegEx": "Chan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Chan et al\\.", "year": 1994}, {"title": "Domain adaptation with active learning for word sense disambiguation", "author": ["Y.S. Chan", "H.T. Ng"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chan and Ng,? \\Q2007\\E", "shortCiteRegEx": "Chan and Ng", "year": 2007}, {"title": "Learning with compositional semantics as structural inference for subsentential sentiment analysis", "author": ["Y. Choi", "C. Cardie"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Choi and Cardie,? \\Q2008\\E", "shortCiteRegEx": "Choi and Cardie", "year": 2008}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification", "author": ["S. Dasgupta", "V. Ng"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),", "citeRegEx": "Dasgupta and Ng,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta and Ng", "year": 2009}, {"title": "Topic-wise, sentiment-wise, or otherwise? Identifying the hidden dimension for unsupervised text classification", "author": ["S. Dasgupta", "V. Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Dasgupta and Ng,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta and Ng", "year": 2009}, {"title": "Mining clustering dimensions", "author": ["S. Dasgupta", "V. Ng"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Dasgupta and Ng,? \\Q2010\\E", "shortCiteRegEx": "Dasgupta and Ng", "year": 2010}, {"title": "Towards subjectifying text clustering", "author": ["S. Dasgupta", "V. Ng"], "venue": "In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Dasgupta and Ng,? \\Q2010\\E", "shortCiteRegEx": "Dasgupta and Ng", "year": 2010}, {"title": "Domain adaptation for statistical classifiers", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "III and Marcu,? \\Q2006\\E", "shortCiteRegEx": "III and Marcu", "year": 2006}, {"title": "Finding alternative clusterings using constraints", "author": ["I. Davidson", "Z. Qi"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "Davidson and Qi,? \\Q2007\\E", "shortCiteRegEx": "Davidson and Qi", "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of American Society of Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Kernel k-means, spectral clustering and normalized cuts", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Dhillon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2004}, {"title": "A min-max cut algorithm for graph partitioning and data clustering", "author": ["C. Ding", "X. He", "H. Zha", "M. Gu", "H.D. Simon"], "venue": "In Proceedings of the 2001 International Conference on Data Mining (ICDM),", "citeRegEx": "Ding et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2001}, {"title": "Active learning by labeling features", "author": ["G. Druck", "B. Settles", "A. McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Druck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Druck et al\\.", "year": 2009}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Duam\u00e9"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Duam\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Duam\u00e9", "year": 2007}, {"title": "Learning to classify documents according to genre", "author": ["A. Finn", "N. Kushmerick"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Finn and Kushmerick,? \\Q2006\\E", "shortCiteRegEx": "Finn and Kushmerick", "year": 2006}, {"title": "The disputed Federalist Papers: SVM feature selection via concave minimization", "author": ["G. Fung"], "venue": "In Proceedings of the 2003 Conference on Diversity in Computing,", "citeRegEx": "Fung,? \\Q2003\\E", "shortCiteRegEx": "Fung", "year": 2003}, {"title": "Knowledge transfer via multiple model local structure mapping", "author": ["J. Gao", "W. Fan", "J. Jiang", "J. Han"], "venue": "In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Gao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2008}, {"title": "Modeling latent biographic attributes in conversational genres", "author": ["N. Garera", "D. Yarowsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),", "citeRegEx": "Garera and Yarowsky,? \\Q2009\\E", "shortCiteRegEx": "Garera and Yarowsky", "year": 2009}, {"title": "Margin based feature selection \u2013 theory and algorithms", "author": ["R. Gilad-Bachrach", "A. Navot", "N. Tishby"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Gilad.Bachrach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gilad.Bachrach et al\\.", "year": 2004}, {"title": "Non-redundant data clustering", "author": ["D. Gondek", "T. Hofmann"], "venue": "In Proceedings of the 4th IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "Gondek and Hofmann,? \\Q2004\\E", "shortCiteRegEx": "Gondek and Hofmann", "year": 2004}, {"title": "The envelope of variation in multidimensional register and genre analyses", "author": ["A. Grieve-Smith"], "venue": "Language and Computers,", "citeRegEx": "Grieve.Smith,? \\Q2006\\E", "shortCiteRegEx": "Grieve.Smith", "year": 2006}, {"title": "An investigation of linguistic features and clustering algorithms for topical document clustering", "author": ["V. Hatzivassiloglou", "L. Gravano", "A. Maganti"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Hatzivassiloglou et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hatzivassiloglou et al\\.", "year": 2000}, {"title": "Locality preserving indexing for document representation", "author": ["X. He", "D. Cai", "H. Liu", "Ma", "W.-Y"], "venue": "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "He et al\\.,? \\Q2004\\E", "shortCiteRegEx": "He et al\\.", "year": 2004}, {"title": "Locality discriminating indexing for document classification", "author": ["J. Hu", "W. Deng", "J. Guo", "W. Xu"], "venue": "In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) (Poster),", "citeRegEx": "Hu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2007}, {"title": "Mining opinion features in customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Simultaneous unsupervised learning of disparate clusterings", "author": ["P. Jain", "R. Meka", "I.S. Dhillon"], "venue": "In Proceedings of SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["J. Jiang", "C. Zhai"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Jiang and Zhai,? \\Q2007\\E", "shortCiteRegEx": "Jiang and Zhai", "year": 2007}, {"title": "A two-stage approach to domain adaptation for statistical classifiers", "author": ["J. Jiang", "C. Zhai"], "venue": "In Proceedings of the 16th Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Jiang and Zhai,? \\Q2007\\E", "shortCiteRegEx": "Jiang and Zhai", "year": 2007}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "In Scholkopf,", "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceedings of the 16th International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Extracting social meaning: Identifying interactional style in spoken conversation", "author": ["D. Jurafsky", "R. Ranganath", "D. McFarland"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT),", "citeRegEx": "Jurafsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2009}, {"title": "Spectral learning", "author": ["S. Kamvar", "D. Klein", "C. Manning"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kamvar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kamvar et al\\.", "year": 2003}, {"title": "On clusterings: Good, bad and spectral", "author": ["R. Kannan", "S. Vempala", "A. Vetta"], "venue": "Journal of the ACM,", "citeRegEx": "Kannan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2004}, {"title": "Sentiment classifiation of movie reviews using contextual valence shifters", "author": ["A. Kennedy", "D. Inkpen"], "venue": "Computational Intelligence,", "citeRegEx": "Kennedy and Inkpen,? \\Q2006\\E", "shortCiteRegEx": "Kennedy and Inkpen", "year": 2006}, {"title": "Computational methods in authorship attribution", "author": ["M. Koppel", "J. Schler", "S. Argamon"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Koppel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koppel et al\\.", "year": 2009}, {"title": "Feature subset selection for support vector machines using confident margin", "author": ["M. Kugler", "K. Aoki", "S. Kuroyanagi", "A. Iwata", "A. Nugroho"], "venue": "In Proceedings of the 2005 IEEE International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Kugler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kugler et al\\.", "year": 2005}, {"title": "Semi-supervised graph-based clustering: A kernel approach", "author": ["B. Kulis", "S. Basu", "I. Dhillon", "R. Mooney"], "venue": "Machine Learning,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge", "author": ["T. Li", "Y. Zhang", "V. Sindhwani"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Multiple sets of features for automatic genre classification of web documents", "author": ["C. Lim", "K. Lee", "G. Kim"], "venue": "Information Processing and Management,", "citeRegEx": "Lim et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2005}, {"title": "Spectral domain-transfer learning", "author": ["X. Ling", "W. Dai", "G. Xue", "Q. Yang", "Y. Yu"], "venue": "In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Ling et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2008}, {"title": "Text classification by labeling words", "author": ["B. Liu", "X. Li", "W.S. Lee", "P.S. Yu"], "venue": "In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Employing EM and pool-based active learning for text classification", "author": ["A.K. McCallum", "K. Nigam"], "venue": "In Proceedings of the 15th International Conference on Machine Learning (ICML),", "citeRegEx": "McCallum and Nigam,? \\Q1998\\E", "shortCiteRegEx": "McCallum and Nigam", "year": 1998}, {"title": "Structured models for fine-to-coarse sentiment analysis", "author": ["R. McDonald", "K. Hannan", "T. Neylon", "M. Wells", "J. Reynar"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),", "citeRegEx": "McDonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2007}, {"title": "Sentiment mixture: Modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra", "H. Su", "C. Zhai"], "venue": "In Proceedings of the 16th World Wide Web Conference (WWW),", "citeRegEx": "Mei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2007}, {"title": "Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus", "author": ["S. Mohammad", "C. Dunne", "B. Dorr"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Mohammad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2009}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A. Ng", "M. Jordan", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine Learning,", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Pang and Lee,? \\Q2008\\E", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Thumbs up? Sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Contextual valence shifters", "author": ["L. Polanyi", "A. Zaenen"], "venue": null, "citeRegEx": "Polanyi and Zaenen,? \\Q2006\\E", "shortCiteRegEx": "Polanyi and Zaenen", "year": 2006}, {"title": "An interactive algorithm for asking and incorporating feature feedback into support vector machines", "author": ["H. Raghavan", "J. Allan"], "venue": "In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Raghavan and Allan,? \\Q2007\\E", "shortCiteRegEx": "Raghavan and Allan", "year": 2007}, {"title": "Semi-supervised polarity lexicon induction", "author": ["D. Rao", "D. Ravichandran"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Rao and Ravichandran,? \\Q2009\\E", "shortCiteRegEx": "Rao and Ravichandran", "year": 2009}, {"title": "Learning extraction patterns for subjective expressions", "author": ["E. Riloff", "J. Wiebe"], "venue": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Riloff and Wiebe,? \\Q2003\\E", "shortCiteRegEx": "Riloff and Wiebe", "year": 2003}, {"title": "Interactive feature space construction using semantic information", "author": ["D. Roth", "K. Small"], "venue": "In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Roth and Small,? \\Q2009\\E", "shortCiteRegEx": "Roth and Small", "year": 2009}, {"title": "On the use of linear programming for unsupervised text classification", "author": ["M. Sandler"], "venue": "In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Sandler,? \\Q2005\\E", "shortCiteRegEx": "Sandler", "year": 2005}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys,", "citeRegEx": "Sebastiani,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Document-word co-regularization for semi-supervised sentiment analysis", "author": ["V. Sindhwani", "P. Melville"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "Sindhwani and Melville,? \\Q2008\\E", "shortCiteRegEx": "Sindhwani and Melville", "year": 2008}, {"title": "The effect of OCR errors on stylistic text classification", "author": ["S. Stein", "S. Argamon", "O. Frieder"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR) (Poster),", "citeRegEx": "Stein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Stein et al\\.", "year": 2006}, {"title": "Employing thematic variables for enhancing classification accuracy within author discrimination experiments", "author": ["G. Tambouratzis", "M. Vassiliou"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Tambouratzis and Vassiliou,? \\Q2007\\E", "shortCiteRegEx": "Tambouratzis and Vassiliou", "year": 2007}, {"title": "Adapting naive Bayes to domain adaptation for sentiment analysis", "author": ["S. Tan", "X. Cheng", "Y. Wang", "H. Xu"], "venue": "In Proceedings of the 31st European Conference on Information Retrieval (ECIR),", "citeRegEx": "Tan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2009}, {"title": "Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews", "author": ["P. Turney"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turney,? \\Q2002\\E", "shortCiteRegEx": "Turney", "year": 2002}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "In Proceedings of the 18th International Conference on Machine Learning (ICML),", "citeRegEx": "Wagstaff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wagstaff et al\\.", "year": 2001}, {"title": "Using bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis", "author": ["X. Wan"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wan,? \\Q2008\\E", "shortCiteRegEx": "Wan", "year": 2008}, {"title": "Segmentation using eigenvectors: A unifying view", "author": ["Y. Weiss"], "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),", "citeRegEx": "Weiss,? \\Q1999\\E", "shortCiteRegEx": "Weiss", "year": 1999}, {"title": "Learning subjective language", "author": ["J.M. Wiebe", "T. Wilson", "R. Bruce", "M. Bell", "M. Martin"], "venue": "Computational Linguistics,", "citeRegEx": "Wiebe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2004}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J.M. Wiebe", "P. Hoffmann"], "venue": "In Proceedings of the Joint Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "An optimal graph theoretic appproach to data clustering and its application to image segmentation", "author": ["Z. Wu", "R.M. Leahy"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wu and Leahy,? \\Q1993\\E", "shortCiteRegEx": "Wu and Leahy", "year": 1993}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Yang and Liu,? \\Q1999\\E", "shortCiteRegEx": "Yang and Liu", "year": 1999}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In Proceedings of the 14th International Conference on Machine Learning (ICML),", "citeRegEx": "Yang and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}], "referenceMentions": [{"referenceID": 75, "context": ", Weiss, 1999; Shi & Malik, 2000; Kannan, Vempala, & Vetta, 2004), we adopt the one proposed by Ng, Jordan, and Weiss (2001), as it is arguably the most widely-used.", "startOffset": 2, "endOffset": 125}, {"referenceID": 57, "context": "We follow Ng et al. (2001) and employ a normalized dual form of the usual Laplacian D \u2212 S.", "startOffset": 10, "endOffset": 27}, {"referenceID": 68, "context": "There are theoretical justifications behind spectral clustering, but since the mathematics is quite involved, we will only provide an intuitive justification of this clustering technique in a way that is sufficient for the reader to understand our active clustering algorithm in Section 3, and refer the interested reader to Shi and Malik\u2019s (2000) seminal paper on spectral clustering for details.", "startOffset": 325, "endOffset": 348}, {"referenceID": 78, "context": "One problem with minimizing the cut value, as noticed by Wu and Leahy (1993), is that this objective favors producing unbalanced clusters in which one of them contains a very small number of nodes.", "startOffset": 57, "endOffset": 77}, {"referenceID": 68, "context": "As mentioned by Shi and Malik (2000), this should not be surprising, since the number of edges involved in the cut (and hence the cut value) tends to increase as the sizes of the two clusters become relatively balanced.", "startOffset": 16, "endOffset": 37}, {"referenceID": 68, "context": "As mentioned by Shi and Malik (2000), this should not be surprising, since the number of edges involved in the cut (and hence the cut value) tends to increase as the sizes of the two clusters become relatively balanced. A closer examination of the minimum cut criterion reveals the problem: while it minimizes inter-cluster similarity, it makes no attempt to maximize intra-cluster similarity. To address this weakness, Shi and Malik (2000) propose to minimize instead the normalized cut value, NCut, which takes into account both inter-cluster dissimilarity and intra-cluster similarity.", "startOffset": 16, "endOffset": 441}, {"referenceID": 57, "context": "As Ng et al. (2001) point out, \u201cdifferent authors still disagree on which eigenvectors to use, and how to derive clusters from them\u201d.", "startOffset": 3, "endOffset": 20}, {"referenceID": 67, "context": "Since Shi and Malik (2000) show that the second eigenvector, e2, is the approximate solution to the problem of minimizing the normalized cut, it should perhaps not be surprising that e2 is commonly chosen as the only eigenvector for deriving a partition.", "startOffset": 6, "endOffset": 27}, {"referenceID": 57, "context": "However, we follow Ng et al. (2001) and cluster the points using 2-means in this one-dimensional space.", "startOffset": 19, "endOffset": 36}, {"referenceID": 57, "context": "In fact, since f = Dg, we have to pre-multiply the second eigenvector of L by D to get the solution to Problem (1), but following Ng et al. (2001), we employ the second eigenvector of L directly for clustering, ignoring the term D.", "startOffset": 130, "endOffset": 147}, {"referenceID": 27, "context": "See, for instance, the work of Fung (2003), Gilad-Bachrach, Navot, and Tishby (2004), and Kugler, Aoki, Kuroyanagi, Iwata, and Nugroho (2005) for details.", "startOffset": 31, "endOffset": 43}, {"referenceID": 27, "context": "See, for instance, the work of Fung (2003), Gilad-Bachrach, Navot, and Tishby (2004), and Kugler, Aoki, Kuroyanagi, Iwata, and Nugroho (2005) for details.", "startOffset": 31, "endOffset": 85}, {"referenceID": 27, "context": "See, for instance, the work of Fung (2003), Gilad-Bachrach, Navot, and Tishby (2004), and Kugler, Aoki, Kuroyanagi, Iwata, and Nugroho (2005) for details.", "startOffset": 31, "endOffset": 142}, {"referenceID": 10, "context": "Following common practice in spectral learning for text domains (e.g., Kamvar, Klein, & Manning, 2003; Cai et al., 2005), we compute the similarity between two reviews by taking the dot product of their feature vectors.", "startOffset": 64, "endOffset": 120}, {"referenceID": 10, "context": ", Kamvar, Klein, & Manning, 2003; Cai et al., 2005), we compute the similarity between two reviews by taking the dot product of their feature vectors. As in Ng et al.\u2019s (2001) spectral clustering algorithm, we set the diagonal entries of the similarity matrix to 0.", "startOffset": 34, "endOffset": 176}, {"referenceID": 43, "context": "Second, following Kamvar et al. (2003), we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI), which is the corrected-for-chance version of the Rand Index.", "startOffset": 18, "endOffset": 39}, {"referenceID": 68, "context": "As our first baseline, we adopt Shi and Malik\u2019s (2000) approach and cluster the reviews using only the second eigenvector, e2, as described in Section 2.", "startOffset": 32, "endOffset": 55}, {"referenceID": 57, "context": "See the work of Ng et al. (2001) for more details.", "startOffset": 16, "endOffset": 33}, {"referenceID": 43, "context": "Our third baseline is Kamvar et al.\u2019s (2003) unsupervised clustering algorithm, which, according to the authors, is ideally suited for text clustering, and has recently been proved to be a special case of ratio-cut optimization (Kulis, Basu, Dhillon, & Mooney, 2009).", "startOffset": 22, "endOffset": 45}, {"referenceID": 78, "context": "As mentioned before, L contains all and only those words in Wilson et al.\u2019s (2005) subjectivity lexicon that are marked with a prior polarity of Positive or Negative.", "startOffset": 60, "endOffset": 83}, {"referenceID": 67, "context": ", Latent Semantic Indexing (LSI), Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990) are generally considered non-interpretable (Sebastiani, 2002), unlike a dimension in the original feature space, which typically corresponds to a word type and can therefore be interpreted by a human easily.", "startOffset": 133, "endOffset": 151}, {"referenceID": 21, "context": "Representative members of this family of dimensionality reduction-based clustering algorithms include traditional algorithms that are based on LSI (Deerwester et al., 1990), as well as more recently proposed (and arguably better performing) algorithms such as spectral clustering (Shi & Malik, 2000; Ng et al.", "startOffset": 147, "endOffset": 172}, {"referenceID": 57, "context": ", 1990), as well as more recently proposed (and arguably better performing) algorithms such as spectral clustering (Shi & Malik, 2000; Ng et al., 2001), non-negative matrix factorization (Xu et al.", "startOffset": 115, "endOffset": 151}, {"referenceID": 81, "context": ", 2001), non-negative matrix factorization (Xu et al., 2003), locality preserving indexing (He et al.", "startOffset": 43, "endOffset": 60}, {"referenceID": 34, "context": ", 2003), locality preserving indexing (He et al., 2004), and locality discriminating indexing (Hu et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 35, "context": ", 2004), and locality discriminating indexing (Hu et al., 2007).", "startOffset": 46, "endOffset": 63}, {"referenceID": 78, "context": "As Yang and Liu (1999) put it, text classification is inherently \u201ca supervised learning task\u201d.", "startOffset": 3, "endOffset": 23}, {"referenceID": 40, "context": "Such attempts have led to the development of general-purpose semi-supervised text classification algorithms that combine labeled and unlabeled data using transduction (Joachims, 1999b) or EM (Nigam, McCallum, Thrun, & Mitchell, 2000), the latter of which has been used in combination with active learning (McCallum & Nigam, 1998). More recently, Sandler (2005) has proposed an unsupervised text classification algorithm that is based on mixture modeling and LSI-based dimensionality reduction.", "startOffset": 168, "endOffset": 361}, {"referenceID": 75, "context": "However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexicons from a resource-rich language to the target language (Banea, Mihalcea, Wiebe, & Hassan, 2008; Wan, 2008), or a domain that is \u201csimilar\u201d enough to the target domain (Blitzer et al.", "startOffset": 200, "endOffset": 251}, {"referenceID": 8, "context": "However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexicons from a resource-rich language to the target language (Banea, Mihalcea, Wiebe, & Hassan, 2008; Wan, 2008), or a domain that is \u201csimilar\u201d enough to the target domain (Blitzer et al., 2007).", "startOffset": 311, "endOffset": 333}, {"referenceID": 8, "context": "However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexicons from a resource-rich language to the target language (Banea, Mihalcea, Wiebe, & Hassan, 2008; Wan, 2008), or a domain that is \u201csimilar\u201d enough to the target domain (Blitzer et al., 2007). When the target domain or language fails to meet this requirement, sentiment-based clustering and unsupervised polarity classification become appealing alternatives. Unfortunately, with a few exceptions (e.g., semi-supervised sentiment analysis, Riloff & Wiebe, 2003; Sindhwani & Melville, 2008; Dasgupta & Ng, 2009a; Li, Zhang, & Sindhwani, 2009), these tasks are largely under-investigated in the NLP community. Turney\u2019s (2002) work is perhaps one of the most notable examples of unsupervised polarity classification.", "startOffset": 312, "endOffset": 765}, {"referenceID": 8, "context": "Of particular relevance to our work are domain adaptation techniques specifically developed for text and sentiment classification (e.g., Blitzer, McDonald, & Pereira, 2006; Finn & Kushmerick, 2006; Blitzer et al., 2007; Gao, Fan, Jiang, & Han, 2008; Ling, Dai, Xue, Yang, & Yu, 2008; Tan, Cheng, Wang, & Xu, 2009).", "startOffset": 130, "endOffset": 313}, {"referenceID": 32, "context": "The notion that text collections may be clustered in multiple independent ways has been discussed in the literature on computational stylistics (see Lim, Lee, & Kim, 2005; Biber & Kurjian, 2006; Grieve-Smith, 2006; Tambouratzis & Vassiliou, 2007; Gries, Wulff, & Davies, 2010, for example). In machine learning, there have been attempts to design algorithms for producing multiple clusterings of a dataset. While some of them operate in a semi-supervised setting (e.g., Gondek & Hofmann, 2004; Davidson & Qi, 2007), some are totally unsupervised (e.g., Caruana, Elhawary, Nguyen, & Smith, 2006; Jain, Meka, & Dhillon, 2008). For instance, Caruana et al.\u2019s (2006) meta clustering algorithm produces m different clusterings of a dataset by running k-means m", "startOffset": 195, "endOffset": 663}], "year": 2010, "abstractText": "While traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the author\u2019s mood, gender, age, or sentiment. Without knowing the user\u2019s intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the user\u2019s intention or having a human construct a feature space in an interactive manner during the clustering process. With the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. We demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}