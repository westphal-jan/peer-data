{"id": "1401.5693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Sentence Compression as Tree Transduction", "abstract": "This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.", "histories": [["v1", "Wed, 15 Jan 2014 05:19:15 GMT  (376kb)", "http://arxiv.org/abs/1401.5693v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["trevor anthony cohn", "mirella lapata"], "accepted": false, "id": "1401.5693"}, "pdf": {"name": "1401.5693.pdf", "metadata": {"source": "CRF", "title": "Sentence Compression as Tree Transduction", "authors": ["Trevor Cohn", "Mirella Lapata"], "emails": ["tcohn@inf.ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In recent years, interest in word processing and word processing methods has grown considerably, starting with the question of answering and machine translation. At its core, this is about the ability to make reproductions, namely whether and to what extent it is possible to reconstruct people at all. In recent years, it has been shown that reproductions of word processing and word processing processes can reconstruct and reconstruct. In recent years, it has been shown that reproductions of word processing and word processing processes are capable of reconstructing and reconstructing."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to move to another world in which they are able to live, in which they want to live."}, {"heading": "3. Problem Formulation", "text": "As already mentioned, we formulate sentence compression as a tree-to-tree transcription problem using weighted synchronous grammar coupled with a large margin training process. Our model learns from a parallel corpus of input (uncompressed) and output (compressed) pairs (x1, y1),... to predict a target labeled tree y from a source tree x. We capture the dependence between x and y as a weighted STSG, which we define in the following section. Section 3.2 discusses how to extract such a grammar from a parallel corpus. Each rule has a score, as does each ngram in the output tree, from which the total score of a compression y for sentence x can be derived. We introduce our scoring function in Section 3.3 and explain our training algorithm in Section 3.5. Within this framework, decoding boils down to finding the best target tree that is limited by the grammar."}, {"heading": "3.1 Synchronous Grammar", "text": "This is essentially a transductive procedure for all nodes, (vS, RT). [S, RT)] for all nodes, (vS, RT) for all nodes, (vT) for all nodes, (vT) for all nodes, (vT) for one rule < vT > < (S, NS) for the nodes, (RS, RT) for all nodes, (vT) for all nodes, (vT) for one rule < vT > < (S, rewrite node vS in x) as nodes, (vT) for all variables, u (cS, cT) aligned child nodes, (vS, cT)."}, {"heading": "3.2 Grammar", "text": "This year it is more than ever before in the history of the city."}, {"heading": "3.3 Linear Model", "text": "While a STSG 2006 defines a converter capable of mapping a source tree into many possible target trees, it is of little use without any kind of weighting against grammatical trees constructed with meaningful STSG productions that provide fluently compressed target sentences. Ideally, the model would define a scoring function via target trees or strings, but we will instead work with derivatives. In general, there may be many derivatives that all produce the same target tree, a situation called specious ambiguity. To fully account for this unclear ambiguity, we would require an aggregation of all derivatives that produce the same target tree. This would break the polynomial-time dynamic program used for inference problems, NP-complete (Knight, 1999). To this end, we define a scoring function via derivatives: Score (d; w) = < D = < D (5) (w) where a derivative consists of rules w > d are."}, {"heading": "3.4 Decoding", "text": "The decoding aims to find the best target tree licensed by the grammar as the source tree. As mentioned above, we deal with derivatives instead of target trees. The decoding finds the maximizing derivative, d *, of: d * = argmax d: source (d) = x score (d; w) (7), where x is the (given) source tree, the source (d) that extracts the source tree from the derivative d and score is defined in (5). The maximization is performed over the space of derivatives for the given source tree as defined by the transduction process shown in algorithm. The maximization problem in (7) is solved with the diagram-based dynamic program that is shown. This extends past inference algorithms for weighted STSGs (Eisner, 2003), which assume that the.e. derivative function must be compressed with ipriming."}, {"heading": "3.5 Training", "text": "We now turn to the problem of how derivatives are evaluated in our model."}, {"heading": "3.6 Loss Functions", "text": "The training algorithms described above are highly modular and, in theory, a wide range of loss functions can be supported. There are no generally accepted assessment measures for text compression. A zero-one loss would be easy to define, but inappropriate for our problem, 10. We also experimented with other heuristics, including selecting the random derivative and selecting the derivative that is even slightly different from the reference derivative (all using the same search algorithm, but with a different goal). Of these, only the maximum scoring derivative was competitive with the maximum heuristic.as It would always punish target derivatives that differ only slightly from the reference derivative. Ideally, we would like a loss with a broader scoring range, which may differ from the reference derivative. Some of these derivatives may be good compressions, while others may be completely ungrammatical. For this reason, we have developed a text-loss-function set of text for the text."}, {"heading": "4. Features", "text": "We developed two broad classes of traits that are applied to grammar rules and ngrams of target terminals. We defined only a single ngram trait, the conditional log probability of a trigram language model. However, this was applied to the BNC (100 million words) using the SRI language modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing. For each rule < X, Y > < \u03b1, \u03b3, \u0445 >, we extract traits according to the templates listed below. Our templates lead to binary indicator traits, unless these traits are explicitly specified. These traits perform a Boolean test if the test succeeds and 0 otherwise. An example rule and the corresponding traits are shown in Table 2.Type: Whether the rule was extracted from the training set and / or created as a Delete rule."}, {"heading": "5. Experimental Set-up", "text": "In this section, we present our experimental setup to assess the performance of the typesetting compression model described above. We provide details of the corpora used, briefly present the McDonald's model used to compare with our approach (2006), and explain how the system performance was evaluated."}, {"heading": "5.1 Corpora", "text": "The first is the Ziff-Davis corpus, a popular choice in sentence compression literature, derived from a collection of news articles about computer products; it was created automatically by comparing sentences that occur in an article with sentences that occur in an abstraction (Knight & Marcu, 2002); the other two corpus were created manually; commenters were asked to produce target compressions by deleting foreign words from the source without changing the word order (Clarke & Lapata, 2008); one corpus was sampled from written sources, 11. Available from http: / / homepagessen.inf.ac.com / s0460084 /.the British National Corpus (BNC) and the American News Text Corpus, while the other corpus was sampled from written sources."}, {"heading": "5.2 Comparison with State-of-the-Art", "text": "In this approach, sentence compression is formalized as a classification task: pairs of words from the source sentence are classified as adjacent or not in the target compression. Let x = x1,.. xN denote a source sentence with a target compression y = y1,.. yM in which each yi occurs in x. L (yi) represents the word yi as the target on the index of the word in the source (subject to the limitation that L (yi) < L (yi + 1)) represents the target compression y for a sentence x as a dot product between a high-dimensional character representation, f, via bigrams and a corresponding weight vector, w, score (x, y; w) = M \u00b2."}, {"heading": "5.3 Evaluation", "text": "Following Knight and Marcu (2002), we conducted two separate experiments. In the first experiment, participants were confronted with a source set and its target compression and asked to rate how well the compression preserved the most important information from the source set. In the second experiment, they were asked to rate the grammar of the compressed results. In both cases, they used a five-level rating scale on which a high number indicated better performance. We randomly selected 20 sentences from the test section of each corpus. These sentences were automatically compressed by our system and McDonald's (2006). We also included gold standard compressions. Therefore, our materials consisted of 180 (20 x 3 x 3) source sets. A Latin square design ensured that the subjects could not see two different compressions of the same set. We collected thermodratations from 30 unpaid volunteers, who all used their own grammatical corrections. Both studies were conducted nexlike Internet."}, {"heading": "6. Results", "text": "The framework presented in Section 3 is quite flexible: depending on the extraction strategy of the grammar, the selection of characteristics and the loss function, different classes of models can be derived. Before presenting our results on the test set, we discuss the specific model used in our experiments and explain how its parameters were instantiated."}, {"heading": "6.1 Model Selection", "text": "We have obtained syntactical analysis for source and target structures where only a limited number of subjects are allowed. It is expected that the predicted subjects contain a number of errors, although we do not have gold standard trees to quantify these errors or quantify their impact on prediction outcomes. However, we have noticed that errors in the source structures do not always negatively affect the performance of the model. In many cases, the model was able to recover from these errors and still produce good output compressions."}, {"heading": "7. Model Comparison", "text": "This model uses a grammar with unxical and lexical rules (recursion depth 1), a hamming loss based on tokens, and all the features of Section 4. However, the model has been trained separately on each corpus (training part). We discuss first our results and then the way we focus on human work. Table 6 illustrates the performance of our model (Transducer1) on CLP. We also report on the results of the same corporation with McDonald's (McDonald's) model (McDonald's) and the improved version (Clarke's) of Clarke and Lapata. We also present the compression rate for each system and the reference gold standard. In all cases that our tree transducer model runs McDonald's model and the improved ILP-based version."}, {"heading": "8. Conclusions", "text": "We have developed a system that provides space for all kinds of reproductions using grammar. Each grammar rule is assigned a weight that is discriminated against within a large margin model (Tsochantaridis et al., 2005). A specialized algorithm is used to learn the model weights and find the best scanner compressions. We argue that the source code is freely available, both for work and for work. There are a number of reasons why the proposed framework is attractive. Synchronous grammar provides the ability to capture reproductions that go beyond the word deletion, as well as changes in non-terminated categories."}, {"heading": "Acknowledgments", "text": "We would like to thank Philip Blunsom for his insightful comments and suggestions and the anonymous referees whose feedback has helped to significantly improve this work. In particular, we would like to thank James Clarke for providing us with his implementations of the Clarke and Lapata's (2008) and McDonald's (2006) models. We would like to thank the EPSRC for its support (grants GR / T04540 / 01 and GR / T04557 / 01), which has made use of the resources of the Edinburgh Compute and Data Facility (ECDF), which is partially supported by the eDIKT initiative. A preliminary version of this work has been published in the 2007 EMNLP / CoNLL Proceedings."}], "references": [{"title": "Syntax directed translations and the pushdown assembler", "author": ["A.V. Aho", "J.D. Ullman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Aho and Ullman,? \\Q1969\\E", "shortCiteRegEx": "Aho and Ullman", "year": 1969}, {"title": "Learning dependency translation models as collections of finite state head transducers", "author": ["H. Alshawi", "S. Bangalore", "S. Douglas"], "venue": "Computational Linguistics,", "citeRegEx": "Alshawi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Alshawi et al\\.", "year": 2000}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.A.D. Pietra", "V.J.D. Pietra"], "venue": "Computational Linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Design of a multi-lingual, parallel-processing statistical parsing engine", "author": ["D. Bikel"], "venue": "In Proceedings of the 2nd International Conference on Human Language Technology Research,", "citeRegEx": "Bikel,? \\Q2002\\E", "shortCiteRegEx": "Bikel", "year": 2002}, {"title": "Robust accurate statistical annotation of general text", "author": ["E.J. Briscoe", "J. Carroll"], "venue": "In Proceedings of the Third International Conference on Language Resources and Evaluation,", "citeRegEx": "Briscoe and Carroll,? \\Q2002\\E", "shortCiteRegEx": "Briscoe and Carroll", "year": 2002}, {"title": "Simplifying text for language impaired readers", "author": ["J. Carroll", "G. Minnen", "D. Pearce", "Y. Canning", "S. Devlin", "J. Tait"], "venue": "In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Carroll et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Motivations and methods for text simplification", "author": ["R. Chandrasekar", "C.D.B. Srinivas"], "venue": "In Proceedings of the 16th International Conference on Computational Linguistics,", "citeRegEx": "Chandrasekar and Srinivas,? \\Q1996\\E", "shortCiteRegEx": "Chandrasekar and Srinivas", "year": 1996}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang,? \\Q2007\\E", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "Models for sentence compression: A comparison across domains, training requirements and evaluation measures", "author": ["J. Clarke", "M. Lapata"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Clarke and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2006}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Clarke and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2008}, {"title": "Sentence compression beyond word deletion", "author": ["T. Cohn", "M. Lapata"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics,", "citeRegEx": "Cohn and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2008}, {"title": "Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Head-driven statistical models for natural language parsing", "author": ["M.J. Collins"], "venue": "Ph.D. thesis,", "citeRegEx": "Collins,? \\Q1999\\E", "shortCiteRegEx": "Collins", "year": 1999}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Crammer and Singer,? \\Q2003\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2003}, {"title": "A noisy-channel model for document compression", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "In Proceedings of the 40th Annual Meeting of thev Association for Computational Linguistics,", "citeRegEx": "III and Marcu,? \\Q2002\\E", "shortCiteRegEx": "III and Marcu", "year": 2002}, {"title": "Learning non-isomorphic tree mappings for machine translation", "author": ["J. Eisner"], "venue": "In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Eisner,? \\Q2003\\E", "shortCiteRegEx": "Eisner", "year": 2003}, {"title": "What\u2019s in a translation rule", "author": ["M. Galley", "M. Hopkins", "K. Knight", "D. Marcu"], "venue": "In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Lexicalized Markov grammars for sentence compression", "author": ["M. Galley", "K. McKeown"], "venue": "In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Galley and McKeown,? \\Q2007\\E", "shortCiteRegEx": "Galley and McKeown", "year": 2007}, {"title": "Training tree transducers", "author": ["J. Grael", "K. Knight"], "venue": "In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Grael and Knight,? \\Q2004\\E", "shortCiteRegEx": "Grael and Knight", "year": 2004}, {"title": "Natural language based reformulation resource and wide exploitation for question answering", "author": ["U. Hermjakob", "A. Echihabi", "D. Marcu"], "venue": "In Proceedings of 11th Text Retrieval Conference,", "citeRegEx": "Hermjakob et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hermjakob et al\\.", "year": 2002}, {"title": "Speech summarization: an approach through word extraction and a method for evaluation", "author": ["C. Hori", "S. Furui"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Hori and Furui,? \\Q2004\\E", "shortCiteRegEx": "Hori and Furui", "year": 2004}, {"title": "Sentence reduction for automatic text summarization", "author": ["H. Jing"], "venue": "In Proceedings of the 6th Applied Natural Language Processing Conference,", "citeRegEx": "Jing,? \\Q2000\\E", "shortCiteRegEx": "Jing", "year": 2000}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Joachims,? \\Q2005\\E", "shortCiteRegEx": "Joachims", "year": 2005}, {"title": "Paraphrasing predicates from written language to spoken language using the web", "author": ["N. Kaji", "M. Okamoto", "S. Kurohashi"], "venue": "In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Kaji et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kaji et al\\.", "year": 2004}, {"title": "Decoding complexity in word-replacement translation models", "author": ["K. Knight"], "venue": "Computational Linguistics,", "citeRegEx": "Knight,? \\Q1999\\E", "shortCiteRegEx": "Knight", "year": 1999}, {"title": "Summarization beyond sentence extraction: a probabilistic approach to sentence compression", "author": ["K. Knight", "D. Marcu"], "venue": "Artificial Intelligence,", "citeRegEx": "Knight and Marcu,? \\Q2002\\E", "shortCiteRegEx": "Knight and Marcu", "year": 2002}, {"title": "Discovery of inference rules for question answering", "author": ["D. Lin", "P. Pantel"], "venue": "Natural Language Engineering,", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Discriminative sentence compression with soft syntactic constraints", "author": ["R. McDonald"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "McDonald,? \\Q2006\\E", "shortCiteRegEx": "McDonald", "year": 2006}, {"title": "Example-based sentence reduction using the hidden markov model", "author": ["M.L. Nguyen", "S. Horiguchi", "A. Shimazu", "B.T. Ho"], "venue": "ACM Transactions on Asian Language Information Processing,", "citeRegEx": "Nguyen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2004}, {"title": "The alignment template approach to statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney", "year": 2004}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In Proceedings of the 40th Annual Meeting of thev Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["S. Petrov", "L. Barrett", "R. Thibaux", "D. Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar", "author": ["S. Riezler", "T.H. King", "R. Crouch", "A. Zaenen"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Riezler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2003}, {"title": "Synchronous tree-adjoining grammars", "author": ["S. Shieber", "Y. Schabes"], "venue": "In Proceedings of the 13th International Conference on Computational Linguistics,", "citeRegEx": "Shieber and Schabes,? \\Q1990\\E", "shortCiteRegEx": "Shieber and Schabes", "year": 1990}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Supervised and unsupervised learning for sentence compression", "author": ["J. Turner", "E. Charniak"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turner and Charniak,? \\Q2005\\E", "shortCiteRegEx": "Turner and Charniak", "year": 2005}, {"title": "Sentence compression for automated subtitling: A hybrid approach", "author": ["V. Vandeghinste", "Y. Pan"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "Vandeghinste and Pan,? \\Q2004\\E", "shortCiteRegEx": "Vandeghinste and Pan", "year": 2004}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["D. Wu"], "venue": "Computational Linguistics,", "citeRegEx": "Wu,? \\Q1997\\E", "shortCiteRegEx": "Wu", "year": 1997}, {"title": "A syntax-based statistical translation model", "author": ["K. Yamada", "K. Knight"], "venue": "In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Yamada and Knight,? \\Q2001\\E", "shortCiteRegEx": "Yamada and Knight", "year": 2001}], "referenceMentions": [{"referenceID": 21, "context": "The aim is to produce a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000).", "startOffset": 125, "endOffset": 137}, {"referenceID": 27, "context": "Other approaches model compression discriminatively as subtree deletion (Riezler, King, Crouch, & Zaenen, 2003; Nguyen, Horiguchi, Shimazu, & Ho, 2004; McDonald, 2006).", "startOffset": 72, "endOffset": 167}, {"referenceID": 15, "context": "Specifically, we adopt the synchronous tree substitution grammar (STSG) formalism (Eisner, 2003) which can model non-isomorphic tree structures while having efficient inference algorithms.", "startOffset": 82, "endOffset": 96}, {"referenceID": 15, "context": "Specifically, we adopt the synchronous tree substitution grammar (STSG) formalism (Eisner, 2003) which can model non-isomorphic tree structures while having efficient inference algorithms. We show how such a grammar can be induced from a parallel corpus and propose a discriminative model for the rewriting task which can be viewed as a weighted tree-to-tree transducer. Our learning framework makes use of the large margin algorithm put forward by Tsochantaridis, Joachims, Hofmann, and Altun (2005) which efficiently learns a prediction function to minimize a given loss function.", "startOffset": 83, "endOffset": 501}, {"referenceID": 38, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004).", "startOffset": 48, "endOffset": 58}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004).", "startOffset": 154, "endOffset": 168}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004). Sentence compression bears some resemblance to machine translation. Instead of translating from one language into another, we are translating long sentences into shorter ones within the same language. It is therefore not surprising that previous work has also adopted SCFGs for the compression task. Specifically, Knight and Marcu (2002) proposed a noisychannel formulation of sentence compression.", "startOffset": 155, "endOffset": 596}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004). Sentence compression bears some resemblance to machine translation. Instead of translating from one language into another, we are translating long sentences into shorter ones within the same language. It is therefore not surprising that previous work has also adopted SCFGs for the compression task. Specifically, Knight and Marcu (2002) proposed a noisychannel formulation of sentence compression. Their model consists of two components: a language model P (y) whose role is to guarantee that the compression output is grammatical and a channel model P (x|y) capturing the probability that the source sentence x is an expansion of the target compression y. Their decoding algorithm searches for the compression y which maximizes P (y)P (x|y). The channel model is a stochastic SCFG, the rules of which are extracted from a parsed parallel corpus and their weights estimated using maximum likelihood. Galley and McKeown (2007) show how to obtain improved SCFG probability estimates through Markovization.", "startOffset": 155, "endOffset": 1185}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004). Sentence compression bears some resemblance to machine translation. Instead of translating from one language into another, we are translating long sentences into shorter ones within the same language. It is therefore not surprising that previous work has also adopted SCFGs for the compression task. Specifically, Knight and Marcu (2002) proposed a noisychannel formulation of sentence compression. Their model consists of two components: a language model P (y) whose role is to guarantee that the compression output is grammatical and a channel model P (x|y) capturing the probability that the source sentence x is an expansion of the target compression y. Their decoding algorithm searches for the compression y which maximizes P (y)P (x|y). The channel model is a stochastic SCFG, the rules of which are extracted from a parsed parallel corpus and their weights estimated using maximum likelihood. Galley and McKeown (2007) show how to obtain improved SCFG probability estimates through Markovization. Turner and Charniak (2005) note that SCFG rules are not expressive enough to model structurally complicated compressions as they are restricted to trees of depth 1.", "startOffset": 155, "endOffset": 1290}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004). Sentence compression bears some resemblance to machine translation. Instead of translating from one language into another, we are translating long sentences into shorter ones within the same language. It is therefore not surprising that previous work has also adopted SCFGs for the compression task. Specifically, Knight and Marcu (2002) proposed a noisychannel formulation of sentence compression. Their model consists of two components: a language model P (y) whose role is to guarantee that the compression output is grammatical and a channel model P (x|y) capturing the probability that the source sentence x is an expansion of the target compression y. Their decoding algorithm searches for the compression y which maximizes P (y)P (x|y). The channel model is a stochastic SCFG, the rules of which are extracted from a parsed parallel corpus and their weights estimated using maximum likelihood. Galley and McKeown (2007) show how to obtain improved SCFG probability estimates through Markovization. Turner and Charniak (2005) note that SCFG rules are not expressive enough to model structurally complicated compressions as they are restricted to trees of depth 1. They remedy this by supplying their synchronous grammar with a set of more general \u201cspecial\u201d rules. For example, they allow rules of the form \u3008NP,NP\u3009 \u2192 \u3008[NP NP 1 CC NP 2 ],NP 1 \u3009 (boxed subscripts are added to distinguish between the two NPs). Our own work formulates sentence compression in the framework of synchronous treesubstitution grammar (STSG, Eisner, 2003). STSG allows to describe non-isomorphic tree pairs (the grammar rules can comprise trees of arbitrary depth) and is thus suited to textrewriting tasks which typically involve a number of local modifications to the input text. Especially if each modification can be described succinctly in terms of syntactic transformations, such as dropping an adjectival phrase or converting a passive verb phrase into active form. STSG is a restricted version of synchronous tree adjoining grammar (STAG, Shieber & Schabes, 1990) without an adjunction operation. STAG affords mild context sensitivity, however at increased cost of inference. SCFG and STSG are weakly equivalent, that is, their string languages are identical but they do not produce equivalent tree pairs. For example, in Figure 2, rules (1)\u2013(4) can be expressed as SCFG rules, but rule (5) cannot because both the source and target fragments are two level trees. In fact it would be impossible to describe the trees in Figure 1 using a SCFG. Our grammar rules are therefore more general than those obtained by Knight and Marcu (2002) and can account for more elaborate tree divergences.", "startOffset": 155, "endOffset": 2882}, {"referenceID": 7, "context": "Examples include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore, & Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), and several variants of tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004). Sentence compression bears some resemblance to machine translation. Instead of translating from one language into another, we are translating long sentences into shorter ones within the same language. It is therefore not surprising that previous work has also adopted SCFGs for the compression task. Specifically, Knight and Marcu (2002) proposed a noisychannel formulation of sentence compression. Their model consists of two components: a language model P (y) whose role is to guarantee that the compression output is grammatical and a channel model P (x|y) capturing the probability that the source sentence x is an expansion of the target compression y. Their decoding algorithm searches for the compression y which maximizes P (y)P (x|y). The channel model is a stochastic SCFG, the rules of which are extracted from a parsed parallel corpus and their weights estimated using maximum likelihood. Galley and McKeown (2007) show how to obtain improved SCFG probability estimates through Markovization. Turner and Charniak (2005) note that SCFG rules are not expressive enough to model structurally complicated compressions as they are restricted to trees of depth 1. They remedy this by supplying their synchronous grammar with a set of more general \u201cspecial\u201d rules. For example, they allow rules of the form \u3008NP,NP\u3009 \u2192 \u3008[NP NP 1 CC NP 2 ],NP 1 \u3009 (boxed subscripts are added to distinguish between the two NPs). Our own work formulates sentence compression in the framework of synchronous treesubstitution grammar (STSG, Eisner, 2003). STSG allows to describe non-isomorphic tree pairs (the grammar rules can comprise trees of arbitrary depth) and is thus suited to textrewriting tasks which typically involve a number of local modifications to the input text. Especially if each modification can be described succinctly in terms of syntactic transformations, such as dropping an adjectival phrase or converting a passive verb phrase into active form. STSG is a restricted version of synchronous tree adjoining grammar (STAG, Shieber & Schabes, 1990) without an adjunction operation. STAG affords mild context sensitivity, however at increased cost of inference. SCFG and STSG are weakly equivalent, that is, their string languages are identical but they do not produce equivalent tree pairs. For example, in Figure 2, rules (1)\u2013(4) can be expressed as SCFG rules, but rule (5) cannot because both the source and target fragments are two level trees. In fact it would be impossible to describe the trees in Figure 1 using a SCFG. Our grammar rules are therefore more general than those obtained by Knight and Marcu (2002) and can account for more elaborate tree divergences. Moreover, by adopting a more expressive grammar formalism, we can naturally model syntactically complex compressions without having to specify additional rules (as in Turner & Charniak, 2005). A synchronous grammar will license a large number of compressions for a given source tree. Each grammar rule typically has a score from which the overall score of a compression y for sentence x can be derived. Previous work estimates these scores generatively as discussed above. We opt for a discriminative training procedure which allows for the incorporation of all manner of powerful features. We use the large margin technique proposed by Tsochantaridis et al. (2005). The framework is attractive in that it supports a configurable loss function, which describes the extent to which a predicted target tree differs from", "startOffset": 155, "endOffset": 3601}, {"referenceID": 24, "context": "McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.", "startOffset": 0, "endOffset": 16}, {"referenceID": 18, "context": "An example is in the work of Hori and Furui (2004), who propose a model for automatically transcribed spoken text.", "startOffset": 29, "endOffset": 51}, {"referenceID": 8, "context": "Clarke and Lapata (2008) show that such unsupervised models can be greatly improved when linguistically motivated constraints are used during decoding.", "startOffset": 0, "endOffset": 25}, {"referenceID": 24, "context": "Algorithm 3 is an extension of Galley, Hopkins, Knight, and Marcu\u2019s (2004) technique for extracting a", "startOffset": 48, "endOffset": 75}, {"referenceID": 24, "context": "This would break the polynomial-time dynamic program used for inference, rendering inference problem NP-complete (Knight, 1999).", "startOffset": 113, "endOffset": 127}, {"referenceID": 15, "context": "This extends earlier inference algorithms for weighted STSGs (Eisner, 2003) which assume that the scoring function must decompose with the derivation, i.", "startOffset": 61, "endOffset": 75}, {"referenceID": 7, "context": "This is equivalent to using as our grammar the intersection between the original grammar and an ngram language model, as explained by Chiang (2007) in the context of string transduction with an SCFG.", "startOffset": 134, "endOffset": 148}, {"referenceID": 7, "context": "We adopt a popular approach in syntax-inspired machine translation to address this problem (Chiang, 2007).", "startOffset": 91, "endOffset": 105}, {"referenceID": 7, "context": "We adopt a popular approach in syntax-inspired machine translation to address this problem (Chiang, 2007). Firstly, we use a beam-search, which limits the number of different ngram contexts stored in each chart cell to a constant, W . This changes the base in the complexity term, leading to an improved O(SRW V ) but which is still exponential in the number of variables. In addition, we use Chiang\u2019s cube-pruning heuristic to further limit the number of combinations. Cube-pruning uses a heuristic scoring function which approximates the conditional log-probability from a ngram language model with the logprobability from a unigram model.8 This allows us to visit the combinations in best-first order under the heuristic scoring function until the beam is filled.The beam is then rescored using the correct scoring function. This can be done cheaply in O(WV ) time, leading to an overall time complexity of decoding to O(SRWV ). We refer the interested reader to the work of Chiang (2007) for further details.", "startOffset": 92, "endOffset": 992}, {"referenceID": 11, "context": "Possibilities include perceptron training (Collins, 2002), log-linear optimisation of the conditional log-likelihood (Berger, Pietra, & Pietra, 1996) and large margin methods.", "startOffset": 42, "endOffset": 57}, {"referenceID": 11, "context": "Possibilities include perceptron training (Collins, 2002), log-linear optimisation of the conditional log-likelihood (Berger, Pietra, & Pietra, 1996) and large margin methods. We base our training on Tsochantaridis et al.\u2019s (2005) framework for learning Support Vector Machines (SVMs) over structured output spaces, using the SVMstruct implementation.", "startOffset": 43, "endOffset": 231}, {"referenceID": 11, "context": "Possibilities include perceptron training (Collins, 2002), log-linear optimisation of the conditional log-likelihood (Berger, Pietra, & Pietra, 1996) and large margin methods. We base our training on Tsochantaridis et al.\u2019s (2005) framework for learning Support Vector Machines (SVMs) over structured output spaces, using the SVMstruct implementation.9 The framework supports a configurable loss function which is particularly appealing in the context of sentence compression and more generally text-to-text generation. It also has an efficient training algorithm and powerful regularization. The latter is is critical for discriminative models with large numbers of features, which would otherwise over-fit the training sample at the expense of generalization accuracy. We briefly summarize the approach below; for a more detailed description we refer the interested reader to the work of Tsochantaridis et al. (2005). Traditionally SVMs learn a linear classifier that separates two or more classes with the largest possible margin.", "startOffset": 43, "endOffset": 919}, {"referenceID": 35, "context": "little difference between the two rescaling methods (Tsochantaridis et al., 2005).", "startOffset": 52, "endOffset": 81}, {"referenceID": 35, "context": "little difference between the two rescaling methods (Tsochantaridis et al., 2005). We use margin rescaling for the practical reason that it can be approximated more accurately than can slack rescaling by our chart based inference method. The optimization problem in (10) is approximated using an algorithm proposed by Tsochantaridis et al. (2005). The algorithm finds a small set of constraints from the fullsized optimization problem that ensures a sufficiently accurate solution.", "startOffset": 53, "endOffset": 347}, {"referenceID": 22, "context": "In order to calculate these losses the chart must be stratified by the loss function\u2019s arguments (Joachims, 2005).", "startOffset": 97, "endOffset": 113}, {"referenceID": 38, "context": "10 This is found using Algorithm 5, a chart-based dynamic program similar to the alignment algorithm for inverse transduction grammars (Wu, 1997).", "startOffset": 135, "endOffset": 145}, {"referenceID": 34, "context": "This was trained on the BNC (100 million words) using the SRI Language Modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing.", "startOffset": 88, "endOffset": 103}, {"referenceID": 27, "context": "We give details of the corpora used, briefly introduce McDonald\u2019s (2006) model used for comparison with our approach, and explain how system output was evaluated.", "startOffset": 55, "endOffset": 73}, {"referenceID": 8, "context": "Our experiments on CLspoken and CLwritten followed Clarke and Lapata\u2019s (2008) partition of training, test, and development sets.", "startOffset": 51, "endOffset": 78}, {"referenceID": 8, "context": "Our experiments on CLspoken and CLwritten followed Clarke and Lapata\u2019s (2008) partition of training, test, and development sets. The partition sizes are shown in Table 3. In the case of the Ziff-Davis corpus, Knight and Marcu (2002) had not defined a development set.", "startOffset": 51, "endOffset": 233}, {"referenceID": 27, "context": "We evaluated our results against McDonald\u2019s (2006) discriminative model.", "startOffset": 33, "endOffset": 51}, {"referenceID": 27, "context": "The maximization is solved using a semi-Markov Viterbi algorithm (McDonald, 2006).", "startOffset": 65, "endOffset": 81}, {"referenceID": 25, "context": "The maximization is solved using a semi-Markov Viterbi algorithm (McDonald, 2006). The model parameters are estimated using the Margin Infused Relaxed Algorithm (MIRA Crammer & Singer, 2003), a discriminative large-margin online learning technique. McDonald (2006) uses a similar loss function to our Hamming loss (see (12)) but without an explicit length penalty.", "startOffset": 66, "endOffset": 265}, {"referenceID": 8, "context": "Clarke and Lapata (2008) reformulate McDonald\u2019s (2006) model in the context of integer linear programming (ILP) and augment it with constraints ensuring that the compressed output is grammatically and semantically well formed.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Clarke and Lapata (2008) reformulate McDonald\u2019s (2006) model in the context of integer linear programming (ILP) and augment it with constraints ensuring that the compressed output is grammatically and semantically well formed.", "startOffset": 0, "endOffset": 55}, {"referenceID": 8, "context": "Clarke and Lapata (2008) reformulate McDonald\u2019s (2006) model in the context of integer linear programming (ILP) and augment it with constraints ensuring that the compressed output is grammatically and semantically well formed. For example, if the target sentence has negation, this must be included in the compression; If the source verb has a subject, this must also be retained in the compression. They generate and solve an ILP for every source sentence using the branch-and-bound algorithm. Since they obtain performance improvements over McDonald\u2019s model on several corpora, we also use it for comparison against our model. To summarize, we believe that McDonald\u2019s (2006) model is a good basis for comparison for several reasons.", "startOffset": 0, "endOffset": 677}, {"referenceID": 24, "context": "Following Knight and Marcu (2002), we conducted two separate experiments.", "startOffset": 10, "endOffset": 34}, {"referenceID": 24, "context": "Following Knight and Marcu (2002), we conducted two separate experiments. In the first experiment participants were presented with a source sentence and its target compression and asked to rate how well the compression preserved the most important information from the source sentence. In the second experiment, they were asked to rate the grammaticality of the compressed outputs. In both cases they used a five point rating scale where a high number indicates better performance. We randomly selected 20 sentences from the test portion of each corpus. These sentences were compressed automatically by our system and McDonald\u2019s (2006) system.", "startOffset": 10, "endOffset": 636}, {"referenceID": 32, "context": "We also report results using F1 computed over grammatical relations (Riezler et al., 2003).", "startOffset": 68, "endOffset": 90}, {"referenceID": 3, "context": "We obtained syntactic analyses for source and target sentences with Bikel\u2019s (2002) parser.", "startOffset": 68, "endOffset": 83}, {"referenceID": 3, "context": "16 Compared to dependencies extracted from the predicted parses using Bikel\u2019s (2002) parser on the output string, we observe that the relation F1 score increases uniformly for all tasks, by between 2.", "startOffset": 70, "endOffset": 85}, {"referenceID": 25, "context": "We also report results on the same corpora using McDonald\u2019s (2006) model (McDonald) and the improved version (Clarke ILP) put forward by Clarke and Lapata (2008).", "startOffset": 49, "endOffset": 67}, {"referenceID": 8, "context": "We also report results on the same corpora using McDonald\u2019s (2006) model (McDonald) and the improved version (Clarke ILP) put forward by Clarke and Lapata (2008). We also present the compression rate for each system and the reference gold standard.", "startOffset": 137, "endOffset": 162}, {"referenceID": 27, "context": "to modify our chart-based decoder in a fashion similar to McDonald (2006). However, we leave this to", "startOffset": 58, "endOffset": 74}, {"referenceID": 27, "context": "We carried out an Analysis of Variance (Anova) to examine the effect of system type (McDonald, Transducer, Reference) on the compression ratings. The Anova revealed a reliable effect on all three corpora. We used post-hoc Tukey tests to examine whether the mean ratings for each system differed significantly (p < 0.01). On the CLspoken corpus the Transducer is perceived as significantly better than McDonald, both in terms of grammaticality and importance. We obtain the same result for the CLwritten corpus. The two systems achieve similar performances on Ziff-Davis (the grammaticality and importance score do not differ significantly). Ziff-Davis seems to be a less challenging corpus than CLspoken or CLwritten and less likely to highlight differences among systems. For example, Turner and Charniak (2005) present several variants of the noisy-channel model, all of which achieve compressions of similar quality on Ziff-Davis (grammaticality ratings varied by only \u00b10.", "startOffset": 85, "endOffset": 813}, {"referenceID": 27, "context": "Nevertheless, compression rate can be indirectly manipulated by adopting loss functions that encourage or discourage compression or directly during decoding by stratifying the chart for length (McDonald, 2006).", "startOffset": 193, "endOffset": 209}, {"referenceID": 35, "context": "Each grammar rule is assigned a weight which is learned discriminatively within a large margin model (Tsochantaridis et al., 2005).", "startOffset": 101, "endOffset": 130}, {"referenceID": 27, "context": "We evaluated our model on three compression corpora (CLspoken, CLwritten, and ZiffDavis) and showed that in most cases it yields results superior to state-of-the-art (McDonald, 2006).", "startOffset": 166, "endOffset": 182}, {"referenceID": 7, "context": "Our feature ablation study revealed that ngram features are beneficial, mirroring a similar finding in the machine translation literature (Chiang, 2007).", "startOffset": 138, "endOffset": 152}, {"referenceID": 16, "context": "This concurs with Galley and McKeown (2007) who also find that lexicalization yields better compression output.", "startOffset": 18, "endOffset": 44}, {"referenceID": 7, "context": "An obvious extension concerns porting the framework to other rewriting applications such as document summarization (Daum\u00e9 III & Marcu, 2002) or machine translation (Chiang, 2007).", "startOffset": 164, "endOffset": 178}, {"referenceID": 8, "context": "Special thanks to James Clarke for sharing his implementations of Clarke and Lapata\u2019s (2008) and McDonald\u2019s (2006) models with us.", "startOffset": 66, "endOffset": 93}, {"referenceID": 8, "context": "Special thanks to James Clarke for sharing his implementations of Clarke and Lapata\u2019s (2008) and McDonald\u2019s (2006) models with us.", "startOffset": 66, "endOffset": 115}], "year": 2009, "abstractText": "This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.", "creator": "TeX"}}}