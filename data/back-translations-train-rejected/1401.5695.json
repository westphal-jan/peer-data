{"id": "1401.5695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.", "histories": [["v1", "Wed, 15 Jan 2014 05:39:01 GMT  (1372kb)", "http://arxiv.org/abs/1401.5695v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tahira naseem", "benjamin snyder", "jacob eisenstein", "regina barzilay"], "accepted": false, "id": "1401.5695"}, "pdf": {"name": "1401.5695.pdf", "metadata": {"source": "CRF", "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "authors": ["Tahira Naseem", "Benjamin Snyder", "Jacob Eisenstein", "Regina Barzilay"], "emails": ["TAHIRA@CSAIL.MIT.EDU", "BSNYDER@CSAIL.MIT.EDU", "JACOBE@CSAIL.MIT.EDU", "REGINA@CSAIL.MIT.EDU"], "sections": [{"heading": null, "text": "The central assumption of our work is that by combining keywords from multiple languages, the structure of each one becomes clearer. We look at two ways to apply this intuition to the problem of unattended tagging: a model that merges the tag structures for a language pair directly into a single sequence, and a second model that incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques. Our results show that by incorporating multilingual evidence, we can achieve impressive performance gains across a range of scenarios."}, {"heading": "1. Introduction", "text": "In fact, most of them are a form of self-realization that is capable of surpassing itself."}, {"heading": "2. Related Work", "text": "We identify two broad areas of related work: multilingual learning and the generation of part-of-speech tags without labeled data. Our discussion of multilingual learning focuses on unattended approaches involving two or more languages. We then describe related work to unattended and semi-supervised models of part-of-speech tagging."}, {"heading": "2.1 Multilingual Learning", "text": "The potential of multilingual data as a rich source of linguistic knowledge has been recognized since the early days of empirical processing of natural language. As multilingualism differs greatly from language to language, uncommented multilingual data can serve as a learning signal in an unattended environment. We are particularly interested in methods for sharing more than two languages and comparing our approach with relevant previous work. Multilingual learning can also be applied in a semi-supervised environment, typically by projecting annotations across a parallel corpus into another language where such resources do not exist (e.g. Yarowsky, Ngai, & Wicentowski, 2000; Diab & Resnik, 2002; Pado \ufffd & Lapata, 2006; Xi & Hwa, 2005). As our focus is on the unattended introduction of translinguistic structures, we do not address this area."}, {"heading": "2.1.1 BILINGUAL LEARNING", "text": "In fact, it is the case that one sees oneself in a position to be in, and that one sees oneself in a position to abide by the rules that one has set oneself in order to fulfil."}, {"heading": "2.1.2 BEYOND BILINGUAL LEARNING", "text": "While most work in multilingual learning focuses on bilingual analysis, some models work on more than one language pair and then combine them: Genzel (2005) describes a method of using a multilingual lexicon from a group of related languages, which first induces bilingual models for each language pair and then combines them. We take a different approach by learning from all languages simultaneously, rather than combining bilingual outcomes. A related strand of research is machine multisource translation (Och & Ney, 2001; Utiyama & Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico, & Cattoni, 2008), which aims to translate from multiple source languages into a single target language."}, {"heading": "2.2 Unsupervised Part-of-Speech Tagging", "text": "In general, the unattended setting allows for the use of declarative knowledge of the relationship between tags and word types, in the form of a dictionary of allowed keywords for most common words. This setup is called \"unsupervised\" by Toutanova and Johnson, but is referred to as \"semisupervised\" in most other papers on the subject (e.g. Goldwater & Griffiths, 2007)."}, {"heading": "3. Models", "text": "The motivating hypothesis of this paper is that patterns of ambiguity differ systematically at the level of language. By considering multiple languages simultaneously, the overall inherent ambiguity in each language can be reduced. However, with the potential benefits of using multilingual information, there is a challenge that reflects syntactic and semantic characteristics. To this end, we are developing models that characterize parallel text streams in multiple languages while maintaining linguistic characteristics and emissions at the same time."}, {"heading": "3.1 Bilingual Unsupervised Tagging: A Merged Node Model", "text": "In the bilingual merged node model, the cross-language context is incorporated by creating common bi-tag nodes for aligned words. However, it would be too strong to insist that aligned words have an identical tag; in fact, it may not even be appropriate to assume that two languages share identical tag sets. However, if two words are aligned, we want to select their tags together. To make this possible, we let the values of the bi-tag nodes range across all possible tag pairs < t, t, t \"> T,\" where T and T represent \"the tagsets for each language.\" The tags t and t \"need not be identical, but we believe they are systematically related. This is modelled using a coupling distribution that is multinomical across all tag pairs. The parameter \u043c is combined with the default transition distribution for the product-of-experts model."}, {"heading": "3.1.1 MERGED NODE MODEL: GENERATIVE STORY", "text": "Our generative history is based on the existence of two tag sets, T and T, and two vocabularies W and W, one for each language. To facilitate representation, we formulate our model with Bigram tag dependencies. In our experiments, however, we use a trigram model (without any Bigram interpolation), which is a trivial extension of the described model."}, {"heading": "3.1.2 MERGED NODE MODEL: INFERENCE", "text": "The aim of our follow-up practice is to obtain transition parameters and emission parameters that can be applied to monolingual test data. Ideally, we would select the parameters that have the highest marginal probability due to the observed words x and alignments a, \u03c6 = argmax. (Ideally, we would select the parameters that have the highest marginal probability due to the structure of our model allows us to decompose the common probability, it is not possible to analytically marginalize all the hidden variables. We resort to the standard Monte Carlo approach, in which marginalization is done by sampling. By repeatedly sampling individual hidden variables according to the corresponding distributions, we obtain a Markov chain that guarantees that convergence is centered to a stationary distribution."}, {"heading": "3.2 Multilingual Unsupervised Tagging: A Latent Variable Model", "text": "The model described in the previous section is designed for multilingual data; as we will see in Section 5, it makes very effective use of this data. However, many resources contain more than two languages: for example, Europarl contains eleven, and the Multext East corpus contains eight. However, this raises the question of how best to make use of all available resources when multilingual data is available.One option would be to train separate bilingual models and then combine their results at test dates, either by coordination or another heuristic one. However, we believe that translingual information reduces ambiguity in training time, so that it would be preferable to learn from multiple languages together during training. Indeed, the results in Section 5 show that joint training exceeds such a choice scheme. Another alternative would be to attempt to expand the bilingual model developed in the previous section. While such expansion is possible in principle, the merged node model does not scale."}, {"heading": "3.2.1 PROPAGATING CROSS-LINGUAL PATTERNS WITH SUPERLINGUAL TAGS", "text": "To learn repeated multilingual patterns, we need to limit the number of values that the multilingual tags can take, and thus the number of distributions they provide. For example, we could allow multilingual tags to take whole-language values from 1 to K, with each full-language value indexing a separate set of tag distributions. Each set of distributions should correspond to a discovered multilingual pattern in the data. One set of distributions could favor nouns in each language, and another might favor verbal distributions, although heterogeneous distributions (e.g. favoring distributions in one language and prepositions in others) are also possible. Instead of fixing the number of translingual tag values to any size, we leave it unlimited by using a non-parametric Bayesian model. To promote the desired multilingual clustering behavior, we use a previous process."}, {"heading": "3.2.2 LATENT VARIABLE MODEL: GENERATIVE STORY", "text": "For n languages, we assume the existence of n tagsets T 1,.., Tn and vocabulary, W 1,.., Wn, one for each language. Table 1 summarizes all relevant parameters. For clarity, the generative process is described only with bigram transition dependencies, but our experiments use a trigram model without bigram interpolations. 1. Transition and emission parameters. For each language, a transition distribution via tags T's and an emission distribution via tags T's are described, but our experiments use a trigram model without bigram interpolations. 1. Transition and emission parameters for each language. 2. Superlingual tag parameters. Draw an infinite sequence of distributions via tags T's,."}, {"heading": "3.2.3 LATENT VARIABLE MODEL: INFERENCE", "text": "As in section 3.1.2, we employ a sample-based inference procedure (i + i i). Again, closed forms are used to analyze the emission parameters (i + i). (i + i). (i) We must still select the part-of-the-language tag and the superlingual tag weighting. (i) The remainder of the section describes the sampling equations for these different linguistic terms. (i) The part-of-the-language tag for the language distribution at position i (yi), (i), c, c, c, c, c, c, c, c), c), c), c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "3.3 Implementation", "text": "This section describes implementation details necessary to reproduce our experiments, details for the merged node and latent variable models, and our monolingual baseline."}, {"heading": "3.3.1 INITIALIZATION", "text": "An initialization phase is required to generate the initial settings for the word tags and hyperparameters, as well as for the superlingual tags in the latent variable model.The initialization is as follows: \u2022 Monolingual model tags: Random, with uniform probability among the tag dictionary entries for the output word. - Hyper parameter \u03b80, \u03c60: Initialized to 1.0 \u2022 Merged node tags: Random, with uniform probability among the tag dictionary entries for the output word. For merged tag nodes, each slot from the tag dictionary of the output word is selected in the appropriate language.- Hyperparameter success0, insp0, perspec0: Initialized to 1.0 \u2022 Latent Variable model tags: Set to the final estimate from the monolingual model. - Overlingual tags: First, a set of 14 translingual tag values is assumed - each value corresponds to a part of the tag."}, {"heading": "3.3.2 HYPERPARAMETER ESTIMATION", "text": "Both models have symmetrical Dirichlet Priors \u03b80 and \u03c60, for the emission and transition distributions, respectively. The merged node model also has symmetrical Dirichlet Priors prior \u03c90 on the coupling parameter. We re-evaluate these Priors during the inference, based on non-informative hyperpriors. Hyperparameter re-evaluation applies the Metropolis-Hastings algorithm after each full epoch of sampling. In addition, we initially perform 200 iterations on velocity convergence. MetropolisHastings is a sampling technique that draws a new value u from a supply distribution and makes a stochastic decision on whether to accept the new sample (Gelman et al., 2004). This decision is based on the proposal distribution and the common probability of u with the observed and sampled variables xygig. We proceed from an improper preceding mass (P) revisibility, which indicates a uniformity of probability."}, {"heading": "3.3.3 FINAL PARAMETER ESTIMATES", "text": "The ultimate goal of the training is to learn models that can be applied to incalculable monolingual data. Therefore, we need to make estimates for the transition and emission parameters \u03c6 and \u03b8. Our sampling method focuses on the tags y. we construct maximum a posteriori estimates y that specify the most likely tag sequences for the aligned training corpus. The predicted tags y are then combined with the priorities \u03c60 and \u03b80 to create maximum a posteriori estimates of transition and emission parameters. These learned parameters are then applied to the monolingual test data to find the most likely tag sequences using the Viterbi algorithm. For the monolingual and fused node models, we perform 200 iterations of the sample and select the modal tag settings in each slot. Further samples did not yield different results. For the latent model, we select the last 1000 sample values from the iterable sample and the last 1000 samples."}, {"heading": "4. Experimental Setup", "text": "We perform a series of empirical evaluations to quantify the contribution of bilingual and multilingual information to the unattended identification of linguistic areas. Our initial evaluation follows the standard procedures established for the unsupervised identification of linguistic areas: Given a tag dictionary (i.e. a set of possible identifiers for each word type), the model selects the appropriate tag for each symbol that occurs in a text. In this section, we first describe the parallel data and annotations on linguistic areas used for the system evaluation (Smith & Eisner, 2005; Goldwater & Griffiths, 2007)."}, {"heading": "4.1 Data", "text": "s novel \"Eighty-Four\" in original English, as well as its translation into seven languages - Bulgarian, Czech, Estonian, Hungarian, Slovenian, Serbian and Romanian.6 Each translation was created by a different translator and published separately in print by various publishers. This data set contains representatives from four language families - Slavic, Romance, Ugric and Germanic.This data is distributed as part of the publicly available Multext East Corpus Version 3 (Erjavec, 2004).The corpus provides detailed morphological annotations at the symbol level, including language building blocks. In addition, a lexicon is provided for each language.5. This proposal is identical to the parameter re-evaluation applied to the Goldwater and Griffiths emission and transition prizes (2007)."}, {"heading": "4.2 Alignments", "text": "In our experiments, we use sentence-level alignments provided for in the Multext East Corpus. Word-level alignments are calculated for each language pair using GIZA + + (Och & Ney, 2003). The approach to dealing with these alignments differs between merged node and latent variable models."}, {"heading": "4.2.1 MERGED NODE MODEL", "text": "We get 28 parallel bilingual corporas by looking at all the pairings of the eight languages. To create one-to-one alignments at word level, we cut the one-to-many alignments in each direction. This process results in an alignment of about half of the marks in each bilingual parallel corpus. We continue to automatically remove crossings as these cycles would trigger in the graphical model. We use a simple heuristic: crossings are removed based on the order in which they appear from left to right; this step eliminates an average of 3.62% of the edges. Table 2 shows the number of aligned words for each language pair after removing crossings. For more detailed statistics on the total number of alignments, see Appendix A."}, {"heading": "4.2.2 LATENT VARIABLE MODEL", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.3 Tagset", "text": "In our experiments, we focused on the main syntactic category, which is encoded as the first letter of the provided labels. In our first experiment, we distinguished between 14 parts of the language, 11 of which are common to all languages in our experiments. Appendix B lists the tag repository for each of the eight languages. In our first experiment, we assumed that a complete tag lexicon is available, so that the set of possible phrases for each word is known in advance. We used the tag dictionaries provided in the Multext East Corpus. The average number of possible tags per token is 1.39. We also experimented with incomplete tag dictionaries, in which entries are only available for words that appear more than five or ten times in the corpus. In other words, the total tag set of 14 labels per day is punctuated."}, {"heading": "4.4 Monolingual Comparisons", "text": "As a monolingual baseline, we use the unsupervised Bayesian Hidden Markov Model (HMM) by Goldwater and Griffiths (2007). This model, which they call BHMM1, modifies the standard HMM by adding priors and executing Bayesian conclusions. Its performance is state-of-the-art in unsupervised models. The Bayesian HMM is a particularly informative baseline, as our model is reduced to this baseline when there is no match in the data. This implies that any increase in performance above the baseline can only be attributed to the multilingual aspect of our model. We used our own implementation after verifying that its performance on the Penn Treebank corpus was identical to that of Goldwater and Griffiths. To provide an additional point of comparison, we use a supervised hidden Markov model trained on the commented corpus."}, {"heading": "4.5 Test Set Inference", "text": "We use the same procedure to apply all models (the monolingual model, the bilingual merged node model, and the latent variable model) to test data. After training, trigram transition and word emission probabilities are calculated using the number of tags assigned in the last training session. Similarly, the final sampled values of the hyperparameters are selected as smoothing parameters. We then apply Viterbi decoding to determine the most likely tag sequences for each monolingual test set. We report results for multilingual and monolingual experiments on average of five passes and for bilingual experiments on average of three passes. The average standard deviation of accuracy over multiple passes is less than 0.25, unless the lexicon is limited to the 100 most common words. In this case, the standard deviation is 1.11 for monolingual model, 0.85 for fused model and 1.40 for the latent model."}, {"heading": "5. Results", "text": "In this section, we will first report on the performance of the two models in complete and reduced lexicon cases. Next, we will report on the results of a semi-monitored experiment in which a subset of languages annotated text during the training. Finally, we will examine the sensitivity of both models to hyperparameter values and provide runtime statistics for the latent variable model to increase the number of languages.7. http: / / nlp.stanford.edu / software / tagger.shtml"}, {"heading": "5.1 Full Lexicon Experiments", "text": "Our experiments show that both the combined node and the latent variable models significantly improve identification accuracy. As the combined node model is limited to language pairs, we provide average results across all possible pairings. In addition, we also consider two methods for combining predictions from multiple bilingual pairs: one with a matching scheme and the other with an oracle to select the best pairings (see below for more details). As shown in row 4 of the table, the combined node model achieves an average accuracy of 93.2%, an improvement of two percentage points over the monolingual basis."}, {"heading": "5.2 Reduced Lexicon Experiments", "text": "In realistic application scenarios, we may not have a tag dictionary with coverage of the entire lexicon. We will consider three reduced lexicon: removing all words with a number of five or less; removing all words with a number of ten or less; and keeping only the 100 most common words. Words that are removed from the lexicon can take on any given day, increasing the overall difficulty of the task. These results are presented in Table 5 and summarized graphically in Figure 3. In all cases, the monolingual model is less robust in terms of reducing lexicon coverage than the multilingual models. In the case of the 100-word lexicon, the latent variable model achieves an accuracy of 57.9%, compared with 53.8% for the monolingual base language. However, the combined node model achieves a slightly higher average performance of 59.5% for the other two scenarios, the latent variable model, which is trained on all eight languages."}, {"heading": "5.3 Indirect Supervision", "text": "Although the main focus of this work is on unattended learning, we also provide some results that suggest that multilingual learning can be applied to scenarios with different amounts of commented data. Indeed, these scenarios are quite realistic, as previously trained and highly accurate markers are usually available for at least some of the languages in a parallel corpus. We apply our latent variable model to these scenarios by simply having the markers of commented data (in any subset of languages) fixed as and observed throughout the sampling process. From a strictly probable perspective, this is the right approach. However, we note that in practice heuristics and objective functions that place greater emphasis on the monitored part of the data can provide better results. We do not examine this possibility here.Table 7 gives results for two scenarios of indirect monitoring: where only one of the eight languages has commented data, and where all but one of the languages has commented data."}, {"heading": "5.4 Hyperarameter Sensitivity and Runtime Statistics", "text": "Both models use hyperparameters for the emission and transition distribution before time (\u03b80 and \u03c60, respectively) and the merged node model uses an additional hyperparameter for the coupling distribution before time (\u03c90). These hyperparameters are all updated during the inference method. However, for the experiments described above, we used two additional hyperparameters that remained unchanged: the concentration parameter of the Dirichlet process (\u03b1) and the parameter of the base distribution for superlingual tags (\u03940). For the experiments described above, we used the initialization values listed in Section 3.3.1. Here, we examine the sensitivity of the models to different initializations of \u03b80, \u03c60 and perspec0 and to different fixed values of \u03b1 and \u03940. Tables 8 and 9 show the results obtained for the merged node and the variable models, each using a complete lexicon. We observe that both models produce very similar results across a wide range of values."}, {"heading": "6. Analysis", "text": "In this section, the following further analyses will be performed: (i) factors influencing the effectiveness of language pairs in bilingual models, (ii) the incremental value of adding more languages in the latent variable model, (iii) the superlingual tags and corresponding cross-lingual patterns learned through the latent variable model, and (iv) whether multilingual data is more helpful than additional monolingual data. We will focus here on the full lexicon scenario, although we expect our analysis to extend to the various reduced lexicon cases considered above."}, {"heading": "6.1 Predicting Effective Language Pairings", "text": "As shown in Table 10, the performance of the merged node model for each target language differs significantly from pairing to pairing. Furthermore, the identity of the optimally helpful language pairing also depends greatly on the respective target language. For example, Slovenian achieves a great improvement when paired with the closely related Slavic language Serbian (+ 7.4 and + 0), but only a slight improvement when paired with English (+ 1.8). On the other hand, the best performance is achieved with Bulgarian when paired with English (+ 6), rather than with closely related Slavic languages (+ 2.4 and + 0). Therefore, optimal pairings do not simply correspond to language kinship. We find that when applying multilingual learning to morphological segmentation, the best results were obtained for related languages, but only after incorporating declarative knowledge of their phonological relationships at a lower level."}, {"heading": "6.1.1 CROSS-LINGUAL ENTROPY", "text": "In an earlier paper (Snyder et al., 2008), we proposed using translingual entropy as a posthumous explanation for the variation in coupling performance, a measure that calculates the entropy of a marker decision in one language given the identity of an aligned tag in the other. While translingual entropy seemed to be well correlated with the relative performance of the four languages considered in this paper, we found that it did not correlate as strongly for all eight languages considered here. We calculated the Pearson correlation coefficient (Myers & Well, 2002) between the relative bilingual performance and translingual entropy, and for each target language, we evaluated the remaining seven languages using two metrics: how well the paired language contributes to improved target performance, and the translingual entropy of the target language in the face of coupled language. We calculated the Pearson correlation coefficient between these two rankings to assess their degree of overlap in the table 19."}, {"heading": "6.1.2 ALIGNMENT DENSITY", "text": "We find that even if translingual entropy had a higher correlation with performance, it would have little practical use in an unattended scenario, since its estimation requires a marked corpus. Next, we consider the density of paired lexical matches between language pairs as a predictive measure of coupled performance. As alignments are the multilingual anchors of our models, greater adaptation density should offer greater opportunities for linguistic transfer as a practical matter. From a linguistic standpoint, this measurement can also indirectly capture correspondence between two languages. Furthermore, this measurement has the advantage that it can be calculated from an unmarked corpus using automatically obtained GIZA + matches. As before, we rate the other languages for each target language by relative bilingual performance and the percentage of words in the target language to which they provide adjustments."}, {"heading": "6.1.3 MODEL CHOICE", "text": "The choice of the model can also contribute to the variability patterns we observe between language pairs. To test this hypothesis, we have applied our latent variable model to all language pairs. The results of this experiment are in Table 11. As in the case of the merged node model, the performance of each target language strongly depends on the choice of partner. However, the exact variability patterns in this case differ from those of the merged node model. To measure this variability, we compare the pair preferences for each language under each of the two models. Specifically, for each target language, we rank the remaining seven languages by their contribution under each of our two models and calculate the Pearson coefficient between these two rankings. As can be seen in the last column of Table 19 in the appendix, we find a coefficient of 0.49 between the two rankings indicating a positive, though far from perfect correlation."}, {"heading": "6.1.4 UTILITY OF EACH LANGUAGE AS A BILINGUAL PARTNER", "text": "We also analyze the overall helpfulness of each language. As before, for each target language, we rank the remaining seven languages according to the degree to which they contribute to increased performance of the target language in a bilingual model. We can then ask whether the helpfulness provided by each of the eight languages is correlated with each other - in other words, whether languages tend to be universally helpful (or not), or whether helpfulness is strongly dependent on the identity of the target language. We look at all pairs of the target languages and calculate the rank correlation between their rankings of the six additional languages they have in common (without the two target languages themselves). If we rate this paired helpfulness as helpfulness, we get a coefficient of 0.20 for the merged node model and 0.21 for the latent variable model. These low correlations indicate that helpfulness is decisively dependent on the language in question."}, {"heading": "6.2 Adding Languages in the Latent Variable Model", "text": "As the number of languages increases, we have the latent variable model with all possible sub-ranges in the full lexicon size. For comparison, the monolingual and average bilingual results should be mentioned. In all scenarios, the accuracy of the available languages is varied."}, {"heading": "6.3 Analysis of the Superlingual Tag Values", "text": "In this section, we analyze the superlingual tags and their corresponding part-of-speech distributions as learned through the latent variable model. Remember that each superlingual tag intuitively represents a discovered multilingual context, and that it is through these that multilingual information is disseminated. Formally, however, each superlingual tag provides a complete distribution for each language, enabling both primary and secondary preferences to be encoded separately for each language. These preferences then interact with the linguistic context (i.e., the surrounding parts of the speech and the corresponding word). We place a dirichlet process in front of the superlingual tags, so that the number of sampled values is dictated by the complexity of the data. Indeed, as shown in Table 12, the number of superlingual tags sampled steadily increases with the number of languages delivered."}, {"heading": "6.3.1 PERFORMANCE WITH REDUCED DATA", "text": "One possible objection to the assertions made in this section is that the improved results may be due merely to the addition of more data, so that the multilingual aspect of the model may be irrelevant. We test this idea by evaluating the monolingual, fused, and latent variable systems of training sets, which reduce the number of examples by half. Multilingual models in this environment have exactly half as much data as the monolingual model in the original experiment. As shown in Table 14, both the monolingual baseline and our models are fairly insensitive to this drop in data. In fact, when trained on half the corpus, our two models still perform better than the monolingual model trained on the entire corpus, suggesting that the performance gains demonstrated by multilingual learning cannot be explained by the addition of additional data alone."}, {"heading": "7. Conclusions", "text": "This year it is more than ever before."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the National Science Foundation (CAREER scholarship IIS0448168 and scholarship IIS-0835445, IIS-0904684) and the Microsoft Research Faculty Fellowship. Thanks to Michael Collins, Tommi Jaakkola, Amir Globerson, Fernando Pereira, Lillian Lee, Yoong Keok Lee, Maria Polinsky and the anonymous reviewers for helpful comments and suggestions. Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of NSF.11. Isolating languages are those with a similar morph-to-word ratio, and synthetic languages are those that allow easy combination of multiple morphs into single words. English is an example of an isolating language, while Hungarian is a synthetic language."}, {"heading": "Appendix A. Alignment Statistics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix C. Stanford Tagger Performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix B. Tag Repository", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix D. Rank Correlation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix E. Universal Helpfulness", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Trainable grammars for speech recognition", "author": ["J. Baker"], "venue": "In Proceedings of the Acoustical Society of America", "citeRegEx": "Baker,? \\Q1979\\E", "shortCiteRegEx": "Baker", "year": 1979}, {"title": "Part-of-speech tagging in context", "author": ["M. Banko", "R.C. Moore"], "venue": "In Proceedings of the COLING,", "citeRegEx": "Banko and Moore,? \\Q2004\\E", "shortCiteRegEx": "Banko and Moore", "year": 2004}, {"title": "Phrase-based statistical machine translation with pivot languages", "author": ["N. Bertoldi", "M. Barbaiani", "M. Federico", "R. Cattoni"], "venue": "In International Workshop on Spoken Language Translation Evaluation Campaign on Spoken Language Translation (IWSLT),", "citeRegEx": "Bertoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2008}, {"title": "Unsupervised sense disambiguation using bilingual probabilistic models", "author": ["I. Bhattacharya", "L. Getoor", "Y. Bengio"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bhattacharya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bhattacharya et al\\.", "year": 2004}, {"title": "Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging", "author": ["E. Brill"], "venue": "Computational Linguistics,", "citeRegEx": "Brill,? \\Q1995\\E", "shortCiteRegEx": "Brill", "year": 1995}, {"title": "Word-sense disambiguation using statistical methods", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Brown et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1991}, {"title": "Improving statistical machine translation efficiency by triangulation", "author": ["Y. Chen", "A. Eisele", "M. Kay"], "venue": "In Proceedings of LREC", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Chiang,? \\Q2005\\E", "shortCiteRegEx": "Chiang", "year": 2005}, {"title": "Machine translation by triangulation: Making effective use of multiparallel corpora", "author": ["T. Cohn", "M. Lapata"], "venue": "In Proceedings of ACL", "citeRegEx": "Cohn and Lapata,? \\Q2007\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2007}, {"title": "Language universals and linguistic typology: Syntax and morphology", "author": ["B. Comrie"], "venue": null, "citeRegEx": "Comrie,? \\Q1989\\E", "shortCiteRegEx": "Comrie", "year": 1989}, {"title": "Two languages are more informative than one", "author": ["I. Dagan", "A. Itai", "U. Schwall"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Dagan et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1991}, {"title": "An unsupervised method for word sense tagging using parallel corpora", "author": ["M. Diab", "P. Resnik"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Diab and Resnik,? \\Q2002\\E", "shortCiteRegEx": "Diab and Resnik", "year": 2002}, {"title": "MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora", "author": ["T. Erjavec"], "venue": "In Fourth International Conference on Language Resources and Evaluation, LREC,", "citeRegEx": "Erjavec,? \\Q2004\\E", "shortCiteRegEx": "Erjavec", "year": 2004}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M. Escobar", "M. West"], "venue": "Journal of the american statistical association,", "citeRegEx": "Escobar and West,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West", "year": 1995}, {"title": "A bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Bayesian data analysis", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Rubin"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2004}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Inducing a multilingual dictionary from a parallel multitext in related languages", "author": ["D. Genzel"], "venue": "In Proceedings of HLT/EMNLP,", "citeRegEx": "Genzel,? \\Q2005\\E", "shortCiteRegEx": "Genzel", "year": 2005}, {"title": "Markov chain Monte Carlo in practice", "author": ["W. Gilks", "S. Richardson", "D. Spiegelhalter"], "venue": null, "citeRegEx": "Gilks et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gilks et al\\.", "year": 1996}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S. Goldwater", "T.L. Griffiths"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Goldwater and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Goldwater and Griffiths", "year": 2007}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "In Proceedings of the Ninth International Conference on Artificial Neural Networks,", "citeRegEx": "Hinton,? \\Q1999\\E", "shortCiteRegEx": "Hinton", "year": 1999}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In Proceedings of EMNLP/CoNLL,", "citeRegEx": "Johnson,? \\Q2007\\E", "shortCiteRegEx": "Johnson", "year": 2007}, {"title": "Experiments in parallel-text based grammar induction", "author": ["J. Kuhn"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Kuhn,? \\Q2004\\E", "shortCiteRegEx": "Kuhn", "year": 2004}, {"title": "Word translation disambiguation using bilingual bootstrapping", "author": ["C. Li", "H. Li"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Li and Li,? \\Q2002\\E", "shortCiteRegEx": "Li and Li", "year": 2002}, {"title": "The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem", "author": ["J.S. Liu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Liu,? \\Q1994\\E", "shortCiteRegEx": "Liu", "year": 1994}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo,? \\Q1994\\E", "shortCiteRegEx": "Merialdo", "year": 1994}, {"title": "Current Issues in Linguistic Theory: Recent Advances in Natural Language Processing, chap. Unsupervised Natural Language Disambiguation Using Non-Ambiguous Words", "author": ["R. Mihalcea"], "venue": null, "citeRegEx": "Mihalcea,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea", "year": 2004}, {"title": "Research Design and Statistical Analysis (2nd edition)", "author": ["J.L. Myers", "A.D. Well"], "venue": null, "citeRegEx": "Myers and Well,? \\Q2002\\E", "shortCiteRegEx": "Myers and Well", "year": 2002}, {"title": "Exploiting parallel texts for word sense disambiguation: an empirical study", "author": ["H.T. Ng", "B. Wang", "Y.S. Chan"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Ng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Statistical multi-source translation", "author": ["F.J. Och", "H. Ney"], "venue": "In MT Summit", "citeRegEx": "Och and Ney,? \\Q2001\\E", "shortCiteRegEx": "Och and Ney", "year": 2001}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "Optimal constituent alignment with edge covers for semantic projection", "author": ["S. Pad\u00f3", "M. Lapata"], "venue": "In Proceedings of ACL, pp", "citeRegEx": "Pad\u00f3 and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3 and Lapata", "year": 2006}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "A perspective on word sense disambiguation methods and their evaluation", "author": ["P. Resnik", "D. Yarowsky"], "venue": "In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What,", "citeRegEx": "Resnik and Yarowsky,? \\Q1997\\E", "shortCiteRegEx": "Resnik and Yarowsky", "year": 1997}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N.A. Smith", "J. Eisner"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Smith and Eisner,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of the ACL/HLT,", "citeRegEx": "Snyder and Barzilay,? \\Q2008\\E", "shortCiteRegEx": "Snyder and Barzilay", "year": 2008}, {"title": "Unsupervised multilingual learning for pos tagging", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "Adding more languages improves unsupervised multilingual part-of-speech tagging: A bayesian non-parametric approach", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Snyder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2009}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Toutanova and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2008}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of NAACL/HLT,", "citeRegEx": "Utiyama and Isahara,? \\Q2006\\E", "shortCiteRegEx": "Utiyama and Isahara", "year": 2006}, {"title": "Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora", "author": ["D. Wu"], "venue": "In IJCAI,", "citeRegEx": "Wu,? \\Q1995\\E", "shortCiteRegEx": "Wu", "year": 1995}, {"title": "Machine translation with a stochastic grammatical channel", "author": ["D. Wu", "H. Wong"], "venue": "In Proceedings of the ACL/COLING,", "citeRegEx": "Wu and Wong,? \\Q1998\\E", "shortCiteRegEx": "Wu and Wong", "year": 1998}, {"title": "A backoff model for bootstrapping resources for non-English languages", "author": ["C. Xi", "R. Hwa"], "venue": "In Proceedings of EMNLP, pp", "citeRegEx": "Xi and Hwa,? \\Q2005\\E", "shortCiteRegEx": "Xi and Hwa", "year": 2005}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["D. Yarowsky", "G. Ngai", "R. Wicentowski"], "venue": "In Proceedings of HLT,", "citeRegEx": "Yarowsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 10, "context": "Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991).", "startOffset": 111, "endOffset": 151}, {"referenceID": 5, "context": "Word sense disambiguation (WSD) was among the first successful applications of automated multilingual learning (Dagan et al., 1991; Brown et al., 1991).", "startOffset": 111, "endOffset": 151}, {"referenceID": 5, "context": "Bilingual data has been leveraged in this way in a variety of WSD models (Brown et al., 1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002; Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual data closely approximates that of manual annotation (Ng et al.", "startOffset": 73, "endOffset": 216}, {"referenceID": 29, "context": ", 1991; Resnik & Yarowsky, 1997; Ng, Wang, & Chan, 2003; Diab & Resnik, 2002; Li & Li, 2002; Bhattacharya, Getoor, & Bengio, 2004), and the quality of supervision provided by multilingual data closely approximates that of manual annotation (Ng et al., 2003).", "startOffset": 240, "endOffset": 257}, {"referenceID": 0, "context": "The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified bilingual grammar.", "startOffset": 29, "endOffset": 42}, {"referenceID": 7, "context": "These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005).", "startOffset": 120, "endOffset": 151}, {"referenceID": 39, "context": "Multilingual learning has previously been applied to syntactic analysis; a pioneering effort was the inversion transduction grammar of Wu (1995). This method is trained on an unannotated parallel corpus using a probabilistic bilingual lexicon and deterministic constraints on bilingual tree structures.", "startOffset": 135, "endOffset": 145}, {"referenceID": 0, "context": "The inside-outside algorithm (Baker, 1979) is used to learn parameters for manually specified bilingual grammar. These ideas were extended by subsequent work on synchronous grammar induction and hierarchical phrase-based translation (Wu & Wong, 1998; Chiang, 2005). One characteristic of this family of methods is that they were designed for inherently multilingual tasks such as machine translation and lexicon induction. While we share the goal of learning from multilingual data, we seek to induce monolingual syntactic structures that can be applied even when multilingual data is unavailable. In this respect, our approach is closer to the unsupervised multilingual grammar induction work of Kuhn (2004). Starting from the hypothesis that trees induced over parallel sentences should exhibit cross-lingual structural similarities, Kuhn uses word-level alignments to constrain the set of plausible syntactic constituents.", "startOffset": 30, "endOffset": 709}, {"referenceID": 16, "context": "For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages.", "startOffset": 14, "endOffset": 28}, {"referenceID": 16, "context": "For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. This work first induces bilingual models for each pair of languages and then combines them. We take a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och & Ney, 2001; Utiyama & Isahara, 2006; Cohn & Lapata, 2007; Chen, Eisele, & Kay, 2008; Bertoldi, Barbaiani, Federico, & Cattoni, 2008) where the goal is to translate from multiple source languages to a single target language. By using multi-source corpora, these systems alleviate sparseness and increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation systems build separate bilingual models and then select a final translation from their output. For instance, a method developed by Och and Ney (2001) generates several alternative translations from source sentences expressed in different languages and selects the most likely candidate.", "startOffset": 14, "endOffset": 957}, {"referenceID": 8, "context": "Cohn and Lapata (2007) consider a different generative model: rather than combining alternative sentence translations in a post-processing step, their model estimates the target phrase translation distribu-", "startOffset": 0, "endOffset": 23}, {"referenceID": 22, "context": "Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities.", "startOffset": 56, "endOffset": 100}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.", "startOffset": 63, "endOffset": 78}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004).", "startOffset": 63, "endOffset": 290}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities. Such Bayesian priors permit integration over parameter settings, yielding models that perform well across a range of settings. This is particularly important in the case of small datasets, where many of the counts used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work. Several recent papers have explored the development of alternative training procedures and model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number of overlapping features in a conditional log-linear formulation.", "startOffset": 63, "endOffset": 1346}, {"referenceID": 21, "context": "This setup is referred to as \u201csemisupervised\u201d by Toutanova and Johnson (2008), but is considered \u201cunsupervised\u201d in most other papers on the topic (e.g., Goldwater & Griffiths, 2007). Our evaluation considers tag dictionaries of varying levels of coverage. Since the work of Merialdo (1994), the hidden Markov model (HMM) has been the most common representation2 for unsupervised tagging (Banko & Moore, 2004). Part-of-speech tags are encoded as a linear chain of hidden variables, and words are treated as emitted observations. Recent advances include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater & Griffiths, 2007), which places prior distributions on tag transition and word-emission probabilities. Such Bayesian priors permit integration over parameter settings, yielding models that perform well across a range of settings. This is particularly important in the case of small datasets, where many of the counts used for maximum-likelihood parameter estimation will be sparse. The Bayesian setting also facilitates the integration of other data sources, and thus serves as the departure point for our work. Several recent papers have explored the development of alternative training procedures and model structures in an effort to incorporate more expressive features than permitted by the generative HMM. Smith and Eisner (2005) maintain the HMM structure, but incorporate a large number of overlapping features in a conditional log-linear formulation. Contrastive estimation is used to provide a training criterion which maximizes the probability of the observed sentences compared to a set of similar sentences created by perturbing word order. The use of a large set of features and a discriminative training procedure led to strong performance gains. Toutanova and Johnson (2008) propose an LDA-style model for unsupervised part-of-speech tagging, grouping words through a latent layer of ambiguity classes.", "startOffset": 63, "endOffset": 1801}, {"referenceID": 20, "context": "Haghighi and Klein (2006) also use a variety of morphological features, learning in an undirected Markov Random Field that permits overlapping features.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995; Mihalcea, 2004)", "startOffset": 102, "endOffset": 131}, {"referenceID": 27, "context": "In addition to the basic HMM architecture, other part-of-speech tagging approaches have been explored (Brill, 1995; Mihalcea, 2004)", "startOffset": 102, "endOffset": 131}, {"referenceID": 21, "context": "Product-of-expert models (Hinton, 1999) allow each information source to exercise very strong negative influence on the probability of tags that they consider to be inappropriate, as compared with additive models.", "startOffset": 25, "endOffset": 39}, {"referenceID": 25, "context": "This technique is known as collapsed sampling; it is guaranteed never to increase sampling variance, and will often reduce it (Liu, 1994).", "startOffset": 126, "endOffset": 137}, {"referenceID": 19, "context": "The integral is tractable due to Dirichlet-multinomial conjugacy, and an identical marginalization was applied in the monolingual Bayesian HMM of Goldwater and Griffiths (2007). For unaligned tags, it is also possible to exactly marginalize the parameter \u03c6 governing transitions.", "startOffset": 146, "endOffset": 177}, {"referenceID": 14, "context": "To encourage the desired multilingual clustering behavior, we use a Dirichlet process prior (Ferguson, 1973).", "startOffset": 92, "endOffset": 108}, {"referenceID": 35, "context": "At the same time, draw an infinite sequence of mixture weights \u03c0 \u223c GEM(\u03b1), where GEM(\u03b1) indicates the stick-breaking distribution (Sethuraman, 1994) with concentration parameter \u03b1 = 1.", "startOffset": 130, "endOffset": 148}, {"referenceID": 15, "context": "MetropolisHastings is a sampling technique that draws a new value u from a proposal distribution, and makes a stochastic decision about whether to accept the new sample (Gelman et al., 2004).", "startOffset": 169, "endOffset": 190}, {"referenceID": 15, "context": "We observe an acceptance rate of approximately 1/6, which is in line with standard recommendations for rapid convergence (Gelman et al., 2004).", "startOffset": 121, "endOffset": 142}, {"referenceID": 12, "context": "This data is distributed as part of the publicly available Multext-East corpus, Version 3 (Erjavec, 2004).", "startOffset": 90, "endOffset": 105}, {"referenceID": 38, "context": "In our initial publication (Snyder et al., 2008), we used a subset of this data, only including sentences that have one-to-one alignments between all four languages considered in that paper.", "startOffset": 27, "endOffset": 48}, {"referenceID": 19, "context": "This proposal is identical to the parameter re-estimation applied for emission and transition priors by Goldwater and Griffiths (2007). 6.", "startOffset": 104, "endOffset": 135}, {"referenceID": 33, "context": "We apply the standard maximum-likelihood estimation and perform inference using Viterbi decoding with pseudo-count smoothing for unknown words (Rabiner, 1989).", "startOffset": 143, "endOffset": 158}, {"referenceID": 19, "context": "As our monolingual baseline we use the unsupervised Bayesian hidden Markov model (HMM) of Goldwater and Griffiths (2007). This model, which they call BHMM1, modifies the standard HMM by adding priors and by performing Bayesian inference.", "startOffset": 90, "endOffset": 121}, {"referenceID": 19, "context": "The accuracy of the monolingual English tagger is relatively high compared to the 87% reported by Goldwater and Griffiths (2007) on the WSJ corpus.", "startOffset": 98, "endOffset": 129}, {"referenceID": 38, "context": "In a previous publication (Snyder et al., 2008) we proposed using cross-lingual entropy as a posthoc explanation for variation in coupling performance.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "An important direction for future work is to incorporate even more sources of multilingual information, such as additional languages and declarative knowledge of their typological properties (Comrie, 1989).", "startOffset": 191, "endOffset": 205}], "year": 2009, "abstractText": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.", "creator": null}}}