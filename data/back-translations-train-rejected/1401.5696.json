{"id": "1401.5696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the Web", "abstract": "The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of resolvers probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.", "histories": [["v1", "Wed, 15 Jan 2014 05:33:07 GMT  (448kb)", "http://arxiv.org/abs/1401.5696v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander pieter yates", "oren etzioni"], "accepted": false, "id": "1401.5696"}, "pdf": {"name": "1401.5696.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the Web", "authors": ["Alexander Yates", "Oren Etzioni"], "emails": ["yates@temple.edu", "etzioni@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "One problem that becomes a real challenge of this magnitude is that HOW systems often extract claims that contain millions of different strings from the Web (e.g., the TextRunner system from Banko, Cafarella, Soderland, Broadhead, & Etzioni, 2007). One problem that becomes a real challenge is that HOW systems often extract claims that describe the same real object or relationship using different names. For example, a HOW system might also have an Extraktc \u00a9 2009 AI Access Foundation. All rights reserved."}, {"heading": "2. Previous Work", "text": "The resolution of synonyms involves two tasks, including searching for synonyms for extracted objects and relations; resolving objects is very similar to the task of transverse documentary resolution (Bagga & Baldwin, 1998), in which the goal is to capture cluster events of named entities in corporate groups; Pedersen and Kulkarni (Pedersen & Kulkarni, 2007; Kulkarni & Pedersen, 2008), whose names work in web documents and in e-mails using agglomerative entities and a heuristic similarity; Li, Morie, and Roth (2004a, 2004b), use a graphical model of maximizing and databases of common nicknames, titles, etc.to achieve high accuracy on a cross-document entity-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-task-to-title-title-title-title-title, etc.to achieve high accuracy on a cross-document entity-task-task-task-task-list using vectors, and relic-relic-vectors."}, {"heading": "3. The Formal Synonym Resolution Problem", "text": "A synonym resolution system for HOW takes a series of extractions as input and returns a set of clusters, each cluster containing synonymous object strings or relation strings. Specifically, the input is a dataset D containing extracted assertions of the form a = (r, o1,...), where r is a relation string and each oi is an object string representing the arguments for the relationship. In this work, all assertions are taken as binary, i.e. n = 2. The output of a synonym resolution system is a cluster or group of clusters of strings in D. Leave S the set of all different strings in D. A cluster in S is a set of C-2S, so that all clusters in C are different, and they cover the entire set: c-C = S-c1, c2-C-c2 = GP. Each cluster in the output cluster represents the assumption of the system that all the synonings within the cluster are not strings and no strings outside the cluster."}, {"heading": "3.1 The Single-Sense Assumption", "text": "The formal representation of the synonym resolution described above makes an important simplistic assumption: each string is assumed to belong to exactly one cluster. In language, however, strings often have multiple meanings; i.e., they are polysemous. Polysemous strings cannot be adequately represented by using an instance in which each string belongs exactly to a cluster. For most of this paper, we will make the one-dimensional assumption, but Section 8 illustrates an extension to resolver that removes these assumptions. As an example of the representation problems posed by polysemy, we consider the name \"President Roosevelt.\" In certain contexts, this name is synonymous with \"President Franklin D. Roosevelt\" as a cluster of ability, but in other contexts it is synonymous with \"President Theodore Roosevelt.\""}, {"heading": "3.2 Subproblems in Synonym Resolution", "text": "The problem of resolving synonyms can be divided into two subproblems: firstly, how to measure the similarity or probability of synonyms between pairs of strings in S; and secondly, how to form clusters in such a way that all elements in each cluster are very similar to each other and relatively little similarity to elements in other clusters. Resolver uses a generative probabilistic model to find the similarity between strings. In strings si and sj, let Ri, j be the random variable for the event that si and sj refer to the same entity. Let R t i, j the event that Ri, j is true, and R f i, j the event that it is false. Let Dx denote the set of extractions in D that contain stringx. Given D and S, the first partial task of resolving synonyms is to find P (Rti, j | Dsi, Dsj) for all pairs si and sj."}, {"heading": "4. Models for String Comparisons", "text": "Our probabilistic model provides a formal, rigorous method of resolving synonyms in the absence of training data. It has two sources of evidence: the similarity of the strings themselves (i.e., working distance) and the similarity of the assertions in which they occur. This second source of evidence is sometimes referred to as distributional similarity (Hindle, 1990).Section 4.1 presents a simple model for predicting whether a string pair is synonymous based on the similarity of the strings. Section 4.2 then presents a model called Extracted Shared Property (ESP) model for predicting whether a string pair will co-reference based on its distributional similarity. Section 4.3 compares the ESP model with other methods for calculating distributional similarity to give an intuition of how it will behave. Finally, Sections 4.4 and 4.5 provide a method for combining the ESP model and the string similarity model to obtain a general prediction of synonyms between two strings."}, {"heading": "4.1 String Similarity Model", "text": "Many objects appear with multiple names, which are substrings, acronyms, abbreviations, or other simple variations of each other. Thus, the similarity of string can be an important source of whether two strings refer to each other (Cohen, 1998). The probabilistic string similarity model (SSM) of the resolver assumes a similarity function sim (s1, s2): STRING \u00d7 STRING \u2192 [0, 1]. The model determines the probability of s1, which co-references s2, to a smoothed version of similarity: P (Rti, j | sim (s1, s2)) = \u03b1 \u0445 sim (s1, s2) + 1\u03b1 + \u03b2As \u03b1, the probability transitions from 1 / \u03b2 (at \u03b1 = 0) to the value of the similarity function (for very large \u03b1) increase. The particular choice of \u03b1 and \u03b2 makes little difference to the results of the resolution, so long as they are chosen so that the resulting probability lies below 1 / \u03b2 (at \u03b1 = 0) between the similarity function and the element (1) (or the similarity function = 1)."}, {"heading": "4.2 The Extracted Shared Property Model", "text": "This year, the number of cases mentioned in the United States has increased many times, many times more than the number of cases mentioned."}, {"heading": "4.3 Comparison of ESP with Other Distributional Similarity Metrics", "text": "It is important to note that sMI, as we describe it here, is our own implementation of the similarity of text (DIRT) (Lin & Pantel, 2001) is not the complete systematics that we describe only briefly. (D) It is important to note that sMI, as we describe it, is our own implementation of the similarity of text (DIRT) and Pantel (2001). (D) It is important to note that sMI, as we describe it here, is not available to us. (D) It is important to note that sMI, as we describe it, is our own implementation of the similarity of text (DIRT) and Pantel (2001). (D) It is important to note that sMI, as we describe it here. (D) It is important to note that sMI, as we describe it, is our own implementation of the similarity of text (DIRT) and Pantel (2001)."}, {"heading": "4.4 Combining the Evidence", "text": "For every possible synonymous relationship, Resolver considers two probable proofs. Let Egg, j be the proof of ESP and let E s i, j be the proof of SSM. Our method of combining the two uses Na \ufffd \u0131ve Baye's assumption that each piece of evidence is conditionally independent, since the synonymous relationship is given: P (Esi, j, E e, j | Ri, j) = P (Esi, j | Ri, j) P (Eei, j | Ri, j) (6) Given this simplistic assumption, we can combine the proofs to find the probability of a correlation by applying Bayes \"rule to both sides (we omit the i, j indexes for the brevity): P (Rt | Es, Ee) = P (R t | Ee) P (Rt | Ee) P (Rt | Ee) (1 \u2212 P (Rt))."}, {"heading": "4.5 Comparing Clusters of Strings", "text": "Our algorithm fuses string clusters using the above models. However, these models give probabilities for synonymous decisions between two individual strings, not two string clusters. We have experimented with several different methods for determining the probability of synonymy from the individual probability values for each string pair, using the same assumptions of independence. This approach provides a formal probability framework for the problem, which is easy and efficient to calculate. In other experiments, we have found that simply the mean or geometric averages of the string pairs are included in the probability modeling (or even the harmonic mean) of the string pair results are slightly improved. For completeness, we now offer a brief explanation of the probability calculation for the clusters."}, {"heading": "5. Resolver\u2019s Clustering Algorithm", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "5.1 Algorithm Analysis", "text": "The following analyses show that an iteration of linkages takes more time than a complete analysis of linkages between linkages in step 4. To simplify the analysis, we consider only those properties that contain a relationship between linkages and an argumentation of 1 string. (The same analysis leads to a slightly looser linkages of linkages that are even better than those of O (D). The number of linkages is derived from the size of the linkages of linkages of linkages between the linkages of linkages, linkages and linkages. (D) The number of linkages is given by the size of the linkages of the linkages of the linkages of the comparisons, which are made for each property that is limited by the upper limit. (D) The number of linkages is determined by the size of the linkages of linkages of linkages, linkages, linkages, and linkages of the comparisons of the comparisons of the comparisons, which is determined by the number of the number of (the number of the number of the number of D)."}, {"heading": "5.2 Relation to Other Speed-Up Techniques", "text": "In fact, most of them are able to go in search of new ways that they are able to outwit themselves. Most of them are able to go in search of new ways that they describe. Most of them are able to go in search of new ways that they describe. The search for new ways that they describe. The search for new ways that they describe. The search for new ways that they describe. The search for new ways that they describe. The search for new ways that they describe. The search for new ways that they describe. You describe.The search for new ways that they describe. You describe.You describe.The search for new ways that they describe.You describe.You describe all. The search for new ways that they describe.You describe.You describe.You describe.You describe.You describe."}, {"heading": "6. Experiments", "text": "The first experiment compares the performance of the various similarity metrics and shows that the output clusters of the resolver are significantly better than those of ESP or SSM, and that the ESP clusters are significantly better than those of sMI or CSM. The second experiment measures the sensitivity of the ESP model to its hidden parameters and shows that it is able to outperform both the sMI and CSM models in a very wide range of parameter settings."}, {"heading": "6.1 Experimental Setup", "text": "The models are tested on a dataset of 2.1 million assertions extracted from a web crawl. All models run over all assertions, but only compare the objects or relationships that appear at least 25 times in the data to give the distributional similarity models sufficient data to estimate the similarity. Although this limitation limits the applicability of resolvers, we note that it is intuitive that this should be necessary for uncontrolled clustering, since such systems by definition start with no knowledge of a string. You need to see some examples before it is reasonable to make decisions about them. We also note that Downey, Schoenmackers, and Etzioni (2007) have shown for another problem how bootstrapping techniques can leverage the power of high-frequency examples to produce accurate models for low-frequency items.Only correct nouns3 are compared, and only those relations that do not contain punctuation or uppercase letters."}, {"heading": "6.2 Clustering Analysis", "text": "Our first experiment compares the accuracy and retrieval of clusters with five similarity metrics: Two types of previous work used in paraphrase discovery, CSM and sMI; two components of resolver, ESP and SSM; and the full resolver system.The accuracy and retrieval of cluster formation is measured as follows: Hypothesis clusters are matched with gold clusters so that each hypotheses cluster matches no more than one gold cluster, and vice versa. This imaging is calculated to maximize the number of elements in hypotheses clusters that overlap with elements in the matching gold clusters; all of these intersecting elements are marked as correct; all elements in a hypotheses cluster that do not overlap with the corresponding gold cluster threshold are marked as false or irrelevant if they do not occur at all in the gold clusters."}, {"heading": "6.3 Sensitivity Analysis", "text": "The ESP model requires a parameter for the number of potential properties of a string, but the performance of ESP is not very sensitive to the exact value of this parameter. As described in Section 4.2, we assume that the number of potential properties is multiples of the number of extractions for a string. In the experiments above, we chose values of N = 30 for objects and N = 500 for relationships, as they worked well on held data. However, Tables 3 and 4 show that the actual values of these parameters can vary across a wide range, while ESP is still able to surpass sMI and CSM. In these experiments, we measured precision and callback for only the similarity metrics, without performing any clustering. We used similarity metrics to sort the pairs of strings (but only those pairs that share at least one property), correcting in descending order of similarity."}, {"heading": "6.4 Discussion", "text": "In all experiments, ESP outperforms both CSM and sMI. Sensitivity analysis shows that this is true for a wide range of hidden parameters for ESP, both for objects and for relationships. In addition, the improvement of ESP over the comparative metrics is true when the metrics are used in clustering the data. sMI's performance is broadly the same as CSM in each experiment. Somewhat surprisingly, sMI performs worse in both the similarity experiments and the cluster experiments than in clustering, although it is designed for similarity. Results show that the three distribution similarity models below the SSM model work in their own way for both objects and relationships, both in the similarity experiments and in the cluster experiments, with the one exception being the cluster experiment for relationships where SSM had a bad memory and therefore had lower F1 values than ESP and CSM. This is to be expected as a CSM, MESI, MSI and very MI signal prediction."}, {"heading": "7. Similar and Identical Pairs", "text": "As the above error analysis suggests, similar objects that are not exact synonyms represent a large fraction of Resolver's errors. In this section, three techniques are described to deal with such errors. For example, Resolver is likely to make a mistake with the pair Virginia and West Virginia. They share many characteristics because they have the same type (US states), and they have a high similarity of strings. Perhaps the simplest approach to determining that these two are not synonymous is simply to collect more data about them. Although they are highly similar, they certainly will not share all of their characteristics; they have different governors, for example. However, the amount of data required to decide that they are not identical is huge and simply not available.Fortunately, there are more complex decision-making techniques using the available data. One approach is to consider the distribution of words that occur between candidate synonyms as synonyms."}, {"heading": "7.1 Web Hitcounts for Synonym Discovery", "text": "While names for two similar objects often occur together in the same sentence, it is relatively rare for two different names of the same object to occur in the same sentence. Furthermore, synonymous pairs tend to occur in an idiosyncratic context that is distinctly different from the contexts of similar pairs. Resolver takes advantage of this fact by querying the web to determine how often a pair of strings appears together in a large corpus in certain contexts. If the number of hits is high, Resolver can prevent merging. Specifically, if a candidate is the synonym pair s1 and s2, the coordination phrase filter uses a discriminator set (Etzioni et al., 2005) of the form \"s1 and s2.\" It then calculates 4. It is also a function.a variant of pointedly reciprocal information obtained by coordination result (s1, s2) = hit (s1 and s2) = hit (s2)."}, {"heading": "7.2 Function Filtering", "text": "Functions and reverse functions can help distinguish between similar and identical pairs. For example, Virginia and West Virginia have different uppercase letters: Richmond and Charleston. If these two facts are extracted, and Resolver knows that the relationship capital is a reverse function, it should prevent Virginia and West Virginia from merging. Given a synonym pair x1 and x2, the function filter prevents mergers between strings that have different values for the same function. Specifically, it decides that two strings y1 and y2 match if their string similarity is above a high threshold. It prevents merging between x1 and x2 if a function f and extractions f (x1, y1) and f (x2, y2) exist, and there are no such extractions, so that y1 and y2 match (and vice versa for reverse functions). Experiments described in Section 7.4 show that the function filter can improve the precision of pre-selected functions, without actually affecting them by means of a pop function."}, {"heading": "7.3 Function Weighting", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "7.4 Experiments", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "8.2 Experiment with Cross-Document Entity Resolution", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "9. Conclusion and Future Work", "text": "In fact, it is so that it is a matter of a pure structure which is capable of hiding itself in a position, without it being a matter of a structure, in which it is a matter of a structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is not a matter of an structure, but of a structure, in which it is a matter of a structure, in which it is a matter of an structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, of a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a, a structure, a, a structure, a, a, a structure, a, a structure, a, a structure, a, a structure, a structure, a, a structure, a, a structure, a, a structure, a structure, a, a structure, a structure, a structure, a structure, a, a structure, a structure, a, a, a structure, a structure, a, a structure, a structure, a, a structure, a, a structure, a, a, a, a structure, a, a structure, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "Acknowledgments", "text": "This research was partially supported by Temple University, NSF grants IIS-0535284 and IIS-0312988, ONR grants N00014-08-1-0431, and gifts from Google, and was conducted at the Turing Center of the University of Washington and the Center for Information Science and Technology of Temple University. We thank the anonymous reviewers and the JAIR co-editor responsible for this work for their helpful comments and suggestions. We would also like to thank the KnowItAll group at the University of Washington for their feedback and support."}, {"heading": "Appendix A. Derivation of the Extracted Shared Property Model", "text": "It is a method for calculating the probability that two strings are synonymous, since they share a certain number of extractions in a dataset. This appendix gives a derivation of the properties that are shared between the two properties. Let si and sj be two strings, each with a set of extracted properties egg and Ej. Let Ui and Uj be the random variable for the synonymous relationship between si and sj, where Ri, j = R denotes the event that they are, and Rfi, j that they do not match. ESP model states that the probability of Rti, j contains the probability of selecting the observed number of matching properties from two urns."}, {"heading": "Appendix B. Fast Calculation of the Extracted Shared Property Model", "text": "The ESP model can be expensive to calculate if it is done incorrectly, so we use two > techniques to accelerate the calculation immensely. For reference, the complete formulation of the model is: P (Rti, j | k, ni, nj, Pi, Pj) = (Pmin k) \u2211 r, s \u2265 0 (Si, j \u2212 k r + s) (Pi \u2212 Pmin ni \u2212 (k + r)))) (Pj \u2212 Pmin nj \u2212 (k + s)))) \u2211 k \u2264 Si, j \u2264 Pmin (Si, j k) \u0445 r, s \u2265 0 (Si, j \u2212 r + s) (Pi \u2212 Si, j \u2212 ni \u2212 r)))) (Pj \u2212 Si, j nj \u2212 Pmin (k + s))) (19) Note that the equation includes three sums exceeding O (Pmin), O (ni) and O (nj \u2212 r)."}, {"heading": "Appendix C. A Better Bound on the Number of Comparisons Made by", "text": "The resolution algorithms mSection 4 showed that the resolution cluster algorithms initially have a number of times that are considered in terms of the number of extractions. < M (N log N) is then better than O (M2) for Zipf distributed data is controlled by a form parameter we call z. The above assertion applies to each form parameter z < 2, as shown below, in natural data the form parameter is usually very close to z = 1, and in resolver data it was observed to z < 1.Let S be the set of distinct strings in a series of extractions D. For each s-S, let freq (s) the number of extractions."}], "references": [{"title": "Web Information Extraction and User Information Needs: Towards Closing the Gap", "author": ["E. Agichtein"], "venue": "IEEE Data Engineering Bulletin issue on Web-Scale Data,", "citeRegEx": "Agichtein,? \\Q2006\\E", "shortCiteRegEx": "Agichtein", "year": 2006}, {"title": "Web people search: results of the first evaluation and the plan for the second", "author": ["J. Artile", "S. Sekine", "J. Gonzalo"], "venue": "In Proceeding of the 17th international conference on World Wide Web", "citeRegEx": "Artile et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Artile et al\\.", "year": 2008}, {"title": "Entity-based cross-document coreferencing using the vector space model", "author": ["A. Bagga", "B. Baldwin"], "venue": "In COLING-ACL", "citeRegEx": "Bagga and Baldwin,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin", "year": 1998}, {"title": "Open information extraction from the web", "author": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": "In IJCAI", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment", "author": ["R. Barzilay", "L. Lee"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Barzilay and Lee,? \\Q2003\\E", "shortCiteRegEx": "Barzilay and Lee", "year": 2003}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["R. Barzilay", "K. McKeown"], "venue": "In Proceedings of ACL/EACL", "citeRegEx": "Barzilay and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Barzilay and McKeown", "year": 2001}, {"title": "Unsupervised learning of contextual role knowledge for coreference resolution", "author": ["D. Bean", "E. Riloff"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL)", "citeRegEx": "Bean and Riloff,? \\Q2004\\E", "shortCiteRegEx": "Bean and Riloff", "year": 2004}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In Proceeings of the 23rd International Conference on Machine Learning (ICML)", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Relational Clustering for Multi-type Entity Resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In 11th ACM SIGKDD Workshop on Multi Relational Data Mining", "citeRegEx": "Bhattacharya and Getoor,? \\Q2005\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2005}, {"title": "Query-time entity resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In KDD", "citeRegEx": "Bhattacharya and Getoor,? \\Q2006\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2006}, {"title": "Support vector machines for paraphrase identification and corpus construction", "author": ["C. Brockett", "W.B. Dolan"], "venue": "In International Workshop on Paraphrasing", "citeRegEx": "Brockett and Dolan,? \\Q2005\\E", "shortCiteRegEx": "Brockett and Dolan", "year": 2005}, {"title": "Noun Phrase Coreference as Clustering", "author": ["C. Cardie", "K. Wagstaff"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora", "citeRegEx": "Cardie and Wagstaff,? \\Q1999\\E", "shortCiteRegEx": "Cardie and Wagstaff", "year": 1999}, {"title": "Providing database-like access to the web using queries based on textual similarity", "author": ["W.W. Cohen"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data", "citeRegEx": "Cohen,? \\Q1998\\E", "shortCiteRegEx": "Cohen", "year": 1998}, {"title": "A comparison of string distance metrics for name-matching tasks. In IIWeb", "author": ["W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": null, "citeRegEx": "Cohen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2003}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1990}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions", "author": ["D. Davidov", "A. Rappoport"], "venue": "In Proceedings of the ACL", "citeRegEx": "Davidov and Rappoport,? \\Q2008\\E", "shortCiteRegEx": "Davidov and Rappoport", "year": 2008}, {"title": "Reference reconciliation in complex information spaces", "author": ["X. Dong", "A. Halevy", "J. Madhavan"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2005}, {"title": "A Probabilistic Model of Redundancy in Information Extraction", "author": ["D. Downey", "O. Etzioni", "S. Soderland"], "venue": null, "citeRegEx": "Downey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2005}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["D. Downey", "S. Schoenmackers", "O. Etzioni"], "venue": "In ACL", "citeRegEx": "Downey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2007}, {"title": "Unsupervised named-entity extraction from the web: An experimental study", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A. Popescu", "T. Shaked", "S. Soderland", "D. Weld", "A. Yates"], "venue": "Artificial Intelligence,", "citeRegEx": "Etzioni et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2005}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proceedings of the 25th Conference on Very Large Databases (VLDB)", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Unsupervised Coreference Resolution in a Nonparametric Bayesian Model", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proceedings of the ACL", "citeRegEx": "Haghighi and Klein,? \\Q2007\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2007}, {"title": "Discovering relations among named entities from large corpora", "author": ["T. Hasegawa", "S. Sekine", "R. Grishman"], "venue": "In Proceedings of the ACL", "citeRegEx": "Hasegawa et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hasegawa et al\\.", "year": 2004}, {"title": "The merge/purge problem for large databases", "author": ["M.A. Hernandez", "S.J. Stolfo"], "venue": null, "citeRegEx": "Hernandez and Stolfo,? \\Q1995\\E", "shortCiteRegEx": "Hernandez and Stolfo", "year": 1995}, {"title": "Noun classification from predicage-argument structures", "author": ["D. Hindle"], "venue": null, "citeRegEx": "Hindle,? \\Q1990\\E", "shortCiteRegEx": "Hindle", "year": 1990}, {"title": "Word Sense Disambiguation: The State of the Art", "author": ["N. Ide", "J. Veronis"], "venue": "Computational Linguistics,", "citeRegEx": "Ide and Veronis,? \\Q1998\\E", "shortCiteRegEx": "Ide and Veronis", "year": 1998}, {"title": "Probabilistic coreference in information extraction", "author": ["A. Kehler"], "venue": "In EMNLP", "citeRegEx": "Kehler,? \\Q1997\\E", "shortCiteRegEx": "Kehler", "year": 1997}, {"title": "The (non)utility of predicateargument frequencies for pronoun interpretation", "author": ["A. Kehler", "D. Appelt", "L. Taylor", "A. Simma"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL)", "citeRegEx": "Kehler et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kehler et al\\.", "year": 2004}, {"title": "Statistical predicate invention", "author": ["S. Kok", "P. Domingos"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Machine Learning", "citeRegEx": "Kok and Domingos,? \\Q2007\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2007}, {"title": "Name Discrimination and Email Clustering Using Unsupervised Clustering of Similar Contexts", "author": ["A. Kulkarni", "T. Pedersen"], "venue": "Journal of Intelligent Systems (Special Issue: Recent Advances in Knowledge-Based Systems and Their Applications),", "citeRegEx": "Kulkarni and Pedersen,? \\Q2008\\E", "shortCiteRegEx": "Kulkarni and Pedersen", "year": 2008}, {"title": "An algorithm for pronominal anaphora resolution", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational Linguistics,", "citeRegEx": "Lappin and Leass,? \\Q1994\\E", "shortCiteRegEx": "Lappin and Leass", "year": 1994}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "In Proceedings of the 37th ACL", "citeRegEx": "Lee,? \\Q1999\\E", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Word clustering and disambiguation based on co-occurence data", "author": ["H. Li", "N. Abe"], "venue": "In COLING-ACL,", "citeRegEx": "Li and Abe,? \\Q1998\\E", "shortCiteRegEx": "Li and Abe", "year": 1998}, {"title": "Identification and tracing of ambiguous names: Discriminative and generative approaches", "author": ["X. Li", "P. Morie", "D. Roth"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "Robust reading: Identification and tracing of ambiguous names", "author": ["X. Li", "P. Morie", "D. Roth"], "venue": "In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "DIRT \u2013 Discovery of Inference Rules from Text", "author": ["D. Lin", "P. Pantel"], "venue": null, "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "An investigation of practical approximate nearest neighbor algorithms", "author": ["T. Liu", "A.W. Moore", "A. Gray", "K. Yang"], "venue": "In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Unsupervised personal name disambiguation", "author": ["G. Mann", "D. Yarowsky"], "venue": "In CoNLL", "citeRegEx": "Mann and Yarowsky,? \\Q2003\\E", "shortCiteRegEx": "Mann and Yarowsky", "year": 2003}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schuetze"], "venue": null, "citeRegEx": "Manning and Schuetze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Schuetze", "year": 1999}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["A. McCallum", "K. Nigam", "L. Ungar"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Conditional models of identity uncertainty with application to noun coreference", "author": ["A. McCallum", "B. Wellner"], "venue": null, "citeRegEx": "McCallum and Wellner,? \\Q2004\\E", "shortCiteRegEx": "McCallum and Wellner", "year": 2004}, {"title": "Using decision trees for coreference resolution", "author": ["J. McCarthy", "W. Lehnert"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence", "citeRegEx": "McCarthy and Lehnert,? \\Q1995\\E", "shortCiteRegEx": "McCarthy and Lehnert", "year": 1995}, {"title": "Introduction to WordNet: An on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": "International Journal of Lexicography,", "citeRegEx": "Miller et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "The Field Matching Problem: Algorithms and Applications", "author": ["A.E. Monge", "C. Elkan"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "Monge and Elkan,? \\Q1996\\E", "shortCiteRegEx": "Monge and Elkan", "year": 1996}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["V. Ng", "C. Cardie"], "venue": null, "citeRegEx": "Ng and Cardie,? \\Q2002\\E", "shortCiteRegEx": "Ng and Cardie", "year": 2002}, {"title": "Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences", "author": ["B. Pang", "K. Knight", "D. Marcu"], "venue": "In Proceedings of HLT/NAACL", "citeRegEx": "Pang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2003}, {"title": "Unsupervised Discrimination of Person Names in Web Contexts", "author": ["T. Pedersen", "A. Kulkarni"], "venue": "In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics", "citeRegEx": "Pedersen and Kulkarni,? \\Q2007\\E", "shortCiteRegEx": "Pedersen and Kulkarni", "year": 2007}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st ACL", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Information Extraction from Unstructured Web Text", "author": ["Popescu", "A.-M"], "venue": "Ph.D. thesis,", "citeRegEx": "Popescu and A..M.,? \\Q2007\\E", "shortCiteRegEx": "Popescu and A..M.", "year": 2007}, {"title": "A hierarchical graphical model for record linkage", "author": ["P. Ravikumar", "W.W. Cohen"], "venue": "In UAI", "citeRegEx": "Ravikumar and Cohen,? \\Q2004\\E", "shortCiteRegEx": "Ravikumar and Cohen", "year": 2004}, {"title": "Learning Dictionaries for Information Extraction by Multilevel Bootstrapping", "author": ["E. Riloff", "R. Jones"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Riloff and Jones,? \\Q1999\\E", "shortCiteRegEx": "Riloff and Jones", "year": 1999}, {"title": "Introduction to Modern Information", "author": ["Yates", "G. Etzioni Salton", "M. McGill"], "venue": null, "citeRegEx": "Yates et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Yates et al\\.", "year": 1983}, {"title": "Automatic Paraphrase Discovery based on Context and Keywords", "author": ["S. Hill. Sekine"], "venue": null, "citeRegEx": "Sekine,? \\Q2005\\E", "shortCiteRegEx": "Sekine", "year": 2005}, {"title": "Entity Resolution with Markov Logic", "author": ["P. Paraphrasing. Singla", "P. Domingos"], "venue": null, "citeRegEx": "Singla and Domingos,? \\Q2006\\E", "shortCiteRegEx": "Singla and Domingos", "year": 2006}, {"title": "Overview of the TREC-2002 question-answering track", "author": ["E. COLING/ACL. Voorhees"], "venue": "In TREC. Winkler, W", "citeRegEx": "Voorhees,? \\Q2002\\E", "shortCiteRegEx": "Voorhees", "year": 2002}, {"title": "Selective Studies and the Principle of Relative Frequency in Language", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf,? \\Q1932\\E", "shortCiteRegEx": "Zipf", "year": 1932}], "referenceMentions": [{"referenceID": 0, "context": "Web Information Extraction (WIE) systems (Zhu, Nie, Wen, Zhang, & Ma, 2005; Agichtein, 2006; Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, & Yates, 2005) extract assertions that describe a relation and its arguments from Web text.", "startOffset": 41, "endOffset": 174}, {"referenceID": 38, "context": "Mann and Yarowsky (2003) use a combination of extracted features and term vectors including proper names in context to cluster ambiguous names on the Web.", "startOffset": 0, "endOffset": 25}, {"referenceID": 53, "context": "Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005).", "startOffset": 114, "endOffset": 156}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system. More recently, Haghighi and Klein (2007) use a graphical model combining local salience features and global entity features to perform unsupervised coreference, achieving an F1 score of 70.", "startOffset": 0, "endOffset": 293}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system. More recently, Haghighi and Klein (2007) use a graphical model combining local salience features and global entity features to perform unsupervised coreference, achieving an F1 score of 70.1 on MUC-6. Two systems use automatically extracted information to help make coreference resolution decisions, much like Resolver does. Kehler, Appelt, Taylor, and Simma (2004) use statistics over automatically-determined predicateargument structures to compare contexts between pronouns and their potential antecedents.", "startOffset": 0, "endOffset": 618}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases.", "startOffset": 0, "endOffset": 1076}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations.", "startOffset": 0, "endOffset": 1687}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations. Davidov and Rappoport (2008) use a heuristic clustering method to find groups of relation patterns that can be used to extract instances.", "startOffset": 0, "endOffset": 1774}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations. Davidov and Rappoport (2008) use a heuristic clustering method to find groups of relation patterns that can be used to extract instances. Hasegawa et al. (2004) automatically extract relationships from a large corpus and cluster relations, using the Cosine Similarity Metric (Salton & McGill, 1983) and a hierarchical clustering technique like Resolver\u2019s.", "startOffset": 0, "endOffset": 1906}, {"referenceID": 32, "context": "Resolver\u2019s method of determining the similarity between two strings is an example of a broad class of metrics called distributional similarity metrics (Lee, 1999), but it has significant advantages over traditional distributional similarity metrics for the synonym resolution task.", "startOffset": 151, "endOffset": 162}, {"referenceID": 25, "context": "\u201d (Hindle, 1990) Previous distributional similarity metrics, however, have been designed for comparing words based on terms appearing in the same document, rather than extracted properties.", "startOffset": 2, "endOffset": 16}, {"referenceID": 27, "context": "Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla & Domingos, 2006), but of course these systems depend on the availability of training data, and even on a significant number of labeled examples per relation of interest.", "startOffset": 72, "endOffset": 137}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model.", "startOffset": 14, "endOffset": 27}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model. Several other recent approaches leverage domain-specific information and heuristics for object resolution. For example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. Winkler (1999) provides a survey of this area.", "startOffset": 14, "endOffset": 731}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model. Several other recent approaches leverage domain-specific information and heuristics for object resolution. For example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. Winkler (1999) provides a survey of this area. Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla & Domingos, 2006), but of course these systems depend on the availability of training data, and even on a significant number of labeled examples per relation of interest. One promising new approach to clustering in a relational domain is the Multiple Relational Clusterings (MRC) algorithm (Kok & Domingos, 2007). This approach, though not specific to synonym resolution, can find synonyms in a set of unlabeled, relational extractions without domain-specific heuristics. The approach is quite recent, and so far no detailed experimental comparison has been conducted. Resolver\u2019s probabilistic model is partly inspired by the ball-and-urns abstraction of information extraction presented by Downey, Etzioni, and Soderland (2005) Resolver\u2019s task and probability model are different from theirs, but many of the same modeling as-", "startOffset": 14, "endOffset": 1612}, {"referenceID": 25, "context": "This second source of evidence is sometimes referred to as distributional similarity (Hindle, 1990).", "startOffset": 85, "endOffset": 99}, {"referenceID": 12, "context": "evidence for whether two strings co-refer (Cohen, 1998).", "startOffset": 42, "endOffset": 55}, {"referenceID": 18, "context": "ESP models the extraction of assertions as a generative process, much like the URNS model (Downey et al., 2005).", "startOffset": 90, "endOffset": 111}, {"referenceID": 36, "context": "It is important to note that sMI as we describe it here is our own implementation of the similarity metric described by Lin and Pantel (2001), and is not the complete DIRT system.", "startOffset": 120, "endOffset": 142}, {"referenceID": 23, "context": "We now compare ESP with one of the more popular of these metrics, the Cosine Similarity Metric (CSM), which has previously been used in synonym resolution work (Mann & Yarowsky, 2003; Hasegawa et al., 2004).", "startOffset": 160, "endOffset": 206}, {"referenceID": 56, "context": "The parameter z is known as the Zipf parameter, and for naturally-occurring text it has typically been observed to be around 1 (Zipf, 1932; Manning & Schuetze, 1999).", "startOffset": 127, "endOffset": 165}, {"referenceID": 37, "context": "Resolver operates in a space of hundreds of thousands of dimensions (the number of distinct extract properties), while the fastest of these techniques have been applied to spaces of around a few thousand dimensions (Liu et al., 2004).", "startOffset": 215, "endOffset": 233}, {"referenceID": 23, "context": "Several experiments below test Resolver and ESP, and demonstrate their improvement over related techniques in paraphrase discovery, sMI (Lin & Pantel, 2001) and the Cosine Similarity Metric (CSM) (Salton & McGill, 1983; Hasegawa et al., 2004; Mann & Yarowsky, 2003).", "startOffset": 196, "endOffset": 265}, {"referenceID": 20, "context": "Specifically, given a candidate synonym pair s1 and s2, the Coordination-Phrase Filter uses a discriminator phrase (Etzioni et al., 2005) of the form \u201cs1 and s2\u201d.", "startOffset": 115, "endOffset": 137}, {"referenceID": 55, "context": "We tested Resolver\u2019s ability to handle polysemous names on a data set of 300 documents from 1998-2000 New York Times articles in the TREC corpus (Voorhees, 2002).", "startOffset": 145, "endOffset": 161}, {"referenceID": 34, "context": "Li et al. (2004b) automatically ran a named-entity tagger on these documents and manually corrected them to identify approximately 4,000 occurrences of people\u2019s names.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "Li et al. (2004b) automatically ran a named-entity tagger on these documents and manually corrected them to identify approximately 4,000 occurrences of people\u2019s names. They then manually annotated the occurrences to form a gold standard set of coreferential clusters. For each named entity occurrence in this data set, we extracted the set of the closest E named entities, with E set to 100, to represent the context for the named entity occurrence. We then ran Resolver to cluster the entity occurrences. We set ESP\u2019s latent parameter N to 30, as in the experiments above. We did not have any development data to set the merge threshold, so we used the following strategy: we arbitrarily picked a single occurrence of a common name from this data set (Al Gore), found a somewhat uncommon variant of the name (Vice President Al Gore), and set the threshold at a value just below the similarity score for this pair (7.5). For every round of merging in Resolver\u2019s clustering algorithm, we filtered the top 20 proposed merges using the Coordination Phrase Filter, with the same threshold as used in the previous experiments. Li et al. propose a generative model of entity coreference that we compare against. Their model requires databases of information about titles, first names, last names, genders, nicknames, and common transformations of these attributes of people\u2019s names to help compute the probability of coreference. It uses Expectation-Maximization over the given data set to compute parameters, and an inference algorithm that is O(N2) in the number of word occurrences N . Full details are provided by Li et al. (2004b).", "startOffset": 0, "endOffset": 1630}, {"referenceID": 34, "context": "Following Li et al., we evaluate clusters using precision and recall calculated as follows: let Op be the set of entity occurrence pairs that are predicted to be coreferential (i.e., they belong to the same cluster), and let Oa denote the set of correct coreferential pairs, as calculated from the manual clustering. Then precision P = |Op\u2229Oa| |Op| , recall R = |Op\u2229Oa| |Oa| , and F1 = 2PR P+R . Table 8 shows the results of running Resolver on this data set, as well as the best results reported by Li et al. (2004b) on the same data.", "startOffset": 10, "endOffset": 518}, {"referenceID": 34, "context": "Following Li et al., we evaluate clusters using precision and recall calculated as follows: let Op be the set of entity occurrence pairs that are predicted to be coreferential (i.e., they belong to the same cluster), and let Oa denote the set of correct coreferential pairs, as calculated from the manual clustering. Then precision P = |Op\u2229Oa| |Op| , recall R = |Op\u2229Oa| |Oa| , and F1 = 2PR P+R . Table 8 shows the results of running Resolver on this data set, as well as the best results reported by Li et al. (2004b) on the same data. 5 In follow-up work, Li et al. (2004a) demonstrate that their unsupervised model outperforms three supervised techniques that learn parameters for how much different attributes (first name, honorifics, etc.", "startOffset": 10, "endOffset": 575}, {"referenceID": 34, "context": "In follow-up work, Li et al. (2004a) report an F1 score of 95.", "startOffset": 19, "endOffset": 37}, {"referenceID": 40, "context": "Heuristic methods like the Canopies method (McCallum et al., 2000) require O(M2) comparisons, where M is the number of distinct strings in the data.", "startOffset": 43, "endOffset": 66}], "year": 2009, "abstractText": "The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fullyimplemented system that runs in O(KN log N) time in the number of extractions, N , and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of Resolver\u2019s probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.", "creator": "dvips(k) 5.96 Copyright 2007 Radical Eye Software"}}}