{"id": "1401.5697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "abstract": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.", "histories": [["v1", "Wed, 15 Jan 2014 05:21:01 GMT  (599kb)", "http://arxiv.org/abs/1401.5697v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["evgeniy gabrilovich", "shaul markovitch"], "accepted": false, "id": "1401.5697"}, "pdf": {"name": "1401.5697.pdf", "metadata": {"source": "CRF", "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "authors": ["Evgeniy Gabrilovich", "Shaul Markovitch"], "emails": ["gabr@yahoo-inc.com", "shaulm@cs.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in a world in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which they, in fact, live, in which they, in which they, in fact, in which they, in which they, in fact, in which they, in which they, in which they, in fact, in which they, in a world, in which they, in which they, are able to put themselves, are able to put themselves, in a world, in"}, {"heading": "2. Explicit Semantic Analysis", "text": "Another way to interpret the meaning of \"cat\" is by the strength of its association with concepts we know: \"cat\" refers strongly to the terms \"cat\" and \"pet,\" slightly less strongly to the terms \"mouse\" and \"Tom & Jerry\" etc. We use this latter association-based method to assign semantic interpretation to words and text fragments. We assume the availability of a vector of basic concepts, C1, and we represent each text fragment t by a vector of weights, w1, etc., wn, where wi represents the strength of the association between t and Ci. Thus, the set of basic concepts can be considered as canonical n-dimensional septic space, and the sequence of this text segment corresponds in the sequence, whereby we refer to the vector very effectively as a vector."}, {"heading": "2.1 Using Wikipedia as a Repository of Basic Concepts", "text": "In order to build a general semantic interpretation that can represent the meaning of texts for a variety of tasks, the set of basic concepts must meet the following requirements: 1. It should be sufficiently comprehensive to include concepts in a wide variety of topics; 2. It should be constantly maintained so that new concepts can be added immediately as necessary; 3. Since the ultimate goal is to interpret natural language, we want the concepts to be natural, that is, concepts to be recognized and used by humans; 4. Each concept should have a text so that we can determine the strength of its affinity with any term in the language."}, {"heading": "2.2 Building a Semantic Interpreter", "text": "Given a set of concepts, C1,., Cn, and a set of associated documents, d1,.., dn, we build a sparse table T, in which each of the n columns corresponds to a concept, and each of the rows corresponds to a word that occurs in i = 1... n di. An entry T [i, j] in the table corresponds to the TFIDF value of the term ti in the document djT [i, j] = tf (ti, dj) \u00b7 log n dfi, 3. Here we use the titles of articles as a convenient way to refer to the articles, but our algorithm treats the articles as atomic concepts. Where the frequency of the term astf (ti, dj) = {1 + log count (ti, dj), if count (ti, dj) > 0, and dfi = | dk: ti, vdk} is the number of documents in the collection that contain the term ti."}, {"heading": "2.3 Using the Link Structure", "text": "It is only natural for an electronic encyclopaedia to provide hyperlinks. As a result, a typical Wikipedia article has many more links to other entries than articles in conventional printed encyclopaedias. This link structure can be used in many ways. Note that each link is associated with an anchor text (clickable highlighted expression), the anchor text is not always identical to the canonical name of the target article, and different anchor texts are used to refer to the same article in different contexts. Thus, anchor texts that refer to the Federal Reserve include, for example, \"Fed,\" \"US Federal Reserve Board,\" \"US Federal Reserve System,\" Board of Governors of the Federal Reserve, \"Federal Reserve Bank,\" \"Foreign Exchange Reserves,\" and \"Free Banking Era.\" Therefore, anchor texts provide alternative names, variable spellings, and related formulations of the target concept that we use to provide the next generation kernel text frequently to mirror the target text."}, {"heading": "2.3.1 Second-order Interpretation", "text": "Knowledge concepts can be subject to many relationships, including generalization, meronymy (\"part of\"), holonymy and synonymy, as well as more specific relationships such as \"capital of,\" \"place of birth / date of,\" etc. Wikipedia is a notable example of a knowledge repository that has such relationships represented by the hypertext links between Wikipedia articles. These links encode a large amount of knowledge that is not found in article texts. As a result, the use of this knowledge is likely to lead to better models of interpretation. We therefore distinguish between first-order models that use only the knowledge encrypted in Wikipedia articles, and second-order models that include the knowledge encrypted in article texts. Similarly, we refer to the information gained by linking articles as second-order information.As a rule, the existence of a link implies a relationship between the terms it connects."}, {"heading": "2.3.2 Concept Generality Filter", "text": "Not all new concepts identified by links are equally useful. Relevance of the newly added concepts is certainly important, but not the only criterion. Suppose we get an input text called \"Google Search.\" Which additional concept is probably more useful to describe the input: Nigritude ultramarine (a specially crafted meaningless phrase used in a search engine optimization competition) or website? Let's assume that the input is \"artificial intelligence\" - which concept is likely to contribute more to the representation of that input, John McCarthy (computer scientist) or logic? We believe that in both examples the second concept would be more useful because it is not overly specific. Consequently, we assume that we should add linked concepts sparingly by only those that are \"more general\" than the concepts that triggered them. But how can we judge the universality of the concepts? While this is generally difficult (no pun, we suggest the following two numbers)."}, {"heading": "3. Using Explicit Semantic Analysis for Computing Semantic", "text": "Relationship of TextsIn this section we discuss the application of our semantic interpretation methodology to the automatic assessment of the semantic relationship of words and texts.5"}, {"heading": "3.1 Automatic Computation of Semantic Relatedness", "text": "How related are \"cat\" and \"mouse\"? And what about \"preparing a manuscript\" and \"writing an article\"? The ability to quantify the semantic connections of texts underlies many basic tasks in computational linguistics, including word decoding, information extraction, word and text clustering, and error correction (Budanitsky & Hirst, 2006). Reflections on the semantic connections of natural language utterances are routinely performed by humans, but remain an insurmountable obstacle for computers. Humans do not judge text connections merely at the level of text words. Words trigger reflections on a much deeper level that manipulate concepts - the fundamental units of meaning that serve to organize and share human knowledge. Thus, people interpret the specific formulation of a document in the much larger context of their background knowledge and experience."}, {"heading": "3.2 Implementation Details", "text": "Although Wikipedia has nearly a million articles, not all of them are equally useful for generating features. Some articles correspond to overly specific concepts (e.g. Metnal, the ninth level of the Mayan underworld) or are otherwise unlikely to be useful for later text categorization (e.g. certain dates or a list of events that occur in a given year); other articles are simply too short so that we cannot reliably classify texts according to the corresponding concepts. We developed a series of simple heuristics to circumvent the set of terms by discarding articles with fewer than 100 continuous words or fewer than 5 incoming and outgoing links; we also discard articles that describe specific data, as well as Wikipedia disambiguations, category pages and the like. After cutting out, 171,332 articles were left that use these rare terms for the first generation of 15,000 articles."}, {"heading": "3.2.1 Preprocessing of Wikipedia XML Dump", "text": "Wikipedia data is available online at http: / / download.wikimedia.org. All data is distributed in XML format, and several packaged versions are available: article texts, editing history, page titles list, interlingual links, etc. In this project, we only use article texts, but ignore information about article authors and page change history. \u2022 Before building the semantic interpreter, we perform a series of operations on the distributed XML dump: \u2022 We simplify the original XML by removing all fields not used in the feature generation, such as author id and last modification times. \u2022 Wikipedia syntax defines a proprietary format for linking between articles, while the name of the article to which reference is enclosed in parentheses (e.g. \"[USA]\"). We map all articles to numerical IDs, and for each article, a list of IDs is created."}, {"heading": "3.2.2 The Effect of Knowledge Breadth", "text": "Wikipedia is constantly expanding with new material as volunteer editors contribute new articles and expand existing articles. Consequently, we suspected that such addition of information would be advantageous to ESA as it would be based on a larger knowledge base. To verify this assumption, we also acquired a recent Wikipedia snapshot dated March 26, 2006. Table 1 compares the amount of information between two Wikipedia snapshots we used. The number of articles displayed in the table reflects the total number of articles at the time the snapshot was taken. The next table row (the number of concepts used) reflects the number of concepts that remain after the truncation, as explained at the beginning of Section 3.2. In the following sections, we will confirm that the use of a larger knowledge base is advantageous for ESA by comparing the results with the two Wikipedia snapshots. Therefore, no further reduction in dimensions is carried out, and each text snapshot is displayed within a range of 1,332 to 1,332 features (within a later Wikipedia feature range)."}, {"heading": "3.2.3 Inverted Index Pruning", "text": "We eliminate false associations between articles and words by setting the weights of these concepts to zero, whose weights are too low for a given term. The inverted index truncation algorithm works as follows: We first sort all concepts for a given word by their TFIDF weights in decreasing order. We then scan the resulting sequence of concepts with a sliding window of length 100 and shorten the sequence in which the difference in scores between the first and last concepts in the window falls below 5% of the highest value concept for that word (which is positioned first in the sequence). This technique provides for rapid crashes in the concept of scores, which would mean that the concepts in the tail of the sequence are only loosely associated with the word (i.e., although the word appeared in the articles that correspond to these concepts, it is not really characteristic of the article content."}, {"heading": "3.2.4 Processing Time", "text": "Using world knowledge requires additional calculations. This additional calculation includes the (one-time) pre-processing step where the semantic interpreter is built, as well as the actual online mapping of input texts into interpretation vectors. On a standard workstation, parsing the Wikipedia XML dump takes about 7 hours, and building the semantic interpreter takes less than one hour. After building the semantic interpreter, its throughput (i.e. generating interpretation vectors for text input) is several hundred words per second. Given the improvements in calculating semantic relationships and the accuracy of the text categorization we report in Sections 3 and 4, we believe that the additional processing time is well compensated."}, {"heading": "3.3 Empirical Evaluation of Explicit Semantic Analysis", "text": "Human judgments based on a reference set of text pairs can therefore by definition be considered correct, a kind of \"gold standard\" by which computer algorithms are evaluated. Several studies have measured correlations between judges and found them to be consistently high (Budanitsky & Hirst, 2006; Jarmasz, 2003; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002a), r = 0.88 \u2212 0.95. These results are to be expected - after all, it is this consensus that enables people to understand each other. Consequently, our evaluation boils down to calculating the correlation of ESA correlations with human judgments.In order to better evaluate the Wikipedia-based semantic interpretation, we have also implemented a semantic interpreter based on another large-scale knowledge repository - the Web Epideology Directory - the largest intelligence (http: / DP) category."}, {"heading": "3.3.1 Test Collections", "text": "In this paper, we use two sets of data that, to the best of our knowledge, are the largest publicly available collections of their kind. (6) For both sets of tests, we use the correlation of computer-associated scores with human scores to evaluate algorithmic performance. To evaluate word kinship, we use the WordSimilarity-353 collection (Finkelstein et al., 2002a; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002b), which contains 353 word pairs representing different degrees of similarity. (7) Each pair has 13-16 human judgments made by individuals with university degrees who have either native or otherwise very fluent command of the English language. Word pairs have been assigned matching values on the scale from 0 (fully related words) to 10 (very related or identical words)."}, {"heading": "3.3.2 Prior Work", "text": "A number of previous studies suggested a variety of approaches to calculating word similarity using WordNet, Roget's Thesaurus, and LSA. Table 2 presents the results of applying these approaches to the WordSimilarity 353 test collection. Jarmasz (2003) replicated the results of several WordNet-based methods and compared them to a new approach based on Roget's Thesaurus. Hirst and St-Onge (1998) considered WordNet as a graph and considered the length and directness of the graph connecting two nodes. Leacock and Khodorov (1998) also used the length of the shortest graph path, and normalized it by the maximum taxonomy depth. Jiang and Conrath (1997), and later Resnik (1999), used the notion of the information content of the lowest node subsuming two given words. Lin (1998b) suggested a compilation of word similarity."}, {"heading": "3.3.3 Results", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves by surviving themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are not able to survive themselves. \"(...) Most of them are able to survive themselves.\" (...) Most of them are able to survive themselves. \"(...) Most of them are not able to survive themselves.\""}, {"heading": "4. Using Explicit Semantic Analysis for Text Categorization", "text": "In this section, we evaluate the benefits of using external knowledge for text categorization."}, {"heading": "4.1 Background on Text Categorization", "text": "Text categorization (TC) deals with the mapping of category names to natural language documents. Categories come from a set of labels (possibly organized in a hierarchy), and each document can be mapped to one or more categories. Text categorization systems are therefore useful in a variety of tasks, such as forwarding messages and emails to appropriate desks, identifying junk e-mails, or correctly handling intelligence reports. The majority of existing text categorization systems represent text as a bag of words and use a variant of the vector space model with different weighting schemes (Salton & McGill, 1983). The features commonly used in text classification are weighted frequencies of individual words. State categorization systems for text categorization use a variety of production techniques, such as support for vector machines, k-next-neighbor algorithms, and neural networks."}, {"heading": "4.2 Using ESA for Feature Generation", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "4.3 Test Collections", "text": "We have a brief description of the test categories we have used to evaluate our methodology. We have a much more detailed description of these test categories in Appendix B. 1. Reuters-21578 (Reuters, 1997) is historically the most commonly used dataset in the text category, but we have the ModApte breakdown (9603) and two categories of documents that relate to 10 largest categories and 90 categories. 2. Newcomer (20NG) is a well-balanced dataset of 20 categories, each containing 1000 documents. 3. Movie Reviews (movies) (movies) (movies) that deal with the topic, and Vaithyanathan (2002) defines a sensitive task in which the one's positive or negative opinion about the movies is expressed."}, {"heading": "4.4 Experimentation Procedure", "text": "We used support vector machines14 as our learning algorithm to build text indicators, as previous studies have found that SVMs perform best for text categorization (Sebastiani, 2002; Dumais, Platt, Heckerman, & Sahami, 1998; Yang & Liu, 1999). According to common practice, we use the Precision Recall Break Even Point (BEP) to measure the performance of text categorization. BEP is defined with reference to the standard measures of precision and recall, whereby precision is the percentage of true mapping of document categories among all mappings predicted by the classifier, and the percentage of applicable document categories predicted by the classifier. BEP is achieved by either matching the classifier so that precision is the same as memory, or by including multiple (Precision, return points) the indicators of all indicators, including the macroindicators of all indicators, BEP is expected to include the indicators of all indicators, BEP is expected to include the points of all indicators, BEP is expected to include the indicators of all indicators."}, {"heading": "4.4.1 Text Categorization Infrastructure", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "4.4.3 Using the Feature Generator", "text": "The core engine of Explicit Semantic Analysis was implemented as described in Section 3.2. We used the multi-resolution approach to feature generation, where document contexts were classified at the level of individual words, complete sentences, paragraphs, and ultimately the entire document. 20 For each context, features were generated from the ten best matching concepts produced by the feature generator."}, {"heading": "4.5 Wikipedia-based Feature Generation", "text": "In this section we report on the results of an experimental evaluation of our methodology."}, {"heading": "4.5.1 Qualitative Analysis of Feature Generation", "text": "To illustrate our approach, we show functions generated for multiple fragments of text. Wherever applicable, we provide brief explanations of the concepts generated; in most cases, the explanations are taken from Wikipedia (Wikipedia, 2006). \u2022 Text: \"Wal-Mart supply chain goes in real time.\" Top 10 generated functions: (1) Wal-Mart; (2) Sam Walton; (3) Sears Holdings Corporation; (4) Target Corporation; (5) Albertsons; (6) ASDA; (7) RFID; (8) Hypermarket; (9) United Food and Commercial Workers; (10) Chain storeSelected explanations: (2) Wal-Mart founder; (5) prominent competitors of WalMart; (6) a Wal-Mart subsidiary in the UK; (7) Radio Frequency, a technology that Wal-Mart very commonly uses to manage its shares."}, {"heading": "4.5.2 The Effect of Feature Generation", "text": "Table 4 shows the results of using the Wikipedia-based feature generation, showing significant improvements (p < 0.05) in bold. The different rows of the table correspond to the performance of the various data sets and their subsets, as defined in Section 4.3. We consistently observed greater improvements in macro-averaged BEP dominated by the effectiveness of categorization in small categories, consistent with our expectations that the contribution of encyclopaedic knowledge should be particularly prominent for categories with few training examples. Categorization performance was improved for virtually all data sets, with notable improvements of up to 30.4% for RCV1 and 18% for OHSUMED. Using the Wilcoxon test, we found that the Wikipedia-based baseline classifier with p < 10 \u2212 5 is significantly superior in both micro- and macro-averaged cases."}, {"heading": "4.5.3 Classifying Short Documents", "text": "We hypothesized that Wikipedia-based feature generation should be particularly useful for classifying short documents. Table 5 presents the results of this evaluation using the data sets defined in Section 4.3. In most cases, feature generation resulted in greater improvements for short documents than for normal documents. OHSUMED, where \"pure\" experimentation with short documents is possible (see Section 4.3), has shown that the Wikipedia-based baseline classifier with p < 2 \u00b7 10 \u2212 6 is significantly superior to the baseline classifier with p < 2 \u00b7 10 \u2212 6. These results support our hypothesis that encyclopaedic knowledge should be particularly useful when it comes to categorizing short documents that are insufficiently represented by the standard vocabulary."}, {"heading": "4.5.4 Using Inter-article links as Concept Relations", "text": "As we can see in Table 6, the use of links to generate more general characteristics is a better strategy in the absolute majority of cases. As we explain in Section 2.3, linkages between articles can be regarded as relationships between concepts represented by the articles. Consequently, the use of these linkages allows us to identify additional concepts related to the context analyzed, resulting in a better representation of the context with additional relevant generated characteristics."}, {"heading": "5. Related Work", "text": "In the past, there have been a number of attempts to present the meaning of natural language, and this task has subsequently proved to be very difficult and not very advanced."}, {"heading": "5.1 Semantic Similarity and Semantic Relatedness", "text": "In this study, we look at the \"semantic relationship of words\" rather than \"semantic similarity\" or \"semantic distance,\" which are also often used in literature. In their extensive study of kinship measurements, which relate to the way the individual words are used, Budanitsky and Hirst (2006) argue that the concept of kinship is much more general than the concept of similarity, as if the earlier concept of similarity encompassed many different types of specific relationships, including the meaning of words that are related to each other. They also argued that computer-aided linguistic applications often require measurements of similarity rather than narrowly defined measures of similarity, such as similarity. For example, literal meaning can be used to discambiguate words from context and not just similar words. Budanitsky and Hirst (2006) also argued that the concept of semantic distance could be confusing due to the different ways that it is used in the literature to relatedness from our appraisal approach to something semantic."}, {"heading": "5.2 Feature Generation for Text Categorization", "text": "In fact, it is such that most of them will be able to put themselves in a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5.2.1 Feature Generation Using Electronic Dictionaries", "text": "Several studies conducted with the help of WordNet's electronic dictionary have failed to yield benefits for larger tasks that have suffered even from some accuracy; several other studies conducted with the help of WordNet's electronic dictionary (Fellbaum, 1998) and other domain-specific dictionaries (Scott, 1998; Scott & Matwin, 1999; Urena-Lopez, Buenaga, & Gomez, 2001; Wang, McKay, Abbass, & Barlow, 2003; Bloehdorn & Hotho, 2004).Scott and Matwin (1999) have attempted to add additional features to traditional word representation by using the symbolic classification system Ripper (Cohen, 1995).This study evaluated words based on syntactic terms and statistically motivated phrases, as well as on WordNet syntax synses23. In the latter case, the system generalized using the hypernym hierarchy of WordNet and replaced bags of words with syntheses."}, {"heading": "5.2.2 Using Unlabeled Examples", "text": "To the best of my knowledge, with the exception of the above studies using WordNet, there have been no attempts to automatically use large repositories of structured background knowledge to generate features. An interesting approach to using unstructured background knowledge has been suggested by Zelikovitz and Hirsh (2000), but this paper uses a collection of unlabeled examples as an intermediary to compare test examples with the training examples. In particular, if an unknown test instance does not appear to resemble labeled training instances, unlabeled examples that are similar to the two can be used as \"bridges.\" This approach makes it possible to handle the situation where the training and the test document have few or no words in common. Unlabeled documents are used to define a cosmic similarity metric that is then used by the KNN algorithm for actual text categorization."}, {"heading": "6. Conclusions", "text": "In this paper, we have proposed an explicit semantic analysis - a semantic interpretation methodology that does not necessarily address two topics. To make computers with knowledge of the world, we use Wikipedia to build a semantic interpreter that represents the meaning of texts in a very high-dimensional space of knowledge-based concepts. These concepts correspond to Wikipedia articles, and our methodology provides a fully automatic way to tap into the collective knowledge of tens and hundreds of thousands of people. The concept-based presentation of text contains information that cannot be derived solely from the input text, and therefore replaces the conventional terms of word representation. We believe that the most important aspects of the proposed approach are the ability to address synonymy and polysemy, which are arguably the two most important problems in NLP. Thus, the two texts can discuss the same topic with other words, and the conventional pocket of words approach will not be possible to identify this commonality."}, {"heading": "Acknowledgments", "text": "We thank Michael D. Lee and Brandon Pincombe for providing their document similarity data. We also thank Deepak Agarwal for advice on assessing the statistical significance of results in calculating semantic relationships. This work was supported in part by funds from the European Commission-sponsored MUSCLE Network of Excellence. The current address of the first author is Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054, USA.25. A longer list of examples can be found at http: / / en.wikipedia.org / wiki / Category: Online encyclopedias."}, {"heading": "Appendix A. The effect of knowledge breadth in text categorization", "text": "In this appendix, we examine the effects of feature generation using a more recent Wikipedia snapshot, as defined in Section 3.2.2. As we can see from Table 7, using the greater knowledge leads, on average, to greater improvements in the performance of text categorization. Although the performance difference between the two versions is admittedly small, it is consistent across all datasets (a similar situation occurs when assessing the role of external knowledge in calculating semantic relationships, see Section 3.3.3)."}, {"heading": "Appendix B. Test Collections for Text Categorization", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "Appendix C. Additional Examples of Feature Generation for Text", "text": "In this appendix, we list a number of additional generational examples of characteristics. \u2022 Text: \"The development of T-cell leukemia following the otherwise successful treatment of three patients with X-linked severe combined immunodeficiency (X-SCID) in gene therapy studies with hematopoietic stem cells in Iraq has led to a re-evaluation of this approach. By using a mouse model of X-SCID gene therapy, we can determine that the corrective therapeutic gene IL2RG itself can contribute to the genesis of T-cell lymphoma, affecting a third of affected animals. Gene therapy studies for X-SCID, which are based on the assumption that IL2RG is minimally oncogenic, may therefore pose some risk to patients.\" Top 10 generated characteristics: (1) Leukemia; (2) Severe combined immunodeficiency; (3) Cancer; (5) AIDS; (6) Chapter. \""}, {"heading": "Appendix D. Comparing Knowledge Sources for Feature Generation:", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves just because they are not able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...) Most people who are able to survive themselves are not able to survive themselves. (...). (...) (...) (...) (...) () (...) () (...) () (...) () (...) () (...) () () ()) (...) () () ()) (...) () () ()) () ()) ()) (...) () ()) ()) () ()) () ()) () ()) () ()) ()) () () ()) () () ()) () () ()) () () ()) () () ()) () () ()) () ()) () () ()) ()) () () ()) ()) () ()) () ()) ()) () () ()) ()) ()) () ())) () ())) () ()) ()) () ()) () () ())) () ()) () ()) () ()) ()) () ()) () ()) () () ())) () () ()) () () () ()) () () ()) () () () ()) () () () ()) () () () () () ()) () () ()) () ()) () () ()) () () ()) () () ()) () () ()) () (())) () () () () () () () ()) () () () () ())) () () ("}], "references": [], "referenceMentions": [], "year": 2009, "abstractText": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.", "creator": " TeX output 2009.03.21:1246"}}}