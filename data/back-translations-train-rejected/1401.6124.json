{"id": "1401.6124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Iterative Universal Hash Function Generator for Minhashing", "abstract": "Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the $h_{a,b}$ function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of $h_{a,b}$ functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to $1.38$ compared to the traditional approach.", "histories": [["v1", "Thu, 23 Jan 2014 19:03:38 GMT  (18kb)", "http://arxiv.org/abs/1401.6124v1", "6 pages, 4 tables, 1 algorithm"]], "COMMENTS": "6 pages, 4 tables, 1 algorithm", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["fabricio olivetti de franca"], "accepted": false, "id": "1401.6124"}, "pdf": {"name": "1401.6124.pdf", "metadata": {"source": "META", "title": "Iterative Universal Hash Function Generator for Minhashing", "authors": [], "emails": ["olivetti@ieee.org"], "sections": [{"heading": null, "text": "ar Xiv: 140 1.61 24v1 [cs.LG] 2 3Ja nMinhashing is a technique for estimating the Jaccard index between two groups by taking advantage of the probability of collision in a random permutation. To speed up the calculation, a random permutation can be roughly calculated by using a universal hash function such as the ha, b function proposed by Carter and Wegman. A better estimate of the Jaccard index can be achieved by using many of these randomly generated hash functions. In this paper, a new iterative method for generating a series of ha, b functions is developed that eliminates the need for a list of random values and avoids the multiplication operation during the calculation. The properties of the generated hash functions remain those of a universal hash function family. This is possible due to the random nature of the characteristics that occur on sparse data sets. Results show that the key comparison is achieved by maintaining the uniformity of 38 functions during the hash to 1.0:"}, {"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Min-wise Independent Permutations Locality Sensitive Hashing", "text": "If the characteristics of the data set examined can be described as sentences, as is the case on many semi-structure data (i.e., text documents), the similarity between two objects can be calculated by the Jaccard index or Jaccard similarity. (1) where O1 and 2 the two objects are compared and there is a value between 0 and 1, the latter meaning that the two objects are the same. Let us illustrate this concept with an example of comparing two text documents: d1 = \"The cat ate the mouse\" d2 = \"The mouse ate the cheese\" One data strcuture used to represent these documents is an order set of terms included in a document, or the bag-of-words representations: d1 = {ate, cat, mouse, the} d2 = {ate, cheese, mouse, the} The Jaccard similarity between these two sets will be: J (d1, d2) = d1, d2, d2, d2."}, {"heading": "3. Iterative Procedure", "text": "In other words, if we have M characteristics, in M different permutations, each characteristic is expected to have the minimum hash value once.In many real world applications the set of characteristics that describe each object is sparse (small percentage of complete functionality) and represents a randomness that follows a probability distribution.The randomness of the feature occurrence can be used to avoid the random value calculation. First, let's define the i-th hash function, a sequence of N functions such as: hi = (a + i) x \u00b7 b mod P."}, {"heading": "4. Experimental Results", "text": "This section will be divided into two subsections: the first will test the uniformity of the distribution of the hated values on m buckets and the probability that an x value will be selected as Minahsh is uniform; the second will show that the estimation error of the Jaccard index is similar between the two approaches, with a slight advantage for the iterative process and finally the acceleration achieved."}, {"heading": "4.1. Uniformity of Distribution", "text": "The uniformity of a universal hash function means that in a hash function with m buckets, the probability that a value x is assigned to a bucket with a random hash function h is proportional to 1 / m. To verify whether the family of iterative hash functions has this property, 100 independent experiments with a different value x were randomly assigned in the range [0, P [, where P = 7, 757 was the selected prime number to be hashed in one of 100 different buckets. To evaluate the uniformity, a test square was performed for each experiment with \u03b1 = 0.05 with 1000 hash functions. The choice for this number of hash functions is based on the recommendation that the expected number for each bucket should be 5 to 10 so that the test can be performed in a square to return a precise response."}, {"heading": "4.2. Hash Function Performance", "text": "Next, the processing time for both approaches was compared to quantify the acceleration achieved with the iterative hash function. To this end, 100 independent experiments were conducted to generate and apply 100,000 hash functions for a subset of 500 documents from the 20 newsgroups records [14] and 10,000 hash functions for a subset of 3, 891 documents from the classic dataset [15] This code was written in C + + with the Boost library and compiled with g + + 4.7.3, using only the option flag -O2 on an Intel i5 2.5 GHz with a single core. The operating system used was the Mint 15 Linux distribution and the source code for all reported experiments can be found at http: / / github.com / folivetti / HBLCoClust /. Table 3 reports the average and standard deviation from the time consumed by each experiment."}, {"heading": "5. Conclusion", "text": "In this paper, an optimization of the universal hash generator, which is commonly used to estimate the Jaccard coefficient between two groups, was proposed, eliminating the need to create two large lists of random values and the repetition of a multiplication operation throughout the calculation by switching the function creation to an iterative method, in which a given hash function hi (x) depends on the previously generated function plus an incrementation. First, it was tested whether this iterative method retains the universality properties of the random hash functions. Next, the speed achieved with this method was measured and the average gain was between 1.25 and 1.38 from the time measured with the random approach matched the uniformity of the bucket assignment and the minhashing distribution as such, it can be used as a universal hash function. Furthermore, the average gain achieved with this method was measured and the average gain was between 1.25 and 1.38 from the time measured with the random approach matched with the Jaccashing function. Furthermore, it was easily demonstrated with another large advantage, in terms of an estimation error, that the Jaccard remained the same with regard to a large experiment."}], "references": [{"title": "On bias", "author": ["J.H. Friedman"], "venue": "variance, 0/1loss, and the curse-of-dimensionality, Data mining and knowledge discovery 1 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature extraction: foundations and applications", "author": ["I. Guyon"], "venue": "Vol. 207, Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Tutorial, Dimensionality reduction the probabilistic way, ICML2008 Tutorial", "author": ["I.N.D. Lawrence"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Feature subset selection by means of a bayesian artificial immune system", "author": ["P.A. Castro", "F.J. Von Zuben"], "venue": "in: Hybrid Intelligent Systems, 2008. HIS\u201908. Eighth International Conference on, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Metaheuristics for feature selection: application to sepsis outcome prediction", "author": ["S.M. Vieira", "L.F. Mendon\u00e7a", "G.J. Farinha", "J.M. Sousa"], "venue": "in: Evolutionary Computation (CEC), 2012 IEEE Congress on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "R", "author": ["S. Har-Peled", "P. Indyk"], "venue": "Motwani, Approximate nearest neighbor: Towards removing the curse of dimensionality., Theory OF Computing 8 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "in: Compression and Complexity of Sequences 1997. Proceedings", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Universal classes of hash functions", "author": ["J. Carter", "M.N. Wegman"], "venue": "Journal of Computer and System Sciences 18 (2) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Locality sensitive hashing for similarity search using mapreduce on large scale data", "author": ["R. Szmit"], "venue": "in: Language Processing and Intelligent Information Systems, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable overlapping co-clustering of word-document data", "author": ["F.O.D. Franca"], "venue": "in: Machine Learning and Applications (ICMLA), 2012 11th International Conference on, Vol. 1, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "in: Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "The need for such calculation may be demanding for large data sets with many features leading to the so called curse of dimensionality [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Some methods used to reduce the dimensionality of a data set are Feature Selection [2], Feature Extraction [3] and Probabilistic Dimension Reduction [4].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "Since this is a combinatorial problem, it is mostly solved with meta-heuristic approaches [5, 6].", "startOffset": 90, "endOffset": 96}, {"referenceID": 5, "context": "Since this is a combinatorial problem, it is mostly solved with meta-heuristic approaches [5, 6].", "startOffset": 90, "endOffset": 96}, {"referenceID": 6, "context": "One of these techniques is called Locality-Sensitive Hashing [8, 9] and was successfuly applied to different applications including text and image retrieval.", "startOffset": 61, "endOffset": 67}, {"referenceID": 7, "context": "One of such technique is called Minhashing [10], and it was specifically crafted to approximate the Jaccard Index between two sets.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "One of such approaches, as devised by Carter and Wegman [11], requires that two random values are drawn from an uniform distribution for each function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "The need for an optimization is justified since this procedure is often used on very large datasets [12] and with algorithms that requires a very large number of those hash functions [13] where a small speed up might imply on an economy of hours of processing.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "The need for an optimization is justified since this procedure is often used on very large datasets [12] and with algorithms that requires a very large number of those hash functions [13] where a small speed up might imply on an economy of hours of processing.", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "The technique known as Min-wise independent permutations locality sensitive hashing (MinHash) [8], allows to quickly estimate the similarity of two sets approximating the Jaccard Index.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "One way to avoid this additional cost is by using an universal hash function such as one of those proposed in [11]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "For this end it was performed 100 independent experiments to generate and apply 100, 000 hash functions for a subset of 500 documents from the 20 Newsgroups dataset [14] and 10, 000 hash functions for a subset of 3, 891 documents from the Classic dataset [15].", "startOffset": 165, "endOffset": 169}], "year": 2014, "abstractText": "Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the ha,b function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of ha,b functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to 1.38 compared to the traditional approach.", "creator": "LaTeX with hyperref package"}}}