{"id": "1401.6131", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Controlling Complexity in Part-of-Speech Induction", "abstract": "We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.", "histories": [["v1", "Thu, 16 Jan 2014 05:20:08 GMT  (499kb)", "http://arxiv.org/abs/1401.6131v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jo\\~ao v gra\\c{c}a", "kuzman ganchev", "luisa coheur", "fernando pereira", "ben taskar"], "accepted": false, "id": "1401.6131"}, "pdf": {"name": "1401.6131.pdf", "metadata": {"source": "CRF", "title": "Controlling Complexity in Part-of-Speech Induction", "authors": ["Jo\u00e3o V. Gra\u00e7a", "Kuzman Ganchev", "Lu\u00edsa Coheur", "Fernando Pereira", "Ben Taskar"], "emails": ["JOAO.GRACA@L2F.INESC-ID.PT", "KUZMAN@GOOGLE.COM", "LUISA.COHEUR@L2F.INESC-ID.PT", "PEREIRA@GOOGLE.COM", "TASKAR@CIS.UPENN.EDU"], "sections": [{"heading": null, "text": "The standard model with maximum probability that Markov hides for this task performs poorly due to its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning target to control its capacity using parametric and non-parametric constraints. Our approach forces the rarity of word-category associations, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also offer an open source implementation of the algorithm. Our experiments in five different languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared to previous methods for the same task."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules that they have given themselves. (...) In fact, it is the case that they are able to break the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules. \"(...)"}, {"heading": "2. Models", "text": "The model for all our experiments is based on a first order HMM. We refer to the sequence of words in a sentence as bold face x and the sequence of hidden states as bold face y. for a sentence with the length l, we therefore have l hidden state variables yi. {1,.., J}, 1 \u2264 i \u2264 l, where J is the number of possible POS tags, and l observation variables xi. [1], 1 \u2264 i \u2264 l, where V is the number of word types. To simplify the notation, we assume that each tag sequence is preceded by the conventional start day y0 = start, which allows us to write as p (y1 | y0) the initial state probability of the HMM. The probability of a sentence x together with a certain hidden state sequence y is given by: p (x, \u2212 y) = 1 pt (yi \u2212 pt) po (xi | yi), () that we observe the state ypo (i) is (that po)."}, {"heading": "2.1 Multinomial Emission Model", "text": "Standard HMMs use multinomial emission and transition probabilities. That is, for a general word xi and a day yi, the observation probability po (xi | yi) and the transition probability pt (yi | yi \u2212 1) are multinomial distributions. In the experiments, we simply refer to this model as HMM. This model has a very large number of parameters due to the large number of word types (see Table 1). A common convention we follow is to lower case words that occur only once in the corpus and to map words to a special symbol \"unk.\""}, {"heading": "2.2 Maximum Entropy Emission Model", "text": "In this paper, we use a simple modification of the HMM model discussed in the previous section: In addition, we present conditional probability distributions as maximum entropy (loglinear) models. Specifically, the emission probability is expressed as follows: po (x | y) = exp (\u03b8 \u00b7 f (x, y)) \u2211 x \"exp (\u03b8 \u00b7 f (x, y))) (2), where f (x, y) is a feature function, x covers all word types, and \u03b8 are the model parameters. We refer to this model as HMM + ME. In addition to word identity, the features include orthographic and morphologically inspired terms such as the presence of uppercase letters, digits, and common sufferers. The feature groups are described in Section 5. The idea of replacing the multinomial models of an HMM with maximum entropy models is 2003, is not a feature that has been reapplied before, and is also very helpful for certain areas (e.g. identity)."}, {"heading": "3. Learning", "text": "In Section 5, we describe experiments comparing the HMM model with the ME model under three learning scenarios: maximum probability training with the EM algorithm (Dempster, Laird, & Rubin, 1977) for HMM and HMM + ME, gradient-based probability optimization for the HMM + ME model, and PR with sparsity constraints (Gra\u00e7a et al., 2009) for both HMM and HMM + ME. This section describes all three learning algorithms. In the following, we describe the entire corpus, a list of sentences according to X = (x1, x2,., xN) and the corresponding tag sequences according to Y = (y1, y2,., yN)."}, {"heading": "3.1 Maximum Likelihood with EM", "text": "\"Standard HMM training looks for model parameters that maximize the log probability of observed Y values (Log likelihood: L (\u03b8) = Log likelihood: L (Log likelihood: L = 1 Log likelihood: L = 1 Log likelihood: N = 1 Log likelihood: N = 1 Log likelihood: N = 1 Log likelihood: N = 1 Log likelihood: N = 1 Log likelihood: L = 1 Log likelihood: N = 1 Log likelihood: L (X, Y), (4) but we use the corpus notation for consistency with Section 3.3. Due to the latent variables Y, the log likelihood function for the HMM model is not convex in the model parameters, and the model is equipped with the EM algorithm."}, {"heading": "3.2 Maximum Likelihood with Direct Gradient", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.3 Controlling Tag Ambiguity with PR", "text": "One problem with unattended HMM POS markings is that the maximum probability that there will be a spread that allows for many different markings for a word in a particular context is that we do not know that we will try to control this marking because it is meant to be informative about the grammatical role of the word. In the following paragraphs, we describe a measure of ambiguity, that of Gra\u00e7a et al. (2009), that we will try to understand this marking with hard markings, so that we will start and expand the discussion to distributions such as \"stock.\" Intuitively, we would add a small subset of all possible markers to all markers of the \"stock\" (in this case noun and verb).For a hard mapping of markers to the whole corpus, Y, we could count how many different markers in Y are used for the word."}, {"heading": "4. Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5. Experiments", "text": "In this section, we present encouraging results to validate the proposed method in six different test scenarios using different metrics. Highlights include: \u2022 An emission model with maximum entropy with a Markov transition model trained with the ambiguity penalty improves over regular HMM in all cases with an average improvement of 10.4% (according to the 1 Many metric). \u2022 Compared to a wide range of current POS induction systems, our method delivers the best results for all languages except English. Furthermore, the method appears to be less sensitive to certain test conditions than previous methods. \u2022 The induced clusters are useful features in training supervised POS taggers and improve test accuracy many times or more than clusters acquired through competing methods."}, {"heading": "5.1 Corpora", "text": "In our experiments, we test several POS induction methods in five languages using manually marked corpora at the POS for these languages. Table 1 summarizes characteristics of the test corpora: The Wall Street Journal's share of Penn Treebank (Marcus et al., 1993) (we consider both the 17-day version of Smith & Eisner, 2005 (En17) and the 45-day version (En45))); the Bosnian subset of Portuguese Floresta Sinta (c) tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with only the 12 coarse tags); the Spanish corpus of Cast3LB Treebank (Civit & Mart\u00ed, 2004) (Es); and the Danish Dependency Treebank (DDT) (Kromann, Matthias T., 2003) (Dk)."}, {"heading": "5.2 Experimental Setup", "text": "We compare our work with two types of methods: those that create a single cluster for each word type PR (type-level marker), and those that allow different markers on different occurrences of a word type (token-level marker). For type-level marker, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992) 1 and Clark (2003) 2. \"We also have the recently proposed LDC system (Lamar, Maron, & Beehive, 2010) 3, with the configuration described in their papers for PTB45 and PTB17, and the PTB17 configuration for the other companies. It should be noted that we are not conducting our experiment with the SVME system."}, {"heading": "5.3 HMM+ME+Sp Performance", "text": "This year it is more than ever before."}, {"heading": "5.4 Error Analysis", "text": "Figure 2 shows the distribution of real tags and clusters for both the HMM model (left) and the HMM + ME + Sp model (right) on the En17 corpus. Each bar represents a cluster labeled by the tag assigned to it after performing the 1 Many Mapping. Colors represent the number of words with the corresponding True tag. To avoid clutter, true tags that were never used to label a cluster are grouped into \"Others.\" We observe that both models divide common tags like \"nouns\" into several hidden states. This split explains many of the errors in both models. By using 5 states for nouns instead of 7, HMM + ME + Sp is able to use more states for adjectives. Another improvement results from a better grouping of prepositions. For example, \"to\" is grouped with punctuation by the MM, while HMM + Sp is grouped correctly to prepositions."}, {"heading": "5.5 State-of-the-Art Comparison", "text": "This year it has come to the point that it will be able to put itself at the top, \"he said.\" It is very important that we are able to put ourselves at the top, \"he said."}, {"heading": "5.6 Using the Clusters", "text": "To further compare the different POS induction methods, we are experimenting with a simple semi-supervised scheme in which we use the learned clusters as features in a supervised POS tagger. Basically, the supervised model has the same features as the HMM + ME model, except that we use all word identities and suffixes regardless of frequency. We trained the supervised model based on averaged perceptions for a number of iterations, which were selected as follows: split the training set into 20% for development and 80% for training and select the number of iterations to optimize accuracy on the development set. Finally, we trained the entire training set using iterations and reported the results on a 500-set testset.We added the default features to the learned hidden state for the current token, for each unattended method (BROWN, CLARK, LDC, HM + SME)."}, {"heading": "6. Conclusion", "text": "In this paper, we investigated the task of totally uncontrolled POS induction in five different languages. We identified and proposed solutions to three main problems of the simple hidden Markov model, which was widely used for this task: i) the atomic treatment of words, the ignorance of orthographic and morphological information - which we addressed by replacing multinomial word distributions with small maximum entropy models; ii) an excessive number of parameters that allow the models to adjust irrelevant correlations - which we addressed by discarding parameters with little support in the corpus; iii) a training regime (maximum probability) that allows for very high verbal ambiguity - which we devoted ourselves by punishing the PR framework with a word ambiguity. We show that all of these solutions improve the performance of the model and that the improvements are additive. Compared to the regular MM, we achieve our most important improvement by improving our system in English, except for the fact that we achieve an impressive 10.4% improvement in each of our systems compared with the average."}, {"heading": "Acknowledgments", "text": "Jo\u00e3o V. Gra\u00e7a was supported by a grant from the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia (SFRH / BD / 27528 / 2006) and the FCT project CMU-PT / HuMach / 0039 / 2008 and by FCT (INESC-ID multiannual funding) through the PIDDAC programme. Kuzman Ganchev was partially supported by NSF ITR EIA 0205448. Ben Taskar was partially supported by the DARPA CSSG 2009 Award and the ONR 2010 Young Investigator Award. Lu\u00edsa Coheur was partially supported by FCT (INESC-ID multiannual funding) through the PIDDAC programme."}, {"heading": "Appendix A. Unsupervised Optimization", "text": "In fact, most of them will be able to move to a different world, in which they will be able to move to a different world than they are able to move to a different world."}, {"heading": "Appendix B. Evaluation Metrics", "text": "In fact, it is as if most of us are able to surpass ourselves by taking responsibility for ourselves. (...) It is not as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}], "references": [{"title": "Treebanks: Building and Using Parsed Corpora", "author": ["A. Abeill\u00e9"], "venue": null, "citeRegEx": "Abeill\u00e9,? \\Q2003\\E", "shortCiteRegEx": "Abeill\u00e9", "year": 2003}, {"title": "Floresta Sinta(c)tica: a treebank for Portuguese", "author": ["S. Afonso", "E. Bick", "R. Haber", "D. Santos"], "venue": "In Proc. LREC,", "citeRegEx": "Afonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Baum et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1970}, {"title": "Painless unsupervised learning with features", "author": ["T. Berg-Kirkpatrick", "A. Bouchard-C\u00f4t\u00e9", "J. DeNero", "D. Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Conditional and joint models for grapheme-to-phoneme conversion", "author": ["S. Chen"], "venue": "In Proc. ECSCT", "citeRegEx": "Chen,? \\Q2003\\E", "shortCiteRegEx": "Chen", "year": 2003}, {"title": "Two decades of unsupervised POS induction: How far have we come", "author": ["C. Christodoulopoulos", "S. Goldwater", "M. Steedman"], "venue": "In Proc. EMNLP,", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Building cast3lb: A spanish treebank", "author": ["M. Civit", "M. Mart\u00ed"], "venue": "Research on Language & Computation,", "citeRegEx": "Civit and Mart\u00ed,? \\Q2004\\E", "shortCiteRegEx": "Civit and Mart\u00ed", "year": 2004}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["A. Clark"], "venue": "In Proc. EACL", "citeRegEx": "Clark,? \\Q2003\\E", "shortCiteRegEx": "Clark", "year": 2003}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Toward unsupervised whole-corpus tagging", "author": ["D. Freitag"], "venue": "In Proc. COLING. Association for Computational Linguistics", "citeRegEx": "Freitag,? \\Q2004\\E", "shortCiteRegEx": "Freitag", "year": 2004}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganchev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "A comparison of Bayesian estimators for unsupervised hidden Markov model POS taggers", "author": ["J. Gao", "M. Johnson"], "venue": "In In Proc. EMNLP,", "citeRegEx": "Gao and Johnson,? \\Q2008\\E", "shortCiteRegEx": "Gao and Johnson", "year": 2008}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S. Goldwater", "T. Griffiths"], "venue": "In In Proc. ACL,", "citeRegEx": "Goldwater and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Goldwater and Griffiths", "year": 2007}, {"title": "Parameter vs. posterior sparisty in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "F. Pereira", "B. Taskar"], "venue": "In Proc. NIPS", "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2009}, {"title": "Expectation maximization and posterior constraints", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar"], "venue": null, "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2007}, {"title": "Posterior Regularization Framework: Learning Tractable Models with Intractable Constraints", "author": ["Gra\u00e7a", "J. a. d. A. V"], "venue": "Ph.D. thesis, Universidade Te\u0301cnica de Lisboa, Instituto Superior Te\u0301cnico", "citeRegEx": "Gra\u00e7a and V.,? \\Q2010\\E", "shortCiteRegEx": "Gra\u00e7a and V.", "year": 2010}, {"title": "Prototype-driven learning for sequence models", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proc. HTLNAACL. ACL", "citeRegEx": "Haghighi and Klein,? \\Q2006\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2006}, {"title": "Evaluating unsupervised part-of-speech tagging for grammar induction", "author": ["III Headden", "W. P", "D. McClosky", "E. Charniak"], "venue": "In Proc. COLING,", "citeRegEx": "Headden et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Headden et al\\.", "year": 2008}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["R. Hwa", "P. Resnik", "A. Weinberg", "C. Cabezas", "O. Kolak"], "venue": "Special Issue of the Journal of Natural Language Engineering on Parallel Texts,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["M. Johnson"], "venue": "In In Proc. EMNLP-CoNLL", "citeRegEx": "Johnson,? \\Q2007\\E", "shortCiteRegEx": "Johnson", "year": 2007}, {"title": "The Danish Dependency Treebank and the underlying linguistic theory", "author": ["Kromann", "Matthias T"], "venue": "In Second Workshop on Treebanks and Linguistic Theories (TLT),", "citeRegEx": "Kromann and T.,? \\Q2003\\E", "shortCiteRegEx": "Kromann and T.", "year": 2003}, {"title": "Latent-descriptor clustering for unsupervised POS induction", "author": ["M. Lamar", "Y. Maron", "E. Bienenstock"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "SVD and clustering for unsupervised POS tagging", "author": ["M. Lamar", "Y. Maron", "M. Johnson", "E. Bienenstock"], "venue": "In Proceedings of the ACL 2010 Conference: Short Papers,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "Simple type-level unsupervised POS tagging", "author": ["Y.K. Lee", "A. Haghighi", "R. Barzilay"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M. Marcus", "M. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Evaluation criteria for unsupervised POS induction", "author": ["Y. Maron", "M. Lamar", "E. Bienenstock"], "venue": null, "citeRegEx": "Maron et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maron et al\\.", "year": 2010}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["S. Martin", "J. Liermann", "H. Ney"], "venue": "In Speech Communication,", "citeRegEx": "Martin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Martin et al\\.", "year": 1998}, {"title": "Comparing clusterings\u2014an information based distance", "author": ["M. Meil\u0103"], "venue": "J. Multivar. Anal.,", "citeRegEx": "Meil\u0103,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103", "year": 2007}, {"title": "Tagging English text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational linguistics,", "citeRegEx": "Merialdo,? \\Q1994\\E", "shortCiteRegEx": "Merialdo", "year": 1994}, {"title": "Crouching Dirichlet, hidden Markov model: Unsupervised POS tagging with context local tag generation", "author": ["T. Moon", "K. Erk", "J. Baldridge"], "venue": "In Proc. EMNLP,", "citeRegEx": "Moon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2010}, {"title": "A new view of the EM algorithm that justifies incremental, sparse and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "Neal and Hinton,? \\Q1998\\E", "shortCiteRegEx": "Neal and Hinton", "year": 1998}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["A. Ratnaparkhi"], "venue": "In Proc. EMNLP. ACL", "citeRegEx": "Ratnaparkhi,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi", "year": 1996}, {"title": "Minimized models for unsupervised part-of-speech tagging", "author": ["S. Ravi", "K. Knight"], "venue": null, "citeRegEx": "Ravi and Knight,? \\Q2009\\E", "shortCiteRegEx": "Ravi and Knight", "year": 2009}, {"title": "The NVI clustering evaluation measure", "author": ["R. Reichart", "A. Rappoport"], "venue": "In Proc. CONLL", "citeRegEx": "Reichart and Rappoport,? \\Q2009\\E", "shortCiteRegEx": "Reichart and Rappoport", "year": 2009}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Rosenberg and Hirschberg,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg and Hirschberg", "year": 2007}, {"title": "Optimization with EM and expectationconjugate-gradient", "author": ["R. Salakhutdinov", "S. Roweis", "Z. Ghahramani"], "venue": "In Proc. ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Distributional part-of-speech tagging", "author": ["H. Sch\u00fctze"], "venue": "In Proc. EACL,", "citeRegEx": "Sch\u00fctze,? \\Q1995\\E", "shortCiteRegEx": "Sch\u00fctze", "year": 1995}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "In Proc. ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Building a Linguistically Interpreted Corpus of Bulgarian: the BulTreeBank", "author": ["K. Simov", "P. Osenova", "M. Slavcheva", "S. Kolkovska", "E. Balabanova", "D. Doikoff", "K. Ivanova", "A. Simov", "E. Simov", "M. Kouylekov"], "venue": "In Proc. LREC", "citeRegEx": "Simov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Simov et al\\.", "year": 2002}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["N. Smith", "J. Eisner"], "venue": "In Proc. ACL. ACL", "citeRegEx": "Smith and Eisner,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Unsupervised multilingual learning for POS tagging", "author": ["B. Snyder", "T. Naseem", "J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-ofspeech tagging", "author": ["K. Toutanova", "M. Johnson"], "venue": "In Proc. NIPS,", "citeRegEx": "Toutanova and Johnson,? \\Q2007\\E", "shortCiteRegEx": "Toutanova and Johnson", "year": 2007}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "A simple unsupervised learner for POS disambiguation rules given only a minimal lexicon", "author": ["Q. Zhao", "M. Marcus"], "venue": "In Proc. EMNLP", "citeRegEx": "Zhao and Marcus,? \\Q2009\\E", "shortCiteRegEx": "Zhao and Marcus", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This lack of supervised data will likely persist in the near future because of the investment required for accurate linguistic annotation: it took two years to annotate 4,000 sentences with syntactic parse trees for the Chinese Treebank (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005) and four to seven years to annotate 50,000 sentences across a range of languages (Abeill\u00e9, 2003).", "startOffset": 366, "endOffset": 381}, {"referenceID": 32, "context": "Supervised learning of taggers from POS-annotated training text is a well-studied task, with several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova, Klein, Manning, & Singer, 2003; Shen, Satta, & Joshi, 2007).", "startOffset": 147, "endOffset": 237}, {"referenceID": 29, "context": "For the first one, in addition to raw text, we are given a dictionary containing the possible tags for each word and the goal is to disambiguate the tags of a particular word occurrence (Merialdo, 1994).", "startOffset": 186, "endOffset": 202}, {"referenceID": 37, "context": "Recent work on this task typically relies on distributional or morphological features, since words with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Sch\u00fctze, 1995; Clark, 2003).", "startOffset": 198, "endOffset": 271}, {"referenceID": 8, "context": "Recent work on this task typically relies on distributional or morphological features, since words with the same grammatical function tend to occur in similar contexts and to have common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Sch\u00fctze, 1995; Clark, 2003).", "startOffset": 198, "endOffset": 271}, {"referenceID": 37, "context": "Some approaches assume (for computational and statistical simplicity) that each word can only have one tag, aggregating all local contexts through distributional clustering (Sch\u00fctze, 1995).", "startOffset": 173, "endOffset": 188}, {"referenceID": 4, "context": "Most approaches that do not make the one-tag-per-word assumption take the form of a hidden Markov model (HMM) where the hidden states represent word classes and the observations are word sequences (Brown et al., 1992; Johnson, 2007).", "startOffset": 197, "endOffset": 232}, {"referenceID": 20, "context": "Most approaches that do not make the one-tag-per-word assumption take the form of a hidden Markov model (HMM) where the hidden states represent word classes and the observations are word sequences (Brown et al., 1992; Johnson, 2007).", "startOffset": 197, "endOffset": 232}, {"referenceID": 8, "context": "That information is critical to generalization in many languages (Clark, 2003).", "startOffset": 65, "endOffset": 78}, {"referenceID": 20, "context": "As a result, when maximizing the marginal likelihood, common words typically tend to be associated with every tag with some non-trivial probability (Johnson, 2007).", "startOffset": 148, "endOffset": 163}, {"referenceID": 14, "context": "To address this problem we use the posterior regularization (PR) framework (Gra\u00e7a, Ganchev, & Taskar, 2007; Ganchev, Gra\u00e7a, Gillenwater, & Taskar, 2010) to constrain the ambiguity of word-tag associations via a sparsity-inducing penalty on the model posteriors (Gra\u00e7a et al., 2009).", "startOffset": 261, "endOffset": 281}, {"referenceID": 5, "context": "The idea of replacing the multinomial models of an HMM by maximum entropy models is not new and has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al.", "startOffset": 141, "endOffset": 153}, {"referenceID": 3, "context": "The idea of replacing the multinomial models of an HMM by maximum entropy models is not new and has been applied before in different domains (Chen, 2003), as well as in POS induction (Berg-Kirkpatrick et al., 2010; Gra\u00e7a, 2010).", "startOffset": 183, "endOffset": 227}, {"referenceID": 14, "context": "In Section 5 we describe experiments comparing the HMM model to the ME model under three learning scenarios: maximum likelihood training using the EM algorithm (Dempster, Laird, & Rubin, 1977) for both HMM and HMM+ME, gradient-based likelihood optimization for the HMM+ME model, and PR with sparsity constraints (Gra\u00e7a et al., 2009) for both HMM and HMM+ME.", "startOffset": 312, "endOffset": 332}, {"referenceID": 3, "context": "While likelihood is traditionally optimized with EM, Berg-Kirkpatrick et al. (2010) find that for the HMM with the maximum entropy emission model, higher likelihood and better accuracy can be achieved by with a gradient-based likelihood-optimization method.", "startOffset": 53, "endOffset": 84}, {"referenceID": 14, "context": "In the following paragraphs we describe a measure of tag ambiguity proposed by Gra\u00e7a et al. (2009) that we will attempt to control.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "In the following paragraphs we describe a measure of tag ambiguity proposed by Gra\u00e7a et al. (2009) that we will attempt to control. It is easier to understand this measure with hard tag assignments, so we start with that and thene extend the discussion to distributions over tags. Consider a word such as \u201cstock\u201d. Intuitively, we would like all occurrences of \u201cstock\u201d to be tagged with a small subset of all possible tags (noun and verb, in this case). For a hard assignment of tags to the entire corpus, Y, we could count how many different tags are used in Y for occurrences of the word \u201cstock.\u201d If instead of a single tagging of the corpus, we have a distribution q(Y) over assignments, we need to generalize this ambiguity measure. Instead of asking was a particular tag ever used for the word \u201cstock\u201d, we would ask what is the maximum probability with which a particular tag was used for the word \u201cstock\u201d. Then instead of counting the number of tags, we would sum these probabilities. As motivation, Figure 1 shows the distribution of tag ambiguity across words for two corpora. As we see from Figure 1, when we train using the EM procedure described in Section 3.1, the HMM and ME models grossly overestimates the tag ambiguity of almost all words. However when the same models are trained using PR to penalize the tag ambiguity, both models (HMM+Sp, HMM+ME+Sp) achieve a tag ambiguity closer to the truth. More formally, Gra\u00e7a et al. (2009) define this measure in terms of constraint features \u03c6(X,Y).", "startOffset": 79, "endOffset": 1448}, {"referenceID": 4, "context": "Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have been used to cluster words according to how their immediately contiguous words are distributed.", "startOffset": 72, "endOffset": 92}, {"referenceID": 4, "context": "Several influential methods, most notably mutual-information clustering (Brown et al., 1992), have been used to cluster words according to how their immediately contiguous words are distributed. Although those methods were not explicitly designed for POS induction, the resulting clusters capture some syntactic information (see also Martin, Liermann, & Ney, 1998, for a different method with a similar objective). Clark (2003) refined the distributional clustering approach by adding morphological and word frequency information, to obtain clusters that more closely resemble POS tags.", "startOffset": 73, "endOffset": 428}, {"referenceID": 20, "context": "Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007).", "startOffset": 157, "endOffset": 222}, {"referenceID": 32, "context": "To alleviate these problems, Sch\u00fctze (1995) used frequency cutoffs, singular-value decomposition of co-occurrence matrices, and approximate co-clustering through two stages of SVD, with the clusters from the first stage used instead of individual words to provide vector representations for the second-stage clustering.", "startOffset": 29, "endOffset": 44}, {"referenceID": 16, "context": "Lamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Sch\u00fctze (1995) and achieve close to state-of-the-art performance.", "startOffset": 17, "endOffset": 32}, {"referenceID": 16, "context": "Lamar, Maron and Johnson (2010) have recently revised the two-stage SVD model of Sch\u00fctze (1995) and achieve close to state-of-the-art performance.", "startOffset": 17, "endOffset": 96}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution.", "startOffset": 21, "endOffset": 34}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution.", "startOffset": 21, "endOffset": 53}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering.", "startOffset": 21, "endOffset": 370}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words.", "startOffset": 21, "endOffset": 980}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words. Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type.", "startOffset": 21, "endOffset": 1106}, {"referenceID": 8, "context": "However, as noted by Clark (2003) and Johnson (2007), using maximum likelihood training for such models does not achieve good results: maximum likelihood training tends to result in very ambiguous distributions for common words, in contradiction with the rather sparse wordtag distribution. Several approaches have been proposed to mitigate this problem. Freitag (2004) clusters the most frequent words using a distributional approach and co-clustering. To cluster the remaining (infrequent) words, the author trains a second-order HMM where the emission probabilities for the frequent words are fixed to the clusters found earlier and emission probabilities for the remaining words are uniform. Several studies propose using Bayesian inference with an improper Dirichlet prior to favor sparse model parameters and hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). This was further refined by Moon, Erk, and Baldridge (2010) by representing explicitly the different ambiguity patterns of function and content words. Lee, Haghighi, and Barzilay (2010) take a more direct approach to reducing tag ambiguity by explicitly modeling the set of possible tags for each word type. Their model first generates a tag dictionary that assigns mass to only one tag for each word type to reflect lexicon sparsity. This dictionary is then used to constrain a Dirichlet prior from which the emission probabilities are drawn by only having support for word-tag pairs in the dictionary. Then a token-level HMM using those emission parameters and transition parameters draw from a symmetric Dirichlet prior are used for tagging the entire corpus. The authors also show improvements by using morphological features when creating the dictionary. Their system achieves state-of-art results for several languages. It should be noted that a common issue with the above sparsity-inducing approaches is that sparsity is imposed at the parameter level, the probability of word given tag, while the desired sparsity is at the posterior level, the probability of tag given word. Gra\u00e7a et al. (2009) use the PR framework to penalize ambiguous posteriors distributions of words given tokens, which achieves better results than the Bayesian sparsifying Dirichlet priors.", "startOffset": 21, "endOffset": 2125}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions.", "startOffset": 15, "endOffset": 46}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions.", "startOffset": 15, "endOffset": 63}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English.", "startOffset": 15, "endOffset": 299}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007).", "startOffset": 15, "endOffset": 733}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly simplifies the problem \u2013 most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity \u2013 the accuracy of such methods is still significantly behind supervised methods. To address the remaining ambiguity by imposing additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in the HMM via a integer program.", "startOffset": 15, "endOffset": 1468}, {"referenceID": 3, "context": "Most recently, Berg-Kirkpatrick et al. (2010) and Gra\u00e7a (2010) proposed replacing the multinomial distributions of the HMM by maximum entropy (ME) distributions. This allows the use of features to capture morphological information, and achieve very promising results. Berg-Kirkpatrick et al. (2010) also find that optimizing the likelihood with L-BFGS rather than EM leads to substantial improvements, which we show not to be the case beyond English. We also note briefly POS induction methods that rely on a prior tag dictionary indicating for each word type what POS tags it can have. The POS induction task is then, for each word token in the corpus, to disambiguate between the possible POS tags, as described by Merialdo (1994). Unfortunately, the availability of a large manually-constructed tag dictionary is unrealistic and much of the later work tries to reduce the required dictionary size in different ways, by generalizing from a small dictionary with only a handful of entries (Smith & Eisner, 2005; Haghighi & Klein, 2006; Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although this approach greatly simplifies the problem \u2013 most words can only have one tag and, furthermore, the cluster-tag mappings are predetermined, thus removing an extra level of ambiguity \u2013 the accuracy of such methods is still significantly behind supervised methods. To address the remaining ambiguity by imposing additional sparsity, Ravi and Knight (2009) minimize the number of possible tag-tag transitions in the HMM via a integer program. Finally, Snyder, Naseem, Eisenstein, and Barzilay (2008) jointly train a POS induction system over parallel corpora in several languages, exploiting the fact that different languages present different ambiguities.", "startOffset": 15, "endOffset": 1611}, {"referenceID": 25, "context": "Table 1 summarizes characteristics of the test corpora: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) (we consider both the 17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 39, "context": ", 1993) (we consider both the 17-tag version of Smith & Eisner, 2005 (En17) and the 45-tag version (En45)); the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt); the Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with only the 12 coarse tags); the Spanish corpus from the Cast3LB treebank (Civit & Mart\u00ed, 2004) (Es); and the Danish Dependency Treebank (DDT) (Kromann, Matthias T.", "startOffset": 245, "endOffset": 265}, {"referenceID": 24, "context": "We report results from the type-level HMM (TLHMM) (Lee et al., 2010) when applicable, since we were not able to run that system.", "startOffset": 50, "endOffset": 68}, {"referenceID": 20, "context": "In addition, we also compared to a multinomial HMM with a sparsifying Dirichlet prior on the parameters (HMM+VB) trained using variational Bayes (Johnson, 2007).", "startOffset": 145, "endOffset": 160}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2.", "startOffset": 88, "endOffset": 108}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2.", "startOffset": 88, "endOffset": 126}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2. Following Headden, McClosky, and Charniak (2008), we trained the CLARK system with both 5 and 10 hidden states for the letter HMM and ran it for 10 iterations; the BROWN system was run according with the instructions accompanying the code.", "startOffset": 88, "endOffset": 177}, {"referenceID": 4, "context": "For type-level tagging, we use two standard baselines, BROWN and CLARK, as described by Brown et al. (1992)1 and Clark (2003)2. Following Headden, McClosky, and Charniak (2008), we trained the CLARK system with both 5 and 10 hidden states for the letter HMM and ran it for 10 iterations; the BROWN system was run according with the instructions accompanying the code. We also ran the recently proposed LDC system (Lamar, Maron, & Bienenstock, 2010)3, with the configuration described in their paper for PTB45 and PTB17, and the PTB17 configuration for the other corpora. It should be noted that we did not carry out our experiments with the SVD2 system (Lamar, Maron and Johnson, 2010), since SVD2 is superseded by LDC according to its authors. For token-level tagging, we experimented with the feature-rich HMM as presented by BergKirkpatrick et al. (2010), trained both using EM training (BK+EM) and direct gradient (BK+DG), using the configuration provided by the authors4.", "startOffset": 88, "endOffset": 858}, {"referenceID": 3, "context": "Implementation provided by Berg-Kirkpatrick et al. (2010).", "startOffset": 27, "endOffset": 58}, {"referenceID": 14, "context": "We used the results that worked best for English (En17) (Gra\u00e7a et al., 2009), regularizing only words that occur at least 10 times, with \u03c3 = 32, and use the same configuration for all the other scnenarios.", "startOffset": 56, "endOffset": 76}, {"referenceID": 28, "context": "We evaluate all systems using four common metrics for POS induction: 1-Many mapping, 1-1 mapping (Haghighi & Klein, 2006), variation of information (VI) (Meil\u0103, 2007), and validity measure (V) (Rosenberg & Hirschberg, 2007).", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "001, corresponding to the best values reported by Johnson (2007). For PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Gra\u00e7a et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 14, "context": "For PR training, we initialize with 30 EM iterations and then run for 170 iterations of PR, following Gra\u00e7a et al. (2009). We used the results that worked best for English (En17) (Gra\u00e7a et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 3, "context": "We use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010), while the reduced feature set was described by Gra\u00e7a (2010).", "startOffset": 68, "endOffset": 99}, {"referenceID": 3, "context": "We use two different feature sets: the large feature set is that of Berg-Kirkpatrick et al. (2010), while the reduced feature set was described by Gra\u00e7a (2010). We apply count-based feature selection to both the identity and suffix features.", "startOffset": 68, "endOffset": 160}, {"referenceID": 13, "context": "This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather used a value reported by Gra\u00e7a et al. (2009) to work for the multinomial emission HMM.", "startOffset": 172, "endOffset": 192}, {"referenceID": 13, "context": "This is especially surprising since we did not optimize the strength of the tag-ambiguity penalty for the maximum-entropy emission HMM, but rather used a value reported by Gra\u00e7a et al. (2009) to work for the multinomial emission HMM. Experiments reported by Gra\u00e7a (2010) show that tuning this parameter can further improve performance.", "startOffset": 172, "endOffset": 271}, {"referenceID": 3, "context": "As reported by Berg-Kirkpatrick et al. (2010), the way in which the objective is optimized can have a big impact on the overall results.", "startOffset": 15, "endOffset": 46}, {"referenceID": 44, "context": "Removing the noun-adjective distinction, as suggested by Zhao and Marcus (2009), would increase performance of both models by about 6%.", "startOffset": 57, "endOffset": 80}, {"referenceID": 14, "context": "This confirms previous results by Gra\u00e7a et al. (2009). Comparing the models in lines 5-8 to those in lines 1-3, we see", "startOffset": 34, "endOffset": 54}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al.", "startOffset": 109, "endOffset": 140}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results.", "startOffset": 109, "endOffset": 246}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora.", "startOffset": 109, "endOffset": 537}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also, because we were not able to rerun the experiments for TLHMM, we were not able to compute the information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better than the other systems. This is not surprising since they have the ability to model morphological regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with sparsifying Dirichlet priors. As noted by Gra\u00e7a et al. (2009), models trained in this way tend to have a cluster distribution that more closely resemble the true POS distribution (some clusters with lots of words and some with few words) which favors the 1-1 metric (a description of the particularity of the 1-1 metric is discussed in Appendix B).", "startOffset": 109, "endOffset": 1305}, {"referenceID": 3, "context": "We explore the training procedure in more detail in Appendix A, comparing also our implementation to that of Berg-Kirkpatrick et al. (2010). For brevity, Table 3 contains only the results from the implementation of Berg-Kirkpatrick et al. (2010). Our implementation produces comparable, but not quite identical results. Lines 11-12 of Table 3 display the two methods that attempt to control tag ambiguity and have a feature-rich representation to capture morphological information. The results for TLHMM are taken from Lee et al. (2010), so we do not report results for the En17 and Bg corpora. Also, because we were not able to rerun the experiments for TLHMM, we were not able to compute the information-theoretic metrics. Consequently, the comparison for TLHMM is slightly less complete than for the other methods. Both TLHMM and HMM+ME+Sp perform competitively or better than the other systems. This is not surprising since they have the ability to model morphological regularity while also penalizing high ambiguity. Comparing TLHMM with HMM+ME+Sp, we see that HMM+ME+Sp performs better on the 1-Many metric. In contrast, TLHMM performs better on 1-1. One possible explanation is that the underlying model in TLHMM is a Bayesian HMM with sparsifying Dirichlet priors. As noted by Gra\u00e7a et al. (2009), models trained in this way tend to have a cluster distribution that more closely resemble the true POS distribution (some clusters with lots of words and some with few words) which favors the 1-1 metric (a description of the particularity of the 1-1 metric is discussed in Appendix B). To summarize, for all non-English languages and all metrics except 1-1, the HMM+ME+Sp system performs better than all the other systems. For English, BK+DG wins for the 45-tag corpus, while LDC wins for the 17-tag corpus. The HMM+ME+Sp system is fairly robust, performing well on all corpora and best on several of them, which allow us to conclude that it is not tuned to any particular corpus or evaluation metric. The performance of HMM+ME+Sp is tightly related to the performance of the underlying HMM+ME system. In Appendix A we present a discussion about the performance of different optimization methods for HMM+ME. We compare our HMM+ME implementation to that of BK+EM and BK+DG and show that there are some significant differences in performance. However, its not clear by the results which one is better, and why it performs better in a given situation. As mentioned by Clark (2003), morphological information is particularly useful for rare words.", "startOffset": 109, "endOffset": 2484}, {"referenceID": 36, "context": "A related study (Salakhutdinov et al., 2003) compares the convergence rate of EM and direct gradient training, and identifies conditions when EM achieves Newton-like behavior, and when it achieves first-order convergence.", "startOffset": 16, "endOffset": 44}, {"referenceID": 3, "context": "Table 5: EM vs direct gradient from Berg-Kirkpatrick et al. (2010) implementation compared with our implementaion of EM of the HMM with maximum-entropy emission probabilities.", "startOffset": 36, "endOffset": 67}, {"referenceID": 14, "context": "This metric tends to favor systems that produce an exponential distribution on the size of each induced cluster independent of the clusters\u2019 true quality, and it does not correlate well with the information theoretic metrics (Gra\u00e7a et al., 2009).", "startOffset": 225, "endOffset": 245}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007).", "startOffset": 91, "endOffset": 104}, {"referenceID": 28, "context": "VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007).", "startOffset": 80, "endOffset": 93}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007). However, the range of VI values is dataset-dependent (VI lies in [0, 2 logN ] whereN is the number of POS tags) which does not allow a comparison across datasets with different N . The validity-measure (V) is also an entropy-based measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as VI. It has been reported to give a high score when a large number of clusters exist, even if these are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport, 2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010) present an extensive comparison between evaluation metrics.", "startOffset": 92, "endOffset": 1165}, {"referenceID": 28, "context": "The information-theoretic measures we use for evaluation are variation of information (VI) (Meil\u0103, 2007) and validity-measure (V) (Rosenberg & Hirschberg, 2007). Both are based on the entropy and conditional entropy of the tags and induced clusters. VI has desirable geometric properties \u2013 it is a metric and is convexly additive (Meil\u0103, 2007). However, the range of VI values is dataset-dependent (VI lies in [0, 2 logN ] whereN is the number of POS tags) which does not allow a comparison across datasets with different N . The validity-measure (V) is also an entropy-based measure and always lies in the range [0, 1], but does not satisfy the same geometric properties as VI. It has been reported to give a high score when a large number of clusters exist, even if these are of low quality (Reichart & Rappoport, 2009). Other information-theoretic measures have been proposed that better handle different numbers of clusters, for instance NVI (Reichart & Rappoport, 2009). However, in this work all testing conditions will be on the same corpora with the same number of clusters so that problem does not exist. Christodoulopoulos, Goldwater, and Steedman (2010) present an extensive comparison between evaluation metrics. In related work Maron, Lamar, and Bienenstock (2010) present another empirical study about metrics and conclude that the VI metric can produce results that contradict the true quality of the induced clustering, by giving very high scores to very simple baseline systems, for instance assigning the same label to all words.", "startOffset": 92, "endOffset": 1278}], "year": 2011, "abstractText": "We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via parametric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task.", "creator": "TeX"}}}