{"id": "1401.6421", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Riffled Independence for Efficient Inference with Partial Rankings", "abstract": "Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. Simultaneously addressing all of these challenges i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based represen- tations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data.", "histories": [["v1", "Thu, 23 Jan 2014 02:42:39 GMT  (2838kb)", "http://arxiv.org/abs/1401.6421v1", "arXiv admin note: text overlap witharXiv:1202.3734"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1202.3734", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jonathan huang", "ashish kapoor", "carlos guestrin"], "accepted": false, "id": "1401.6421"}, "pdf": {"name": "1401.6421.pdf", "metadata": {"source": "CRF", "title": "Riffled Independence for Efficient Inference with Partial Rankings", "authors": ["Jonathan Huang", "James H. Clark", "Ashish Kapoor", "Carlos Guestrin"], "emails": ["jhuang11@stanford.edu", "akapoor@microsoft.com", "guestrin@cs.cmu.edu"], "sections": [{"heading": null, "text": "However, modelling such distributions presents several arithmetical challenges due to the factorial size of the ranking over an item set. However, some of these challenges are familiar to the artificial intelligence community, such as the compact representation of a distribution over a combinatorial large space and the efficient execution of likely conclusions from these representations. However, in terms of ranking, the additional challenge of what we call human task complexity - users are rarely willing to provide a complete ranking over a long list of candidates, but often prefer to provide partial ranking information - is a difficult task, but necessary if we want to scale problems with non-trivial size. In this paper, we show that the recently proposed refined independence assumptions that allow efficient inferences to be drawn and can be learned from partial ranking data do not properly and efficiently address each of the above challenges, but partially address the partially refractory complexity of the rankings."}, {"heading": "1. Probabilistic Modeling of Ranking Data: Three Challenges", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "2. Riffled Independence For Rankings", "text": "The order of precedence of the order of precedence of the order of precedence is less than the order of precedence of the order of precedence of the order of precedence of the order of precedence of the order of precedence of the order of precedence. The order of precedence of the order of precedence of the order of precedence of the order of precedence is less than the order of precedence of the order of precedence of the order of precedence of the order of precedence. The order of precedence of the order of precedence of the order of precedence is less than the order of precedence of the order of precedence of the order of precedence. The order of precedence of the order of precedence of the order of precedence of the order of precedence is less than the order of the order of precedence of the order of precedence of the order of precedence of the order of precedence. The order of precedence of the order of the order of all possible precedence of the order of precedence of the order of the order of precedence is less than the order of the order of the precedence of the precedence of the order of the order of the precedence of the order of the order of the order of the order of precedence of the precedence of the order of the rankings is less than the order of the order of the order of the precedence of the precedence of the order of the order of the precedence of the order of the order of the rankings."}, {"heading": "2.1 Hierarchical Riffle Independent Models", "text": "It is natural that you have to go in search of a solution worthy of the name. (...) It is not that you have to get involved in a solution. (...) It is not that you have to get involved in a solution. (...) It is that you have to get involved in a solution. (...) It is that you have to get involved in a solution. (...) It is that you have to get involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution. (...) It is like getting involved in a solution."}, {"heading": "2.2 Model Estimation", "text": "However, given the hierarchical structure of a model, it is more difficult to estimate the correct structure of a model. However, the most important finding is that if two subsets A and B are independent of a hierarchical riffle model, the independence relationship between i-A and j-k-B must apply to all i-A and j-k-B. Our structure learning algorithms work by looking for these \"triplex\" independence relationships within the data. We push back interested readers to the details in (Huang & Guestrin, 2012). Note that in our previous work, we assumed that our algorithms had access to a data set consisting of i.i.d. complete rankings. In the current work, we will loosen our assumptions by allowing users to provide incomplete ranked data, but not every single part of our ranking is complete."}, {"heading": "3. Decomposable Observations", "text": "In this section we describe a decomposition for a certain class of probability functions over the space of rankings, in which the observations are \"factored\" into simpler parts. If an observation O is decomposable in this way, we show that one can efficiently divide a division before distribution on O. For simplicity in this paper, we focus primarily on subset observations, whose liquidity with some subsets of rankings in Sn.Definition 7 (Subset Observations)."}, {"heading": "4. Complete Decomposability", "text": "The condition of Proposition 10 that the previous and the observation must dissolve in relation to exactly the same hierarchy is sufficient for an efficient inference, but it may at first glance seem so restrictive that the proposition is useless in practice. In order to overcome this limitation of \"hierarchy-specific\" decomposition, we are researching a special family of observations (which we call fully decomposable) for which the property of decomposition does not specifically depend on a particular hierarchy, which means in particular that efficient conclusions are always possible (provided that an efficient representation of the previous distribution is also possible). To show how an observation can dissolve in relation to multiple hierarchies, let us again consider the first place observation O = \"Corn comes first.\" We argued in Example 8 that O is a decomposable observation."}, {"heading": "5. Complete Decomposability of Partial Ranking Observations", "text": "In this section, we discuss the mathematical problem of the complete characterization of the class of fully decomposable observations. \"Our main contribution in this section is to show that fully decomposable observations can lead precisely to partial rankings of the position.\" We begin our discussion by introducing partial rankings that allow positions to be tied to a ranking by referring to a ranking represented from the vertical bar. \"The partial ranking of the ranking 3 corresponds to the collection of partial rankings that allow a division.\" (i.e., the term partial ranking of restaurants should not be confused with two other standard objects. \"(1) The partial ranking of the ranking corresponds to the collection of rankings that occupy rank items3. As noted by Ailon (2007),\" the term partial ranking should not be confused with two other standard objects here. \""}, {"heading": "5.1 An Impossibility Result", "text": "Theorem 21 (Reversal of Theorem 19): Any observation that can be completely dissected takes the form of a partial ranking (C-P). Theorem 19 and 21 together form a significant insight into the nature of rankings and show that the concepts of partial rankings and disjointed independence are closely linked. In fact, our result shows that it is even possible to define partial rankings by complete decomposition! Theorem 21 practically shows that there is no algorithm based on simple multiplicative updates of parameters that can condition precisely on observations that do not take the form of partial rankings. The complexity of the compilation of observations that are not partial rankings remains open. We suspect that approximate conclusions are necessary in order to handle more complex observations efficiently."}, {"heading": "5.2 Proof of the Impossiblity Result (Theorem 21)", "text": "Since this evidence is much longer and less obvious than the evidence for its reversal (Theorem 19), we outline the main ideas that guide the evidence here, pointing out that the definition of the linear range of a series of vectors in a vector space is the intersection of all linear subspaces that contain this series of vectors. To prove that Theorem 21 must be the intersection of all partial rankings, we present analogous concepts of a series of ranking elements. Definition 22 (rspan and pspan) is a collection of rankings. We define pspan (X) the intersection of all partial rankings, the X. Likewise, we define rspan (X) the intersection of all fully decomposable observations, the X."}, {"heading": "5.3 Going Beyond Subset Observations", "text": "Although we have given all our results for sub-functions so far, we are now commenting on what our theory would actually look like if we had taken general probability functions into account. (To avoid confusion, we refer here to a more general class of functions that we call fully degradable functions, instead of the fully degradable sub-functions of definition 11. Definition.) A function h: Sn \u2192 R is called a fully degradable function if it is set independently of any hierarchy above the item. We refer to the collection of all possible fully degradable functions as C: as we discuss, C and C: are very much alike. It is fairly easy to restore Theorem 19 in the general case of fully degradable functions: theorem. Each sub-gradable function is a fully degradable function that is fully degradable. Unfortunately, the evidence for its conversion (Theorem 21) is not easily generalizable and can only be used to show that support (partition) is complete."}, {"heading": "5.4 Conditioning on Noisy Observations", "text": "We conclude this section with a comment on the handling of noise in observations. While in this work we have assumed that observed partial arrangements always correspond to the underlying overall ranking of a user, there are situations in which one would like to model a noisier environment in which the partial arrangements are less likely to be reproduced incorrectly. A natural model that takes into account noise, for example, could be: L (O | \u03c3) = {1 \u2212 \u03b5, if there were otherwise a weighted mixture of the previous distribution and the rear distribution. (5.2) If a prior distribution factorises H in terms of a hierarchy, then conditioning the loud probability of equation 5.2 leads to a rear distribution that can be written as a weighted mixture of the previous distribution and the rear distribution that would have resulted from the conditioning of a noise-free observation. While each component of this rear distribution factorises with respect to H, the mixture itself does not take into account the general distribution (and should not be taken into account according to our theory)."}, {"heading": "6. Model Estimation from Partially Ranked Data", "text": "In many ranking-based applications, data sets are predominantly sub-rankings, not full rankings, because sub-rankings are typically easier and quicker for humans to specify. In addition, many data sets are heterogeneous, and some contain rankings of different kinds. In this section, we use the efficient inference algorithm proposed in Section 5 to estimate a riffle-independent model of partially ranked data. Since estimating a model using partially ranked data is typically more difficult than estimating a model using only full rankings, it is common practice (e.g. Huang & Guestrin, 2010) to simply ignore the partial rankings in a data set. The ability of a method to integrate all available data can lead to significantly improved model accuracy, as can the applicability of this part of the method, which we initially call efficient."}, {"heading": "6.1 Censoring Interpretations of Partial Rankings", "text": "The model estimation problem for complete rankings is presented as follows: In view of the i.i.d. training examples \u03c3 (1),.., \u03c3 (m) (consisting of complete rankings), which are drawn from a hierarchical, riffle-independent distribution h, we examine the structure and parameters of h.In the partial ranking order, we again assume that i.i.d. draws are drawn, but that each training example \u03c3 (i) is subjected to a censorship process that produces a partial ranking consistent with \u03c3 (i). For example, censorship could only allow the classification of the top k points of \u03c3 (i). While we allow arbitrary types of partial rankings to arise from censorship, we often assume that the partial ranking type resulting from censorship \u03c3 (i) does not depend on \u03c3 (i) itself."}, {"heading": "6.2 Algorithm", "text": "As with many such problems, we use an expectation maximization (EM) approach, in which we apply inferences to calculate a trailing distribution across the entire ranking. In our case, we then apply the Huang and Guestrin algorithms (2010, 2012), which were designed to calculate the hierarchical structure of a model and its parameters from a dataset full of ranking."}, {"heading": "7. Related Work", "text": "Rankings and permutations have recently become an active field of research in the field of machine learning, in part because of the hinged role they play in seeking information and identifying preferences. Algorithms such as RankSVM (Joachims, 2002) and RankBoost (Freund, Iyer, Schapire, & Singer, 2003) have, for example, been successful in large-scale ranking problems occurring in web search. However, the main objectives of our work differ from these web scale settings - instead of seeking a single \"optimal\" ranking in terms of objective function, we strive to understand a large collection of rankings by means of density estimation. Below, we outline two major lines of research that have influenced our work."}, {"heading": "7.1 Additive and Multiplicative Decompositions", "text": "Our work builds in particular on a thread of recent work on tractable models for permutation data based on functional degradation. Kondor, Howard and Jebara (2007) and Huang, Guestrin and Guibas (2008, 2009) looked at additive degradation of a distribution into a weighted sum of Fourier base functions. This work shows that low frequency Fourier assumptions can often be effective in dealing with the representative complexity of working with distributions over permutations. In particular, they show that conditioning previous distributions to the \"low frequency\" probability functions that often occur in multi-object tracking problems can be performed particularly efficiently. Unfortunately, low frequency assumptions are not so applicable to distributions defined by rankings and to specifically address ranking problems, Huang and Guestrin (2009, 2010) introduced the concept of fractured independence as a useful generalization of probable independence for rankings."}, {"heading": "7.2 Mallows Models", "text": "Our work also fits into a larger corpus of research on the known mallows distribution via rankings, parameterized by: h (\u03c3; \u03c6, \u03c30). (7.1), in which the function d\u03c4 refers to the Kendall's tau distance metric on rankings (see Figure 6). However, a mallows distribution (Equation 7.1) can always be seen as a special case of a hierarchical ranking in which the positions are sequentially derived from the model one by one (see Figure 6). Mallows models (as well as other similar distance models) have the advantage of being able to represent distributions for very large n compactly and admit that previous distributions are efficient (Meila, Phadnis, Patterson, & Bilmes, 2007)."}, {"heading": "8. Experiments", "text": "In this section, we demonstrate our method for learning hierarchical riffle-independent models from partial orders of simulated data as well as real data sets from different domains. In all experiments, we initialize distributions so that they are uniform, and do not use random restarts."}, {"heading": "8.1 Datasets", "text": "In addition to approximately 5000 complete rankings, the APA Dataset has more than 10,000 top-k rankings of 5 candidates, rather than roughly three preferred rankings of four candidates. In previous work, we had only used the complete rankings of APA data (Huang & Guestrin, 2010, 2012), but now we are able to use the entire dataset. Figure 7 (a) shows for each k visit in 2002 over 60,000 top-k rankings of 14 candidates. As with APA data, we have only used the complete rankings of Meath data in previous work, but here we are using the entire dataset (Gormley & Murphy, 2007), taken from the Irish Parliament elections, has over 60,000 top-k rankings of 14 candidates. As with APA data, we only use the complete rankings of Meath data in previous work, but here we are using the entire dataset (Gormley & Murphy, 2007). Figure 7 (b) shows that each of the USA's top 14 rankings has 60k rankings."}, {"heading": "8.2 APA Structure Learning Results", "text": "Due to the exceptionally large number of complete rankings in the APA data, the gains made by the additional use of partially ranked data are insignificant. To better illustrate the benefits of partial rankings, we scanned a data set of 300 rankings (including complete and partial rankings) and present the results with this smaller data set. The results of structural learning using only the complete rankings of these 300 training examples (consisting of about 100 examples) show that you get the structure in Figure 8 (a) that does not actually match the \"correct\" structure of Figure 3 (a) learned using 5000 complete rankings. Figures 8 (b) and 8 (c) show the results of our EM algorithm with the former, which indicates the resulting structure after only one EM iteration, and the latter the result after structural convergence that occurs through the third iteration, showing that our method \"can only learn from 300.\""}, {"heading": "8.3 Structure Discovery with EM with Larger n.", "text": "Our experiments have led to several observations about the use of EM for learning with partial assemblies. First, we observe that typical runs quickly converge to a fixed structure, with no more than three EM iterations. Figure 10 shows the progress of EM on the Sarah Palin data, whose structure converges with the third iteration. As expected, the log probability increases with each iteration, and we note that the structure becomes more interpretable - for example, the amount of leaves {0, 2, 3} corresponds to the three posts about Palin's wardrobe prior to the election, while the posts from the set of leaves {1, 4, 6} are related to verbal faux-pas Palin made during the campaign. Note that this structure is discovered exclusively from data about visitation orders, and that in our experiments no text information was used. Second, the number of EM iterations needed to achieve convergence in log iterations is calculated with almost all of the total number of algorithms we have based on the larger number of parts of time."}, {"heading": "8.4 The Value of Partial Rankings", "text": "We first learned models based on synthetic data from a hierarchy, using 343 complete rankings plus varying numbers of sub-rankings (between 0 and 64,000). We repeat each setting with 20 bootstrap attempts and calculate the protocol probability of a test set of 5000 examples for evaluation. For speed, we learn a structure H only once and fix H to learn parameters for each study. Figure 11 (b), which calculates the test log probability depending on the number of sub-rankings provided to the training set, shows that we are actually able to learn more accurate distributions as more and more data is made available in the form of sub-rankings."}, {"heading": "8.5 Comparing to a Nonparametric Model", "text": "Comparing the performance of corrugated independent models with other approaches was not possible in previous work because we were unable to process partial rankings. However, using the methods developed in our current work, we compare corrugated independent models with the modern non-parametric estimator of Lebanon and Mao (2008) (to which we hereby refer as an LM08 estimator) on the same data (by setting their regularization parameter to C = 1.2.5 or 10 by means of a validation set). Figure 11 (b) shows (of course) that when the data is synthetically extracted from a corrugated independent model, our EM method significantly exceeds the LM08 estimator. We point out that in theory of LM08 it is guaranteed to perform (under appropriate conditions) if sufficient training examples are given. For the meath data, which are only approximately corrugated, we have trained on subsets of the size 5,000 and 25,000 (tests remaining on data)."}, {"heading": "9. Future Directions", "text": "There are several possible extensions to the current work. Below we list some of these open questions and extensions."}, {"heading": "9.1 Inference with Incomplete Rankings", "text": "We have shown in this paper that a fractured structure of independence can be exploited to condition an observation if and only if it takes the form of a portion of the ranking. Although the space of the part of the ranking is both rich and useful in many constellations, it does not cover an important class of observations: that of incomplete rankings, which are defined as ranking (or partial ranking) of a subset of the item. Theorem 21, for example, shows that the conditioning problem for paired observations of the form \"Apples are preferred over Bananas\" is not degradable. Note that top-k rankings are considered to be \"complete\" rankings, since they implicitly rank all other items in the last n-k positions. How can we then condition traceable to incomplete rankings? One possible approach is the conversion into a Fourier postal representation using the method of conditional pairing of Huestin 2008, which would most likely be a different structure (Huestin 2012, Huestrin & Huestin 2012)."}, {"heading": "9.2 Reexamining Data Independence Assumptions", "text": "In this paper, we have consistently assumed that training examples are independent and identically distributed. In practice, however, these are not always safe assumptions, as a number of factors can affect the validity of both factors. For example, in an Internet survey in which a user is required to perform a series of tasks to rank preferences one after the other, there is concern that the user's previous ranking tasks may distort the results of his future leaders.Another source of bias is the reference ranking that may be presented, in which the user is asked to rearrange positions by \"dragging and dropping.\" On the one hand, displaying all of the same ranking tasks may distort the resulting data. On the other hand, displaying a different reference ranking for each user may mean that the training examples are not exactly identically distributed. In practice, however, another form of bias lies in the partial ranking types that are shown in the data. To formulate our EM algorithm, we have assumed that the typical user's preferences only partially affect, for example, whether or not the user's preferences exist in practice."}, {"heading": "9.3 Probabilistic Modeling of Strategic Voting", "text": "It is interesting to look at the differences between the actual vote distributions considered in this paper and the approximate riffle distributions. Let's take, for example, the APA dataset, in which the optimal approximation through a riffle hierarchy reflects the underlying political coalitions within the organization. However, there are some significant differences in comparison between the approximation and the empirical distribution. For example, the riffle hierarchy underestimates the number of votes received by candidate 3 (a research psychologist) who ultimately won the election.One possible explanation for the discrepancy could be the notion that voters tend to vote strategically in APA elections, with stronger candidates from opposing political coalitions ranked lower, rather than revealing their true preferences.An interesting line of future work is to detect and examine the presence of such strategic coordinations in the datasets. Open issues include (1) the mathematical verification of whether or not strategic voting data (for example, if APA is strategic enough to influence the voting structure)."}, {"heading": "10. Conclusion", "text": "In probabilistic reasoning problems, it is often the case that certain types of data suggest certain distributional representations; for example, the sparse dependence structure in the data often suggests a Markov random field (or other graphical model) representation (Friedman, 1997, 1998); for low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate; for preference ranking scenarios, one has to deal with the \"complexity of human tasks\" - the difficulty for a human being to classify a long list of items, and often leads to partial rather than fully ranked data. In this paper, we have shown that when data takes the form of partial rankings, hierarchical riffle-independent models are a natural representation.As with conjugated priors, we showed that a riff-guaranteed independent model can maintain its factorial structure."}, {"heading": "Acknowledgments", "text": "This project was formulated and largely carried out during an internship by Jonathan Huang at Microsoft Research. Further work was partially supported by ONR under MURI N000140710747 and ARO under MURI W911NF0810242. Carlos Guestrin was partially funded by NSF Career IIS-064422. We thank Eric Horvitz, Ryen White, Dan Liebling and Yi Mao for the discussion."}, {"heading": "Appendix A. Proofs", "text": "In this appendix, we provide supplementary evidence for some of the theoretical results in this paper.A.1 Proof Theorem 19To prove Theorem 19 (as well as subsequent results), we refer to precedence. Definition 32. Given a partial precedence order of type \u03b3, we refer to the precedence occupied by the two factors. (Note: R \u00b2 i depends only on the results and can be written as R \u00b2 1.) And we refer to the following basic fact relating to precedence: rank 33. Point 1 + 1,.), we refer to the precedence occupied by the two factors. (R \u00b2 r = [2], the precedence i + 1,. And we refer to the following basic facts relating to precedence: rank 33. Point 1 |."}, {"heading": "1|23 12|3 13|2 2|13 3|12 23|1", "text": "We prove three things that together support the theorem: (1) that the algorithm is contained in at least one vertical bar at each stage of the X precedence order, and (3) that all vertical bars are contained in each element of the X.1. First, we note that the algorithm contains at least one vertical bar at each stage of the X precedence order, and if all vertical bars are contained in the elements of the X.1. There is no disagreement about relative ordering.2. We now show that at each stage of the algorithm, each element of the Xt precedence order is a subset of the X.2 precedence (X), and each element of the X.2 precedence is that each element of the X.2 is a subset of the X.3 (X)."}], "references": [{"title": "Aggregation of partial rankings, p-ratings and top-m lists", "author": ["N. Ailon"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, SODA \u201907,", "citeRegEx": "Ailon,? \\Q2007\\E", "shortCiteRegEx": "Ailon", "year": 2007}, {"title": "Voting schemes for which it can be difficult to tell who won", "author": ["J.J. Bartholdi", "C.A. Tovey", "M. Trick"], "venue": "Social Choice and Welfare,", "citeRegEx": "Bartholdi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bartholdi et al\\.", "year": 1989}, {"title": "Cluster analysis of heterogeneous rank data", "author": ["L.M. Busse", "P. Orbanz", "J. Buhmann"], "venue": "In The 24th Annual International Conference on Machine Learning,", "citeRegEx": "Busse et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Busse et al\\.", "year": 2007}, {"title": "Distance based ranking models", "author": ["M.A. Fligner", "J.S. Verducci"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Fligner and Verducci,? \\Q1986\\E", "shortCiteRegEx": "Fligner and Verducci", "year": 1986}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Learning belief networks in the presence of missing values and hidden variables", "author": ["N. Friedman"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Friedman,? \\Q1997\\E", "shortCiteRegEx": "Friedman", "year": 1997}, {"title": "The bayesian structural em algorithm", "author": ["N. Friedman"], "venue": "In The 14th Conference on Uncertainty in Artificial Intelligence, UAI \u201998,", "citeRegEx": "Friedman,? \\Q1998\\E", "shortCiteRegEx": "Friedman", "year": 1998}, {"title": "A latent space model for rank data", "author": ["C. Gormley", "B. Murphy"], "venue": "In Proceedings of the 2006 conference on Statistical network analysis,", "citeRegEx": "Gormley and Murphy,? \\Q2007\\E", "shortCiteRegEx": "Gormley and Murphy", "year": 2007}, {"title": "Efficient probabilistic inference with partial ranking queries", "author": ["J. Huang", "A. Kapoor", "C. Guestrin"], "venue": "In The 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Probabilistic Reasoning and Learning on Permutations: Exploiting Structural Decompositions of the Symmetric Group", "author": ["J. Huang"], "venue": "Ph.D. thesis,", "citeRegEx": "Huang,? \\Q2011\\E", "shortCiteRegEx": "Huang", "year": 2011}, {"title": "Riffled independence for ranked data", "author": ["J. Huang", "C. Guestrin"], "venue": "Advances in Neural Information Processing Systems 22,", "citeRegEx": "Huang and Guestrin,? \\Q2009\\E", "shortCiteRegEx": "Huang and Guestrin", "year": 2009}, {"title": "Learning hierarchical riffle independent groupings from rankings", "author": ["J. Huang", "C. Guestrin"], "venue": "In Proceedings of the 27th Annual International Conference on Machine Learning,", "citeRegEx": "Huang and Guestrin,? \\Q2010\\E", "shortCiteRegEx": "Huang and Guestrin", "year": 2010}, {"title": "Uncovering the riffled independence structure of ranked data", "author": ["J. Huang", "C. Guestrin"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Huang and Guestrin,? \\Q2012\\E", "shortCiteRegEx": "Huang and Guestrin", "year": 2012}, {"title": "Efficient inference for distributions on permutations", "author": ["J. Huang", "C. Guestrin", "L. Guibas"], "venue": "Advances in Neural Information Processing Systems 20,", "citeRegEx": "Huang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2008}, {"title": "Fourier theoretic probabilistic inference over permutations", "author": ["J. Huang", "C. Guestrin", "L.J. Guibas"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims,? \\Q2002\\E", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Multi-object tracking with representations of the symmetric group", "author": ["R. Kondor", "A. Howard", "T. Jebara"], "venue": "Proceedings of the Eleventh", "citeRegEx": "Kondor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kondor et al\\.", "year": 2007}, {"title": "Group theoretical methods in machine learning", "author": ["R. Kondor"], "venue": "Ph.D. thesis,", "citeRegEx": "Kondor,? \\Q2008\\E", "shortCiteRegEx": "Kondor", "year": 2008}, {"title": "Conditional models on the ranking poset", "author": ["G. Lebanon", "J. Lafferty"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "Lebanon and Lafferty,? \\Q2003\\E", "shortCiteRegEx": "Lebanon and Lafferty", "year": 2003}, {"title": "Non-parametric modeling of partially ranked data", "author": ["G. Lebanon", "Y. Mao"], "venue": "Advances in Neural Information Processing Systems 20,", "citeRegEx": "Lebanon and Mao,? \\Q2008\\E", "shortCiteRegEx": "Lebanon and Mao", "year": 2008}, {"title": "Learning mallows models with pairwise preferences", "author": ["T. Lu", "C. Boutilier"], "venue": "In The 28th Annual International Conference on Machine Learning,", "citeRegEx": "Lu and Boutilier,? \\Q2011\\E", "shortCiteRegEx": "Lu and Boutilier", "year": 2011}, {"title": "Analyzing and Modeling Rank Data", "author": ["J.I. Marden"], "venue": null, "citeRegEx": "Marden,? \\Q1995\\E", "shortCiteRegEx": "Marden", "year": 1995}, {"title": "Consensus ranking under the exponential model", "author": ["M. Meila", "K. Phadnis", "A. Patterson", "J. Bilmes"], "venue": "Tech. rep. 515,", "citeRegEx": "Meila et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meila et al\\.", "year": 2007}, {"title": "Investigating behavioral variability in web search", "author": ["R. White", "S. Drucker"], "venue": "In Proceedings of the 16th international conference on World Wide Web, WWW \u201907,", "citeRegEx": "White and Drucker,? \\Q2007\\E", "shortCiteRegEx": "White and Drucker", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first author\u2019s dissertation (Huang, 2011).", "startOffset": 228, "endOffset": 241}, {"referenceID": 21, "context": "There were five candidates in the election: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).", "startOffset": 136, "endOffset": 150}, {"referenceID": 9, "context": "In Figure 3(a), we reproduce the hierarchical structure that was learned using a fully ranked subset of the APA data consisting of 5000 training examples in Huang and Guestrin (2012). There were five candidates in the election: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).", "startOffset": 157, "endOffset": 183}, {"referenceID": 21, "context": "Candidates are enumerated as: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).", "startOffset": 122, "endOffset": 136}, {"referenceID": 0, "context": "As remarked by Ailon (2007), we note that \u201cThe term partial ranking used here should not be confused with two other standard objects: (1) Partial order, namely, a reflexive, transitive anti-symmetric binary", "startOffset": 15, "endOffset": 28}, {"referenceID": 5, "context": "When the structure is unknown, we use a structural EM approach, which is analogous to methods from the graphical models literature for structure learning from incomplete data (Friedman, 1997, 1998). Unfortunately, the (riffled independence) structure learning algorithm of Huang and Guestrin (2010) is unable to directly use the posterior distributions computed from the E-step.", "startOffset": 176, "endOffset": 299}, {"referenceID": 5, "context": "When the structure is unknown, we use a structural EM approach, which is analogous to methods from the graphical models literature for structure learning from incomplete data (Friedman, 1997, 1998). Unfortunately, the (riffled independence) structure learning algorithm of Huang and Guestrin (2010) is unable to directly use the posterior distributions computed from the E-step. Instead, observing that sampling from riffle independent models can be done efficiently and exactly (as opposed to, for example, MCMC methods), we simply sample full rankings from the posterior distributions computed in the E-step and pass these full rankings into the structure learning algorithm of Huang and Guestrin (2010). The number of samples that are necessary, instead of scaling factorially, scales according to the number of samples required to detect riffled independence (which under mild assumptions is polynomial in n, Huang & Guestrin, 2010).", "startOffset": 176, "endOffset": 706}, {"referenceID": 15, "context": "Algorithms such as the RankSVM (Joachims, 2002) and RankBoost (Freund, Iyer, Schapire, & Singer, 2003), for example, have been successful in the large scale ranking problems that appear in web search.", "startOffset": 31, "endOffset": 47}, {"referenceID": 13, "context": "Kondor, Howard, and Jebara (2007) and Huang, Guestrin, and Guibas (2008, 2009) considered additive decompositions of a distribution into a weighted sum of Fourier basis functions.", "startOffset": 0, "endOffset": 34}, {"referenceID": 9, "context": "1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by one (Huang, 2011) (see Figure 6).", "startOffset": 156, "endOffset": 169}, {"referenceID": 9, "context": "1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by one (Huang, 2011) (see Figure 6). Mallows models (as well as other similar distance based models) have the advantage that they can compactly represent distributions for very large n, and admit conjugate prior distributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameters has been a popular problem for statisticians \u2014 recovering the optimal \u03c30 from data is known as the consensus ranking or rank aggregation problem and is known to be NP -hard (Bartholdi, Tovey, & Trick, 1989). Many authors have focused on approximation algorithms instead. Like Gaussian distributions, Mallows models tend to lack flexibility, and so Lebanon and Mao (2008) propose a nonparametric model of ranked (and partially ranked) data based on placing weighted Mallows kernels on top of training examples, which, as they show, can", "startOffset": 157, "endOffset": 813}, {"referenceID": 18, "context": "Lebanon and Mao (2008), as we have mentioned, developed a nonparametric model based on Mallows models which can handle arbitrary types of partial rankings.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "This closed form equation of Fligner and Verducci (1986), however, can be seen as a very special case of our setting since Mallows models can always be shown to factor riffle independently according to a chain structure.", "startOffset": 29, "endOffset": 57}, {"referenceID": 20, "context": "Finally in recent related work, Lu and Boutilier (2011) considered an even more general class of observations based on DAG (directed acyclic graph) based observations in which probabilities of rankings which are not consistent with a DAG of relative ranking relations are set to zero.", "startOffset": 32, "endOffset": 56}, {"referenceID": 9, "context": "In previous work, we had used only the full rankings of the APA data (Huang & Guestrin, 2010, 2012), but now we are able to use the entire dataset. Figure 7(a) plots, for each k \u2208 {1, . . . , 5}, the number of ballots in the APA data of length k. Likewise, the Meath dataset (Gormley & Murphy, 2007) which was taken from the 2002 Irish Parliament election has over 60,000 top-k rankings of 14 candidates. As with the APA data, we had used only the full rankings of the Meath data in previous work, but here we use the entire dataset. Figure 7(b) plots, for each k \u2208 {1, . . . , 14}, the number of ballots in the Meath data of length k. In particular, note that the vast majority of ballots in the dataset consist of partial rather than full rankings, with over half of the electorate preferring to list only their favorite three or four candidates. We can run inference (Algorithm 1) on over 5000 top-k examples for the Meath data in 10 seconds on a dual 3.0 GHz Pentium machine with an unoptimized Python implementation. Using \u2018brute force\u2019 inference, we estimate that the same job would require roughly one hundred years. We extracted a third dataset from a database of searchtrails collected by White and Drucker (2007), in which browsing sessions of roughly 2000 users were logged during 20082009.", "startOffset": 70, "endOffset": 1223}, {"referenceID": 19, "context": "We compare our method against the work by Lebanon and Mao (2008) in two settings: (1) training on all available data and (2) training on the subset of full rankings.", "startOffset": 42, "endOffset": 65}, {"referenceID": 19, "context": "Using the methods developed in our current paper, however, we compare riffle independent models with the state-of-the-art nonparametric estimator of Lebanon and Mao (2008) (to which we hereby refer as the LM08 estimator) on the same data (setting their regularization parameter to be C =1,2,5, or 10 via a validation set).", "startOffset": 149, "endOffset": 172}, {"referenceID": 13, "context": "How then, can we tractably condition on incomplete rankings? One possible approach is to convert to a Fourier representation using the methods from (Huang & Guestrin, 2012), then conditioning on a pairwise ranking observation using the Fourier domain conditioning algorithm proposed in (Huang et al., 2008).", "startOffset": 286, "endOffset": 306}, {"referenceID": 14, "context": "For low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate.", "startOffset": 94, "endOffset": 128}, {"referenceID": 17, "context": "For low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate.", "startOffset": 94, "endOffset": 128}], "year": 2012, "abstractText": "Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity \u2014 users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. Simultaneously addressing all of these challenges \u2014 i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data \u2014 is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based representations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data. 1. Probabilistic Modeling of Ranking Data: Three Challenges Rankings arise in a number of machine learning application settings such as preference analysis for movies and books (Lebanon & Mao, 2008) and political election analysis (Gormley & Murphy, 2007; Huang & Guestrin, 2010). In many of these problems, it is of great interest to build statistical models over ranking data in order to make predictions, form recommendations, discover latent trends and structure and to construct human-comprehensible data summaries. c \u00a92012 AI Access Foundation. All rights reserved. Huang, Kapoor & Guestrin Modeling distributions over rankings is a difficult problem, however, due to the fact that as the number of items being ranked increases, the number of possible rankings increases factorially. This combinatorial explosion forces us to confront three central challenges when dealing with rankings. First, we need to deal with storage complexity \u2014 how can we compactly represent a distribution over the space of rankings?1 Then there is algorithmic complexity \u2014 how can we efficiently answer probabilistic inference queries given a distribution? Finally, we must contend with what we refer to as human task complexity, which is a challenge stemming from the fact that it can be difficult to accurately elicit a full ranking over a large list of candidates from a human user; choosing from a list of n! options is no easy task and users typically prefer to provide partial information. Take the American Psychological Association (APA) elections, for example, which allow their voters to rank order candidates from favorite to least favorite. In the 1980 election, there were five candidates, and therefore 5! = 120 ways to rank those five candidates. Despite the small candidate list, most voters in the election preferred to only specify their top-k favorite candidates rather than writing down full rankings on their ballots (see Figure 1). For example, roughly a third of voters simply wrote down their single favorite candidate in this 1980 election. These three intertwined challenges of storage, algorithmic, and human task complexity are the central issues of probabilistic modeling for rankings, and models that do not efficiently handle all three sources of complexity have limited applicability. In this paper, we examine a flexible and intuitive class of models for rankings based on a generalization of probabilistic independence called riffled independence, proposed in our recent work (Huang & Guestrin, 2009, 2010). While our previous papers have focused primarily on representational (storage complexity) issues, we now concentrate on inference and incomplete observations (i.e., partial rankings), showing that in addition to storage complexity, riffle independence based models can efficiently address issues of algorithmic and human task complexity. In fact the two issues of algorithmic and human task complexity are intricately linked for riffle independent models. By considering partial rankings, we give users more flexibility to provide as much or as little information as they care to give. In the context of partial ranking data, the most relevant inference queries also take the form of partial rankings. For example, we might want to predict a voter\u2019s second choice candidate given information about his first choice. One of our main contributions in this paper is to show that inference for such partial ranking queries can be performed particularly efficiently for riffle independent models. The main contributions of our work are as follows:2 \u2022 We reveal a natural and fundamental connection between riffle independent models and partial rankings. In particular, we show that the collection of partial rankings over an item set form a complete characterization of the space of observations upon 1. Note that it is common to wonder why one would care to represent a distribution over all rankings if the number of sample rankings is never nearly as large. This problem that the number of samples is always much smaller than n! however, means that most rankings are never observed, limiting our ability to estimate the probability of an arbitrary ranking. The only way to overcome the paucity of samples is to exploit representational structure, which is very much in alignment with solving the storage complexity issue. 2. This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first author\u2019s dissertation (Huang, 2011).", "creator": "TeX"}}}