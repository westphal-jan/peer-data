{"id": "1401.6875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World", "abstract": "To tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring speech and eye gaze during interaction to automatically acquire new words. Although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. First, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. While these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge base about the domain and vocabulary. While the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. Automated recognition of closely-coupled speech-gaze streams based on conversation context is important. To address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance.", "histories": [["v1", "Thu, 16 Jan 2014 04:48:43 GMT  (519kb)", "http://arxiv.org/abs/1401.6875v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shaolin qu", "joyce y chai"], "accepted": false, "id": "1401.6875"}, "pdf": {"name": "1401.6875.pdf", "metadata": {"source": "CRF", "title": "Context-Based Word Acquisition for Situated Dialogue in a Virtual World", "authors": ["Shaolin Qu", "Joyce Y. Chai"], "emails": ["qushaoli@cse.msu.edu", "jchai@cse.msu.edu"], "sections": [{"heading": "1. Introduction", "text": "The idea that came up in previous work (Yu Ballard, 2004; Dahi, 2007) is not new: although the vocabulary in question is automatically outside the knowledge of the system, the system tends to fail. As conversation interfaces become increasingly important in many areas of application, such as long-distance relationships with robots (Lemon, Gruenstein, & Peters, 2002; Fong & Nourbakhsh, 2005) and interaction with other visual actors, the ability to learn new words during online conversation becomes indispensable. Unlike the traditional dialog systems in which users converse, they can draw on a graphical representation or a virtual world while interacting with artificial language. This unique setting provides an opportunity for automatic capture of vocabulary. During interaction, the visual perception of the user (e.g. by looking at the eye) provides a potential channel for the system to automatically learn new words."}, {"heading": "2. Related Work", "text": "In the last few years, the number of people able to work in the USA, Europe, Asia, Asia, Africa, Asia, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa"}, {"heading": "3. Domain and Data", "text": "The difference lies in two aspects: 1) This data set was collected during a mixed initiative human-machine conversation, whereas the data in our previous study was based only on questions and answers; 2) User studies in this work were conducted in the context of a situated dialogue in which human users are immersed in a complex virtual world and can move around in the virtual environment. 3) The application of our virtual world is for treasure hunting. A human user is immersed in a virtual world (i.e., a castle) and can move to search for hidden treasures. The user must consult with a remote expert."}, {"heading": "3.2 Data Preprocessing", "text": "The word size of the speech transcript is 1082, including 757 nouns / adjectives; the user's speech was also automatically recognized online by Microsoft speech recognition with a word error rate (WER) of 48.1% for the 1-best recognition; the vocabulary size of the 1-best speech recognition is 3041, including 1643 nouns / adjectives; the nouns and adjectives in the transcriptions and the recognized 1-best hypotheses were automatically identified by the Stanford Part-of-Speech Tagger (Toutanova & Manning, 2000; Toutanova, Klein, Manning, & Singer, 2003); the collected language and the glances were automatically paired by the system; each time the system recognized a sentence limit of the speech, it paired the recognized speech with the glances that had fixed the object."}, {"heading": "4. Translation Models for Word Acquisition", "text": "Since we work on conversation systems in which users interact with a visual scene, we consider the task of word acquisition to be the association of words with visual entities in the domain. Given the parallel linguistic and vision-fixed entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate the word-entity association probabilities p (w | e). Words with the highest association probabilities are selected as acquired words for entity e."}, {"heading": "4.1 Base Model I", "text": "Using the translation model I (Brown, Pietra, Pietra, & Mercer, 1993), in which each word is equally likely to be aligned with each entity, we have p (w | e) = 1 (l + 1) m x j = 1 l \u2211 i = 0 p (wj | ei) (1), where l and m are the lengths of the entity and the word sequences respectively."}, {"heading": "4.2 Base Model II", "text": "Using Translation Model II (Brown et al., 1993), where alignments depend on word / entity positions and word / entity sequence lengths, we have (w | e) = m \u0441j = 1 l \u2211 i = 0 p (aj = i | j, m, l) p (wj | ei) (2), where aj = i means wj is aligned with ei. If aj = 0, wj is not aligned with any entity (e0 stands for a zero entity), we refer to this model as Model-2. Compared to Model-1, Model-2 takes into account the sequence of words and entities in word capture. EM algorithms are used to estimate the probabilities p (w | e) in the translation models."}, {"heading": "5. Incorporating User Language Behavior in Word Acquisition", "text": "In Model-2, the temporal distance between ei and wj asd (ei, wj) = 0 (ei) ei ei ei ei ei ei ei ei ei ei ei ei ei ei ei ei ei i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i"}, {"heading": "6. Incorporating Domain Knowledge in Word Acquisition", "text": "Suppose a user says, \"There is a lamp on the dresser\" while looking at a lamp object on a table. Due to its simultaneous occurrence with the lamp object, the words dresser and lamp in the translation models could both be associated with the lamp object. To solve this kind of association problem, semantic knowledge of the domain and the words can be helpful. For example, knowing that the word lamp is more semantically related to the object lamp can help the system not associate the word dresser with the lamp object."}, {"heading": "6.1 Domain Modeling", "text": "The domain model contains all the domain-related semantic concepts. In our current work, all the properties of the domain units are represented by domain concepts. Essential properties include: semantic type, color, size, shape and material. We use WordNet synsets to represent the domain concepts (i.e. synsets in the format of the word # part-of-speech # senseid \"). The\" sense-id \"here represents the specific WordNet sense associated with the word when it represents the concept. Thus, the SEM PLATE and COLOR domain concepts of the entity plate have the same terms as entities.\""}, {"heading": "6.2 Semantic Relatedness of Word and Entity", "text": "We calculate the semantic affinity of a word w and an entity e based on the semantic similarity between w and the properties of e using the domain model as a bridge. Specifically, semantic affinity SR (e, w) is defined as SR (e, w) = max i, jsim (s (cie), sj (w) (6), where cie is the i-th property of the entity e, s (c i e) is the synthesis of the property c i e as defined in the domain model, sj (w) is the j-th synthesis of the word w as defined in WordNet, and sim (\u00b7, \u00b7) is the similarity of two synsets. We calculated the similarity value of two synsets based on the path length between them. The similarity value is inversely proportional to the number of nodes along the shortest path between the synsets defined in WordNet. If the two synsets have the same similarity, the maximum similarity of the 2004 (the 1) similarity was determined."}, {"heading": "6.3 Word Acquisition with Word-Entity Semantic Relatedness", "text": "We can use the semantic relativization of word and entity to help the system obtain semantically compatible words for each entity, thereby improving word acquisition performance. Semantic relativization can be applied in two ways: re-processing the learned word-entity probabilities by linking them to semantic relativization (w), or directly influencing the learning of word-entity associations by limiting the alignment of word and entity in the translation models. 6.3.1 Rescoring with semantic relativization. In the acquired word list for an entity ei, each word wj has an association probability p (wj | ei), which is learned from a translation model. We use the semantic relativization SR (ei, wj) to redistribute the probability mass for each wj."}, {"heading": "7. Incorporating Conversation Context in Word Acquisition", "text": "As already mentioned, not all speech-gaze pairs are useful for word capture. If the language does not have a word related to one of the rigid entities of the gaze, this instance only adds noise to the word capture. Therefore, we should identify the tightly coupled speech-gaze pairs and use them only for word capture. In this section, we first describe the feature extraction based on the interactivity of the conversation, then we describe the use of a logistic regression classifier to predict whether a language-gaze pair is a tightly coupled language-gaze instance - an instance in which at least one noun or adjective in the speech stream refers to a certain rigid entity in the gaze stream. To train the classifier for predicting the language-gaze, we manually designated each instance whether it is a tightly coupled language-gaze instance based on the language script and gaze fixations."}, {"heading": "7.1 Features Extraction", "text": "For a parallel language view instance, the following features are automatically extracted."}, {"heading": "7.1.1 Speech Features (S-Feat)", "text": "Let cw be the number of nouns and adjectives in the utterance, and ls the length of the speech. The following characteristics are taken from the language: \u2022 cw - number of nouns and adjectives. Further nouns and adjectives are expected in the utterance of the user describing entities. \u2022 cw / ls - normalized number of nouns / adjectives. The effect of the linguistic length ls on cw is taken into account."}, {"heading": "7.1.2 Gaze Features (G-Feat)", "text": "Note that several fixations may have the same fixation unit, lie is the total length of all fixation units fixed to unit ei. We extract the following characteristics from the viewing stream: \u2022 ce - number of different fixers. Fewer fixers are expected when the user describes units as he looks at them. \u2022 ce / ls - normalized number of units. Considered is the effect of the language's temporal length on unit. \u2022 maxi (lie) - maximum fixation length. At least one fixer unit will be long enough when the user describes units as he looks at them. \u2022 means (lie) - average fixation length. The average fixation length is expected to be longer when the user describes objects as he looks at them. \u2022 var (lie) - variance of fixation lengths. The variance of fixation lengths will be smaller if the scene is viewed as a fixation length, while the fixation scene is determined only by the lighting theme."}, {"heading": "7.1.3 User Activity Features (UA-Feat)", "text": "During the interaction with the system, the activity of the user can also be useful to determine whether the user's gaze is closely linked to the content of the language. The following characteristics are extracted from the user's activities: \u2022 maximum distance of the user's movements - maximum change of the user's position (3D coordinates) during the language length. It is expected that the user moves in a smaller area while viewing and describing units. \u2022 Variance of user positions It is expected that the user moves less frequently while viewing and describing units."}, {"heading": "7.1.4 Conversation Context Features (CC-Feat)", "text": "When talking to the system (i.e., the \"expert\"), the user's language and gaze behavior are influenced by the state of the conversation. For each speech-gaze instance, we use the previous system response type as a nominal feature to predict whether it is a tightly coupled language-gaze instance. In our treasure hunt domain, there are eight types of system responses in two categories: \"System Initiative Answers: \u2022 Specific Seeing - the system asks if the user sees a specific entity, e.g.\" Do you see a similar object before? \"\u2022 Non-Specific Seeing - the system asks if the user sees something, e.g.\" Do you see something different?, \"\" Tell me what you see. \"\u2022 Predict - the system asks if the user sees something before, e.g.\" Have you seen a similar object before?. \"\u2022 Describe - the system asks the user to describe in detail what the user sees, e.g.\" Initiative. \""}, {"heading": "7.2 Logistic Regression Model", "text": "Considering the extracted feature x and the \"close coupled\" label y of each instance in the training set, we train a logistic return model (Cessie & Houwelingen, 1992) to predict whether an instance is a tightly coupled instance (y = 1) or not (y = 0). In the logistic return model, the probability that yi = 1, given the feature xi = (xi1, x i 2,.., x i m), is modelled by p (yi | xi) = exp (\u2211 m j = 1 \u03b2jx i j) 1 + exp (\u0445 mj = 1 \u03b2jx i j), whereby \u03b2j are the characteristic weights to be learned. Log probability l of the data (X, y) isl (\u03b2) = \u2211 i [yi log p (yi | xi) + (1 \u2212 yi | xi) log (1 \u2212 p (yi | xi)]]] In the logistic parameters, the parameters are estimated by a more stable (maximized)."}, {"heading": "7.3 Evaluation of Speech-gaze Identification", "text": "Since the goal of identifying tightly coupled language and visual instances is to improve word capture and we are only interested in capturing nouns and adjectives, only instances with recognized nouns / adjectives are used to train the logistical regression classifier. Of the 2969 instances with recognized nouns / adjectives and eye fixations, only instances with recognized nouns / adjectives are labeled as tightly coupled in 2002 (67.4%). Prediction precision was evaluated by 10-fold cross validation. Table 1 shows the precision of prediction and recall when different characteristics are used. As shown in the table, when more characteristics are used, the predictive precision of prediction precision increases and recall decreases. It is important to note that prediction precision for word capture is more critical than recall when sufficient amounts of data are available. Noisy instances in which the view is not associated with the wrong word content, although it may be associated with the wrong result."}, {"heading": "8. Evaluation of Word Acquisition", "text": "Every practical conversation system starts with a basic knowledge base (vocabulary). We assume that the system already has a default word for each unit in its standard vocabulary. The default word of a unit indicates the semantic type of unit. For example, the word \"barrel\" is the default word for the unit barrel. Among the acquired words, we evaluate only the new words that are not in the vocabulary of the system. For example, the word \"barrel\" would be excluded from the candidate words acquired for the unit barrel."}, {"heading": "8.1 Grounding Words to Domain Concepts", "text": "Based on the translation models for word acquisition (Section 5 & 6), we can obtain the word-entity association probability p (w | e), which provides a means of grounding words for entities. In conversation systems, an important goal of word acquisition is to make the system understand the semantic meaning of new words. Word acquisition by grounding words for objects is not always sufficient to identify their semantic meaning. Suppose the word green is grounded to a green chair object, so is the word chair. Although the system is aware that green is a word describing the green chair, it does not know that the word green refers to the color of the chair, while the word chair refers to the semantic type of the chair. Thus, after learning the word-entity associations p (w | e) through the translation models, we must further make ground words into domain terms to determine the properties of the entity."}, {"heading": "8.2 Evaluation Metrics", "text": "Following the standard evaluation of information gathering, the following metrics are used to evaluate the words acquired for domain concepts (i.e. entity properties). \u2022 Precision \u2211 ce # words correctly obtained for ce \u2211 ce # words acquired for ce \u2211 ce # words acquired for ce \u2022 Remember that ce # words obtained for ce \u2211 ce # \"gold standard\" words from ce \u2022 Average Average Average Precision (MAP) MAP = \u2211 e \u2211 Nw r = 1 P (r) \u00d7 rel (r) Ne # where Ne is the number of \"gold standard\" words of all properties of entity e, Nw is the vocabulary size, P (r) the acquisition precision at a certain threshold r, rel (r) is a binary function indicating whether the word ranked r is a \"gold standard\" word for any property of entity e."}, {"heading": "8.3 Evaluation Results", "text": "To investigate the impact of language-glance information and semantic knowledge on word acquisition, we compare the word-acquisition performance of the following models: \u2022 Model-1 - Base Model I without word-entity alignment (Equation (1). \u2022 Model-1-r - Model-1 with semantic kinship Rescoring of the word-entity association. \u2022 Model-2 - Base Model II with positional alignment (Equation (2)). \u2022 Model-2s - improved model with semantic alignment (Equation (8). \u2022 Model-2t - improved model with temporal alignment (Equation (3). \u2022 Model-2ts - improved model with temporal and semantic alignment (Equation (10). \u2022 Model-2t-r - Model-2t with semantic kinship Rescoring of the word-entity association)."}, {"heading": "8.3.1 Results of Using Speech-Gaze Temporal Information", "text": "Figure 5 shows the interpolated precision retrieval curves and mean average accuracies (MAPs) of Model-2t and the base models Model-1 and Model-2. As shown in the figure, Model-2 does not improve word capture compared to Model-1. This result shows that it is not helpful to consider the index-based positioning of word and unit for word capture. By incorporating the temporal orientation of language and view, Model-2t consistently achieves higher precision than Model-1. In terms of MAP, Model-2t significantly increases MAP (t = 3.08, p < 0.002) compared to Model-1. This means that the use of the temporal orientation of language and view improves text capture."}, {"heading": "8.3.2 Results of Using Domain Semantic Relatedness", "text": "Figure 6 shows the results of the use of semantic references in word capture. As shown in the figure, the use of semantic references improves word acquisition, regardless of whether they are used to re-evaluate the word-entity association (model-1-r) or to restrict word-entity alignment (model-1). Compared to model-1, semantic references can also be used together with the temporal information of the linguistic gaze to improve word capture. Compared to model-1, the MAP is significantly improved by model-2ts (t = 5.36, p < 0.001) and model-2s (t = 5.59, p < 0.001) when semantic references are used together with temporal references to improve word capture."}, {"heading": "8.3.3 Results Based on Identified Closely Coupled Speech-Gaze Streams", "text": "We have shown that the model-2t-r, which takes into account both the temporal orientation of the word and the semantic relationality of the word, achieves the best word acquisition performance. Therefore, the model-2t-r is used to evaluate word acquisition based on the tightly coupled language gauze data identified. As the model-2t-r requires linking domain models with external knowledge sources (e.g. WordNet) that may not be available for some applications, we also evaluate the effect of identifying tightly coupled language entry streams on word capture with the model-2t, which only includes language entry time periods. We evaluate the effect of automatically identifying tightly coupled language entry agencies on word capture through a 10-fold cross validation. In each fold, 10% of the data sets were used to inform the logistical regression classifier for predicting tightly coupled language instances."}, {"heading": "8.3.4 Comparison of Results Based on Speech Recognition and Transcript", "text": "In order to show the effect of speech recognition quality on word acquisition, we also compare acquisition performance based on language transcript and 1-best recognition. If word acquisition is based on language transcript, the word sequence in the parallel language transcript contains nouns and adjectives in the language transcript. Accordingly, the language characteristic used for coupled language transcript identification is extracted from the language transcript. Figures 9 and 10 show the word acquisition performance of Model-2t and Model-2t-r using all instances and using only predicted coupled instances based on the language transcript or 1-best recognition. As shown in the illustrations, the quality of speech recognition performance is critical to word acquisition performance. As expected, word acquisition performance based on language transcript is much better than in language recognition."}, {"heading": "9. Examples", "text": "Table 2 shows the 10 best candidate words acquired by Model-1, Model2t and Model-2t-r on the basis of all instances of speaking gaze and Model-2t-r on the basis of tightly coupled instances. The probabilities of these candidate words are also given in the table.Table 3 shows another example of the 10 best candidate words acquired by the four different models for the entity stool. Model-1 acquires four correct words in the 10-best list. Although Model-2t also acquires four correct words in the 10-best list, the rankings of these words are higher. As both language-glance words are better in chronological alignment and domain-semantic relationship in repetition, Model-2t-r gets seven correct words in the 10-best list."}, {"heading": "10. The Effect of Online Word Acquisition on Language Understanding", "text": "An important goal of word acquisition is to use the acquired new words to support language comprehension in the subsequent conversation. To demonstrate the impact of online word acquisition on language comprehension, we conduct simulation studies based on the data we collect. In these simulations, the system starts with an initial knowledge base - a vocabulary associated with domain concepts. The system continuously expands its knowledge base by acquiring words from users with Model-2t-r, which includes both temporal information about language gazes and semantic relationships between domains.The advanced knowledge base is used to understand the language of new users.We evaluate language comprehension performance based on the Concept Identification Rate (CIR): CIR = # correctly identified concepts in the 1-best language recognition # language transcriptsWe simulate the process of online word acquisition and assess their impact on language comprehension in two situations: 1) The training system begins with a small training set, and 2)"}, {"heading": "10.1 Simulation 1: When the System Starts with No Training Data", "text": "The first simulation follows this practice. The system has a standard vocabulary to start without training data. The standard vocabulary contains a \"seed word\" for each domain concept. On the basis of the collected data of 20 users, the simulation process goes through the following steps: \u2022 For the user index i = 1, 2,.., 20: - Rate the CIR of the utterances of the i-th user (1-best speech recognition) with the current system vocabulary. - Capture words from all instances (with 1-best speech recognition) of the user 1 \u00b7 i. - Add verified new words to the system vocabulary among the 10-best words acquired. In the above process, the language understanding of each user's performance is dependent on their own language and the position of the user in the user sequence. To reduce the effect of word acquisition on the understanding of the language, we can have a better word vocabulary. In the process above, the user's own language performance is limited to each of the language as well as the user's own ability to understand each language."}, {"heading": "10.2 Simulation 2: When the System Starts with Training Data", "text": "Many conversation systems use real user data to derive domain vocabulary. To follow this practice, the second simulation provides the system with some training data. The training data serves two purposes: 1) to build an initial vocabulary of the system; 2) to train a classifier to predict the closely related speaking and visual instances of new users. Using the collected data of 20 users, the simulation process goes through the following steps: \u2022 Use the data of the first users as training data; purchase words from the training instances (with language transcript); add the verified 10-best words to the vocabulary of the system as \"seed words\"; build a classifier with the training data to predict closely linked language instances. \u2022 Evaluate the effect of incremental word acquisition on the CIR of the remaining (20-m) users who predicted the vocabulary scheme with the remaining 20-m (of the system)."}, {"heading": "10.3 The Effect of Speech Recognition on Online Word Acquisition and Language Understanding", "text": "The simulation results in Figures 11 and 12 are based on the most recognized speech hypotheses with a relatively high WHO (48.1%).In order to show the effect of speech recognition quality on online word acquisition and speech comprehension, we are also running Simulation 1 and Simulation 2 based on language transcripts.The simulation processes are the same as those based on 1-best speech recognition except that word acquisition is based on language transcripts and CIR is also evaluated on language transcripts in the new simulations. Figure 13 shows the CIR curves based on language transcripts during online conversation. Word acquisition improves the system's speech comprehension when more users have communicated with the system. This is consistent with the CIR curves on the 1-best speech recognition. However, the CIRs based on language transcripts are much higher than what is confirmed on speech comprehension CIR1."}, {"heading": "11. Discussion and Future Work", "text": "Our experimental results have shown that the inclusion of additional information improves word acquisition compared to completely uncontrolled approaches. However, our current approaches have several limitations. Firstly, the inclusion of domain knowledge through semantic kinship based on WordNet can limit the words acquired to those words appearing in WordNet. This is certainly not desirable, but this limitation can be easily addressed by changing the way the word probability distribution is tailored through semantic kinship (in Section 6.3.1 and Section 6.3.2). For example, one simple way is to keep the probability mass for these words out of WordNet and only adjust the distribution of these words occurring in WordNet based on their semantic kinship with the object. Secondly, in our current approach, words related to words are limited to words detected by the language recognition mechanism. As shown in Section 8.3.4, language recognition performance is quite poor in our experiments."}, {"heading": "12. Conclusions", "text": "Motivated by the psycholinguistic results, we are investigating the use of the moment for automatic text capture in multimodal conversation systems. This paper presents several improved models that incorporate user language behavior, expertise and conversation context into text acquisition. Our experiments have shown that these improved models significantly improve word acquisition performance.Recent advances in eye-tracking technology have made available non-intrusive eye-tracking devices that tolerate head movements and provide high tracking quality. Integration of eye tracking with conversation interfaces is no longer out of reach. We believe that integrating instant with automatic text capture is another potential approach to enhancing the robustness of human-machine conversations."}, {"heading": "Acknowledgments", "text": "This work was supported by IIS-0347548 and IIS-0535112 from the National Science Foundation. We would like to thank anonymous reviewers for their valuable comments and suggestions."}], "references": [{"title": "Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models", "author": ["P.D. Allopenna", "J.S. Magnuson", "M.K. Tanenhaus"], "venue": "Journal of Memory & Language,", "citeRegEx": "Allopenna et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Allopenna et al\\.", "year": 1998}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "N. de Freitas", "D. Forsyth", "D. Blei", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Barnard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2003}, {"title": "Minding the clock", "author": ["K. Bock", "D.E. Irwin", "D.J. Davidson", "W. Leveltb"], "venue": "Journal of Memory and Language,", "citeRegEx": "Bock et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bock et al\\.", "year": 2003}, {"title": "The mathematic of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Watching the eyes when talking about size: An investigation of message formulation and utterance planning", "author": ["S. Brown-Schmidt", "M.K. Tanenhaus"], "venue": "Journal of Memory and Language,", "citeRegEx": "Brown.Schmidt and Tanenhaus,? \\Q2006\\E", "shortCiteRegEx": "Brown.Schmidt and Tanenhaus", "year": 2006}, {"title": "Utilizing visual attention for cross-modal coreference interpretation", "author": ["D. Byron", "T. Mampilly", "V. Sharma", "T. Xu"], "venue": "In Proceedings of the Fifth International and Interdisciplinary Conference on Modeling and Using Context", "citeRegEx": "Byron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Byron et al\\.", "year": 2005}, {"title": "Using eye movements to determine referents in a spoken dialogue system", "author": ["E. Campana", "J. Baldridge", "J. Dowding", "B. Hockey", "R. Remington", "L. Stone"], "venue": "In Proceedings of the Workshop on Perceptive User Interface", "citeRegEx": "Campana et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Campana et al\\.", "year": 2001}, {"title": "Ridge estimators in logistic regression", "author": ["S.L. Cessie", "J.V. Houwelingen"], "venue": "Applied Statistics,", "citeRegEx": "Cessie and Houwelingen,? \\Q1992\\E", "shortCiteRegEx": "Cessie and Houwelingen", "year": 1992}, {"title": "Learning to sportscast: A test of grounded language acquisition", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proceedings of 25th International Conference on Machine Learning (ICML)", "citeRegEx": "Chen and Mooney,? \\Q2008\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2008}, {"title": "Gaze-Contigent Automatic Speech Recognition", "author": ["N.J. Cooke"], "venue": "Ph.D. thesis, University of Birminham", "citeRegEx": "Cooke,? \\Q2006\\E", "shortCiteRegEx": "Cooke", "year": 2006}, {"title": "Looking at the rope when looking for the snake: Conceptually mediated eye movements during spoken-word recognition", "author": ["D. Dahan", "M.K. Tanenhaus"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "Dahan and Tanenhaus,? \\Q2005\\E", "shortCiteRegEx": "Dahan and Tanenhaus", "year": 2005}, {"title": "Domain inference in incremental interpretation", "author": ["D. DeVault", "M. Stone"], "venue": "In Proceedings of ICoS", "citeRegEx": "DeVault and Stone,? \\Q2003\\E", "shortCiteRegEx": "DeVault and Stone", "year": 2003}, {"title": "Eye movements as a window into real-time spoken language comprehension in natural contexts", "author": ["K. Eberhard", "M. Spivey-Knowiton", "J. Sedivy", "M. Tanenhaus"], "venue": "Journal of Psycholinguistic Research,", "citeRegEx": "Eberhard et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Eberhard et al\\.", "year": 1995}, {"title": "Between linguistic attention and gaze fixations inmultimodal conversational interfaces", "author": ["R. Fang", "J.Y. Chai", "F. Ferreira"], "venue": "In Proceedings of the International Conference on Multimodal Interfaces (ICMI),", "citeRegEx": "Fang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2009}, {"title": "A probabilistic incremental model of word learning in the presence of referential uncertainty", "author": ["A. Fazly", "A. Alishahi", "S. Stevenson"], "venue": "In Proceedings of the 30th Annual Conference of the Cognitive Science Society", "citeRegEx": "Fazly et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2008}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Intentional context in situated language learning", "author": ["M. Fleischman", "D. Roy"], "venue": "In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL)", "citeRegEx": "Fleischman and Roy,? \\Q2005\\E", "shortCiteRegEx": "Fleischman and Roy", "year": 2005}, {"title": "Interaction challenges in human-robot space", "author": ["T.W. Fong", "I. Nourbakhsh"], "venue": "exploration. Interactions,", "citeRegEx": "Fong and Nourbakhsh,? \\Q2005\\E", "shortCiteRegEx": "Fong and Nourbakhsh", "year": 2005}, {"title": "Grounded semantic composition for visual scenes", "author": ["P. Gorniak", "D. Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gorniak and Roy,? \\Q2004\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2004}, {"title": "What the eyes say about speaking", "author": ["Z. Griffin", "K. Bock"], "venue": "Psychological Science,", "citeRegEx": "Griffin and Bock,? \\Q2000\\E", "shortCiteRegEx": "Griffin and Bock", "year": 2000}, {"title": "Gaze durations during speech reflect word selection and phonological", "author": ["Z.M. Griffin"], "venue": "encoding. Cognition,", "citeRegEx": "Griffin,? \\Q2001\\E", "shortCiteRegEx": "Griffin", "year": 2001}, {"title": "Why look? Reasons for eye movements related to language production", "author": ["Z.M. Griffin"], "venue": null, "citeRegEx": "Griffin,? \\Q2004\\E", "shortCiteRegEx": "Griffin", "year": 2004}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["B.J. Grosz", "A.K. Joshi", "S. Weinstein"], "venue": "Computational Linguistics,", "citeRegEx": "Grosz et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "The use of eye movements in human-computer interaction techniques: What you look at is what you get", "author": ["R.J.K. Jacob"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Jacob,? \\Q1991\\E", "shortCiteRegEx": "Jacob", "year": 1991}, {"title": "Attention and Effort. Prentice-Hall, Inc., Englewood Cliffs", "author": ["D. Kahneman"], "venue": null, "citeRegEx": "Kahneman,? \\Q1973\\E", "shortCiteRegEx": "Kahneman", "year": 1973}, {"title": "Where is \u201cit\u201d? event synchronization in gaze-speech input systems", "author": ["M. Kaur", "M. Termaine", "N. Huang", "J. Wilder", "Z. Gacovski", "F. Flippo", "C.S. Mantravadi"], "venue": "In Proceedings of the International Conference on Multimodal Interfaces (ICMI)", "citeRegEx": "Kaur et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kaur et al\\.", "year": 2003}, {"title": "SUEDE: A wizard of oz prototyping tool for speech user interfaces", "author": ["S. Klemmer", "A. Sinha", "J. Chen", "J. Landay", "N. Aboobaker", "A. Wang"], "venue": "In Proceedings of ACM Symposium on User Interface Software and Technology,", "citeRegEx": "Klemmer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Klemmer et al\\.", "year": 2000}, {"title": "Collaborative activities and multitasking in dialogue systems", "author": ["O. Lemon", "A. Gruenstein", "S. Peters"], "venue": "Traitement Automatique des Langues,", "citeRegEx": "Lemon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lemon et al\\.", "year": 2002}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Automated vocabulary acquisition and interpretation in multimodal conversational systems", "author": ["Y. Liu", "J. Chai", "R. Jin"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL)", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Viewing and naming objects: eye movements during noun phrase production", "author": ["A. Meyer", "A. Sleiderink", "W. Levelt"], "venue": "Cognition,", "citeRegEx": "Meyer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 1998}, {"title": "Towards a model of face-to-face grounding", "author": ["Y. Nakano", "G. Reinstein", "T. Stocky", "J. Cassell"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Nakano et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nakano et al\\.", "year": 2003}, {"title": "WordNet::Similarity - measuring the relatedness of concepts", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Pedersen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "What\u2019s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces", "author": ["Z. Prasov", "J.Y. Chai"], "venue": "In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI)", "citeRegEx": "Prasov and Chai,? \\Q2008\\E", "shortCiteRegEx": "Prasov and Chai", "year": 2008}, {"title": "An exploration of eye gaze in spoken language processing for multimodal conversational interfaces", "author": ["S. Qu", "J.Y. Chai"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),", "citeRegEx": "Qu and Chai,? \\Q2007\\E", "shortCiteRegEx": "Qu and Chai", "year": 2007}, {"title": "Incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems", "author": ["S. Qu", "J.Y. Chai"], "venue": null, "citeRegEx": "Qu and Chai,? \\Q2008\\E", "shortCiteRegEx": "Qu and Chai", "year": 2008}, {"title": "The role of interactivity in human-machine conversation for automatic word acquisition", "author": ["S. Qu", "J.Y. Chai"], "venue": "In Proceedings of the 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Qu and Chai,? \\Q2009\\E", "shortCiteRegEx": "Qu and Chai", "year": 2009}, {"title": "Conversing with the user based on eye-gaze patterns", "author": ["P. Qvarfordt", "S. Zhai"], "venue": "In Proceedings of the Conference on Human Factors in Computing Systems (CHI)", "citeRegEx": "Qvarfordt and Zhai,? \\Q2005\\E", "shortCiteRegEx": "Qvarfordt and Zhai", "year": 2005}, {"title": "Eye movements in reading and information processing - 20 years of research", "author": ["K. Rayner"], "venue": "Psychological Bulletin,", "citeRegEx": "Rayner,? \\Q1998\\E", "shortCiteRegEx": "Rayner", "year": 1998}, {"title": "Learning visually-grounded words and syntax for a scene description task", "author": ["D. Roy"], "venue": "Computer Speech and Language,", "citeRegEx": "Roy,? \\Q2002\\E", "shortCiteRegEx": "Roy", "year": 2002}, {"title": "Learning words from sights and sounds, a computational model", "author": ["D. Roy", "A. Pentland"], "venue": "Cognitive Science,", "citeRegEx": "Roy and Pentland,? \\Q2002\\E", "shortCiteRegEx": "Roy and Pentland", "year": 2002}, {"title": "A framework for fast incremental interpretation during speech decoding", "author": ["W. Schuler", "S. Wu", "L. Schwartz"], "venue": "Computational Linguistics,", "citeRegEx": "Schuler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuler et al\\.", "year": 2009}, {"title": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic", "author": ["J.M. Siskind"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Siskind,? \\Q2001\\E", "shortCiteRegEx": "Siskind", "year": 2001}, {"title": "Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution", "author": ["M.J. Spivey", "M.K. Tanenhaus", "K.M. Eberhard", "J.C. Sedivy"], "venue": "Cognitive Psychology,", "citeRegEx": "Spivey et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Spivey et al\\.", "year": 2002}, {"title": "Learning lexicons from spoken utterances based on statistical model selection", "author": ["R. Taguchi", "N. Iwahashi", "T. Nose", "K. Funakoshi", "M. Nakano"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Taguchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taguchi et al\\.", "year": 2009}, {"title": "A robust selection system using real-time multi-modal user-agent interactions", "author": ["K. Tanaka"], "venue": "In Proceedings of the International Conference on Intelligent User Interfaces (IUI)", "citeRegEx": "Tanaka,? \\Q1999\\E", "shortCiteRegEx": "Tanaka", "year": 1999}, {"title": "Integration of visual and linguistic information in spoken language comprehension", "author": ["M. Tanenhaus", "M. Spivey-Knowiton", "K. Eberhard", "J. Sedivy"], "venue": null, "citeRegEx": "Tanenhaus et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Tanenhaus et al\\.", "year": 1995}, {"title": "Feature-Rich Part-of-Speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC),", "citeRegEx": "Toutanova and Manning,? \\Q2000\\E", "shortCiteRegEx": "Toutanova and Manning", "year": 2000}, {"title": "Embodied agents for multiparty dialogue in immersive virtual worlds", "author": ["D. Traum", "J. Rickel"], "venue": "In Proceedings of the 1st international joint conference on Autonomous Agents and Multi-Agent Systems", "citeRegEx": "Traum and Rickel,? \\Q2002\\E", "shortCiteRegEx": "Traum and Rickel", "year": 2002}, {"title": "Integration of eye-gaze, voice and manual response in multimodal user interfaces", "author": ["J. Wang"], "venue": "In Proceedings of IEEE International Conference on Systems, Man and Cybernetics,", "citeRegEx": "Wang,? \\Q1995\\E", "shortCiteRegEx": "Wang", "year": 1995}, {"title": "A multimodal learning interface for grounding spoken language in sensory perceptions", "author": ["C. Yu", "D. Ballard"], "venue": "ACM Transactions on Applied Perceptions,", "citeRegEx": "Yu and Ballard,? \\Q2004\\E", "shortCiteRegEx": "Yu and Ballard", "year": 2004}, {"title": "Manual and gaze input cascaded (MAGIC) pointing", "author": ["S. Zhai", "C. Morimoto", "S. Ihde"], "venue": "In Proceedings of the Conference on Human Factors in Computing Systems (CHI),", "citeRegEx": "Zhai et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 1999}, {"title": "Overriding errors in a speech and gaze multimodal architecture", "author": ["Q. Zhang", "A. Imamiya", "K. Go", "X. Mao"], "venue": "In Proceedings of the International Conference on Intelligent User Interfaces (IUI)", "citeRegEx": "Zhang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 38, "context": "This is evident in both language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, & Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005; Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) and language production (Meyer, Sleiderink, & Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003; Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001).", "startOffset": 290, "endOffset": 449}, {"referenceID": 20, "context": "This is evident in both language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, & Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005; Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) and language production (Meyer, Sleiderink, & Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003; Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001).", "startOffset": 290, "endOffset": 449}, {"referenceID": 30, "context": "Specifically in human language production, which is directly relevant to automated computer interpretation of human language, studies have found significant temporal regularities between the mentioned objects and the corresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000).", "startOffset": 237, "endOffset": 293}, {"referenceID": 38, "context": "Specifically in human language production, which is directly relevant to automated computer interpretation of human language, studies have found significant temporal regularities between the mentioned objects and the corresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000).", "startOffset": 237, "endOffset": 293}, {"referenceID": 21, "context": "In object naming tasks, the onset of a word begins approximately one second after a speaker has looked at the corresponding visual referent (Griffin, 2004), and gazes are longer the more difficult the name of the referent is to retrieve (Meyer et al.", "startOffset": 140, "endOffset": 155}, {"referenceID": 30, "context": "In object naming tasks, the onset of a word begins approximately one second after a speaker has looked at the corresponding visual referent (Griffin, 2004), and gazes are longer the more difficult the name of the referent is to retrieve (Meyer et al., 1998; Griffin, 2001).", "startOffset": 237, "endOffset": 272}, {"referenceID": 20, "context": "In object naming tasks, the onset of a word begins approximately one second after a speaker has looked at the corresponding visual referent (Griffin, 2004), and gazes are longer the more difficult the name of the referent is to retrieve (Meyer et al., 1998; Griffin, 2001).", "startOffset": 237, "endOffset": 272}, {"referenceID": 30, "context": "About 100-300 ms after the articulation of the object name begins, the eyes move to the next object relevant to the task (Meyer et al., 1998).", "startOffset": 121, "endOffset": 141}, {"referenceID": 24, "context": "There are different types of eye movements (Kahneman, 1973).", "startOffset": 43, "endOffset": 59}, {"referenceID": 39, "context": ", color, size, shape) of objects (Roy & Pentland, 2002; Roy, 2002).", "startOffset": 33, "endOffset": 66}, {"referenceID": 42, "context": "motion events that were represented by force dynamics encoding the support, contact, and attachment relations between objects in video images (Siskind, 2001).", "startOffset": 142, "endOffset": 157}, {"referenceID": 23, "context": "Eye gaze has long been explored in human-computer interaction for direct manipulation interfaces as a pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde, 1999).", "startOffset": 118, "endOffset": 173}, {"referenceID": 50, "context": "Eye gaze has long been explored in human-computer interaction for direct manipulation interfaces as a pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde, 1999).", "startOffset": 118, "endOffset": 173}, {"referenceID": 45, "context": "In different speech and eye gaze systems, eye gaze has been explored for the purpose of mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004), as a complement to the speech channel for reference resolution (Campana, Baldridge, Dowding, Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo, & Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005) and speech recognition (Cooke, 2006; Qu & Chai, 2007), and for managing human-computer dialogue (Qvarfordt & Zhai, 2005).", "startOffset": 110, "endOffset": 157}, {"referenceID": 9, "context": "In different speech and eye gaze systems, eye gaze has been explored for the purpose of mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004), as a complement to the speech channel for reference resolution (Campana, Baldridge, Dowding, Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo, & Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005) and speech recognition (Cooke, 2006; Qu & Chai, 2007), and for managing human-computer dialogue (Qvarfordt & Zhai, 2005).", "startOffset": 436, "endOffset": 466}, {"referenceID": 29, "context": "In our previous work, we have experimented the application of IBM Translation Model 1 in vocabulary acquisition through gaze modeling in a conversation setting (Liu et al., 2007).", "startOffset": 160, "endOffset": 178}, {"referenceID": 9, "context": "In different speech and eye gaze systems, eye gaze has been explored for the purpose of mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004), as a complement to the speech channel for reference resolution (Campana, Baldridge, Dowding, Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo, & Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005) and speech recognition (Cooke, 2006; Qu & Chai, 2007), and for managing human-computer dialogue (Qvarfordt & Zhai, 2005). Eye gaze has been explored recently for word acquisition. For example, Yu and Ballard (2004) proposed an embodied multimodal learning interface for word acquisition, especially through eye movement.", "startOffset": 437, "endOffset": 628}, {"referenceID": 3, "context": "Using the translation model II (Brown et al., 1993), where alignments are dependent on word/entity positions and word/entity sequence lengths, we have", "startOffset": 31, "endOffset": 51}, {"referenceID": 30, "context": "For example, it shows that speakers look at an object about a second before they say it, but about 100-300 ms after articulation of the object name begins, the eyes move to the next object relevant to the task (Meyer et al., 1998).", "startOffset": 210, "endOffset": 230}, {"referenceID": 29, "context": "Since the conversation setting in our study is much more complex than the simple settings in psycholinguistic research, we found larger variations on the offset (Liu et al., 2007) in our data.", "startOffset": 161, "endOffset": 179}, {"referenceID": 44, "context": "To break this barrier, inspired by previous work (Yu & Ballard, 2004; Taguchi et al., 2009), we are currently extending our approach to incorporate grounding acoustic phoneme sequences to domain concepts.", "startOffset": 49, "endOffset": 91}, {"referenceID": 44, "context": "To break this barrier, inspired by previous work (Yu & Ballard, 2004; Taguchi et al., 2009), we are currently extending our approach to incorporate grounding acoustic phoneme sequences to domain concepts. Another limitation of our current approaches is that they are incapable of acquiring multiword expressions. They can only map single words to domain concepts. However, we did observe multiword expressions (e.g., Rubik\u2019s cube) in our data. We will examine this issue in our future work by incorporating more linguistic knowledge and by modeling \u201cfertility\u201d of entities, for example, as in IBM Model 3 and 4. The simplicity of our current models also limits word acquisition performance. For example, the alignment model based on temporal information directly incorporates findings from psycholinguistic studies. These studies were generally conducted in a much simpler settings without interaction. The recent work by Fang, Chai, and Ferreira (2009) has shown correlations between intensity of gaze fixations and objects denoted by linguistic centers", "startOffset": 70, "endOffset": 954}], "year": 2010, "abstractText": "To tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring speech and eye gaze during interaction to automatically acquire new words. Although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. First, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. While these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge base about the domain and vocabulary. While the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. Automated recognition of closely-coupled speech-gaze streams based on conversation context is important. To address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance.", "creator": "TeX"}}}