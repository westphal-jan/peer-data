{"id": "1401.6984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2014", "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN", "abstract": "The Kaldi toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our open-source recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.", "histories": [["v1", "Mon, 27 Jan 2014 19:55:34 GMT  (157kb)", "http://arxiv.org/abs/1401.6984v1", "unpublished manuscript"]], "COMMENTS": "unpublished manuscript", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["yajie miao"], "accepted": false, "id": "1401.6984"}, "pdf": {"name": "1401.6984.pdf", "metadata": {"source": "CRF", "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN", "authors": ["Yajie Miao"], "emails": [], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "3. PDNN: Yet Another Python Toolkit for Deep Neural Networks", "text": "This year it is as far as it has ever been until the next round."}], "references": [{"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Speech and Audio Processing, Special Issue on Deep Learning for Speech and Lang Processing, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proc. ASRU, pp. 24\u201329, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371-3408, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "Artificial Intelligence and Statistics , 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["G.E. Hinton"], "venue": "UTML TR., Deparment of Computer Science, University of Toronto, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving low-resource CD-DNN- HMM using dropout and multilingual DNN training", "author": ["Y. Miao", "F. Metze"], "venue": "Proc. Interspeech, pp. 2237-2241, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout networks for low-resource speech recognition", "author": ["Y. Miao", "F. Metze", "S. Rawat"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout neural networks for speech recognition", "author": ["M. Cai", "Y. Shi", "J. Liu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural network acoustic models using generalized maxout networks", "author": ["X. Zhang", "J. Trmal", "D. Povey", "S. Khudanpur"], "venue": "submitted to ICASSP 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. ICASSP, pp. 8614-8618, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Neural Networks for Language-Universal Feature Extraction and Cross-language Hybrid Systems", "author": ["Y. Miao", "F. Metze"], "venue": "submitted to ICASSP 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Proc. ICASSP, pp. 3377-3381, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Modular Combination of Deep Neural Networks for Acoustic Modeling", "author": ["J. Gehring", "W. Lee", "K. Kilgour", "I. Lane", "Y. Miao", "A. Waibel"], "venue": "Proc. Interspeech, pp. 94-98, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have shown superior performance over the traditional state-of-the-art GMMHMM on ASR tasks [1, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have shown superior performance over the traditional state-of-the-art GMMHMM on ASR tasks [1, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 2, "context": "This command trains stacked denoising autoencoders (SdAs) [3], mostly used from DNN pretraining.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "RBM training involves maximizing the likelihood of the observations with the contrastive divergence algorithm [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Interested readers can refer to [5] for details on RBM.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "This distribution is governed by a pre-specified probability referred to as dropout factor in [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "For testing (recognition), network parameters need to be scaled properly according to the value of the dropout factor [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.", "startOffset": 23, "endOffset": 32}, {"referenceID": 7, "context": "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.", "startOffset": 23, "endOffset": 32}, {"referenceID": 8, "context": "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.", "startOffset": 23, "endOffset": 32}, {"referenceID": 9, "context": "CNNs can outperform DNNs on LVCSR tasks [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Figure 3 shows our CNN architecture which works slightly different from the existing proposals [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Interested readers can refer to [11] for more details.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "4 shows the Deep Bottleneck Feature (DBNF) architecture [12] which our recipes are using to extract bottleneck front-end.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Hybrid system over spliced bottleneck features [13]", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "Architecture for the Deep Bottleneck Feature (DBNF) network [12].", "startOffset": 60, "endOffset": 64}], "year": 2014, "abstractText": "The Kaldi 1 toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.", "creator": "PScript5.dll Version 5.2.2"}}}