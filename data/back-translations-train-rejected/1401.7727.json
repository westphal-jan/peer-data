{"id": "1401.7727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2014", "title": "Security Evaluation of Support Vector Machines in Adversarial Environments", "abstract": "Support Vector Machines (SVMs) are among the most popular classification techniques adopted in security applications like malware detection, intrusion detection, and spam filtering. However, if SVMs are to be incorporated in real-world security systems, they must be able to cope with attack patterns that can either mislead the learning algorithm (poisoning), evade detection (evasion), or gain information about their internal parameters (privacy breaches). The main contributions of this chapter are twofold. First, we introduce a formal general framework for the empirical evaluation of the security of machine-learning systems. Second, according to our framework, we demonstrate the feasibility of evasion, poisoning and privacy attacks against SVMs in real-world security problems. For each attack technique, we evaluate its impact and discuss whether (and how) it can be countered through an adversary-aware design of SVMs. Our experiments are easily reproducible thanks to open-source code that we have made available, together with all the employed datasets, on a public repository.", "histories": [["v1", "Thu, 30 Jan 2014 03:37:18 GMT  (559kb,D)", "http://arxiv.org/abs/1401.7727v1", "47 pages, 9 figures; chapter accepted into book 'Support Vector Machine Applications'"]], "COMMENTS": "47 pages, 9 figures; chapter accepted into book 'Support Vector Machine Applications'", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["battista biggio", "igino corona", "blaine nelson", "benjamin i p rubinstein", "davide maiorca", "giorgio fumera", "giorgio giacinto", "and fabio roli"], "accepted": false, "id": "1401.7727"}, "pdf": {"name": "1401.7727.pdf", "metadata": {"source": "CRF", "title": "Security Evaluation of Support Vector Machines in Adversarial Environments", "authors": ["Battista Biggio", "Igino Corona", "Blaine Nelson", "Benjamin I. P. Rubinstein", "Davide Maiorca", "Giorgio Fumera", "Giorgio Giacinto", "Fabio Roli"], "emails": ["battista.biggio@diee.unica.it", "igino.corona@diee.unica.it", "davide.maiorca@diee.unica.it", "fumera@diee.unica.it", "giacinto@diee.unica.it", "roli@diee.unica.it", "blaine.nelson@gmail.com", "ben@bipr.net"], "sections": [{"heading": null, "text": "Battista Biggio, Igino Corona, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto and Fabio Roli, Faculty of Electrical Engineering and Electrical Engineering, University of Cagliari, Piazza d'Armi 09123, Cagliari, Italy. e-mail: {battista.biggio, igino.corona, davide.maiorca} @ diee.unica.it e-mail: {fumera, giacinto, roli} @ diee.unica.it Blaine Nelson Institute of Informatics, University of Potsdam, August-Bebel-Stra\u00dfe 89, 14482 Potsdam, Germany. e-mail: blaine.nelson @ gmail.com Benjamin I. P. Rubinstein IBM Research, Lvl 5 / 204 Lygon Street, Carlton, VIC 3053, Australia. e-mail: blaine.nelson @ bipr.net1ar Xiv: 140 1.77 27v1 [s.LG] 3 0Ja, Rolacgio, Giacignza, Giacmi and Giacari, Electrical Engineering, University of Italy."}, {"heading": "1 Introduction", "text": "This year, it is time for the Presidency of the Council of the European Union to move into the EU in order to occupy the Presidency of the Council of the European Union."}, {"heading": "2 Background", "text": "In this section we will discuss the main concepts used in this chapter. First, we will present our notation and summarize the SVM learning problem. Then, we will motivate the need for a correct assessment of the safety of a learning algorithm so that it can be applied to safety-sensitive tasks. Learning can generally be described as a process in which data is used to form a hypothesis that is better than an a priori hypothesis that was created without the data. For our purposes, the hypotheses are presented as functions of the form f: X \u2192 Y that assign an input point x-X to a class y-Y; that is, under observation from the input space X, a hypothesis f is drawn in the output space Y. In binary classification, the output space is binary and we use Y = {\u2212 1, + 1}. In the classical supervised learning environment, we receive a paired training dataset {xi, yi, bigyi} in connection with Y, Y = 1 ruby Ruby, 1 ruby, 1 ruby, 1 by 1 by 1, 1 by 1 by 1."}, {"heading": "2.1 Support Vector Machines", "text": "In its simplest formulation, an SVM was extended to non-linearly separable classification problems by Vapnik margin [25]. Its decision-making function is therefore f (x) = characters (w > x + b), with characters (a) = + 1 (\u2212 1) if a range of 0 (a < 0), and w and b are learned to determine the position of the decision hyperplane in the attribute space: the normal w-plane of the hyperplane gives its orientation and b is its displacement. Thus, the learning task is to find a hyperplane that separates the two classes well. While many hyperplanes may be sufficient for this task, the SVM hyperplane separates both the training samples of the two classes and provides a maximum distance from itself to the next training point (this distance is called the margin of the classifiers), since maximum margin learning generally reduces generalization errors [65]. Although originally designed for linearly separable classification tasks (SVMs), hard margins were not developed."}, {"heading": "2.2 Machine Learning for Computer Security: Motivation, Trends, and Arms Races", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2.3 Adversarial Machine Learning", "text": "As the above examples show, the introduction of machine learning techniques into security-related tasks has many useful aspects, and it has become necessary to some extent due to the increasing complexity and variability of recent attacks and zero-day exploits. However, there are good reasons to believe that machine learning techniques will be subject to carefully designed attacks even in the near future, as the logical next step in the arms race outlined above. As machine learning techniques were not originally designed to withstand manipulation by intelligent and adaptive adversaries, it would be reckless to naively trust these learners in a secure system. Instead, one must carefully consider whether these techniques can introduce new vulnerabilities that compromise the security of the overall system, or whether they can be safely adopted. In other words, we must address the question raised by Barreno et al. [5] Can machine learning be safe?"}, {"heading": "2.3.1 Reactive and Proactive Arms Races", "text": "This year, it has come to the point where there is only one person who will be able to retaliate."}, {"heading": "2.3.2 Previous Work on Security Evaluation", "text": "The first line of research focuses on identifying potential weaknesses in learning algorithms and assessing the impact of the corresponding attacks on the classifier target group; e.g., [4, 5, 18, 36, 40, 41, 42, 46]; the second examines the development of appropriate countermeasures and learning algorithms that are robust against known attacks; e.g., [26, 41, 57]. Although some previous work addresses aspects of the empirical assessment of classifier security, which is often implicitly defined as the performance deterioration that has arisen under a (simulated) attack, to our knowledge a systematic treatment of this process from a unifying perspective has only been described in our recent work [12]. Prior to this, security assessment is generally conducted within a specific application area, such as spam filtering and network intrusion."}, {"heading": "2.3.3 A Taxonomy of Potential Attacks against Machine Learning Algorithms", "text": "In this context, it should be noted that the case concerns a case in which a person was killed who was unable to injure himself."}, {"heading": "3 A Framework for Security Evaluation", "text": "In Sections 2.3 and 2.3.1, we argued the need to simulate a proactive arms race as a means of improving system security. We also argued that assessing a classifier's security properties through simulations of various potential attack scenarios is a critical step in this arms race to identify the most relevant vulnerabilities and suggest how to potentially address them. Here, we summarize our recent work proposing a new framework for designing proactive safe classifiers by addressing the shortcomings of the aforementioned reactive security cycle. Namely, our approach allows for empirical evaluation of a classifier's security during its design phase by outlining the first three steps of the proactive arms race, as shown in Figure 2: (i) identifying potential attack scenarios, (ii) elaborating the corresponding attacks, and (iii) systematically assessing their security implications during its design phase by representing the first three steps of the proactive arms race by engaging in the proactive arms race."}, {"heading": "3.1 Modeling the Adversary", "text": "The proposed model of the adversary is based on specific assumptions about his goal, system knowledge and ability to modify the underlying data distribution by manipulating individual samples. It allows the classification designer to model the attacks identified in the attack taxonomy described in Section 2.3.3 [4, 5, 36]. However, within our framework, application-specific constraints can also be included in the definition of the adversary's ability. Therefore, it can be used to derive practical guidelines for the development of optimal attack strategies and to guide the design of resilient classifiers."}, {"heading": "3.1.1 Adversary\u2019s Goal", "text": "According to the taxonomy, first presented by Barreno et al. [5] and extended by Huang et al. [36], the target of the enemy should be defined on the basis of the expected security breach, which may be a breach of integrity, availability or privacy (see Section 2.3.3), and also depending on the specificity of the attack, which ranges from targeted to indiscriminate. Furthermore, as proposed by Laskov and Kloft [42] and Kloft and Laskov [40], the target of the opponent should be defined in terms of an objective function that the opponent is willing to maximize, allowing a formal characterization of the optimal attack strategy. For example, in an indiscriminate attack on integrity, the opponent may aim to maximize the number of spam emails that elude detection while their content is minimally manipulated [26, 46, 53] while in an indiscriminate attack on integrity the enemy may aim to maximize the number of the ficate availability caused by the number of 52."}, {"heading": "3.1.2 Adversary\u2019s Knowledge", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "3.1.3 Adversary\u2019s Capability", "text": "We provide some guidelines on how the attacker can manipulate samples and their data distribution. As discussed in Section 2.3.3 [4, 5, 36], the opponent can control both training and test data (causal attacks), or only test data (causal attacks). Therefore, we should indicate whether the opponent can manipulate training (TR) and / or testing (TS), as they can be manipulated according to different attack strategies of the opponent. (c.i) We should indicate whether the opponent can manipulate training (TR) and / or testing (TS)."}, {"heading": "3.1.4 Attack Strategy", "text": "Once specific assumptions have been made about the target, knowledge and capabilities of the adversary, it is possible to calculate the optimal attack strategy that corresponds to the hypothetical attack scenario, i.e. the adversary's model, which amounts to solving the optimization problem defined according to the adversary's goal, with appropriate constraints defined in accordance with the adversary's assumed knowledge and capabilities. The attack strategy can then be used to generate the desired attack samples, which must then be uniformly merged with the rest of the data to create appropriate training and testing kits for the desired security assessment, as explained in the previous section. Specific examples of how to derive optimal attacks against SVMs and how to use training and test data to properly integrate them are discussed in Sections 4 and 5.3. See [12] for more details on defining the data distribution and resampling algorithm."}, {"heading": "3.2 How to use our Framework", "text": "We summarize here the steps that can be taken to use our framework correctly in certain application scenarios: 1. hypotheses an attack scenario by identifying the target of a real adversary and following the taxonomy in [4, 5, 36]; 2. define the knowledge of the adversary according to (k.i-v) and capabilities that match (c.i-iv); 3. formulate the appropriate optimization problem and, if necessary, design the associated attack strategy; 4. prepare the collected (training and testing) data accordingly; 5. evaluate the security of the classifier on the newly collected data (including attack samples); 6. repeat the assessment for different knowledge and / or capabilities of the adversary, if necessary; or hypotheses another attack scenario. In the next sections, we will show how our framework can be applied to investigate three security threats to SVMs: evaluate, evaluate, and we will discuss the findings and data breaches as appropriate."}, {"heading": "4 Evasion Attacks against SVMs", "text": "In this section, we look at the problem of SVM circumvention at test time, i.e. how to optimally manipulate samples at test time to avoid detection. [53] The problem of test-time circumvention has been considered in previous work, although either limited to simple decision-making functions such as linear classifiers [26, 46], or to cover any convective classifiers [53] that divide the attribute space into two groups, one of which is convex but does not include the most interesting families of nonlinear classifiers such as neural networks or SVMs. [33] In contrast to this earlier work, the methods presented in our recent paper [8] and in this section show that the evaluation of nuclear-based classifiers at test time can be realized using a gradient approach derived from Golland's technique of discriminatory direction. [33] To further simplify the attacker's efforts, we demonstrate empirically that even if the opponent does not know the rubric classification function, we classify the strategy-1 accurately."}, {"heading": "4.1 Modeling the Adversary", "text": "We first introduce our notation, give our assumptions about the attack scenario, and then derive the appropriate optimal attack strategy. Notation. We consider a classification algorithm f: X 7 \u2192 Y that assigns samples represented in a attribute space x-X, a label in the set of predefined classes y: Y = {\u2212 1, + 1}, where \u2212 1 (+ 1) represents the legitimate (malicious) class. The label fx = f (x) given by a classifier is typically obtained by the threshold of a continuous discrimination function g: X 7 \u2192 R. Without loss of generality, we assume that f (x) = \u2212 1 if g (x) < 0 and + 1 otherwise. Also, note that we use fx to refer to a label that is the classifier for the point x (and not the true label of that dot), the dot and the shortening of the dot."}, {"heading": "4.1.1 Adversary\u2019s Goal", "text": "Malicious (positive) samples are manipulated to bypass the classifier. However, the opponent may be satisfied if a sample x is found to be g (x) < \u2212 \u03b5, where \u03b5 > 0 is a small constant. However, as mentioned in Section 3.1.1, these attacks can be easily defeated by simply adjusting the decision threshold to a slightly more conservative value (e.g. to achieve a lower false negative rate at the expense of a higher false positive rate).For this reason, we assume a smarter opponent whose goal is to have her attack sample classified as legitimate with the greatest confidence. Analytically, this statement can be expressed as follows: Find attack sample x, which minimizes the value of the discriminatory function of the classifier g (x)."}, {"heading": "4.1.2 Adversary\u2019s Knowledge", "text": "We examine two hostile attitudes. In the first case, the opponent has perfect knowledge (PK) of the target classifier (k.v), i.e. he knows the attribute space (k.ii) and the function g (x) (k.iiiv). Therefore, the markings of the target classifier (k.ii) are not needed. In the second case, it is assumed that the opponent has limited knowledge (LK) of the classifier. We assume that he knows the attribute representation (k.ii) and the learning algorithm (k.ii), but that he does not know the learned classifier g (x) (k.iv). In both cases, we assume that the attacker does not have knowledge of the training set (k.i). Within the LK scenario, the opponent cannot know the true discrimination function g (x), but rather approach the marking as g (x) by forming a substitute class."}, {"heading": "4.1.3 Adversary\u2019s Capability", "text": "In the workaround setting, the opponent can only manipulate test data (c.i), i.e. he has no way to influence training data. We further assume that class priorities cannot be modified (c.ii) and that all malicious test samples are affected by the attack (c.iii). In other words, we are interested in simulating an exploratory, indiscriminate attack. The ability of the opponent to manipulate the characteristics of each sample (c.iv) should be defined based on application-specific limitations. However, on a more general level, we can bind the attack point to a maximum distance from the original attack sample, dmax, which is then a parameter of our evaluation. Similar to the previous work, the definition of a suitable distance measurement d: X \u00d7 X 7 \u2192 R to the specific application domain [26, 46, 53]. Note that this distance can limit the opponent's efforts or the cost of manipulating the 52 factors, for example, as the increased attack rate should reflect the greater effectiveness of the 46 factors."}, {"heading": "4.1.4 Attack Strategy", "text": "Within the framework of the attack model described in sections 4.1.1, 4.1.2 and 4.1.3, we use a method that is not applied in practice, however, when it is a targeted malicious sample x0 (the true target of the opponent), an optimal attack strategy finds a sample x * to minimize g or its estimate g *, which is limited to its modification distance of x0: x * = argmin x g (x, x0). For several classifiers, minimizing g (x) can be considered equivalent to maximizing the estimated posterior p (fx = \u2212 x); for example, neural networks, since they directly output a posterior estimate, and for SVMs, since their posterior function is estimated as the sigmoid function of x to the SVM hyperlevel [55]. Generally, this is a non-linear optimization that can be optimized using many known techniques."}, {"heading": "4.2 Evasion Attack Algorithm", "text": "Algorithm 1 describes a method for optimizing the problem of equation (1). It iteratively modifies the target point x in the attribute space as x \"\u2190 x \u2212 t\" E, where \"E\" is a unit vector oriented towards the gradient of our objective function, and \"t\" is the step size. We assume that \"g\" can be differentiated almost anywhere (subgradients can be used for discontinuities). If \"g\" is not differentiable or not smooth enough for a gradient descent to work well, it is also possible to rely on the term \"mimicry / KDE\" for optimizing the equation (1). Algorithm 1 \"Gradient descent\" attack method \"Input\": the starting point, x0; the step size \"t\"; the exchange parameter \"and\" Far > 0. \"Output: x,\" the final point of attack. \""}, {"heading": "4.2.1 Gradient of Support Vector Machines", "text": "For SVMs, g (x) = \u2211 i \u03b1iyik (x, xi) + b results, so the gradient is calculated from the gradient of this kernel process, which is calculable for many numerical cores. In the following, we give the kernel gradients for three main cases: (a) the linear kernel, (b) the RBF kernel, and (c) the polynomial kernel. (a) Linear kernel. In this case, the kernel is simply given using k (x, xi) = < x, xi >. Accordingly, the kernel k (x, xi) = xi (we remind the reader that the gradient must be calculated using the current attack sample x. \u2212 \u2212 In this case, the kernel is simply given using k (x, xi \u2212 w = < x)."}, {"heading": "4.2.2 Gradients of Kernel Density Estimators", "text": "As with SVMs, the gradient of the grain density estimator depends on the gradient of its core. Here, we looked at the generalized RBF cores of the form (x \u2212 xih) = exp (\u2212 d (x, xi) h), where d (\u00b7, \u00b7) is an arbitrary suitable distance function. Here, we used the same distance d (\u00b7, \u00b7) used in Equation (1), but they can generally be different. For \"2- and\" 1 standards (i.e. RBF and laplac cores), the KDE (sub) gradients are each given by: \u2212 2 nh = i | fi = \u2212 1exp (\u2212 xi \u00b2 2h) (x \u2212 xi), \u2212 1 nh = i | fi = \u2212 1exp (\u2212 x \u2212 xi). Therefore, note that the scaling factor here is proportional to O (1nh)."}, {"heading": "4.2.3 Gradient Descent Attack in Discrete Spaces", "text": "In such cases, we need to find viable neighbors x that cause a steep descent, i.e. a maximum reduction of E (x). A simple approach to this problem is to probe E at any point in a small neighborhood of x: x. However, this approach requires a large number of queries. For classifiers with a differentiable decision function, we can instead use the neighbor whose difference to x best matches E (x); i.e. the update is done with x (z \u2212 x). Therefore, the solution for the above alignment is simply to modify a feature that meets argmaxi (x) i, for which the corresponding change leads to a viable state."}, {"heading": "4.3 Experiments", "text": "In this section, we will first report on some experimental results on the MNIST task of handwritten digit classification [32, 45], which visually illustrate how the proposed algorithm modifies digits to mislead the classification. This dataset is particularly useful because the visual nature of handwritten digit data provides a semantic meaning for attacks, and then we will show the effectiveness of the proposed attack on a more realistic and practical scenario: detection of malware in PDF files."}, {"heading": "4.3.1 Handwritten Digits", "text": "We will first focus on a two-class sub-problem of distinguishing between two different digits from the MNIST dataset 2525. Each digit example is presented as a grayscale image of 28 x 28 pixels arranged in raster scan order to give feature vectors of d = 28 x 28 = 784 values. We normalize each feature (pixels) x f [0,1] d by dividing its value by 255, and we limit the attack samples to this range. Accordingly, we optimize Equation (1) below 0 \u2264 x f \u2264 1 for all f. For our attacker, we assume that the perfect knowledge (PK) attack scenario. We used the Manhattan distance (\"1 standard) as removal function, d, both for the kernel density estimator (i.e., a laplacian kernel value) and for the constraint (x, x0)."}, {"heading": "4.3.2 Malware Detection in PDF Files", "text": "We will focus on the problem of distinguishing between legitimate and malicious PDFs, a popular malware distribution medium [63]. PDFs are excellent vectors for malicious code, due to their flexible logical structure, which can be described by a hierarchy of connected objects. As a result, an attack can easily be hidden in a PDF to bypass file type filtering. Furthermore, the PDF format allows a variety of resources that can be embedded in the document, including JavaScript, Flash, and even binary programs. The type of the embedded object is specified by keywords, and its contents are in a data stream. Several recent works allow machine learning techniques to detect malicious PDFs that use the logical structure of the file to accurately identify the malware. In this case study, we will use the function representation of Maiorca et al. [49], in which each feature corresponds to the tally of a given key in the PDFs."}, {"heading": "4.4 Discussion", "text": "In this section, we proposed a simple algorithm to bypass SVMs with differentiated nuclei, and, more generally, any classification with a differentiated discrimination function. We investigated the effectiveness of the attack in case of perfect knowledge of the attacked system. Furthermore, we showed that SVMs can still be circumvented with a high probability, even if the opponent can only learn a copy of the replacement data. We believe that the proposed attack formulation can easily be extended to classifiers with non-differentiated discriminatory functions, such as decision trees and k-nearest neighborhoods. Our analysis suggests that the improvement of classification security is to be expected."}, {"heading": "5 Poisoning Attacks against SVMs", "text": "In the previous section, we have developed a simple algorithm that allows the evasion of classifiers during the test period and experimentally demonstrated how to exploit them to avoid detection by SVMs and nuclear-based classification techniques. At this point, we present a different type of attack based on our work in [14]. Its goal is to force the attacked SVM to manipulate as many samples as possible during the test period by poisoning the training data, i.e. by injecting well-designed attack samples into the training set. Note that in this case, the test data is assumed not to be manipulated by the attacker. Poisoning attacks are staged during the classification training, and can thus target adaptive or online classifiers as well as classifiers that are retrained during the test period."}, {"heading": "5.1 Modeling the Adversary", "text": "As with the evasive attacks in Section 4.1, we model the attack scenario and derive the corresponding optimal attack strategy for poisoning.28 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, RoliNotation. In the following, we assume that an SVM was trained on a dataset Dtr = {xi, yi} ni = 1 with xi-Rd and yi-Rd {\u2212 1, + 1}. The matrix of kernel values between two sets of points is designated K, while Q = K-yy > denotes its labeled version, and \u03b1 denotes the dual variables of the SVM corresponding to each training point. Depending on the value of \u03b1i, the training points are designated as margin support vectors (0 < \u03b1i < C, set S), error support vectors (\u03b1i = C, set E or reserve) corresponding to the QR index, corresponding to the lower matrix values."}, {"heading": "5.1.1 Adversary\u2019s Goal", "text": "In a poisoning attack, the target of the attacker is to find a series of points, the addition of which to Dtr reduces the classification accuracy of the SVM to a maximum. For the sake of simplicity, we begin to consider the addition of a single point of attack (x *, y *). The choice of the designation y * is arbitrary, but fixed. We designate the class of this chosen designation as the attacking class and the other as the attacked class."}, {"heading": "5.1.2 Adversary\u2019s Knowledge", "text": "According to Section 3.1.2, we assume that the adversary knows the training samples (k.i), the attribute representation (k.ii), an SVM learning algorithm (k.iii), and the learned SVM parameters (k.iv), since they can be derived by the adversary by solving the SVM learning problem on the known training set. Finally, we assume that no feedback from the adversary (k.v) will be exploited. These assumptions amount to considering a worst-case analysis that allows us to calculate the maximum error rate that the adversary can cause by poisoning. In fact, this is useful to verify whether and under what circumstances poisoning can be a relevant threat to SVM. Although in practice it is very difficult for an adversary to have perfect knowledge of the training data, collecting a surrogate data set from the same distribution may not be so complicated; for example, the detection of a network packet can be easily infiltrated by an attacker."}, {"heading": "5.1.3 Adversary\u2019s Capability", "text": "According to Section 3.1.3, we assume that the attacker can only manipulate training data (c.i), that he can manipulate the class before and the class-related distribution of class y * of the attack point by essentially inserting a number of attack points of this class in sequence into the training data (c.ii-iii), and that he can change the characteristic values of the security assessment of SVMs in adverse environments (c.iv). Specifically, we will restrict the attack point to being within a box, i.e. xlb \u2264 x \u2264 xub."}, {"heading": "5.1.4 Attack Strategy", "text": "Under the above assumptions, the optimal attack strategy boils down to solving the following optimization problem: x \u0445 = argmaxx P (x) = \u2211 mk = 1 (1 \u2212 yk fx (xk)) + = m \u2211 k = 1 (\u2212 gk) + (2) s.t. xlb \u2264 x \u2264 xub, (3) whereby the hinge loss must be maximized on a separate validation quantity Dval = {xk, yk} mk = 1 in order to avoid the consideration of another regularization concept in the objective function, because the attacker aims to maximize the SVM generalization error and not only his empirical assessment of the training data."}, {"heading": "5.2 Poisoning Attack Algorithm", "text": "In this section, we assume the role of the attacker and develop a method to optimize x * according to Equation (2). Since the objective function is not linear, we use a gradient ascension algorithm, in which the attack vector is initialized by cloning any point from the attacked class and its label is reversed. This initialized attack point (with iteration 0) is characterized by x0. In principle, x0 can be any point sufficiently deep within the scope of the attacking class. However, if this point is too close to the boundary of the attacking class, the iteratively adjusted attack point can develop into a reserve point that stops further progress. Calculation of the gradient of the validation error depends crucially on the assumption that the structure of the sentences S, E, and R will not change during the update. In general, it is difficult to determine the largest step t along the gradient direction p that preserves this structure."}, {"heading": "5.2.1 Gradient Computation", "text": "We will now discuss how to calculate the course of our objective function. - > Convenience: We will now refer to the target point as xc instead of x.30 Biggio, Corona, Nelson, Rubinstein, Fumera, Giacinto, Rolio, Rolio, clue against an SVM input: Dtr, the training data; y, the validation data; y, the class label of the attack point; x0, the starting situation; t, the step size. Output: x, the final target point. 1: {3} The validation of SVM on Dtr. 2: p, the class label of the SVM solution on Dtr."}, {"heading": "5.2.2 Kernelization", "text": "Equation (8) shows that the gradient of the objective function in iteration p is determined only by the gradients of the matrix Q. In particular, this depends on the chosen core. In the following, we report on the expressions of these gradients for three common nuclei (see section 4.2.1): \u2022 Linear core: \u2202 Kic \u2202 xc = \u2202 (xi \u00b7 xc) \u2202 xc = xi \u2022 Polynomial core: \u2202 Kic \u2202 xc = \u2202 (xi \u00b7 xc + R) d-xc (xi \u00b7 xc + R) d-1xi \u2022 RBF core: \u2202 Kic \u2202 xc = \u0445 e-xi \u2212 xi \u2212 xc | | 2 Kong xc = 2\u03b3K (xi, xc) (xi \u2212 xc) (xi \u2212 xc) The dependence of these gradients on the current target point, xc, can be avoided by using the previous target point, which our method is sufficiently harmless."}, {"heading": "5.3 Experiments", "text": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST dataset for handwritten number recognition [32, 45]."}, {"heading": "5.3.1 Two-dimensional Toy Example", "text": "Here we consider a two-dimensional example in which each class follows a Gauss with mean and covariance matrices indicated by \u00b5 \u2212 = [\u2212 1,5,0], \u00b5 + = [1,5,0], \u03a3 \u2212 = \u03a3 + = 0,6I. The points from the negative distribution have the designation \u2212 1 (shown in the following figures as red) and otherwise + 1 (shown as blue). Training and validation sets, Dtr and Dval, consist of 25 and 500 points per class, respectively. In the experiment presented below, the red class is the attacking class. That is, a random point of the blue class is selected and its designation is inverted to serve as the starting point for our method. Our gradient ascent method is then used to refute this attack until its termination condition is met. The trajectory of the attack is traced as the black line in Figure 7 for the two linear cores (upper two degrees) and the RF core (bottom two degrees)."}, {"heading": "5.3.2 Handwritten Digits", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.4 Discussion", "text": "The intoxication attack presented in this section, summarized from our previous work in [14], is a first step towards a security analysis of the SVM against training data attacks. Although our ascent method is not optimal, it has a surprisingly large impact on the accuracy of the SVM classification. Several potential improvements to the presented method still need to be explored in future work. For example, one can investigate the effectiveness of such an attack with replacement data, that is, if the training data is not known to the opponent, who can collect samples from the same distribution to learn the copy of a classifier (similar to the limited knowledge considered in the identification attacks of Section 4). Another improvement could be to consider the simultaneous optimization of multi-point attacks, although we have already shown that greedy, sequential single point attacks can be quite successful. An interesting analysis of the vulnerability of the SVM to intoxications proposed from this work is to consider the effects of the attack on other functions besides the loss of modification."}, {"heading": "6 Privacy Attacks against SVMs", "text": "We now consider a third scenario in which the attacker's goal is to cause a breach of the confidentiality of the training data. In contrast to earlier sections, our focus here is primarily on the exploration of countermeasures, while we consider attacks only briefly in the context of the security assessment of SVMs in adversarial environments 37 of lower bounds. We adopt the formal framework of Dwork et al. [30], in which a randomized mechanism \u03b2 -differential privacy is intended to preserve when the likelihood of the mechanism's production performance changes by at most \u03b2 when a training date is arbitrarily changed (or even removed). The power of this framework, which has gained near-universal favor since its introduction, is that it rigorously quantifies privacy and provides strong guarantees, even against powerful adversaries with knowledge of almost all training data, knowledge of the mechanism (excluding its source of randomness), access to the data, classification mechanisms, making access to the results virtually arbitrary."}, {"heading": "6.1 Modeling the Adversary", "text": "First, we apply our framework to define the threat model to defend against attacks on privacy in the broader context of differentiated privacy, and then we focus on specific countermeasures in the form of modifications to SVM learning that ensure differentiated privacy."}, {"heading": "6.1.1 Adversary\u2019s Goal", "text": "The ultimate objective of the attacker in this section is to determine the characteristics and / or designation of an individual training date. The general approach of the opponent towards this goal is to inspect (any number of) test time classifications created by a published classifier trained on the data, or to inspect the classifier directly. The definition of differential privacy and the special mechanisms derived from it can be modified for related targets to determine the characteristics of multiple training dates; we focus on the above conventional case without losing universality."}, {"heading": "6.1.2 Adversary\u2019s Knowledge", "text": "As indicated above, we provide our adversary with significant knowledge of the learning system in order to derive countermeasures that can withstand very strong attacks. In fact, the concept of differential privacy, as opposed to more syntactic notions of privacy such as k-anonymity [64], was inspired by decades of work in cryptography that introduced mathematical formalism to an age-old problem and brought 38 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Rolisignificant practical success. Specifically, we consider a scenario in which the adversary has complete knowledge of the raw input characteristics representation, the learning algorithm (of the entire mechanism including the form of randomization that he introduces, although not the source of randomness), and the form of its decision-making function (in this case a limited SVM), in which the learned classifiers have the parameters (the kernel feature mapping, the primary vector, and the notion of the SVM) and the unrestricted (in this case) of the classification of the data (in which the SVM)."}, {"heading": "6.1.3 Adversary\u2019s Capability", "text": "Like our assumptions about the knowledge of the attacker, we place weak limits on the ability of the opponent. We assume an opponent who can manipulate both training and test data (c.i), although the latter may be subsumed by the attacker's complete knowledge of the decision function and the parameters learned - for example, he may implement his own classifier and execute it at will, or he may submit or manipulate test points submitted to an assigned classifier. Our attack model does not make assumptions about the origin of the training or test data. The data does not need to be independently or even scanned according to a distribution - the definition of differential privacy given below allows for worst-case assumptions about the training data, and even the test data may be arbitrary. Thus, the opponent may have arbitrary abilities to modify class priorities, training data characteristics and labels (c.ii-iv), except that the opponent attacking the system does not directly modify the target data because it is not a personal knowledge (that it is not a personal one)."}, {"heading": "6.1.4 Attack Strategy", "text": "While no practical data protection attacks on SVM have been investigated in the past - an open problem discussed in Section 6.3 - a general approach would be to approximate the reversal of the learning map on the released SVM parameterization (either primary weight vector or dual variables) around the known part of the training data. In practice, this could be achieved by adopting a similar approach to Section 5, which repeats a first guess of a missing training point by performing gradient steps of the differential in the SVM parameter vector with respect to the safety assessment of SVMs in adverse environments 39 missing training data. One interpretation of this approach is a simulation: To guess a missing training date that provides access to the remainder of the training set and SVM solution on all data, the SVM is simulated for guesses of the missing date and updates the guesses in directions that the theoretical interim solutions will appropriately address later in the case."}, {"heading": "6.2 Countermeasures with Provable Guarantees", "text": "Faced with an adversary with such strong knowledge and capabilities as described above, it may seem difficult to take effective countermeasures, especially given the complication of abundant access to page information, which is often used in public attacks on privacy [51, 64]. However, the crux that allows privacy to be preserved under these conditions lies in the fact that the amount released is an aggregate statistic of sensitive data; intuitively, the more data aggregated, the less sensitive a statistic should be to changes or the removal of a single date. We now present results of our recent work quantifying this effect [59] within the framework of a differential privacy."}, {"heading": "6.2.1 Background on Differential Privacy", "text": "We start with the return to the keyterm definition that is Dwork et al. (30) We begin with the return to the keyterm definition that is Dwork et al. (30) We begin with the return to the keyterm definition. (30) We begin with the return to the keyterm definition that is Dwork et al. (30) We begin with the return to the keyterm definition that is Dwork et al. (30) We begin with the return to the keyterm definition that is Dwork et al. (30)"}, {"heading": "6.2.2 Global Sensitivity of Linear SVM", "text": "Unlike many previous papers, which applied the \"Laplace mechanism\" to the achievement of a differential privacy, in which assessors studied are often dissected as linear functions of 7. Remember that the zero-mean multivariable Laplace distribution with scale parameters s has a density proportional to exp (\u2212 xxx1 / s). Safety evaluation of SVMs in adversarial environments 41 Algorithm 3 Privacy preserving SVM input: D the training data; C > 0 soft-margin parameters; Kernel, which produces a feature space with finite dimension F and difficult L2 standard; Privacy parameter \u03b2 > 0 Output: learned weight vector etc."}, {"heading": "6.2.3 Differentially-Private SVMs", "text": "So far, we have found that algorithm 3, which learns an SVM and returns the resulting weight vector with additional Laplace noise, preserves \u03b2-differential privacy. More noise is added to the weight vector if either (i) a higher level of privacy is desired (smaller \u03b2), (ii) the SVM fits closer to the data (higher C) or (iii) the data is more distinguishable (higher Q or F - the curse of dimensionality). Hidden in the above approach is the dependence on n: Typically, we take C like 1 / n to achieve consistency, in which case we see noise decreasing with larger training data - comparable to less individual influence - as expected [59].Problem in the above approach is the destruction of serviceability due to the preservation of differential privacy. An approach to quantify this effect involves the notion that we maintain the usefulness of 59."}, {"heading": "6.3 Discussion", "text": "In this section, we have summarized our recent findings on strong countermeasures against privacy attacks on the SVM. We have shown how, through controlled addition of noise, SVM learning in end-dimensional attribute spaces can create both privacy and usage guarantees. These results can be extended to certain translation-invariant cores, including the infinite RBF [59]. This extension adopts a large-scale learning technique in which a dual solution of SVM for large training data sizes is not feasible. Instead, a primary SVM problem is solved with a random kernel that fully approximates the desired kernel."}, {"heading": "7 Concluding Remarks", "text": "In fact, it is the case that one is able to find a solution that enables one to find a solution that adapts to the needs of the people."}], "references": [{"title": "Proceedings of the 1st ACM Workshop on AISec (AISec)", "author": ["D. Balfanz", "Staddon", "J. (eds."], "venue": "ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Proceedings of the 2nd ACM Workshop on Security and Artificial Intelligence (AISec)", "author": ["D. Balfanz", "Staddon", "J. (eds."], "venue": "ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A. Joseph", "J. Tygar"], "venue": "Machine Learning 81, 121\u2013148", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Can machine learning be secure? In: ASIACCS \u201906: Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, pp", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "16\u201325. ACM, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A learning-based approach to reactive security", "author": ["A. Barth", "B.I.P. Rubinstein", "M. Sundararajan", "J.C. Mitchell", "D. Song", "P.L. Bartlett"], "venue": "IEEE Transactions on Dependable and Secure Computing 9(4), 482\u2013493", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial environments", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "the 10th International Workshop on Multiple Classifier Systems (MCS), Lecture Notes in Computer Science, vol. 6713, pp. 350\u2013359. Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "H. Blockeel et al. (ed.) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Part III, Lecture Notes in Artificial Intelligence, vol. 8190, pp. 387\u2013402. Springer-Verlag Berlin Heidelberg", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Poisoning attacks to compromise face templates", "author": ["B. Biggio", "L. Didaci", "G. Fumera", "F. Roli"], "venue": "the 6th IAPR International Conference on Biometrics (ICB)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey and experimental evaluation of image spam filtering techniques", "author": ["B. Biggio", "G. Fumera", "I. Pillai", "F. Roli"], "venue": "Pattern Recognition Letters 32(10), 1436 \u2013 1446", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics (SMC), pp. 977\u2013982", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering 99(PrePrints), 1", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Poisoning adaptive biometric systems", "author": ["B. Biggio", "G. Fumera", "F. Roli", "L. Didaci"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition, Lecture Notes in Computer Science, vol. 7626, pp. 417\u2013425", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Practical privacy: the SuLQ framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "Proceedings of the 24th Symposium on Principles of Database Systems, pp. 128\u2013138", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning 24(2), 123\u2013140", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "Journal of Machine Learning Research 13, 2617\u20132654", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of classifiers: Practical considerations for security applications", "author": ["A.A. C\u00e1rdenas", "J.S. Baras"], "venue": "AAAI Workshop on Evaluation Methods for Machine Learning", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "The 5th ACM Workshop on Artificial Intelligence and Security (AISec)", "author": ["A.A. C\u00e1rdenas", "B. Nelson", "Rubinstein", "B.I. (eds."], "venue": "ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental and decremental support vector machine learning", "author": ["G. Cauwenberghs", "T. Poggio"], "venue": "T.K. Leen, T.G. Dietterich, V. Tresp (eds.) NIPS, pp. 409\u2013415. MIT Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research 12(Mar), 1069\u20131109", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence (AISec)", "author": ["Y. Chen", "A.A. C\u00e1rdenas", "R. Greenstadt", "Rubinstein", "B. (eds."], "venue": "ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "On robust properties of convex risk minimization methods for pattern recognition", "author": ["A. Christmann", "I. Steinwart"], "venue": "Journal of Machine Learning Research 5, 1007\u20131034", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Adversarialib: a general-purpose library for the automatic evaluation of machine learning-based classifiers under adversarial attacks", "author": ["I. Corona", "B. Biggio", "D. Maiorca"], "venue": "URL http:// sourceforge.net/projects/adversarialib/", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning 20, 273\u2013297", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 99\u2013108", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Transactions on Neural Networks 10(5), 1048\u20131054", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Interscience", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "Proceedings of the 3rd Theory of Cryptography Conference (TCC 2006), pp. 265\u2013284", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proceedings of the 15th Conference on USENIX Security Symposium", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pp. 353\u2013360", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Discriminative direction for kernel classifiers", "author": ["P. Golland"], "venue": "Neural Information Processing Systems (NIPS), pp. 745\u2013752", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Proceedings of the 3rd ACM Workshop on Artificial Intelligence and Security (AISec)", "author": ["Greenstadt", "R. (ed."], "venue": "ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust Statistics: The Approach Based on Influence Functions", "author": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"], "venue": "Probability and Mathematical Statistics. John Wiley and Sons, New York, NY, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1986}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "Proceedings of the 4th ACM Workshop on Artificial Intelligence and Security (AISec), pp. 43\u201357", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 405\u2013412", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Security analysis of online centroid anomaly detection", "author": ["M. Kloft", "P. Laskov"], "venue": "Journal of Machine Learning Research 13, 3647\u20133690", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "Proceedings of the 6th Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "Proceedings of the 2nd ACM Workshop on Security and Artificial Intelligence, pp. 1\u20134", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning in adversarial environments", "author": ["P. Laskov", "R. Lippmann"], "venue": "Machine Learning 81, 115\u2013119", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. M\u00fcller", "E. S\u00e4ckinger", "P. Simard", "V. Vapnik"], "venue": "International Conference on Artificial Neural Networks, pp. 53\u201360", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 641\u2013647", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the 2nd Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Handbook of matrices", "author": ["H. L\u00fctkepohl"], "venue": "John Wiley & Sons", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1996}, {"title": "A pattern recognition system for malicious PDF files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "MLDM, pp. 510\u2013524", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust Statistics: Theory and Methods", "author": ["R.A. Maronna", "R.D. Martin", "V.J. Yohai"], "venue": "Probability and Mathematical Statistics. John Wiley and Sons, New York, NY, USA", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "Proceedings of the 29th IEEE Symposium on Security and Privacy, pp. 111\u2013125", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Proceedings of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats, pp. 1\u20139", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J.D. Tygar"], "venue": "Journal of Machine Learning Research 13, 1293\u20131332", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Using an ensemble of one-class SVM classifiers to harden payload-based anomaly detection systems", "author": ["R. Perdisci", "G. Gu", "W. Lee"], "venue": "Proceedings of the International Conference on Data Mining (ICDM), pp. 488\u2013498", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers, pp. 61\u201374", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2000}, {"title": "What-if analysis", "author": ["S. Rizzi"], "venue": "Encyclopedia of Database Systems pp. 3525\u20133529", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["R.N. Rodrigues", "L.L. Ling", "V. Govindaraju"], "venue": "Journal of Visual Languages and Computing 20(3), 169\u2013179", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "Antidote: understanding and defending against poisoning of anomaly detectors", "author": ["B.I. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "Lau", "S.h.", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "Proceedings of the 9th Conference on Internet Measurement Conference (IMC), pp. 1\u201314", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["B.I.P. Rubinstein", "P.L. Bartlett", "L. Huang", "N. Taft"], "venue": "Journal of Privacy and Confidentiality 4(1), 65\u2013100", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "Computational Learning Theory, Lecture Notes in Computer Science, vol. 2111, pp. 416\u2013426", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT Press", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2001}, {"title": "Malicious PDF detection using metadata and structural features", "author": ["C. Smutz", "A. Stavrou"], "venue": "Proceedings of the 28th Annual Computer Security Applications Conference, pp. 239\u2013248", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Detection of malicious PDF files based on hierarchical document structure", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "Proceedings of the 20th Annual Network & Distributed System Security Symposium (NDSS)", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2013}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal on Uncertainty, Fuzziness and Knowledge-based Systems 10(5), 557\u2013570", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2002}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag, Inc.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1995}, {"title": "On attacking statistical spam filters", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "Proceedings of the 1st Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2004}, {"title": "2010 IBM x-force mid-year trend & risk report", "author": ["R. Young"], "venue": "Tech. rep., IBM", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Support Vector Machines (SVMs) are among the most successful techniques that have been applied for this purpose [28, 54].", "startOffset": 112, "endOffset": 120}, {"referenceID": 48, "context": "Support Vector Machines (SVMs) are among the most successful techniques that have been applied for this purpose [28, 54].", "startOffset": 112, "endOffset": 120}, {"referenceID": 38, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 350, "endOffset": 354}, {"referenceID": 0, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 1, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 31, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 20, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 17, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 10, "context": "In Section 3, we summarize our recently defined framework for the empirical evaluation of classifiers\u2019 security [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "We discuss our recently devised evasion attacks against SVMs [8] in Section 4, and review and extend our recent work [14] on poisoning attacks against SVMs in Section 5.", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "We discuss our recently devised evasion attacks against SVMs [8] in Section 4, and review and extend our recent work [14] on poisoning attacks against SVMs in Section 5.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "In order to support the reproducibility of our experiments, we published all the code and the data employed for the experimental evaluations described in this paper [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 59, "context": "While many hyperplanes may suffice for this task, the SVM hyperplane both separates the training samples of the two classes and provides a maximum distance from itself to the nearest training point (this distance is called the classifier\u2019s margin), since maximum-margin learning generally reduces generalization error [65].", "startOffset": 318, "endOffset": 322}, {"referenceID": 23, "context": "Although originally designed for linearly-separable classification tasks (hard-margin SVMs), SVMs were extended to non-linearly-separable classification problems by Vapnik [25] (soft-margin SVMs), which allow some samples to violate the margin.", "startOffset": 172, "endOffset": 176}, {"referenceID": 54, "context": "1 This is an instance of the Representer Theorem which states that solutions to a large class of regularized ERM problems lie in the span of the training data [60].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 3, "context": "[5]: can machine learning be secure? At the center of this question is the effect an adversary can have on a learner by violating the stationarity assumption that the training data used to train the classifier comes from the same distribution as the test data that will be classified by the learned classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Further, as in most security tasks, predicting how the data distribution will change is difficult, if not impossible [12, 36].", "startOffset": 117, "endOffset": 125}, {"referenceID": 33, "context": "Further, as in most security tasks, predicting how the data distribution will change is difficult, if not impossible [12, 36].", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "Hence, adversarial learning problems are often addressed as a proactive arms race [12], in which the classifier designer tries to anticipate the next adversary\u2019s move, by simulating and hypothesizing proper attack scenarios, as discussed in the next section.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "Under this setting, the arms race can be modeled as the following cycle [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "1 A conceptual representation of the reactive arms race [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "2), it has only recently been formalized within a more general framework for the empirical evaluation of a classifier\u2019s security [12], which we summarize in Section 3.", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "2 Although in certain abstract models we have shown how regret-minimizing online learning can be used to define reactive approaches that are competitive with proactive security [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "2 A conceptual representation of the proactive arms race [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 3, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 16, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 33, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 35, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 36, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 37, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 40, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 24, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 36, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 51, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": "Although some prior work does address aspects of the empirical evaluation of classifier security, which is often implicitly defined as the performance degradation incurred under a (simulated) attack, to our knowledge a systematic treatment of this process under a unifying perspective was only first described in our recent work [12].", "startOffset": 329, "endOffset": 333}, {"referenceID": 24, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 36, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 41, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 60, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 36, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 41, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 60, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 10, "context": "This shortcoming highlights the need for a more general set of security guidelines and a more systematic definition of classifier security evaluation, that we began to address in [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 2, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 15, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 24, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 33, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 35, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 37, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 40, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 47, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 2, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 33, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 34, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 35, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 37, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 10, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 3, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 33, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 12, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 34, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 35, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 46, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 52, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 24, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 28, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 36, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 41, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 60, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 2, "context": "[4] and here we outline these with respect to a PDF malware detector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Here, we summarize our recent work [12] that proposes a new framework for designing proactive secure classifiers by addressing the shortcomings of the reactive security cycle raised above.", "startOffset": 35, "endOffset": 39}, {"referenceID": 50, "context": "This amounts to performing a more systematic what-if analysis of classifier security [56].", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 33, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "[5] and extended by Huang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[36], the adversary\u2019s goal should be defined based on the anticipated security violation, which might be an integrity, availability, or privacy violation (see Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Further, as suggested by Laskov and Kloft [42] and Kloft and Laskov [40], the adversary\u2019s goal should be defined in terms of an objective function that the adversary is willing to maximize.", "startOffset": 42, "endOffset": 46}, {"referenceID": 35, "context": "Further, as suggested by Laskov and Kloft [42] and Kloft and Laskov [40], the adversary\u2019s goal should be defined in terms of an objective function that the adversary is willing to maximize.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 40, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 47, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 46, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 390, "endOffset": 398}, {"referenceID": 12, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 390, "endOffset": 398}, {"referenceID": 26, "context": "The adversary\u2019s knowledge of the attacked system can be defined based on the main components involved in the design of a machine learning system, as described in [29] and depicted in Figure 3.", "startOffset": 162, "endOffset": 166}, {"referenceID": 26, "context": "3 A representation of the design steps of a machine learning system [29] that may provide sources of knowledge for the adversary.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "Although potentially too pessimistic, this worst-case setting allows one to compute a lower bound on the classifier performance when it is under attack [26, 41].", "startOffset": 152, "endOffset": 160}, {"referenceID": 36, "context": "Although potentially too pessimistic, this worst-case setting allows one to compute a lower bound on the classifier performance when it is under attack [26, 41].", "startOffset": 152, "endOffset": 160}, {"referenceID": 40, "context": "v), either to directly find optimal or nearly-optimal attack instances [46, 53], or to learn a surrogate classifier, which can then serve as a template to guide the attack against the actual classifier.", "startOffset": 71, "endOffset": 79}, {"referenceID": 47, "context": "v), either to directly find optimal or nearly-optimal attack instances [46, 53], or to learn a surrogate classifier, which can then serve as a template to guide the attack against the actual classifier.", "startOffset": 71, "endOffset": 79}, {"referenceID": 2, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 33, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 2, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 3, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 33, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 12, "context": "[14] the attack samples had to be injected into the training data, and each attack sample depended on the current training data, which also included past attack samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "3 See [12] for more details on the definition of the data distribution and the resampling algorithm.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 3, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 33, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 24, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 40, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 47, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 205, "endOffset": 209}, {"referenceID": 6, "context": "In contrast to this prior work, the methods presented in our recent work [8] and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gradient-descent-based approach derived from Golland\u2019s technique of discriminative directions [33].", "startOffset": 73, "endOffset": 76}, {"referenceID": 30, "context": "In contrast to this prior work, the methods presented in our recent work [8] and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gradient-descent-based approach derived from Golland\u2019s technique of discriminative directions [33].", "startOffset": 296, "endOffset": 300}, {"referenceID": 24, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 40, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 47, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 24, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 40, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 46, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 47, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 49, "context": ", for neural networks, since they directly output a posterior estimate, and for SVMs, since their posterior can be estimated as a sigmoidal function of the distance of x to the SVM hyperplane [55].", "startOffset": 192, "endOffset": 196}, {"referenceID": 28, "context": "The extra component favors attack points to imitate features of known samples classified as legitimate, as in mimicry attacks [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "In this section, we first report some experimental results on the MNIST handwritten digit classification task [32, 45], that visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 110, "endOffset": 118}, {"referenceID": 39, "context": "In this section, we first report some experimental results on the MNIST handwritten digit classification task [32, 45], that visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 110, "endOffset": 118}, {"referenceID": 39, "context": "We first focus on a two-class sub-problem of discriminating between two distinct digits from the MNIST dataset [45].", "startOffset": 111, "endOffset": 115}, {"referenceID": 61, "context": "We focus now on the problem of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [67].", "startOffset": 133, "endOffset": 137}, {"referenceID": 43, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 56, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 57, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 43, "context": "[49] in which each feature corresponds to the tally of occurrences of a given keyword in the PDF file.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Similar feature representations were also exploited in [62, 63].", "startOffset": 55, "endOffset": 63}, {"referenceID": 57, "context": "Similar feature representations were also exploited in [62, 63].", "startOffset": 55, "endOffset": 63}, {"referenceID": 43, "context": "The features (keywords) were extracted from each training set as described in [49]; on average, 100 keywords were found in each run.", "startOffset": 78, "endOffset": 82}, {"referenceID": 44, "context": "We report our results in Figure 6, in terms of the false negative (FN) rate attained by the targeted classifiers as a function of the maximum allowable number of modifications, dmax \u2208 [0,50].", "startOffset": 184, "endOffset": 190}, {"referenceID": 9, "context": "Generative classifiers can be modified, by explicitly modeling the attack distribution, as in [11], and discriminative classifiers can be modified similarly by adding generated attack samples to the training set.", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": ", n-gram analysis [31].", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "A similar technique has been already exploited in to address the pre-image problem of kernel methods [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "Other interesting extensions include (i) considering more effective strategies such as those proposed by [46, 53] to build a small but representative set of surrogate data to learn the surrogate classifier and (ii) improving the classifier estimate \u011d(x); e.", "startOffset": 105, "endOffset": 113}, {"referenceID": 47, "context": "Other interesting extensions include (i) considering more effective strategies such as those proposed by [46, 53] to build a small but representative set of surrogate data to learn the surrogate classifier and (ii) improving the classifier estimate \u011d(x); e.", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "using an ensemble technique like bagging to average several classifiers [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "Here we present another kind of attack, based on our work in [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 7, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 35, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 46, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 52, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 35, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 52, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 5, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 46, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 11, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 18, "context": "After each update of the attack point xp, the optimal solution can be efficiently recomputed from the solution on Dtr, using the incremental SVM machinery [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "3: repeat 4: Re-compute the SVM solution on Dtr\u222a{x,y} using the incremental SVM [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "This can expressed as an adiabatic update condition using the technique introduced in [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 42, "context": "Solving these equations and computing an inverse matrix via the Sherman-MorrisonWoodbury formula [48] yields the following gradients:", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset [32, 45].", "startOffset": 247, "endOffset": 255}, {"referenceID": 39, "context": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset [32, 45].", "startOffset": 247, "endOffset": 255}, {"referenceID": 29, "context": "We now quantitatively validate the effectiveness of the proposed attack strategy on the MNIST handwritten digit classification task [32, 45], as with the evasion attacks in Section 4.", "startOffset": 132, "endOffset": 140}, {"referenceID": 39, "context": "We now quantitatively validate the effectiveness of the proposed attack strategy on the MNIST handwritten digit classification task [32, 45], as with the evasion attacks in Section 4.", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "The poisoning attack presented in this section, summarized from our previous work in [14], is a first step toward the security analysis of SVM against training data attacks.", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "We review our recent work [59] deriving mechanisms for releasing SVM classifiers trained on privacy-sensitive data while maintaining the data\u2019s privacy.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[30], in which a randomized mechanism is said to preserve \u03b2 -differential privacy, if the likelihood of the mechanism\u2019s output changes by at most \u03b2 when a training datum is changed arbitrarily (or even removed).", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "Indeed the notion of differential privacy, as opposed to more syntactic notions of privacy such as k-anonymity [64], was inspired by decades-old work in cryptography that introduced mathematical formalism to an age-old problem, yielding", "startOffset": 111, "endOffset": 115}, {"referenceID": 45, "context": "Given an adversary with such strong knowledge and capabilities as described above, it may seem difficult to provide effective countermeasures particularly considering the complication of abundant access to side information that is often used in publicized privacy attacks [51, 64].", "startOffset": 272, "endOffset": 280}, {"referenceID": 58, "context": "Given an adversary with such strong knowledge and capabilities as described above, it may seem difficult to provide effective countermeasures particularly considering the complication of abundant access to side information that is often used in publicized privacy attacks [51, 64].", "startOffset": 272, "endOffset": 280}, {"referenceID": 53, "context": "We now present results from our recent work that quantifies this effect [59], within the framework of differential privacy.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "7 The well-established proof technique [30] follows from the definition of the Laplace distribution involving the same norm as used in our measure of global sensitivity, and the triangle inequality: for any training set D , D \u2032 \u223cD , response g \u2208H , and privacy parameter \u03b2", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "data [15], measuring the sensitivity of the SVM appears to be non-trivial owing to the non-linear influence an individual training datum may have on the learned SVM.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": ", the VC dimension, which is not always possible to control, and for the RBF kernel SVM is infinite) [61].", "startOffset": 101, "endOffset": 105}, {"referenceID": 53, "context": "In recent work [59], we showed how these existing stability measurements for the SVM can be adapted to provide the following L1-global sensitivity bound.", "startOffset": 15, "endOffset": 19}, {"referenceID": 53, "context": "We omit the proof, which is available in the original paper [59] and which follows closely the previous measurements for algorithmic stability.", "startOffset": 60, "endOffset": 64}, {"referenceID": 53, "context": "Hidden in the above is the dependence on n: typically we take C to scale like 1/n to achieve consistency in which case we see that noise decreases with larger training data\u2014 akin to less individual influence\u2014as expected [59].", "startOffset": 220, "endOffset": 224}, {"referenceID": 53, "context": "One approach to quantifying this effect, involves bounding the following notion of utility [59].", "startOffset": 91, "endOffset": 95}, {"referenceID": 53, "context": ", PAC learns) [59].", "startOffset": 14, "endOffset": 18}, {"referenceID": 53, "context": "Using the Chernoff tail inequality and known moment-generating functions, we establish the following bound on the utility of this private SVM [59].", "startOffset": 142, "endOffset": 146}, {"referenceID": 53, "context": "These results can be extended to certain translation-invariant kernels including the infinite-dimensional RBF [59].", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 118, "endOffset": 126}, {"referenceID": 44, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 118, "endOffset": 126}], "year": 2014, "abstractText": "Support Vector Machines (SVMs) are among the most popular classification techniques adopted in security applications like malware detection, intrusion detection, and spam filtering. However, if SVMs are to be incorporated in real-world security systems, they must be able to cope with attack patterns that can either mislead the learning algorithm (poisoning), evade detection (evasion), or gain information about their internal parameters (privacy breaches). The main contributions of this chapter are twofold. First, we introduce a formal general framework for the empirical evaluation of the security of machine-learning systems. Second, according to our framework, we demonstrate the feasibility of evasion, poisoning and privacy attacks against SVMs in real-world security problems. For each attack technique, we evaluate its impact and discuss whether (and how) it can be countered through an adversary-aware design of SVMs. Our experiments are easily reproducible thanks to open-source code that we have made available, together with all the employed datasets, on a public repository. Battista Biggio, Igino Corona, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d\u2019Armi 09123, Cagliari, Italy. e-mail: {battista.biggio,igino.corona,davide.maiorca}@diee.unica.it e-mail: {fumera,giacinto,roli}@diee.unica.it Blaine Nelson Institut f\u00fcr Informatik, Universit\u00e4t Potsdam, August-Bebel-Stra\u00dfe 89, 14482 Potsdam, Germany. e-mail: blaine.nelson@gmail.com Benjamin I. P. Rubinstein IBM Research, Lvl 5 / 204 Lygon Street, Carlton, VIC 3053, Australia. e-mail: ben@bipr.net 1 ar X iv :1 40 1. 77 27 v1 [ cs .L G ] 3 0 Ja n 20 14 2 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli", "creator": "LaTeX with hyperref package"}}}