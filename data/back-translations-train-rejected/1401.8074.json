{"id": "1401.8074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2014", "title": "Empirically Evaluating Multiagent Learning Algorithms", "abstract": "There exist many algorithms for learning how to play repeated bimatrix games. Most of these algorithms are justified in terms of some sort of theoretical guarantee. On the other hand, little is known about the empirical performance of these algorithms. Most such claims in the literature are based on small experiments, which has hampered understanding as well as the development of new multiagent learning (MAL) algorithms. We have developed a new suite of tools for running multiagent experiments: the MultiAgent Learning Testbed (MALT). These tools are designed to facilitate larger and more comprehensive experiments by removing the need to build one-off experimental code. MALT also provides baseline implementations of many MAL algorithms, hopefully eliminating or reducing differences between algorithm implementations and increasing the reproducibility of results. Using this test suite, we ran an experiment unprecedented in size. We analyzed the results according to a variety of performance metrics including reward, maxmin distance, regret, and several notions of equilibrium convergence. We confirmed several pieces of conventional wisdom, but also discovered some surprising results. For example, we found that single-agent $Q$-learning outperformed many more complicated and more modern MAL algorithms.", "histories": [["v1", "Fri, 31 Jan 2014 07:02:58 GMT  (800kb)", "http://arxiv.org/abs/1401.8074v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["erik zawadzki", "asher lipson", "kevin leyton-brown"], "accepted": false, "id": "1401.8074"}, "pdf": {"name": "1401.8074.pdf", "metadata": {"source": "CRF", "title": "Empirically Evaluating Multiagent Learning Algorithms", "authors": ["Erik Zawadzki", "Asher Lipson", "Kevin Leyton-Brown"], "emails": ["kevinlb}@cs.ubc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 140 1,80 74v1 [cs.GT] 3 1Ja n20 14Keywords: game theory, multi-agent systems, amplification learning, empirical algorithms"}, {"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Algorithms and Past Experimental Work", "text": "MAL algorithms have been researched for over half a century, and this rich investigation has yielded not only a wealth of competing algorithms, but also various different formulations of problems. Does an algorithm know the game's reward functions before the game begins, or do reward functions need to be learned? How many opponents can face an algorithm? What signals about the opponent's actions can we observe in this essay? Can an algorithm rely on being able to determine the stage play's Nash equilibrium or other computer-consuming game features? Each of these assumptions changes the learning problem. In this section, we describe the algorithms we examine in this essay, and examine past experimental assessments of MAL algorithms. The creators of the algorithms we describe have answered the above questions in different ways, reflecting the broader community's disagreement about exactly what problem MAL algorithms should solve."}, {"heading": "2.1. FICTITIOUS PLAY", "text": "Fictional game (Brown, 1951) is probably the earliest example of a learning algorithm for two-player repetition games. Essentially, fictional game assumes that the opponent is playing an unknown and potentially mixed stationary strategy, and attempts to assess this strategy based on the empirical distribution of the opponent's actions - the frequency counts as probabilities for each of his actions. To grasp the frequency, fictional game clearly counts as having to be able to observe the opponent's actions, and the algorithm then responds best to this estimated strategy with each iteration. Since fictional game must calculate the best answer, it also requires complete knowledge of its own number combinations. Fictional game is guaranteed to converge in a Nash balance in self-play for a limited number of games. These games are supposed to have the fictional feature (see, for example, Monderer and Shapley (1996), fictional game is fictional game without this balance for a limited number of games, which is best)."}, {"heading": "2.2. DETERMINED", "text": "Certain or \"tyrannical\" (see, for example, Powers and Shoham (2005)) is an algorithm that solves the multi-agent learning problem by ignoring it. MAL algorithms typically change their behavior by adapting to signals about the game. However, certain algorithms, as the name suggests, simply rely on other algorithms to adapt their strategies to it. Certain algorithms list the stage play Nash balance and select the one that maximizes their personal reward in balance; then it plays out its corresponding action forever. 2 Certainly, determination can lead to some obvious problems. For example, in self-play, two determined agents can stubbornly perform actions from different balances, resulting in an average below-balance reward. Moreover, enumerating all Nash balances not only requires complete knowledge of each agent's reward function, but is also computationally costly, limiting the use of this relatively small strategy."}, {"heading": "2.3. TARGETED ALGORITHMS", "text": "Next, we will focus on two so-called targeted algorithms that focus on playing against certain classes of opponents, both of which are based on identifying what the opponent is doing (with particular attention to stationary and Nash balance), and then updating their behavior based on that assessment. AWESOME (Conitzer and Sandholm, 2003; Conitzer and Sandholm, 2007) tracks the opponent's behavior at different game periods and tries to maintain hypotheses about his game. For example, AWESOME tries to determine whether the other algorithm is playing a particular stage game called Nash balance. If so, AWESOME reacts with its own component of that particular balance and tries to maintain hypotheses about his game. This particular balance is known in advance to all implementations of AWESOME in order to avoid balance selection problems in self-play. There are other situations in which similarly fictitious games occur, and there are others."}, {"heading": "2.4. Q-LEARNING ALGORITHMS", "text": "A broad family of MAL algorithms is based on Q-Learning (Watkins and Dayan, 1992), Q = Q (Q = Q), which is an algorithm to find the optimal policy in Markov Decision Processes (MDPs). This family of MAL algorithms does not explicitly model the strategies of the adversary; instead, they content themselves with learning the expected discounted reward for taking an action and then follow a stationary policy encoded in the Q function. To learn the Q function, algorithms typically take random exploratory steps with a low (possibly decreasing) probability. Each algorithm in this family has a different way of selecting its strategy based on this Q function."}, {"heading": "2.5. GRADIENT ALGORITHMS", "text": "The specific details of this updating process depend on the individual algorithms, but the common feature is that they increase the likelihood of high reward actions and decrease the likelihood of unsuccessful actions. This family of algorithms is similar to Q-Learning in that they do not explicitly model their opponent's strategies and instead treat them as part of a non-stationary environment."}, {"heading": "2.6. PREVIOUS EXPERIMENTAL RESULTS", "text": "In fact, a considerable number of papers from the literature describe experimental comparisons that are often related to a particular MAL algorithm."}, {"heading": "3. Platform", "text": "The empirical experiments that have just been described were generally conducted with unique code tailored to the study of a particular feature of a particular algorithm. This experimental design has a number of negative consequences. Firstly, it reduces the reproducibility of experiments, for example by obscuring the details of the algorithm implementation. Finally, even if the source code is available for the original experiment, its usefulness can make it difficult to rededicate it for follow-up studies or new experiments. Finally, it is available for free download at http: / / www.cs.ubc.ca / Kevinlb / malt. In this section, we describe our solution to this problem: an open and reusable platform called MALT (Multiagent Learning Testbed) 2.0, which is available for free download."}, {"heading": "3.1. DEFINITIONS", "text": "An ordered pair of two algorithms is a pairing. This pair is ordered because many games with two players are asymmetrical: the payoff structure for the player in the row is different from the payoff structure for the player in the column. In the case where an algorithm is paired with a copy of itself (but with different internal states and independent random seeds), it is called self play. We focus on drawing games from distributions called game generators. A particular sample from a game generator is a game instance. Prisoner's dilemma is a game generator and a sample game instance is a certain set of payoffs that obey the preferential order of the prisoner. Other game generators are more heterogeneous; for example, one that we will discuss later from the space of all strategically different 2 x 2 games. A pairing and a game instance taken together, the algorithms are called match."}, {"heading": "3.2. PLATFORM STRUCTURE", "text": "In this section we give an overview of the structure of the platform. The five steps to run an experiment with the platform are summarized primarily in Figure 1. There are three important components of this platform: the configuration GUI, the experiment engine (the piece that simulates the repetitive games) and the visualization GUI. We describe each in Turn.The first step is to set up the experiment. First, a group of algorithms must be selected and algorithm parameters set. Second, a number of GAMUT game distributions must be selected and parameters for these games must be selected. Third, general experimental parameters must be specified, such as the number of iterations for each simulation. These decisions are encoded in human-readable text files and can be generated either with a provided GUI or with batch scripts. The second step is to generate a job file that will generate for each desired match. Each job file refers to the job balance, and balance files."}, {"heading": "3.3. ALGORITHM IMPLEMENTATIONS", "text": "In fact, most of us are able to set out in search of new paths to follow."}, {"heading": "4. Experimental Setup and Statistical Methods", "text": "As described in the preamble, this paper makes two major contributions. The first is the MALT platform, which we have now explained; the second is a demonstration of what MALT can do. Specifically, we have conducted a large-scale experiment with the aim of investigating the empirical relationship between average reward and other performance indicators (e.g. equilibrium convergence; regret) that have been taken into account in the literature. In this section, we describe the setup of this experiment and some of the statistical tools used in our analyses. We have examined all eleven of the algorithms described in \u00a7 3.3 and defined their parameters as they were described. We note that this choice was important because some algorithms are very sensitive to parameter settings. Nevertheless, we consider the question of parameter optimization outside the scope of our study and made parameter settings."}, {"heading": "4.1. BOOTSTRAPPING", "text": "If we are conducting an experiment in which two algorithms are running on a number of PSMs, then a natural way to compare their performance is to compare the mean of a measurement of their performance (average reward, for example), but if we conclude that \"the mean of an algorithm A is higher than the mean of the algorithm B,\" how robust is that assertion? If we are conducting this experiment again, are we confident that it would support the same conclusion? A good way to verify the results of an experiment is to run it several times. For example, imagine that we are conducting an experiment 100 times and find that 95 is the mean of the sample of algorithm A between [a, a] and that 95 is the mean of the sample of algorithm B between [b, b]."}, {"heading": "4.2. KOLMOGOROV-SMIRNOV TEST", "text": "While bootstrapping is useful to see if summary statistics are significantly different, we will also want to check whether two distributions themselves are significantly different. To determine whether two distributions are different, we use the KolmogorovSmirnov (KS) test. This test is non-parametric, which means that it does not assume that the underlying data comes from a known (e.g. normal) probability distribution. The KS test works by examining the maximum vertical distance between two CDFs. Two distributions are considered to be significantly different if that maximum vertical distance exceeds a certain level of significance, \u03b1. In our analysis, we use the default \u03b1 = 0.05, unless otherwise stated."}, {"heading": "4.3. SPEARMAN\u2019S RANK CORRELATION TEST", "text": "Spearman's rank correlation test is a method of determining whether or not there is a significant monotonous relationship between two paired variables. For example, we would like to show that there is a significant monotonous relationship between the size of the plot set of a game and the average reward of an agent. Like the KS test, Spearman's rank correlation test is not parametric; the relationship between the two variables can be positive (high values of one variable correlate with high values of the other variable) or negative (high values of one variable correlate with low values of the other)."}, {"heading": "4.4. ASSESSING CONVERGENCE", "text": "We are interested in studying the convergence behavior of the MAL algorithms. A natural solution to this problem is to perform a statistical test to determine whether a part of the run has the same action distribution as a later part. For example, we could check whether a later empirical action distribution has been drawn from the same distribution as an earlier sample (which determines that empirical mixed strategies were stationary) or whether an empirical action distribution profile has been drawn from a given mixed strategy profile (which establishes convergence to a Nash balance). Two obvious candidates for such a test are the Fisher Exact Test (FET) and Pearson Test, which can be used to check whether two multinomic action samples are drawn from a distribution (which establish convergence to a Nash balance)."}, {"heading": "4.5. PROBABILISTIC DOMINATION", "text": "The concept of probabilistic dominance can be used to argue that one distribution should be preferred to another in terms of a given performance metric. Specifically, a Solution Quality Distribution (SQD) A dominates another SQD B, if the q quantile of A is higher than the q quantile of B. If there are two algorithms, A and B, that try to maximize the reward, and A's SQD probably dominates B, then regardless of the reward value r, there are more sequences of A than of B that provide a reward of at least r. Probabilistic dominance is stronger than an assertion of the mean of the distributions: dominance implies higher averages."}, {"heading": "5. Empirical Evaluation of MAL Algorithms: Average Reward", "text": "As we discussed at the beginning of this post, we consider the average reward to be the most basic metric for assessing the performance of a MAL algorithm. We take the average in terms of the actions sampled, not the mixed strategy submitted. Formally, where iterations 1 to T refer to the 10,000 iterations we have recorded, we define the average reward an algorithm receives in a single match as r-value (T) i = 1-value T = 1 r (t) i. In this section, we examine the average reward metric in detail. We start in \u00a7 5.1 by comparing algorithms according to their \"raw\" average reward, setting an average not only across iterations, but also across generators and opponents. Next, we examine each of these dimensions separately. In \u00a7 5.2, we examine the algorithm performance between different generators and also examine the effects of game size. \"In \u00a7 5.3, we also examine the relationships between the algorithms and the different opponents in each case."}, {"heading": "5.1. \u201cRAW\u201d AVERAGE REWARD", "text": "First, we look at the \"raw\" average performance of each algorithm, averaged over the number of iterations, games and opponents. OBSERVATION 1. Q-Learning and RV\u03c3 (t) achieved the highest rewards on the large distribution. Q-Learning had the highest mean reward at 0.714, although RV\u03c3 (t) was scarce at an average of 0.710 (see Figure 2). We noticed significant differences within the reward data, and all other sample averages of the algorithms were still within a standard deviation of Q-Learning, including randomness (which reached a sample mean of 0.480).These rankings were not all significant. However, the small mean difference between Q-Learning and RV\u03c3 (t) does not indicate that Q-Learning was a better algorithm (in terms of averages) on the large distribution of games and opponents."}, {"heading": "5.2. PER-GENERATOR AVERAGE REWARD AND THE EFFECT OF GAME SIZE", "text": "As you can see in Figure 4, the performance of each algorithm varies considerably in the different game generators. However, this number makes it difficult to determine the best algorithms for generators that challenge all algorithms. So we can see that minimax-Q-IDR and, coincidentally, all other algorithms are inferior to the other algorithms in a wide range of generators, and Q-learning and RV2 tended to do well. OBSERVATION 2. Q-Learning was the best or one of the best algorithms to use for most algorithms.We define the best algorithms for a set of algorithms whose mean is 95%."}, {"heading": "5.3. PER-OPPONENT AVERAGE REWARD AND THE ALGORITHM GAME", "text": "We consider each algorithm as an average reward on a pro-opponent basis. OBSERVATION 4: Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "5.4. PROBABILISTIC DOMINATION OF ONE ALGORITHM BY ANOTHER", "text": "Now we consider the following question: Is a particular algorithm likely to be dominated by any alternative algorithm in terms of average reward in the face of a fixed opponent? OBSERVATION 6. Q-Learning was the only algorithm that was never probabilistically dominated by any other algorithm in terms of average reward. Q-Learning had the best performance in terms of probable dominance. Q-Learning and RV\u03c3 (t) were the least dominated algorithms: AWESOME was only likely to determine against a fictitious opponent that was itself dominated by Q-Learning; RV\u03c3 (t) was dominated by Q-Learning when playing against the Minimax Q variants, and also determined when playing against RV\u03c3 (t). Overall, dominance by another algorithm in self-play was a common trend; only AWESOME, determined and Q-Learning avoided being dominated by another algorithm when playing oneself."}, {"heading": "5.5. SELF PLAY", "text": "A closer analysis shows that for most algorithms, there was indeed a significant relationship between self-play and low reward. OBSERVATION 7. Most algorithms achieved a lower average reward in self-play. Distribution of self-play reward runs for AWESOME, certainly, fictional play and meta were probably dominated by the distribution of self-play reward. While the same did not apply to the gradient algorithms (they achieved less low self-play reward runs), their self-play tools were still significantly lower than their non-self-play tools. We confirmed this by looking at the 95% bootstrapped percentile intervals. There was no significant relationship between minimax-Q and minimax-Q-IDR."}, {"heading": "5.6. ALGORITHM SIMILARITY", "text": "Finally, we examine similarities between the ability of algorithms to achieve high reward. We can assign some of our algorithms to Q-Q pairs that have been tested on average by three large blocks. First, AWESOME and Meta are similar because they manage both portfolios that contain and are determined by a fictitious game. Likewise, we expect them to be similar to the fictitious game and to certain algorithms themselves. Second, GIGA-WoLF, GSA and RV\u03c3 (t) are similar because they all follow a reward gradient. Finally, minimax-Q and minimax-Q-IDR are similar because the latter is the same as the former, except for the addition of an IDR pre-processing step. We call these the portfolio, gradients and minimax blocks. We could also suspect that Q-learning, an algorithm that explicitly models the opponent's model, could be similar to the gradient algorithm, zero algorithms."}, {"heading": "6. Empirical Evaluation of MAL Algorithms: Other Metrics", "text": "In \u00a7 6.1, we examine regret, especially taking into account regret, the likely dominance of one algorithm by another and the relationship to reward. In \u00a7 6.2, we assess the tendency of algorithms to converge with stationary strategies. \u00a7 6.3 looks at convergence to the Nash equilibrium of stage play and relates this parameter to reward. In \u00a7 6.4, we look at the ability of algorithms to achieve at least their maximum rewards, and consider both the tendency per opponent to achieve maximum performance and the relationship to reward. Finally, in \u00a7 6.5, we measure the tendency of algorithms to converge to reward profiles consistent with the Nash equilibrium of endlessly repeated play."}, {"heading": "6.1. REGRET", "text": "Regret is the difference between the reward an algorithm would have received by playing the best static pure regret and the reward it received: Regret (~ \u03c3i, ~ a \u2212 i) = max a-AiT \u2211 t = 1 [r (a, a (t) \u2212 E [r (t) i, a (t) \u2212 i)] (5) The best static pure strategy is determined after the run, based on the assumption that the opponent's strategy would not change in each turn. We will use the strategy of regret - as opposed to one that uses the sampled actions the algorithm has performed - following bowling (2004a). Instead of looking at the total amount of regret over all 10,000 recorded iterations, we will discuss the mean regret about these iterations. As player payments are limited to the [0, 1] interval, the mean regret can give a better sense of the extent of regret in relation to possible reward."}, {"heading": "6.2. STRATEGIC STATIONARITY", "text": "In fact, it is such that most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to fight, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "6.3. CONVERGENCE TO STAGE-GAME NASH EQUILIBRIUM", "text": "It is not only a question of the expression, but also a question of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of the expression, of"}, {"heading": "6.4. MAXMIN DISTANCE", "text": "An agent's maxmin value is the largest sum he can guarantee, regardless of the behavior of his opponent. (6) We call this difference maxmin distance, noting that it is negative.OBSERVATION 12. Q-learning is the largest amount he can acquire more often than any other method. (6) We call this difference maxmin distance, noting that it is negative.OBSERVATION 12. Q-learning is an enforceable method. (6) We call it maxmin distance, remark. OBSERVATION 12. Q-learning can be an enforceable method. (6) We call this difference maxmin distance, remark. OBSERVATION 12. Q-learning is an enforceable method. (6) We call it negative.OBSERVATION 12. Q-learning is an enforceable method."}, {"heading": "6.5. CONVERGENCE TO REPEATED-GAME NASH EQUILIBRIUM", "text": "The payout profiles available in Nash for a repetitive game are exactly the enforceable profiles (see, for example, Osborne and Rubinstein (1994)). To determine whether a particular strategy profile is an equilibrium of a repetitive game, it is also necessary to consider how these strategies behave outside of the equilibrium trajectory (e.g., how they punish deviations by the other agent).While the algorithms we examined lack punishment mechanisms, it is still useful to assess how often they approach payout profiles that are consistent with repetitive Nash balances. We therefore build on the results of Section 6.4 and wonder how often both algorithms achieved enforceable payout mechanisms."}, {"heading": "7. Discussion and Conclusion", "text": "In this article we described MALT, a standardized test system for multipliers. This test system allows researchers to focus on the development and implementation of strategies. We also have an in-depth analysis of a large experiment that we have developed using strategies."}, {"heading": "Acknowledgements", "text": "Thanks to Nando de Freitas for his involvement in the early stages of this project and for helping us develop the reasoning in Appendix A. Thanks to Holger H. Hoos and Yevgeniy Vorobeychik for feedback on drafts, and to anonymous reviewers at the MLJ for helpful suggestions, and to David Ludgate for help with coding. Finally, thanks to Vincent Conitzer for providing code for AWESOME and helping us to us.39"}, {"heading": "Appendix A. Independent vs. Stratified Sampling", "text": "For all the experiments described in this article, we looked at the expected performance of an agreement denoted by f (\u00b5), where f is a metric function, \u00b5 \u0445 M is a match, and \u0435 \u0445 Z is a random seed that fully determines any non-deterministic behavior in both algorithms. However, the sample (instance / seed pairing) clearly defines a run. In designing our experiment, we had to choose whether to stratify runs on the basis of the match. For example, if we had enough time to perform 100 simulations, we could have performed either a single run on 100 matches or 10 runs on 10 matches. The stratification provides clearly more detailed data on the role that randomization plays in each match. However, to estimate general summary statistics - averages and quantifications - stratifications should be avoided. Formally, two schemes of sampling M and Z. Under independent sampling M and Z are sampled individually."}], "references": [{"title": "Evolutionary Tournament-Based Comparison of Learning and Non-Learning", "author": ["S. Airiau", "S. Saha", "S. Sen"], "venue": null, "citeRegEx": "Airiau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Airiau et al\\.", "year": 2007}, {"title": "Algorithms and Simulated Annealing", "author": ["B. Banerjee", "J. Peng"], "venue": "AAAI 11. Banerjee, B. and J. Peng:", "citeRegEx": "Banerjee and Peng,? \\Q2004\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2004}, {"title": "Rational and convergent learning in stochastic games", "author": ["M. Alberta. Bowling", "M. Veloso"], "venue": "Artificial Intelligence", "citeRegEx": "Bowling and Veloso,? \\Q2001\\E", "shortCiteRegEx": "Bowling and Veloso", "year": 2001}, {"title": "Iterative solution of games by ficticious play", "author": ["G. Brown"], "venue": null, "citeRegEx": "Brown,? \\Q1951\\E", "shortCiteRegEx": "Brown", "year": 1951}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. New York. Claus", "C. Boutilier"], "venue": null, "citeRegEx": "Claus and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Claus and Boutilier", "year": 1997}, {"title": "AWESOME: A General Multiagent Learning Algorithm that Converges", "author": ["V. Conitzer", "T. Sandholm"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Correlated-Q learning", "author": ["A. Greenwald", "K. Hall"], "venue": "Operations Research", "citeRegEx": "Greenwald and Hall,? \\Q2003\\E", "shortCiteRegEx": "Greenwald and Hall", "year": 2003}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["Columbia", "Vancouver", "M. Canada. Littman"], "venue": "ICML", "citeRegEx": "Columbia et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Columbia et al\\.", "year": 1994}, {"title": "A 2\u00d7 2 game without the fictitious play property", "author": ["A. Sela"], "venue": "Games and Economic Behavior", "citeRegEx": "Monderer and Sela,? \\Q1996\\E", "shortCiteRegEx": "Monderer and Sela", "year": 1996}, {"title": "Fictitious play property for games with identical interests", "author": ["D. Monderer", "L. Shapley"], "venue": "Journal of Economic", "citeRegEx": "Monderer and Shapley,? \\Q1996\\E", "shortCiteRegEx": "Monderer and Shapley", "year": 1996}, {"title": "Run the GAMUT: a comprehensive approach", "author": ["E. Nudelman", "J. Wortman", "K. Leyton-Brown", "Y. Shoham"], "venue": null, "citeRegEx": "Nudelman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nudelman et al\\.", "year": 2004}, {"title": "evaluating game-theoretic algorithms", "author": ["M. Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne and Rubinstein,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein", "year": 1994}, {"title": "The 2x2 Game", "author": ["Computing", "Vienna", "A. Austria. Rapoport", "M. Guyer", "D. Gordon"], "venue": "Artificial Intelligence", "citeRegEx": "Computing et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Computing et al\\.", "year": 1976}, {"title": "If multi-agent learning is the answer, what is the question?", "author": ["Y. Press. Shoham", "R. Powers", "T. Grenager"], "venue": null, "citeRegEx": "Shoham et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shoham et al\\.", "year": 2007}, {"title": "Nash convergence of gradient dynamics in general-sum games", "author": ["S. Singh", "M. Kearns", "Y. Mansour"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Introduction to Stochastic Search and Optimization: Estimation, Simulation and Control", "author": ["J.C. Spall"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Reinforcement Learning, An Introduction", "author": ["New Jersey: John Wiley", "R. Sons. Sutton", "A. Barto"], "venue": "Machine Learning", "citeRegEx": "Wiley et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wiley et al\\.", "year": 1999}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": null, "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "Some prominent examples include algorithms by Littman (1994), Singh et al. (2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 58, "endOffset": 82}, {"referenceID": 4, "context": "Some prominent examples include algorithms by Littman (1994), Singh et al. (2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 58, "endOffset": 105}, {"referenceID": 4, "context": "Some prominent examples include algorithms by Littman (1994), Singh et al. (2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 58, "endOffset": 132}, {"referenceID": 4, "context": "Some prominent examples include algorithms by Littman (1994), Singh et al. (2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 58, "endOffset": 149}, {"referenceID": 4, "context": "Some prominent examples include algorithms by Littman (1994), Singh et al. (2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 58, "endOffset": 175}, {"referenceID": 1, "context": "(2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007).", "startOffset": 101, "endOffset": 126}, {"referenceID": 1, "context": "(2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007). We take the position that the best multiagent learning (MAL) algorithm is the one that achieves the highest possible average reward.", "startOffset": 101, "endOffset": 160}, {"referenceID": 1, "context": "(2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007). We take the position that the best multiagent learning (MAL) algorithm is the one that achieves the highest possible average reward.1 Under this view, the problem faced by the designer of a MAL algorithm is qualitatively the same as the problem faced by the designer of a single-agent reinforcement learning algorithm. However, there is a fundamental difference between the two settings. In the stationary environment faced by classical reinforcement learners, the concept of an optimal policy is well defined, and hence learning algorithms can attempt to identify this policy. In a multiagent environment, the best policy to follow depends on the actions taken by the opponent, and thus on the ways in which the opponent\u2019s future behavior will be affected by the learner\u2019s 1 For alternatives, see Shoham et al. (2007)\u2014who called the approach that we espouse the \u201cprescriptive, noncooperative agenda\u201d\u2014or Sandholm (2007).", "startOffset": 101, "endOffset": 980}, {"referenceID": 1, "context": "(2000), Hu and Wellman (2003), Greenwald and Hall (2003), Bowling (2004a), Powers and Shoham (2005), Banerjee and Peng (2006), and Conitzer and Sandholm (2007). We take the position that the best multiagent learning (MAL) algorithm is the one that achieves the highest possible average reward.1 Under this view, the problem faced by the designer of a MAL algorithm is qualitatively the same as the problem faced by the designer of a single-agent reinforcement learning algorithm. However, there is a fundamental difference between the two settings. In the stationary environment faced by classical reinforcement learners, the concept of an optimal policy is well defined, and hence learning algorithms can attempt to identify this policy. In a multiagent environment, the best policy to follow depends on the actions taken by the opponent, and thus on the ways in which the opponent\u2019s future behavior will be affected by the learner\u2019s 1 For alternatives, see Shoham et al. (2007)\u2014who called the approach that we espouse the \u201cprescriptive, noncooperative agenda\u201d\u2014or Sandholm (2007).", "startOffset": 101, "endOffset": 1081}, {"referenceID": 11, "context": ", (Nudelman et al., 2004; Powers and Shoham, 2005)).", "startOffset": 2, "endOffset": 50}, {"referenceID": 15, "context": ") on two-action games (Singh et al., 2000) or constant-sum games (Littman, 1994).", "startOffset": 22, "endOffset": 42}, {"referenceID": 4, "context": ", 2000) or constant-sum games (Littman, 1994). We also mention as an aside that MAL experiments have been conducted in settings that are neither generalizations nor restrictions of our setting, such as the population-based work by Axelrod (1987) and Airiau et al.", "startOffset": 43, "endOffset": 246}, {"referenceID": 0, "context": "We also mention as an aside that MAL experiments have been conducted in settings that are neither generalizations nor restrictions of our setting, such as the population-based work by Axelrod (1987) and Airiau et al. (2007).", "startOffset": 203, "endOffset": 224}, {"referenceID": 3, "context": "Fictitious play (Brown, 1951) is probably the earliest example of a learning algorithm for two-player games repeated games.", "startOffset": 16, "endOffset": 29}, {"referenceID": 3, "context": "Fictitious play (Brown, 1951) is probably the earliest example of a learning algorithm for two-player games repeated games. Essentially, fictitious play assumes that the opponent is playing an unknown and potentially mixed stationary strategy, and tries to estimate this strategy from the opponent\u2019s empirical distribution of actions\u2014the frequency counts for each of its actions normalized to be probabilities. Clearly, in order to collect the frequency counts fictitious playmust be able to observe the opponent\u2019s actions. The algorithm then, at each iteration, best responds to this estimated strategy. Because fictitious play needs to calculate a best response, it also assumes complete knowledge of its own payoffs. Fictitious play is guaranteed to converge to a Nash equilibrium in self play for a restricted set of games. These games are said to have the fictitious play property (see, for instance Monderer and Shapley (1996); for an example of a simple 2 \u00d7 2 game without this property see Monderer and Sela (1996)).", "startOffset": 17, "endOffset": 933}, {"referenceID": 3, "context": "Fictitious play (Brown, 1951) is probably the earliest example of a learning algorithm for two-player games repeated games. Essentially, fictitious play assumes that the opponent is playing an unknown and potentially mixed stationary strategy, and tries to estimate this strategy from the opponent\u2019s empirical distribution of actions\u2014the frequency counts for each of its actions normalized to be probabilities. Clearly, in order to collect the frequency counts fictitious playmust be able to observe the opponent\u2019s actions. The algorithm then, at each iteration, best responds to this estimated strategy. Because fictitious play needs to calculate a best response, it also assumes complete knowledge of its own payoffs. Fictitious play is guaranteed to converge to a Nash equilibrium in self play for a restricted set of games. These games are said to have the fictitious play property (see, for instance Monderer and Shapley (1996); for an example of a simple 2 \u00d7 2 game without this property see Monderer and Sela (1996)).", "startOffset": 17, "endOffset": 1023}, {"referenceID": 6, "context": "Correlated-Q (Greenwald and Hall, 2003) does", "startOffset": 13, "endOffset": 39}, {"referenceID": 15, "context": "GIGA-WoLF is the latest algorithm in a line of gradient learners that started with IGA (Singh et al., 2000).", "startOffset": 87, "endOffset": 107}, {"referenceID": 18, "context": "1) and strategic convergence to a Nash equilibrium when playing against GIGA (Zinkevich, 2003) in two-player two-action games.", "startOffset": 77, "endOffset": 94}, {"referenceID": 1, "context": "RV\u03c3(t) (Banerjee and Peng, 2006) belongs to a second line of gradient algorithms that started with ReDVaLeR (Banerjee and Peng, 2004).", "startOffset": 108, "endOffset": 133}, {"referenceID": 3, "context": "Littman (1994) 6 1 1 3 1\u00d7 10 Claus and Boutilier (1997) 2 3 1 - 100 1 - 100 50-2500 Greenwald and Hall (2003) 7 5 1 2500 - 3333 1\u00d7 10 Bowling (2004b) 2 6 1 ? 1\u00d7 10 Nudelman et al.", "startOffset": 29, "endOffset": 56}, {"referenceID": 3, "context": "Littman (1994) 6 1 1 3 1\u00d7 10 Claus and Boutilier (1997) 2 3 1 - 100 1 - 100 50-2500 Greenwald and Hall (2003) 7 5 1 2500 - 3333 1\u00d7 10 Bowling (2004b) 2 6 1 ? 1\u00d7 10 Nudelman et al.", "startOffset": 29, "endOffset": 110}, {"referenceID": 3, "context": "Littman (1994) 6 1 1 3 1\u00d7 10 Claus and Boutilier (1997) 2 3 1 - 100 1 - 100 50-2500 Greenwald and Hall (2003) 7 5 1 2500 - 3333 1\u00d7 10 Bowling (2004b) 2 6 1 ? 1\u00d7 10 Nudelman et al.", "startOffset": 29, "endOffset": 150}, {"referenceID": 3, "context": "Littman (1994) 6 1 1 3 1\u00d7 10 Claus and Boutilier (1997) 2 3 1 - 100 1 - 100 50-2500 Greenwald and Hall (2003) 7 5 1 2500 - 3333 1\u00d7 10 Bowling (2004b) 2 6 1 ? 1\u00d7 10 Nudelman et al. (2004) 3 13 100 10 1\u00d7 10 Powers and Shoham (2005) 11 21 ? ? 2\u00d7 10 Banerjee and Peng (2006) 2 1 1 1 16000 Conitzer and Sandholm (2007) 3 2 1 1 2500", "startOffset": 29, "endOffset": 187}, {"referenceID": 3, "context": "Littman (1994) 6 1 1 3 1\u00d7 10 Claus and Boutilier (1997) 2 3 1 - 100 1 - 100 50-2500 Greenwald and Hall (2003) 7 5 1 2500 - 3333 1\u00d7 10 Bowling (2004b) 2 6 1 ? 1\u00d7 10 Nudelman et al. (2004) 3 13 100 10 1\u00d7 10 Powers and Shoham (2005) 11 21 ? ? 2\u00d7 10 Banerjee and Peng (2006) 2 1 1 1 16000 Conitzer and Sandholm (2007) 3 2 1 1 2500", "startOffset": 29, "endOffset": 230}, {"referenceID": 1, "context": "(2004) 3 13 100 10 1\u00d7 10 Powers and Shoham (2005) 11 21 ? ? 2\u00d7 10 Banerjee and Peng (2006) 2 1 1 1 16000 Conitzer and Sandholm (2007) 3 2 1 1 2500", "startOffset": 66, "endOffset": 91}, {"referenceID": 1, "context": "(2004) 3 13 100 10 1\u00d7 10 Powers and Shoham (2005) 11 21 ? ? 2\u00d7 10 Banerjee and Peng (2006) 2 1 1 1 16000 Conitzer and Sandholm (2007) 3 2 1 1 2500", "startOffset": 66, "endOffset": 134}, {"referenceID": 4, "context": "While four of the eleven algorithms tested in this study were simple stationary-strategy baselines, the remaining seven were MAL algorithms including Hyper-Q (Tesauro, 2004), WoLF-PHC (Bowling and Veloso, 2002), and a joint action learner (Claus and Boutilier, 1997).", "startOffset": 239, "endOffset": 266}, {"referenceID": 3, "context": "For example, in Littman (1994) two versions of minimax-Q and two versions of Q-learning were tested, with each version differing only in its training regime. In Greenwald and Hall (2003), four versions of Correlated-Qwere tested against Q-learning and Friend-Q and Foe-Q (Littman, 2001).", "startOffset": 28, "endOffset": 187}, {"referenceID": 3, "context": "For example, in Littman (1994) two versions of minimax-Q and two versions of Q-learning were tested, with each version differing only in its training regime. In Greenwald and Hall (2003), four versions of Correlated-Qwere tested against Q-learning and Friend-Q and Foe-Q (Littman, 2001). Foe-Q is the same as minimax-Q. To our knowledge, the experiment that considered the greatest variety of algorithms was Powers and Shoham (2005). While four of the eleven algorithms tested in this study were simple stationary-strategy baselines, the remaining seven were MAL algorithms including Hyper-Q (Tesauro, 2004), WoLF-PHC (Bowling and Veloso, 2002), and a joint action learner (Claus and Boutilier, 1997).", "startOffset": 28, "endOffset": 433}, {"referenceID": 11, "context": "However with the creation of GAMUT (Nudelman et al., 2004), a suite of game generators, generating large game sets is now easy.", "startOffset": 35, "endOffset": 58}, {"referenceID": 2, "context": "(2004) also performed one of the largest previous MAL experiments, using three MAL algorithms (minimax-Q, WoLF (Bowling and Veloso, 2001), and Q-learning) on 100 game instances from each of thirteen distributions.", "startOffset": 111, "endOffset": 137}, {"referenceID": 4, "context": "Finally, previous experiments have differed substantially in the number of iterations considered, ranging from 50 (Claus and Boutilier, 1997) to 1 \u00d7 106 (Bowling, 2004b).", "startOffset": 114, "endOffset": 141}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer.", "startOffset": 13, "endOffset": 38}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer.", "startOffset": 13, "endOffset": 116}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer. Initially, this limitation was partly due to the difficulty of creating a large number of diverse game instances. However with the creation of GAMUT (Nudelman et al., 2004), a suite of game generators, generating large game sets is now easy. Indeed, Nudelman et al. (2004) also performed one of the largest previous MAL experiments, using three MAL algorithms (minimax-Q, WoLF (Bowling and Veloso, 2001), and Q-learning) on 100 game instances from each of thirteen distributions.", "startOffset": 13, "endOffset": 459}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer. Initially, this limitation was partly due to the difficulty of creating a large number of diverse game instances. However with the creation of GAMUT (Nudelman et al., 2004), a suite of game generators, generating large game sets is now easy. Indeed, Nudelman et al. (2004) also performed one of the largest previous MAL experiments, using three MAL algorithms (minimax-Q, WoLF (Bowling and Veloso, 2001), and Q-learning) on 100 game instances from each of thirteen distributions. Some recent papers have also leveraged GAMUT, such as Powers and Shoham (2005). Finally, previous experiments have differed substantially in the number of iterations considered, ranging from 50 (Claus and Boutilier, 1997) to 1 \u00d7 106 (Bowling, 2004b).", "startOffset": 13, "endOffset": 745}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer. Initially, this limitation was partly due to the difficulty of creating a large number of diverse game instances. However with the creation of GAMUT (Nudelman et al., 2004), a suite of game generators, generating large game sets is now easy. Indeed, Nudelman et al. (2004) also performed one of the largest previous MAL experiments, using three MAL algorithms (minimax-Q, WoLF (Bowling and Veloso, 2001), and Q-learning) on 100 game instances from each of thirteen distributions. Some recent papers have also leveraged GAMUT, such as Powers and Shoham (2005). Finally, previous experiments have differed substantially in the number of iterations considered, ranging from 50 (Claus and Boutilier, 1997) to 1 \u00d7 106 (Bowling, 2004b). Iterations in a repeated game are typically divided into \u201csettling in\u201d (also called a \u201cburn-in\u201d period) and \u201crecording\u201d phases, allowing the algorithms time to converge to stable behavior before results are recorded. Powers and Shoham (2005) recorded the final 20 000 of 200 000 iterations and Nudelman et al.", "startOffset": 13, "endOffset": 1159}, {"referenceID": 1, "context": "For example, Banerjee and Peng (2006) used only a single 3 \u00d7 3 action \u201csimple coordination game\u201d and Littman (1994) probed algorithm behavior with a single grid-world version of soccer. Initially, this limitation was partly due to the difficulty of creating a large number of diverse game instances. However with the creation of GAMUT (Nudelman et al., 2004), a suite of game generators, generating large game sets is now easy. Indeed, Nudelman et al. (2004) also performed one of the largest previous MAL experiments, using three MAL algorithms (minimax-Q, WoLF (Bowling and Veloso, 2001), and Q-learning) on 100 game instances from each of thirteen distributions. Some recent papers have also leveraged GAMUT, such as Powers and Shoham (2005). Finally, previous experiments have differed substantially in the number of iterations considered, ranging from 50 (Claus and Boutilier, 1997) to 1 \u00d7 106 (Bowling, 2004b). Iterations in a repeated game are typically divided into \u201csettling in\u201d (also called a \u201cburn-in\u201d period) and \u201crecording\u201d phases, allowing the algorithms time to converge to stable behavior before results are recorded. Powers and Shoham (2005) recorded the final 20 000 of 200 000 iterations and Nudelman et al. (2004) used the final 10 000 of 100 000 iterations.", "startOffset": 13, "endOffset": 1234}, {"referenceID": 5, "context": "4. Meta Meta is implemented according to the pseudo-code in Powers and Shoham (2005). The Powers and Shoham (2005) implementation of meta used a distance measure based on the Hoeffding Inequality, even though the pseudo-code called for using an l2 norm.", "startOffset": 0, "endOffset": 85}, {"referenceID": 5, "context": "4. Meta Meta is implemented according to the pseudo-code in Powers and Shoham (2005). The Powers and Shoham (2005) implementation of meta used a distance measure based on the Hoeffding Inequality, even though the pseudo-code called for using an l2 norm.", "startOffset": 0, "endOffset": 115}, {"referenceID": 5, "context": "Gradient Algorithms Our implementation of GIGA-WoLF follows the original pseudo-code and uses the learning rate and step size schedules from the original experiments by Bowling (2004a) as defaults; see Table VI. We note, however, that these step sizes were set for drawing smooth trajectories and may not necessarily yield strong performance, and furthermore that the original experiments for GIGA-WoLF involved more iterations than we simulated (106 as compared to 105). For GIGA-WoLF\u2019s retraction map operation (the function that maps an arbitrary vector in R to the closest probability vector in terms of l2 distance) we used an algorithm based on the method described in Govindan and Wilson (2003). GIGA-WoLF has two variants: in one it assumes that it can counterfactually determine the reward for playing an arbitrary action in the previous iteration, and in the other it only knows the reward for the the action that it played and has to approximate the rewards for the other actions.", "startOffset": 181, "endOffset": 702}, {"referenceID": 1, "context": "RV\u03c3(t) is a implementation of the algorithm given in Banerjee and Peng (2006). Some initial experiments showed that the settings of the algorithm used in the paper performed very poorly, and so we used some hand-picked parameter settings that were more aggressive and seemed to perform better.", "startOffset": 53, "endOffset": 78}, {"referenceID": 3, "context": "14 (see, for example, Shoham and Leyton-Brown (2008)).", "startOffset": 40, "endOffset": 53}, {"referenceID": 3, "context": "14 (see, for example, Shoham and Leyton-Brown (2008)). We also considered a variant of minimax-Q in which iterative domination removal (IDR) is used as a preprocessing step. To our knowledge, we were the first to propose this algorithm in Lipson (2005); we dubbed it minimax-Q-IDR.", "startOffset": 40, "endOffset": 253}, {"referenceID": 3, "context": "14 (see, for example, Shoham and Leyton-Brown (2008)). We also considered a variant of minimax-Q in which iterative domination removal (IDR) is used as a preprocessing step. To our knowledge, we were the first to propose this algorithm in Lipson (2005); we dubbed it minimax-Q-IDR. In each step of the iterative IDR algorithm mixed-strategy domination is checked using a linear program (see, for example, Shoham and Leyton-Brown (2008)).", "startOffset": 40, "endOffset": 436}, {"referenceID": 5, "context": "AWESOME converged in 54.3% of its runs, and determined converged in 53.1% of its runs. Determined was better than AWESOME at converging to a Pareto-optimal Nash equilibrium (a Nash equilibrium that was not Pareto-dominated by any other Nash equilibrium). AWESOMEmost frequently converged to a Pareto-dominated equilibrium. This was likely influenced by the way that our implementation of AWESOME picked its \u2018special\u2019 equilibrium:7 the first equilibrium found by the Lemke-Howson algorithm, without attention to whether it was, e.g., Pareto-dominated. AWESOME also tended to attain lower reward when it converged to a Pareto-dominated Nash equilibrium than when it did not converge or converged to a non-dominated Nash equilibrium. Figure 24 shows the extent to which each algorithm converged to a stage-game Nash equilibrium in self play. Notice how often determined converged: this indicates that the games we studied often possessed one Nash equilibrium that was the best for both agents. Indeed, we can see that a surprisingly high number of games had a unique stage-game Nash equilibrium (58.5%). We expect that convergence results would look qualitatively different with generators that were much less likely to produce games with unique equilibria. Observe that AWESOME nearly always converged. Recall that we previously found that AWESOME received lower average reward in self-play than non-self-play runs (\u00a7 5.5). Now we can conclude that this failure to achieve high rewards was not due to a failure to reach equilibrium. An interesting modification of the AWESOME algorithm would be to use its self-play machinery to converge to stable strategies that are not stage-game Nash equilibria, such as the socially-optimal 7 The original paper, Conitzer and Sandholm (2007), left the method of picking the \u2018special\u2019 equilibrium unspecified.", "startOffset": 22, "endOffset": 1778}, {"referenceID": 11, "context": ", Osborne and Rubinstein (1994)).", "startOffset": 2, "endOffset": 32}, {"referenceID": 5, "context": "38 \u2212 More examination of the relationship between performance and game properties like size; \u2212 More detailed investigation of algorithm behavior on instances from single generators; \u2212 Investigation of additional algorithms like Hyper-Q (Tesauro, 2004) and Nash-Q (Hu and Wellman, 1998); \u2212 Study of N -player repeated games and stochastic games (along the lines of Vu et al. (2005)).", "startOffset": 249, "endOffset": 381}], "year": 2014, "abstractText": "Abstract. There exist many algorithms for learning how to play repeated bimatrix games. Most of these algorithms are justified in terms of some sort of theoretical guarantee. On the other hand, little is known about the empirical performance of these algorithms. Most such claims in the literature are based on small experiments, which has hampered understanding as well as the development of new multiagent learning (MAL) algorithms. We have developed a new suite of tools for running multiagent experiments: the MultiAgent Learning Testbed (MALT). These tools are designed to facilitate larger and more comprehensive experiments by removing the need to build one-off experimental code. MALT also provides baseline implementations of many MAL algorithms, hopefully eliminating or reducing differences between algorithm implementations and increasing the reproducibility of results. Using this test suite, we ran an experiment unprecedented in size. We analyzed the results according to a variety of performance metrics including reward, maxmin distance, regret, and several notions of equilibrium convergence. We confirmed several pieces of conventional wisdom, but also discovered some surprising results. For example, we found that single-agent Q-learning outperformed many more complicated and more modern MAL algorithms.", "creator": "LaTeX with hyperref package"}}}