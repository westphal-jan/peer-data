{"id": "1401.8269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2014", "title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "abstract": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, \"buy\" entails \"own\". Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word-context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.", "histories": [["v1", "Fri, 31 Jan 2014 19:42:19 GMT  (58kb)", "http://arxiv.org/abs/1401.8269v1", "to appear in Natural Language Engineering"]], "COMMENTS": "to appear in Natural Language Engineering", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["peter d turney", "saif m mohammad"], "accepted": false, "id": "1401.8269"}, "pdf": {"name": "1401.8269.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["saif.mohammad}@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 140 1.82 69v1 [cs.CL]"}, {"heading": "1 Introduction", "text": "In fact, the majority of people who work for the rights of women and men are working for the rights of men and women who work for the rights of women and men. Indeed, the women and men who work for the rights of men and women are working for the rights of women and men. Indeed, the women and men who work for the rights of women and men are working for the rights of men and women. Indeed, it is the case that women and men who work for the rights of men and women have the same rights and obligations."}, {"heading": "3 Semantic relations and lexical entailment", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Related work", "text": "The first RTE challenge took place in 2005 (Dagan, Glickman, and Magnagan 2006), and it has been a regular occurrence since 2001 RTE systems have included a module for recognizing lexical entanglements (Hickl, Bensley, Williams, Roberts, Rink, and Shi 2006; Herrera, Pen, as, and Verdejo 2006). The early RLE modules typically used a symmetrical similarity measurement, such as the cosmic metric (Salton and McGill 1983), the LIN measurement (Lin 1998), or a measurement based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that the entanglement of nature and symmetrical metrics can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) suggested an asymmetrical similarity for the degree to which a word can be replaced by a word without substantially changing the meaning of the judgement."}, {"heading": "5 Performance measures", "text": "One difference between an asymmetric benchmark (such as balAPinc) and a classification model based on supervised machine learning (such as ConVecs or SimDiffs) is that the former provide a real value, while the latter provide a binary value classification (0 = does not involve it and 1 = does not involve it). However, this difference is superficial. Many supervised learning algorithms (including the algorithms we use here) are able to generate a real probability value (the probability that the given example belongs in class 1), and it is also easy to create a binary class from a real value by setting a threshold on the score.In our experiments (Section 8) we evaluate all three algorithms both as real asymmetric similarity measures and as binary classifiers. We use average precision (AP) as a measure of performance for real values according to Kotlerman."}, {"heading": "5.1 Average precision", "text": "Suppose we have issued a query to a search engine and it has returned a ranking of N documents, sorted in descending order of their automatically estimated relevance to our query. That is, P (r) is the precision of the ranking if we have labeled the list of all documents as either relevant or irrelevant to the given query. Let's leave the list of r-th documents 1 if the r-th document is labeled as relevant, 0 otherwise AP is defined as follows (Buckley and Voorhees 2000): AP = respected no = 1 [r) \u00b7 rel (r) \u00b7 rel (r) be the list of r-th documents is relevant, 0 other."}, {"heading": "5.2 Precision, recall, F-measure, and accuracy", "text": "Like CAP, precision and recall were originally conceived as performance measures for information gathering systems. A system's precision is an estimate of the conditional probability that a document is truly relevant to a query when the system says it is relevant. A system's recall is an estimate of the conditional probability that the system will say that a document is relevant to a query when it is really relevant.There is a trade-off between precision and recall; one can be optimized at the expense of the other. The F measure is the harmonic mean of precision and recall. It is designed to reward a balance of precision and recall. Accuracy is a natural and intuitive measure of performance, but it is sensitive to the relative magnitude of classes. It is easy to interpret accuracy when we have two equally large classes, but it is difficult to interpret one class when one is much larger than the other. The F measure is a better measure when classes are not accounted."}, {"heading": "6 Three approaches to lexical entailment", "text": "All three approaches are based on word context matrices. An introduction to the concepts behind word context matrices can be found in the survey paper from Turney and Pantel (2010).In preliminary experiments with our development data sets Dev1 and Dev2, we optimized the three approaches to optimize their performance.We describe how Dev1 and Dev2 were generated in Section 8.1.1. For each algorithm, we selected the matrix or matrices that were most accurate with the development data.For both balAPinc and ConVecs, we chose the word context matrix from Turney, Neuman, Assaf and Cohen (2011).For SimDiffs, we selected two word context matrices from Turney (2012).3ConVecs and SimDiffs use vector machines (SVMs) for supervised learning. We used the development datasets to select the best microcontext matrices from the kernel."}, {"heading": "6.1 The context inclusion hypothesis: balAPinc", "text": "It is not as if we are in the geometric meaning of the word balAPinc (u, v) \u00b7 LIN (u, v) \u00b7 LIN (u, v) \u00b7 LIN (u, v) \u00b7 LIN (u, v) \u00b7 LIN (u, v)): To define APinc and LIN, we must first introduce some terms. Kotlerman et al. (2010): balAPinc with terms from set theory, whereas ConVecs3 copies of all three matrices are available here."}, {"heading": "6.2 The context combination hypothesis: ConVecs", "text": "It is a way in which the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments we use the word-context matrix X of Turney et al. (2011), as in Section 6.1, but now we flatten it with a drunken word w and the j-th context c. (2012) We use the word-context matrix X of Turney et al. (2011), as in Section 6.1, but we flatten it with a drunken word w and the j-th context c. (2011), we use the word-context matrix X of Turney et al. (2011), as in Section 6.1, but now we flatten it with an intoxicated word w and the j-th context c. (2011), we use the word-context matrix X of Turney et al. (2011), as in Section 6.1, but now we flatten it with an intoxicated word w and the j-th context c. (2011), we use the word-context matrix X of Turney et al."}, {"heading": "6.3 The similarity differences hypothesis: SimDiffs", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcsrVo"}, {"heading": "7 Three datasets for lexical entailment", "text": "This section describes the three datasets we use in our experiments.12 The first two datasets have been used in the past mainly for lexical research of connections.11 The third dataset was used for semantic relationship research.11 This dataset is available at http: / / ogden.basic-english.org / word2000.html.The KDSZ dataset was introduced by Kotlerman et al. (2010) to evaluate balAPinc. The dataset contains 3,772 word pairs, 1,068 marked correlations and 2,704 marked correlations. It was created by introducing a dataset of 3,200 marked word pairs from Zhitomirsky-Geffet and Dagan (2009)."}, {"heading": "7.2 The BBDS dataset", "text": "The BBDS dataset was compiled by Baroni et al. (2012) and applied to the evaluation of balAPinc and ConVecs. In their paper, Baroni et al. (2012) discuss several different datasets. We use the dataset they call N1 | = N2, described in Section 3.3. The dataset contains 2,770 pairs of words, 1,385 labeled endings and 1,385 labeled endings. All 1385 labeled endings are hyponym-hypernoun pairs, such as pope | = Leader. The pairs were automatically generated from WordNet and then manually validated. Although the class sizes are balanced, 50% of the endings and 50% of the endings are not, the BBDS dataset is not representative of the variety of semantic relationships that entail withdrawal, as we will see in Section 7.3."}, {"heading": "7.3 The JMTH dataset", "text": "Jurgens et al. (2012) created a semantic relationship dataset for SemEval-2012 Task 2: Measuring Degrees of Relational Similarity.13 This dataset contains 3,218 word pairs labeled with seventy-nine types of semantic relationships. In this section we describe"}, {"heading": "12 Personal communication with Zhitomirsky-Geffet in March 2012.", "text": "In the second half of the last decade, in which the world is still in order, we are dealing with a global crisis in which the global financial and economic crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the global financial crisis, the"}, {"heading": "7.3.2 Inter-annotator agreement", "text": "As we mentioned above, we assume that all word pairs within a subcategory belong to the same class (either all match or none match). To verify this assumption, we first randomly selected 100 word pairs, 50 labeled bring with them, and 50 labeled bring nothing with them. We hid the labels and then labeled them independently manually, first using the relational definition of the lexical mapping, and then a second time using the substitutional definition of the lexical mapping (see section 2). Table 4 shows the percentage agreement between our manual labels and the automatic labeling generated from the SemEval 2012 dataset by mapping in Tables 2 and 3. With the relational definition of lexical mapping, we agreed to 81% of the labels. The match between our manual labels and the labels generated from the SemEval 2012 dataset by mapping in Tables 2 and 3 is that the 81% of the labels are all aligned to the 81% of the datasets that were automatically assigned to the 81."}, {"heading": "8 Experiments", "text": "This section examines the three approaches to lexical enailment (balAPinc, ConVecs and SimDiffs) on the three datasets."}, {"heading": "8.1 Experiments with the JMTH dataset", "text": "For the first set of experiments we used the JMTH dataset (section 7.3) with 2,308 pairs of words, 1,154 in class 0 and 1,154 in class 1."}, {"heading": "8.1.1 Experimental setup", "text": "For the experiments, we divided the dataset into three (approximately) equal parts, two development levels (Dev1 and Dev2) and one test set (test), the columns were random, unless the balance of class sizes was maintained in all three sub-ranges. Dev1 and Dev2 both contain 768 pairs, and test contains 772 pairs. Table 5 shows the number of word pairs in the test set for each of the ten highest categories. In tables 2 and 3 we have seen that a = b both for all sub-categories of ID a = b a 6 | = b | a b 5 the number of word pairs in the test set for each of the ten highest class inclusions 0 12 55 0 67 2 sub-categories of 72 7 27 118 3 similar 28 15 15 Contrast 9 25 0 26 60 5 Attribute 56 0 26 95 6 Non-Attribute 0 40 0 31 71 7 Case Relations 6 24 0 27 57 8 Cause Purpose 21 0 29 65 9 Reference 34 12 74 - Total 125 Class 125 Category-2125."}, {"heading": "8.1.2 Results", "text": "According to Fisher's Exact Test (Agresti 1996), the accuracy of ConVecs and SimDiffs does not differ significantly from the accuracy of SimDiffs (72.4%) at the confidence level of 95%. The other performance yardsticks (AP0, AP1, Pre, Rec, and F) follow the same general pattern as accuracy, which we would normally expect for a balanced dataset. The last column in Table 6 shows the confidence interval for accuracy, calculated using the Wilson method. Table 7 shows how the accuracy of the three algorithms varies across the ten high-level categories in the test set, which is usually true for a balanced dataset. The last column in Table 6 shows the confidence interval of 95% calculated using the Wilson method. Table 7 shows how the accuracy of the three algorithms varies across the ten high-level categories in the test set."}, {"heading": "1 1 1 1 0.80 0.79 0.724 0.724 0.724 72.4 69.1\u201375.4", "text": "Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...) Look at how the situation has evolved in recent years. (...)"}, {"heading": "8.2 Experiments with the KDSZ dataset", "text": "The second set of experiments used the KDSZ dataset (section 7.1) with 3,772 pairs of words, 2,704 in class 0 and 1,068 in class 1."}, {"heading": "8.2.1 Experimental setup", "text": "We experimented with four different ways of splitting the data set. The evaluation column in Table 10 indicates the experimental setup (data set splitting). The standard evaluation is a ten-fold cross-validation in which the folds are random. This evaluation yields relatively high values, because although each pair in the KDSZ dataset is unique, many pairs share a common term. This makes supervised learning easier because a couple in the test fold often have a term with several pairs in the training folds. The clustered evaluation is designed to be more demanding than the standard evaluation. The clustered evaluation is a ten-fold cross-validation with a non-random number of pairs with common terms, it is not possible to construct ten folds so that there are absolutely no terms to be divided by any two folds. Therefore, we gave high priority to isolating the most common words into single folds, but we allowed some less common words into more than one fold."}, {"heading": "8.2.2 Results", "text": "In Table 10, the four experimental setups (standard, cluster, balanced, and different) are given in order of increasing challenge and increasing realism. Of the four experimental setups, we believe that the differing evaluation is the most difficult and realistic. If an RLE module is part of a commercial RTE system, the module will inevitably encounter pairs of words in the field that it has seen during the training. Different evaluation comes closest to approaching field usage. On the various evaluations, balAPinc achieves an accuracy of 58.2%, ConVecs has an accuracy of 56.1%, and SimDiffs reaches 57.4%."}, {"heading": "8.3 Experiments with the BBDS dataset", "text": "The final set of experiments used the BBDS data set (section 7.2). The data set has 2,770 word pairs, 1,385 in class 0 and 1,385 in class 1.Algorithm Evaluation AP0 AP1 Pre Rec F Acc 95% C.I.balAPinc standard 0.79 0.73 0.722 0.722 72.2 70.5-73.822 0.79 0.73 0.722 0.722 0.722 0.822 0.722 0.722 0.72.2 70.5-73.8 different 0.79 0.73 0.701 0.687 0.682 68.7 67.0-70.4ConVecs standard 0.95 0.876 0.876 87.6 86.3-88.8clustered 0.92 0.80.829 0,821 0.819 80.1 80.6-83.5 different 0.72 0.71 0,652 0.651 0.650 65.1 63.3-66.9SimDiffs standard 0.97 0.97 0.97 0.97 0.913 0.90.90.90.90.91.3 0.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.90.70.70.70.70.70.70.70.80.70.70.80.70.80.80.70.80.80.70.80.70.80.80.70.80.80.80.70.80.80.80.70.80.80.80.70.80.80.80.80.70.80.80.80.70.80.80.80.80.70.80.80.80.80.80.80.70.80.80.80.80.80.80.70.80.80.80.80.80.80.70.80.80.80.80.80.80.80.80.80.70.80.80.80.80.80.80.80.80.70.80.80.80.80.80.80.80.80.80.80.80."}, {"heading": "8.3.1 Experimental setup", "text": "We experimented with three different methods to divide the data set. In Table 11, the evaluations follow the same arrangements as in Table 10. However, there is no balanced structure, because the BBDS data set is already balanced. In the different evaluations, the algorithms are trained on the JMTH data set and evaluated on the BBDS. This is the most realistic evaluation structure."}, {"heading": "8.3.2 Results", "text": "In Table 11, balAPinc comes to an accuracy of 68.7%, ConVecs to an accuracy of 65.1%, and SimDiffs to 74.5%, all of which differ at the confidence level of 95%, according to Fisher's Exact Test. BBDS data were used by Baroni et al. (2012) to compare balAPinc with ConVecs. They used two different assessment approaches that are similar to our standard and other setups. for balAPinc with a standard setup, they reached an accuracy of 70.1%, just below our result of 72.2%. The difference is probably due to slight differences in the word context matrices we used. For balAPinc with a different setup, their accuracy was 70.4%, compared to our 68.7%. They used their own independent data set to match balAPinc while we used the JMTH dataset, while we used our word context with 66.6% and our training accuracy."}, {"heading": "16 These accuracy numbers and the numbers reported in the next paragraph are taken", "text": "from Table 2 in Baroni et al. (2012). The algorithm JMTH Accuracy KDSZ Accuracy BBDS AccuracybalAPinc 57.3 58.2 68.7 ConVecs 70.2 56.1 65.1 SimDiffs 72.4 57.4 74.5 dataset) was less similar to the BBDS dataset than their own independent dataset, which made our different setup more difficult than theirs. Nevertheless, the accuracies are closer than one might expect given the differences in the setups."}, {"heading": "9 Discussion of results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "10 Limitations and future work", "text": "We have evaluated RLE directly, but most applications would use RLE as a module within a larger system. Future work will be necessary to show that our results can predict with a direct evaluation how an RLE module will be performed as part of a larger system. Although SimDiffs performs better than the competition, there is much room for improvement in RTE performance. Based on this proposed future work, Shnarch, Barak and Dagan are given in the contexts of the sentences."}, {"heading": "11 Conclusion", "text": "In this paper, we evaluated three different algorithms for RLE on three different datasets. Each algorithm is based on a different hypothesis about lexical relationships. We found that SimDiffs performs best in two of the three datasets. In the third dataset, there is no significant difference between the three algorithms. SimDiffs \"performance suggests that similarity differences are useful traits for recognizing lexical relationships. We approach lexical relationships as a supervised learning problem of semantic relationships. The results suggest that this is a promising approach for lexical relationships, which builds a bridge between the study of lexical relationships and semantic relationships. We hope that this link will strengthen research in both areas."}, {"heading": "Acknowledgements", "text": "Thanks to Lili Kotlerman, Ido Dagan, Idan Szpektor and Maayan ZhitomirskyGeffet for providing a copy of the KDSZ dataset and answering questions. Thanks to Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do and Chung-chieh Shan forNatural Language Engineering for their very helpful comments."}], "references": [{"title": "An Introduction to Categorical Data Analysis", "author": ["A. Agresti"], "venue": "Wiley, New York, NY.", "citeRegEx": "Agresti,? 1996", "shortCiteRegEx": "Agresti", "year": 1996}, {"title": "Using hypernymy acquisition to tackle (part of) textual entailment", "author": ["E. Akhmatova", "M. Dras"], "venue": "Proceedings of the 2009 Workshop on Applied Textual Inference at ACL-IJCNLP 2009, pp. 52\u201360, Suntec, Singapore.", "citeRegEx": "Akhmatova and Dras,? 2009", "shortCiteRegEx": "Akhmatova and Dras", "year": 2009}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["I. Androutsopoulos", "P. Malakasiotis"], "venue": "Journal of Artificial Intelligence Research, 38, 135\u2013187.", "citeRegEx": "Androutsopoulos and Malakasiotis,? 2010", "shortCiteRegEx": "Androutsopoulos and Malakasiotis", "year": 2010}, {"title": "Entailment above the word level in distributional semantics", "author": ["M. Baroni", "R. Bernardi", "Do", "N.-Q.", "C. Shan"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pp. 23\u201332, Avignon, France.", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Cognitive and Psychometric Analysis of Analogical Problem Solving", "author": ["I.I. Bejar", "R. Chaffin", "S.E. Embretson"], "venue": "Springer-Verlag, New York, NY.", "citeRegEx": "Bejar et al\\.,? 1991", "shortCiteRegEx": "Bejar et al\\.", "year": 1991}, {"title": "Evaluating evaluation measure stability", "author": ["C. Buckley", "E. Voorhees"], "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 33\u201340, Athens, Greece. ACM.", "citeRegEx": "Buckley and Voorhees,? 2000", "shortCiteRegEx": "Buckley and Voorhees", "year": 2000}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods, 39(3), 510\u2013526.", "citeRegEx": "Bullinaria and Levy,? 2007", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods, 44, 890\u2013907.", "citeRegEx": "Bullinaria and Levy,? 2012", "shortCiteRegEx": "Bullinaria and Levy", "year": 2012}, {"title": "Efficiency vs", "author": ["S. B\u00fcttcher", "C. Clarke"], "venue": "effectiveness in terabyte-scale information retrieval. In Proceedings of the 14th Text REtrieval Conference (TREC 2005), Gaithersburg, MD.", "citeRegEx": "B\u00fcttcher and Clarke,? 2005", "shortCiteRegEx": "B\u00fcttcher and Clarke", "year": 2005}, {"title": "Experiments with LSA scoring: Optimal rank and basis", "author": ["J. Caron"], "venue": "In Proceedings of the SIAM Computational Information Retrieval Workshop,", "citeRegEx": "Caron,? \\Q2001\\E", "shortCiteRegEx": "Caron", "year": 2001}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering, 15(4), i\u2013xvii.", "citeRegEx": "Dagan et al\\.,? 2009", "shortCiteRegEx": "Dagan et al\\.", "year": 2009}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177\u2013190, New York, NY. Springer.", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Constraints based taxonomic relation classification", "author": ["Q.X. Do", "D. Roth"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pp. 1099\u20131109, Cambridge, MA.", "citeRegEx": "Do and Roth,? 2010", "shortCiteRegEx": "Do and Roth", "year": 2010}, {"title": "Exploiting the Wikipedia structure in local and global classification of taxonomic relations", "author": ["Q.X. Do", "D. Roth"], "venue": "Natural Language Engineering, 18(2), 235\u2013262.", "citeRegEx": "Do and Roth,? 2012", "shortCiteRegEx": "Do and Roth", "year": 2012}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["J.R. Firth"], "venue": "Studies in Linguistic Analysis, pp. 1\u201332. Blackwell, Oxford.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["M. Geffet", "I. Dagan"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005), pp. 107\u2013114, Ann Arbor, MI.", "citeRegEx": "Geffet and Dagan,? 2005", "shortCiteRegEx": "Geffet and Dagan", "year": 2005}, {"title": "SemEval-2007 Task 4: Classification of semantic relations between nominals", "author": ["R. Girju", "P. Nakov", "V. Nastase", "S. Szpakowicz", "P. Turney", "D. Yuret"], "venue": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval 2007), pp. 13\u201318, Prague, Czech Republic.", "citeRegEx": "Girju et al\\.,? 2007", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Lexical reference: A semantic matching subtask", "author": ["O. Glickman", "I. Dagan", "E. Shnarch"], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pp. 172\u2013179, Sydney, Australia.", "citeRegEx": "Glickman et al\\.,? 2006", "shortCiteRegEx": "Glickman et al\\.", "year": 2006}, {"title": "Matrix Computations (Third edition)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press, Baltimore, MD.", "citeRegEx": "Golub and Loan,? 1996", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, 10(23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M. Hearst"], "venue": "Proceedings of the 14th Conference on Computational Linguistics (COLING92), pp. 539\u2013545, Nantes, France.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["I. Hendrickx", "S.N. Kim", "Z. Kozareva", "P. Nakov", "D.O. S\u00e9aghdha", "S. Pad\u00f3", "M. Pennacchiotti", "L. Romano", "S. Szpakowicz"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 33\u201338, Uppsala, Sweden.", "citeRegEx": "Hendrickx et al\\.,? 2010", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Textual entailment recognition based on dependency analysis and WordNet", "author": ["J. Herrera", "A. Pe\u00f1as", "F. Verdejo"], "venue": "Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, Vol. 3944 of Lecture Notes in Computer Science, pp. 231\u2013 239. Springer.", "citeRegEx": "Herrera et al\\.,? 2006", "shortCiteRegEx": "Herrera et al\\.", "year": 2006}, {"title": "Recognizing textual entailment with LCC\u2019s GROUNDHOG system", "author": ["A. Hickl", "J. Bensley", "J. Williams", "K. Roberts", "B. Rink", "Y. Shi"], "venue": "Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment, Venice, Italy.", "citeRegEx": "Hickl et al\\.,? 2006", "shortCiteRegEx": "Hickl et al\\.", "year": 2006}, {"title": "Metalogic: An Introduction to the Metatheory of Standard First Order Logic", "author": ["G. Hunter"], "venue": "University of California Press, Berkeley, CA.", "citeRegEx": "Hunter,? 1996", "shortCiteRegEx": "Hunter", "year": 1996}, {"title": "SemEval-2012 Task 2: Measuring degrees of relational similarity", "author": ["D.A. Jurgens", "S.M. Mohammad", "P.D. Turney", "K.J. Holyoak"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pp. 356\u2013364, Montr\u00e9al, Canada.", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Directional distributional similarity for lexical inference", "author": ["L. Kotlerman", "I. Dagan", "I. Szpektor", "M. Zhitomirsky-Geffet"], "venue": "Natural Language Engineering, 16(4), 359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Handbook of Latent Semantic Analysis", "author": ["T.K. Landauer", "D.S. McNamara", "S. Dennis", "W. Kintsch"], "venue": "Lawrence Erlbaum, Mahwah, NJ.", "citeRegEx": "Landauer et al\\.,? 2007", "shortCiteRegEx": "Landauer et al\\.", "year": 2007}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp. 25\u201332, College Park, MD.", "citeRegEx": "Lee,? 1999", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Automatic retrieval and clustering of similar words", "author": ["D. Lin"], "venue": "Proceedings of the 17th international conference on Computational linguistics, pp. 768\u2013774, Montreal, Quebec, Canada. Association for Computational Linguistics.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "DIRT \u2013 discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001, pp. 323\u2013328, San Francisco, CA.", "citeRegEx": "Lin and Pantel,? 2001", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Manning and Sch\u00fctze,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "Bar-ilan university\u2019s submission to rte-5", "author": ["S. Mirkin", "R. Bar-Haim", "J. Berant", "I. Dagan", "E. Shnarch", "A. Stern", "I. Szpektor"], "venue": "TAC 2009, Gaithersburg, MD.", "citeRegEx": "Mirkin et al\\.,? 2009a", "shortCiteRegEx": "Mirkin et al\\.", "year": 2009}, {"title": "Evaluating the inferential utility of lexical-semantic resources", "author": ["S. Mirkin", "I. Dagan", "E. Shnarch"], "venue": "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pp. 558\u2013566, Athens, Greece.", "citeRegEx": "Mirkin et al\\.,? 2009b", "shortCiteRegEx": "Mirkin et al\\.", "year": 2009}, {"title": "Non-classical lexical semantic relations", "author": ["J. Morris", "G. Hirst"], "venue": "Workshop on Computational Lexical Semantics, HLT-NAACL-04, Boston, MA.", "citeRegEx": "Morris and Hirst,? 2004", "shortCiteRegEx": "Morris and Hirst", "year": 2004}, {"title": "Exploring noun-modifier semantic relations", "author": ["V. Nastase", "S. Szpakowicz"], "venue": "Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5), pp. 285\u2013301, Tilburg, The Netherlands.", "citeRegEx": "Nastase and Szpakowicz,? 2003", "shortCiteRegEx": "Nastase and Szpakowicz", "year": 2003}, {"title": "Basic English: A General Introduction with Rules and Grammar", "author": ["C.K. Ogden"], "venue": "Kegan Paul, Trench, Trubner and Co., London.", "citeRegEx": "Ogden,? 1930", "shortCiteRegEx": "Ogden", "year": 1930}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22, 1345\u20131359.", "citeRegEx": "Pan and Yang,? 2010", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "WordNet::Similarity \u2013 Measuring the relatedness of concepts", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "Demonstration Papers at HLT-NAACL 2004, pp. 38\u201341, Boston, MA.", "citeRegEx": "Pedersen et al\\.,? 2004", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods: Support Vector Learning, pp. 185\u2013208, Cambridge, MA. MIT Press.", "citeRegEx": "Platt,? 1998", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Classifying the semantic relations in nouncompounds via a domain-specific lexical hierarchy", "author": ["B. Rosario", "M. Hearst"], "venue": "Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP01), pp. 82\u201390, Pittsburgh, PA.", "citeRegEx": "Rosario and Hearst,? 2001", "shortCiteRegEx": "Rosario and Hearst", "year": 2001}, {"title": "The descent of hierarchy, and selection in relational semantics", "author": ["B. Rosario", "M. Hearst", "C. Fillmore"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pp. 247\u2013254, Philadelphia, PA.", "citeRegEx": "Rosario et al\\.,? 2002", "shortCiteRegEx": "Rosario et al\\.", "year": 2002}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": "McGraw-Hill, New York, NY.", "citeRegEx": "Salton and McGill,? 1983", "shortCiteRegEx": "Salton and McGill", "year": 1983}, {"title": "Extracting lexical reference rules", "author": ["E. Shnarch", "L. Barak", "I. Dagan"], "venue": null, "citeRegEx": "Shnarch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shnarch et al\\.", "year": 2009}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pp. 801\u2013808, Sydney, NSW, Australia.", "citeRegEx": "Snow et al\\.,? 2006", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Learning entailment rules for unary templates", "author": ["I. Szpektor", "I. Dagan"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pp. 849\u2013856, Manchester, UK.", "citeRegEx": "Szpektor and Dagan,? 2008", "shortCiteRegEx": "Szpektor and Dagan", "year": 2008}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3), 379\u2013416.", "citeRegEx": "Turney,? 2006", "shortCiteRegEx": "Turney", "year": 2006}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research, 44, 533\u2013585.", "citeRegEx": "Turney,? 2012", "shortCiteRegEx": "Turney", "year": 2012}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 680\u2013690, Edinburgh, UK.", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research, 37, 141\u2013188.", "citeRegEx": "Turney and Pantel,? 2010", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "A general framework for distributional similarity", "author": ["J. Weeds", "D. Weir"], "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2003), pp. 81\u201388, Sapporo, Japan.", "citeRegEx": "Weeds and Weir,? 2003", "shortCiteRegEx": "Weeds and Weir", "year": 2003}, {"title": "Characterising measures of lexical distributional similarity", "author": ["J. Weeds", "D. Weir", "D. McCarthy"], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING \u201904), pp. 1015\u20131021, Geneva, Switzerland.", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Third Edition", "author": ["I.H. Witten", "E. Frank", "M.A. Hall"], "venue": "Morgan Kaufmann, San Francisco.", "citeRegEx": "Witten et al\\.,? 2011", "shortCiteRegEx": "Witten et al\\.", "year": 2011}, {"title": "Bootstrapping distributional feature vector quality", "author": ["M. Zhitomirsky-Geffet", "I. Dagan"], "venue": "Computational Linguistics, 35(3), 435\u2013461.", "citeRegEx": "Zhitomirsky.Geffet and Dagan,? 2009", "shortCiteRegEx": "Zhitomirsky.Geffet and Dagan", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Recognizing textual entailment (RTE) is a popular task in natural language processing research, due to its relevance for text summarization, information retrieval, information extraction, question answering, machine translation, paraphrasing, and other applications (Androutsopoulos and Malakasiotis 2010).", "startOffset": 266, "endOffset": 305}, {"referenceID": 10, "context": "The text entails the hypothesis if the meaning of the hypothesis can be inferred from the meaning of the text, according to typical human interpretations of the text and the hypothesis (Dagan et al. 2009).", "startOffset": 185, "endOffset": 204}, {"referenceID": 15, "context": "In many cases, to recognize when one sentence entails another, we must first be able to recognize when one word entails another (Geffet and Dagan 2005).", "startOffset": 128, "endOffset": 151}, {"referenceID": 49, "context": ") Vector space models (VSMs) of semantics have been particularly useful for lexical semantics (Turney and Pantel 2010), hence it is natural to apply them to RLE.", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": "These models are inspired by the distributional hypothesis (Harris 1954; Firth 1957):", "startOffset": 59, "endOffset": 84}, {"referenceID": 14, "context": "These models are inspired by the distributional hypothesis (Harris 1954; Firth 1957):", "startOffset": 59, "endOffset": 84}, {"referenceID": 15, "context": "The idea is to design a measure that captures the context inclusion hypothesis (Geffet and Dagan 2005):", "startOffset": 79, "endOffset": 102}, {"referenceID": 15, "context": "This is our paraphrase of what Geffet and Dagan (2005) call the distributional inclusion hypothesis.", "startOffset": 31, "endOffset": 55}, {"referenceID": 24, "context": "The context inclusion hypothesis is inspired by model theory in formal logic (Hunter 1996).", "startOffset": 77, "endOffset": 90}, {"referenceID": 40, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 35, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 46, "context": "Semantic relation classification is the task of learning to recognize when a word pair is an instance of a given semantic relation class (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju, Nakov, Nastase, Szpakowicz, Turney, and Yuret 2007).", "startOffset": 137, "endOffset": 299}, {"referenceID": 20, "context": "There is a relatively large body of work on semantic relation classification in general, with good results on the hyponym\u2013 hypernym relation in particular (Hearst 1992; Snow, Jurafsky, and Ng 2006).", "startOffset": 155, "endOffset": 197}, {"referenceID": 3, "context": "ConVecs is based on the context combination hypothesis (Baroni et al. 2012):", "startOffset": 55, "endOffset": 75}, {"referenceID": 3, "context": "This algorithm was not given a name by Baroni et al. (2012). For ease of reference, we will call it ConVecs (concatenated vectors).", "startOffset": 39, "endOffset": 60}, {"referenceID": 3, "context": "This hypothesis was not explicitly stated by Baroni et al. (2012) but it is implicit in their approach.", "startOffset": 45, "endOffset": 66}, {"referenceID": 40, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 41, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 35, "context": "for supervised learning with word pairs (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003).", "startOffset": 40, "endOffset": 115}, {"referenceID": 25, "context": "We use the datasets of Kotlerman et al. (2010), Baroni et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 3, "context": "(2010), Baroni et al. (2012), and Jurgens, Mohammad, Turney, and Holyoak (2012).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "(2010), Baroni et al. (2012), and Jurgens, Mohammad, Turney, and Holyoak (2012). The experimental results are reported in Section 8.", "startOffset": 8, "endOffset": 80}, {"referenceID": 29, "context": "Some people might feel that lion and cat suggest either the hyponym\u2013hypernym relation (assuming cat means feline) or the coordinate relation (assuming that cat means house cat). Coordinates are words with a shared hypernym. Lion and house cat share the hypernym feline. If cat means house cat, then lion and cat are coordinates. A hyponym implies its hypernym, but coordinates do not imply each other. Lion implies cat in the feline sense but not in the house cat sense. Thus these two relations (hyponym\u2013hypernym and coordinate) do not agree on whether lion implies cat. In this case, we believe that the hyponym\u2013hypernym is more natural, so we say that lion implies cat. For people who feel both semantic relations are natural, the third condition says that there is no entailment; for them, lion does not imply cat. The third condition could be modified for different uses. For our dataset (Section 7.3), we chose to err on the side of non-entailment, but ideally the choice would be made based on the downstream application. For some applications, it may be better to err on the side of entailment. One possibility is to give higher weight to some relations and use the weighting to choose between entailment and nonentailment when two or more relations disagree. The weighting could be based on the corpus frequency of the relations or the contexts in which the words appear. To apply the relational definition of lexical entailment, it is helpful to have a taxonomy of semantic relations, to provide options for R. In this paper, we use the taxonomy of Bejar, Chaffin, and Embretson (1991), which includes seventynine subcategories of semantic relations, grouped into ten high-level categories.", "startOffset": 108, "endOffset": 1596}, {"referenceID": 15, "context": "Zhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition of lexical entailment was intended to capture only substitutional cases of entailment.", "startOffset": 12, "endOffset": 38}, {"referenceID": 15, "context": "One limitation of substitutability as defined by Zhitomirsky-Geffet and Dagan (2009) is that it does not allow lexical entailment from one part of speech to another.", "startOffset": 61, "endOffset": 85}, {"referenceID": 30, "context": "Patterns like this have been learned from corpora (Lin and Pantel 2001) and applied successfully to RTE (Mirkin, Bar-Haim, Berant, Dagan, Shnarch, Stern, and Szpektor 2009a).", "startOffset": 50, "endOffset": 71}, {"referenceID": 15, "context": ", for synonyms) (Geffet and Dagan 2005; Kotlerman et al. 2010).", "startOffset": 16, "endOffset": 62}, {"referenceID": 26, "context": ", for synonyms) (Geffet and Dagan 2005; Kotlerman et al. 2010).", "startOffset": 16, "endOffset": 62}, {"referenceID": 14, "context": "1) was labeled using Zhitomirsky-Geffet and Dagan\u2019s (2009) substitutional definition.", "startOffset": 33, "endOffset": 59}, {"referenceID": 4, "context": "3), the pairs were generated from Bejar et al.\u2019s (1991) taxonomy.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "Some researchers have applied semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Baroni et al. 2012), but Zhitomirsky-Geffet and Dagan", "startOffset": 85, "endOffset": 130}, {"referenceID": 3, "context": "Some researchers have applied semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Baroni et al. 2012), but Zhitomirsky-Geffet and Dagan", "startOffset": 85, "endOffset": 130}, {"referenceID": 14, "context": "We agree with Zhitomirsky-Geffet and Dagan (2009) that some sub-cases of part\u2013 whole involve lexical entailment and other sub-cases do not.", "startOffset": 26, "endOffset": 50}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories.", "startOffset": 36, "endOffset": 58}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories. We claim that eight of the ten subcategories involve entailment and two do not involve entailment, which is consistent with the claim that \u2018lexical entailment does not cover all cases of meronyms\u2019 (in the above quotation). Regarding \u2018ocean and water and murder and death\u2019 (in the above quotation), the word pair ocean:water is an instance of Bejar et al.\u2019s (1991) object:stuff subcategory (ID 2g in the taxonomy) and murder:death is an instance of the cause:effect subcategory (ID 8a).", "startOffset": 36, "endOffset": 498}, {"referenceID": 4, "context": "One of the high-level categories in Bejar et al.\u2019s (1991) taxonomy is part\u2013whole (ID 2 in the taxonomy), which has ten subcategories. We claim that eight of the ten subcategories involve entailment and two do not involve entailment, which is consistent with the claim that \u2018lexical entailment does not cover all cases of meronyms\u2019 (in the above quotation). Regarding \u2018ocean and water and murder and death\u2019 (in the above quotation), the word pair ocean:water is an instance of Bejar et al.\u2019s (1991) object:stuff subcategory (ID 2g in the taxonomy) and murder:death is an instance of the cause:effect subcategory (ID 8a). Regarding relations for which there is lexical entailment in both directions, synonymy (ID 3a) is readily handled by marking it as entailing in both directions (see Tables 2 and 3 in Section 7.3). We believe that Zhitomirsky-Geffet and Dagan\u2019s (2009) argument is correct for high-level categories but incorrect for subcategories.", "startOffset": 36, "endOffset": 871}, {"referenceID": 4, "context": "In our experiments (Section 8), we train the algorithms using data based on Bejar et al.\u2019s (1991) taxonomy and then test them on previous lexical entailment datasets.", "startOffset": 76, "endOffset": 98}, {"referenceID": 4, "context": "In our experiments (Section 8), we train the algorithms using data based on Bejar et al.\u2019s (1991) taxonomy and then test them on previous lexical entailment datasets. We do not claim that Bejar et al.\u2019s (1991) taxonomy handles all cases of lexical entailment, but our results suggest that it covers enough cases to be effective.", "startOffset": 76, "endOffset": 210}, {"referenceID": 4, "context": "work may discover lexical entailments that do not fit readily in Bejar et al.\u2019s (1991) taxonomy, but we believe that the taxonomy can be expanded to handle exceptions as they are discovered.", "startOffset": 65, "endOffset": 87}, {"referenceID": 42, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 96, "endOffset": 120}, {"referenceID": 29, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 138, "endOffset": 148}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005).", "startOffset": 348, "endOffset": 371}, {"referenceID": 15, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 45, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 53, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 26, "context": "This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010).", "startOffset": 182, "endOffset": 288}, {"referenceID": 40, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 41, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 35, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 46, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 16, "context": "Compared to the number of papers on lexical entailment, there is a relatively large body of literature on semantic relation classification (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007).", "startOffset": 139, "endOffset": 246}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence.", "startOffset": 349, "endOffset": 384}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004).", "startOffset": 349, "endOffset": 584}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004). This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010). We describe balAPinc in detail in Section 6.1. Glickman, Dagan, and Shnarch (2006) define lexical reference, which is somewhat similar to lexical entailment, but it is defined relative to a specific text, such as a sentence.", "startOffset": 349, "endOffset": 1123}, {"referenceID": 15, "context": "The early RLE modules typically used a symmetric similarity measure, such as the cosine measure (Salton and McGill 1983), the LIN measure (Lin 1998), or a measure based on WordNet (Pedersen, Patwardhan, and Michelizzi 2004), but it was understood that entailment is inherently asymmetric and any symmetric measure can only be a rough approximation (Geffet and Dagan 2005). Lee (1999) proposed an asymmetric similarity measure for the degree to which a word a can be replaced by a word b in a sentence, without substantially changing the meaning of the sentence. Weeds and Weir (2003) introduced an asymmetric similarity measure for the degree to which a specific term a is subsumed by a more general term b (see also Weeds, Weir, and McCarthy 2004). This idea was developed further, specifically for application to lexical entailment, in a series of papers that culminated in the balAPinc measure of the degree to which a entails b (Geffet and Dagan 2005; Szpektor and Dagan 2008; Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010). We describe balAPinc in detail in Section 6.1. Glickman, Dagan, and Shnarch (2006) define lexical reference, which is somewhat similar to lexical entailment, but it is defined relative to a specific text, such as a sentence. Mirkin, Dagan, and Shnarch (2009b) define entailment between lexical elements, which includes entailment between words and non-compositional elements.", "startOffset": 349, "endOffset": 1300}, {"referenceID": 16, "context": "\u2022 SemEval-2007 Task 4: Classification of Semantic Relations between Nominals (Girju et al. 2007) \u2013 seven semantic relation classes \u2022 SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals (Hendrickx, Kim, Kozareva, Nakov, S\u00e9aghdha, Pad\u00f3,", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": "\u2022 SemEval-2012 Task 2: Measuring Degrees of Relational Similarity (Jurgens et al. 2012) \u2013 seventy-nine semantic relation classes", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 12, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 3, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 13, "context": "Only a few papers apply semantic relation classification to lexical entailment (Akhmatova and Dras 2009; Do and Roth 2010; Baroni et al. 2012; Do and Roth 2012).", "startOffset": 79, "endOffset": 160}, {"referenceID": 40, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 41, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 35, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 46, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 16, "context": "Most algorithms for semantic relation classification are supervised (Rosario and Hearst 2001; Rosario et al. 2002; Nastase and Szpakowicz 2003; Turney 2006; Girju et al. 2007), although some are not (Hearst 1992).", "startOffset": 68, "endOffset": 175}, {"referenceID": 20, "context": "2007), although some are not (Hearst 1992).", "startOffset": 29, "endOffset": 42}, {"referenceID": 25, "context": "\u2019s (1991) classes using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012).", "startOffset": 49, "endOffset": 70}, {"referenceID": 25, "context": "We use average precision (AP) as a performance measure for real-valued scores, following Kotlerman et al. (2010). We use precision, recall, F-measure, and accuracy as performance measures for binary-valued classification, following Baroni et al.", "startOffset": 89, "endOffset": 113}, {"referenceID": 3, "context": "We use precision, recall, F-measure, and accuracy as performance measures for binary-valued classification, following Baroni et al. (2012). The balAPinc measure (balanced average precision for distributional inclusion) is", "startOffset": 118, "endOffset": 139}, {"referenceID": 5, "context": "AP is defined as follows (Buckley and Voorhees 2000):", "startOffset": 25, "endOffset": 52}, {"referenceID": 5, "context": "Buckley and Voorhees (2000) demonstrate that AP is more stable and more discriminating than several alternative performance measures for information retrieval systems.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Buckley and Voorhees (2000) demonstrate that AP is more stable and more discriminating than several alternative performance measures for information retrieval systems. The definition of AP reflects a bias in information retrieval. For a typical query and a typical document collection, most documents are irrelevant and the emphasis is on finding the few relevant documents. In machine learning, if we have two classes, 0 and 1, they are usually considered equally important. Kotlerman et al. (2010) emphasize the class 1 (entails), but we believe class 0 (does not entail) is also important.", "startOffset": 0, "endOffset": 500}, {"referenceID": 26, "context": "In their experiments, Kotlerman et al. (2010) report only AP1.", "startOffset": 22, "endOffset": 46}, {"referenceID": 46, "context": "For an introduction to the concepts behind word\u2013context matrices, see the survey paper by Turney and Pantel (2010).", "startOffset": 90, "endOffset": 115}, {"referenceID": 46, "context": "For both balAPinc and ConVecs, we chose the word\u2013context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word\u2013context matrices from Turney (2012).", "startOffset": 69, "endOffset": 109}, {"referenceID": 46, "context": "For both balAPinc and ConVecs, we chose the word\u2013context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word\u2013context matrices from Turney (2012). ConVecs and SimDiffs use support vector machines (SVMs) for supervised learning.", "startOffset": 69, "endOffset": 178}, {"referenceID": 26, "context": "The balAPinc asymmetric similarity measure is a balanced combination of the asymmetric APinc measure (Kotlerman et al. 2010) with the symmetric LIN measure (Lin 1998).", "startOffset": 101, "endOffset": 124}, {"referenceID": 29, "context": "2010) with the symmetric LIN measure (Lin 1998).", "startOffset": 37, "endOffset": 47}, {"referenceID": 26, "context": "We include balAPinc in our experiments because Kotlerman et al. (2010) experimentally compared it with a wide range of asymmetric similarity measures and found that balAPinc had the best performance.", "startOffset": 47, "endOffset": 71}, {"referenceID": 26, "context": "Kotlerman et al. (2010) define balAPinc with terminology from set theory, whereas ConVecs", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "We will use the set theoretical terminology of Kotlerman et al. (2010) and the linear algebraic terminology of Turney and Pantel (2010), so that the reader can easily see both perspectives.", "startOffset": 47, "endOffset": 71}, {"referenceID": 26, "context": "We will use the set theoretical terminology of Kotlerman et al. (2010) and the linear algebraic terminology of Turney and Pantel (2010), so that the reader can easily see both perspectives.", "startOffset": 47, "endOffset": 136}, {"referenceID": 6, "context": "Let the matrix X be the result of calculating the positive pointwise mutual information (PPMI) between the word w and the context c for each element fij in F (Bullinaria and Levy 2007; Turney and Pantel 2010).", "startOffset": 158, "endOffset": 208}, {"referenceID": 49, "context": "Let the matrix X be the result of calculating the positive pointwise mutual information (PPMI) between the word w and the context c for each element fij in F (Bullinaria and Levy 2007; Turney and Pantel 2010).", "startOffset": 158, "endOffset": 208}, {"referenceID": 49, "context": "The value of an element xij in X is defined as follows (Turney and Pantel 2010):", "startOffset": 55, "endOffset": 79}, {"referenceID": 29, "context": "4 ConVecs and SimDiffs are fundamentally linear algebraic in conception, whereas balAPinc is fundamentally set theoretic. We cannot readily describe all three systems with only one kind of notation. 5 Other measures of word association may be used instead of PPMI. See Chapter 5 of Manning and Sch\u00fctze (1999) for a good survey of association measures.", "startOffset": 41, "endOffset": 309}, {"referenceID": 29, "context": ") LIN is defined as follows (Lin 1998):", "startOffset": 28, "endOffset": 38}, {"referenceID": 26, "context": "In balAPinc (Equation 16), the LIN measure is combined with the APinc measure because the APinc measure by itself tends to be sensitive to cases where |Fu| or |Fv| are unusually small (Kotlerman et al. 2010).", "startOffset": 184, "endOffset": 207}, {"referenceID": 8, "context": "The corpus was indexed with the Wumpus search engine (B\u00fcttcher and Clarke 2005), which is designed for passage retrieval, rather than document retrieval.", "startOffset": 53, "endOffset": 79}, {"referenceID": 45, "context": "In the experiments with balAPinc in Section 8, the PPMI matrix X is the same matrix as used by Turney et al. (2011). The matrix has 114,501 rows and 139,246 columns.", "startOffset": 95, "endOffset": 116}, {"referenceID": 8, "context": "The corpus was indexed with the Wumpus search engine (B\u00fcttcher and Clarke 2005), which is designed for passage retrieval, rather than document retrieval. Suppose fij is an element in the matrix of raw co-occurrence frequencies F. The i-th row of the matrix corresponds to an n-gram w in WordNet and the j-th column of the matrix corresponds to a unigram c. The value of fij was calculated by sending the query w to Wumpus and counting the frequency of c in the retrieved passages. The matrix is described in detail in Section 2.1 of Turney et al. (2011).", "startOffset": 54, "endOffset": 554}, {"referenceID": 49, "context": "It is common to smooth the PPMI matrix by applying a truncated singular value decomposition (SVD) (Turney and Pantel 2010).", "startOffset": 98, "endOffset": 122}, {"referenceID": 3, "context": "Baroni et al. (2012) also found that balAPinc works better without SVD smoothing (see their Footnote 3).", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc. In ConVecs, we represent a word pair a :b by the concatentation of the context vectors a for a and b for b. We apply a supervised learning algorithm to a training set of word pairs, where each word pair is represented by concatenated context vectors that are labeled entails or does not entail. The supervised learning algorithm generates a classification model, which enables us to assign labels to new word pairs, not present in the training data. Let X be a word\u2013context matrix, where the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments, we use the word\u2013context matrix X from Turney et al. (2011), as in Section 6.", "startOffset": 28, "endOffset": 764}, {"referenceID": 3, "context": "With the ConVecs algorithm, Baroni et al. (2012) were able to match the performance of balAPinc. In ConVecs, we represent a word pair a :b by the concatentation of the context vectors a for a and b for b. We apply a supervised learning algorithm to a training set of word pairs, where each word pair is represented by concatenated context vectors that are labeled entails or does not entail. The supervised learning algorithm generates a classification model, which enables us to assign labels to new word pairs, not present in the training data. Let X be a word\u2013context matrix, where the value of the cell xij in X is given by the PPMI between the i-th word w and the j-th context c. In our experiments, we use the word\u2013context matrix X from Turney et al. (2011), as in Section 6.1, but now we smooth X with a truncated SVD. SVD decomposes X into the product of three matrices U\u03a3V, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length, UU = VV = I) and \u03a3 is a diagonal matrix of singular values (Golub and Van Loan 1996). If X is of rank r, then \u03a3 is also of rank r. Let \u03a3k, where k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by selecting the corresponding columns from U and V. The matrix Uk\u03a3kV T k is the matrix of rank k that best approximates the original matrix X, in that it minimizes the approximation errors. That is, X\u0302 = Uk\u03a3kV T k minimizes \u2016X\u0302\u2212X\u2016F over all matrices X\u0302 of rank k, where \u2016 . . . \u2016F denotes the Frobenius norm (Golub and Van Loan 1996). We represent a word pair a : b using row vectors from the matrix Uk\u03a3 p k. If a and b correspond to row vectors a and b in Uk\u03a3 p k, then a : b is represented by the 2k-dimensional vector that is the concatenation of a and b. We normalize a and b to unit length before we concatenate them. There are two parameters in Uk\u03a3 p k that need to be set. The parameter k controls the number of latent factors and the parameter p adjusts the weights of the factors, by raising the corresponding singular values in \u03a3pk to the power p. The parameter k is well-known in the literature (Landauer, McNamara, Dennis, and Kintsch 2007), but p is less familiar. Caron (2001) introduced p for improving the performance of truncated SVD with term\u2013document matrices in information retrieval.", "startOffset": 28, "endOffset": 2233}, {"referenceID": 5, "context": "lexical semantics is supported by the empirical evaluations of Bullinaria and Levy (2012) and Turney (2012).", "startOffset": 63, "endOffset": 90}, {"referenceID": 5, "context": "lexical semantics is supported by the empirical evaluations of Bullinaria and Levy (2012) and Turney (2012). In the following experiments (Section 8), we explore a range of values for p and k.", "startOffset": 63, "endOffset": 108}, {"referenceID": 3, "context": "Baroni et al. (2012) use k = 300 and p = 1.", "startOffset": 0, "endOffset": 21}, {"referenceID": 39, "context": "We also use Weka and a polynomial kernel, but we use the sequential minimal optimization (SMO) SVM in Weka (Platt 1998), because it can generate real-valued probability estimates, as well as binary-valued classes.", "startOffset": 107, "endOffset": 119}, {"referenceID": 52, "context": "The probability estimates are based on fitting the outputs of the SVM with logistic regression models (Witten et al. 2011).", "startOffset": 102, "endOffset": 122}, {"referenceID": 3, "context": "For their supervised learning algorithm, Baroni et al. (2012) used Weka with LIBSVM.", "startOffset": 41, "endOffset": 62}, {"referenceID": 47, "context": "SimDiffs uses two different word\u2013context matrices, a domain matrix, D, and a function matrix, F (Turney 2012).", "startOffset": 96, "endOffset": 109}, {"referenceID": 3, "context": "8 Baroni et al. (2012) mention k = 300 in their Footnote 3.", "startOffset": 2, "endOffset": 23}, {"referenceID": 46, "context": "Turney (2012) demonstrated that domain and function matrices work together synergetically when applied to semantic relations.", "startOffset": 0, "endOffset": 14}, {"referenceID": 46, "context": "In experiments with the development datasets (Dev1 and Dev2), we tried using the domain and function matrices with balAPinc and ConVecs, but both algorithms worked better with the word\u2013context matrix from Turney et al. (2011). For SimDiffs, the combination of the domain and function matrices from Turney (2012) had the best performance on the development datasets.", "startOffset": 205, "endOffset": 226}, {"referenceID": 46, "context": "In experiments with the development datasets (Dev1 and Dev2), we tried using the domain and function matrices with balAPinc and ConVecs, but both algorithms worked better with the word\u2013context matrix from Turney et al. (2011). For SimDiffs, the combination of the domain and function matrices from Turney (2012) had the best performance on the development datasets.", "startOffset": 205, "endOffset": 312}, {"referenceID": 46, "context": "The domain and function matrices are based on the same corpus as the word\u2013 context matrix from Turney et al. (2011). Wumpus was used to index the corpus and search for passages, in the same way as described in Section 6.", "startOffset": 95, "endOffset": 116}, {"referenceID": 46, "context": "The columns are more complex; Turney (2012) provides a detailed description of the columns and other aspects of the matrices.", "startOffset": 30, "endOffset": 44}, {"referenceID": 15, "context": "Consider the example murder |= death, suggested by the quotation from Zhitomirsky-Geffet and Dagan (2009) in Section 3.", "startOffset": 82, "endOffset": 106}, {"referenceID": 36, "context": "For R, the set of reference words, we use 2,086 words from Basic English (Ogden 1930).", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "For R, the set of reference words, we use 2,086 words from Basic English (Ogden 1930). Thus a word pair a : b is represented by 2,086 \u00d7 4 = 8,344 features. The words of Basic English were selected by Ogden (1930) to form a core vocabulary, sufficient to represent most other English words by paraphrasing.", "startOffset": 74, "endOffset": 213}, {"referenceID": 25, "context": "The KDSZ dataset was introduced by Kotlerman et al. (2010) to evaluate balAPinc.", "startOffset": 35, "endOffset": 59}, {"referenceID": 15, "context": "It was created by taking a dataset of 3,200 labeled word pairs from Zhitomirsky-Geffet and Dagan (2009) and adding 572 more labeled pairs.", "startOffset": 80, "endOffset": 104}, {"referenceID": 15, "context": "It was created by taking a dataset of 3,200 labeled word pairs from Zhitomirsky-Geffet and Dagan (2009) and adding 572 more labeled pairs. The labeling of the original subset of 3,200 pairs is described in detail by ZhitomirskyGeffet and Dagan (2009). The definition of lexical entailment that the judges used was the substitutional definition given in Section 2.", "startOffset": 80, "endOffset": 251}, {"referenceID": 3, "context": "The BBDS dataset was created by Baroni et al. (2012) and has been applied to evaluating both balAPinc and ConVecs.", "startOffset": 32, "endOffset": 53}, {"referenceID": 3, "context": "The BBDS dataset was created by Baroni et al. (2012) and has been applied to evaluating both balAPinc and ConVecs. In their paper, Baroni et al. (2012) discuss several different datasets.", "startOffset": 32, "endOffset": 152}, {"referenceID": 25, "context": "The original SemEval-2012 dataset was generated in two phases, using Amazon\u2019s Mechanical Turk (Jurgens et al. 2012).", "startOffset": 94, "endOffset": 115}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories.", "startOffset": 96, "endOffset": 116}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1.", "startOffset": 96, "endOffset": 346}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1. The first four types of information come from Bejar et al. (1991) and the rest were added by Jurgens et al.", "startOffset": 96, "endOffset": 487}, {"referenceID": 4, "context": "The original dataset consists of word pairs labeled using the relation classification scheme of Bejar et al. (1991). This is a hierarchical classification system with ten high-level categories, each of which has between five and ten subcategories, for a total of seventy-nine distinct subcategories. For each subcategory in Bejar et al.\u2019s (1991) relation taxonomy, we have several types of information, shown in Table 1. The first four types of information come from Bejar et al. (1991) and the rest were added by Jurgens et al. (2012). The original SemEval-2012 dataset was generated in two phases, using Amazon\u2019s Mechanical Turk (Jurgens et al.", "startOffset": 96, "endOffset": 536}, {"referenceID": 4, "context": "Semantic relation categories 1 to 5, based on Bejar et al. (1991)", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "Semantic relation categories 6 to 10, based on Bejar et al. (1991)", "startOffset": 47, "endOffset": 67}, {"referenceID": 15, "context": "1 that Zhitomirsky-Geffet and Dagan (2009) had inter-annotator agreements in the 90% range, whereas our agreement is 81%.", "startOffset": 19, "endOffset": 43}, {"referenceID": 15, "context": "The agreement of 89% is close to the levels reported by Zhitomirsky-Geffet and Dagan (2009). On the other hand, the number of pairs labeled entails drops from 48-51% for the relational definition to 22-25% for the substitional definition.", "startOffset": 68, "endOffset": 92}, {"referenceID": 0, "context": "4%), according to Fisher\u2019s Exact Test (Agresti 1996).", "startOffset": 38, "endOffset": 52}, {"referenceID": 46, "context": ") Let Gen (general) refer to the matrix from Turney et al. (2011) and let Dom and Fun refer to the domain and function matrices from Turney (2012).", "startOffset": 45, "endOffset": 66}, {"referenceID": 46, "context": ") Let Gen (general) refer to the matrix from Turney et al. (2011) and let Dom and Fun refer to the domain and function matrices from Turney (2012). In Section 6, we mentioned that we performed experiments on the development datasets (Dev1 and Dev2) in order to select the matrices for each algorithm.", "startOffset": 45, "endOffset": 147}, {"referenceID": 37, "context": "It is a gap that learning algorithms inevitably face in real applications (Pan and Yang 2010).", "startOffset": 74, "endOffset": 93}, {"referenceID": 26, "context": "Kotlerman et al. (2010) reported AP1 without AP0, but there is a trade-off between AP1 and AP0.", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "Kotlerman et al. (2010) reported AP1 without AP0, but there is a trade-off between AP1 and AP0. Kotlerman et al. (2010) did not attempt to evaluate balAPinc as a classifier, so they did not report precision, recall, F-measure, or accuracy.", "startOffset": 0, "endOffset": 120}, {"referenceID": 3, "context": "The BBDS data was used by Baroni et al. (2012) to compare balAPinc with ConVecs.", "startOffset": 26, "endOffset": 47}, {"referenceID": 3, "context": "The BBDS data was used by Baroni et al. (2012) to compare balAPinc with ConVecs. They used two different evaluation setups, similar to our standard and different setups. For balAPinc using a standard setup, they obtained an accuracy of 70.1%, slighly below our result of 72.2%. The difference is likely due to minor differences in the word\u2013context matrices that we used. For balAPinc using a different setup, their accuracy was 70.4%, compared to our 68.7%. They used their own independent dataset to tune balAPinc, whereas we used the JMTH dataset. Given that our word\u2013context matrices and our training data are different from theirs, the accuracies are closer than might be expected. For ConVecs using a standard setup, Baroni et al. (2012) report an accuracy of 88.", "startOffset": 26, "endOffset": 743}, {"referenceID": 3, "context": "16 These accuracy numbers and the numbers reported in the next paragraph are taken from Table 2 in Baroni et al. (2012).", "startOffset": 99, "endOffset": 120}, {"referenceID": 26, "context": "This is closer to the evaluation setup of Kotlerman et al. (2010). In this table, we do not use bold font to mark significant differences, because there is no agreement on the appropriate statistical test for AP1.", "startOffset": 42, "endOffset": 66}, {"referenceID": 17, "context": "Related to this proposed future work, Shnarch, Barak, and Dagan (2009) evaluated lexical reference rules (Glickman et al. 2006) derived from Wikipedia on the RTE-4 dataset.", "startOffset": 105, "endOffset": 127}, {"referenceID": 3, "context": "Baroni et al. (2012) have achieved promising results with quantifier phrases, such as all dogs |= some dogs.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Baroni et al. (2012) have achieved promising results with quantifier phrases, such as all dogs |= some dogs. Looking at Tables 2 and 3 in Section 7.3, we see a high density of 1\u2019s (entails) for class-inclusion and part-whole. The strong connection between these two categories and lexical entailment may explain why Morris and Hirst (2004) call hypernymy and meronymy classical relations, whereas the relation in chapel:funeral (spacetime, location:activity, ID 9c) is non-classical (this is one of their examples of a non-classical relation).", "startOffset": 0, "endOffset": 340}], "year": 2014, "abstractText": "Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, buy entails own. Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a : b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a : b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word\u2013context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Along the way, we address some of the concerns raised in past research, regarding the treatment of RLE as a problem of semantic relation classification, and we suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}