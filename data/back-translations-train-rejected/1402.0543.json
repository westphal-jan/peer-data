{"id": "1402.0543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "How Does Latent Semantic Analysis Work? A Visualisation Approach", "abstract": "By using a small example, an analogy to photographic compression, and a simple visualization using heatmaps, we show that latent semantic analysis (LSA) is able to extract what appears to be semantic meaning of words from a set of documents by blurring the distinctions between the words.", "histories": [["v1", "Mon, 3 Feb 2014 23:09:28 GMT  (147kb,D)", "http://arxiv.org/abs/1402.0543v1", "13 pages, 6 figures, 2 tables"]], "COMMENTS": "13 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jan koeman", "william rea"], "accepted": false, "id": "1402.0543"}, "pdf": {"name": "1402.0543.pdf", "metadata": {"source": "CRF", "title": "How Does Latent Semantic Analysis Work? A Visualisation Approach", "authors": ["Jan Koeman"], "emails": ["(jpk39@uclive.ac.nz),", "(bill.rea@canterbury.ac.nz,"], "sections": [{"heading": null, "text": "Keywords: latent semantic analysis, singular value decomposition, artificial intelligence, visualization"}, {"heading": "1 Introduction", "text": "Latent semantic analysis (LSA) was patented in 1988 (US patent 4,839,853) and is a widely used technique in the processing of natural language to analyse the relationships between a number of documents and the words they contain. LSA literature is extensive, see, for example, the Landuaer et al. (2011) collection and the many references it contains. This literature includes a number of excellent descriptions of LSA mathematics such as Deerwater et al. (1990), Berry et al. (1995) and Martin and Berry (2011).ar Xiv: 140 2.05 43v1 [Despite the existence of these excellent mathematical representations of how LSA works, it is not generally understood. For example, Tunkelang (2008) recently discussed three alternative hypotheses but did not come to any conclusions. We suspect the reason that the LSA works, essential knowledge of linear algebra and matrix compositions is simply required."}, {"heading": "2 Example", "text": "This year, it has come to the point where it only takes one year to find a solution."}, {"heading": "3 Conclusions", "text": "The argument put forward in this note that the LSA extracts what appears to be a semantic meaning to humans is a consequence of the blurring of the distinction between words within a corpus, an idea that was previously advanced by one of the authors of the LSA. However, by using simple visualization tools and an analogy to photographic compression, we have made this mechanism much easier to understand for those who do not have the mathematical knowledge necessary to follow the arguments of the existing literature. Document title c1: Human Machine Interface for ABC Computer Applications c2: A Survey of User Opinions on Computer System Response Time c3: The EPS User Interface Management System c4: System and Human System Engineering Tests of EPS c5: Relation of User Perceived Response Time to Error Measurement 1: Generation of Random, Binary, Ordered Trees m2: The Intersection of Paths in Trees in Width 3-4 Order Poll: A Quasi-Trees: A"}], "references": [{"title": "Using Linear Algebra for Intelligent Information Retrieval", "author": ["M.W. Berry", "S.T. Dumais", "G.W. O\u2019Brien"], "venue": "SIAM Review", "citeRegEx": "Berry et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Berry et al\\.", "year": 1995}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwater", "S.T. Dumais", "F.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science 41 (6), 391\u2013407.", "citeRegEx": "Deerwater et al\\.,? 1990", "shortCiteRegEx": "Deerwater et al\\.", "year": 1990}, {"title": "Matrix Alegbra", "author": ["J.E. Gentle"], "venue": "Springer.", "citeRegEx": "Gentle,? 2007", "shortCiteRegEx": "Gentle", "year": 2007}, {"title": "An introduction to latent semantic analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Processes 25 (2-3), 259\u2013284. http://dx.doi.org/10.1080/0168539809545028.", "citeRegEx": "Landauer et al\\.,? 1998", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "LSA as a Theory of Meaning", "author": ["T.K. Landuaer"], "venue": "T. K. Landuaer, D. S. McNamara, S. Dennis, and W. Kintsch (Eds.), Handbook of Latent Semantic Analysis, pp. 3\u201334. Routledge.", "citeRegEx": "Landuaer,? 2011", "shortCiteRegEx": "Landuaer", "year": 2011}, {"title": "Mathematical Foundations Behind Latent Semantic Analysis", "author": ["D.I. Martin", "M.W. Berry"], "venue": "T. K. Landuaer, D. S. McNamara, S. Dennis, and W. Kintsch (Eds.), Handbook of Latent Semantic Analysis, pp. 35\u201355. Routledge.", "citeRegEx": "Martin and Berry,? 2011", "shortCiteRegEx": "Martin and Berry", "year": 2011}, {"title": "Matrix Analysis and Applied Linear Algebra", "author": ["C.D. Meyer"], "venue": "SIAM.", "citeRegEx": "Meyer,? 2000", "shortCiteRegEx": "Meyer", "year": 2000}, {"title": "November). Why does latent semantic analysis work? http://thenoisychannel.com/2008/11/01/why-does-latentsemantic-analysis-work/. Accessed 7-Jan-2014", "author": ["D. Tunkelang"], "venue": null, "citeRegEx": "Tunkelang,? \\Q2008\\E", "shortCiteRegEx": "Tunkelang", "year": 2008}, {"title": "Fundamentals of Matrix Computation (Second ed.)", "author": ["D.S. Watkins"], "venue": "Wiley Inter-Science", "citeRegEx": "Watkins,? \\Q2002\\E", "shortCiteRegEx": "Watkins", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "The literature on LSA is extensive, see, for example, the book-length collection Landuaer et al. (2011) and the many references therein.", "startOffset": 81, "endOffset": 104}, {"referenceID": 0, "context": "Among this literature are a number of excellent decriptions of the mathematics of LSA such as Deerwater et al. (1990), Berry et al.", "startOffset": 94, "endOffset": 118}, {"referenceID": 0, "context": "(1990), Berry et al. (1995) and Martin and Berry (2011).", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(1990), Berry et al. (1995) and Martin and Berry (2011).", "startOffset": 8, "endOffset": 56}, {"referenceID": 5, "context": "For example, recently Tunkelang (2008), discussed three alternative hypothesis but reached no conclusions.", "startOffset": 22, "endOffset": 39}, {"referenceID": 2, "context": "4) or Gentle (2007, Sect. 7.7) among many others. To those without such knowledge the discussion of the use of singular values or eigenvalues (for details of the exact nature of the relationship between singular values and eigenvalues see Meyer (2000) p.", "startOffset": 6, "endOffset": 252}, {"referenceID": 3, "context": "Table (1) contains the data used by Landauer et al. (1998) to illustrate LSA.", "startOffset": 36, "endOffset": 59}, {"referenceID": 3, "context": "Table (1) contains the data used by Landauer et al. (1998) to illustrate LSA. Landauer et al. (1998) constructed an example in which there are two distinct concepts, human-computer interaction (documents c1-c5) and graph theory (documents m1-m4), which share only a single common word \u201csurvey\u201d.", "startOffset": 36, "endOffset": 101}, {"referenceID": 3, "context": "Table (1) contains the data used by Landauer et al. (1998) to illustrate LSA. Landauer et al. (1998) constructed an example in which there are two distinct concepts, human-computer interaction (documents c1-c5) and graph theory (documents m1-m4), which share only a single common word \u201csurvey\u201d. Frequently occurring words like \u201cand\u201d, \u201cof\u201d and \u201cthe\u201d are routinely omitted because they tend to obfuscate an LSA. Landauer et al. (1998) chose words which appear in at least two titles of their small corpus for inclusion in the LSA.", "startOffset": 36, "endOffset": 433}, {"referenceID": 4, "context": "This argument regarding blurring is essentially that put forward by Landuaer (2011) where the blurriness is compared to that which occurs in human vision when squinting.", "startOffset": 68, "endOffset": 84}, {"referenceID": 3, "context": "One of the key steps in LSA is trying to optimize the amount of blurring of the distinction between the words that is undertaken by altering the number of eigenvectors retained in the approximation, see Landauer et al. (1998) Figure (5).", "startOffset": 203, "endOffset": 226}, {"referenceID": 3, "context": "One of the key steps in LSA is trying to optimize the amount of blurring of the distinction between the words that is undertaken by altering the number of eigenvectors retained in the approximation, see Landauer et al. (1998) Figure (5). In the Landauer et al. (1998) example, the goal of an LSA would be to remove sufficient fine detail from the word-document matrix that the words in the two distinct document groups become relatively indistinguishable, yet retain enough detail that the two groups do not become merged.", "startOffset": 203, "endOffset": 268}, {"referenceID": 3, "context": "Table 1: The original document titles and word-by-content matrix from Landauer et al. (1998) Figure (1) of the titles of nine documents.", "startOffset": 70, "endOffset": 93}], "year": 2014, "abstractText": "By using a small example, an analogy to photographic compression, and a simple visualization using heatmaps, we show that latent semantic analysis (LSA) is able to extract what appears to be semantic meaning of words from a set of documents by blurring the distinctions", "creator": "LaTeX with hyperref package"}}}