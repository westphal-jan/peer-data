{"id": "1402.0560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Safe Exploration of State and Action Spaces in Reinforcement Learning", "abstract": "In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.", "histories": [["v1", "Tue, 4 Feb 2014 01:34:25 GMT  (2948kb)", "http://arxiv.org/abs/1402.0560v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["javier garcia", "fernando fernandez"], "accepted": false, "id": "1402.0560"}, "pdf": {"name": "1402.0560.pdf", "metadata": {"source": "CRF", "title": "Safe Exploration of State and Action Spaces in Reinforcement Learning", "authors": ["Javier Gar\u0107\u0131a", "Fernando Fern\u00e1ndez"], "emails": ["fjgpolo@inf.uc3m.es", "ffernand@inf.uc3m.es"], "sections": [{"heading": "1. Introduction", "text": "Most RL tasks focus on maximizing long-term reward; RL researchers pay increasing attention not only to long-term reward, but also to long-term reward maximization, but also to the safety of approaches to the Sequential Decision Problem (SDPs)."}, {"heading": "2. Definitions", "text": "In order to illustrate the safety concept of our approach, a navigation problem is illustrated in Figure 1 below. However, in the navigation problem presented in Figure 1, it is necessary to learn to get from a certain starting state to a destination state, since there are a number of demonstration paths of the robot. However, in this environment, we assume that the task will be difficult due to a stochastic and complex dynamics of the environment (e.g. an extremely irregular surface in the case of a robot navigation area or wind effects in the case of the robot floating task).This stochasticity makes it impossible to complete the task each time with exactly the same trajectory. Furthermore, the problem assumes that a series of demonstrations of a basic controller performing the task (the continuous black lines) are also given.This series of demonstrations is composed of different trajectories covering a well-defined region of the state space (the region within the rectangle).Our approach is based on the addition of small quantities of ground noise to influence the ground line on the noise or perturbations."}, {"heading": "2.1 Error and Non-Error States", "text": "In this work, we follow as far as we can the notation set out in Geibel et al. (2005) for defining our concept of risk. In their study, Geibel et al associate risk with error states and error-free states, the former being understood as a state in which it is undesirable or dangerous to enter it. Definition 1 error and error-free states. Let us consider S as a series of error-free states and vice versa as error-free states. A state is an undesirable end state in which the control of the agent ends when s is reached with damage or injury to the agent, the learning system or any external entities. The current episode is considered as a series of error-free states with error-free states, and where the control of the agent normally ends without damage or injury. With respect to RL, when the agent enters an error state, the current episode ends with damage to the learning system (or other systems); while the episode enters a non-error state, with no harm and the ghost episode ends as normal."}, {"heading": "2.2 Known and Unknown States in Continuous Action and State Spaces", "text": "(s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s2). (s3). (s3). (s3). (s3). (s2). (3). (4). (4). (4). (4). (4). (4). (4) (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4).). (4.). (4. (4). (4). (4.). (4.). (4). (4). (4).). (4. (4.). (4.). (4.).). (4.). (4.).). (4. (4.). (4.).). (4."}, {"heading": "2.3 The Advantages of Using Prior Knowledge and Predetermined Exploration Policies", "text": "This section highlights the advantages of using teacher knowledge in RL, namely (i) to provide initial knowledge of the task to be learned and (ii) to support the exploratory process. Furthermore, we explain why we consider this knowledge in RL to be indispensable in order to tackle highly complex and realistic problems with large, continuous states and areas of action, and in which a particular action can lead to undesirable consequences."}, {"heading": "2.3.1 Providing Initial Knowledge about the Task", "text": "Most RL algorithms begin with learning without prior knowledge of the task to be learned. However, in such cases, exploration strategies such as \u2212 are used greedily. Applying this strategy leads to a random exploration of the state and the action areas where the optimal policy is never encountered. However, this problem is exacerbated in areas with extremely large and continuous state and action areas where random exploration will never visit the regions of the necessary spaces to explore (near) optimal strategies. In addition, in many real RL tasks, random exploration is performed with real robots in order to obtain information from the environment."}, {"heading": "2.3.2 Supporting the Exploration Process", "text": "While providing the agent with initial knowledge helps to mitigate the problems associated with random exploration, this alone is not enough to prevent the undesirable situations that arise in subsequent explorations in order to improve the learners \"ability to learn. An additional mechanism is necessary to guide this subsequent exploration process in such a way that the agent can possibly be kept far away from catastrophic conditions. In this paper, instead of using the policy derived from the current value function, a teacher is used for selecting actions in unknown states. One way to prevent the agent from encountering unknown states during the exploration process would be by calling for a teacher demonstration for each state in the state from the outset. However, such a strategy is not possible due to (i) its computational impracticality in view of the extremely large number of states in the state and (ii) the fact that the teacher should not be forced to give action for each state as many states will be optimal for learning policies."}, {"heading": "2.4 The Risk Parameter", "text": "In order to maximize the security of exploration, it seems advisable that the movement through the state space is not arbitrary, but rather that the known space can be extended only gradually, starting from a known state. Such exploration is carried out by disrupting the approximate error trajectories caused by the policy. Disturbance of trajectories is achieved by adding Gaussian random noise to the actions in B. In other words, the Gaussian exploration takes placearound the current approximation of the action ai to the current known state sc, with ci = (si, V (si))))) and d (si) = minjo j. The action carried out is determined by a Gaussian distribution with the mean of the action selected in the instance in B. If ai is called the algorithm action as output, the probability of selection is higher."}, {"heading": "3. The PI-SRL Algorithm", "text": "The PI-SRL algorithm consists of two main steps, which are described in detail below."}, {"heading": "3.1 First Step: Modeling Baseline Behaviors by CBR", "text": "The first step of the PI-SRL is an approach to behavior-based cloning, in which CBR is used to enable a software agent to behave in a similar way to a teacher (basic behavior) \u03c0T (Floyd et al., 2008). While the LfD approaches are renamed differently depending on what is learned (Argall et al., 2009) to avoid terminological inconsistencies, we consider behavior-based cloning (also known as imitation of learning processes) to be an area of LfD whose goal is to reproduce / mimic the underlying teacher policy (Peters, Tedrake, Roy, & Morimoto, 2010; Abbott, 2008). If we use CBR for behavior-based cloning, a case can be built using behavior received from the environment and the teacher's corresponding command. PI-SRL correctly mimics the goal of the first step, the behavior of cases being stored in a case basis."}, {"heading": "3.2 Second Step: Improving the Learned Baseline Behavior", "text": "In this step of the PI-SRL algorithm, the safe case-based approach of the individual episodes is evaluated differently. < < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "3.3 Parameter Setting Design", "text": "One of the main difficulties in applying the PI-SRL algorithm to a given problem is deciding on an appropriate number of parameter values for the threshold \u03b8, the risk parameter \u03c3, the update threshold and the maximum number of cases. An incorrect value for the parameters \u03b8 can lead to a misjudgement of a condition as it is known, and potentially to damage or injury in the agent. Unlike the parameters that are not related to risk but directly related to the performance of the algorithm, the parameter is used to determine how good an episode must be in relation to the best episode, as only the best episodes are used to update the case base B. If the bad episodes are used to update the convergence and performance of the algorithm."}, {"heading": "4. Experimental Results", "text": "In this section, we present the experimental results selected from the use of PI-SRL for policy learning in four different domains: the parking problem (Lee & Lee, 2008), polar behavior (Sutton & Barto, 1998), helicopter risk (Ng et al., 2003), and business simulator SIMBA (Borrajo et al., 2010). For each of these domains, we have proposed learning near-optimal policies that minimize car accidents, poldisequilibrium, helicopter crashes, and corporate bankruptcies, each during the learning phase. All four domains are stochastical in our experiments. While both helicopters and business simulator SIMBA are inherently stochastic and generalized domains, we have made auto-parking and polarization domains."}, {"heading": "4.1 Car Parking Problem", "text": "The parking problem is shown in Figure 10 and is based on the RL literature (Cichosz, 1996). A car that is shown as a rectangle in Figure 10 is initially located within a limited area represented by the dark, fixed lines referred to as the driving area. The goal for the learning agent is to navigate the car from its starting point to the garage so that the car is fully within, in a minimum number of steps. The car cannot move outside the driving area. Figure 10 (b) shows the two possible paths that the car can take from the starting point to the garage to perform the task correctly. We consider the optimal policy for the domain that reaches the destination state in the shortest time and is simultaneously failsafe. The state space of the domain is described by three continuous variables, namely the coordinates of the center of the car xt and the angles between the car axle and the X coordinate system of the car."}, {"heading": "4.2 Pole-Balancing", "text": "As the name suggests, the goal in the pole balancing problem is to balance a pole vertically on the top of a moving trolley (Sutton & Barto, 1998).The condition description consists of a four-dimensional vector containing the angle \u03c6, the radial velocity \u03c6 \u2032, the basket position x and the velocity x \u2032.The action consists of a real evaluated force used to push the basket.In this study, the reward is calculated to promote measures to keep the pole as upright as possible on the basket and the basket as centered as possible on the track. Thus, the reward in step t is calculated as rt = 1 \u2212 (high) the reward is executed in step t (high)."}, {"heading": "4.3 Helicopter Hovering", "text": "As the name suggests, the aim of this domain is to bring a helicopter as close as possible to a defined position defined by an episode. The task is challenging for two main reasons: firstly, both the state and the action space are highly dimensional and continuous (more precisely, the state space is 12-dimensional and the action space 4-dimensional); secondly, it is a generalized domain whose behavior is modified by the wind factor; a helicopter sequence consists of 6000 steps, although it may end prematurely when the helicopter crashes; and, the first step of PI-SRL is performed to mimic the behavior of the baseline."}, {"heading": "4.4 SIMBA", "text": "An example of such a tool is the SIMulator for Business Administration (SIMBA) (SIMBA) (Borrajo et al., 2010). SIMBA is a competitive simulator because agents can compete with other agents by managing different virtual enterprises. The simulator, the result of over twenty years of experience with both university students and executives, emulates business realities using the same variables, relationships and events that exist in the business world. Its goal is to provide users with an integrated vision of a company by using basic business management techniques, simplifying complexity and emphasizing the content and principles with the greatest educational value (Borrajo et al., 2010). In the experiments conducted here, the learner competes against five hand-coded agents (Borrajo et al., 2010). Decision-making in SIMBA is an episodic task in which decisions are made sequentially."}, {"heading": "5. Related Work", "text": "In the literature, Reinforcement Learning (RL) and case-based reasoning (CBR) techniques have been combined in different ways. In the work of Bianchi et al. (2009), a new approach is presented that allows the use of cases as heuristics to accelerate RL algorithms. Furthermore, Sharma et al. (2007) use a combination of CBR and RL (CARL) to achieve a transfer while playing against Game AI across a variety of scenarios in MadRTS TM, a commercial real-time strategy game. CBR has also been used to approximate the state values in RL (Fork & Riedmiller, 2005)."}, {"heading": "5.1 Approaches Based on the Return and its Variance", "text": "It has long been known in the literature that the optimal policy and the optimal expected return of an MDP are relatively sensitive to parameter fluctuations (even an optimal policy may perform poorly in some cases due to the stochastic nature of the problem).In order to mitigate this problem, the agent may try to maximize the return associated with the worst-case scenario, even if the case is highly improbable. Thus, the risk in this trend refers to the worst results of the yield R = \u2211 t = 0 \u03b3trt or its variance. An example of such an approach is the worst-case control, which seeks to optimize the worst-case outcome of R (Coraluppi & Marcus, 1999; Heger, 1994).In the worst-case scenario, control strategies are focused exclusively on risk-avoidance measures. A policy is considered optimal when its worst-case return is superior, but the approach is too restrictive as it fully takes into account very rare scenarios."}, {"heading": "5.2 Risk-sensitive Approaches based on Error States.", "text": "In this second trend of approaches, the concept of risk is based on the definition of error states or fatal transitions. Thus, for example, Geibel et al. (2005) establish risk function as the probability of entering a fault state. Instead, Hans et al. (2008) consider a transition to be fatal if the corresponding reward is below a certain threshold. In the first case, and as demonstrated in Section 4, \u03c1 is learned by TD methods that require error states (i.e., auto collisions, pole balancing imbalances, helicopter crashes, and company bankruptcies) to be revisited repeatedly in order to approximate risk function and subsequently avoid dangerous situations. In the second case, the concept of risk is again linked to reward. Furthermore, the above studies either (i) assume that system dynamics are known, (ii) that adverse conditions are tolerated during research, or, contrary to our paper, (ii) that problems can be addressed on a high-dimensional and continuous basis."}, {"heading": "5.3 Approaches Using Teachers", "text": "The latest trend in approaches is based on the use of teachers in three different ways: (i) to start the learning algorithm (i.e. as an initialization process), (ii) to derive a policy from a finite demonstration set, and (iii) to guide the exploration process."}, {"heading": "5.3.1 Bootstrapping the Learning Algorithm", "text": "In the work of Driessens and Sz'eroski (2004), a bootstraping method for relational RL is used, in which a finite series of demonstrations are recorded by a human expert and later submitted to a regression algorithm, allowing the regression algorithm to build a partial Q function that can later be used to guide further exploration of state space using a Boltzmann exploration strategy. Smart and Kaelbling (2000) also use examples to boot the Q-Learning approach for their HEDGER algorithm. Initial knowledge bootstrapped into the Q-Learning approach allows the agent to learn more effectively and helps to reduce the time spent on random actions. Teacher behavior is also used as a form of population formation in neuroevolutionary approaches (Yao, 1999; Siebel & Sommer, 2007).Evolutionary methods are used to optimize the weights of neural networks, but based on a teacher's weights (or a protyp)."}, {"heading": "5.3.2 Deriving a Policy from a Finite Set of Demonstrations", "text": "All approaches that fall into this category are summarized according to the field Learning from Demonstration (LfD) (Argall et al., 2009). Highlights are the study by Abbeel et al. (2010), which is based on apprenticeship training, and the approach consists of three different steps. In the first, a teacher demonstrates the task to be learned and logs the state-based pathways of the teacher demonstration. In the second step, all state-based pathways are used to learn a dynamic model for the system. In this model, an (nearly) optimal strategy is also found that uses any reinforcement learning algorithm (RL). Finally, the policy obtained should be tested by executing it on the real system. In the work of Tang etal. (2010), an algorithm based on apprenticeship training is presented."}, {"heading": "5.3.3 Guiding the Exploration Process", "text": "In the context of the relational RL, Driessens and Sz eroski (2004) also use a predetermined teacher policy and not a policy derived from the current Q function hypothesis (which is not informative in the early learning phase) for the selection of measures. In this approach, episodes performed by a teacher are linked to normal exploration episodes, a mixture of teacher and normal exploration that makes it easier for the regression algorithm to distinguish between useful and bad measures. In the context of LfD, there are other approaches, including teacher counseling (Argall et al., 2009), which are designed to improve learning performance and provide information that goes beyond what is provided by a demonstration dataset. In this approach, after an initial task demonstration by the teacher, the agent directly requests additional evidence from the teacher in very different states than was previously demonstrated or in states where a single measure cannot be selected with certainty (Velnova & lizos are not mentioned in these trends, 2008, 2008, 2008, and 2008 respectively)."}, {"heading": "6. Conclusions", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "Acknowledgments", "text": "This study was partially supported by the Spanish MICIIN projects TIN2008-06701-C0303, TRA2009-0080 and CCG10-UC3M / TIC-5597. We would like to thank Raquel Fuentetaja Piza \u0301 n, Assistant Professor at the Universidad Carlos III de Madrid in the Planning & Learning Group (PLG), for her generous and valuable comments during the revision of this paper. We would also like to thank Jose \u0301 Antonio Mart \u0301 \u0131n, Assistant Professor at the Universidad Complutense de Madrid, for his valuable comments on his evolutionary RL algorithm."}], "references": [{"title": "Case-Based Reasoning; Foundational Issues, Methodological Variations, and System Approaches", "author": ["A. Aamodt", "E. Plaza"], "venue": "AI Communications,", "citeRegEx": "Aamodt and Plaza,? \\Q1994\\E", "shortCiteRegEx": "Aamodt and Plaza", "year": 1994}, {"title": "Autonomous Autorotation of an RC Helicopter", "author": ["P. Abbeel", "A. Coates", "T. Hunter", "A.Y. Ng"], "venue": "In ISER,", "citeRegEx": "Abbeel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2008}, {"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "I. J. Robotic Res.,", "citeRegEx": "Abbeel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2010}, {"title": "Robocup 2007: Robot soccer world cup xi", "author": ["R.G. Abbott"], "venue": "chap. Behavioral Cloning for Simulator Validation,", "citeRegEx": "Abbott,? \\Q2008\\E", "shortCiteRegEx": "Abbott", "year": 2008}, {"title": "Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms", "author": ["D.W. Aha"], "venue": "International Journal Man-Machine Studies,", "citeRegEx": "Aha,? \\Q1992\\E", "shortCiteRegEx": "Aha", "year": 1992}, {"title": "Instance-based learning algorithms", "author": ["D.W. Aha", "D. Kibler"], "venue": "In Machine Learning,", "citeRegEx": "Aha and Kibler,? \\Q1991\\E", "shortCiteRegEx": "Aha and Kibler", "year": 1991}, {"title": "Behavioral cloning of student pilots with modular neural networks", "author": ["C.W. Anderson", "B.A. Draper", "D.A. Peterson"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "Anderson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2000}, {"title": "A Survey of Robot Learning from Demonstration", "author": ["B. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Case-based reasoning: Survey and future directions", "author": ["B. Bartsch-Sprl", "M. Lenz", "A. Hbner"], "venue": "XPS, Vol. 1570 of Lecture Notes in Computer Science,", "citeRegEx": "Bartsch.Sprl et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bartsch.Sprl et al\\.", "year": 1999}, {"title": "Improving reinforcement learning by using case-based heuristics", "author": ["R. Bianchi", "R. Ros", "R.L. de M\u00e1ntaras"], "venue": null, "citeRegEx": "Bianchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bianchi et al\\.", "year": 2009}, {"title": "SIMBA: A Simulator for Business Education and Research", "author": ["F. Borrajo", "Y. Bueno", "I. de Pablo", "Santos", "B. n", "F. Fern\u00e1ndez", "J. Gar\u0107\u0131a", "I. Sagredo"], "venue": "Decission Support Systems,", "citeRegEx": "Borrajo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Borrajo et al\\.", "year": 2010}, {"title": "Proceedings of the workshop on value function approximation, machine learning conference 1995.", "author": ["J. Boyan", "A. Moore", "R. Sutton"], "venue": "Technical Report CMU-CS-95206", "citeRegEx": "Boyan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boyan et al\\.", "year": 1995}, {"title": "Confidence-based policy learning from demonstration using gaussian mixture models", "author": ["S. Chernova", "M. Veloso"], "venue": "In Joint Conference on Autonomous Agents and Multi-Agent Systems", "citeRegEx": "Chernova and Veloso,? \\Q2007\\E", "shortCiteRegEx": "Chernova and Veloso", "year": 2007}, {"title": "Multi-thresholded approach to demonstration selection for interactive robot learning", "author": ["S. Chernova", "M. Veloso"], "venue": "In Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction,", "citeRegEx": "Chernova and Veloso,? \\Q2008\\E", "shortCiteRegEx": "Chernova and Veloso", "year": 2008}, {"title": "Truncating temporal differences: On the efficient implementation of td(lambda) for reinforcement learning", "author": ["P. Cichosz"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Cichosz,? \\Q1995\\E", "shortCiteRegEx": "Cichosz", "year": 1995}, {"title": "Truncated temporal differences with function approximation: Successful examples using cmac", "author": ["P. Cichosz"], "venue": "In Proceedings of the Thirteenth European Symposium on Cybernetics and Systems Research (EMCSR-96)", "citeRegEx": "Cichosz,? \\Q1996\\E", "shortCiteRegEx": "Cichosz", "year": 1996}, {"title": "Risk-Sensitive and Minimax Control of DiscreteTime", "author": ["S.P. Coraluppi", "S.I. Marcus"], "venue": "Finite-State Markov Decision Processes. AUTOMATICA,", "citeRegEx": "Coraluppi and Marcus,? \\Q1999\\E", "shortCiteRegEx": "Coraluppi and Marcus", "year": 1999}, {"title": "Risk-aware decision making and dynamic programming", "author": ["B. Defourny", "D. Ernst", "L. Wehenkel"], "venue": "In NIPS 2008 Workshop on Model Uncertainty and Risk in RL", "citeRegEx": "Defourny et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Defourny et al\\.", "year": 2008}, {"title": "Relational instance based regression for relational rl", "author": ["K. Driessens", "J. Ramon"], "venue": "In International Conference of Machine Learning (ICML),", "citeRegEx": "Driessens and Ramon,? \\Q2003\\E", "shortCiteRegEx": "Driessens and Ramon", "year": 2003}, {"title": "Integrating guidance into relational reinforcement learning", "author": ["K. Driessens", "S. D\u017eeroski"], "venue": "Machine Learning,", "citeRegEx": "Driessens and D\u017eeroski,? \\Q2004\\E", "shortCiteRegEx": "Driessens and D\u017eeroski", "year": 2004}, {"title": "Local feature weighting in nearest prototype classification", "author": ["F. Fernandez", "P. Isasi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Fernandez and Isasi,? \\Q2008\\E", "shortCiteRegEx": "Fernandez and Isasi", "year": 2008}, {"title": "Two steps reinforcement learning", "author": ["F. Fern\u00e1ndez", "D. Borrajo"], "venue": "International Journal of Intelligent Systems,", "citeRegEx": "Fern\u00e1ndez and Borrajo,? \\Q2008\\E", "shortCiteRegEx": "Fern\u00e1ndez and Borrajo", "year": 2008}, {"title": "Toward a domain-independent case-based reasoning approach for imitation: Three case studies in gaming", "author": ["M.W. Floyd", "B. Esfandiari"], "venue": "In Workshop on Case-Based Reasoning for Computer Games at the 18th International Conference on Case-Based Reasoning (ICCBR),", "citeRegEx": "Floyd and Esfandiari,? \\Q2010\\E", "shortCiteRegEx": "Floyd and Esfandiari", "year": 2010}, {"title": "A Case-Based Reasoning Approach to Imitating Robocup Players", "author": ["M.W. Floyd", "B. Esfandiari", "K. Lam"], "venue": "In Proceedings of the 21st International Florida Artificial Intelligence Research Society Conference,", "citeRegEx": "Floyd et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Floyd et al\\.", "year": 2008}, {"title": "Representations for learning control policies", "author": ["J. Forbes", "D. Andre"], "venue": "In The University of New South,", "citeRegEx": "Forbes and Andre,? \\Q2002\\E", "shortCiteRegEx": "Forbes and Andre", "year": 2002}, {"title": "Cbr for state value function approximation in reinforcement learning", "author": ["T. Gabel", "M. Riedmiller"], "venue": "In Proceedings of the 6th International Conference on Case-Based Reasoning (ICCBR", "citeRegEx": "Gabel and Riedmiller,? \\Q2005\\E", "shortCiteRegEx": "Gabel and Riedmiller", "year": 2005}, {"title": "Reinforcement Learning with Bounded Risk", "author": ["P. Geibel"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Geibel,? \\Q2001\\E", "shortCiteRegEx": "Geibel", "year": 2001}, {"title": "Risk-sensitive Reinforcement Learning Applied to Control under Constraints", "author": ["P. Geibel", "F. Wysotzki"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Geibel and Wysotzki,? \\Q2005\\E", "shortCiteRegEx": "Geibel and Wysotzki", "year": 2005}, {"title": "Safe Exploration for Reinforcement Learning", "author": ["A. Hans", "D. Schneegass", "A.M. Sch\u00e4fer", "S. Udluft"], "venue": "In European Symposium on Artificial Neural Network,", "citeRegEx": "Hans et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hans et al\\.", "year": 2008}, {"title": "Consideration of Risk in Reinforcement Learning", "author": ["M. Heger"], "venue": "In 11th International Conference on Machine Learning,", "citeRegEx": "Heger,? \\Q1994\\E", "shortCiteRegEx": "Heger", "year": 1994}, {"title": "Seeding the initial population of a multi-objective evolutionary algorithm using gradient-based information", "author": ["A.G. Hern\u00e1ndez-D\u0131\u0301az", "C.A.C. Coello", "F. Perez", "R. Caballero", "J.M. Luque", "L.V. SantanaQuintero"], "venue": "In IEEE Congress on Evolutionary Computation,", "citeRegEx": "Hern\u00e1ndez.D\u0131\u0301az et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hern\u00e1ndez.D\u0131\u0301az et al\\.", "year": 2008}, {"title": "A real-time model-based reinforcement learning architecture for robot control", "author": ["T. Hester", "M. Quinlan", "P. Stone"], "venue": "Tech. rep. arXiv e-Prints 1105.1749,", "citeRegEx": "Hester et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hester et al\\.", "year": 2011}, {"title": "Multiagent reinforcement learning in stochastic games with continuous action spaces", "author": ["A.X. Jiang"], "venue": null, "citeRegEx": "Jiang,? \\Q2004\\E", "shortCiteRegEx": "Jiang", "year": 2004}, {"title": "Reinforcement learning: A survey", "author": ["L. Kaelbling", "M. Littman", "A. Moore"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Reinforcement learning for games: failures and successes", "author": ["W. Konen", "T. Bartz-Beielstein"], "venue": "In Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers,", "citeRegEx": "Konen and Bartz.Beielstein,? \\Q2009\\E", "shortCiteRegEx": "Konen and Bartz.Beielstein", "year": 2009}, {"title": "Neuroevolutionary reinforcement learning for generalized helicopter control", "author": ["R. Koppejan", "S. Whiteson"], "venue": "In GECCO 2009: Proceedings of the Genetic and Evolutionary Computation Conference,", "citeRegEx": "Koppejan and Whiteson,? \\Q2009\\E", "shortCiteRegEx": "Koppejan and Whiteson", "year": 2009}, {"title": "Neuroevolutionary reinforcement learning for generalized control of simulated helicopters", "author": ["R. Koppejan", "S. Whiteson"], "venue": "Evolutionary Intelligence,", "citeRegEx": "Koppejan and Whiteson,? \\Q2011\\E", "shortCiteRegEx": "Koppejan and Whiteson", "year": 2011}, {"title": "Multiple Designs of Fuzzy Controllers for Car Parking", "author": ["Lee", "J.-Y", "J.-J"], "venue": "Using Evolutionary Algorithm,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Investment science", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1998\\E", "shortCiteRegEx": "Luenberger", "year": 1998}, {"title": "Reinforcement learning for average reward zero-sum games", "author": ["S. Mannor"], "venue": "COLT, Vol. 3120 of Lecture Notes in Computer Science,", "citeRegEx": "Mannor,? \\Q2004\\E", "shortCiteRegEx": "Mannor", "year": 2004}, {"title": "Exa: An effective algorithm for continuous actions reinforcement learning problems", "author": ["J. Martin H", "J. de Lope"], "venue": "In Industrial Electronics,", "citeRegEx": "H and Lope,? \\Q2009\\E", "shortCiteRegEx": "H and Lope", "year": 2009}, {"title": "Learning Autonomous Helicopter Flight with Evolutionary Reinforcement Learning", "author": ["J.A. Mart\u0301\u0131n H", "J. Lope"], "venue": "In 12th International Conference on Computer Aided Systems Theory (EUROCAST),", "citeRegEx": "H. and Lope,? \\Q2009\\E", "shortCiteRegEx": "H. and Lope", "year": 2009}, {"title": "Risk-Sensitive reinforcement learning", "author": ["O. Mihatsch", "R. Neuneier"], "venue": "Machine Learning,", "citeRegEx": "Mihatsch and Neuneier,? \\Q2002\\E", "shortCiteRegEx": "Mihatsch and Neuneier", "year": 2002}, {"title": "Safe exploration in markov decision processes. CoRR, abs/1205.4810", "author": ["T.M. Moldovan", "P. Abbeel"], "venue": null, "citeRegEx": "Moldovan and Abbeel,? \\Q2012\\E", "shortCiteRegEx": "Moldovan and Abbeel", "year": 2012}, {"title": "Learning automata - a survey", "author": ["K.S. Narendra", "M.A.L. Thathachar"], "venue": "Ieee Transactions On Systems Man And Cybernetics,", "citeRegEx": "Narendra and Thathachar,? \\Q1974\\E", "shortCiteRegEx": "Narendra and Thathachar", "year": 1974}, {"title": "Learning automata: an introduction", "author": ["K.S. Narendra", "M.A.L. Thathachar"], "venue": null, "citeRegEx": "Narendra and Thathachar,? \\Q1989\\E", "shortCiteRegEx": "Narendra and Thathachar", "year": 1989}, {"title": "Autonomous Helicopter Flight via Reinforcement Learning", "author": ["A.Y. Ng", "H.J. Kim", "M.I. Jordan", "S. Sastry"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Genetic programming with user-driven selection: Experiments on the evolution of algorithms for image enhancement", "author": ["R. Poli", "S. Cagnoni"], "venue": "In Genetic Programming", "citeRegEx": "Poli and Cagnoni,? \\Q1997\\E", "shortCiteRegEx": "Poli and Cagnoni", "year": 1997}, {"title": "A collaborative reinforcement learning approach to urban traffic control optimization", "author": ["A. Salkham", "R. Cunningham", "A. Garg", "V. Cahill"], "venue": "In Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "Salkham et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salkham et al\\.", "year": 2008}, {"title": "Experiments with reinforcement learning in problems with continuous state and action spaces", "author": ["J.C. Santama\u0155\u0131a", "R.S. Sutton", "A. Ram"], "venue": "Adaptive Behavior,", "citeRegEx": "Santama\u0155\u0131a et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Santama\u0155\u0131a et al\\.", "year": 1998}, {"title": "Transfer learning in real-time strategy games using hybrid cbr/rl", "author": ["M. Sharma", "M. Holmes", "J. Santamaria", "A. Irani", "C. Isbell", "A. Ram"], "venue": "Proceedings of the Twentieth International Joint Conference on Artificial Intelligence", "citeRegEx": "Sharma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2007}, {"title": "Evolutionary reinforcement learning of artificial neural networks", "author": ["N.T. Siebel", "G. Sommer"], "venue": "International Journal of Hybrid Intelligent Systems,", "citeRegEx": "Siebel and Sommer,? \\Q2007\\E", "shortCiteRegEx": "Siebel and Sommer", "year": 2007}, {"title": "Practical reinforcement learning in continuous spaces", "author": ["W.D. Smart", "L.P. Kaelbling"], "venue": "In Artificial Intelligence,", "citeRegEx": "Smart and Kaelbling,? \\Q2000\\E", "shortCiteRegEx": "Smart and Kaelbling", "year": 2000}, {"title": "Effective reinforcement learning for mobile robots", "author": ["W.D. Smart", "L.P. Kaelbling"], "venue": "In ICRA,", "citeRegEx": "Smart and Kaelbling,? \\Q2002\\E", "shortCiteRegEx": "Smart and Kaelbling", "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Parameterized maneuver learning for autonomous helicopter flight", "author": ["J. Tang", "A. Singh", "N. Goehausen", "P. Abbeel"], "venue": "In International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Tang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2010}, {"title": "Metric learning for reinforcement learning agents", "author": ["M.E. Taylor", "B. Kulis", "F. Sha"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Reinforcement Learning in Continuous Action Spaces", "author": ["H. Van Hasselt", "M.A. Wiering"], "venue": "In Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Hasselt and Wiering,? \\Q2007\\E", "shortCiteRegEx": "Hasselt and Wiering", "year": 2007}, {"title": "Exploration and Inference in Learning from Reinforcement", "author": ["J. Wyatt"], "venue": "University of Edinburgh", "citeRegEx": "Wyatt,? \\Q1997\\E", "shortCiteRegEx": "Wyatt", "year": 1997}, {"title": "Evolving artificial neural networks", "author": ["X. Yao"], "venue": "PIEEE: Proceedings of the IEEE,", "citeRegEx": "Yao,? \\Q1999\\E", "shortCiteRegEx": "Yao", "year": 1999}], "referenceMentions": [{"referenceID": 38, "context": "Another example can be found in portfolio theory where analysts are expected to find a portfolio that maximizes profit while avoiding risks of considerable losses (Luenberger, 1998).", "startOffset": 163, "endOffset": 181}, {"referenceID": 3, "context": "In the first, baseline behavior (robust albeit suboptimal) is approximated using behavioral cloning techniques (Anderson, Draper, & Peterson, 2000; Abbott, 2008).", "startOffset": 111, "endOffset": 161}, {"referenceID": 34, "context": "In the work of Koppejan and Whiteson (2011), singlelayers perceptrons are evolved, albeit starting from a prototype network whose weights correspond to a baseline policy provided by helicopter control task competition software (Abbeel, Coates, Hunter, & Ng, 2008).", "startOffset": 15, "endOffset": 44}, {"referenceID": 34, "context": "In the work of Koppejan and Whiteson (2011), singlelayers perceptrons are evolved, albeit starting from a prototype network whose weights correspond to a baseline policy provided by helicopter control task competition software (Abbeel, Coates, Hunter, & Ng, 2008). This approach can be viewed as a simple form of population seeding which has proven to be advantageous in numerous evolutionary methods (e.g. see Hern\u00e1ndez-D\u0131\u0301az, Coello, Perez, Caballero, Luque, & Santana-Quintero, 2008; Poli & Cagnoni, 1997). In the work of Mart\u0301\u0131n and de Lope (2009), the weights of neural networks are also evolved by inserting several baseline policies (including that provided in the helicopter control task competition software) into the initial population.", "startOffset": 15, "endOffset": 552}, {"referenceID": 26, "context": "As will be described in Section 5 in greater detail, this new definition is completely different from traditional definitions of \u201crisk\u201d found in the literature (Geibel, 2001; Mihatsch & Neuneier, 2002; Geibel & Wysotzki, 2005).", "startOffset": 160, "endOffset": 226}, {"referenceID": 26, "context": "In this paper, we follow as far we can the notation presented in Geibel et al. (2005) for the definition of our concept of risk.", "startOffset": 65, "endOffset": 86}, {"referenceID": 14, "context": ", it has been successfully applied to car parking (Cichosz, 1995), pole-balancing (Martin H & de Lope, 2009), helicopter hovering control (Martin H & de Lope, 2009) and SIMBA (Borrajo et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 10, "context": ", it has been successfully applied to car parking (Cichosz, 1995), pole-balancing (Martin H & de Lope, 2009), helicopter hovering control (Martin H & de Lope, 2009) and SIMBA (Borrajo et al., 2010).", "startOffset": 175, "endOffset": 197}, {"referenceID": 32, "context": "As a result of this assumption, approximation techniques can be used, such that actions that generate similar effects can be grouped together as one action (Jiang, 2004).", "startOffset": 156, "endOffset": 169}, {"referenceID": 33, "context": "In continuous action spaces, the need for generalization techniques is even greater (Kaelbling et al., 1996).", "startOffset": 84, "endOffset": 108}, {"referenceID": 23, "context": "The first step of PI-SRL is an approach for behavioral cloning, using CBR to allow a software agent to behave in a similar manner to a teacher policy (baseline behavior) \u03c0T (Floyd et al., 2008).", "startOffset": 173, "endOffset": 193}, {"referenceID": 7, "context": "Whereas LfD approaches are named differently according to what is learned (Argall et al., 2009), to prevent terminological inconsistencies here, we consider behavioral cloning (also known as imitation learning) to be an area of LfD whose goal is the reproduction/mimicking of the underlying teacher policy \u03c0T (Peters, Tedrake, Roy, & Morimoto, 2010; Abbott, 2008).", "startOffset": 74, "endOffset": 95}, {"referenceID": 3, "context": ", 2009), to prevent terminological inconsistencies here, we consider behavioral cloning (also known as imitation learning) to be an area of LfD whose goal is the reproduction/mimicking of the underlying teacher policy \u03c0T (Peters, Tedrake, Roy, & Morimoto, 2010; Abbott, 2008).", "startOffset": 221, "endOffset": 275}, {"referenceID": 4, "context": "Several approaches to the removal of ineffectual cases during training exist, including Aha\u2019s IBx algorithms (Aha, 1992) or any nearest prototype approach (Fernandez & Isasi, 2008).", "startOffset": 109, "endOffset": 120}, {"referenceID": 58, "context": "In this way, good sequences are provided for the updates since it has been shown that such sequences of experiences can cause an adaptive agent to converge to a stable and useful policy, whereas bad sequences may cause an agent to converge to an unstable or bad policy (Wyatt, 1997).", "startOffset": 269, "endOffset": 282}, {"referenceID": 54, "context": "Van Hasselt and Wiering (2007) also update the value function using only the actions that potentially lead to a higher return.", "startOffset": 4, "endOffset": 31}, {"referenceID": 23, "context": "Forbes and Andres (2002) suggest the removal of cases that contribute least to the overall approximation, while Driessens and Ramon (2003) pursue a more error-oriented view and propose the deletion of cases that contribute most to the prediction error of other examples.", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "Forbes and Andres (2002) suggest the removal of cases that contribute least to the overall approximation, while Driessens and Ramon (2003) pursue a more error-oriented view and propose the deletion of cases that contribute most to the prediction error of other examples.", "startOffset": 112, "endOffset": 139}, {"referenceID": 18, "context": "Forbes and Andres (2002) suggest the removal of cases that contribute least to the overall approximation, while Driessens and Ramon (2003) pursue a more error-oriented view and propose the deletion of cases that contribute most to the prediction error of other examples. The principal drawback of these more sophisticated measures is their complexity. The determination of the case(s) to be removed involves the computation of a score value for each ci \u2208 B, which in turn requires at least one retrieval and regression, respectively, for each cj \u2208 B (j 6= i). Such entire repeated sweeps through the case-base entail an enormous computational load. Gabel and Riedmiller (2005) compute a different score metric for each ci \u2208 B, requiring the computation of the set of the k-nearest neighbors around ci.", "startOffset": 112, "endOffset": 677}, {"referenceID": 46, "context": ", increasing number of variables describing states and actions): the car parking problem (Lee & Lee, 2008), pole-balancing (Sutton & Barto, 1998), helicopter hovering (Ng et al., 2003) and the business simulator SIMBA (Borrajo et al.", "startOffset": 167, "endOffset": 184}, {"referenceID": 10, "context": ", 2003) and the business simulator SIMBA (Borrajo et al., 2010).", "startOffset": 41, "endOffset": 63}, {"referenceID": 26, "context": "While Geibel and Wysotzki (2005) consider only finite (discretized) action sets in their study, their algorithm has been adapted here for continuous action sets.", "startOffset": 6, "endOffset": 33}, {"referenceID": 15, "context": "The car parking problem is represented in Figure 10 and originates from the RL literature (Cichosz, 1996).", "startOffset": 90, "endOffset": 105}, {"referenceID": 58, "context": "In this way, good sequences of experiences are provided to the updates, since it has been proven that good sequences of experiences can cause an adaptive agent to converge to a stable and useful policy, while bad sequences may cause an agent to converge to an unstable or poor policy (Wyatt, 1997).", "startOffset": 284, "endOffset": 297}, {"referenceID": 10, "context": "An example of such a tool is the SIMulator for Business Administration (SIMBA) (Borrajo et al., 2010).", "startOffset": 79, "endOffset": 101}, {"referenceID": 10, "context": "Its objective is to provide users with an integrated vision of a company, using basic techniques of business management, simplifying complexity and emphasizing the content and principles with the greatest educational value (Borrajo et al., 2010).", "startOffset": 223, "endOffset": 245}, {"referenceID": 10, "context": "In the experiments performed here, the learning agent competes against five handcoded agents (Borrajo et al., 2010).", "startOffset": 93, "endOffset": 115}, {"referenceID": 10, "context": ") (Borrajo et al., 2010).", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": "In the work of Bianchi et al. (2009), a new approach is presented permitting the use of cases as heuristics to speed up RL algorithms.", "startOffset": 15, "endOffset": 37}, {"referenceID": 9, "context": "In the work of Bianchi et al. (2009), a new approach is presented permitting the use of cases as heuristics to speed up RL algorithms. Additionally, Sharma et al. (2007) use a combination of CBR and RL (called CARL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTS TM, a commercial Real Time Strategy game.", "startOffset": 15, "endOffset": 170}, {"referenceID": 29, "context": "An example of such an approach is worst-case control where the worst possible outcome of R is to be optimized (Coraluppi & Marcus, 1999; Heger, 1994).", "startOffset": 110, "endOffset": 149}, {"referenceID": 29, "context": "The \u03b1\u2212 value of the return m\u0302\u03b1 introduced by Heger (1994) can be seen as an extension of the worst case control of MDPs.", "startOffset": 45, "endOffset": 58}, {"referenceID": 26, "context": "Thus, Geibel et al. (2005) , for instance, establish the risk function as the probability of entering in an error state.", "startOffset": 6, "endOffset": 27}, {"referenceID": 26, "context": "Thus, Geibel et al. (2005) , for instance, establish the risk function as the probability of entering in an error state. Instead, Hans et al (2008) consider a transition to be fatal if the corresponding reward is less than a given threshold \u03c4 .", "startOffset": 6, "endOffset": 148}, {"referenceID": 59, "context": "Teacher behaviors are also used as a form of population seeding in neuroevolution approaches (Yao, 1999; Siebel & Sommer, 2007).", "startOffset": 93, "endOffset": 127}, {"referenceID": 30, "context": "While this approach has been proven advantageous in numerous applications of evolutionary methods (Hern\u00e1ndez-D\u0131\u0301az et al., 2008; Koppejan & Whiteson, 2009), Koppejan\u2019s algorithm nevertheless also seems somewhat ad-hoc and designed for a specialized set of environments.", "startOffset": 98, "endOffset": 155}, {"referenceID": 51, "context": "Smart and Kaelbling (2000) also use examples, training runs to bootstrap the Q-learning approach for their HEDGER algorithm.", "startOffset": 0, "endOffset": 27}, {"referenceID": 51, "context": "Smart and Kaelbling (2000) also use examples, training runs to bootstrap the Q-learning approach for their HEDGER algorithm. The initial knowledge bootstrapped into the Q-learning approach allows the agent to learn more effectively and helps reduce the time spent with random actions. Teacher behaviors are also used as a form of population seeding in neuroevolution approaches (Yao, 1999; Siebel & Sommer, 2007). Evolutionary methods are used to optimize the weights of neural networks, but starting from a prototype network whose weights correspond to a teacher (or baseline policy). Using this technique, RL Competition helicopter hovering task winners Martin et al. (2009) developed an evolutionary RL algorithm in which several teachers are provided in the initial population.", "startOffset": 0, "endOffset": 677}, {"referenceID": 7, "context": "All approaches falling under this category are framed according to the field of Learning from Demonstration (LfD) (Argall et al., 2009).", "startOffset": 114, "endOffset": 135}, {"referenceID": 1, "context": "Highlighting the study by Abbeel et al. (2010) based on apprenticeship learning, the approach is composed of three distinct steps.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": "In the context of LfD, there are other approaches which include teacher advice (Argall et al., 2009).", "startOffset": 79, "endOffset": 100}, {"referenceID": 33, "context": "However, as we clearly explain in Section 2, we consider both assumptions to be logical assumptions derived from generalization principles in the RL literature (Kaelbling et al., 1996; Jiang, 2004).", "startOffset": 160, "endOffset": 197}, {"referenceID": 32, "context": "However, as we clearly explain in Section 2, we consider both assumptions to be logical assumptions derived from generalization principles in the RL literature (Kaelbling et al., 1996; Jiang, 2004).", "startOffset": 160, "endOffset": 197}], "year": 2012, "abstractText": "In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.", "creator": "TeX"}}}