{"id": "1402.0565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Lifted Variable Elimination: Decoupling the Operators from the Constraint Language", "abstract": "Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. In this article, we decouple lifted inference from the constraint language. We define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints extension) rather than on the syntactic level, making them language-independent. As a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. We empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.", "histories": [["v1", "Tue, 4 Feb 2014 01:35:39 GMT  (566kb)", "http://arxiv.org/abs/1402.0565v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nima taghipour", "daan fierens", "jesse davis", "hendrik blockeel"], "accepted": false, "id": "1402.0565"}, "pdf": {"name": "1402.0565.pdf", "metadata": {"source": "CRF", "title": "Lifted Variable Elimination: Decoupling the Operators from the Constraint Language", "authors": ["Nima Taghipour", "Daan Fierens", "Jesse Davis", "Hendrik Blockeel"], "emails": ["nima.taghipour@cs.kuleuven.be", "daan.fierens@cs.kuleuven.be", "jesse.davis@cs.kuleuven.be", "hendrik.blockeel@cs.kuleuven.be"], "sections": [{"heading": "1. Introduction", "text": "This is a major challenge in this area of how to draw conclusions without knowing what X stands for. However, many approaches to SRL can transform their knowledge into a propositional graphical model before performing inferences. By doing so, they lose the ability to understand the logical variables at the level of logical models."}, {"heading": "2. Lifted Variable Elimination by Example", "text": "Although the elimination of variables is based on simple intuition, it is relatively complicated, and an accurate description requires a level of technical detail that is not conducive to a clear understanding. Therefore, we will first illustrate the basic principles of eliminating variables using a simple example, without reference to the technical terminology introduced later. Let's start with the description of the example; next, we will illustrate the elimination of variables using this example and show how it can be reversed."}, {"heading": "2.1 The Workshop Example", "text": "This example comes from Milch et al. (2008). Suppose a new workshop is organized. If the workshop is popular (i.e., many people attend it), it could be the beginning of a series. Whether or not a person is likely to attend depends on the topic.We introduce a random variable T, which indicates the topic of the workshop, and a random variable S, which indicates whether the workshop becomes a series. We look at N people, and for each person i we include a random variable Ai, which indicates whether I am participating in it.Each random variable has a finite domain from which it takes values, i.e. {ai, ml,.} for T, {yes, no} for S, and {true, false} for each ai.The common probability distribution of these variables can be specified by an undirected graphic model. A number of factors shows dependencies between the random variables in such a model. In our model, there are two types of factors."}, {"heading": "2.2 Variable Elimination", "text": "From now on we refer to the values which are calculated by means of a variable with the corresponding lower case letters (e.g. ai as shorthand for Ai = ai). (Suppose we want the marginal probability distribution Pr (S) = 0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-"}, {"heading": "2.3 Lifted Inference: Exploiting Symmetries Among Factors", "text": "In the above example, VE obtained exponential acceleration compared to the previously discussed naive calculation by avoiding many redundant calculations and reduced the computation time from O (2N) to O (N). N can still be large. One can become even more efficient if one knows that certain factors have the same potential function. In our example, VE calculates the same product N times: in expression 6 the factors insp1 (Ai, S) and insp2 (T, Ai) are the same for all i, and thus their product insp12 (Ai, S, T) = \u03c61 (Ai, S) insp2 (T, Ai) is cancelled. It also calculates the sum insp12 (Ai, S, T) N times. This redundancy arises because in our probability model all N people behave the same way (Ai, i.e. all Ai calculations are interchangeable."}, {"heading": "2.4 Lifted Inference: Exploiting Symmetries within Factors", "text": "Let us now consider a second elimination order, in which we first have T and then the Ai: P-r (S) = 1 cost (O-value) (O-value) (A-value) = 1 cost (O-value) (A-value) = 1 cost (O-value) = 1 cost (O-value) (N-value) (N-value) (N-value) (N-value) = 1 cost (A-value) (N-value)) (N-value)) (N-value) (N-value) = 1 cost) (A-value) (A-value))). The inner sum (T) first multiplies all factors (T, Ai) into a factor (T, A1, A1,)."}, {"heading": "2.5 Capturing the Symmetries", "text": "It is clear that cancellation can result in important accelerations if certain symmetries between factors or between the inputs of an individual factor are actually present. In order to use these, it is essential that one can specify which variables are interchangeable and therefore can cause these symmetries. However, in our example from the workshop it is assumed that, for example, not every person has the same preferences regarding topics, but there are two types of people and different potentials (\u2264 1 and \u03c62b) are associated with each type of person. It is clear that instead of formula 7, \u2211 T-1, not every person has the same preferences regarding topics, but there are two types of people, and different potentials involving a language (T, Ak).It is clear that instead of formula 7, \u2211 T-1. \""}, {"heading": "3. Representation", "text": "Such symmetries often occur in models that have repetitive structures, such as plates (Getoor & Taskar, 2007, Ch. 7) or, more generally, in probabilistic-logical models. Probabilistic-logical modelling languages (also called probabilistic-relational languages) combine the objective and inferential aspects of first-order logic with those of probability theory. First-order logic languages refer to objects (possibly of different types) in any universe and to properties or relationships between these objects. Formulas in these languages can express that a property applies to a particular object or to a whole series of objects. For example, the fact that all humans are mortal could be written as \"x\": Human (x) \u2192 Mortal (x) \u2192 Mortal (x) - logical models can similarly express the true knowledge of all objects."}, {"heading": "3.1 A Constraint-based Representation Formalism", "text": "There are a number of random variables of which one can say that they are either in a position to move, or in a position to move, or in a position to move, in a position to be, in a position to be, in a position to be, in a position to be, in a position to feel, in a position to feel, in a position to feel, in a position to feel, in a position to feel, in a position to understand, in a position to understand, in a position to be, in a position to put oneself, in a position to put oneself, in a position to understand, in a position to understand oneself, in a position to understand oneself, in a position to understand oneself, in a position to understand oneself, in a position to understand, in a position to understand, in a position to understand, in a position to understand oneself, in a position to understand oneself, in a position to understand, in a position to understand oneself, in a position to understand oneself, in a position to understand oneself, in a position to understand, in a position to understand on eself, in a position to understand oneself, in a position to understand, in a position to understand oneself, in a position to understand on eself, in a position to understand, in a position to understand on eself, in a position to understand, in a position to understand on eself, in a position to understand, in a position to understand on eself, in a position to understand, in a position, in a position to understand, in a position in a position to understand, in a position in a position in a position in a position to understand, in a position in a position in a position to understand oneself, in a position to understand, in a position in a position in a position to understand, in a position in a position in a position in a position in a position in a position in a position in a position in a position in a position a position in a position in a position in a position in a position in a position a position in a position in a position in a position in a position in a position in a position a position a position a position a position a position a position in a position a position a position a position a position a position a position a position a position a position a position a position a position"}, {"heading": "3.2 Counting Formulas", "text": "Milk et al. (2008) introduced the idea of counting formulas and (parameterized) counting Randvars.A counting formula is a syntactical construct of the form # Xi-C [P (X)], where Xi-X is the counted logvar.A grounded counting formula is a counting formula in which all the arguments of the atom P (X), except the counted logvar, are constants. It defines a counting randvar (CRV) whose meaning is as follows. First, we define the set of Randvars that it covers as RV (# X). C [P]) = RV (P)] = RV (P).The value of CRV is determined by the values of the Randvars it covers. More specifically, it is a histogram that indicates that a rating of RV (P) is given."}, {"heading": "4. The GC-FOVE Algorithm: Outline", "text": "In fact, the fact is that most of them will be able to move to a different world in which they are able than to another world in which they are able, in which they live, in which they live, in which they will live."}, {"heading": "4.1 Constraint Language", "text": "In C-FOVE, a constraint is a set of pairs of equations between a single logvar and a constant, or between two logvars. Thus, for example, C-FOVE can represent a friend (X, Y) | X 6 = ann, but not a friend (X, Y) (X, Y). Table 1 provides some more examples of PRVs that C-FOVE cannot / can not represent, and Figure 3 illustrates this visually. Basically, C-FOVE can only use conjunctive constraints, not disjunctive and C-FOVE operators, to operate directly on that representation. GC-FOVE, on the other hand, allows constraint on the logvars, and can therefore handle all these constraints."}, {"heading": "4.2 Lifted Absorption", "text": "Absorption (van der Gaag, 1996) is an additional operator in VE that is known to increase efficiency by removing a random variable from a model when its evaluation is known, and rewriting the model into an equivalent that does not contain the variable. C-FOVE, like its predecessors, does not use absorption, and its inclusion could actually have adverse effects due to symmetry breakage. However, the extensively complete limitation language of GC-FOVE not only allows absorption to be used more effectively, but even to eliminate it."}, {"heading": "4.3 Summary of Contributions", "text": "We are now at a point where we can more precisely summarize the contributions of this theory.1. We present the first description of the elimination of collected variables, which decouples the collected inference algorithm from the constraint representation it uses, by taking the C-FOVE algorithm and redefining its operators so that they become independent of the underlying constraint mechanism. This is achieved by defining the operators in terms of relational algebra operators. This redefinition generalizes the operators and clarifies at a higher level how they work.2. We present a mechanism for displaying constraints that is extensively complete. It is closed among the relational algebra operators and enables their efficient execution. In itself, this is a minor contribution, but it is necessary to maintain an operative system.3. We present a new operator that is capable of controlling the high-level effects of our own 5.We demonstrate the constration.4)."}, {"heading": "5. GC-FOVE\u2019s Operators", "text": "This section provides detailed information on the operators of GC-FOVE, which can conceptually be divided into two categories: operators who manipulate potential functions, and operators who refine the model so that the first type of operator can be applied. We will start with three operators that belong to the first category: upscale multiplication, upscale summing, and conversion, which can be considered as generalized versions of the corresponding operators of C-FOVE; algorithmically, they are similar. Next, we will discuss the fragmentation, fragmentation, expansion, and counting of normalization. Because they work specifically on the constraints, they are more different from the operators of C-FOVE. We will systematically compare them with the latter, and show each time that the constraint language and operators of C-FOVE force them to create finer-grained models than necessary, while GC-FOVE, due to its extensively complete constraint language, always represents diesel and constraint language."}, {"heading": "5.1 Lifted Multiplication", "text": "The raised multiplication operator multiplies whole parameters at once, rather than multiplying the soil factors separately that cover both factors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008). Figure 4 illustrates this for two factors g1 = 0 factors (S (X)) | C and g2 = 0 (S (S (S) {S (X), A (X) | C, where C = (X) \u2032., \"xn). Lifted multiplication is equivalent to n multiplications on the ground level. The above figure is deceptively simple for several reasons, with the naming of the logvars indicating that logvars in g1 X in g2 coincide. In fact, g2 could have several logvars with different names. An alignment between the parameters is necessary and shows how logvars correspond to each other in different parameters (de Salvo Braz, 2007)."}, {"heading": "5.2 Lifted Summing-Out", "text": "As soon as a PRV occurs in only one parfactor (see Figure 6), we can sum up a factor j from that parfactor (Milk et al., 2008). We start with an example of an increased summation that will help motivate the formal definition of the operator. Example 8. Consider parfactor g = \u03c6 (S (X), F (X), Y (X, Y), C (X), in which C = {(xi, yi, j). We assume that Y is standardized as a numerical value w.r.t X in C. We want to sum up the randvars F (xi, yi), i), RV (F) | C) on the ground level. Each Randvar F (xi, yi) appears in exactly one soil factor i (S (xi), F (xi, yi)."}, {"heading": "5.3 Counting Conversion", "text": "The counting of Randvars can be present in the original model, but it can also be replaced by an operation called conversion (Milch et al., 2008) (see also Section 2.4). To see why this is useful, we consider a parfactor g = \u03c6 (S (X), F (X, Y))) | C, with C = {xi} n i = 1 \u00b7 {yj} m j = 1, and assume that we want to eliminate S (X). To do this, we must first make sure that each S (xi) occurs in only one factor. On the ground, this can be achieved for a given S (xi) by multiplying all factors (S)."}, {"heading": "5.4 Splitting and Shattering", "text": "If the conditions for cancelling the multiplication, addition and conversion are not met, it is necessary to reformulate the model with respect to parameters that meet these factors. For example, if the g1 = \u03c61 (S (X)) and g2 (S (S), which include both factors, cannot be multiplied without creating unwanted de-operator conversion, then it is possible to implement the G1 and g2 (S) conversion: (1) g = 2 (A). G: a parameter in G (2). X: a logvar in logvar (A). It is exactly an atom-Ai with X (2) logvar in logvar (A) logvar in logvar (3)."}, {"heading": "5.5 Expansion of Counting Formulas", "text": "When handling parameters with counting formulas to rewrite a P (C) RV into the independent, we use the process of expansion (Milch et al., 2008). If we split a group randvars RV (V) into a partition {RV (Vi)} m i = 1, each count randvar \u03b3 counting the values of RV (V) must be expanded, i.e. replaced by a group randvars. Example 10. Suppose we have to count the values of the Randvars in RV (Vi). In parallel, the potential that originally had V as argument must be replaced by a potential that has all Vi as arguments; we call this potential expansion.Example 10. Suppose we have to split g1 = \u03c61 (# S) (X)))."}, {"heading": "5.5.1 Expansion of CRVs", "text": "Let us first consider the simplest possible type of CRV: # X [P (X)] # # # # # # # # # # (P) # # # # (P) # # (P) = (C) It counts how many values of X in C, P (X) have a certain value. If one of them is empty, the other is equal to C, which means that the CRV can stay as it is and no expansion is necessary. In the following, we assume that splitting C into two non-empty subsets C1 and C2 {P (X) # # # C2 is trivial, but one problem is that both of the resulting formulas will occur in a single parameter, and a constraint is always associated with a parameter, not with a particular argument of a parameter."}, {"heading": "5.5.2 Expansion of PCRVs", "text": "Consider the case where \u03c0X\\ {X} (C) is not a single factor, i.e. we have a parameterized CRV that represents a group of CRVs, counting the values of each subset of RV (V). Given a division of condition C, we must expand each underlying CRV and the corresponding potential. Condition C \u00b2 (C) is no longer singleton: it associates the correct values of X1 and X2 with each tuple in XX\\ {X} (C). However, since the result of the potential expansion depends on the size of the partitions, n1 and n2, only those CRVs that have the same (n1, n2) result in identical potentials after expansion and can be grouped into a factor."}, {"heading": "5.6 Count Normalization", "text": "If this property does not apply, it can be achieved by normalizing the parameter involved, resulting in a partition of the parameter into parameters to which the property applies (Milk et al., 2008). Specifically, this property requires a partition of the parameter into parameters to which the property applies (Milk et al., 2008). Specifically, the parameter is split according to definition 1. The formal definition of the count normalization is count-normalized in operator 6.Operator. Inputs: (1) g = group (A) | C: a parameter in G (2) Y | Z: sets of logvars specifying the desired normalization in C conditions."}, {"heading": "5.7 Absorption: Handling Evidence", "text": "When the value of a randvar is observed, this makes the probable conclusion more efficient: the randvar can be removed from the model, which can introduce additional dependencies into the model. However, there is also a negative effect: observations can break symmetries among random factors. Therefore, it is important to treat observations in a way that preserves as much symmetry as possible. In order to deal effectively with observations in an increased way, we introduce the novel operator of increased absorption. In the ground setting, absorption works as follows (van der Gaag, 1996). In the face of a factor (A) and an observation Ai = ai with Ai, absorption replaces the proof (A) with a factor of increased absorption. In the ground setting, it works as follows (van der Gaag,"}, {"heading": "5.8 Grounding a Logvar", "text": "To illustrate this, we consider a model that consists of a single parfactor \u03c6 (R (X, Y), R (Y, Z), R (X, Z)) | C, which expresses a probable variant of transitivity. As there is only one factor, no multiplications are required before the elimination of variables can begin. However, due to the structure of the parameter, no single PRV can be eliminated (the prerequisites for elevated summing and conversion are not met, and none of the other operators can change this). In cases like this, when no other operators can be applied, VE can always resort to a last operator: the grounding of a logvarX in a parfactor g (de Salvo Braz, 2007; Milch et al., 2008)."}, {"heading": "6. Representing and Manipulating the Constraints", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "7. Experiments", "text": "We assume that the ability to perform fewer calculations by capturing more symmetries will far outweigh these costs in typical inference tasks. In this section, we will compare the performance of C-FOVE and GC-FOVETREES (GC-FOVE using the tree representation from Section 6) to empirically confirm this hypothesis. In particular, we will examine how the performance varies depending on two parameters: (i) the domain size and (ii) the amount of evidence. We will also empirically investigate whether GC-FOVETREES can solve inference tasks that are beyond the reach of C-FOVE. In this section, GC-FOVE stands for GC-VETREES."}, {"heading": "7.1 Methodology and Datasets", "text": "We are comparing C-FOVE and GC-FOVE with synthetic and real-world data based on several sequence tasks. We are using the version of C-FOVE extended by a general parfactor multiplication (de Salvo Braz, 2007).3 For the implementation of GC-FOVE, we used the publicly available C-FOVE code (Milch, 2008), so that implementations are as comparable as possible.4 In all experiments, the undirected model has parameters whose limitations are all representative.3 This allows C-FOVE to perform some tasks in completely different ways where it would otherwise have to resort to grounding, e.g. the domain of social networks (Jha et al., 2010).4. GC-FOVE is available at http: / / dtai.cs.kuleuven.be / ml / systems / gc-fove.by C-FOVE. Thus, GC-FOVE has no initial advantage, which makes the comparison conservative."}, {"heading": "7.1.1 Experiments with Synthetic Data", "text": "With regard to synthetic data, we evaluate our algorithm based on three standard benchmark problems. The first domain is called workshop attributes (Milk et al., 2008). Here, m describes different attributes (e.g. theme, date, etc.) of the workshop, and a corresponding factor for each attribute indicates the dependence between the participation of each person and the attribute. The second domain includes the following parameters. \u03c61 (Attends (X), Attr1)... \u03c6m (Attends (X), Attrm) \u03c6m + 1 (Attends (X), series) The second domain is called competing workshops (Milch et al., 2008). It models the fact that people are more likely to attend a workshop when it is a \"hot topic\" and that the number of participants influences whether the workshop becomes a series (X). The theory contains the following parameters (Attends (X) and Attrivars (Y)."}, {"heading": "7.1.2 Experiments with Real-World Data", "text": "The first, WebKB (Craven & Slattery, 1997), contains data on more than 1200 web pages, including their class (e.g. \"course page\"), text content (set of words), and hyperlinks between pages. The model consists of several parameters that indicate, for example, how the classes of two linked pages depend on each other. Our inference task concerns linking prediction, where the class information for a subset of all pages is observed, and the task is to calculate the probability of a hyperlink between two pages. We use a page class that dictates in the model for each run, and the average runtime over multiple runs for each class. We used the following set of parameters."}, {"heading": "7.2 Influence of the Domain Size", "text": "In the first group of experiments, we use the synthetic data sets to measure the effect of domain size (number of objects) on runtime. We vary the domain size from 50 to 1000 objects and keep the proportion of observed edge vars (relative to the number of observable edge vars) constant at 20%. Figures 10 (a) to 10 (c) show the performance on all three synthetic data sets. In all three models, GC-FOVE outperforms C-FOVE in all domain sizes. As the number of objects in the domain increases, the runtime for both algorithms increases. In the social network domain, the runtime increases at a much slower rate than that of C-FOVE in all three models. In the first two tasks, GC-FOVE is between one and two orders of magnitude faster than C-GOVE, in the largest domain sizes. In the social network domain, the difference in the size of the FOC-VE domain is not observed with a fixed VE group: FOC-1000 objects cannot be more clearly observed with a FOE-VE."}, {"heading": "7.3 Influence of the Amount of Evidence", "text": "In the second group of experiments, we measure the effect of the percentage of observed Randvars on runtime using synthetic datasets. We fix the domain size and vary the percentage of observed Randvars from 0% to 100%. Note that this is a percentage of all \"observable\" Randvars (e.g. all Randvars of the form Smokes (x)), not all Randvars of any kind (i.e. 100% does not mean that there are no more unobserved variables). Figures 11 (a) to 11 (c) show the performance on all three synthetic domains with the size of 1000 objects. To better demonstrate the behavior of C-FOVE on the social domain, Figure 11 (d) shows the performance on a domain with only 25 objects. Both algorithms show similar trends in the three domains."}, {"heading": "7.4 Performance on Real-World Data", "text": "In the final set of experiments, we compared the algorithms of the two real datasets WebKB and Yeast. On both datasets, we varied the percentage of observed page classes or functions from 0% to 100% in increments of 10%. Figures 12 (a) and 12 (b) illustrate the results. In these experiments, C-FOVE was only able to solve the zero-proof problems; in the other cases, it typically ran out of memory after up to an hour of computing time on a machine with 30 GB of memory. Its failure is primarily due to the large number of observations, which often forces it to resort to ground-level inferences for a large number of objects. GC-FOVE, on the other hand, is successful for all experimental conditions. Moreover, GC-FOVE can consistently solve the problems in a few seconds."}, {"heading": "8. Conclusions", "text": "Astonishingly, most advanced inference algorithms use the same class of constraints based on pairing (in) equality (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inferences by means of upscale dissemination (Singla & Domingos, 2008). In this paper, we have shown that this class of constraints is excessively restrictive. We have proposed extensively using complete constraint languages that can capture more symmetry between objects and allow for more upper-level operations."}, {"heading": "Acknowledgments", "text": "Daan Fierens is supported by the Flemish Research Foundation (FWO-Vlaanderen), Jesse Davis is partially supported by the KULeuven Research Fund (CREA / 11 / 015 and OT / 11 / 051) and the EU FP7 Marie Curie Career Integration Grant (# 294068), which was funded by GOA / 08 / 008 \"Probabilistic Logic Learning\" of the KULeuven Research Fund. The authors thank Maurice Bruynooghe and Guy Van den Broeck for interesting discussions and comments on this work and this text, as well as the reviewers for their constructive comments and very concrete suggestions for improving the article."}, {"heading": "Appendix A. Correctness Proof for Lifted Absorption", "text": "In this appendix, we prove the correctness of the novel absorption operator. We begin by providing some lemmas.Remember that a set of parameters G is a compact method for defining a set of factors where Gr (G) = {f | f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f (Af).Furthermore, G \u00b2 f \u00b2 f \u00b2 f \u00b2 f (G) defines the same probability distribution. So formally: G \u00b2 G \u00b2 PG (A) = PG \u00b2 p (A) = PG \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f \u00b2 f (G) f \u00b2 f \u00b2 f (G).The following lemmas are easy to prove by applying the above definition and adhering to the fact that gr (G \u00b2) f \u00b2 f = gr (G) gr (G).Lemma 1."}, {"heading": "Appendix B. Computational Complexity of Lifted Absorption", "text": "Applying the increased absorption to a parameter g = \u03c6 (A) | C has the complexity O (| C |) + O (Size (\u03c6) \u00b7 log | C |), where | C | is the cardinality (number of tuples) of the constraint C, and Size (\u03c6) corresponds to the product of the range magnitudes of arguments A, i.e., Size (\u03c6) = VP-A | range (Ai) |. The first term in the complexity, O (| C |), arises because the absorption contains a projection of the constraint C, which in the worst case (with an extended representation) has the complexity O (| C |). The second term, O (Size (\u03c6) \u00b7 log | C |), is the complexity of the calculation of the new potential function, which involves manipulation of the inputs of Size (\u03c6) (in a tabular representation), and its exponence having complexity (O | C)."}], "references": [{"title": "Extended lifted inference with joint formulas", "author": ["U. Apsel", "R.I. Brafman"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Apsel and Brafman,? \\Q2011\\E", "shortCiteRegEx": "Apsel and Brafman", "year": 2011}, {"title": "Lifted inference for relational continuous models", "author": ["J. Choi", "D. Hill", "E. Amir"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "Relational learning with statistical predicate invention: Better models for hypertext", "author": ["M. Craven", "S. Slattery"], "venue": "Machine Learning,", "citeRegEx": "Craven and Slattery,? \\Q1997\\E", "shortCiteRegEx": "Craven and Slattery", "year": 1997}, {"title": "An integrated approach to learning Bayesian networks of rules", "author": ["J. Davis", "E.S. Burnside", "I. de Castro Dutra", "D. Page", "V.S. Costa"], "venue": "In Proceedings of 16th European Conference on Machine Learning (ECML),", "citeRegEx": "Davis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2005}, {"title": "Probabilistic inductive logic programming: Theory and applications", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz"], "venue": "Ph.D. thesis,", "citeRegEx": "Braz,? \\Q2007\\E", "shortCiteRegEx": "Braz", "year": 2007}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Braz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2005}, {"title": "Lifted arbitrary constraint solving for lifted probabilistic inference", "author": ["R. de Salvo Braz", "S. Saadati", "H. Bui", "C. OReilly"], "venue": "In Proceedings of the 2nd International Workshop on Statistical Relational AI (StaRAI),", "citeRegEx": "Braz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2012}, {"title": "Constraint processing", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q2003\\E", "shortCiteRegEx": "Dechter", "year": 2003}, {"title": "An Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": null, "citeRegEx": "Getoor and Taskar,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "Probabilistic theorem proving", "author": ["V. Gogate", "P. Domingos"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Gogate and Domingos,? \\Q2011\\E", "shortCiteRegEx": "Gogate and Domingos", "year": 2011}, {"title": "Lifted inference seen from the other side : The tractable features", "author": ["A. Jha", "V. Gogate", "A. Meliou", "D. Suciu"], "venue": "In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Jha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jha et al\\.", "year": 2010}, {"title": "Counting belief propagation", "author": ["K. Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kersting et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kersting et al\\.", "year": 2009}, {"title": "Constraint processing in lifted probabilistic inference", "author": ["J. Kisynski", "D. Poole"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kisynski and Poole,? \\Q2009\\E", "shortCiteRegEx": "Kisynski and Poole", "year": 2009}, {"title": "Lifted aggregation in directed first-order probabilistic models", "author": ["J. Kisynski", "D. Poole"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Kisynski and Poole,? \\Q2009\\E", "shortCiteRegEx": "Kisynski and Poole", "year": 2009}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "First-order bayes-ball", "author": ["W. Meert", "N. Taghipour", "H. Blockeel"], "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Meert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meert et al\\.", "year": 2010}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Milch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2008}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Poole,? \\Q2003\\E", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Database management systems (3", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": null, "citeRegEx": "Ramakrishnan and Gehrke,? \\Q2003\\E", "shortCiteRegEx": "Ramakrishnan and Gehrke", "year": 2003}, {"title": "Bisimulation-based approximate lifted inference", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Sen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2009}, {"title": "Prdb: managing and exploiting rich correlations in probabilistic databases", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "VLDB Journal,", "citeRegEx": "Sen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2009}, {"title": "Lifted first-order belief propagation", "author": ["P. Singla", "P. Domingos"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Singla and Domingos,? \\Q2008\\E", "shortCiteRegEx": "Singla and Domingos", "year": 2008}, {"title": "Approximate Lifted Belief Propagation", "author": ["P. Singla", "A. Nath", "P. Domingos"], "venue": "In Proceedings of the 1st International Workshop on Statistical Relation AI (StaRAI),", "citeRegEx": "Singla et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2010}, {"title": "Lifted variable elimination with arbitrary constraints", "author": ["N. Taghipour", "D. Fierens", "J. Davis", "H. Blockeel"], "venue": "In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Taghipour et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2012}, {"title": "Conditioning in first-order knowledge compilation and lifted probabilistic inference", "author": ["G. Van den Broeck", "J. Davis"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Broeck and Davis,? \\Q2012\\E", "shortCiteRegEx": "Broeck and Davis", "year": 2012}, {"title": "Lifted Probabilistic Inference by First-Order Knowledge Compilation", "author": ["G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L. De Raedt"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Broeck et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Broeck et al\\.", "year": 2011}, {"title": "On evidence absorption for belief networks", "author": ["L.C. van der Gaag"], "venue": "Int. J. Approx. Reasoning,", "citeRegEx": "Gaag,? \\Q1996\\E", "shortCiteRegEx": "Gaag", "year": 1996}], "referenceMentions": [{"referenceID": 17, "context": "To address this problem, Poole (2003) introduced the concept of lifted inference for graphical models.", "startOffset": 25, "endOffset": 38}, {"referenceID": 17, "context": "Section 2 illustrates the principles of lifted variable elimination by example, and briefly states how this work improves upon the state of the art, C-FOVE (Milch et al., 2008).", "startOffset": 156, "endOffset": 176}, {"referenceID": 17, "context": "1 The Workshop Example This example is from Milch et al. (2008). Suppose a new workshop is organized.", "startOffset": 44, "endOffset": 64}, {"referenceID": 17, "context": "At the time of writing, the C-FOVE system (Milch et al., 2008) is considered the state of the art in lifted variable elimination.", "startOffset": 42, "endOffset": 62}, {"referenceID": 18, "context": "We use a representation formalism based on undirected graphical models that is closely related to the one used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 158, "endOffset": 212}, {"referenceID": 17, "context": "We use a representation formalism based on undirected graphical models that is closely related to the one used in earlier work on lifted variable elimination (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 158, "endOffset": 212}, {"referenceID": 17, "context": "The concepts introduced in this section have also been introduced in earlier work (de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 82, "endOffset": 123}, {"referenceID": 15, "context": "1 A Constraint-based Representation Formalism An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001).", "startOffset": 140, "endOffset": 166}, {"referenceID": 8, "context": ",Xn) is a tuple of logvars, and CX is a subset of D(X) = \u00d7iD(Xi) (Dechter, 2003).", "startOffset": 65, "endOffset": 80}, {"referenceID": 14, "context": "1 A Constraint-based Representation Formalism An undirected model is a factorization of a joint distribution over a set of random variables (Kschischang et al., 2001). Given a set of random variables X = {X1,X2, . . . ,Xn}, a factor consists of a potential function \u03c6 and an assignment of a random variable to each of \u03c6\u2019s inputs. For instance, the factorization f(X1,X2,X3) = \u03c6(X1,X2)\u03c6(X2,X3) contains two different factors (even if their potential functions are the same). Likewise, in our probabilistic-logical representation framework, a model is a set of factors. The random variables they operate on are properties of, and relationships between, objects in the universe. We now introduce some terminology to make this more concrete. We assume familiarity with set and relational algebra (union \u222a, intersection \u2229, difference \\, set partitioning, selection \u03c3C , projection \u03c0X , attribute renaming \u03c1, join \u22b2\u22b3); see, for instance, the work of Ramakrishnan and Gehrke (2003). The term \u201cvariable\u201d can be used in both the logical and probabilistic context.", "startOffset": 141, "endOffset": 975}, {"referenceID": 15, "context": "2 Counting Formulas Milch et al. (2008) introduced the idea of counting formulas and (parametrized) counting randvars.", "startOffset": 20, "endOffset": 40}, {"referenceID": 13, "context": "We use the definition of Kisynski and Poole (2009a) for parfactors, as it allows us to simplify the notation.", "startOffset": 25, "endOffset": 52}, {"referenceID": 17, "context": "At a high level, it is similar to C-FOVE (Milch et al., 2008), the current state-of-the-art system in lifted variable elimination, but it differs in the definition and implementation of its operators.", "startOffset": 41, "endOffset": 61}, {"referenceID": 18, "context": "1 Lifted Multiplication The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 160, "endOffset": 214}, {"referenceID": 17, "context": "1 Lifted Multiplication The lifted multiplication operator multiplies whole parfactors at once, instead of separately multiplying the ground factors they cover (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 160, "endOffset": 214}, {"referenceID": 17, "context": "2 Lifted Summing-Out Once a PRV occurs in only one parfactor, it can be summed out from that parfactor (Milch et al., 2008).", "startOffset": 103, "endOffset": 123}, {"referenceID": 17, "context": "3 Counting Conversion Counting randvars may be present in the original model, but they can also be introduced into parfactors by an operation called counting conversion (Milch et al., 2008) (see also Section 2.", "startOffset": 169, "endOffset": 189}, {"referenceID": 17, "context": "This Mul function is identical to Milch et al.\u2019s (2008) num-assign.", "startOffset": 34, "endOffset": 56}, {"referenceID": 5, "context": "A similar condition for FOVE\u2019s counting elimination is mentioned by de Salvo Braz (2007). To see why precondition 3 is necessary, consider the parfactor g = \u03c6(S(X),#Y [A(Y )]) |(X,Y ) \u2208 {(x1, y2), (x1, y3), (x2, y1), (x2, y3), (x3, y1), (x3, y2)}, which does not satisfy it.", "startOffset": 77, "endOffset": 89}, {"referenceID": 18, "context": "The above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 51, "endOffset": 105}, {"referenceID": 17, "context": "The above is a simple case of splitting parfactors (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 51, "endOffset": 105}, {"referenceID": 18, "context": "C-FOVE operates per logvar, and splits off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch et al., 2008).", "startOffset": 112, "endOffset": 145}, {"referenceID": 17, "context": "C-FOVE operates per logvar, and splits off each value in a separate partition (splitting based on substitution) (Poole, 2003; Milch et al., 2008).", "startOffset": 112, "endOffset": 145}, {"referenceID": 17, "context": "5 Expansion of Counting Formulas When handling parfactors with counting formulas, to rewrite a P(C)RV into the proper from, we employ the operation of expansion (Milch et al., 2008).", "startOffset": 161, "endOffset": 181}, {"referenceID": 17, "context": "C-FOVE uses expansion based on substitution (Milch et al., 2008).", "startOffset": 44, "endOffset": 64}, {"referenceID": 17, "context": "When this property does not hold, it can be achieved by normalizing the involved parfactor, which amounts to splitting the parfactor into parfactors for which the property does hold (Milch et al., 2008).", "startOffset": 182, "endOffset": 202}, {"referenceID": 17, "context": "In cases like this, when no other operators can be applied, lifted VE can always resort to a last operator: grounding a logvarX in a parfactor g (de Salvo Braz, 2007; Milch et al., 2008).", "startOffset": 145, "endOffset": 186}, {"referenceID": 11, "context": ", on the social network domain (Jha et al., 2010).", "startOffset": 31, "endOffset": 49}, {"referenceID": 17, "context": "The first domain is called workshop attributes (Milch et al., 2008).", "startOffset": 47, "endOffset": 67}, {"referenceID": 17, "context": "\u03c6m(Attends(X), Attrm) \u03c6m+1(Attends(X), Series) The second domain is called competing workshops (Milch et al., 2008).", "startOffset": 95, "endOffset": 115}, {"referenceID": 11, "context": "The third domain is called social network (Jha et al., 2010) and it models people\u2019s smoking habits, their chance of having asthma, and the dependence of a persons habits and diseases on their friendships.", "startOffset": 42, "endOffset": 60}, {"referenceID": 18, "context": "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).", "startOffset": 114, "endOffset": 247}, {"referenceID": 17, "context": "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).", "startOffset": 114, "endOffset": 247}, {"referenceID": 11, "context": "Surprisingly, most lifted inference algorithms use the same class of constraints based on pairwise (in)equalities (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Jha et al., 2010; Kisynski & Poole, 2009b; Van den Broeck et al., 2011); the main exception is the work on approximate inference using lifted belief propagation (Singla & Domingos, 2008).", "startOffset": 114, "endOffset": 247}, {"referenceID": 10, "context": ", the works of Jha et al. (2010) and Van den Broeck et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 10, "context": ", the works of Jha et al. (2010) and Van den Broeck et al. (2011), and further optimizing constraint handling.", "startOffset": 15, "endOffset": 66}, {"referenceID": 5, "context": "With respect to the latter, an interesting direction is the recent work of de Salvo Braz, Saadati, Bui, and OReilly (2012) that employs a logical representation for constraints, which is extensionally complete, and presents specialized constraint processing methods for this representation.", "startOffset": 84, "endOffset": 123}], "year": 2013, "abstractText": "Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. In this article, we decouple lifted inference from the constraint language. We define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints\u2019 extension) rather than on the syntactic level, making them language-independent. As a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. We empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.", "creator": "dvips(k) 5.992 Copyright 2012 Radical Eye Software"}}}