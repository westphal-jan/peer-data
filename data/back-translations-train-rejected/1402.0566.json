{"id": "1402.0566", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs", "abstract": "This article presents the state-of-the-art in optimal solution methods for decentralized partially observable Markov decision processes (Dec-POMDPs), which are general models for collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative Bayesian games (CBGs), we describe several advances that greatly expand the range of Dec-POMDPs that can be solved optimally. First, we introduce lossless incremental clustering of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing optimality. Second, we introduce incremental expansion of nodes in the GMAA* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the nodes depth. This is particularly beneficial when little clustering is possible. In addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec-POMDPs. We provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally, we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.", "histories": [["v1", "Tue, 4 Feb 2014 01:35:59 GMT  (7334kb)", "http://arxiv.org/abs/1402.0566v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["frans adriaan oliehoek", "matthijs t j spaan", "christopher amato", "shimon whiteson"], "accepted": false, "id": "1402.0566"}, "pdf": {"name": "1402.0566.pdf", "metadata": {"source": "CRF", "title": "Incremental Clustering and Expansion for Faster Optimal Planning in Decentralized POMDPs", "authors": ["Frans A. Oliehoek", "Christopher Amato", "Shimon Whiteson"], "emails": ["frans.oliehoek@maastrichtuniversity.nl", "m.t.j.spaan@tudelft.nl", "camato@csail.mit.edu", "s.a.whiteson@uva.nl"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "1.1 Contributions", "text": "This year it is more than ever before."}, {"heading": "1.2 Organization", "text": "The article is structured as follows: Section 2 provides background information on the Dec-POMDP model, the GMAA * heuristic search solution method, and appropriate heuristics. Section 3 introduces lossless clustering of CBGs and their integration into GMAA *. Section 4 introduces the gradual expansion of search nodes. Empirical evaluation of the proposed techniques is described in Section 5. We will discuss the related work in Section 6. Future work will be discussed in Section 7 and conclusions will be drawn in Section 8."}, {"heading": "2. Background", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they are able to"}, {"heading": "2.1 Heuristic Search Methods", "text": "Most of these methods fall into one of two categories: dynamic programming and heuristic search methods. Dynamic programming methods adopt a backward or \"bottom-up\" perspective by first considering strategies for the last step t = h \u2212 1 and using them to develop strategies for stage t = h \u2212 2, etc. In contrast, heuristic search methods adopt a forward or top-down perspective by first constructing plans for t = 0 and extending them to later stages. In this article, we focus on the heuristic search approach, which has shown state-of-the-art results. As we clarify in this section, this method can be interpreted as looking over a tree of cooperative Bajian games (CBGs). These CBGs provide a convenient abstraction layer that facilitates the explanation of the techniques introduced in this article."}, {"heading": "2.1.1 Multiagent A*", "text": "Szer, Charpillet and Zilberstein (2005) introduced a common political search method called Multiagent A * (MAA *)."}, {"heading": "2.1.2 The Bayesian Game Perspective", "text": "It is an extension of the well-known strategy game (also known as normal form game) in which each agent holds some private information (Osborne & Rubinstein, 1994).A CBG is a BG in which the agents receive identical payouts.In the Bayesian game perspective, each node q in the GMAA * search tree, along with its corresponding partial common policy t, defines a CBG (Oliehoek, Spaan & Vlassis, 2008)."}, {"heading": "2.2 Heuristics", "text": "To perform a heuristic search, GMAA * defines the heuristic value V (t) on the basis of (2,3). In contrast, the Bayesian game perspective (2,8) uses these two formulations, which are equivalent if the heuristic value V (t) faithfully represents the expected immediate reward (Oliehoek, Spaan, & Vlassis, 2008). As a result, GMAA * is complete over CBGs (and thus finds optimal solutions), as set out in the following theory. Theorem 1. Using a heuristic formula of the formQ (~ successt, a) = Est [R (s t, a) | ~ successt] + 1 [V (~ \u03b8t + 1) | ~ successt, a], (2,9) where V (~ successt + 1) and Qb (~ certaint, a) = Est [R (s, a) = Est [R (s, a), a), ~ quantity of the quantity, a quantity of the quantity, a quantity of the quantity + 1, a quantity of the quantity, a quantity of the quantity + 1 (1), a quantity of the quantity, a quantity of the quantity of the quantity, a (1), a quantity of the quantity of the quantity of the quantity of the quantity, a (1), a quantity of the quantity of the quantity, a quantity of the quantity of the quantity of the quantity of the quantity of the quantity, a (1 + 2), a quantity of the quantity of the quantity of the quantity of the quantity of the quantity of the quantity (1, a), a quantity of the quantity of the quantity of the quantity of the quantity of the quantity of the quantity of the quantity (1, a (1), a (1), a), a quantity of the quantity of the quantity of the quantity of the quantity of the quantity of the quantity of the quantity (1 (1, a (1)."}, {"heading": "2.2.1 QMDP", "text": "One way to obtain a permissible heuristic Q value (~ \u03b8, a) is to resolve the underlying MDP, i.e. to assume that the joint action is chosen by a single \"puppeteer\" agent who can observe the true state. This approach, known as QMDP (Littman, Cassandra, & Kaelbling, 1995), uses the MDP value function Qt, \u0445 M (s t, a), which can be calculated using standard techniques of dynamic programming (Puterman, 1994). To transform the Qt values into Q-M (~ successt, a) values, we calculate: Q-M (~ \u03b8 t, a) = IS-SQt, \u0445 M (s, a) Pr (s | ~ successt, b0) values. (2,10) The solution of the underlying MDP has a linear complexity of time that makes it easy, especially compared to the Dec-PODP, to store the PODP value for each PODP."}, {"heading": "2.2.2 QPOMDP", "text": "Similar to the underlying MDP, the underlying POMDP of a Dec-POMDP ~ Q can be defined directly for shared QDP use, i.e., provided the joint action is selected by a single agent with access to the joint observation. 77. Alternatively, this POMDP can be considered a multi-agent POMDP tree in which the agents can immediately broadcast their private observations.The resulting solution can be used as a heuristic solution known as QPOMDP (Szer et al., 2005; Roth, Simmons, & Veloso, 2005).The optimal QPOMDP value function fulfills: Q-P (b t, a) = R (bt, a) -VDP value of the joint action (ot + 1 | bt, a) max at + 1 Q-P (b, & Veloso).The optimal QPOMDP value function is satisfactory: Q-P (b, a)."}, {"heading": "2.2.3 QBG", "text": "A third heuristic, called QBG, assumes that each actor in the team only has access to his individual observations, but can communicate with a delay of one step.8 We define QBG asQ * B (~ \u03b8t, a) = R (~ \u03b8t, a) + max\u03b2 \u2211 ot + 1 * OPr (ot + 1 | ~ \u03b8t, a) Q * B (~ \u03b8t + 1, \u03b2 (ot + 1)))), (2.12) where \u03b2 = < \u03b21 (o t + 1),..., \u03b2n (o t + 1 n) > is a tuple of individual strategies \u03b2i: Oi \u2192 Ai for the CBG constructed for ~ \u0442t, a. Like QPOMDP, QBG can also be represented by vectors (Varaiya & Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) and the same two precursors of the compilation (Tree and Vector, showing a more reliable vlassis), with a stronger olic (Oliehek)."}, {"heading": "3. Clustering", "text": "In this section we also have a new approach to improving scalability in relation to the individual AOHs. This leads to discussing the number of generated nodes in the GAA and thus the number of generated nodes in the GAA."}, {"heading": "3.1 Lossless Clustering in Dec-POMDPs", "text": "In this section, we discuss lossless clustering based on the concept of probabilistic equivalence. We show that this clustering is lossless by showing that probabilistic equivalence implies the best reaction equivalence, which describes the conditions under which a rational agent selects the same action for two of its types. To prove this implication, we show that the best response depends only on the multi-agent belief (i.e. the probability distribution across states and strategies of the other agents), which is the same for two probabilistically equivalent stories. Relations to other equivalence concepts are discussed in Section 6."}, {"heading": "3.1.1 Probabilistic Equivalence Criterion", "text": "We first present the probability equivalence criterion that can be used to decide whether two individual progression criteria can be included in the evaluation. (2) However, it is likely that both distribution criteria (2) are equivalent (2). (3) These distribution criteria can be regarded as the condition of Pr (s). (3) These probabilities can be regarded as the condition of Pr (s). (2) In sub-sections 3.1.2-3.1.4 we formally prove that PE is a sufficient criterion to guarantee that clustering is loss-free. (In the remainders of Section 3.1.1 we discuss some key properties of the PE criterion in order. (2) We can formally prove that PE is a sufficient criterion."}, {"heading": "3.1.2 Sub-Tree Policies, Multiagent Beliefs and Expected Future Value", "text": "To describe the relationship between multi-agent beliefs and probable equivalence, we must first discuss the strategies that an agent can follow = = 3.5. \"We begin with the introduction of the concept of sub-tree politics. As illustrated in Figure 2 (on page 456), a (deterministic) policy \u03c0i can be presented as a tree with nodes labeled with actions and edges labeled with observations: The root node corresponds to the first action taken, other nodes specify the action for the observation history encoded by the path from the root node. Thus, it is possible to define sub-tree strategies that correspond to sub-tree politics (also illustrated in Figure 2). Specifically, we write the action w ~ o ti = Agents-ti = h \u2212 ti (3.5) for the sub-tree politics of the sub-tree politics of the sub-tree politics, to define sub-strategy-trees corresponds to the sub-tree politics."}, {"heading": "3.1.3 Best-Response Equivalence Allows Lossless Clustering of Histories", "text": "We can refer to the same multi-agent beliefs and conclude that we will always apply the same multi-agent beliefs. Lemma 2 (PE implies multi-agent equivalence) is the same (PE implies multi-agent equivalence) For any other policy (PE implies multi-agent equivalence implicitly implies multi-agent equivalence: PE (~ 2) is the best answer to the question why both strategies (S, 6 = i) imply the same multi-agent policy (S, 6 = i). This problem shows that two AOHs produce the same multi-agent belief when they produce the same multi-agent belief. Intuitively, this gives us a justification for such AOHs together: since a multi-agent belief is a sufficient statistic, we should do the same if we have the same multi-agent belief, but Lemma 2 shows that the same multi-agent beliefs are both multiagent-induced."}, {"heading": "3.1.4 Clustering with Commitment in CBGs", "text": "Although it is now clear that two AOHs summarized in the CBG can be summarized, so that this result operationally requires an additional step (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2 = 6). For this purpose, we use the abstraction layer provided by the Bayesian Games. Let us remember that in the CBG, for one stage, the AOHs correspond to the types that these types want to summarize in the CBG. In order to achieve the clustering of two types of measures, we must introduce a new type of measures that can replace them by defining them: p = i Pr (\u03b8 c i = i), p = i), p = i), p = i, p = i, p = i, p = i, p = p, p, p =, p, p, p =, p, p, p =, p, p, p, p =, p, p, p =, p, p, p, p =, p, p, p =, p, p, p =, p, p, p =, p, p, p =, p =, p =, p, p, p = i, p = i, p = i, p = i, p, p = i, p, p = i, p, p, p = i, p, p = p, p, p, p = i, p, p, p =, p, p, p, p =, p, p =, p, p, p =, p, p, p, p =, p =, p, p, p =, p, p, p, p, p, p =, p =, p =, p, p = i = i, p, p = i, p = i, p, p, p, p =, p =, p = i, p = i, p, p =, p, p =, p =, p, p, p, p, p, p =, p, p, p = i, p =, p = i, p, p =, p,"}, {"heading": "3.2 GMAA* with Incremental Clustering", "text": "In this article we focus on the application possibilities within the CBG group, which each perform a clustering of the individual gradients (types) and only then can apply the (reduced) CBG solution by modifying the Expand procedure (algorithm 5) to cluster the CBG procedure, before we call GenerateAllChildrenForCBG.6, clustering shows a CBG approach. It takes a CBG as input and returns the CBG link."}, {"heading": "3.3 Improved Heuristic Representation", "text": "Since the number of types can be reduced, GMAA * -IC has the potential to scale wider horizons. However, this has important consequences for the calculation of heuristics. Previous research has shown that the upper limit provided by QMDP is often too loose for effective heuristic search (see Fig. 5), that there are two approaches to the calculation that are necessary to store narrower heuristics such as QPOMDP or QBG."}, {"heading": "4. Incremental Expansion", "text": "This year it is so far that it is not even a year before it is time for the next round."}, {"heading": "4.1 GMAA* with Incremental Clustering and Expansion", "text": "We start by using a CBG solution that can generate a new policy. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "4.2 Theoretical Guarantees", "text": "In this section, we prove that GMAA * -IC and GMAA * -ICE are search equivalents. As a direct result, we note that GMAA * -ICE is complete, which means that the integration of incremental expansion is the optimal guarantee of GMAA * -IC. Definition 10. We call two GMAA * variants search equivalent if they select exactly the same sequence of non-wildcard nodes that were previously common strategies for expansion in the search tree using the Select Operator. For GMAA * -IC and GMAA * -ICE, we show that the set of selected nodes is identical. However, the set of extended nodes may be different; in fact, it is precisely these differences that incremental expansion exploits. Theoretically, 5. GMAA * -ICE and GMAA * -IC are search equivalents. Proof proof: The proof is listed in Section A.4 of the appendix."}, {"heading": "4.3 Incremental CBG Solvers", "text": "This year it is more than ever before."}, {"heading": "5. Experiments", "text": "In this section, we will empirically test and validate all proposed techniques: lossless clustering of common histories, step-by-step expansion of search nodes, and hybrid heuristic representations. After introducing the experimental setup, we will compare the performance of GMAA * -IC and GMAA * -ICE with that of GMAA * to a number of benchmark problems from the literature. Next, we will compare the performance of the proposed methods with modern optimal and approximate Dec-POMDP methods, followed by a case study of scaling behavior in terms of the number of agents. Finally, we will compare the memory requirements of hybrid heuristic representation with those of tree and vector representations."}, {"heading": "5.1 Experimental Setup", "text": "The most well-known Dec-POMDP benchmarks are the Dec-Tiger guidelines (Nair et al., 2003) and BroadcastChannel (Hansen et al., 2004) Problems. Dec-Tiger has been extensively reported in Section 2. In BroadcastChannel, two agents must transmit messages via a communication channel, but if both agents simultaneously transmit a collision that is noisily observed by the agents. The FireFighting problem represents a team of n firefighters who have to put out fires in a number of nearby houses (Oliehoek, Spaan, & Vlassis, 2008). Any agent may choose to move into one of the houses to fight fires in that location. If two agents are in the same house, they will completely extinguish any fire there. The (negative) reward of the team of firefighters depends on the intensity of the fire in each house; if all fires have been extinguished, the reward is zero."}, {"heading": "5.2 Comparing GMAA*, GMAA*-IC, and GMAA*-ICE", "text": "We compared GMAA * -IC and GMAA * -D, which use the hybrid QBG representation. While all methods calculate an optimal policy, we expect them to be more efficient than GMAA * -D when we report the results. Furthermore, we expect them to achieve further improvements in terms of acceleration and scaling; the results are shown in Table 2. For all entries in which we report the results, the QBG heuristics could be calculated."}, {"heading": "5.2.1 Analysis of Clustering Histories", "text": "These are averages, as the algorithm in the CBGs is constructed for the last stage without clustering, resulting in clustering of varying sizes. In Dec-Tiger, however, the time taken by GMAA is constant. * -IC is more than 3 orders of magnitude less than that of GMAA * IC. In Dec-Tiger, the time taken by GMAA * -IC is more than 3 orders of magnitude less than that of GMAA * for horizon h = 4, this test problem has 3.82e29 common policies, and no other method has been able to solve it optimally. GMAA * -IC, however, is able to do this in a reasonable time. In Dec-Tiger, there are clear symmetries between 19."}, {"heading": "5.2.2 Analysis of Incremental Expansion", "text": "In Dec-Tiger for h = 5, GMAA * -ICE achieves an acceleration of three orders of magnitude and can calculate a solution for h = 6, as opposed to GMAA * IC. For GridSmall, it achieves a large acceleration for h = 4 and fast solutions for h = 5 and 6, with GMAA * IC running out of memory. Similar positive results are obtained for FireFighting, Cooperative Box Pushing and Recycling Robots. Note that using QMDP GMAA * -ICE is able to calculate solutions well above h = 1000 for the FireFighting problem, which is in stark contrast to GMAA * IC, which only calculates solutions for h = 3 using this heuristics."}, {"heading": "5.2.3 Analysis of Hybrid Heuristic Representation", "text": "Fig. 9 illustrates the memory requirements in terms of the number of parameters (i.e. real numbers) for the tree, vector and hybrid representations for QBG, the latter being calculated according to algorithm 9. Results for the vector representation are omitted when these representations grow beyond the boundaries. Effectiveness of the vector section depends on the problem and complexity of the value function, which can suddenly increase, as is the case in Fig. 9c. These results show that the hybrid representation for multiple Benchmark Dec POMDPs allows significant memory savings, enabling the calculation of narrow heuristics for longer horizons."}, {"heading": "5.3 Comparing to Other Methods", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "5.4 Scaling to More Agents", "text": "All benchmark issues in our results presented so far have been limited to two agents. Here, we present a case study on FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis, 2008), a variation of FireFighting that allows for more agents in which each agent can only fight fires at two houses rather than at all of them. Table 6 highlights the size of these problems, including the total number of common strategies for different horizons. We compared GMAA *, GMAA * -IC, GMAA * -ICE (all with a QMDP heuristic), BruteForceSearch and DP-IPG, with a maximum runtime of 12 hours and operation on an Intel Core i7 CPU averaged over 10 runs. BruteForceSearch is a simple optimal algorithm that lists and evaluates all common strategies, and was implemented in the same code base as the GMAA * variations."}, {"heading": "5.5 Discussion", "text": "Overall, the empirical results show that incremental clustering and expansion can result in dramatic performance gains at various levels. In addition, the results are on Broad-22. In Hotel 1, DP-IPG performs particularly well because the problem structure has a limited accessibility, meaning each actor can fully observe its local problems (but not the other actor's) and in all local states, unless there is an action that dominates all other countries. As a result, DP-IPG can generate a small number of potentially optimal policy approaches. CastChannel illustrates a crucial advantage of our approach: If a problem has a property that makes a large amount of clustering possible, our clustering method automatically exploits property without a predefined representation of it.Of course, not all problems allow large reductions over clustering. A domain property that allows for clustering when the previous common policy is encountered during GMAA * makes the observations superfluous, as with BroadcastChannel and FireFighting we see in certain Demmers."}, {"heading": "6. Related Work", "text": "In this section, we will discuss a number of methods related to the universal strategies proposed in this article, some of which have already been discussed in previous sections. In Section 3, we will point out that our clustering method is closely related to the approach of Emery-Montemerlo et al. (2005), but also fundamentally different because our method is lossless. In Section 3.3, we will discuss links to the approach of Boularias and Chaib-draa (2008), which bundles political values in clusters. However, this is in contrast to our approach, which encompasses history and thus politics itself, leading to greater scalability.In Section 3.1.2, we will discuss the relationship between our notion of probable equivalence (PE) and multiagent belief. However, there is another idea of faith that is applied in the JESP solution method (Nair et al, 2003), which is very similar to PE distribution."}, {"heading": "7. Future Work", "text": "(...). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (It.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (It. (it.). (it.). (it.). (it.). (It. (it.). (it.). (it.). (it.). (It. (it.). (it.). (it. (it.). (it.). (It. (.). (it.). (It. (.). (it.). (it. (.). (it.). (It. (.). (it.). (it. (.).). (it. (it.). (it. (.).). (it. (it.). (it. (it.). (it. (it.). (it.). (it. (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (.). (. (.). (it.). (it.). (it.). (it. (. (it.). (it.). (it.). (it. (it. (it.). (it.). (it.). (. (it.).). (it. (it. (it.). (.).). (it.). (it. (it.). (it.).). (it.).). (it. (it.). (it. (.).). (it.).). (it."}, {"heading": "8. Conclusions", "text": "This article presented a number of methods that advance the state of the art in terms of best practice for Dec-POMDPs. Specifically, we presented several advances that aim to broaden the horizons through which optimal solutions can be found. These advances build on the GMAA * heuristic search approach and include loss-free incremental clustering of GMAA * solved CBGs, incremental expansion of nodes in the GMAA * search tree, and hybrid heuristic representations. We provided theoretical guarantees that both incremental clustering and incremental expansion produce algorithms that are both complete and search-equivalent. Finally, we presented extensive empirical results showing that GMAA * ICE Dec POMDPs can optimally solve unprecedented-size POMDPs."}, {"heading": "Acknowledgments", "text": "We thank Raghav Aras and Abdeslam Boularias for providing their code. Research is supported in part by the AFOSR MURI Project # FA9550-09-1-0538 and in part by the NWO CATCH Project # 640.005.003. M.S. is funded by the FP7 Marie Curie Actions Individual Fellowship # 275217 (FP7-PEOPLE-2010-IEF)."}, {"heading": "Appendix A. Appendix", "text": "Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet Outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet outlet"}], "references": [{"title": "Agent influence as a predictor of difficulty for decentralized problem-solving", "author": ["M. Allen", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence", "citeRegEx": "Allen and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Allen and Zilberstein", "year": 2007}, {"title": "Optimal fixed-size controllers for decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Amato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2006}, {"title": "Optimizing memory-bounded controllers for decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Amato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2010}, {"title": "Finite-state controllers based on Mealy machines for centralized and decentralized POMDPs", "author": ["C. Amato", "B. Bonet", "S. Zilberstein"], "venue": "In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence", "citeRegEx": "Amato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2010}, {"title": "Bounded dynamic programming for decentralized POMDPs", "author": ["C. Amato", "A. Carlin", "S. Zilberstein"], "venue": "In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Incremental policy generation for finite-horizon DEC-POMDPs", "author": ["C. Amato", "J.S. Dibangoye", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Automated Planning and Scheduling", "citeRegEx": "Amato et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2009}, {"title": "An investigation into mathematical programming for finite horizon decentralized POMDPs", "author": ["R. Aras", "A. Dutech"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Aras and Dutech,? \\Q2010\\E", "shortCiteRegEx": "Aras and Dutech", "year": 2010}, {"title": "Analyzing myopic approaches for multi-agent communication", "author": ["R. Becker", "A. Carlin", "V. Lesser", "S. Zilberstein"], "venue": "Computational Intelligence,", "citeRegEx": "Becker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2009}, {"title": "Decentralized Markov decision processes with event-driven interactions", "author": ["R. Becker", "S. Zilberstein", "V. Lesser"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Becker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Transition-independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Becker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2003}, {"title": "Policy iteration for decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "C. Amato", "E.A. Hansen", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2009}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Dynamic Programming and Optimal Control (3rd ed., Vol. I)", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "Solving efficiently decentralized MDPs with temporal and resource constraints", "author": ["A. Beynier", "Mouaddib", "A.-I"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Beynier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beynier et al\\.", "year": 2011}, {"title": "Exact dynamic programming for decentralized POMDPs with lossless policy compression", "author": ["A. Boularias", "B. Chaib-draa"], "venue": "In Proc. of the International Conference on Automated Planning and Scheduling", "citeRegEx": "Boularias and Chaib.draa,? \\Q2008\\E", "shortCiteRegEx": "Boularias and Chaib.draa", "year": 2008}, {"title": "A comprehensive survey of multi-agent reinforcement learning", "author": ["L. Bu\u015foniu", "R. Babu\u0161ka", "B. De Schutter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,", "citeRegEx": "Bu\u015foniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bu\u015foniu et al\\.", "year": 2008}, {"title": "Value-based observation compression for DEC-POMDPs", "author": ["A. Carlin", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Carlin and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Carlin and Zilberstein", "year": 2008}, {"title": "The multiple sequence alignment problem in biology", "author": ["H. Carrillo", "D. Lipman"], "venue": "SIAM Journal on Applied Mathematics,", "citeRegEx": "Carrillo and Lipman,? \\Q1988\\E", "shortCiteRegEx": "Carrillo and Lipman", "year": 1988}, {"title": "Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Exact and Approximate Algorithms for Partially Observable Markov Decision Processes", "author": ["A.R. Cassandra"], "venue": "Unpublished doctoral dissertation, Brown University.", "citeRegEx": "Cassandra,? 1998", "shortCiteRegEx": "Cassandra", "year": 1998}, {"title": "Search algorithms for m best solutions for graphical models", "author": ["R. Dechter", "N. Flerova", "R. Marinescu"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "Dechter et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 2012}, {"title": "Producing efficient errorbounded solutions for transition independent decentralized MDPs", "author": ["J.S. Dibangoye", "C. Amato", "A. Doniec", "F. Charpillet"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems. (Submitted for publication)", "citeRegEx": "Dibangoye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2013}, {"title": "Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs", "author": ["J.S. Dibangoye", "Mouaddib", "A.-I", "B. Chai-draa"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Dibangoye et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2009}, {"title": "Monte Carlo sampling methods for approximating interactive POMDPs", "author": ["P. Doshi", "P. Gmytrasiewicz"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Doshi and Gmytrasiewicz,? \\Q2009\\E", "shortCiteRegEx": "Doshi and Gmytrasiewicz", "year": 2009}, {"title": "Graphical models for interactive POMDPs: representations and solutions", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Doshi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2008}, {"title": "Heuristic search: theory and applications", "author": ["S. Edelkamp", "S. Schr\u00f6dl"], "venue": null, "citeRegEx": "Edelkamp and Schr\u00f6dl,? \\Q2012\\E", "shortCiteRegEx": "Edelkamp and Schr\u00f6dl", "year": 2012}, {"title": "Using evolution strategies to solve DEC-POMDP problems", "author": ["B. Eker", "H.L. Ak\u0131n"], "venue": "Soft Computing\u2014A Fusion of Foundations, Methodologies and Applications,", "citeRegEx": "Eker and Ak\u0131n,? \\Q2010\\E", "shortCiteRegEx": "Eker and Ak\u0131n", "year": 2010}, {"title": "Solving decentralized POMDP problems using genetic algorithms", "author": ["B. Eker", "H.L. Ak\u0131n"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Eker and Ak\u0131n,? \\Q2013\\E", "shortCiteRegEx": "Eker and Ak\u0131n", "year": 2013}, {"title": "Game-Theoretic Control for Robot Teams", "author": ["R. Emery-Montemerlo"], "venue": "Unpublished doctoral dissertation, Carnegie Mellon University.", "citeRegEx": "Emery.Montemerlo,? 2005", "shortCiteRegEx": "Emery.Montemerlo", "year": 2005}, {"title": "Approximate solutions for partially observable stochastic games with common payoffs", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2004}, {"title": "Game theoretic control for robot teams", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun"], "venue": "In Proc. of the IEEE International Conference on Robotics and Automation", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2005}, {"title": "Equivalence notions and model minimization in Markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2003}, {"title": "A framework for sequential planning in multi-agent settings", "author": ["P.J. Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gmytrasiewicz and Doshi,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2005}, {"title": "Learning to communicate in a decentralized environment", "author": ["C.V. Goldman", "M. Allen", "S. Zilberstein"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Goldman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 2007}, {"title": "Optimizing information exchange in cooperative multi-agent systems", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Goldman and Zilberstein,? \\Q2003\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2003}, {"title": "Decentralized control of cooperative systems: Categorization and complexity analysis", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldman and Zilberstein,? \\Q2004\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2004}, {"title": "Communication-based decomposition mechanisms for decentralized MDPs", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldman and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2008}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["E.A. Hansen", "D.S. Bernstein", "S. Zilberstein"], "venue": "In Proc. of the National Conference on Artificial Intelligence", "citeRegEx": "Hansen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2004}, {"title": "Value-function approximations for partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research, 13 , 33\u201394.", "citeRegEx": "Hauskrecht,? 2000", "shortCiteRegEx": "Hauskrecht", "year": 2000}, {"title": "Decentralized control of finite state Markov processes", "author": ["K. Hsu", "S. Marcus"], "venue": "IEEE Transactions on Automatic Control ,", "citeRegEx": "Hsu and Marcus,? \\Q1982\\E", "shortCiteRegEx": "Hsu and Marcus", "year": 1982}, {"title": "Distributed Artificial Intelligence", "author": ["Huhns", "M.N. (Ed"], "venue": null, "citeRegEx": "Huhns and .Ed...,? \\Q1987\\E", "shortCiteRegEx": "Huhns and .Ed...", "year": 1987}, {"title": "Enhanced A* algorithms for multiple alignments: optimal alignments for several sequences and k-opt approximate alignments for large cases", "author": ["T. Ikeda", "H. Imai"], "venue": "Theoretical Computer Science,", "citeRegEx": "Ikeda and Imai,? \\Q1999\\E", "shortCiteRegEx": "Ikeda and Imai", "year": 1999}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Constraint-based dynamic programming for decentralized POMDPs with structured interactions", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Kumar and Zilberstein,? \\Q2009\\E", "shortCiteRegEx": "Kumar and Zilberstein", "year": 2009}, {"title": "Anytime planning for decentralized POMDPs using expectation maximization", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Kumar and Zilberstein,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Zilberstein", "year": 2010}, {"title": "Point-based backup for decentralized POMDPs: Complexity and new algorithms", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Kumar and Zilberstein,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Zilberstein", "year": 2010}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["M. Littman", "A. Cassandra", "L. Kaelbling"], "venue": "In Proc. of the International Conference on Machine Learning", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Not all agents are equal: scaling up distributed POMDPs for agent networks", "author": ["J. Marecki", "T. Gupta", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Marecki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marecki et al\\.", "year": 2008}, {"title": "On opportunistic techniques for solving decentralized Markov decision processes with temporal constraints", "author": ["J. Marecki", "M. Tambe"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Marecki and Tambe,? \\Q2007\\E", "shortCiteRegEx": "Marecki and Tambe", "year": 2007}, {"title": "Decentralized MDPs with sparse interactions", "author": ["F.S. Melo", "M. Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "Melo and Veloso,? \\Q2011\\E", "shortCiteRegEx": "Melo and Veloso", "year": 2011}, {"title": "A compact mathematical formulation for problems with structured agent interactions", "author": ["H. Mostafa", "V. Lesser"], "venue": "In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Mostafa and Lesser,? \\Q2011\\E", "shortCiteRegEx": "Mostafa and Lesser", "year": 2011}, {"title": "Communication for improving policy computation in distributed POMDPs", "author": ["R. Nair", "M. Roth", "M. Yohoo"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Nair et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2004}, {"title": "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D.V. Pynadath", "S. Marsella"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "author": ["R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In Proc. of the National Conference on Artificial Intelligence", "citeRegEx": "Nair et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2005}, {"title": "Value-Based Planning for Teams of Agents in Stochastic Partially Observable Environments", "author": ["F.A. Oliehoek"], "venue": "Amsterdam University Press. (Doctoral dissertation, University of Amsterdam)", "citeRegEx": "Oliehoek,? 2010", "shortCiteRegEx": "Oliehoek", "year": 2010}, {"title": "Decentralized POMDPs", "author": ["F.A. Oliehoek"], "venue": "M. Wiering & M. van Otterlo (Eds.), Reinforcement learning: State of the art (Vol. 12). Springer Berlin Heidelberg.", "citeRegEx": "Oliehoek,? 2012", "shortCiteRegEx": "Oliehoek", "year": 2012}, {"title": "The cross-entropy method for policy search in decentralized", "author": ["F.A. Oliehoek", "J.F. Kooi", "N. Vlassis"], "venue": "POMDPs. Informatica,", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Tree-based solution methods for multiagent POMDPs with delayed communication", "author": ["F.A. Oliehoek", "M.T.J. Spaan"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "Oliehoek and Spaan,? \\Q2012\\E", "shortCiteRegEx": "Oliehoek and Spaan", "year": 2012}, {"title": "Heuristic search for identical payoff Bayesian games", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "J. Dibangoye", "C. Amato"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Oliehoek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2010}, {"title": "Dec-POMDPs with delayed communication", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "N. Vlassis"], "venue": "In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Oliehoek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2007}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "N. Vlassis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Exploiting locality of interaction in factored Dec-POMDPs", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "S. Whiteson", "N. Vlassis"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "Q-value functions for decentralized POMDPs", "author": ["F.A. Oliehoek", "N. Vlassis"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Oliehoek and Vlassis,? \\Q2007\\E", "shortCiteRegEx": "Oliehoek and Vlassis", "year": 2007}, {"title": "Lossless clustering of histories in decentralized POMDPs", "author": ["F.A. Oliehoek", "S. Whiteson", "M.T.J. Spaan"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Oliehoek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2009}, {"title": "Approximate solutions for factored Dec-POMDPs with many agents", "author": ["F.A. Oliehoek", "S. Whiteson", "M.T.J. Spaan"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems. (Submitted for publication)", "citeRegEx": "Oliehoek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2013}, {"title": "Influence-based abstraction for multiagent systems", "author": ["F.A. Oliehoek", "S. Witwicki", "L.P. Kaelbling"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "Oliehoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2012}, {"title": "Decentralized control of a multiple access broadcast channel: Performance bounds", "author": ["J.M. Ooi", "G.W. Wornell"], "venue": "In Proc. of the 35th conference on decision and control", "citeRegEx": "Ooi and Wornell,? \\Q1996\\E", "shortCiteRegEx": "Ooi and Wornell", "year": 1996}, {"title": "A course in game theory", "author": ["M.J. Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne and Rubinstein,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein", "year": 1994}, {"title": "Efficient planning for factored infinite-horizon DECPOMDPs", "author": ["J. Pajarinen", "J. Peltonen"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Pajarinen and Peltonen,? \\Q2011\\E", "shortCiteRegEx": "Pajarinen and Peltonen", "year": 2011}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Panait and Luke,? \\Q2005\\E", "shortCiteRegEx": "Panait and Luke", "year": 2005}, {"title": "Markov Decision Processes\u2014Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, Inc.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Minimal mental models", "author": ["D.V. Pynadath", "S.C. Marsella"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence", "citeRegEx": "Pynadath and Marsella,? \\Q2007\\E", "shortCiteRegEx": "Pynadath and Marsella", "year": 2007}, {"title": "The communicative multiagent team decision problem: Analyzing teamwork theories and models", "author": ["D.V. Pynadath", "M. Tambe"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pynadath and Tambe,? \\Q2002\\E", "shortCiteRegEx": "Pynadath and Tambe", "year": 2002}, {"title": "The complexity of multiagent systems: the price of silence", "author": ["Z. Rabinovich", "C.V. Goldman", "J.S. Rosenschein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Rabinovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rabinovich et al\\.", "year": 2003}, {"title": "Multiagent coordination by extended Markov tracking", "author": ["Z. Rabinovich", "J.S. Rosenschein"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Rabinovich and Rosenschein,? \\Q2005\\E", "shortCiteRegEx": "Rabinovich and Rosenschein", "year": 2005}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Reasoning about joint beliefs for executiontime communication decisions", "author": ["M. Roth", "R. Simmons", "M. Veloso"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Roth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2005}, {"title": "Exploiting factored representations for decentralized execution in multi-agent teams", "author": ["M. Roth", "R. Simmons", "M. Veloso"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Roth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2007}, {"title": "Improved memory-bounded dynamic programming for decentralized POMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Memory-bounded dynamic programming for DECPOMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "Decentralized planning under uncertainty for teams of communicating agents", "author": ["M.T.J. Spaan", "G.J. Gordon", "N. Vlassis"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Spaan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2006}, {"title": "Interaction-driven Markov games for decentralized multiagent planning under uncertainty", "author": ["M.T.J. Spaan", "F.S. Melo"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Spaan and Melo,? \\Q2008\\E", "shortCiteRegEx": "Spaan and Melo", "year": 2008}, {"title": "The MultiAgent Decision Process toolbox: software for decision-theoretic planning in multiagent systems", "author": ["M.T.J. Spaan", "F.A. Oliehoek"], "venue": "In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains", "citeRegEx": "Spaan and Oliehoek,? \\Q2008\\E", "shortCiteRegEx": "Spaan and Oliehoek", "year": 2008}, {"title": "Scaling up optimal heuristic search in Dec-POMDPs via incremental expansion", "author": ["M.T.J. Spaan", "F.A. Oliehoek", "C. Amato"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Spaan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2011}, {"title": "Multiagent planning under uncertainty with stochastic communication delays", "author": ["M.T.J. Spaan", "F.A. Oliehoek", "N. Vlassis"], "venue": "In Proc. of the International Conference on Automated Planning and Scheduling", "citeRegEx": "Spaan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2008}, {"title": "Multiagent systems", "author": ["K.P. Sycara"], "venue": "AI Magazine, 19 (2), 79\u201392.", "citeRegEx": "Sycara,? 1998", "shortCiteRegEx": "Sycara", "year": 1998}, {"title": "MAA*: A heuristic search algorithm for solving decentralized POMDPs", "author": ["D. Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Szer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2005}, {"title": "On the complexity of decentralized decision making and detection problems", "author": ["J. Tsitsiklis", "M. Athans"], "venue": "IEEE Transactions on Automatic Control ,", "citeRegEx": "Tsitsiklis and Athans,? \\Q1985\\E", "shortCiteRegEx": "Tsitsiklis and Athans", "year": 1985}, {"title": "On delayed sharing patterns", "author": ["P. Varaiya", "J. Walrand"], "venue": "IEEE Transactions on Automatic Control ,", "citeRegEx": "Varaiya and Walrand,? \\Q1978\\E", "shortCiteRegEx": "Varaiya and Walrand", "year": 1978}, {"title": "Exploiting coordination locales in distributed POMDPs via social model shaping", "author": ["P. Varakantham", "Kwak", "J. young", "M.E. Taylor", "J. Marecki", "P. Scerri", "M. Tambe"], "venue": "In Proc. of the International Conference on Automated Planning and Scheduling", "citeRegEx": "Varakantham et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Varakantham et al\\.", "year": 2009}, {"title": "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies", "author": ["P. Varakantham", "J. Marecki", "Y. Yabu", "M. Tambe", "M. Yokoo"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Varakantham et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Varakantham et al\\.", "year": 2007}, {"title": "Winning back the cup for distributed POMDPs: planning over continuous belief spaces", "author": ["P. Varakantham", "R. Nair", "M. Tambe", "M. Yokoo"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Varakantham et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Varakantham et al\\.", "year": 2006}, {"title": "Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents", "author": ["P. Velagapudi", "P. Varakantham", "P. Scerri", "K. Sycara"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Velagapudi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Velagapudi et al\\.", "year": 2011}, {"title": "A Concise Introduction to Multiagent Systems and Distributed Artificial Intelligence", "author": ["N. Vlassis"], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Vlassis,? 2007", "shortCiteRegEx": "Vlassis", "year": 2007}, {"title": "Reward shaping for valuing communications during multi-agent coordination", "author": ["S.A. Williamson", "E.H. Gerding", "N.R. Jennings"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Williamson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2009}, {"title": "Abstracting Influences for Efficient Multiagent Coordination Under Uncertainty", "author": ["S.J. Witwicki"], "venue": "Unpublished doctoral dissertation, University of Michigan, Ann Arbor, Michigan, USA.", "citeRegEx": "Witwicki,? 2011", "shortCiteRegEx": "Witwicki", "year": 2011}, {"title": "Influence-based policy abstraction for weakly-coupled Dec-POMDPs", "author": ["S.J. Witwicki", "E.H. Durfee"], "venue": "In Proc. of the International Conference on Automated Planning and Scheduling", "citeRegEx": "Witwicki and Durfee,? \\Q2010\\E", "shortCiteRegEx": "Witwicki and Durfee", "year": 2010}, {"title": "Point-based policy generation for decentralized POMDPs", "author": ["F. Wu", "S. Zilberstein", "X. Chen"], "venue": "In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Rollout sampling policy iteration for decentralized POMDPs", "author": ["F. Wu", "S. Zilberstein", "X. Chen"], "venue": "In Proc. of Uncertainty in Artificial Intelligence", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Online planning for multi-agent systems with bounded communication", "author": ["F. Wu", "S. Zilberstein", "X. Chen"], "venue": "Artificial Intelligence,", "citeRegEx": "Wu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2011}, {"title": "Communication decisions in multi-agent cooperation: Model and experiments", "author": ["P. Xuan", "V. Lesser", "S. Zilberstein"], "venue": "In Proc. of the International Conference on Autonomous Agents", "citeRegEx": "Xuan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Xuan et al\\.", "year": 2001}, {"title": "A* with partial expansion for large branching factor problems", "author": ["T. Yoshizumi", "T. Miura", "T. Ishida"], "venue": "In Proc. of the National Conference on Artificial Intelligence", "citeRegEx": "Yoshizumi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yoshizumi et al\\.", "year": 2000}, {"title": "Exploiting model equivalences for solving interactive dynamic influence diagrams", "author": ["Y. Zeng", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zeng and Doshi,? \\Q2012\\E", "shortCiteRegEx": "Zeng and Doshi", "year": 2012}, {"title": "Utilizing partial policies for identifying equivalence of behavioral models", "author": ["Y. Zeng", "P. Doshi", "Y. Pan", "H. Mao", "M. Chandrasekaran", "J. Luo"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Zeng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 87, "context": "agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Bu\u015foniu, Babu\u0161ka, & De Schutter, 2008).", "startOffset": 6, "endOffset": 108}, {"referenceID": 95, "context": "agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Bu\u015foniu, Babu\u0161ka, & De Schutter, 2008).", "startOffset": 6, "endOffset": 108}, {"referenceID": 27, "context": "For instance, it may be possible to extrapolate optimal solutions of problems with shorter planning horizons, using them as the starting point of policy search for longer-horizon problems as in the work of Eker and Ak\u0131n (2013), or to use such shorter-horizon, no-communication solutions inside problems with communication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008).", "startOffset": 206, "endOffset": 227}, {"referenceID": 97, "context": "By performing (approximate) influence-based abstraction and influence search (Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions of component problems can potentially be used to find (near-)optimal solutions of larger problems.", "startOffset": 77, "endOffset": 132}, {"referenceID": 55, "context": "In fact, almost all successful approximate Dec-POMDP methods are based on optimal ones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009; Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010) or locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2, and the clustering technique presented in this article forms the basis of a recently introduced approximate clustering technique (Wu, Zilberstein, & Chen, 2011).", "startOffset": 87, "endOffset": 261}, {"referenceID": 6, "context": "In recent years, there have been huge advances in the approximate solution of Dec-POMDPs, leading to the development of solution methods that can deal with large horizons, hundreds of agents and many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a; Oliehoek, 2010; Velagapudi et al., 2011).", "startOffset": 207, "endOffset": 321}, {"referenceID": 55, "context": "In recent years, there have been huge advances in the approximate solution of Dec-POMDPs, leading to the development of solution methods that can deal with large horizons, hundreds of agents and many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a; Oliehoek, 2010; Velagapudi et al., 2011).", "startOffset": 207, "endOffset": 321}, {"referenceID": 94, "context": "In recent years, there have been huge advances in the approximate solution of Dec-POMDPs, leading to the development of solution methods that can deal with large horizons, hundreds of agents and many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a; Oliehoek, 2010; Velagapudi et al., 2011).", "startOffset": 207, "endOffset": 321}, {"referenceID": 94, "context": "The method by Velagapudi et al. (2011) repeatedly computes best responses in a way similar to DP-JESP (Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003).", "startOffset": 14, "endOffset": 39}, {"referenceID": 79, "context": "Whereas Seuken and Zilberstein (2008) argued that GMAA* can at best optimally solve Dec-POMDPs only one horizon further than brute-force search, our results demonstrate that GMAA*-ICE can do much better.", "startOffset": 8, "endOffset": 38}, {"referenceID": 53, "context": "For explanatory purposes, we also consider a much simpler problem, the so-called decentralized tiger problem (Nair et al., 2003).", "startOffset": 109, "endOffset": 128}, {"referenceID": 52, "context": "For a complete specification, see the discussion by Nair et al. (2003).", "startOffset": 52, "endOffset": 71}, {"referenceID": 77, "context": ", the work of Seuken and Zilberstein (2008) and Oliehoek (2012).", "startOffset": 14, "endOffset": 44}, {"referenceID": 55, "context": ", the work of Seuken and Zilberstein (2008) and Oliehoek (2012).", "startOffset": 48, "endOffset": 64}, {"referenceID": 55, "context": "For a more detailed description, see the work of Oliehoek, Spaan, and Vlassis (2008). For a further description of dynamic programming methods and their relationship to heuristic search methods, see the work of Oliehoek (2012).", "startOffset": 49, "endOffset": 85}, {"referenceID": 55, "context": "For a more detailed description, see the work of Oliehoek, Spaan, and Vlassis (2008). For a further description of dynamic programming methods and their relationship to heuristic search methods, see the work of Oliehoek (2012).", "startOffset": 49, "endOffset": 227}, {"referenceID": 29, "context": "The resulting algorithm, generalized MAA* (GMAA*) offers a unified perspective of MAA* and the forward sweep policy computation method (Emery-Montemerlo, 2005), which differ in how they implement GMAA*\u2019s expand operator: forward sweep policy computation solves (i.", "startOffset": 135, "endOffset": 159}, {"referenceID": 54, "context": "Oliehoek, Spaan, and Vlassis (2008) generalized the algorithm by making explicit the expand and selection operators performed in the heuristic search.", "startOffset": 0, "endOffset": 36}, {"referenceID": 88, "context": "Ranking nodes with greater depth higher in case of equal heuristic value helps find tight lower bounds early by first expanding deeper nodes (Szer et al., 2005) and is also useful in incremental expansion.", "startOffset": 141, "endOffset": 160}, {"referenceID": 88, "context": "Therefore, in theory, GMAA* is guaranteed to eventually produce an optimal joint policy (Szer et al., 2005).", "startOffset": 88, "endOffset": 107}, {"referenceID": 79, "context": "Consequently, Seuken and Zilberstein (2008) conclude that MAA* \u201ccan at best solve problems whose horizon is only 1 greater than those that can already be solved by n\u00e4\u0131ve brute force search.", "startOffset": 14, "endOffset": 44}, {"referenceID": 71, "context": "This approach, known as QMDP (Littman, Cassandra, & Kaelbling, 1995), uses the MDP value function Q M (s t,a), which can be computed using standard dynamic programming techniques (Puterman, 1994).", "startOffset": 179, "endOffset": 195}, {"referenceID": 88, "context": "The resulting solution can be used as a heuristic, called QPOMDP (Szer et al., 2005; Roth, Simmons, & Veloso, 2005).", "startOffset": 65, "endOffset": 115}, {"referenceID": 43, "context": ",V t |A|} (Kaelbling et al., 1998).", "startOffset": 10, "endOffset": 34}, {"referenceID": 39, "context": "Since QMDP is an upper bound on the POMDP value function (Hauskrecht, 2000), QPOMDP provides a tighter upper bound to Q\u2217 than QMDP.", "startOffset": 57, "endOffset": 75}, {"referenceID": 20, "context": "Cassandra, Littman, & Zhang, 1997). To overcome this problem, Oliehoek and Spaan (2012) introduce novel tree-based pruning methods.", "startOffset": 0, "endOffset": 88}, {"referenceID": 29, "context": "9 Previous research has also investigated such clustering: Emery-Montemerlo, Gordon, Schneider, and Thrun (2005) propose clustering types based on the profiles of the payoff functions of the CBGs.", "startOffset": 59, "endOffset": 113}, {"referenceID": 55, "context": "The probabilistic equivalence criterion and lossless clustering were introduced by Oliehoek et al. (2009). This article presents a new, simpler proof of the optimality of clustering based on PE.", "startOffset": 83, "endOffset": 106}, {"referenceID": 43, "context": "It can then use this belief to determine the value of any future policy, as it is a sufficient statistic of the history to predict the future rewards (Kaelbling et al., 1998; Bertsekas, 2005).", "startOffset": 150, "endOffset": 191}, {"referenceID": 13, "context": "It can then use this belief to determine the value of any future policy, as it is a sufficient statistic of the history to predict the future rewards (Kaelbling et al., 1998; Bertsekas, 2005).", "startOffset": 150, "endOffset": 191}, {"referenceID": 55, "context": "For a more extensive treatment of these different forms of policy, we refer to the discussion by Oliehoek (2012). Given these concepts, we can define the value of a \u03c4 = k-stages-to-go joint policy starting from state s: V (s,\u03b3) = R(s,a) + \u2211", "startOffset": 97, "endOffset": 113}, {"referenceID": 55, "context": "The proof originally provided by Oliehoek et al. (2009) is based on showing that histories that are PE will induce identical Q-values.", "startOffset": 33, "endOffset": 56}, {"referenceID": 29, "context": "Emery-Montemerlo et al. (2005) showed how clustering can be incorporated at every stage in their algorithm: when the CBG for a stage t is constructed, a clustering of the individual histories (types) is performed first and only afterwards is the (reduced) CBG solved.", "startOffset": 0, "endOffset": 31}, {"referenceID": 55, "context": "Oliehoek, Spaan, and Vlassis (2008) used a tree-based representation for the QPOMDP and QBG heuristics.", "startOffset": 0, "endOffset": 36}, {"referenceID": 59, "context": "However, this point-based backup can be interpreted as a collection of CBGs (Oliehoek et al., 2010).", "startOffset": 76, "endOffset": 99}, {"referenceID": 44, "context": "Kumar and Zilberstein (2010b) tackle a slightly different problem; they introduce a weighted constraint satisfaction approach to solving the point-based backup in dynamic programming for Dec-POMDPs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 59, "context": "We propose a modification of the BaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed in Section 4.", "startOffset": 51, "endOffset": 74}, {"referenceID": 59, "context": "To this end, we propose to modify the Bayesian game Branch and Bound (BaGaBaB) algorithm (Oliehoek et al., 2010).", "startOffset": 89, "endOffset": 112}, {"referenceID": 44, "context": "An exception may be the method of Kumar and Zilberstein (2010b), which employs AND/OR branch and bound search with the EDAC heuristic (and is thus limited to the two-agent case).", "startOffset": 34, "endOffset": 64}, {"referenceID": 53, "context": "The most well-known Dec-POMDP benchmarks are the Dec-Tiger (Nair et al., 2003) and BroadcastChannel (Hansen et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 38, "context": ", 2003) and BroadcastChannel (Hansen et al., 2004) problems.", "startOffset": 29, "endOffset": 50}, {"referenceID": 54, "context": "Experiments were run on an Intel Core i5 CPU running Linux, andGMAA*, GMAA*-IC, and GMAA*-ICE were implemented in the same code-base using the MADP Toolbox (C++) (Spaan & Oliehoek, 2008). The vector-based QBG representation is computed using a variation of Incremental Pruning (adapted for computing Q-functions instead of regular value functions), corresponding to the NaiveIP method as described by Oliehoek and Spaan (2012). To implement the pruning, we employ Cassandra\u2019s POMDP-solve software (A.", "startOffset": 171, "endOffset": 427}, {"referenceID": 55, "context": "Consequently, the performance of GMAA*-IC is much better than all previously reported results, including those of Oliehoek et al. (2009), who were often required to resort to QMDP for larger problems and/or horizons.", "startOffset": 114, "endOffset": 137}, {"referenceID": 55, "context": "Note that in some problem domains we report smaller clusterings than Oliehoek et al. (2009). Due to an implementation mistake, their clustering was overly conservative, and did not in all cases treat two histories as probabilistically equivalent, when in fact they were.", "startOffset": 69, "endOffset": 92}, {"referenceID": 6, "context": "DP-IPG (Amato et al., 2009) performs exact dynamic programing with incremental policy", "startOffset": 7, "endOffset": 27}, {"referenceID": 1, "context": "Additionally, for FireFighting and GridSmall, we use the benchmark versions standard to the literature (Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras and Dutech (2010) use non-standard versions.", "startOffset": 103, "endOffset": 157}, {"referenceID": 1, "context": "The results reported here deviate from those reported by Aras and Dutech (2010). For a number of problems, Aras et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 1, "context": "Additionally, for FireFighting and GridSmall, we use the benchmark versions standard to the literature (Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras and Dutech (2010) use non-standard versions.", "startOffset": 138, "endOffset": 190}, {"referenceID": 1, "context": "Additionally, for FireFighting and GridSmall, we use the benchmark versions standard to the literature (Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras and Dutech (2010) use non-standard versions. This explains the difference between our results and the ones reported in their article (personal communication, Raghav Aras). 21. The goal of Boularias and Chaib-draa (2008) was to find non-dominated joint policies for all initial beliefs.", "startOffset": 138, "endOffset": 392}, {"referenceID": 15, "context": "The DP-LPC algorithm proposed by Boularias and Chaib-draa (2008) also improves the efficiency of optimal solutions by a form of compression.", "startOffset": 33, "endOffset": 65}, {"referenceID": 6, "context": "Table 5, which reports the VMBDP values produced by PBIP-IPG (Amato et al., 2009) (with typical \u2018maxTrees\u2019 parameter settingm), demonstrates that the optimal solutions produced by GMAA*-IC or GMAA*-ICE are of higher quality.", "startOffset": 61, "endOffset": 81}, {"referenceID": 10, "context": "One class of problems where we can say something a priori about the amount of clustering that is possible is the class of Dec-POMDPs with transition and observation independence (Becker et al., 2003).", "startOffset": 178, "endOffset": 199}, {"referenceID": 53, "context": "However, there is yet another notion of belief, employed in the JESP solution method (Nair et al., 2003), that is superficially more similar to the PE distribution.", "startOffset": 85, "endOffset": 104}, {"referenceID": 105, "context": "First, several approaches exploit the notion of behavioral equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012).", "startOffset": 71, "endOffset": 137}, {"referenceID": 105, "context": "The notion of utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) is closer to PE because it also takes into account the (value of the) best-response of agent i (in particular, it clusters two models mj and m \u2032 j if using BR(mj)\u2014the best response against mj\u2014 achieves the same value against mj).", "startOffset": 34, "endOffset": 80}, {"referenceID": 28, "context": "In Section 3, we indicated that our clustering method is closely related to the approach of Emery-Montemerlo et al. (2005) but is also fundamentally different because our method is lossless.", "startOffset": 92, "endOffset": 123}, {"referenceID": 15, "context": "3, we discussed connections to the approach of Boularias and Chaib-draa (2008) which clusters policy values.", "startOffset": 47, "endOffset": 79}, {"referenceID": 10, "context": "There are also connections between PE and work on influence-based abstraction (Becker et al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence (or point in parameter space, Becker et al.", "startOffset": 78, "endOffset": 163}, {"referenceID": 97, "context": "There are also connections between PE and work on influence-based abstraction (Becker et al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence (or point in parameter space, Becker et al.", "startOffset": 78, "endOffset": 163}, {"referenceID": 66, "context": "There are also connections between PE and work on influence-based abstraction (Becker et al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence (or point in parameter space, Becker et al.", "startOffset": 78, "endOffset": 163}, {"referenceID": 6, "context": "As part of our evaluation, we compare against the MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) and DP-IPG (Amato et al., 2009), an extension of the exact dynamic programming algorithm (Hansen et al.", "startOffset": 135, "endOffset": 155}, {"referenceID": 38, "context": ", 2009), an extension of the exact dynamic programming algorithm (Hansen et al., 2004).", "startOffset": 65, "endOffset": 86}, {"referenceID": 53, "context": "Research on finite-horizon DecPOMDPs has considered many other approaches such as bounded approximations (Amato, Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair, Tambe, & Yokoo, 2006) and approximate methods without guarantees (Seuken & Zilberstein, 2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Ak\u0131n, 2010; Oliehoek, Kooi, & Vlassis, 2008; Dibangoye et al.", "startOffset": 169, "endOffset": 229}, {"referenceID": 23, "context": ", 2003; Varakantham, Nair, Tambe, & Yokoo, 2006) and approximate methods without guarantees (Seuken & Zilberstein, 2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Ak\u0131n, 2010; Oliehoek, Kooi, & Vlassis, 2008; Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, & Chen, 2010b).", "startOffset": 92, "endOffset": 310}, {"referenceID": 10, "context": "The resulting models, such as the TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) and NDPOMDP (Nair et al.", "startOffset": 46, "endOffset": 113}, {"referenceID": 54, "context": ", 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) and NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe, & Yokoo, 2008; Kumar & Zilberstein, 2009), can be interpreted as independent (PO)MDPs for", "startOffset": 66, "endOffset": 189}, {"referenceID": 92, "context": ", 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) and NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe, & Yokoo, 2008; Kumar & Zilberstein, 2009), can be interpreted as independent (PO)MDPs for", "startOffset": 66, "endOffset": 189}, {"referenceID": 2, "context": "There are also connections between PE and work on influence-based abstraction (Becker et al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence (or point in parameter space, Becker et al., 2003) is a compact representation of the other agents\u2019 policies. Models of the other agents can be clustered if they lead to the same influence on agent i. However, though more fine-grained, this is ultimately still a form of behavioral equivalence. A final relation to our equivalence notion is the work by Dekel, Fudenberg, and Morris (2006), which constructs a distance measure and topology on the space of types with the goal of approximating the infinite universal type space (the space of all possible beliefs about beliefs about beliefs, etc.", "startOffset": 79, "endOffset": 574}, {"referenceID": 2, "context": "There are also connections between PE and work on influence-based abstraction (Becker et al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence (or point in parameter space, Becker et al., 2003) is a compact representation of the other agents\u2019 policies. Models of the other agents can be clustered if they lead to the same influence on agent i. However, though more fine-grained, this is ultimately still a form of behavioral equivalence. A final relation to our equivalence notion is the work by Dekel, Fudenberg, and Morris (2006), which constructs a distance measure and topology on the space of types with the goal of approximating the infinite universal type space (the space of all possible beliefs about beliefs about beliefs, etc.) for one-shot Bayesian games. Our setting, however, considers a simple finite type space where the types directly correspond to the private histories (in the form of AOHs) in a sequential problem. Thus, we do not need to approximate the universal type space; instead we want to know which histories lead to the same future dynamics from the perspective of an agent. Dekel et al.\u2019s topology does not address this question. Our incremental expansion technique is related to approaches extending A\u2217 to deal with large branching factors in the context of multiple sequence alignment (Ikeda & Imai, 1999; Yoshizumi, Miura, & Ishida, 2000). However, our approach is different because we do not discard unpromising nodes but rather provide a mechanism to generate only the necessary ones. Also, when proposing MAA*, Szer et al. (2005) developed a superficially similar approach that could be applied only to the last stage.", "startOffset": 79, "endOffset": 1608}, {"referenceID": 9, "context": "On the other hand, event-driven interaction models (Becker et al., 2004) consider agents that have individual rewards but can influence each other\u2019s transitions.", "startOffset": 51, "endOffset": 72}, {"referenceID": 91, "context": "Examples are interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs with sparse interactions (Melo & Veloso, 2011), distributed POMDPs with coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions with complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs (Witwicki & Durfee, 2010; Witwicki, 2011).", "startOffset": 168, "endOffset": 219}, {"referenceID": 94, "context": "Examples are interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs with sparse interactions (Melo & Veloso, 2011), distributed POMDPs with coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions with complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs (Witwicki & Durfee, 2010; Witwicki, 2011).", "startOffset": 168, "endOffset": 219}, {"referenceID": 97, "context": ", 2011), event-driven interactions with complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs (Witwicki & Durfee, 2010; Witwicki, 2011).", "startOffset": 127, "endOffset": 168}, {"referenceID": 12, "context": "Also, since the infinite-horizon case is undecidable (Bernstein et al., 2002), the approaches are approximate or optimal given a particular controller size.", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": "While there exists a boundedly optimal approach that can theoretically construct a controller within any \u01eb of optimal, it is only feasible for very small problems or a large \u01eb (Bernstein et al., 2009).", "startOffset": 176, "endOffset": 200}, {"referenceID": 52, "context": ", broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007; Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011).", "startOffset": 39, "endOffset": 351}, {"referenceID": 77, "context": ", broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007; Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011).", "startOffset": 39, "endOffset": 351}, {"referenceID": 101, "context": ", broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007; Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011).", "startOffset": 39, "endOffset": 351}, {"referenceID": 38, "context": "Finally, there are numerous models closely related to Dec-POMDPs, such as POSGs (Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005), and their graphical counterparts (Doshi, Zeng, & Chen, 2008).", "startOffset": 80, "endOffset": 101}, {"referenceID": 8, "context": "On the other hand, event-driven interaction models (Becker et al., 2004) consider agents that have individual rewards but can influence each other\u2019s transitions. More recently, models that allow for limited transition and reward dependence have been introduced. Examples are interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs with sparse interactions (Melo & Veloso, 2011), distributed POMDPs with coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions with complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs (Witwicki & Durfee, 2010; Witwicki, 2011). While the methods developed for these models often exhibit better scaling behavior than methods for standard Dec-(PO)MDPs, they typically are not suitable when agents have extended interactions, e.g., to collaborate in transporting an item. Also, there have been specialized models that consider the timing of actions whose ordering is already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011). Another body of work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein, 2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), in which it is not possible to represent a policy as a tree. These approaches represent policies using finite-state controllers that are then optimized in various ways. Also, since the infinite-horizon case is undecidable (Bernstein et al., 2002), the approaches are approximate or optimal given a particular controller size. While there exists a boundedly optimal approach that can theoretically construct a controller within any \u01eb of optimal, it is only feasible for very small problems or a large \u01eb (Bernstein et al., 2009). There has also been great interest in Dec-POMDPs that explicitly take into account communication. Some approaches try to optimize the meaning of communication actions without semantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon, & Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) while others use fixed semantics (e.g., broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007; Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models used in the first category (e.g., the Dec-POMDP-Com) can be converted to normal Dec-POMDPs (Seuken & Zilberstein, 2008), the contributions of this article are applicable to those settings. Finally, there are numerous models closely related to Dec-POMDPs, such as POSGs (Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005), and their graphical counterparts (Doshi, Zeng, & Chen, 2008). These models are more general in the sense that they consider self-interested settings where each agent has an individual reward function. I-POMDPs are conjectured to also require doubly exponential time (Seuken & Zilberstein, 2008). However, for the I-POMDP there have been a number of recent advances (Doshi & Gmytrasiewicz, 2009). The current paper makes a clear link between best-response equivalence of histories and the notion of best-response equivalence of beliefs in I-POMDPs. In particular, this article demonstrates that two PE action-observation histories (AOHs) induce, given only a past joint policy, a distribution over states and AOHs of other agents, and therefore will induce the same multiagent belief for any future policies of other agents. These induced multiagent beliefs, in turn, can be interpreted as special cases of I-POMDP beliefs where the model of the other agents are sub-intentional models in the form of a fixed policy tree. Rabinovich and Rosenschein (2005) introduced a method that, rather than optimizing", "startOffset": 52, "endOffset": 3945}, {"referenceID": 55, "context": ", 2010a), integrating GMAA* methods for factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010; Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al.", "startOffset": 60, "endOffset": 143}, {"referenceID": 65, "context": ", 2010a), integrating GMAA* methods for factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010; Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al.", "startOffset": 60, "endOffset": 143}, {"referenceID": 29, "context": ", 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011) or using bounded approximations for the heuristics.", "startOffset": 37, "endOffset": 78}, {"referenceID": 101, "context": ", 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011) or using bounded approximations for the heuristics.", "startOffset": 37, "endOffset": 78}, {"referenceID": 101, "context": "In addition to facilitating optimal solutions, we hope these advances will inspire new principled approximation methods, as incremental clustering has already done (Wu et al., 2011), and enable them to be meaningfully benchmarked.", "startOffset": 164, "endOffset": 181}], "year": 2013, "abstractText": "This article presents the state-of-the-art in optimal solution methods for decentralized partially observable Markov decision processes (Dec-POMDPs), which are general models for collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative Bayesian games (CBGs), we describe several advances that greatly expand the range of DecPOMDPs that can be solved optimally. First, we introduce lossless incremental clustering of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing optimality. Second, we introduce incremental expansion of nodes in the GMAA* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the node\u2019s depth. This is particularly beneficial when little clustering is possible. In addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec-POMDPs. We provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally, we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}