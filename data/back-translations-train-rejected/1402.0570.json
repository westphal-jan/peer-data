{"id": "1402.0570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "A Feature Subset Selection Algorithm Automatic Recommendation Method", "abstract": "Many feature subset selection (FSS) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. At the same time, so far there is rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus, FSS algorithm automatic recommendation is very important and practically useful. In this paper, a meta learning based FSS algorithm automatic recommendation method is presented. The proposed method first identifies the data sets that are most similar to the one at hand by the k-nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. Then, it ranks all the candidate FSS algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. The proposed recommendation method is extensively tested on 115 real world data sets with 22 well-known and frequently-used different FSS algorithms for five representative classifiers. The results show the effectiveness of our proposed FSS algorithm recommendation method.", "histories": [["v1", "Tue, 4 Feb 2014 01:37:24 GMT  (855kb)", "http://arxiv.org/abs/1402.0570v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guangtao wang", "qinbao song", "heli sun", "xueying zhang", "baowen xu", "yuming zhou"], "accepted": false, "id": "1402.0570"}, "pdf": {"name": "1402.0570.pdf", "metadata": {"source": "CRF", "title": "A Feature Subset Selection Algorithm Automatic Recommendation Method", "authors": ["Guangtao Wang", "Qinbao Song", "Heli Sun", "Xueying Zhang", "Baowen Xu", "Yuming Zhou"], "emails": ["gt.wang@stu.xjtu.edu.cn", "qbsong@mail.xjtu.edu.cn", "hlsun@mail.xjtu.edu.cn", "zhangxueying.725@stu.xjtu.edu.cn", "bwxu@nju.edu.cn", "zhouyuming@nju.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "This leads not only to an understanding of the data, but also to an improvement in the performance of a learner by improving the generalisation capacity and the ability of the learner to interpret."}, {"heading": "2. Preliminaries", "text": "In this section, we first describe the meta-characteristics used to characterize datasets, and then introduce the multi-criterion evaluation metric used to measure the performance of FSS algorithms."}, {"heading": "2.1 Meta-features of Data Sets", "text": "Our proposed FSS algorithm recommendation method is based on the relationship between the performance of FSS algorithms and the meta-characteristics of datasets. Due to the ubiquity of \"garbage in, garbage out\" (Lee, Lu, Ling, & Ko, 1999) in the field of data mining, the selection of meta-characteristics is crucial for our proposed FSS recommendation method. Meta-characteristics are metrics that can be extracted from datasets and used to uniformly characterize different datasets reflecting the underlying characteristics. Meta-characteristics should not only be calculated conveniently and efficiently, but also in the context of the performance of machine learning algorithms (Castiello et al, 2005)."}, {"heading": "2.2 Multi-criteria Metric for FSS Algorithm Evaluation", "text": "In this section, we first present the classical indicators that evaluate the performance of the FSS algorithm. Subsequently, by analyzing the user requirements in practical application on the basis of these indicators, we propose a new and user-oriented multi-criterion indicator for evaluating the FSS algorithm by combining these indicators with each other."}, {"heading": "2.2.1 Classical Performance Metrics", "text": "This year it is so far that it will be able to remention the mentioned hreeisrcnlrVo until they are able to reconsider and reconsider."}, {"heading": "2.2.2 Multi-Criteria Metric EARR", "text": "s say Accji is the classification accuracy of a classifier with FSS algorithm Ai on dataset Dj (1 \u2264 i \u2264 M, 1 \u2264 j \u2264 N), and tji and nji denote the runtime and number of selected features of an FSS algorithm Ai on dataset Dj, respectively. Then the EARR is defined from Ai to Aj via Dk as defined EARRDkAi, Aj = accki / acc k j1 + \u03b1. Protocol (nki / tkj \u00b7 log) (1 \u2264 i 6 = j \u2264 M, 1) where we have user-defined the respective parameters."}, {"heading": "3. FSS Algorithm Recommendation Method", "text": "In this section we first present the framework for the recommendation of the FSS algorithm and then present in detail the next adjacent recommendation method."}, {"heading": "3.1 Framework", "text": "Based on the assumption that there is a relationship between the performance of an FSS algorithm on a given dataset and the properties of the dataset (also known as meta features), our proposed recommendation method first constructs a meta-knowledge base consisting of dataset meta features and FSS algorithm performance. Then, using the meta-knowledge base, a k-NN-based method is used to model this relationship and recommend appropriate FSS algorithms for a new dataset. Therefore, our proposed recommendation method consists of two parts: Meta Knowledge Base and FSS Algorithm Recommendation. Figure 1 shows the details."}, {"heading": "1) Meta-Knowledge Database Construction", "text": "As already mentioned, the Meta-Knowledge Base consists of the meta-characteristics of a set of historical data sets and the performance of a group of FSS algorithms on it. It is the basis of our proposed recommendation method, and the effectiveness of the recommendations depends heavily on this database.The Meta-Knowledge Base is constructed by the following three steps. Firstly, the meta-characteristics in Table 1 are extracted from each historical data set of the Meta-Features Extraction module. Then, each candidate FSS algorithm is applied to each historical data set. Classification accuracy, runtime of the feature selection and the number of selected features are recorded, and the corresponding value of the Performance Metric EARR is calculated. This is achieved by the module \"Performance-Metric Calculation.\" Finally, for each data set, a tuple is composed of the meta characteristics and the values of the Performance Metric Algorithms."}, {"heading": "3.2 Recommendation Method", "text": "In order to recommend suitable FSS algorithms for a new dataset Dnew based on its next k datasets, there are two fundamental problems that need to be solved: i) how to identify its next k datasets and ii) how to recommend suitable FSS algorithms based on these k datasets. 1) k next datasets Identification The next k datasets of Dnew are identified by calculating the distance between Dnew and each historical dataset based on their meta characteristics. The smaller the distance, the more similar the corresponding dataset to Dnew.In order to effectively calculate the distance between two datasets, the L1 standard distance p (Atkeson, Moore, & Schaal, 1997) is assumed as it is easy to understand and calculate, and its ability to measure the similarity between two datasets was demonstrated by Brazdil et al al al al. (2003)."}, {"heading": "2) FSS algorithm recommendation", "text": "After receiving the next datasets from Dnew, the performance of the candidates FSS algorithms on Dnew is evaluated against the next datasets. (D) Then, the algorithms with the best performance are recommended. (D) Then, the performance of Ai on the next datasets of Dnew and EARRDjAi = k is evaluated the performance measures of the FSS algorithm Ai on dataset Dj Dknn (1 \u2264 j \u2264 k).Then, the performance of Ai on the new dataset Dnew can be evaluated. (5) Eq. (5) Eq. (5) indicates that the performance of the FSS algorithms is on Dnew."}, {"heading": "4. Experimental Results and Analysis", "text": "In this section, we experimentally evaluate the proposed Feature Subset Select (FSS) method by recommending algorithms based on the benchmark data sets."}, {"heading": "4.1 Benchmark Data Sets", "text": "Our experiment uses 115 widely used real-world datasets from various fields such as computers, images, life, biology, physics, and text 4, ranging in size from 10 to 24863 instances, and the number of their characteristics from 5 to 27680. Statistical summaries of these datasets are presented in Table 2, regarding the number of instances (referred to as I), the number of characteristics (referred to as F), and the number of target concepts (referred to as T. These datasets are available at http: / / archive.ics.uci.edu / ml / datasets.html, http: / / featureselection.asu.edu / datasets.php, http: / / sci2s.ugr.es / keel / datasets.php, http: / / www. upo.es / eps / bigs / datasets.html, and http: / / tunedit.org / repo / data."}, {"heading": "4.2 Experimental Setup", "text": "In order to evaluate the performance of the proposed FSS algorithm recommendation method, to further verify whether the proposed method is potentially useful in practice, and to confirm the reproducibility of our experiments, we set up the experimental study as follows."}, {"heading": "4.2.1 FSS Algorithms", "text": "The FSS algorithms can be divided into two broad categories: wrappers and filters (Molina et al., 2002; Kohavi & John, 1997).The wrapper method uses the error rate of the classification algorithm as an evaluation function to measure a subset of characteristics, while the evaluation function of the filter method is independent of the classification algorithm.The accuracy of the wrapper method is usually high; however, the generality of the result is limited and the computational complexity.In comparison, the filter method of generality and the computational complexity.Due to the fact that the wrapper method is computationally expensive (Dash & Liu, 1997), the filter method is usually a good choice when the number of characteristics is very large. Therefore, in our experiment we focus on the filter method and the Livic algorithms that are based on filters."}, {"heading": "1) Search methods", "text": "i) Sequential Forward Search (SFS, 2009): Starting from the empty set, add sequentially the feature that leads to the highest value of objective function to the current feature subset.ii) Sequential Backward Search (SBS): Starting from the complete set, sequentially eliminate the feature that results in a smallest or no decrease in the value of objective function from the current feature subset.iii) Bi-Direction Search (BiS): A parallel implementation of SFS and SBS. It searches the feature subrange space in two directions. iv) Genetic Search (GS): A randomized search method performed using a simple genetic algorithm (Goldberg, 1989). The genetic algorithm finds the feature subset to maximize specialized output using techniques inspired by natural evolution."}, {"heading": "2) Evaluation measures", "text": "i) Consistency (Liu & Setiono, 1996; Zhao & Liu, 2007): This type of measurement evaluates the value of a feature that is distinguished from the level of consistency in the target concept when the instances are projected onto the feature threshold. However, the consistency of each feature can never be lower than that of the full feature set.ii) Dependence (Hall, 1999; Yu & Liu, 2003): This type of measurement evaluates the value of a subset of features by taking into account the individual predictability of each feature along with the degree of redundancy among those characteristics. FSS methods of measurement are based on the assumption that good features correlate closely with the target concept but are not correlated. (Kira) Distance (Kira & Rendell, 1992; Kononenko, 1994): This type of measurement is based on the assumption that the distance of instances from different target concepts is greater than this."}, {"heading": "4.2.2 Classification Algorithms", "text": "Since the actual relevant characteristics of the real datasets are generally not known in advance, it is not feasible to directly evaluate an FSS algorithm on the basis of the characteristics selected. Classification accuracy is a widely applied metric for evaluating the performance of FSS algorithms and also plays an important role in our proposed performance metric EARR for evaluating various FSS algorithms. However, different classification algorithms have different distortions. An FSS algorithm may be more suitable for some classification algorithms than others (de Souza, 2004). This influences the performance evaluation of FSS algorithms. In order to show that our proposed FSS algorithm recommendation method is not limited to a particular classification algorithm, five representative classification algorithms based on different hypotheses are used: Naive Bayes, Naive Bayes, Bayes Bayes and Bayes, Bayes, Bayes, Bayes, Bayes, Bayes, Bayes, Bayes, es, es, es, es, es, es, es, es, es, es, es, es, es, es."}, {"heading": "4.2.3 Measures to Evaluate the Recommendation Method", "text": "The recommendation of the FSS algorithm is an application of meta-learning. As far as we know, there are no uniform metrics for evaluating the performance of the meta-learning methods. In order to evaluate our proposed method of recommending the FSS algorithm, two metrics are defined, the hit rate and the performance ratio of the recommendations. Let D be a given dataset and Arec be an FSS algorithm recommended by the recommendation method for D, these two metrics can be introduced as follows."}, {"heading": "1) Recommendation hit ratio", "text": "An intuitive evaluation criterion is whether the recommended FSS algorithm Arec meets the requirements of users, that is, whether Arec is the optimal FSS algorithm for D, or whether the performance of Arec on D does not differ significantly from that of the optimal FSS algorithm. Suppose Aopt represents the optimal FSS algorithm for D, and ASetopt refers to the FSS algorithm set in which each algorithm does not differ significantly from Aopt (of course it also includes Aopt). Then, a measure called Recommendation hit can be defined to assess whether the recommended algorithm Arec on D.Definition 1 (Recommendation hit on D.Definition) Definition 1 (Recommendation hit on a dataset D, then the Recommendation Hit (Arec, D) is defined."}, {"heading": "2) Recommendation performance ratio", "text": "The recommended hit rate indicates whether or not a suitable FSS algorithm is recommended for a given dataset, but it cannot tell us how far the recommended algorithm is from the best. Thus, a new measurement, the Recommendation Performance Ratio for a Recommendation, is defined. Definition 3 (Recommendation Performance Ratio) Let EARRrec and EARRopt be the performance of the recommended FSS algorithm Arec or the optimal FSS algorithm on D. Then the Recommendation Performance Ratio (RPR) for Arec (Arec, D) = EARRrec / EAARopt is defined. (8) In this definition, the best performance EARRopt is used as a benchmark. Without the benchmark, it is difficult to determine the recommended algorithms for Arec (Arec, D) as good or not. For example, suppose the EARR is 0.59 D from Arec / Eopt."}, {"heading": "4.2.4 Values of the Parameters \u03b1 and \u03b2", "text": "In the experiment, when presenting the results, two representative pairs of parameters \u03b1 and \u03b2 are used as follows: 1) \u03b1 = 0 and \u03b2 = 0. This setting represents the situation in which the classification accuracy is most important. The higher the classification accuracy compared to the selected characteristics, the better are the corresponding FSS algorithms. 2) \u03b1 6 = 0 and \u03b2 6 = 0. This setting represents the situation in which the user can tolerate precision attenuation and prefer the FSS algorithms with shorter runtime and less selected characteristics. In the experiment, both \u03b1 and \u03b2 are set to 10%, which is significantly different from \u03b1 = \u03b2 = 0. This allows us to explore the effects of these two parameters on the recommendation results. To investigate how parameters affect the selected parameters with respect to the precision of \u03b2 and FSS values by 1% and the proposed number of 1% on the recommended parameters."}, {"heading": "4.3 Experimental Process", "text": "In order to ensure the soundness of our experimental conclusions and to guarantee the reproducibility of the experiments, in this part we present the four key processes used in our experiments: i) meta-knowledge database construction, ii) optimal FSS algorithm to identify a given data set, iii) recommendation method validation and iv) sensitivity analysis of the number of closest data sets on recommendation1) meta-knowledge database ConstruktionProcedure Performance Evaluation Inputs: data = a given data set, i.e, one of the 115 data sets; learner = a given classification algorithm, i.e., one of {Naive Bayes, C4.5, PART, IB1 or Bayes Network}; FSSAlgSet = {FSSAlg1, FSSAlg2, \u00b7 ESSAlg2, \u00b7 ESSAlg22}, the set of the F22, Algorithms."}, {"heading": "2) Optimal FSS algorithm set identification", "text": "The optimal FSS algorithm for a given dataset Di consists of the optimal FSS algorithm for that dataset and those algorithms that do not exhibit a significant performance difference from the optimal FSS algorithm for a given dataset Di. The optimal FSS algorithm for a given dataset Di is determined by a non-parametric Friedman test (1937), followed by a Holm method test (1988) on the performance estimated by the 5 x 10 cross-validation strategy of the 22 FSS algorithms. Otherwise, if the result of the Friedman test shows that there is no significant performance difference between the 22 FSS algorithms, these 22 FSS algorithms are added to the optimal FSS algorithm set. Otherwise, the FSS algorithm with the highest performance is considered as the optimal and added to the optimal FSS algorithm set. Subsequently, the Holm algorithm test is performed to distinguish the optimal FSS algorithms from the rest of the FSS algorithms."}, {"heading": "3) Recommendation method validation", "text": "The leave-one-out strategy is used to empirically evaluate our proposed FSS algorithm recommendation method as follows: For each data set Di (1 \u2264 i \u2264 115) that is considered test data, i) identify the k next data sets from the training data = {D1, \u00b7 \u00b7, Di \u2212 1, Di + 1, \u00b7 \u00b7, D115} without Di; ii) calculate the performance of the 22 candidate FSS algorithms according to Equation (5) based on the k nearest data sets where the value of k is determined by the standard cross-validation strategy, and recommend the first three to Di; and iii) evaluate the recommendations based on the measures introduced in Section 4.2.3."}, {"heading": "4) Sensitivity analysis of the number of the nearest data sets on recommendations", "text": "In order to investigate the impact of the number of closest rows on the recommendations and to provide users with an empirical method for selecting their value, all possible numbers of the closest rows are tested for each row, i.e. when identifying the next k rows for a given row, k is set from 1 to the number of historical rows minus 1 (e.g. 114 in this experiment)."}, {"heading": "4.4 Results and Analysis", "text": "In this section, we present the recommendation results in terms of recommended FSS algorithms, hit rate and performance ratio. Due to the spatial limitation of the paper, we do not list all recommendations, but instead present the results under two significantly different pairs of \u03b1 and \u03b2, i.e. (\u03b1 = 0, \u03b2 = 0) and (\u03b1 = 10%, \u03b2 = 10%). Subsequently, we also provide the experimental results of the influence of user-oriented parameters \u03b1 and \u03b2 on recommendations in terms of classification accuracy, runtime and number of selected features."}, {"heading": "4.4.1 Recommended Algorithms and Hit Ratio", "text": "Figures 2, 3, 4, 5 and 6 show the first recommended FSS algorithms for the 115 datasets when the classification algorithms Naive Bayes, C4.5, PART, IB1 and Bayes Network are recommended respectively. In each figure there are two sub-figures corresponding to the recommendation results for (\u03b1 = 0, \u03b2 = 0) and (\u03b1 = 10%, \u03b2 = 10%) respectively. In each sub-figure we can select the correct and incorrectly recommended algorithms according to the recommendation results. From these figures we determine that for all five classifiers the proposed method is effectively suitable FSS algorithms for most of the 115 datasets. In the case of (\u03b1 = 0, \u03b2 = 0) the number of datasets whose suitable FSS algorithms are correctly recommended is 109 out of 115."}, {"heading": "4.4.2 Recommendation Performance Ratio", "text": "Figures 7 and 8 show the recommendation-performance ratio of the first recommended FSS algorithm for the five classifiers with (\u03b1 = 0, \u03b2 = 0) and (\u03b1 = 10%, \u03b2 = 10%), respectively, and we note that the recommended RPRs of the recommended FSS algorithms are greater than 95% for most datasets and the two settings of \u03b1 and \u03b2, and some of them are up to 100%, regardless of which classifier is used. This indicates that the FSS algorithms recommended by our proposed method are very close to the optimal value. Table 5 shows the average RPRs for the 115 datasets for the five classifiers under different settings of (\u03b1, \u03b2). In this table, the \"Rec\" and \"Def\" columns show the RPR value for each classifier that corresponds to the recommended FSS algorithms and standard FSS algorithms."}, {"heading": "4.4.3 Recommendation Time", "text": "When recommending FSS algorithms for a feature selection problem, the recommendation time is supplied by meta-feature extraction, identification of nearest data sets, and the applicant algorithm ranking according to their performance on k data sets. Of these three recommendation time contributors, only the applicant algorithm ranking with the parameters \u03b1 and \u03b2 of the performance metric EARR is provided. However, the calculation of the performance EARR is independent of the values of \u03b1 and \u03b2. This means that the recommendation time is independent of the specific settings of \u03b1 and \u03b2. Therefore, we present the recommendation time only with (\u03b1 = 0, \u03b2 = 0), and Table 6 shows the details.From Table 6, we find that for a given data set the recommendation time differences for the five classifiers are small. The reason for this is that the recommendation time is mainly controlled by the extraction of meta features in the classification section x2, which has no relationship to the classification feature."}, {"heading": "4.4.4 Impact of the Parameters \u03b1 and \u03b2", "text": "Figures 9, 10, 11, 12 and 13 show the effects of the \u03b1 and \u03b2 settings on classification accuracy, runtime of feature selection, number of selected features, hit rate and RPR value, or exception. Figure 9 shows the classification accuracy of the five classifiers under the different values of \u03b2 and \u03b2. This shows that with the increase in either \u03b1 or \u03b2, users prefer the classification accuracy of the five classifiers using the recommended FSS algorithms to much faster FSS algorithms or the FSS algorithms that can obtain fewer features. Thus, the share of classification accuracy in performance is reduced, which means that the ranking of the FSS algorithms that are executed faster and / or receive fewer features is improved, and the corresponding FSS algorithms are finally selected.Figure 10 shows the runtime of the FSS algorithms that are recommended."}, {"heading": "5. Sensitivity Analysis of the Number of Nearest Data Sets on", "text": "Recommendation Results In this section, we analyze how the number of closest data sets affects recommendation performance. Based on the experimental results, we provide some guidelines for selecting the appropriate number of closest data sets in practice."}, {"heading": "5.1 Experimental Method", "text": "In general, different numbers of the nearest data sets (i.e. k) lead to different recommendations. Therefore, when recommending FSS algorithms for a feature selection problem, an appropriate k value is very important. The k value, which leads to a higher recommendation performance, is preferred. However, the performance difference of the recommendation between two different k values can sometimes be random and not significant. So, to determine a suitable k value from alternatives, we should first determine whether the performance differences between them are statistically significant or not. The non-parametric statistical test Friedman test followed by the Holm procedure test, as proposed in 2006, can be used for this purpose. In the experiment, we performed FSS algorithm recommendations with all possible k values (i.e. from 1 to 114) across the 115 data sets. When identifying the appropriate k values, the non-parametric statistical test is performed as a test where the recommendation is executed best."}, {"heading": "5.2 Results Analysis", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "6. Conclusion", "text": "In this paper, we have presented a recommendation method for FSS algorithms, which aims to support the automatic selection of suitable FSS algorithms for a new problem of feature selection from a number of candidates. The proposed recommendation method consists of building meta-knowledge databases and recommending algorithms. The former obtains the meta-features and performance of all candidate FSS algorithms, while the latter models the relationship between the meta-features and the performance of the FSS algorithm based on a k-NN method and recommends suitable algorithms for a feature selection problem with the built model.We have also thoroughly tested the recommendation method using 115 real data sets, 22 different FSS algorithms and five representative classification algorithms under two typical performance requirements of the user. Experimental results show that our recommendation method is effective."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China with a grant of 61070006."}], "references": [{"title": "Instance-based learning", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": null, "citeRegEx": "Aha et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Aha et al\\.", "year": 1991}, {"title": "Locally weighted learning", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Artificial intelligence review,", "citeRegEx": "Atkeson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Atkeson et al\\.", "year": 1997}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Battiti,? \\Q1994\\E", "shortCiteRegEx": "Battiti", "year": 1994}, {"title": "Metalearning: Applications to data", "author": ["P. Brazdil", "C. Carrier", "C. Soares", "R. Vilalta"], "venue": null, "citeRegEx": "Brazdil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2008}, {"title": "Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results", "author": ["P.B. Brazdil", "C. Soares", "J.P. Da Costa"], "venue": "Machine Learning,", "citeRegEx": "Brazdil et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2003}, {"title": "Addressing the selective superiority problem: Automatic algorithm/model class selection", "author": ["C.E. Brodley"], "venue": "In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "Brodley,? \\Q1993\\E", "shortCiteRegEx": "Brodley", "year": 1993}, {"title": "Meta-data: Characterization of input features for meta-learning", "author": ["C. Castiello", "G. Castellano", "A. Fanelli"], "venue": "Modeling Decisions for Artificial Intelligence,", "citeRegEx": "Castiello et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Castiello et al\\.", "year": 2005}, {"title": "Feature selection for classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent data analysis,", "citeRegEx": "Dash and Liu,? \\Q1997\\E", "shortCiteRegEx": "Dash and Liu", "year": 1997}, {"title": "Consistency-based search in feature selection", "author": ["M. Dash", "H. Liu"], "venue": "Artificial Intelligence,", "citeRegEx": "Dash and Liu,? \\Q2003\\E", "shortCiteRegEx": "Dash and Liu", "year": 2003}, {"title": "Feature selection with a general hybrid algorithm", "author": ["J.T. de Souza"], "venue": "Ph.D. thesis,", "citeRegEx": "Souza,? \\Q2004\\E", "shortCiteRegEx": "Souza", "year": 2004}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar", "year": 2006}, {"title": "Using a data metric for preprocessing advice for data mining applications", "author": ["R. Engels", "C. Theusinger"], "venue": null, "citeRegEx": "Engels and Theusinger,? \\Q1998\\E", "shortCiteRegEx": "Engels and Theusinger", "year": 1998}, {"title": "Generating accurate rule sets without global optimization", "author": ["E. Frank", "I.H. Witten"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Frank and Witten,? \\Q1998\\E", "shortCiteRegEx": "Frank and Witten", "year": 1998}, {"title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "author": ["M. Friedman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Friedman,? \\Q1937\\E", "shortCiteRegEx": "Friedman", "year": 1937}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Characterization of classification algorithms", "author": ["J. Gama", "P. Brazdil"], "venue": "Progress in Artificial Intelligence,", "citeRegEx": "Gama and Brazdil,? \\Q1995\\E", "shortCiteRegEx": "Gama and Brazdil", "year": 1995}, {"title": "Solving feature subset selection problem by a parallel scatter search", "author": ["F. Garci\u2019a Lopez", "M. Garci\u2019a Torres", "B. Melian Batista", "J.A. Moreno Perez", "J.M. MorenoVega"], "venue": "European Journal of Operational Research,", "citeRegEx": "Lopez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lopez et al\\.", "year": 2006}, {"title": "Genetic algorithms in search, optimization, and machine learning. Addison-Wesley Professional", "author": ["D.E. Goldberg"], "venue": null, "citeRegEx": "Goldberg,? \\Q1989\\E", "shortCiteRegEx": "Goldberg", "year": 1989}, {"title": "Large-scale attribute selection using wrappers", "author": ["M. Gutlein", "E. Frank", "M. Hall", "A. Karwath"], "venue": "In Proceedings of IEEE Symposium on Computational Intelligence and Data Mining,", "citeRegEx": "Gutlein et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gutlein et al\\.", "year": 2009}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "Ph.D. thesis, The University of Waikato", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Tabu search for attribute reduction in rough set theory. Soft Computing-A Fusion of Foundations", "author": ["A.R. Hedar", "J. Wang", "M. Fukushima"], "venue": "Methodologies and Applications,", "citeRegEx": "Hedar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hedar et al\\.", "year": 2008}, {"title": "A stagewise rejective multiple test procedure based on a modified bonferroni", "author": ["G. Hommel"], "venue": "test. Biometrika,", "citeRegEx": "Hommel,? \\Q1988\\E", "shortCiteRegEx": "Hommel", "year": 1988}, {"title": "Estimating continuous distributions in Bayesian classifiers", "author": ["G.H. John", "P. Langley"], "venue": "In Proceedings of the eleventh conference on uncertainty in artificial intelligence,", "citeRegEx": "John and Langley,? \\Q1995\\E", "shortCiteRegEx": "John and Langley", "year": 1995}, {"title": "On data and algorithms: Understanding inductive performance", "author": ["A. Kalousis", "J. Gama", "M. Hilario"], "venue": "Machine Learning,", "citeRegEx": "Kalousis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kalousis et al\\.", "year": 2004}, {"title": "Statlog: comparison of classification algorithms on large real-world problems", "author": ["R.D. King", "C. Feng", "A. Sutherland"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "King et al\\.,? \\Q1995\\E", "shortCiteRegEx": "King et al\\.", "year": 1995}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L. Rendell"], "venue": "In Proceedings of the ninth international workshop on Machine learning,", "citeRegEx": "Kira and Rendell,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell", "year": 1992}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "In International joint Conference on artificial intelligence,", "citeRegEx": "Kohavi,? \\Q1995\\E", "shortCiteRegEx": "Kohavi", "year": 1995}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G. John"], "venue": "Artificial intelligence,", "citeRegEx": "Kohavi and John,? \\Q1997\\E", "shortCiteRegEx": "Kohavi and John", "year": 1997}, {"title": "Estimating attributes: analysis and extensions of RELIEF", "author": ["I. Kononenko"], "venue": "In Proceedings of the European conference on machine learning on Machine Learning,", "citeRegEx": "Kononenko,? \\Q1994\\E", "shortCiteRegEx": "Kononenko", "year": 1994}, {"title": "Cleansing data for mining and warehousing", "author": ["M. Lee", "H. Lu", "T. Ling", "Y. Ko"], "venue": "In Proceedings of the 10th International Conference on Database and Expert Systems Applications,", "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "AST: Support for algorithm selection with a CBR approach", "author": ["G. Lindner", "R. Studer"], "venue": "Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "Lindner and Studer,? \\Q1999\\E", "shortCiteRegEx": "Lindner and Studer", "year": 1999}, {"title": "Feature Selection: An Ever Evolving Frontier in Data Mining", "author": ["H. Liu", "H. Motoda", "R. Setiono", "Z. Zhao"], "venue": "In The Fourth Workshop on Feature Selection in Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "A probabilistic approach to feature selection-a filter solution", "author": ["H. Liu", "R. Setiono"], "venue": null, "citeRegEx": "Liu and Setiono,? \\Q1996\\E", "shortCiteRegEx": "Liu and Setiono", "year": 1996}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Liu and Yu,? \\Q2005\\E", "shortCiteRegEx": "Liu and Yu", "year": 2005}, {"title": "Machine learning, neural and statistical classification", "author": ["D. Michie", "D.J. Spiegelhalter", "C.C. Taylor"], "venue": null, "citeRegEx": "Michie et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Michie et al\\.", "year": 1994}, {"title": "Feature selection algorithms: A survey and experimental evaluation", "author": ["L.C. Molina", "L. Belanche", "\u00c0. Nebot"], "venue": "In Proceedings of IEEE International Conference on Data Mining,", "citeRegEx": "Molina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Molina et al\\.", "year": 2002}, {"title": "Development of multi-criteria metrics for evaluation of data mining algorithms", "author": ["G. Nakhaeizadeh", "A. Schnabl"], "venue": "In Proceedings of the 3rd International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "Nakhaeizadeh and Schnabl,? \\Q1997\\E", "shortCiteRegEx": "Nakhaeizadeh and Schnabl", "year": 1997}, {"title": "Towards the personalization of algorithms evaluation in data mining", "author": ["G. Nakhaeizadeh", "A. Schnabl"], "venue": "In Proceedings of the 4th International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "Nakhaeizadeh and Schnabl,? \\Q1998\\E", "shortCiteRegEx": "Nakhaeizadeh and Schnabl", "year": 1998}, {"title": "Floating search methods in feature selection", "author": ["P. Pudil", "J. Novovi\u010dov\u00e1", "J. Kittler"], "venue": "Pattern recognition letters,", "citeRegEx": "Pudil et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pudil et al\\.", "year": 1994}, {"title": "Conceptual base of feature selection consulting", "author": ["P. Pudil", "J. Novovi\u010dov\u00e1", "P. Somol", "R. Vr\u0148ata"], "venue": "system. Kybernetika,", "citeRegEx": "Pudil et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Pudil et al\\.", "year": 1998}, {"title": "Feature selection expertuser oriented approach", "author": ["P. Pudil", "J. Novovi\u010dov\u00e0", "P. Somol", "R. Vr\u0148ata"], "venue": "Advances in Pattern Recognition,", "citeRegEx": "Pudil et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Pudil et al\\.", "year": 1998}, {"title": "C4.5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["M. Robnik-\u0160ikonja", "I. Kononenko"], "venue": "Machine learning,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko", "year": 2003}, {"title": "A review of feature selection techniques", "author": ["Y. Saeys", "I. Inza", "P. Larra\u00f1aga"], "venue": "in bioinformatics. Bioinformatics,", "citeRegEx": "Saeys et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Saeys et al\\.", "year": 2007}, {"title": "Cross-disciplinary perspectives on meta-learning for algorithm selection", "author": ["K.A. Smith-Miles"], "venue": "ACM Computing Surveys,", "citeRegEx": "Smith.Miles,? \\Q2008\\E", "shortCiteRegEx": "Smith.Miles", "year": 2008}, {"title": "Meta analysis of classification algorithms for pattern recognition", "author": ["S.Y. Sohn"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sohn,? \\Q1999\\E", "shortCiteRegEx": "Sohn", "year": 1999}, {"title": "Automatic recommendation of classification algorithms based on data set characteristics", "author": ["Q.B. Song", "G.T. Wang", "C. Wang"], "venue": "Pattern Recognition,", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "A perspective view and survey of meta-learning", "author": ["R. Vilalta", "Y. Drissi"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Vilalta and Drissi,? \\Q2002\\E", "shortCiteRegEx": "Vilalta and Drissi", "year": 2002}, {"title": "The supervised learning no-free-lunch theorems", "author": ["D.H. Wolpert"], "venue": "In Proceedings of 6th Online World Conference on Soft Computing in Industrial Applications,", "citeRegEx": "Wolpert,? \\Q2001\\E", "shortCiteRegEx": "Wolpert", "year": 2001}, {"title": "Feature selection for high-dimensional data: A fast correlationbased filter solution", "author": ["L. Yu", "H. Liu"], "venue": "In Proceedings of The Twentieth International Conference on Machine Leaning,", "citeRegEx": "Yu and Liu,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu", "year": 2003}, {"title": "Searching for interacting features", "author": ["Z. Zhao", "H. Liu"], "venue": "In Proceedings of the 20th International Joint Conference on Artifical Intelligence,", "citeRegEx": "Zhao and Liu,? \\Q2007\\E", "shortCiteRegEx": "Zhao and Liu", "year": 2007}, {"title": "A heuristic-statistical feature selection criterion for inductive machine learning in the real world", "author": ["X. Zhou", "T. Dillon"], "venue": "In Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics,", "citeRegEx": "Zhou and Dillon,? \\Q1988\\E", "shortCiteRegEx": "Zhou and Dillon", "year": 1988}], "referenceMentions": [{"referenceID": 21, "context": "Experiments (Hall, 1999; Zhao & Liu, 2007) have confirmed that there could exist significant differences of performance (e.", "startOffset": 12, "endOffset": 42}, {"referenceID": 5, "context": "However, this solution is quite time-consuming especially for highdimensional data (Brodley, 1993).", "startOffset": 83, "endOffset": 98}, {"referenceID": 21, "context": "However, the published FSS algorithms are rarely tested on the identical group of data sets (Hall, 1999; Zhao & Liu, 2007; Yu & Liu, 2003; Dash & Liu, 2003; Kononenko, 1994).", "startOffset": 92, "endOffset": 173}, {"referenceID": 30, "context": "However, the published FSS algorithms are rarely tested on the identical group of data sets (Hall, 1999; Zhao & Liu, 2007; Yu & Liu, 2003; Dash & Liu, 2003; Kononenko, 1994).", "startOffset": 92, "endOffset": 173}, {"referenceID": 50, "context": "2) At the same time, the famous NFL (No Free Lunch) (Wolpert, 2001) theory tells us that, for a particular data set, different algorithms have different data-conditioned performance, and the performance differences vary with data sets.", "startOffset": 52, "endOffset": 67}, {"referenceID": 46, "context": "The recommendation process can be viewed as a specific application of meta-learning (Vilalta & Drissi, 2002; Brazdil, Carrier, Soares, & Vilalta, 2008) that has been used to recommend algorithms for classification problems (Ali & Smith, 2006; King, Feng, & Sutherland, 1995; Brazdil, Soares, & Da Costa, 2003; Kalousis, Gama, & Hilario, 2004; Smith-Miles, 2008; Song, Wang, & Wang, 2012).", "startOffset": 223, "endOffset": 387}, {"referenceID": 26, "context": "In this paper, the meta-features, which are frequently used in meta-learning (Vilalta & Drissi, 2002; Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Castiello, Castellano, & Fanelli, 2005), are employed to characterize data sets.", "startOffset": 77, "endOffset": 201}, {"referenceID": 4, "context": "In this paper, the meta-features, which are frequently used in meta-learning (Vilalta & Drissi, 2002; Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Castiello, Castellano, & Fanelli, 2005), are employed to characterize data sets.", "startOffset": 77, "endOffset": 201}, {"referenceID": 6, "context": "The meta-features should be not only conveniently and efficiently calculated, but also related to the performance of machine learning algorithms (Castiello et al., 2005).", "startOffset": 145, "endOffset": 169}, {"referenceID": 4, "context": "A number of meta-features have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005; Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer, 1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship between the characteristics of data sets and the performance (e.", "startOffset": 71, "endOffset": 223}, {"referenceID": 6, "context": "A number of meta-features have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005; Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer, 1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship between the characteristics of data sets and the performance (e.", "startOffset": 71, "endOffset": 223}, {"referenceID": 36, "context": "A number of meta-features have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005; Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer, 1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship between the characteristics of data sets and the performance (e.", "startOffset": 71, "endOffset": 223}, {"referenceID": 47, "context": "A number of meta-features have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005; Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer, 1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship between the characteristics of data sets and the performance (e.", "startOffset": 71, "endOffset": 223}, {"referenceID": 26, "context": ", classification accuracy) of learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al., 2004; Smith-Miles, 2008).", "startOffset": 50, "endOffset": 152}, {"referenceID": 4, "context": ", classification accuracy) of learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al., 2004; Smith-Miles, 2008).", "startOffset": 50, "endOffset": 152}, {"referenceID": 25, "context": ", classification accuracy) of learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al., 2004; Smith-Miles, 2008).", "startOffset": 50, "endOffset": 152}, {"referenceID": 46, "context": ", classification accuracy) of learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al., 2004; Smith-Miles, 2008).", "startOffset": 50, "endOffset": 152}, {"referenceID": 6, "context": "The most commonly used meta-features are established focusing on the following three aspects of a data set: i) general properties, ii) statistic-based properties, and iii) informationtheoretic based properties (Castiello et al., 2005).", "startOffset": 210, "endOffset": 234}, {"referenceID": 4, "context": "Actually, a multi-criteria metric adjusted ratio of ratios (ARR) (Brazdil et al., 2003), which combines classification accuracy and runtime together as a unified metric, has been proposed to evaluate the performance of a learning algorithm.", "startOffset": 65, "endOffset": 87}, {"referenceID": 3, "context": "In order to effectively calculate the distance between two data sets, the L1 norm distance (Atkeson, Moore, & Schaal, 1997) is adopted since it is easy to understand and calculate, and its ability in measuring the similarity between two data sets has been demonstrated by Brazdil et al. (2003). Let Fi = <fi,1, fi,2, \u00b7 \u00b7 \u00b7 , fi,h> be the meta-features of data set Di, where fi,p is the value of pth feature of Fi and h is the length of the meta-features.", "startOffset": 272, "endOffset": 294}, {"referenceID": 37, "context": "FSS algorithms can be grouped into two broad categories: Wrapper and Filter (Molina et al., 2002; Kohavi & John, 1997).", "startOffset": 76, "endOffset": 118}, {"referenceID": 17, "context": "iv) Genetic search (GS): A randomized search method which performs using a simple genetic algorithm (Goldberg, 1989).", "startOffset": 100, "endOffset": 116}, {"referenceID": 2, "context": "vi) Rank search (RS) (Battiti, 1994): It uses a feature evaluator (such as gain ratio) to rank all the features.", "startOffset": 21, "endOffset": 36}, {"referenceID": 30, "context": "xii) Ranker (Kononenko, 1994; Kira & Rendell, 1992; Liu & Setiono, 1995): It evaluates each feature individually and ranks the features by the values of their evaluation metrics.", "startOffset": 12, "endOffset": 72}, {"referenceID": 21, "context": "ii) Dependency (Hall, 1999; Yu & Liu, 2003): This kind of measure evaluates the worth of a subset of features by considering the individual predictive ability of each feature along with the degree of redundancy among these features.", "startOffset": 15, "endOffset": 43}, {"referenceID": 30, "context": "iii) Distance (Kira & Rendell, 1992; Kononenko, 1994): This kind of measure is proposed based on the assumption that the distance of instances from different target concepts is greater than that from same target concepts.", "startOffset": 14, "endOffset": 53}, {"referenceID": 49, "context": "0001 suggested by Zhao and Liu (2007). For FSS algorithm \u201cFCBF\u201d, we set the relevance threshold to be the SU (Symmetric Uncertainty) value of the bN/ log Ncth ranked feature suggested by Yu and Liu (2003).", "startOffset": 18, "endOffset": 38}, {"referenceID": 49, "context": "For FSS algorithm \u201cFCBF\u201d, we set the relevance threshold to be the SU (Symmetric Uncertainty) value of the bN/ log Ncth ranked feature suggested by Yu and Liu (2003). For FSS algorithm \u201cRelief-F\u201d, we set the significance threshold to 0.", "startOffset": 148, "endOffset": 166}, {"referenceID": 30, "context": "01 used by Robnik-\u0160ikonja and Kononenko (2003). For FSS algorithm \u201cSignific\u201d, there is a threshold, statistical significance level \u03b1, used to identify the irrelevant features.", "startOffset": 30, "endOffset": 47}, {"referenceID": 43, "context": "5 (Quinlan, 1993), rule-based PART (Frank & Witten, 1998), and instance-based IB1 (Aha, Kibler, & Albert, 1991), respectively.", "startOffset": 2, "endOffset": 17}, {"referenceID": 14, "context": "esis that the features are conditional independent (John & Langley, 1995), while Bayes Net takes into account the feature interaction (Friedman et al., 1997).", "startOffset": 134, "endOffset": 157}, {"referenceID": 28, "context": "For each data set Di (1 \u2264 i \u2264 115), we i) extract its meta-features Fi; ii) calculate the EARRs for the 22 candidate FSS algorithms with the stratified 5\u00d710-fold cross-validation strategy (Kohavi, 1995), and iii) combine the meta-features Fi and the EARR of each FSS algorithm together to form a tuple, which is finally added to the meta-knowledge database.", "startOffset": 188, "endOffset": 202}, {"referenceID": 13, "context": "The optimal FSS algorithm set for a given data set Di is obtained via a non-parametric Friedman test (1937) followed by a Holm procedure test (1988) on the performance, which", "startOffset": 87, "endOffset": 108}, {"referenceID": 13, "context": "The optimal FSS algorithm set for a given data set Di is obtained via a non-parametric Friedman test (1937) followed by a Holm procedure test (1988) on the performance, which", "startOffset": 87, "endOffset": 149}, {"referenceID": 10, "context": "Non-parametric statistical test, Friedman test followed by Holm procedure test as suggested by Dem\u0161ar (2006), can be used for this purpose.", "startOffset": 95, "endOffset": 109}], "year": 2013, "abstractText": "Many feature subset selection (FSS) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. At the same time, so far there is rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus, FSS algorithm automatic recommendation is very important and practically useful. In this paper, a meta learning based FSS algorithm automatic recommendation method is presented. The proposed method first identifies the data sets that are most similar to the one at hand by the k -nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. Then, it ranks all the candidate FSS algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. The proposed recommendation method is extensively tested on 115 real world data sets with 22 wellknown and frequently-used different FSS algorithms for five representative classifiers. The results show the effectiveness of our proposed FSS algorithm recommendation method.", "creator": " TeX output 2013.05.02:0946"}}}