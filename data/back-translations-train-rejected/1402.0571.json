{"id": "1402.0571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Analysis of Watson's Strategies for Playing Jeopardy!", "abstract": "Major advances in Question Answering technology were needed for IBM Watson to play Jeopardy! at championship level -- the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates. In addition, Jeopardy! features four types of decision making carrying great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., \"buzz in.\" Using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a players overall chances to win, when compared with simple \"rule of thumb\" strategies. This article presents our approach to developing Watsons game-playing strategies, comprising development of a faithful simulation model, and then using learning and Monte-Carlo methods within the simulator to optimize Watsons strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulators predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes. We further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.", "histories": [["v1", "Tue, 4 Feb 2014 01:37:44 GMT  (511kb)", "http://arxiv.org/abs/1402.0571v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gerald tesauro", "david c gondek", "jonathan lenchner", "james fan", "john m prager"], "accepted": false, "id": "1402.0571"}, "pdf": {"name": "1402.0571.pdf", "metadata": {"source": "CRF", "title": "Analysis of Watson\u2019s Strategies for Playing Jeopardy!", "authors": ["Gerald Tesauro", "David C. Gondek", "Jonathan Lenchner", "James Fan", "John M. Prager"], "emails": ["gtesauro@us.ibm.com", "dgondek@us.ibm.com", "lenchner@us.ibm.com", "fanj@us.ibm.com", "jprager@us.ibm.com"], "sections": [{"heading": null, "text": "This article presents our approach to developing Watson's game strategies, which involves developing a faithful simulation model and then using learning and MonteCarlo methods within the simulator to optimize Watson's strategic decision making. After detailing our game strategy algorithms, we then focus in particular on validating the accuracy of simulation forecasts and documenting performance improvements with our methods. Quantitative performance benefits are demonstrated both in terms of simple heuristic strategies and the actual performance of human participants in historical episodes. We extend our analysis of the human game with a number of valuable and counterintuitive examples that illustrate how human participants can improve their performance on the show."}, {"heading": "1. Introduction", "text": "The show features challenging questions (called \"clues\" in the language of the show) drawn from a very wide range of topics; the clues can embody all kinds of complex and ambiguous language, including vague insinuations and innuendo, irony, humor and pun. The rules of the game in regular episodes (Jeopardy, 2013) are as follows: There are two main rounds of the game in which each round uses a board of 30 squares, organized as five squares in six different categories, each square containing a hidden clue. Second-round clues have higher dollar values, which probably reflect greater difficulty. In typical games, a square by category and dollar amount is selected by the player in control of the board, and its clues are read out by the hosts."}, {"heading": "1.1 Glossary", "text": "In this section, we provide definitions of various technical terms and notations that are used in subsequent sections to describe our simulation models, strategies or aspects of Jeopardy strategy. \u2022 A, B and C - The players with the highest, second highest and third highest scores, respectively, their current results. \u2022 Accuracy - The probability that a player answers a clue correctly in situations where answering is mandatory (Daily Doubles or Final Jeopardy). \u2022 Anti-Two-Third Bet - Potential Counterstrategy for A in the Two-Third Final Jeopardy scenario (see \"Two-Third Bet\"). After a Two-Third Bet, the B-Model score is a maximum of 4B-2A. This will be less than A if B is less than three-quarters of A. Hence, A could guarantee a win by placing a small bet of no more than 3A-4B. However, such a bet is vulnerable to B making a large bet based on A. A, which is a significant model based on A."}, {"heading": "2. Simulation Model Approach", "text": "Since we are optimizing Watson's strategies over millions of synthetic matches, it is important that the simulations are faithful enough to provide reasonably accurate predictions about various important statistics of live matches. Developing such a simulator required considerable effort, especially in the development of human opponent models. Using a simulator to optimize strategies is a well-established practice in computer game research. Simulated games can provide orders of magnitude where more data than live games are required, and they do not suffer from excessive problems that are often encountered in matching a fixed set of test positions. While it is usually easy to develop a perfect model of game rules, simulation-based approaches can pose a significant challenge when accurate models of opposing strategies are required. In traditional two-player zero-sum board games (backgammon, checkers, chess, etc.), such modeling is usually not required - one can simply aim to minimize the maximum of the game."}, {"heading": "2.1 Daily Double Placement", "text": "We have calculated the common row-column frequencies in the J! archive data of round 1 and round 2 DD placement; the round 2 frequencies are shown in Figure 1. Our analysis confirms well-known observations that DDs tend to be found in the lower rows (third, fourth and fifth) of the board and basically never appear in the second row. We were surprised to discover that there are also column dependencies, i.e. some columns are more likely to contain a DD than others. Thus, DDs will most likely appear in the first column, and least likely appear in the second column."}, {"heading": "2.3 Final Jeopardy Accuracy/Betting Model", "text": "The historical data set we obtained from the J archive shows that human accuracy in answering Final Jeopardy is more of a triple result than a single one: about 50% for average participants, 60% for champions, and 66% for grand champions. Furthermore, it is also clear that the accuracy between participants is positively correlated, with a correlation coefficient that provides the best fit for the data. We use these parameter values to simulate stochastic FJ studies, implementing drawings of three correlated binary Correct / False results, with averages and correlations matched to the corresponding values, by first generating correlated correct real numbers using a multi-variant normal distribution, and then applying appropriately chosen thresholds to convert the desired averages into the desired averages (Leisch, Weingessel, & Hornik, 1998) to determine the exact return rate of a given FJ combination."}, {"heading": "2.4 Regular Clue Model", "text": "Our stochastic process model of regular (non-DD) clues generates a random correlated binary combination of three = 61 J, which indicates which players are trying to sum up, and a random correlated binary combination of three, which indicates whether the players have a correct answer or not. In the case of an challenged buzz, a buzz winner is randomly selected based on the participants \"relative\" buzzability \"(ability to win a challenged buzz, assuming the same in all human games). As described in the glossary of Section 1.1, the buzz-in results are governed by two adjustable parameters, meaning\" buzz rate \"b and\" buzz correlation. \"The right / wrong results of the model are achieved 6. The human win ratios add up to 101%, which determines the probability of a first place lower. Also regulated by two parameters,\" precision @ b \"and\" buzz correlation \"mean.\" We have a simple preference / four. \""}, {"heading": "2.5 Square Selection Model", "text": "Most human participants tend to select within a certain category from top to bottom, and they also tend to stay within a category rather than jumping across categories. There is another faint tendency to select categories that move anywhere from left to right. Based on these observations and the likely effects of Watson's square selection, we developed an average constant model of square selection that stays in the current category with a 60% probability and otherwise jumps into a random other category. When selecting within a category, there is a high probability (90%) of selecting the top available square. In contrast, we model Champion and Grand Champion square selection as DD searches based on the known sequence statistics of DD placement. Strong players generally perform more daily double searches when selecting squares, and when playing against Watson, they quickly adopt open DD search behavior."}, {"heading": "2.6 Multi-game Wagering Model", "text": "In most Jeopardy! contests, the winner is determined by performance in a single game. Given that the show also hosts several annual tournaments, such as the Tournament of Champions, where the final game scores scores over two games to determine first, second, and third place, this clearly means that the betting strategies in Game 1 and Game 2 of the game must differ, and both must be different from the single game bet. Since there is very limited multigame data available from J! archives (only about two dozen finals of the champion), it would be quite difficult to model the expected bets of Jennings and Rutter in our exhibition match purely based on historical data. Fortunately, we were able to make some educated guesses that greatly simplified the task. First, we predicted that they would bet DDs very aggressively in both games, unless they had an overwhelming lead."}, {"heading": "2.7 Model Validation", "text": "Our first effort to confirm the simulator's predictions was about halfway through Watson's first series of sparring games. At that point, the simulator had only been used to develop Watson's Final Jeopardy betting algorithm, so the simulator was basically running a Watson model with heuristic strategies against the average competitor's model. The predicted results were \"ex-post\" (in hindsight) predictions, as we needed data from the live games to set certain simulation parameters, especially with respect to Watson's buzzability. We were encouraged to see that Watson's predicted rates of winning a game (62%) resulting in Final Jeopardy (72%) and by lockout (27%) were within the standard error of the actual rates (64%, 76% and 26%, respectively). There were more significant deviations on the low side in the predicted final results of Watson vs. the real people (15800 vs. 18400 and both actual R300)."}, {"heading": "3. Optimizing Watson\u2019s Strategies Using the Simulation Model", "text": "The simulator described in the previous section allows us to estimate Watson's performance for a particular set of strategy modules for candidates by conducting large-scale contests between a Watson simulation model and two simulated human opponents. Watson's stochastic process models use the same performance indicators (i.e. average trial rate, precision, DD and FJ accuracy) as in the human models; the parameter values were estimated by test sets from the J! archive and were updated several times during the project as Watson improved; the Watson model also estimates buzzability, i.e. the probability of winning the buzz against people of different abilities; these estimates were initially based on informal live demo games against IBM researchers and were subsequently refined based on Watson's performance in sparring games; we also estimated Watson's buzzability against two people at 80% for average participants, 73% for champions and 70% for grand champions; the computing speed was an important factor in designing strategy modules as few seconds need to be executed in single strategy modules, and J in adverse bets."}, {"heading": "3.1 Daily Double Wagering", "text": "We implemented a principled approach to DD betting based on the estimation of Watson's likelihood of correctly answering the DD clue and estimated how a particular bet affects Watson's overall odds of winning if he correctly or incorrectly assesses the DD clue. The first estimate is provided by an in-category DD confidence model. Based on thousands of tests that contain DD clues, Watson's accuracy model is based on the number of previously seen clues in the category that Watson correctly and incorrectly assessed. In order to estimate the impact of a bet on odds of winning, we follow Tesauro's (1995) work in using Reinforcement Learning (Sutton & Barto, 1998) to form a Game State Evaluator (GSE) over the course of millions of simulated Watson vs. Human games. In light of a feature vector description of a current score, we are not implementing Annlinear clues of the functionality."}, {"heading": "3.1.1 Illustrative Example", "text": "Figure 6 illustrates how the DD bet analysis works and how the resulting bet depends heavily on confidence in the category. An example comes from one of the sparring games where Watson received four consecutive clues in the first category at the start of Double Jeopardy and then found the first DD while trying to finish the category. At that point, Watson was 11000 points and people had each 4,200. Watson's confidence in the category took its maximum value, 75%, based on having previously received four out of four correct answers in the category. Watson decided to bet $6,700, which is a highly aggressive bet by human standards. (Fortunately, he got the DD clue right!) The left figure shows that the neural net share estimates for getting the DD (top curve) and wrong (bottom curve) are very smooth at various amounts of the bet, with the curves showing a slight decrease in the gradient of the curve being extremely high relative to the equity curve (the resulting from the equity curve at 75%)."}, {"heading": "3.1.2 Endgame Monte-Carlo Wagers", "text": "For the Series 2 Sparring Games, we significantly increased the simulation speed for regular clues and Final Jeopardy, allowing the replacement of the neural net bet in endgame states with a routine based on live Monte Carlo studies. Essentially, this analysis provides complete knowledge of which bet achieves the highest win rate in the simulation, even though it is still subject to modeling errors and confidence estimation errors. In addition, the primary weakness in Watson's DD strategy has been eliminated, since neural net miscalculations in endgames often resulted in serious errors that could exceed the equity loss of well over 1%. As explained in Section 3.1.3 below, the use of the Monte Carlo analysis for endgame bets resulted in a fairly significant decrease (more than a factor of two) in the overall error rate in DD bets. With few clues remaining before Final Jeopardy, the dependence of equity on the score complexity of a game's behavior can be observed as opposed to the increase in the average score of a game's complexity and behavior."}, {"heading": "3.1.3 Performance Metrics and Error Analysis", "text": "We evaluated the performance of neural net DD betting using two different methods. First, we found an improved win rate in simulations compared to Watson's previous DD betting algorithm, a set of heuristic betting rules that tried to safely add Watson's lead or safely catch up without falling short of certain strategically important points. While the heuristic rules embodied sound logic, they suffered a significant limitation in not taking Watson's trust in the category into account, so that they would generate the same bet regardless of trust.Using heuristic DD betting rules, Watson's simulated win rate was 61%. Because neural net DD bets used a default trust value for each DD, the win rate improved to 64%. When we added the emulation of live DD confidence values to the simulation, the result was another jump in the win rate, to 67%."}, {"heading": "3.1.4 Human DD Error Analysis", "text": "In fact, most people who are able to surpass themselves, to surpass themselves, to surpass themselves and to survive, \"he told the Deutsche Presse-Agentur in an interview with the\" Welt am Sonntag \"newspaper:\" I don't think people are able to surpass themselves. \"He pointed out that people are able to overestimate and overwhelm themselves:\" I don't think they are able to surpass themselves. \"He pointed out that people are able to\" surpass themselves. \""}, {"heading": "3.1.5 Multi-game DD Wagering", "text": "As mentioned in section 2.6, Game 1 and Game 2 of our Exhibition Match require distinct betting strategies, both of which differ from individual bets. We trained separate neural networks for Game 1 and Game 2. Game 2 Net was trained first, using a plausible artificial distribution of Game 1 using reasonable betting odds. After we trained Game 2 Neural Net, we were then able to estimate the expected probabilities that Watson would finish Game 1 in first, second or third position, starting from any combination of Game 1 endpoint numbers, through extensive offline Monte Carlo simulations. We used this to create three reference tables, in cases where Watson finishes Game 1 in first, second or third position, of Watson Match shares in various Game 1 endpoint combinations ranging from (0, 0, 0) to (72000, 0) in increments of 6000. (Since the addition or subtraction of a constant from all Game 1 results we have no effect on match shares, we can subtract the generality without losing."}, {"heading": "3.2 Final Jeopardy Wagering", "text": "Our approach to Final Jeopardy bets involves calculating a \"best response\" strategy that we have recorded more effectively than typical human errors. \"(Fudenberg & J, 1991) (a standardized game theory concept) On the human FJ model presented in Section 2.3. We considered the attempt to calculate a balance strategy, but whether we would know in principle an exemption from the trust of opponents (Fudenberg & Tirole, 1991), but decided against it for two reasons: First, due to the imperfect information in Final Jeopardy (participants know their own trust in view of the category title, but do not know that the trust of opponents would not know the trust), we would in principle have to justify a Bayes-Nash-Nash equilibrium situation (BNE), which entails considerably more modeling and arithmetic challenges. Second, it seems far fetched to assume that an equilibrium situation, since we would have calculated their opponent's balance."}, {"heading": "3.3 Square Selection", "text": "We considered four different factors that may be relevant to Watson's optimal overall goal in deciding which square to select in a particular state of play: \u2022 Choosing a Daily Double Square: Finding the DDs can quickly provide Watson with an excellent opportunity to significantly improve his standing in the game, while also denying other players that opportunity. \u2022 The potential downside is that Watson may have only a slim chance of winning the bet, and that he may have little or no evidence of how likely it is that Watson answers the DD clue correctly. \u2022 Maintaining control over the board: This includes estimating categories and / or square values where Watson has the best chance of winning the buzz and responding correctly. This would give Watson another chance to find a DD number if the selected square turns out to be a regular clue. \u2022 Learning the \"essence\" of a category, i.e. gathering information about the category such as the type of correct answers, to improve the accuracy of the clues in the following category."}, {"heading": "3.3.1 Bayesian DD Probability Calculation", "text": "We calculate pDD (i), the probability that square i contains a square i = square i (Ds) = square i (Ds) = square i (D) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) c (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) c) p (c) c) p (c) c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c) p (c) c) p (c) p (c) c c c c c c c c) c c c) c c) c) c) c) c) c) c) c) c) c) c) c (c) c) c) p (c) c) c) p (c) c) c) c) p (c) c) c) c) p (c) c) c) c) p (c) c) c) c) p (c) c) c) c) c) p (c) c) c) c) c) c) c) p (c) c) c) c) c) c) c) c) p (c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c)"}, {"heading": "3.3.2 Square Selection Performance Metrics", "text": "In Table 5, we report on a comprehensive benchmarking of Watson's performance using five different combinations of different square selection algorithms. The first column refers to strategy if there are available DDs to be played in the round, while the second column refers to strategy after all DDs have been played in the round. In these experiments, only the two-game match format was used with Grand Champion models of human participants. As previously mentioned, these human models use aggressive DDD and FJ bets, and simple DD searches for use of known line statistics when DDs are available. Watson simulations use right / wrong answers drawn from historical categories, so Watson will show learning from revealed answers in a category. As an interesting consequence of Watson's learning, we model human square selection without remaining DDs according to an \"anti-learning strategy\" aimed at frustrating strategies by selecting at the bottom of the category with the greatest potential benefit from learning."}, {"heading": "3.4 Confidence Threshold for Attempting to Buzz", "text": "In the vast majority of the game states, the threshold was set to a default value of close to 50%. While we did not have any analysis suggesting that this was an optimal threshold, we had a strong argument that a 50% threshold would maximize Watson's expected score in both cases, but opponents would have a much better chance of improving their score if Watson did not have a buzz.From an approximate threshold calculation based on a \"max delta\" target (described in Appendix 2), we had suggestive evidence that the initial buzz threshold should be more aggressive."}, {"heading": "3.4.1 Illustrative Examples", "text": "The approximate DP buzz-in algorithm is hard to believe at first glance, but can be estimated after detailed analysis: a so-called \"desperation buzz\" on the last clue, where Watson must buzz in and out of correct answers to avoid being locked out (e.g., let's assume Watson has 4000, the human entrants have deliberately crossed the 10,000 and 2000 threshold, and the final clue is $1200). Generally, aggressive buzzing shows the biggest deviation from standard buzzing near certain critical breakpoints, such as the crossover from third place to second place, or from second to first place. If a player's score is just under one of these breakpoints, aggressive buzzing is usually correct. Conversely, with a score just above a critical breakpoint, players should be much more conservative about the fall.The most critical breakpoint is where a player reaches a guaranteed lockout."}, {"heading": "4. Lessons for Human Contestants", "text": "Now that Watson has retired as a Jeopardy! participant, any future impact of our work to improve Jeopardy! performance will be specific to human participants. In this section, we present a number of interesting insights that could help future participants improve their overall odds of winning."}, {"heading": "4.1 Basics of Final Jeopardy Strategy", "text": "The observed FJ bets in our J archive suggest that many contestants appearing on the show make little effort to learn good strategies for FJ bets, apart from the elementary concept of covering A bets at least 2B-A to cover double the score of B. Although we do not intend to present a definitive treatise on FJ strategies in this section, we can illustrate what we have seen as the most important regions and dividing lines in the FJ strategy room in a single plot. Since FJ scenarios are invariant, each scenario is clearly determined by two variables: B's result in relation to A, and C's result in relation to B. The ratio of B to A is the most important quantity in Final Jeopardy and the most important breakpoint (apart from B < A / 2, which is a lockout) is clearly determined by two variables: B's result in relation to A, C's result in relation to B's final score < which is the most important in relation to B's lockout; and B's minimum score <"}, {"heading": "4.2 More Aggressive DD Wagering", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "4.3 Counterintuitive Buzz-in Thresholds", "text": "Arguably the most counterproductive finding of our analysis of the buzz-in-confidence thresholds is that the attempt to answer may be correct even if a player has negative equity expectations to do so. If we discussed Watson's 50% default threshold with human participants during the sparring games, many of them seemed surprised at such a low equity value, and some even raised vocal objections. While their arguments are not based on quantitative equity estimates, they seem to intuitively realize that Watson's score would decline to 50% confidence after the buzzing, since Watson's goal difference would be on average zero, but one of the opposing results would likely rise. We have tried to explain that this is clearly better than not the buzzing alternative, where Watson would again have zero expected goal changes, but the unhindered opponents would have a greater chance of a goal difference. After we have developed an offline Monte Carlo method for calculating buzz thresholds for humans (see below for details of the MC's appeal for simple thresholds)."}, {"heading": "4.4 Lock-Tie Implications", "text": "We conclude this section by looking at some of the strange and amusing consequences of the lock-tie scenario, in which Lindlock considerations play a role. For Final Jeopardy, where unusual situations arise, in which unusual situations arise, in which B's score is exactly half of A's score. In this scenario, A is likely to bet nothing, so B can tie for first place by betting everything and doing FJ right. This is clearly preferable if you have more than half of A's score, where B's would have to get more than half of A's score, and A to get FJ wrong in order to win. B's preference for a lower score can lead to some unusual strategy decisions (to say the least) near the end of the game, where Dupee (1998) discusses that DD's on the last clue bet, Final 100-D700, is correct with Jeopardy 971000 leading."}, {"heading": "5. Conclusions", "text": "In fact, the situation is that most people who are able to outdo themselves, outdo themselves and outdo themselves \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "Acknowledgments", "text": "We are grateful to the entire IBM global team that made the Watson project possible, to the Sony / JPI team that made Exhibition Match and Sparring Games possible, and to all former Jeopardy! participants who volunteered to participate in live test games with Watson. We thank the anonymous reviewers and Ed Toutant for many helpful comments and suggestions to improve the manuscript."}, {"heading": "Appendix A. Watson\u2019s Competitive Record", "text": "Before Watson appeared on Jeopardy!, Watson played more than 100 \"sparring games\" against former Jeopardy! contestants in a realistic replica of a television studio built at the IBM Research Center in Yorktown Heights, N.Y. The studio featured real Jeopardy! contestants and signalers and used the actual JPI (Jeopardy Productions Inc.) game control system. Content for each game (categories, clues, answers) was delivered directly by JPI and consisted of actual Jeopardy! episodes that had already been recorded but not yet aired. (This eliminated the possibility that contestants could have previously seen the content used in their games.) A professional actor Todd Crain was hired to host the games. To motivate participants, they were rewarded with $1,000 each for the first place finishers and $250 for the second place finishers."}, {"heading": "Appendix B. Buzz Threshold Calculation Details", "text": "We assume that we will not change from the first buzz decision of a player (the \"strategic\" player), and that the two opponents are \"non-strategic\" players, where their buzz decisions are determined by some fixed stochastic processes. We also assume that the buzz decisions of the opponents will not change from the initial buzz decision to a rebound situation. By default, the strategic player is Watson, although we have also developed a similar method to set trust thresholds for human constants. The calculation invoked by Watson in live games is not yet known, since computing returned a trust value before the QA system."}], "references": [{"title": "Breakdown of Will", "author": ["G. Ainslie"], "venue": "Basic Books. Bertsekas, D. P", "citeRegEx": "Ainslie,? \\Q2001\\E", "shortCiteRegEx": "Ainslie", "year": 2001}, {"title": "Rollout algorithms for stochastic scheduling problems", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": "J. of Heuristics,", "citeRegEx": "Bertsekas and Castanon,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1999}, {"title": "The first international RoShamBo programming competition", "author": ["D. Billings"], "venue": "Intl. Computer Games Assn. Journal,", "citeRegEx": "Billings,? \\Q2000\\E", "shortCiteRegEx": "Billings", "year": 2000}, {"title": "The challenge of poker", "author": ["D. Billings", "A. Davidson", "J. Schaeffer", "D. Szafron"], "venue": "Artificial Intelligence,", "citeRegEx": "Billings et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Billings et al\\.", "year": 2002}, {"title": "How to Get on Jeopardy! and Win", "author": ["M. Dupee"], "venue": null, "citeRegEx": "Dupee,? \\Q1998\\E", "shortCiteRegEx": "Dupee", "year": 1998}, {"title": "Building Watson: an Overview of the DeepQA Project", "author": ["D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A.A. Kalyanpur", "C. Welty"], "venue": "AI Magazine,", "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Introduction to \u201cThis is Watson", "author": ["D.A. Ferrucci"], "venue": "IBM J. of Research and Development,", "citeRegEx": "Ferrucci,? \\Q2012\\E", "shortCiteRegEx": "Ferrucci", "year": 2012}, {"title": "GIB: Steps toward an expert-level bridge-playing program", "author": ["M.L. Ginsberg"], "venue": "Proc. of the Sixteenth Intl. Joint Conf. on Artificial Intelligence,", "citeRegEx": "Ginsberg,? \\Q1999\\E", "shortCiteRegEx": "Ginsberg", "year": 1999}, {"title": "Prisoner of Trebekistan: a decade in Jeopardy! Crown Publishers", "author": ["B. Harris"], "venue": null, "citeRegEx": "Harris,? \\Q2006\\E", "shortCiteRegEx": "Harris", "year": 2006}, {"title": "On the generation of correlated artificial binary data", "author": ["F. Leisch", "A. Weingessel", "K. Hornik"], "venue": "Vienna Univ. of Economics and Business Administration, Working Paper No", "citeRegEx": "Leisch et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leisch et al\\.", "year": 1998}, {"title": "An Introduction to Copulas", "author": ["R.B. Nelsen"], "venue": null, "citeRegEx": "Nelsen,? \\Q1999\\E", "shortCiteRegEx": "Nelsen", "year": 1999}, {"title": "Special questions and techniques", "author": ["J.M. Prager", "E.W. Brown", "J. Chu-Carroll"], "venue": "IBM J. of Research and Development,", "citeRegEx": "Prager et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Prager et al\\.", "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing: Volume 1: Foundations,", "citeRegEx": "Rumelhart et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1987}, {"title": "World-championship-caliber Scrabble", "author": ["B. Sheppard"], "venue": "Artificial Intelligence,", "citeRegEx": "Sheppard,? \\Q2002\\E", "shortCiteRegEx": "Sheppard", "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Temporal difference learning and TD-Gammon", "author": ["G. Tesauro"], "venue": "Commun. ACM,", "citeRegEx": "Tesauro,? \\Q1995\\E", "shortCiteRegEx": "Tesauro", "year": 1995}, {"title": "On-line policy improvement using Monte-Carlo search", "author": ["G. Tesauro", "G. Galperin"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Tesauro and Galperin,? \\Q1996\\E", "shortCiteRegEx": "Tesauro and Galperin", "year": 1996}, {"title": "Simulation, learning, and optimization techniques in Watson\u2019s game strategies", "author": ["G. Tesauro", "D.C. Gondek", "J. Lenchner", "J. Fan", "J.M. Prager"], "venue": "IBM J. of Research and Development,", "citeRegEx": "Tesauro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tesauro et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Building the DeepQA architecture, and advancing its performance at Jeopardy! to a competitive level with human contestants, would ultimately require intense work over a four-year period by a team of two dozen IBM Researchers (Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, ..., & Welty, 2010; Ferrucci, 2012).", "startOffset": 225, "endOffset": 315}, {"referenceID": 5, "context": "Building the DeepQA architecture, and advancing its performance at Jeopardy! to a competitive level with human contestants, would ultimately require intense work over a four-year period by a team of two dozen IBM Researchers (Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, ..., & Welty, 2010; Ferrucci, 2012). Rather than discussing Watson\u2019s QA performance, which is amply documented elsewhere, the purpose of this paper is to address an orthogonal and significant aspect of winning at Jeopardy!, namely, the strategic decision-making required in game play. There are four types of strategy decisions: (1) wagering on a Daily Double (DD); (2) wagering in Final Jeopardy (FJ); (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., \u201cbuzz in.\u201d The most critical junctures of a game often occur in the Final Jeopardy round and in playing Daily Doubles, where wagering is required. Selecting a judicious amount to wager, based on one\u2019s confidence, the specific game situation, and the likely outcomes of the remaining clues, can make a big difference in a player\u2019s overall chance to win. Also, given the importance of Daily Doubles, it follows that a player\u2019s square selection strategy when in control of the board should result in a high likelihood of finding a DD. Allowing one\u2019s opponents to find the DDs can lead to devastating consequences, especially when playing against Grand Champions of the caliber of Ken Jennings and Brad Rutter. Furthermore, a contestant\u2019s optimal buzz-in strategy can change dramatically in certain specific end-game scenarios. For example, a player whose score is just below half the leader\u2019s score may need to make a \u201cdesperation buzz\u201d on the last clue in order to avoid a guaranteed loss. Conversely, at just above half the leader\u2019s score, the correct strategy may be to never buzz in. There is scant prior literature on Jeopardy! game strategies, and nearly all of it is qualitative and heuristic, with the sole exception of Final Jeopardy strategy, where substantial quantitative analysis is embodied in the J! Archive (2013) Wagering calculator.", "startOffset": 226, "endOffset": 2119}, {"referenceID": 4, "context": "Additionally, Dupee (1998) provides a detailed analysis of betting in Final Jeopardy, with particular", "startOffset": 14, "endOffset": 27}, {"referenceID": 8, "context": "Harris (2006), one of the show\u2019s top contestants, provides numerous qualitative insights into strategic thinking at championship level, including the importance of seeking DDs in the bottom rows, wagering to position for Final Jeopardy, and protecting a lead late in the game by being cautious on the buzzer.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "\u2022 Precision, Precision@b - For regular clues, the average probability that a player will answer correctly on the fraction of clues (b) in which the player chooses to buzz in and answer (Ferrucci et al., 2010).", "startOffset": 185, "endOffset": 208}, {"referenceID": 2, "context": "By contrast, in repeated normalform games such as Prisoner\u2019s Dilemma and Rock-Paper-Scissors, a one-shot equilibrium strategy is trivial to compute but insufficient to win in tournament competitions (Axelrod, 1984; Billings, 2000).", "startOffset": 199, "endOffset": 230}, {"referenceID": 3, "context": "Poker is another prominent game where opponent modeling is essential to achieve strong play (Billings et al., 2002) .", "startOffset": 92, "endOffset": 115}, {"referenceID": 5, "context": "likewise governed by two parameters, mean \u201cprecision@b\u201d (Ferrucci et al., 2010) or simply mean \u201cprecision\u201d p, and \u201cright/wrong correlation\u201d \u03c1p.", "startOffset": 56, "endOffset": 79}, {"referenceID": 15, "context": "To estimate impact of a bet on winning chances, we follow the work of Tesauro (1995) in using Reinforcement Learning (Sutton & Barto, 1998) to train a Game State Evaluator (GSE) over the course of millions of simulated Watson-vs-humans games.", "startOffset": 70, "endOffset": 85}, {"referenceID": 11, "context": ", gathering information about the category such as the type of correct answers, so as to improve accuracy on subsequent clues in the category (Prager et al., 2012).", "startOffset": 142, "endOffset": 163}, {"referenceID": 7, "context": "In order to achieve acceptable real-time computation taking at most \u223c1-2 seconds, we therefore implemented an Approximate DP calculation in which Equation 5 is only used in the first step to evaluate VK in terms of VK\u22121, and the VK\u22121 values are then based on plain Monte-Carlo trials (Tesauro & Galperin, 1996; Ginsberg, 1999; Sheppard, 2002).", "startOffset": 284, "endOffset": 342}, {"referenceID": 13, "context": "In order to achieve acceptable real-time computation taking at most \u223c1-2 seconds, we therefore implemented an Approximate DP calculation in which Equation 5 is only used in the first step to evaluate VK in terms of VK\u22121, and the VK\u22121 values are then based on plain Monte-Carlo trials (Tesauro & Galperin, 1996; Ginsberg, 1999; Sheppard, 2002).", "startOffset": 284, "endOffset": 342}, {"referenceID": 0, "context": "Many psychological studies have documented \u201cirrational\u201d preferences for taking immediate gains, or avoiding immediate losses, which have been attributed to so-called \u201chyperbolic discounting\u201d (Ainslie, 2001).", "startOffset": 191, "endOffset": 206}, {"referenceID": 4, "context": "For example, Dupee (1998) discusses DD wagering on the last clue before Final Jeopardy, where the DD player has 7100 and the opponents have 9000 and 1000.", "startOffset": 13, "endOffset": 26}, {"referenceID": 10, "context": "We obtain correlated draws from the resulting multi-variate Beta distribution via the \u201ccopula\u201d technique (Nelsen, 1999).", "startOffset": 105, "endOffset": 119}], "year": 2012, "abstractText": "Major advances in Question Answering technology were needed for IBM Watson to play Jeopardy! at championship level \u2013 the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates. In addition, Jeopardy! features four types of decision making carrying great strategic importance: (1) Daily Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., \u201cbuzz in.\u201d Using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a player\u2019s overall chances to win, when compared with simple \u201crule of thumb\u201d strategies. This article presents our approach to developing Watson\u2019s game-playing strategies, comprising development of a faithful simulation model, and then using learning and MonteCarlo methods within the simulator to optimize Watson\u2019s strategic decision-making. After giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulator\u2019s predictions, and documenting performance improvements using our methods. Quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes. We further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.", "creator": "gnuplot 4.2 patchlevel 2 "}}}