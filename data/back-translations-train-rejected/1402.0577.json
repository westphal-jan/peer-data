{"id": "1402.0577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "A Survey on Latent Tree Models and Applications", "abstract": "In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.", "histories": [["v1", "Tue, 4 Feb 2014 01:40:28 GMT  (1125kb)", "http://arxiv.org/abs/1402.0577v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rapha\\\"el mourad", "christine sinoquet", "nevin l zhang", "tengfei liu", "philippe leray"], "accepted": false, "id": "1402.0577"}, "pdf": {"name": "1402.0577.pdf", "metadata": {"source": "CRF", "title": "A Survey on Latent Tree Models and Applications", "authors": ["Rapha\u00ebl Mourad", "Christine Sinoquet", "Nevin L. Zhang", "Tengfei Liu", "Philippe Leray"], "emails": ["raphael.mourad@aliceadsl.fr", "christine.sinoquet@univ-nantes.fr", "lzhang@cse.ust.hk", "liutf@cse.ust.hk", "philippe.leray@univ-nantes.fr"], "sections": [{"heading": "1. Introduction", "text": "In statistics, latent variables (LVs), as opposed to observed variables (OVs), are random variables that are not directly measured. A wide range of statistical models, so-called latent variable models, link a set of OVs to a set of LVs. In these models, LVs explain interdependencies between OVs and therefore provide compact and comprehensible insights into data. In addition, LVs allow the reduction of data dimensions and generation of conditionally independent variables, which greatly simplifies downstream analysis. Applications are numerous and cover many scientific fields, typically in areas such as psychology, sociology, economics, but also biology and artificial intelligence, to name but a few examples. Such areas may require complex constructs that cannot be directly observed. For example, the human personality in psychology and social class in socioeconomics refers to higher abstractions than the reality observed by the Foundation."}, {"heading": "1.1 Context", "text": "The latent tree model (LTM) 1 is a class of latently variable models that has received considerable attention. LTM is a probabilistic tree-structured graphical model in which leaf nodes are observed, while internal nodes can either be observed or diagnosed latently. This model is attractive because its simple structure - a tree - allows simple and efficient inferences, while its latent variables capture complex relationships. A subclass of LTMs was first developed in the phylogenetic community (Felsenstein, 2003). In this context, leaf nodes are observed taxa, while internal nodes represent unobserved taxum ancestors."}, {"heading": "1.2 Contributions", "text": "In this paper, we present a comprehensive study of LTM and a broad view of its recent theoretical and methodological developments. LTM must be noted because it (i) provides deep insights into latent structural discovery (Saitou & Nei, 1987), (ii) is applicable to multidimensional clustering (Chen, Zhang, Liu, Poon, & Wang, 2012), and (iii) allows efficient probabilistic conclusions (Wang, Zhang, & Chen, 2008). Somewhat surprisingly, no comprehensive review of this research area has been published. In addition to reviewing the LTM research area, we also contribute an analysis and perspective that will advance our understanding of the topic. We present a categorization of learning methods: We present generic learning algorithms that implement basic principles, which differ in part from those of the literature because they have been adapted to a broader context."}, {"heading": "1.3 Paper Organization", "text": "Section 2 presents the latent tree model and related theoretical developments. Section 3 examines methods developed to learn latent tree models for the two main situations: learning when structure is known and learning when it is not. Section 4 then presents and describes in detail three types of applications of latent tree models: latent structure discovery, multidimensional clustering and probabilistic conclusions. Other applications such as classification are also discussed. Finally, in the last two sections 5 and 6 conclusions are drawn and future directions indicated."}, {"heading": "2. Theory", "text": "In this section, we first introduce graph terminology and then present LTM. Next, we present latent classes and probable conclusions for clustering. Scoring LTMs are discussed. We also present the concepts of marginal equivalence, equivalence, and model economy that are useful for LTM learning. Then, we explain the need for a trade-off between latent variable complexity and partial structural complexity. Variables are referred to by uppercase letters, e.g. A, B, and C, while lowercase letters refer to values that can assume variables, e.g. a, b, and c. Bold face letters represent groups of objects, i.e. A, B, and C are groups of variables, while a, b, and c are value sets. An observed variable is referred to as X, while a latent variable is referred to as H. A variable that we do not know whether it is observed or latently referred to as V."}, {"heading": "2.1 Graph Theory Terminology", "text": "Before introducing LTM, we must first define graph-related terms illustrated in Figure 1. A graph G (V, E) consists of a series of nodes V and a series of edges E, V, and V. An edge is a pair of nodes (Va, Vb). The edge is undirected (notes Va \u2212 Vb) when (Vb, Va), and directed (notes Va \u2192 Vb) when not. A directed graph is a graph whose edges are all directed. In a directed graph, a node Va is a parent of a node Vb when and only when there is an edge from Va to Vb. The node Vb is then referred to as a child of the node Va. A node are siblings when they share the same edge. A node Vc is a root if it has no parent node. A directed path from a node Vd is simply a sequence of a node Ved."}, {"heading": "2.2 Latent Tree Model", "text": "LTM is a tree-structured graphical model with latent variables. It consists of a tree - the structure - T (V, E) and a series of parameters, \u03b8. The tree can be either directed (i.e. a Bayesian network; Zhang, 2004) or undirected (i.e. a Markov random field; Choi, Tan, Anandkumar, andWillsky, 2011) Both representations are described in Figure 2. The set of nodes V = {V1,..., Vn + m} represents n + m observed and latent variables (i.e. a Markov random field; Choi, Tan, Anandkumar, andWillsky, 2011). XP is the set of observed variables and H = {H1, Hm} is the set of latent variables. Leaf nodes are OVs, while internal nodes can either be observed or latently observed. Variables can be either discrete or continuous."}, {"heading": "2.3 Latent Classes and Clustering", "text": "However, all the latent classes together form a soft partition of the data and define a finite mixing model (FMM). LTM can be considered as several FMMs connected together to form a tree. In view of a data point, the probability of belonging to a particular class can be calculated using the Bayes formula, which is called class mapping. In an LTM, each LV Hj represents a partition of the data. To observe 'and the vector of its values x' = {x '1,..., x'n} based on the set of OVs X, the probability of belonging to a class c of an LV Hj can be calculated as follows: P (Hj = c | x') = P (x 'Hj = c) P (Hj = c), x' s, x's, x. \""}, {"heading": "2.4 Scoring Latent Tree Models", "text": "In theory, any score, such as the Akaike information criterion (AIC) (Akaike, 1970) and the Bayesian information criterion (BIC) (Schwartz, 1978), could be used to evaluate LTMs. In practice, the BIC score is often used for LTMs. Consider a set of n OVs X = {X1,..., Xn} and a collection of N identical and independently distributed (i.i.d.) observations Dx = {x1,..., xN}. BIC consists of two terms: BIC (T, Dx) = log P (Dx | \u03b8ML, T) \u2212 12 dim (T) log N, (6) with prevML the maximum probability parameters, dim (T) the model dimension and N the number of observations. The first term evaluates the adaptation of the model to the data."}, {"heading": "2.5 Model Parsimony", "text": "Consider two LTMs, M = (T, \u03b8) and M \u2032 = (T \u2032, \u03b8 \u2032), which are built on the same set of n OVs, X = {X1,..., Xn}. We say that M and M \u2032 are marginally equivalent if their common distributions on OVs are equal: P (X1,..., Xn | T, \u03b8) = P (X1,..., Xn | T,). (7) If two marginally equivalent models have the same dimension, they are equivalent models.A model M is parsimonious3 if there is no other model M that is marginally equivalent and has a smaller dimension. A parsimonious model has the best possible score. It does not contain redundant LVs or redundant latent classes. It represents the model derived from the data. Two conditions ensure that an LTM does not include redundant LVs (Pearl, 1988)."}, {"heading": "2.6 Trade-off between Latent Variable Complexity and Partial Structure Complexity", "text": "Zhang and Kocka (2004b) differentiated between two types of model complexity in LTM: latent variable complexity refers to LV cardinalities, while sub-structure complexity is 4. In their work, Zhang and Kocka (2004b) call it structural complexity. For better understanding, we prefer to distinguish between (complete) structures that include LV cardinalities and sub-structures that disregard the edges and number of LVs in the diagram. Balance between these two complexities plays an important role when choosing a model. This conflict of objectives is illustrated in Figure 3. Consider, for example, a latent class model (i.e. a model with only one LV, abbreviated LCM) versus an LTM with the same probability of limitation (slightly equivalent models). LCM is the model that exhibits the highest LV complexity and the least substructure complexity."}, {"heading": "3. Statistical Learning", "text": "In this section, we present generic algorithms that implement basic principles for learning LTMs. These algorithms differ in part from those proposed in the literature because they have been adapted to a broader context. In addition, in this survey, we offer a unified representation of algorithms. When learning a model from data, two main situations must be distinguished: when structure is known and only parameters need to be learned, and the more complicated situation when both are unknown."}, {"heading": "3.1 Known Structure", "text": "In the simplest situation, the structure is known, i.e. not only the dependencies between variables, but also the number of LVs and their respective cardinalities (i.e. the number of latent classes).The problem is the estimation of probability parameters. To solve the problem, one can apply Expectation Maximization (EM), the most popular algorithm for determining learning parameters in the face of LVs (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995).Since EM results in a computational load on large LTMs, a more efficient method, which we call LCM-based EM, can be appliced.Other methods that differ from EM, such as spectral techniques, have also been developed."}, {"heading": "3.1.1 Expectation-maximization", "text": "Ideally, when learning parameters, we would like to maximize the log probability for a number of N i.i.d. observed data Dx = {x1,..., xN}: L (\u03b8; Dx) = log P (Dx | \u03b8) = log \u2211 HP (Dx, H | \u03b8). (9) However, to overcome the difficulty, EM implements an iterative approach. Instead, with each iteration, it optimizes the following expected protocol probability, which depends on the current parameters: Q (raked) = EDh | Dx, instead of [log P (Dx, Dh | \u03b8)] (10), where Dx is derived from the missing data Dh = {h1,..., hN}. Note that by completing the CEM, the missing data can easily be completed with the observed parameters MB-LEM (partial LEM-1) and LEM-M parameters."}, {"heading": "INPUT:", "text": "T, the tree structure of the LTM."}, {"heading": "OUTPUT:", "text": "It is very likely that many of the parameters of the LTM.1: T \u2032 -Diagramms rooting (T) / * select an LV as root of T * / 2: Ho \u2190 \u2205 / * initialization of the set of attributed latent variables * / 3: \u03b8. \"4: loop 5: TLCM = {TLCM1,..., TLCMk}. Identify LCMs of the Diagramms (T,\" Ho). 6: \"LCM = {LCM1,\".... LCMk. \"EM. (TLCM).7: if | TLCM | 1 then 8:\".D \".D\".D \".D\".D \".D\".D \".D\" D \".D\".D \"D\".D \".D\" D \".D\" D \".D\" D \".D\" D \".D\" D \".D\" D \".D\" D \".D\" D \".D\".D \"D\".D \".D\" D \".D\".D \"D\".D \".D\" D \".D\".D \"D\".D \".D\" D \".D\".D \"D\".D \".D\" D \".D\" D \".D\".D \".D\".D \"D\".D \".D\".D \".D\".D \".D\".D \".D\" D \".D\".D \".D\".D \".D\" D \".D\".D \"D\".D \".D\".D \".D\".D \".D\" D \".D\".D \".D\".D \".D\".D \"D\".D \".D\".D \".D\".D \".D\" D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\""}, {"heading": "3.1.2 LCM-based EM", "text": "One solution to speed up EM calculations is to concatenate two steps: a first step of the dividend and conquest strategy through local - LCM - learning, followed by a last step of implementing global learning processes. This LCM-based learning method is presented in Algorithm 1 and illustrated in Figure 4. In the first step, the parameters are explained locally by a bottom-up LCM-based learning method. Firstly, a LV is very similar to the learning method proposed for binary trees by Harmeling and Williams."}, {"heading": "3.1.3 Spectral Methods", "text": "The method directly estimates the common distribution of OVs without explicitly restoring the6. We remember that this process facilitates latent data imputation.LTM parameters are useful when LTM parameters are not required, for example, to draw probable conclusions about OVs only. The work of Parikh et al. facilitates the limitation of the approach of Mossel et al. (2006), which requires that all conditional probability tables should be immutable, and generalizes the method of Hsu et al. (2009) specifically for hidden Markov models.Parikh et al. (2011) reforms the message that passes algorithms using an algebraic formulation: P (x) = 1 P (vi | Pavi) = r > (Mj1My1MyJ1r... MyJ1r), (11) where x = J, a root, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message, a message."}, {"heading": "3.1.4 Other Methods", "text": "There are other methods for learning parameters: Gradient pedigree (Kwoh & Gillies, 1996; Binder, Koller, Russel, & Kanazawa, 1997) and variations of the Gauss-Newton method (Xu & Jordan, 1996) accelerate the sometimes slow convergence of EM, but require the evaluation of the first and / or second derivatives of the probability function."}, {"heading": "3.2 Unknown Structure", "text": "Unfortunately, most of the time there is no a priori information about the LTM structure. This forces us to learn every part of the model, i.e. the number of courses, their cardinalities, the dependencies and the parameters. This learning task is a demanding task, for which different methods have been designed. In this section we offer an overview of these algorithms. Also, the determination of the LV cardinalities, as well as the temporal complexity and scalability of algorithms are discussed. We conclude with the compilation of a summary of these learning methods. The approaches to structural learning are divided into three categories: the first consists of search-based methods inspired by the usual Bayesian network learning; the second is based on variable clustering and refers to hierarchical procedures; the last category is based on the notion of distances and derives from phylogenetics."}, {"heading": "3.2.1 Search-based Methods", "text": "Search-based methods aim to find the optimal model based on specific scoring metrics, but for BNs without LVs, the BIC score is often used. BIC suffers from a theoretical deficiency in relation to LTM, as shown in Section 2.4. Empirical results, however, suggest that this deficiency does not appear to affect model quality in practice (Zhang & Kocka, 2004a). Therefore, researchers still use BIC when it comes to learning LTM. Many search methods have been suggested, all of which examine the space of regular LTMs. Here, we focus on: (i) the most naive, which is conceptually simple but very computationally intensive, and (ii) the most advanced, which reduces search space and implements fast parameter learning through local EM."}, {"heading": "Naive Greedy Search", "text": "Naive greedy search (NGS) is to start from an LCM and then visit the space of regular LTM substructures; the neighborhood of the current model is explored by greedy search by operations such as adding or removing a latent node and node displacement7. For each substructure neighbor, the cardinalities of all LVs are optimized by adding or dismissing a state relative to an LV. During the model search (substructure and LV cardinality), candidate models are learned with EM and evaluated on the basis of a score. If the best candidate model shows a score superior to the current model score, then the former is used as seed for the next step. Otherwise, NGS stops and the current model is considered the best model. Therefore, at each step of the search, the learning approach must evaluate the score of a very large number of candidate models, resulting in an enormous computational burden, as the probability must be calculated for each candidate model."}, {"heading": "Advanced Greedy Search", "text": "Advanced Greedy Search (AGS) relies on three strategies to reduce complexity: Advanced Greedy Search is presented in Algorithm 2. Firstly, AGS focuses on a smaller space of models to be explored than NGS (Zhang & Kocka, 2004b), which simultaneously performs a partial structural search and explores the cardinality of the LV. To this end, two additional operators are used: addition and removal of a latent state for an LV. Secondly, AGS pursues a growth restructuring strategy to reduce the complexity of the search space again (Chen, Zhang, & Wang, 2008; Chen et al., 2012). The strategy consists of dividing the five operators into three groups. Each group is applied in a specific step of the model search. Firstly, latent nodes and latent state introduction (NI and SI, respectively) are used to make the current model more complex (3)."}, {"heading": "Operation Granularity", "text": "Starting with the simplest solution (an LCM), Zhang and Kocka (2004b) found that the comparison of the BIC values between the candidate model T and the current T may not be a relevant criterion. The problem is that this strategy always leads to increasing the cardinality of the LCM without introducing LVs into the model (see trade-off between LV complexity and partial structural complexity in Section 2.6). Instead, to address this problem, they propose to evaluate the so-called improvement rate during the growth step: IRBIC (T \u2032, T | DX) = BIC (T \u2032, DX) \u2212 BIC (T \u2032, DX) \u2212 dim (T), (14), which is the difference of the BIC values between the candidate model T \u00b2 and the current model T divided by the difference of their respective dimensions."}, {"heading": "3.2.2 Methods Based on Variable Clustering", "text": "The main disadvantage of search-based methods is that assessing the maximum probability in the presence of vans, as well as the large amount of space to explore by local search, is still time consuming. Variable clustering approaches are efficient and much faster alternatives, all based on two key points: grouping variables to identify vans and constructing a model through a bottom-up strategy. Depending on the structures learned, three main categories have been developed: binary trees, non-binary trees and8. Note that node relocation is used locally after each NI to increase the number of its children.Algorithm 2 Advanced greedy search for LTM learning (AGS, adapted by EAST, Chen et al., 2012)"}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1,..., Xn}."}, {"heading": "OUTPUT:", "text": "T and \u03b8, respectively, the tree structure and the parameters of the LTM construction d.1: (T 0, \u03b80) \u2190 latent class model (X) / * LCM learning with EM * / 2: loop for i = 0, 1,... to convergence 3: (T i \u2032, \u03b8i \u2032) \u2190 Local search (NI-SI, T i, \u03b8i) / * grow * / 4: (T i \u2032, \u03b8i \u2032) \u2190 Local search (NR, T i \u2032, \u03b8i \u2032) / * Restructuring * / 5: (T i + 1, \u03b8i + 1) \u2190 Local search (ND-SD, T i \u2032, successi \u2032) / * thin * / 6: Endloop 7: 8: / * Description of the function Local search (operators, T-I-II-II-II) * / 9: (T-i + 1, successi + 1) \u2190 Local search (T-II-II) \u2190 (T-II-II-II): verj-IS for T1-II, T-II-II-T = T-11-II-II, T-II-II-Z-II-II-II, T-T-II-II-II-II-II-T-II, T-Z-II-II-II-T-II-II, T-I-II-II-II-T-Z-II, T-I-II-II-II-T-II, T-Z-II-II-I-II, T-II-II-I-I-II, T-II-II-I-I-II, T-II-I-II, T-II"}, {"heading": "Binary Trees", "text": "For example, you can easily calculate an agglomerative hierarchical clustering (Xu & Wunsch, 2005) to learn the LTM structure, an algorithm called an agglomerative hierarchical clustering solution (AHCB), which defines the MI between two variables Vi and Vj as follows: I (Vi; Vj) = Vi (Vi) = Vi (Vj) vj (Vj) vj (vi, vj) p (vj) p (15) Single, complete or average linkage is used, depending on the cluster compactness required."}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1,..., Xn}."}, {"heading": "OUTPUT:", "text": "T (V, E) or the tree structure and the parameters of the LTM construction (W \u2190 X / * Variable set initialization * / 2: T (V, E) \u2190 Empty tree (W) / * Tree on W without edges * / 3: Supercharged LV data (lcm) 8: W \u2190 4: Loop 5: {Wi, Wj} \u2190 Pair with highest MI (W) 6: lcm \u2190 latent class model ({Wi, Wj}) 7: H \u2190 Supercharged LV data (lcm) 8: W \u2190 W\\ {Wi, Wj} \u0442 H / * Remove children and add the supercharged parents * / 9: E \u2190 E-edges (lcm); V \u00b2 V \u00b2 H10: Supercharged child parameters (lcm) 11: if | W | = 2 then 12: E \u00b2 edge between the two remaining nodes (W), P \u00b2 -selective E \u00b2 edges (lcm)."}, {"heading": "Non-binary Trees", "text": "Although binary tree modeling works well in practice (Harmeling & Williams, 2011), it would be worth loosening the binary constraint; in fact, it could provide a better model for fidelity and interpretation because fewer LVs would be required; there are several ways to learn non-binary trees without incurring too many additional computational costs; for example, Wang et al. (2008) first learns a binary tree, then each pair of adjacent LVs is removed from the tree, and the remaining node is connected to each child of the distant node. Although Wang et al. \"s approach is stringent, in practice it can lead to finding trees that are very close to binary trees; another solution is to identify cliques of pairs of dependent variables in order to detect the presence of cabilis of the distant node."}, {"heading": "Flat Trees", "text": "The Bridged Island (BI) algorithm by Liu et al. (2012) takes a slightly different approach, first dividing the set of all observed variables into subsets called sibling clusters, then creating an LCM for each sibling cluster by introducing a latent variable and optimizing its cardinality and model parameters, then implementing the values of the latent variables and linking the latent variables to a tree structure using Chow-Liu's algorithm. The EM algorithm is executed once at the end to optimize the parameters of the entire model. To emphasize the difference between BI and the other variable cluster algorithms, we call the models it creates flat LTMs. Determining the sibling clusters is the key step in BI. BI determines the sibling clusters individually. To determine the first sibling cluster, it starts with the other pair of clusters that is already extended with the most variables."}, {"heading": "Forests", "text": "If the number of variables to be analyzed is very large (e.g. 1000 variables), it might be more useful to learn a forest instead of a tree, since many variables may not be significantly interdependent (see Figure 6). We call this model the \"latent forest model\" (LFM). It has many advantages over LTM, such as reducing the complexity of the likely conclusion (which depends on the number of edges). To learn LFM, there are several approaches. For example, in AHCB you can use a cluster validation criterion to decide where to cut the hierarchy. With respect to LCMB-LTM (algorithm 3), there are two options. On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality is equal to 1, this means that the LV is not useful for the algorithm in the structure and maintains the algorithm."}, {"heading": "3.2.3 Distance-based Methods", "text": "This method class was originally developed for phylogenetics (Felsenstein, 2003). A phylogenetic tree is a binary LTM that shows the evolutionary relationships between a set of taxa. Compared to other LTM learning methods, distance-based methods provide strong guarantees for the conclusion of the optimal model. In this section, we first define distances and then introduce learning algorithms: Neighbor Joining, distance-based method dedicated to general LTM learning, and newer spectral methods."}, {"heading": "Distances between Variables", "text": "Distances are limited to LTMs whose all variables share the same state space X, e.g. binary variables (Lake, 1994). Distances are functions of paired distributions. For a discrete tree model G (V, E) (e.g. an LTM), the distance between two variables Vi and Vj is: dij = \u2212 log | det (Jij) | \u221a det (Mi) det (Mj), (18) with Jij the common probability matrix between Vi and Vj (i.e. J ij ab = p (Vi = a, Vj = b), a, b \u00b2 X), and Mi the diagonal marginal probability matrix of Vi (i.e. M iaa = p (Vi = a)). For a special case of discrete tree models called symmetric tree models (Choi et al), the distance has a simpler form."}, {"heading": "Neighbor Joining", "text": "The principle of joining a neighbor (NJ) is quite simple (Saitou & Nei, 1987; Gascuel & Steel, 2006).NJ starts with a star-shaped tree, then iteratively selects two taxa i and j, and creates a new taxum u to connect them together.Selecting a pair tries to optimize the following Q criterion: Q (i, j) = (n \u2212 2) dij \u2212 n dij \u2212 k = 1djk, (21) where n is the number of taxa and dij is the additive tree distance between i and j.The distance between i and the new taxum u is estimated as follows: diu = 12 dij + 12 (n \u2212 2) (n \u00b2 k = 1djk, k = 1djk), (22) and dju is calculated using symmetry."}, {"heading": "Learning Dedicated to General LTM", "text": "In this subsection, we present the latest developments by Choi et al. (2011) for general LTM learning, i.e. learning that is not limited to phylogenetic trees. It is limited to analyzing data whose variables share the same state space, e.g. binary data. Another advantage is that OVs are not necessarily forced to be blank associations (this is seen in the next paragraph). Distance-based general LTM learning (DBG) is implemented in Choi 4, detailed as follows. First, the working node set W is computed with the set of observed variables X = {X1,..., Xn} (line 1). Distances are calculated for all three variables in W."}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1,..., Xn}."}, {"heading": "OUTPUT:", "text": "It is only a matter of time before the LCM trees (D, S, R) that lean on the LCM trees (D, S, R), the LCM trees (D, S, R) that lean on the LCM trees (D, W) that lean on the LCM trees (D, W) that lean on the LCM trees (D, E) that lean on the LCM trees (D, E) that lean on the LCM trees (D, E) that lean on the LCM trees (D, E) that lean on the LCM trees (D, E) that lean on the LCM trees (D, E) that lean on the LCM trees (D) that lean on the LCM trees (D) that lean on the LCM trees (D)."}, {"heading": "Spectral Methods", "text": "On the one hand, their approach can deal with general linear models that contain both categorical and continuous variables; on the other hand, another important improvement of their method is to replace the step in line 5 of algorithm 4 with a quartet of test14 based on spectral techniques (more specific canonical correlation analysis); and the proposed extension is to replace the step in line 5 of algorithm 4 with a test based on spectral techniques (Hair, Black, Babin and Anderson, 2009). Given four observed variables {X1, X2, X3, X4}, the spectral quartet distinguishes between the four possible tree topologies (see Figure 8)."}, {"heading": "3.2.4 Determination of Latent Variable Cardinalities", "text": "In fact, most of them are able to survive on their own, without being held accountable."}, {"heading": "3.2.5 Choosing a Root", "text": "For example, LCM-based parameter learning (see algorithm 1) can be easily done when selecting a root, which can be determined by an a priori knowledge of data. Thus, for example, we can assume that the latent structure of LTM represents a hierarchy of concepts (i.e. a taxonomy in ontology), so that the LV root corresponds to the highest abstract level, whereas an LV node, which has only OVs as children, is interpreted as the lowest abstract level. Indeed, variable cluster-based algorithms imply this a priori knowledge."}, {"heading": "3.2.6 Time Complexity and Scalability", "text": "The complexity of the generic LTM learning algorithms is summarized in Table 1. In the table, we compare algorithms, approaches, models and time complexities. We also give examples of instances of generic algorithms. Online resources are also summarized in Appendix A. However, to simplify the comparison of time complexities, we consider only the number of variables (input data), the number of observations and the number of steps (for search-based algorithms). However, the LTM learning algorithms are compared with the Chow-Liu algorithm for learning a tree without LVs. Details on calculating the complexity of LTM learning algorithms are provided in supplementary material. If the tree does not contain LV, learning of the model can be done efficiently in O (n2N) using the algorithm. The situation is more complicated if the tree contains LTM learning algorithms."}, {"heading": "3.2.7 Summary", "text": "Nevertheless, EM leads to considerable runtimes and local maximums for large LTMs. To solve this problem, LCM-based EM enables fast learning of parameters, while spectral methods help to find the optimal solution when no LTM parameters are required. If the structure is unknown, search-based approaches are standard Bayesian network learning methods. To address this problem, variable cluster methods are efficient alternatives based on the idea of grouping variables to identify LVs in a bottom-up manner. Recently, phylogenetic algorithms have been adapted to general LTM learning and, compared to other methods, they guarantee that the generative LTM structure is accurately restored under certain conditions."}, {"heading": "4. Applications", "text": "In this section, we will discuss and illustrate three types of applications of LTMs: latent structure discovery, multidimensional clustering, and probabilistic inference. At the end of the section, we will also briefly present other applications such as classification."}, {"heading": "4.1 Latent Structure Discovery", "text": "Latent structural discovery aims to uncover: (i) latent information underlying the data, i.e. non-observable variables or abstract concepts that play a role in data analysis, and (ii) latent relationships, i.e. relationships that exist between observed and latent information, and also between parts of the latent information itself. To this end, LTM analysis is a powerful tool for modelling latent information and latent relationships through LVs and graph edges. Thanks to LTMs, latent structural discovery has been applied to several fields: marketing (Zhang, Wang, & Chen, 2008), medicine (Zhang, Nielsen, & Jensen, 2004; Zhang et al.), genetics et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Ninio, Pe'er, & Pupko, 2002."}, {"heading": "4.2 Multidimensional Clustering", "text": "It is only a matter of time before such a development will occur. (...) It is only a matter of time before such a development will occur. (...) It is only a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur. \"(...) It is a matter of time before such a development will occur. (...) It is a matter of time before such a development will occur.\""}, {"heading": "4.3 Probabilistic Inference", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "4.4 Other Applications", "text": "This model assumes that OVs are independent of the class variable, an assumption that is often violated by data, and therefore numerous adjustments have been developed to improve the performance of the classifier. Naive Bayes have been generalized by introducing latent nodes as internal discrete nodes (Zhang et al., 2004) or continuous nodes (Langseth & Nielsen, 2009), which mediate the relationship between leaves and the class variable; the model is identical to an LTM except that the root is observed. Recently, Wang et al. (2011) proposed a classifier based on LTM. For each class, a specific LTM is learned and a latent tree classifier becomes interactive."}, {"heading": "5. Discussion", "text": "In data analysis, LTM is an emerging popular topic because it offers several advantages: - The model enables the discovery of an interpretable latent structure. - Each latent variable should provide a way to express categorical data on clusters, and connections between latent variables should express relationships between the different clusters. - Multiple latent variables organized in a tree greatly improve the flexibility of probabilistic modeling while ensuring linear - i.e. fast - probabilistic conclusions. Table 6 summarizes the applications of LTMs, which combine three types of applications with details, examples, references to generic algorithms, scalability to large datasets, software, and bibliographic references. Extensive research has been undertaken in LTM learning over the past decade. If the structure is known, standard EM and LCM-based EM or spectral methods can be used."}, {"heading": "6. Future Directions", "text": "There are several promising approaches that are being observed, such as a recent paper that LTM has developed for continuous data analysis (Poon, Zhang, Chen, & Wang, 2010; Choi et al., 2011); other authors have examined the relationships between LTM and ontology (Hwang et al., 2006); and LTM-based dependencies between the different types of dependencies in terms of the way they evolve (Mourad, Sinoquet, Dina, & Leray, 2011); although no research has been done on the application of causal discovery and latent trait analysis, we argue that LTM could represent interesting avenues of research. LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon al, 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank four anonymous reviewers for their valuable comments and their help in improving the manuscript. This work was supported by the BIL Bioinformatics Research Project of the Pays de la Loire region, France. Also grateful are the authors Carsten Poulsen (Aalborg University, Denmark) for providing the Danish beer data, Yi Wang (National University of Singapore) for the LTAB algorithm, Tao Chen (EMC Corporation, Beijing, China) for the EAST algorithm, Stefan Harmeling (Max Planck Institute, Germany) for the BIN-A and BIN-G algorithms, and Myung Jin Choi (Two Sigma Investments, USA) and Vincent Tan (University of Wisconsin-Madison, USA) for the RG, CLRG and regCLRG algorithms."}, {"heading": "Appendix A. Online Resources Mentioned", "text": "Software: \u2212 BIN-A, BIN-G, CL and LCM: http: / / people.kyb.tuebingen.mpg.de / harmeling / code / ltt-1.4.tar \u2212 CFHLC: https: / / sites.google.com / site / raphaelmouradeng / home / programs \u2212 DHC, SHC and HSHC: http: / / www.cse.ust.hk / faculty / lzhang / ltm / index.htm (hlcm-distribute.zip) http: / / www.cse.ust.hk / faculty / lzhang / ltm / index.htm (toolBox.zip) \u2212 EAST: http: / / www.cse.ust.hk / faculty / lzhang / index.htm (EAST.zip) \u2212 Laterne: http: / / www.cse.ust.home CLul.tuk / indebty / index.index.zip (index.c.index.c.c.index.index.zip / index.c.index.c.k.index.k.index.k.index.zip / index.k.index.c.c...tty / index.k.index.zip /.index.c.k.index.c..index.zip /.index.k.t.index.zip /.k.index.k.index.k.....tk: http: / / /.k.tu.index.index.....c............tk / index.tk / index.zip /.tr / index.tr /.k.tm"}, {"heading": "Appendix B. Supplemental Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Experiments on Parameter Learning with EM", "text": "We investigated the number of random reboots required in practice to obtain the convergence of EM (Section 3.1.1) and LCMB-EM (Section 3.1.2) to the optimal solution. BIC values are shown in Figure 13. For EM, we used the method of Chickering and Heckerman (1997), which is implemented in the software LTAB (Wang et al., 2008). We analyzed three sets of data: two small (BinTree and Asia) and one large (Tree). For the first two sets of data, convergence was achieved after 20 and 2000 reboots, respectively. For the large set, convergence is never achieved, even after 5000 reboots. For LCMB-EM, we used the software BIN-A (Harmeling & Williams, 2011). We analyzed three sets of data: one small (Asia) and two large sets (Tree, Alarm). Convergence is achieved for the first two sets with only one parameter initialization, while we never achieve more than one solution for the largest set of data, because the unstable convergence is achieved later on."}, {"heading": "B.2 Time Complexity", "text": "We remember that n is the number of variables (input data), N is the number of observations, and s is the number of steps (for search-based algorithms).The temporal complexities of the generic algorithms for LTM learning are detailed as follows: O (s) steps required for the convergence of search-based methods. O (s) steps required for each step are O (s) steps learned for each model. O (s) steps learned for each model are O (s) steps optimized for each LV so that O (n2) new models are generated (Zhang, 2004).For each model, parameters are learned using EM that are isachieved in O (nN)."}, {"heading": "B.3 Description of Datasets Used for Literature Algorithm Comparison", "text": "The data comes from the work of Harmeling and Williams (2011). The complexity of agglomerative hierarchical clustering consists of O (n2N) using the single linkage criterion. The complexity is higher for other criteria. \u2212 BinForest. Datasets composed of a binary forest consist of two trees. One tree has 3 variables (2 leaves and 1 internal node), the other has 5 variables (3 leaves and 2 internal nodes). Only leaf data is used. Tension and test datasets consist of 500 observations. The model comes from the work of Harmeling and Williams (2011). \u2212 Data generated using the well-known Asian network 8 observations."}], "references": [{"title": "Human motion analysis: A review", "author": ["J. Aggarwal", "Q. Cai"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Aggarwal and Cai,? \\Q1999\\E", "shortCiteRegEx": "Aggarwal and Cai", "year": 1999}, {"title": "Statistical predictor identification", "author": ["H. Akaike"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Akaike,? \\Q1970\\E", "shortCiteRegEx": "Akaike", "year": 1970}, {"title": "Spectral methods for learning multivariate latent tree structure", "author": ["A. Anandkumar", "K. Chaudhuri", "D. Hsu", "S.M. Kakade", "L. Song", "T. Zhang"], "venue": "In Twenty-Fifth Conference in Neural Information Processing Systems (NIPS-11)", "citeRegEx": "Anandkumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2011}, {"title": "The performance of neighbor-joining methods of phylogenetic", "author": ["K. Atteson"], "venue": "reconstruction. Algorithmica,", "citeRegEx": "Atteson,? \\Q1999\\E", "shortCiteRegEx": "Atteson", "year": 1999}, {"title": "Inferring parameters and structure of latent variable models by variational Bayes", "author": ["H. Attias"], "venue": "In Proceedings of the 15th Conference on Uncertainty and Artificial Intelligence", "citeRegEx": "Attias,? \\Q1999\\E", "shortCiteRegEx": "Attias", "year": 1999}, {"title": "Haploview: analysis and visualization of LD and haplotype", "author": ["J.C. Barrett", "B. Fry", "J. Maller", "M.J. Daly"], "venue": "maps. Bioinformatics,", "citeRegEx": "Barrett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2005}, {"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russel", "K. Kanazawa"], "venue": "Machine Learning,", "citeRegEx": "Binder et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Binder et al\\.", "year": 1997}, {"title": "Taxonomy with confidence", "author": ["J.A. Cavender"], "venue": "Mathematical Biosciences,", "citeRegEx": "Cavender,? \\Q1978\\E", "shortCiteRegEx": "Cavender", "year": 1978}, {"title": "Quartet-based learning of hierarchical latent class models: Discovery of shallow latent variables", "author": ["T. Chen", "N.L. Zhang"], "venue": "In Proceedings of 9th International Symposium on Artificial Intelligence and Mathematics", "citeRegEx": "Chen and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Chen and Zhang", "year": 2006}, {"title": "Search-based learning of latent tree models", "author": ["T. Chen"], "venue": "Ph.D. thesis,", "citeRegEx": "Chen,? \\Q2008\\E", "shortCiteRegEx": "Chen", "year": 2008}, {"title": "Model-based multidimensional clustering of categorical data", "author": ["T. Chen", "N.L. Zhang", "T. Liu", "K.M. Poon", "Y. Wang"], "venue": "Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Efficient model evaluation in the searchbased approach to latent structure discovery", "author": ["T. Chen", "N.L. Zhang", "Y. Wang"], "venue": "In Proceedings of the Fourth European Workshop on Probabilistic Graphical Models", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables", "author": ["D.M. Chickering", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Chickering and Heckerman,? \\Q1997\\E", "shortCiteRegEx": "Chickering and Heckerman", "year": 1997}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow and Liu,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu", "year": 1968}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "Cooper,? \\Q1990\\E", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Evolutionary trees and the Ising model on the Bethe lattice: A proof of Steel\u2019s conjecture", "author": ["C. Daskalakis", "E. Mossel", "S. Roch"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Daskalakis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learning the dimensionality of hidden variables", "author": ["G. Elidan", "N. Friedman"], "venue": "In Proceedings of the 17th Conference on Uncertainty and Artificial Intelligence", "citeRegEx": "Elidan and Friedman,? \\Q2001\\E", "shortCiteRegEx": "Elidan and Friedman", "year": 2001}, {"title": "A few logs suffice to build (almost) all trees: Part II", "author": ["P.L. Erdos", "L.A. Szekely", "M.A. Steel", "T.J. Warnow"], "venue": "Theoretical Computer Science,", "citeRegEx": "Erdos et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Erdos et al\\.", "year": 1999}, {"title": "Constructing Bayesian networks to predict uncollectible telecommunications accounts", "author": ["K.J. Ezawa", "S.W. Norton"], "venue": "IEEE Expert,", "citeRegEx": "Ezawa and Norton,? \\Q1996\\E", "shortCiteRegEx": "Ezawa and Norton", "year": 1996}, {"title": "A probability model for inferring evolutionary trees", "author": ["J.S. Farris"], "venue": "Systematic Zoology,", "citeRegEx": "Farris,? \\Q1973\\E", "shortCiteRegEx": "Farris", "year": 1973}, {"title": "Inferring phylogenies (2 edition)", "author": ["J. Felsenstein"], "venue": "Sinauer Associates", "citeRegEx": "Felsenstein,? \\Q2003\\E", "shortCiteRegEx": "Felsenstein", "year": 2003}, {"title": "A structural EM algorithm for phylogenetic inference", "author": ["N. Friedman", "M. Ninio", "I. Pe\u2019er", "T. Pupko"], "venue": "Journal of Computational Biology,", "citeRegEx": "Friedman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2002}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Asymptotic model selection for directed networks with hidden variables", "author": ["D. Geiger", "D. Heckerman", "C. Meek"], "venue": "In Proceedings of Twelfth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Geiger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 1996}, {"title": "Multivariate data analysis (7 edition)", "author": ["J.F. Hair", "W.C. Black", "B.J. Babin", "R.E. Anderson"], "venue": null, "citeRegEx": "Hair et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hair et al\\.", "year": 2009}, {"title": "Greedy learning of binary latent trees", "author": ["S. Harmeling", "C.K.I. Williams"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Harmeling and Williams,? \\Q2011\\E", "shortCiteRegEx": "Harmeling and Williams", "year": 2011}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In The 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Learning hierarchical Bayesian networks for large-scale data analysis", "author": ["Hwang", "K.-B", "Kim", "B.-H", "Zhang", "B.-T"], "venue": "In International Conference on Neural Information Processing", "citeRegEx": "Hwang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2006}, {"title": "A computation model for causal and diagnostic reasoning in inference systems", "author": ["J.H. Kim", "J. Pearl"], "venue": "In Proceedings of the 8th International Joint Conference on Artificial Intelligence", "citeRegEx": "Kim and Pearl,? \\Q1983\\E", "shortCiteRegEx": "Kim and Pearl", "year": 1983}, {"title": "GERBIL: Genotype resolution and block identification using likelihood", "author": ["G. Kimmel", "R. Shamir"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Kimmel and Shamir,? \\Q2005\\E", "shortCiteRegEx": "Kimmel and Shamir", "year": 2005}, {"title": "Latent tree copulas", "author": ["S. Kirshner"], "venue": "In Proceedings of the Sixth European Workshop on Probabilistic Graphical Models (PGM-12)", "citeRegEx": "Kirshner,? \\Q2012\\E", "shortCiteRegEx": "Kirshner", "year": 2012}, {"title": "Probabilistic graphical models: Principles and techniques (adaptive computation and machine learning)", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Information theory and statistical learning, chap. MIC: Mutual information based hierarchical clustering", "author": ["A. Kraskov", "P. Grassberger"], "venue": null, "citeRegEx": "Kraskov and Grassberger,? \\Q2009\\E", "shortCiteRegEx": "Kraskov and Grassberger", "year": 2009}, {"title": "Using hidden nodes in Bayesian networks", "author": ["Kwoh", "C.-K", "D.F. Gillies"], "venue": "Artificial Intelligence,", "citeRegEx": "Kwoh et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kwoh et al\\.", "year": 1996}, {"title": "Reconstructing evolutionary trees from DNA and protein sequences: Paralinear distances", "author": ["J.A. Lake"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Lake,? \\Q1994\\E", "shortCiteRegEx": "Lake", "year": 1994}, {"title": "Latent classification models for binary data", "author": ["H. Langseth", "T.D. Nielsen"], "venue": "Pattern Recognition,", "citeRegEx": "Langseth and Nielsen,? \\Q2009\\E", "shortCiteRegEx": "Langseth and Nielsen", "year": 2009}, {"title": "The EM algorithm for graphical association models with missing data", "author": ["S.L. Lauritzen"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Lauritzen,? \\Q1995\\E", "shortCiteRegEx": "Lauritzen", "year": 1995}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Forest density estimation", "author": ["H. Liu", "M. Xu", "H. Gu", "A. Gupta", "J. Lafferty", "L. Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "A novel LTM-based method for multidimensional clustering", "author": ["T.F. Liu", "N.L. Zhang", "A.H. Liu", "L.K.M. Poon"], "venue": "In Proceedings of the Sixth European Workshop on Probabilistic Graphical Models (PGM-12)", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Discrete factor analysis: Learning hidden variables in Bayesian network", "author": ["J. Martin", "K. Vanlehn"], "venue": "Tech. rep.,", "citeRegEx": "Martin and Vanlehn,? \\Q1995\\E", "shortCiteRegEx": "Martin and Vanlehn", "year": 1995}, {"title": "Using a Bayesian belief network to aid differential diagnosis of tropical bovine diseases", "author": ["I.J. McKendrick", "G. Gettinbya", "Y. Gua", "S.W.J. Reidb", "C.W. Revie"], "venue": "Preventive Veterinary Medicine,", "citeRegEx": "McKendrick et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McKendrick et al\\.", "year": 2000}, {"title": "Learning nonsingular phylogenies and hidden Markov models", "author": ["E. Mossel", "S. Roch"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Mossel and Roch,? \\Q2006\\E", "shortCiteRegEx": "Mossel and Roch", "year": 2006}, {"title": "Robust estimation of latent tree graphical models: Inferring hidden states with inexact parameters", "author": ["E. Mossel", "S. Roch", "A. Sly"], "venue": null, "citeRegEx": "Mossel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mossel et al\\.", "year": 2011}, {"title": "Visualization of pairwise and multilocus linkage disequilibrium structure using latent forests", "author": ["R. Mourad", "C. Sinoquet", "C. Dina", "P. Leray"], "venue": "PLoS ONE,", "citeRegEx": "Mourad et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2011}, {"title": "A hierarchical Bayesian network approach for linkage disequilibrium modeling and data-dimensionality reduction prior to genomewide association studies", "author": ["R. Mourad", "C. Sinoquet", "P. Leray"], "venue": "BMC Bioinformatics,", "citeRegEx": "Mourad et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2011}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A.P. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-2011)", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "Recognition of two-person interactions using a hierarchical Bayesian network", "author": ["S. Park", "J.K. Aggarwal"], "venue": "In The first ACM International Workshop on Video Surveillance", "citeRegEx": "Park and Aggarwal,? \\Q2003\\E", "shortCiteRegEx": "Park and Aggarwal", "year": 2003}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Parzen,? \\Q1962\\E", "shortCiteRegEx": "Parzen", "year": 1962}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Variable selection in modelbased clustering: To do or to facilitate", "author": ["L.K.M. Poon", "N.L. Zhang", "T. Chen", "Y. Wang"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-2010)", "citeRegEx": "Poon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "Shortest connection networks and some generalizations", "author": ["R.C. Prim"], "venue": "Bell System Technical Journal,", "citeRegEx": "Prim,? \\Q1957\\E", "shortCiteRegEx": "Prim", "year": 1957}, {"title": "Choosing models for cross-classifications", "author": ["A.E. Raftery"], "venue": "American Sociological Review,", "citeRegEx": "Raftery,? \\Q1986\\E", "shortCiteRegEx": "Raftery", "year": 1986}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Rosenblatt,? \\Q1956\\E", "shortCiteRegEx": "Rosenblatt", "year": 1956}, {"title": "The neighbor-joining method: A new method for reconstructing phylogenetic trees", "author": ["N. Saitou", "M. Nei"], "venue": "Molecular Biology and Evolution,", "citeRegEx": "Saitou and Nei,? \\Q1987\\E", "shortCiteRegEx": "Saitou and Nei", "year": 1987}, {"title": "Estimating the dimension of a model", "author": ["G. Schwartz"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwartz,? \\Q1978\\E", "shortCiteRegEx": "Schwartz", "year": 1978}, {"title": "Kernel embeddings of latent tree graphical models", "author": ["L. Song", "A. Parikh", "E. Xing"], "venue": "In Twenty-Fifth Conference in Neural Information Processing Systems (NIPS-11)", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Designing genome-wide association studies: sample size, power, imputation, and the choice of genotyping chip", "author": ["C.C. Spencer", "Z. Su", "P. Donnelly", "J. Marchini"], "venue": "PLoS Genetics,", "citeRegEx": "Spencer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spencer et al\\.", "year": 2009}, {"title": "My favourite conjecture. http://www.math.canterbury.ac.nz/m.steel/files/misc/conjecture.pdf", "author": ["M. Steel"], "venue": null, "citeRegEx": "Steel,? \\Q2001\\E", "shortCiteRegEx": "Steel", "year": 2001}, {"title": "The complexity of reconstructing trees from qualitative characters and subtrees", "author": ["M. Steel"], "venue": "Journal of Classification,", "citeRegEx": "Steel,? \\Q1992\\E", "shortCiteRegEx": "Steel", "year": 1992}, {"title": "Learning high-dimensional Markov forest distributions: Analysis of error rates", "author": ["V.Y.F. Tan", "A. Anandkumar", "A. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2011}, {"title": "Latent tree classifier. In European Conferences on Symbolic and Quantitative Approaches to Reasoning with Uncertainty", "author": ["Y. Wang", "N.L. Zhang", "T. Chen", "L.K.M. Poon"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Severity of local maxima for the EM algorithm: Experiences with hierarchical latent class models", "author": ["Y. Wang", "N.L. Zhang"], "venue": "In Proceedings of the Third European Workshop on Probabilistic Graphical Models (PGM-06)", "citeRegEx": "Wang and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Wang and Zhang", "year": 2006}, {"title": "Latent tree models and approximate inference in Bayesian networks", "author": ["Y. Wang", "N.L. Zhang", "T. Chen"], "venue": "Journal of Articial Intelligence Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Not so naive Bayes: Aggregating onedependence estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "Large-scale neighbor-joining with NINJA", "author": ["T.J. Wheeler"], "venue": "In Proceedings of the 9th Workshop on Algorithms in Bioinformatics", "citeRegEx": "Wheeler,? \\Q2009\\E", "shortCiteRegEx": "Wheeler", "year": 2009}, {"title": "On convergence properties of the EM algorithm for Gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan", "year": 1996}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Xu and Wunsch,? \\Q2005\\E", "shortCiteRegEx": "Xu and Wunsch", "year": 2005}, {"title": "Clustering (illustrated edition)", "author": ["R. Xu", "D.C. Wunsch"], "venue": null, "citeRegEx": "Xu and Wunsch,? \\Q2008\\E", "shortCiteRegEx": "Xu and Wunsch", "year": 2008}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Effective dimensions of hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka"], "venue": "Journal of Articial Intelligence Research,", "citeRegEx": "Zhang and Kocka,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Kocka", "year": 2004}, {"title": "Efficient learning of hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka"], "venue": "In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Zhang and Kocka,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Kocka", "year": 2004}, {"title": "Latent variable discovery in classification models", "author": ["N.L. Zhang", "T.D. Nielsen", "F.V. Jensen"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Zhang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2004}, {"title": "Discovery of latent structures: Experience with the CoIL Challenge 2000 data set", "author": ["N.L. Zhang", "Y. Wang", "T. Chen"], "venue": "Journal of Systems Science and Complexity,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Latent tree models and diagnosis in traditional Chinese medicine", "author": ["N.L. Zhang", "S. Yuan", "T. Chen", "Y. Wang"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 22, "context": "A subclass of LTMs was first developed in the phylogenetic community (Felsenstein, 2003).", "startOffset": 69, "endOffset": 88}, {"referenceID": 71, "context": "LTM has been previously called \u201chierarchical latent class model\u201d (Zhang, 2004), but this name has been discarded because the model does not inherently reveal a hierarchy.", "startOffset": 65, "endOffset": 78}, {"referenceID": 71, "context": "The tree can be either directed (i.e. a Bayesian network; Zhang, 2004) or undirected (i.", "startOffset": 32, "endOffset": 70}, {"referenceID": 1, "context": "In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring LTMs.", "startOffset": 67, "endOffset": 81}, {"referenceID": 57, "context": "In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring LTMs.", "startOffset": 123, "endOffset": 139}, {"referenceID": 51, "context": "Two conditions ensure that an LTM does not include any redundant LVs (Pearl, 1988):", "startOffset": 69, "endOffset": 82}, {"referenceID": 51, "context": "The notion of parsimony is also called minimality by Pearl (1988).", "startOffset": 53, "endOffset": 66}, {"referenceID": 71, "context": "An LTM is regular (Zhang, 2004) if for any LV H:", "startOffset": 18, "endOffset": 31}, {"referenceID": 71, "context": "6 Trade-off between Latent Variable Complexity and Partial Structure Complexity Zhang and Kocka (2004b) distinguished two kinds of model complexity in LTM: latent variable complexity refers to LV cardinalities while partial structure complexity4 is", "startOffset": 80, "endOffset": 104}, {"referenceID": 71, "context": "In their paper, Zhang and Kocka (2004b) called it structure complexity.", "startOffset": 16, "endOffset": 40}, {"referenceID": 38, "context": "To solve the problem, one can use expectation-maximization (EM), the most popular algorithm for learning parameters in the face of LVs (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995).", "startOffset": 135, "endOffset": 184}, {"referenceID": 64, "context": "Wang and Zhang (2006) showed that a few random restarts suffice when the LTM is small and variables are strongly dependent with each other.", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "This learning procedure is very similar to the one proposed for binary trees by Harmeling and Williams (2011).", "startOffset": 80, "endOffset": 110}, {"referenceID": 48, "context": "Recently, Parikh et al. (2011) applied spectral techniques to LTM parameter learning.", "startOffset": 10, "endOffset": 31}, {"referenceID": 44, "context": "alleviates the restriction of the approach of Mossel et al. (2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "(2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to hidden Markov models.", "startOffset": 109, "endOffset": 127}, {"referenceID": 28, "context": "(2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to hidden Markov models. Parikh et al. (2011) reformulated the message passing algorithm using an algebraic formulation:", "startOffset": 109, "endOffset": 182}, {"referenceID": 48, "context": "We refer to the work of Parikh et al. (2011) for more details about the singular value decomposition and spectral algorithm.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": "For a Bayesian learning, variational Bayes (Attias, 1999) offers a counterpart of EM.", "startOffset": 43, "endOffset": 57}, {"referenceID": 10, "context": "Second, AGS follows a grow-restructure-thin strategy to reduce again the complexity of the search space (Chen, Zhang, & Wang, 2008; Chen et al., 2012).", "startOffset": 104, "endOffset": 150}, {"referenceID": 71, "context": "When starting from the simplest solution (an LCM), Zhang and Kocka (2004b) observed that the comparison of the BIC scores between the candidate model T \u2032 and the current one T might not be a relevant criterion.", "startOffset": 51, "endOffset": 75}, {"referenceID": 65, "context": "OVs and internal nodes are LVs (Wang et al., 2008; Harmeling & Williams, 2011).", "startOffset": 31, "endOffset": 78}, {"referenceID": 27, "context": "Harmeling and Williams (2011) justified selecting the pair of variables showing the highest MI at each step of LCMB-LTM.", "startOffset": 0, "endOffset": 30}, {"referenceID": 61, "context": "For instance, Wang et al. (2008) first learn a binary tree.", "startOffset": 14, "endOffset": 33}, {"referenceID": 40, "context": "The Bridged Island (BI) algorithm by Liu et al. (2012) takes a slightly different approach.", "startOffset": 37, "endOffset": 55}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6).", "startOffset": 17, "endOffset": 47}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered.", "startOffset": 17, "endOffset": 394}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered. To build an LFM, one can also first construct an LTM and then use the independence testing method of Tan et al. (2011) for pruning non-significant edges.", "startOffset": 17, "endOffset": 570}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered. To build an LFM, one can also first construct an LTM and then use the independence testing method of Tan et al. (2011) for pruning non-significant edges. This method provides guarantees to satisfy structural and risk consistencies. Similar works for non-parametric analysis have also been developed by Liu et al. (2011). It is worth mentioning that, to ensure model parsimony, the pruning of non-significant edges in an LTM should be followed by the removal of latent nodes which are no longer connected to a minimum of three nodes.", "startOffset": 17, "endOffset": 771}, {"referenceID": 22, "context": "This class of methods has been originally developed for phylogenetics (Felsenstein, 2003).", "startOffset": 70, "endOffset": 89}, {"referenceID": 36, "context": "binary variables (Lake, 1994).", "startOffset": 17, "endOffset": 29}, {"referenceID": 13, "context": "For a special case of discrete tree models, called symmetric discrete tree models (Choi et al., 2011), distance has a simpler form.", "startOffset": 82, "endOffset": 101}, {"referenceID": 7, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 45, "endOffset": 75}, {"referenceID": 21, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 45, "endOffset": 75}, {"referenceID": 3, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 77, "endOffset": 92}, {"referenceID": 60, "context": "(2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n.", "startOffset": 37, "endOffset": 50}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log n suffices when discretization is not assumed.", "startOffset": 0, "endOffset": 270}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log n suffices when discretization is not assumed. In phylogenetics, the scientist is often faced with a set of different trees11 and the construction of a consensus tree is thus required. The computational complexity of this construction has been studied and a polynomial algorithm has been proposed by Steel et al. (1992).", "startOffset": 0, "endOffset": 606}, {"referenceID": 13, "context": "In this subsection, we present the latest developments of Choi et al. (2011) for general LTM learning, i.", "startOffset": 58, "endOffset": 77}, {"referenceID": 13, "context": "In practice, Choi et al. (2011) restricted the learning to symmetric discrete distributions13 (see definition in Subsection Distances between Variables).", "startOffset": 13, "endOffset": 32}, {"referenceID": 62, "context": "This sample complexity is equal to the one of the Chow-Liu algorithm (Tan et al., 2011).", "startOffset": 69, "endOffset": 87}, {"referenceID": 13, "context": "To diminish the computational complexity of DBG, Choi et al. (2011) propose to first learn a minimum spanning tree (MST) based on distances between OVs.", "startOffset": 49, "endOffset": 68}, {"referenceID": 19, "context": "Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011).", "startOffset": 212, "endOffset": 251}, {"referenceID": 13, "context": "Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011).", "startOffset": 212, "endOffset": 251}, {"referenceID": 55, "context": "(2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 83, "endOffset": 115}, {"referenceID": 50, "context": "(2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 83, "endOffset": 115}, {"referenceID": 2, "context": "On the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed and latent nodes are random vectors rather than scalars.", "startOffset": 17, "endOffset": 42}, {"referenceID": 2, "context": "On the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed and latent nodes are random vectors rather than scalars. Their approach can deal with general linear models containing both categorical and continuous variables. Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011). The proposed extension consists in replacing the step in line 5 of Algorithm 4 by a quartet test14 relying on spectral techniques (more specifically, canonical correlation analysis; Hair, Black, Babin, and Anderson, 2009). Given four observed variables {X1,X2,X3,X4}, the spectral quartet test distinguishes between the four possible tree topologies (see Figure 8). The correct topology is {{Xi,Xj}, {Xi\u2032 ,Xj\u2032}} if and only if: |E[XiX j ]|\u2217 |E[Xi\u2032X j ]|\u2217 > |E[Xi\u2032X j ]|\u2217 |E[XiX j ]|\u2217, (25) where |M |\u2217 := \u03a0`=1\u03c3`(M) is the product of the k largest singular values of matrix M and E[M ] is the expectation of M (estimated using the covariance matrix). On the other hand, Song et al. (2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 17, "endOffset": 1195}, {"referenceID": 8, "context": "The first authors to adapt them to LTM learning were Chen and Zhang (2006).", "startOffset": 53, "endOffset": 75}, {"referenceID": 48, "context": "Similarly to the work of Parikh et al. (2011), message passing and parameter estimation are reformulated through tensor notation (see the work of Song et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 54, "context": "During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion, such as BIC (Raftery, 1986).", "startOffset": 189, "endOffset": 204}, {"referenceID": 71, "context": "However, this solution still remains computationally demanding (Zhang, 2004).", "startOffset": 63, "endOffset": 76}, {"referenceID": 54, "context": "During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion, such as BIC (Raftery, 1986). For each cardinality value, parameters are required to calculate the likelihood appearing in the optimization criterion. For this purpose, random restarts of EM are generally used to learn parameters with a low probability of getting trapped in local maxima. The drawback is that this method cannot be applied to LTM learning, because EM becomes time-consuming when there are several LVs. A better solution consists in using a greedy search approach, starting with a preset value of LV cardinality (generally equal to 2) and incrementing it to meet the optimal criterion. However, this solution still remains computationally demanding (Zhang, 2004). To tackle the issue of computational burden, several strategies have been proposed. For instance, one can simply set a small value for the LV cardinalities. Following this idea, Hwang and collaborators (2006) constrain LVs to binary variables.", "startOffset": 190, "endOffset": 1065}, {"referenceID": 63, "context": "More rigorous, Wang et al.\u2019s approach (2008) relies on regularity (see Section 2.", "startOffset": 15, "endOffset": 45}, {"referenceID": 45, "context": "In the context of large scale data analysis (several thousands of variables), Mourad et al. (2011) proposed to estimate the cardinality of an LV given the number of its children.", "startOffset": 78, "endOffset": 99}, {"referenceID": 71, "context": "NGS Score Tree O(sn5N) DHC (Zhang, 2004)", "startOffset": 27, "endOffset": 40}, {"referenceID": 10, "context": "2 Score Tree O(sn2N) HSHC (Zhang & Kocka, 2004b) EAST (Chen et al., 2012)", "startOffset": 54, "endOffset": 73}, {"referenceID": 65, "context": "AHCB Variable Forest O(n2N) LTAB (Wang et al., 2008) clustering BIN-A (Harmeling & Williams, 2011)", "startOffset": 33, "endOffset": 52}, {"referenceID": 46, "context": "clustering CFHLC (Mourad et al., 2011) BI (Liu et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 41, "context": ", 2011) BI (Liu et al., 2012)", "startOffset": 11, "endOffset": 29}, {"referenceID": 67, "context": "NJ Information Tree O(n3N) NINJA (Saitou & Nei, 1987; Wheeler, 2009) distance", "startOffset": 33, "endOffset": 68}, {"referenceID": 13, "context": "4 Information Tree O(n3N + n4) RG (Choi et al., 2011)", "startOffset": 34, "endOffset": 53}, {"referenceID": 13, "context": "distance O(n2N + n4) CLRG (Choi et al., 2011) regCLRG (Choi et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 13, "context": ", 2011) regCLRG (Choi et al., 2011)", "startOffset": 16, "endOffset": 35}, {"referenceID": 52, "context": "When the tree does not contain any LV, learning the model can be done efficiently in O(n2N) using the Prim\u2019s algorithm (1957). The situation is more complicated when the tree contains LVs.", "startOffset": 102, "endOffset": 126}, {"referenceID": 9, "context": "3: results from the work of Chen (2008).", "startOffset": 28, "endOffset": 40}, {"referenceID": 9, "context": "3: results from the work of Chen (2008).", "startOffset": 28, "endOffset": 40}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16.", "startOffset": 68, "endOffset": 87}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space (binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process binary data.", "startOffset": 68, "endOffset": 274}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space (binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process binary data. Hence the algorithms have not been applied to some datasets. We recall that RG, CLRG and regCLRG do not exactly learn an LTM but instead a tree whose all internal nodes are not compelled to be latent. 17. Although it is not shown in Tables 2 and 3, NJ, RG, CLRG and regCLRG were able to process 1000 binary variables in our experiments. 18. In the work of Mourad et al. (2011), CFHLC implements a window-based approach to scale very large datasets (n \u2265 100k variables).", "startOffset": 68, "endOffset": 681}, {"referenceID": 75, "context": "Thanks to LTMs, latent structure discovery has been applied to several fields: marketing (Zhang, Wang, & Chen, 2008), medicine (Zhang, Nielsen, & Jensen, 2004; Zhang et al., 2008), genetics (Hwang et al.", "startOffset": 127, "endOffset": 179}, {"referenceID": 29, "context": ", 2008), genetics (Hwang et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 18, "endOffset": 59}, {"referenceID": 46, "context": ", 2008), genetics (Hwang et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 18, "endOffset": 59}, {"referenceID": 22, "context": ", 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 26, "endOffset": 84}, {"referenceID": 9, "context": "This application is called \u201cmultidimensional clustering\u201d and has been mainly explored by Chen et al. (2012). Let us illustrate LTM-based multidimensional clustering using dataset from a survey on Danish beer market consumption.", "startOffset": 89, "endOffset": 108}, {"referenceID": 65, "context": "Figure 12: Experiments on ALARM and BARLEY networks: a) running times for LTM learning using the LTAB algorithm (Wang et al., 2008), b) approximation accuracy of probabilistic inference and c) running time for inference.", "startOffset": 112, "endOffset": 131}, {"referenceID": 63, "context": "Figure 12: Experiments on ALARM and BARLEY networks: a) running times for LTM learning using the LTAB algorithm (Wang et al., 2008), b) approximation accuracy of probabilistic inference and c) running time for inference. Approximation accuracy is measured by the Kullback-Leibler divergence between approximate inferred distributions and exact inferred distributions obtained from clique tree propagation on the original BN. N and C designate the sample size and the maximal cardinality of latent variables, respectively. These results come from the work of Wang et al. (2008).", "startOffset": 113, "endOffset": 577}, {"referenceID": 15, "context": "Probabilistic inference in a general BN is known to be an NP-hard task (Cooper, 1990).", "startOffset": 71, "endOffset": 85}, {"referenceID": 48, "context": "However, recent spectral methods (Parikh et al., 2011) considerably reduced model learning phase, because they do not require to learn the model structure.", "startOffset": 33, "endOffset": 54}, {"referenceID": 14, "context": "To tackle this issue, one can approximate the original BN using a maximum weight spanning tree learned relying on Chow and Liu\u2019s algorithm (1968). The drawback is the risk of inaccuracy in inference results.", "startOffset": 114, "endOffset": 146}, {"referenceID": 14, "context": "To tackle this issue, one can approximate the original BN using a maximum weight spanning tree learned relying on Chow and Liu\u2019s algorithm (1968). The drawback is the risk of inaccuracy in inference results. In this context, LTM provides an efficient and simple solution, because: (i) thanks to its tree structure, the model allows linear computations with respect to the number of OVs, and at the same time, (ii) it can represent complex relations between OVs through multiple LVs. Because learning LTM before performing inference can be time-consuming Wang et al. (2008) propose the following strategy: first, offline model learning is performed, then answers to probabilistic queries are quickly computed online.", "startOffset": 114, "endOffset": 573}, {"referenceID": 63, "context": "Hence Wang et al. (2008) propose a tradeoff between inferential complexity and model approximation accuracy by fixing a maximal cardinality C for LVs.", "startOffset": 6, "endOffset": 25}, {"referenceID": 51, "context": "In Figure 12, the LTM-based method is compared to other standard inference approaches: the LCM-based approach, the Chow-Liu (CL) model-based approach and loopy belief propagation (LBP) (Pearl, 1988).", "startOffset": 185, "endOffset": 198}, {"referenceID": 65, "context": "Figure 12a reports running times for LTM learning using the LTAB algorithm (Wang et al., 2008).", "startOffset": 75, "endOffset": 94}, {"referenceID": 74, "context": "Naive Bayes has been generalized by introducing latent nodes as internal discrete nodes (Zhang et al., 2004) or continuous nodes (Langseth & Nielsen, 2009), mediating the relation between leaves and the class variable.", "startOffset": 88, "endOffset": 108}, {"referenceID": 62, "context": "Recently, Wang et al. (2011) proposed a classifier based on LTM.", "startOffset": 10, "endOffset": 29}, {"referenceID": 31, "context": "Kimmel and Shamir (2005) perform efficient haplotypic inference using a two-layer LFM.", "startOffset": 0, "endOffset": 25}, {"referenceID": 31, "context": "Kimmel and Shamir (2005) perform efficient haplotypic inference using a two-layer LFM. Finally, Zhang et al. (2008) applied LTMs to traditional Chinese medicine (TCM).", "startOffset": 0, "endOffset": 116}, {"referenceID": 13, "context": "For instance, a recent work developed LTM for continuous data analysis (Poon, Zhang, Chen, & Wang, 2010; Choi et al., 2011).", "startOffset": 71, "endOffset": 123}, {"referenceID": 29, "context": "Other authors investigated the relationships between LTM and ontology (Hwang et al., 2006), and LTM-based dependence visualization (Mourad, Sinoquet, Dina, & Leray, 2011).", "startOffset": 70, "endOffset": 90}, {"referenceID": 52, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 13, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 58, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 32, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 13, "context": ", 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012). For instance, Poon et al. (2010) proposed a new model, called pouch latent tree model (PLTM).", "startOffset": 8, "endOffset": 96}, {"referenceID": 29, "context": "For instance, when applying LTM to a microarray dataset of yeast cell-cycle, Hwang et al. (2006) showed that some LVs are significantly related to specific gene ontology terms, such as organelle organization or cellular physiological process.", "startOffset": 77, "endOffset": 97}, {"referenceID": 46, "context": "Dependence Visualization: LTM provide a compact and interpretable view of dependences between variables, thanks to its graphical nature and its latent variables (Mourad et al., 2011).", "startOffset": 161, "endOffset": 182}, {"referenceID": 65, "context": "For EM, we used the method of Chickering and Heckerman (1997) which is implemented in the software LTAB (Wang et al., 2008).", "startOffset": 104, "endOffset": 123}, {"referenceID": 12, "context": "For EM, we used the method of Chickering and Heckerman (1997) which is implemented in the software LTAB (Wang et al.", "startOffset": 30, "endOffset": 62}, {"referenceID": 71, "context": "For each step, there are O(n2) new models generated through the use of 3 operators: addition/removal of an LV and node relocation (Zhang, 2004).", "startOffset": 130, "endOffset": 143}, {"referenceID": 71, "context": "For each model, the cardinality is optimized for each LV, so that O(n2) new models are generated (Zhang, 2004).", "startOffset": 97, "endOffset": 110}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011).", "startOffset": 33, "endOffset": 63}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia.", "startOffset": 33, "endOffset": 63}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia. Datasets generated using the well-known Asia network containing 8 binary OVs. The train and test datasets both consist of 100 observations. \u2212 Hannover. Real dataset containing 5 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 3573 and 3589 observations, respectively. The dataset comes from the work of Zhang (2004). \u2212 Car.", "startOffset": 33, "endOffset": 442}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia. Datasets generated using the well-known Asia network containing 8 binary OVs. The train and test datasets both consist of 100 observations. \u2212 Hannover. Real dataset containing 5 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 3573 and 3589 observations, respectively. The dataset comes from the work of Zhang (2004). \u2212 Car. Real dataset containing 7 variables. The dataset has been split into a train dataset and a test dataset. They consist of 859 and 869 observations, respectively. The dataset is available at : http://archive.ics.uci.edu/ml/. Large datasets (10 \u2264 n \u2264 100 variables): \u2212 Tree. Datasets generated using a tree on 50 variables (19 leaves and 31 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations. \u2212 Forest. Datasets generated using a tree on 50 variables (20 leaves and 30 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations. \u2212 Alarm. Datasets generated using the well-known Alarm network containing 37 OVs. The train and test datasets both consist of 1000 observations. \u2212 Coil-42. Real dataset containing 42 variables. The dataset has been split into a train dataset and a test dataset. They consist of 5822 and 4000 observations, respectively. The dataset comes from the work of Zhang and Kocka (2004b). \u2212 NewsGroup.", "startOffset": 33, "endOffset": 1451}], "year": 2013, "abstractText": "In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure a tree allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}