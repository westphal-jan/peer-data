{"id": "1402.0586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Topic Segmentation and Labeling in Asynchronous Conversations", "abstract": "Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.", "histories": [["v1", "Tue, 4 Feb 2014 01:43:35 GMT  (1192kb)", "http://arxiv.org/abs/1402.0586v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shafiq rayhan joty", "giuseppe carenini", "raymond t ng"], "accepted": false, "id": "1402.0586"}, "pdf": {"name": "1402.0586.pdf", "metadata": {"source": "CRF", "title": "Topic Segmentation and Labeling in Asynchronous Conversations", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond T. Ng"], "emails": ["sjoty@qf.org.qa", "carenini@cs.ubc.ca", "rng@cs.ubc.ca"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that you are able to hide without being able to play by the rules."}, {"heading": "2. Related Work", "text": "Three research areas are directly related to our study: topic segmentation, topic labeling, and extraction of the conversation structure of asynchronous conversations."}, {"heading": "2.1 Topic Segmentation", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they"}, {"heading": "2.2 Topic Labeling", "text": "In the first comprehensive approach to the topic culture, Mei, Shen and Zhai (2007) have proposed methods to identify multilateral topic models (e.g., the topic-word distributions realized by LDA); crucial to their approach is how they assume the semantic similarity between a topic-word distribution and a candidate-topic label extracted from the same corpus; they perform such a task by assigning a different word distribution for the label and deriving the Kullback-Leibler divergence between the two distributions; it turns out that this measure corresponds to the weighted mutual information (PMI) of the topic label with the candidate-label, where the weights are actually the probabilities in the topic-word distribution; they use Maximum Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) to select the labels that are relevant but not redundant."}, {"heading": "2.3 Conversational Structure Extraction", "text": "Several approaches have been proposed to capture the underlying conversation structure of a conversation; recent work on synchronous conversations focuses on untangling multi-party chats that have a linear structure; for example, several studies suggest models for untangling multi-party chats (Elsner & Charniak, 2010, 2011; Wang & Oard, 2009; Mayfield, Adamson, & Rose, 2012); on the other hand, asynchronous conversations such as emails and social media services (e.g. Gmail, Twitter) are generally organized into tree-structured conversation threads using headers; and automatic methods have been proposed to uncover such more complex structures (e.g. Wang, Wang, Zhai, & Han, 2011; Aumayr, Chan, & Hayes, 2011). However, the use of quotes in asynchronous conversations can express a conversation structure that is more finely structured and can be more informative than those found in responses to comments between comments (Carenini, al)."}, {"heading": "3. Topic Models for Asynchronous Conversations", "text": "Developing theme segmentation and labeling models for asynchronous conversations is challenging in part because of the specific characteristics of these media. As already mentioned, in asynchronous conversations, as opposed to monologues (e.g. a news article) and synchronous dialogues (e.g. a meeting), topics do not change sequentially and are interwoven. Moreover, as can be seen in Figures 1 and 2, the writing style varies between participants, and many people tend to use informal, short, and ungrammatic sentences, which makes the discourse much less structured. One aspect of asynchronous conversations that may at first glance contribute to theme modeling is that each message has a headline. However, often, headlines do not convey much up-to-to-date information, and sometimes they can even be misleading. For example, in a blog conversation (Figure 2), participants repeatedly talk about different topics using the same title (i.e. \"beautifully beautiful\"), which art does not convey."}, {"heading": "3.1 Topic Segmentation Models", "text": "We are the first to investigate the problem of asynchronous conversation segmentation. Therefore, we first show how the existing models, originally developed for monological and synchronous dialogue, can be naively applied to asynchronous conversations. Then, pointing out their limitations, we propose our novel theme segmentation models for asynchronous conversations."}, {"heading": "3.1.1 Existing Models", "text": "LCSeg et al., 2003) and LDA (Blei et al., 2003) are the two state-of-the-art unsupervised models for topic segmenters (LCSeg) LCSeg is a sequential segmentation model originally developed for transcript segmentation, using the linguistic property called lexical cohesion, and assuming that topic changes are likely to occur where strong word repetitions begin and end. It first calculates lexical chains (Morris & Hirst, 1991) for each non-stop word based on word repetitions. 3 Then, the chains are weighted according to their term frequency and chain length."}, {"heading": "3.1.2 Limitations of Existing Models", "text": "The main limitation of the two models discussed above is that they make the assumption that the Word Bag (BOW) assumptions ignore facts specific to a multi-party, asynchronous conversation. LCSeg only takes into account the frequency of the term and how closely these terms occur in the temporal order of sentences. On the other hand, if the topics are interwoven and do not change sequentially in the temporal order, as is often the case in an asynchronous conversation, LCSeg would not find the thematic segments correct. On the other hand, the only information relevant to LDA is the frequency of the term. Several extensions of the LDA beyond the BOW approach have been proposed. Thus, Wallach (2006) expands the model beyond BOW by taking n-gram sequences into account. Griffiths, Steyvers, Lead and Tenenbaum (2005) represent an extension that is sensitive to word sequences and synchronous conversations. \""}, {"heading": "3.1.3 Proposed Unsupervised Models", "text": "This year, it will be able to leave the country to return to the EU and leave the EU."}, {"heading": "3.1.4 Proposed Supervised Model", "text": "This year it is more than ever before in the history of the city."}, {"heading": "3.2 Topic Labeling Models", "text": "Now that we have methods to identify the topical segments in an asynchronous conversation, the next step in the pipeline is one or more informative descriptions or topic names for each segment to facilitate the interpretation of the topics. We are the first to address this issue in a multinational model like LDA."}, {"heading": "3.2.1 Preprocessing", "text": "In the pre-processing stage, we tokenize the text and apply a syntactical filter to select the words of a specific part of the language (POS). Available at http: / / cogcomp.cs.illinois.edu / page / softwaretext and annotate the tokens with their POS tags. We experimented with five different syntactic filters. They selected (i) nouns, (ii) nouns and adjectives, (iii) nouns, adjectives and verbs, (iv) adjectives, verbs and adverbs, and (v) all words. The filters also exclude the stopwords. The second filter, which selects only nouns and adjectives, achieves the best performance on our development group, which is also consistent with our Tarcea system."}, {"heading": "3.2.2 Word Ranking", "text": "The words selected in the preprocessing step correspond to the nodes in the graph. A direct application of the ranking method described by Mihalcea and Tarau to the respective topic would define the edges based on the co-occurrence relationship between the respective words and then apply the PageRank (Page et al., 1999) algorithm to evaluate the nodes. We argue that co-occurrence relationships may be insufficient to find subject labels in an asynchronous conversation. To better identify the labels, one must take into account aspects specific to asynchronous conversations. In particular, we suggest using two different forms of conversation-specific information in our graphical ranking model: (1) informative clues from the leading sentences of a current segment and (2) the fine-grained conversation structure (i.e., the fragment quotation graph (QG) of a discussion."}, {"heading": "3.2.3 Phrase Generation", "text": "Once we have a ranking of words to describe a current segment, we select the M keywords to construct the keywords (labels) from those keywords. We take a similar approach to Mihalcea and Tarau (2004). Specifically, we mark the M keywords in the text and collapse the sequences of adjacent keywords into keywords. For example, consider the first sentence. \"15th anniversary of the Elder Scrolls series...\" in Figure 2. If \"Elder,\" \"Scrolls\" and \"Series\" are selected as keywords because they appear side by side in the text, they are grouped into a single sentence. \"The score of a keyphrase is then determined by the maximum number of points of its components (i.e. keywords). Instead of constructing the keywords in the post-processing phase, as we do, an alternative approach is to first define the candidate phrases using n-gram sequences or a chunker in the problem, i.e., all of the keywords are inserted."}, {"heading": "3.2.4 Conversation-Level Phrase Re-ranking", "text": "So far, we have only extracted phrases from the topic segment without taking the rest of the conversation into account, and this method does not find a label if some of its components appear outside the segment. In our blog corpus, for example, the security of the phrase server in the humanly authorized topic server security and firewall does not occur in its current segment, but appears throughout the conversation. In fact, in our development set, about 14% or 8% of words in the blog or e-mail label come from the part of the conversation that is outside the topic segment, respectively. Therefore, we suggest extracting informative phrases from the entire conversation, re-evaluating them with respect to each topic (or segment), and combining only the relevant conversation expressions with the segment levels. We classify the words of the entire conversation by using the ranking models described in Section 3.2.2, and reclassifying sentences using the same method described in Section 3.2.3."}, {"heading": "3.2.5 Redundancy Checking", "text": "Once we have the ranking of labels (keyphrases), the final step is to generate the final k labels as output. In selecting multiple labels for a topic, we expect the new labels without redundant information to be diverse in order to achieve broad coverage of the topic. Specifically, we select the labels individually by maximizing the following MMR criterion: l = argmaxl \u00b2 W \u2212 S [\u03c1 Score (l) \u2212 (1 \u2212 \u03c1) maxl \u00b2 S Sim (l \u00b2, l)] (17), where W is the set of all labels and S is the set of labels already selected as output. We define the similarity between two labels l \u00b2 and l: in the Sim (l \u00b2, l)]] (17), where W is the set of all labels and S is the set of labels already selected as output."}, {"heading": "4. Corpora and Metrics", "text": "Due to the lack of publicly accessible corpora of asynchronous conversations with thematic notes, we have developed the first corpora that are provided with thematic information."}, {"heading": "4.1 Data Collection", "text": "For e-mail, we selected our publicly available BC3 e-mail corpus (Ulrich, Murray, & Carenini, 2008), which includes 40 e-mail conversations from the World Wide Web Consortium (W3C) mailing list 14. The BC3 corpus, which was previously commented on with sentence-level speech files, subjectivity, extractive and abstract abstracts, is one of a growing number of corpus used for e-mail research (Carenini et al., 2011). This corpus has an average of 5 e-mails per conversation and a total of 1024 sentences after excluding the quoted sentences. Each conversation also provides the thread structure based on responses to e-mail relationships. For blog, we manually selected 20 conversations of varying length, all short enough to still be feasible for humans, from the popular technology-related news website Slashdot15. Slashdot was chosen because it begins with a moderated conversation between links to the web site, which allows for a reconstruction of individual comments, while providing a more precise user interaction."}, {"heading": "4.2 Topic Annotation", "text": "In fact, most of us are able to play by the rules that we have imposed on ourselves. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...)"}, {"heading": "4.3 Evaluation (and Agreement) Metrics", "text": "In this section, we describe the metrics used to compare different annotations. These metrics measure both how much our annotations correspond to each other, and how well our models and different baselines work. In a particular conversation, different annotations may have different topic numbers, different topic mappings of sentences (i.e. clustering), and different topic names. Below, we describe the metrics used to measure segmentation performance, followed by the metrics used to measure label performance."}, {"heading": "4.3.1 Metrics for Topic Segmentation", "text": "It is indeed the case that most of us are able to survive ourselves, \"he said.\" But it is not the case that we have to get involved in such a situation. \"But he also stressed that he did not want to get involved in such a situation:\" It is not the case that we have to get involved in such a situation. \"He stressed:\" It is not the case that we have to get involved in such a situation. \"But he also stressed:\" It is not the case that we have to get involved in such a situation. \""}, {"heading": "4.3.2 Metrics for Topic Labeling", "text": "It is not as if it is a reactionary candidate, but a reactionary candidate who proves wrong on this issue. (Zesch & Gurevych, 2009) (Zesch & Gurevych, 2009) (Kim, Baldwin, & Kan) The n-gram-based metrics we use for near-misdeeds are similar to those used in text summarization (Lin, 2004), and machine translation that we use. (Zesch & Gurevych, 2004) (Zesch & Gurevych, 2009). (Zesch & Gurevych, 2009) (Zesch & Gurevych, 2009). (Zesch & Gurevych, 2009)"}, {"heading": "4.3.3 Metrics for End-to-End Evaluation", "text": "Just like human annotators, our end-to-end system inputs an asynchronous conversation, finds the thematic segments in the conversation, and then assigns brief descriptions (topic names) to each of the thematic segments. However, it would be fairly easy to calculate agreement on topic names based on mutual overlaps if the number of topics and topic segments were determined across the annotations of a given conversation. However, since different annotators (e.g. system or human) can identify a different number of topics and different aggregations of sentences, measuring the annotator (model or human) agreement on the topic names is not a trivial task. To solve this, we first map the clusters of one annotation (e.g. A1) to the clusters of another (e.g. A2) based on the optimal one-to-one mapping described in the previous section."}, {"heading": "5. Experiments", "text": "In this section, we present our experimental results. First, we show the performance of the segmentation models. Then, we show the performance of the labeling models based on manual segmentation. Finally, we present the performance of the end-to-end system."}, {"heading": "5.1 Topic Segmentation Evaluation", "text": "In this section we present the experimental setup and the results of the segmentation task."}, {"heading": "5.1.1 Experimental Setup for Segmentation", "text": "Our first model is the graph-based, unattended segmentation model presented by Malioutov and Barzilay (2006). Since the sequentiality limitation of theme segmentation in monologue and synchronous dialogue is not held in asynchronous conversation, we implement this model without this limitation. Specifically, this model (let's call it M & B) represents a weighted, undirected conversation topic G (V, E), where nodes V represent the sentences and edge weights w (x, y) represent the cosmic similarity (equation 5) between sentences x and y. It then finds the thematic segments by optimizing the normalized intersection criterion (equation 6), with M & B looking at the conversations globally, but the models only representing lexical similarity. The other five models are LDA + FQG, LCSeg, LCSeg + FQG and the predetermined model SDA = 0,003."}, {"heading": "5.1.2 Results for Segmentation", "text": "In fact, it is not that we are able to analyze and analyze the results of the last weeks and months, but rather that we are able to analyze and analyze the results of the last weeks and months. Indeed, it is not as if we are looking at the results of the last weeks and months of the last weeks and months. Indeed, it is not as if we have looked at the results of the last weeks and months. Indeed, it is as if we have looked at the results of the last weeks and months. Indeed, it is not as if we have looked at the results of the last weeks and months."}, {"heading": "5.2 Topic Labeling Evaluation", "text": "In this section, we present the experimental evaluation of labeling models when the models are provided with manual (or golden) segmentation, allowing us to evaluate their performance regardless of the task of topic segmentation."}, {"heading": "5.2.1 Experimental Setup for Topic Labeling", "text": "As mentioned in Section 4, in the email corpus, the three commenters found 100, 77 and 92 topics (or topical segments) each (269 in total), and in the blog corpus, they found 251, 119 and 192 topics each (562 in total). The commenters wrote a short high-level description for each topic. These descriptions serve as reference topic labels in our review. 23 The goal of the topic caption models is to automatically generate such informative descriptions for each topical segment. We compare our approach with two baselines. The first baseline FreqBL ranks the words by their frequencies. The second baseline LeadBL, expressed by Equation 11, ranks the words only on their relevance in the leading sentences in a topical segment. We also compare our model with two state-of-the-art extraction methods."}, {"heading": "5.2.2 Results for Topic Labeling", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on themselves. (...) It is as if they were able to change the rules. (...) It is as if they were able to change the rules. (...) It is as if they were able to change the rules. (...) It is as if they were able to change the rules. (...) It is as if they were able to change the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to change the rules, to change the rules. (...) () () (() () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () ()) () () () () ()) () () () () () () ()) () () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () () () () (() () () () () () (() (() () () () ((() () () (() () () (() () () (() () (() () () (((() () () (() () () ((() () () ((() () () (() () () (()"}, {"heading": "5.3 Full System Evaluation", "text": "In this section, we present the performance of our end-to-end system. First, we segment a given asynchronous conversation with our best topic segmentator (the monitored model) and then feed its output to our best topic labeller (the CorBias + model). Table 14 presents the human agreement and the agreement of our system with the human commentators on the basis of the best comments of k. For each system comment function, we measure their agreement in w-m-o and w-s-m-o with the three human comments using the method described in section 4.3.3. Note, however, that our system in e-mail receives a 100% agreement in w-m-o-metric for some conversations. However, there is a considerable gap between the mean value and the maximum w-m-o-values. In w-m-m-o, our system achieves a maximum 108% agreement in w-m values, but the mean value varies from W-m-o-o metric for some conversations."}, {"heading": "6. Conclusion and Future Direction", "text": "This paper presents two new corporations of e-mail and blog conversations that are associated with topics that, together with the proposed metrics, allow researchers to quantify their work. We also present a complete computation system for topic segmentation and labeling in asynchronous conversations. Our approach extends to the fine-grained structures of asynchronous conversation that are associated with other forms of conversation. We do this by applying the latest graph-based methods for NLP, such as random passage of paragraphs or word graphs. For topic segmentation, we expand the LDA and LCSeg models to integrate the fine dictatorial structures; the Fragment Quotation Graph (FQG) that we generate to generate two new models."}, {"heading": "Acknowledgments", "text": "We thank the NSERC Canada Graduate Scholarship (CGS-D), the NSERC BIN Strategic Network and the NSERC Discovery Grant for their financial support. We thank the commenters for their great efforts. Many thanks to Gabriel Murray, Jackie Cheung, Yashar Mehdad, Shima Gerani, Kelsey Allen and the anonymous reviewers for their thoughtful suggestions and comments."}, {"heading": "Appendix A. Metrics for Topic Segmentation", "text": "A.1 One-to-One MetricConsider the two different annotations of the same conversation with 10 sentences (indicated by colored boxes) in Figure 11 (a). In each annotation, the themes are distinguished by different colors. For example, the model output has four themes, while the human annotation has three themes. To calculate one-to-one accuracy, we take the model output and map its segments optimally (by calculating the optimal bipartite overlap) with the segments of the gold standard human annotation. In our example, seven out of ten sentences in the model output overlap with the green segment in the human annotation. We transform the model output on the basis of this figure and calculate the percentage of overlap as one-to-accuracy. In our example, seven out of ten sentences overlap with the same annotation, so that the one-to-one accuracy of the model is 70%. We transform the model output on the basis of this figure and calculate the percentage of overlap as one-to-to-to-one accuracy."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.", "creator": "TeX"}}}