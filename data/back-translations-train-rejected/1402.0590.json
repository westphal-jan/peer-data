{"id": "1402.0590", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "A Survey of Multi-Objective Sequential Decision-Making", "abstract": "Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.", "histories": [["v1", "Tue, 4 Feb 2014 01:45:08 GMT  (575kb)", "http://arxiv.org/abs/1402.0590v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["diederik marijn roijers", "peter vamplew", "shimon whiteson", "richard dazeley"], "accepted": false, "id": "1402.0590"}, "pdf": {"name": "1402.0590.pdf", "metadata": {"source": "CRF", "title": "A Survey of Multi-Objective Sequential Decision-Making", "authors": ["Diederik M. Roijers", "Peter Vamplew", "Shimon Whiteson", "Richard Dazeley"], "emails": ["d.m.roijers@uva.nl", "p.vamplew@ballarat.edu.au", "s.a.whiteson@uva.nl", "r.dazeley@ballarat.edu.au"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "2. Background", "text": "A finite goal Markov decision-making process (MDP) is a tuple < S, A, T, R, \u00b5 > where: \u2022 S is a finite series of states, \u2022 A is a finite series of actions, \u2022 T: S \u00b7 A \u00b7 S \u2192 S \u2192 [0, 1] is a transitional function that provides for a probability distribution over initial states for each state, each action and each next state, and the probability that the next state occurs is a discount factor that specifies the relative importance of immediate rewards. The goal of an agent acting in this environment is to maximize the expected return, which is a function of reward."}, {"heading": "3. Motivating Scenarios", "text": "While the MOMDP setting has received considerable attention, it is not immediately obvious why it is a useful complement to the standard MDP or why specific algorithms are necessary for it. In fact, some researchers argue that modeling problems as explicitly multi-objective scenarios is not necessary, and that a scalar reward function is appropriate for all sequential decision-making tasks. \"The most direct formulation of this perspective is Sutton's reward hypothesis, which states that everything we mean by goals and purposes can be considered as maximizing the expected value of the cumulative sum of a received scalar signal (reward),\" does not imply that multi-objective problems do not exist. In fact, this would be a difficult assertion since it is so easy to think of problems that naturally have multiple goals. Instead, the implication of the reward hypothesis is that the resulting MOMDPs can always be converted into individual objective MPs with additional returns."}, {"heading": "4. Problem Taxonomy", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.1 Single versus Multiple Policies", "text": "Following the approach of Vamplew et al. (2011), we are only interested in a policy in which only one policy is pursued, in which several strategies are pursued. (Which case depends on which of the three motivating scenarios discussed in Section 3 is a policy at all.) The reason for this is that they are both characterised by a strict separation of the decision-making process into two phases: the planning or learning phase and the execution phase (although both scenarios are conceptually very different, from an algorithmic perspective they are considered to be identical). (Therefore, in the planning or learning phase, the planning or learning algorithms are not attributable to a single policy (and the corresponding multi-objective values). (This approach should not include policies that are optimal for all strategies). (2011) we are only interested in a single policy, but in a set of strategies. (and the corresponding multi-objective values)"}, {"heading": "4.2 Linear versus Monotonically Increasing Scalarization Functions", "text": "The second key factor influencing the optimal solution of a MOMDP is the type of scalarization function. In this section, two types of scalarization functions are discussed: those that represent linear combinations of rewards and those that merely represent monotonously increasing functions of these functions."}, {"heading": "4.2.1 Linear Scalarization Functions", "text": "A common assumption about the scalarization function (e.g. Natarajan & Tadepalli, 2005; Barrett & Narayanan, 2008) is that f is linear, i.e., it calculates the weighted sum of values for each objective definition 4. A linear scalarization function computes the inner product of a weight vector w and a value vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector simple vector vector vector vector vector vector vector vector vector vector vector vector simple vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "4.2.2 Monotonically Increasing Scalarization Functions", "text": "While the linear scalarization functions are intuitive and simple, they are not always adequate to express the preferences of the user. Example: Suppose, in the mining task mentioned above, there are two minerals that can be mined, and only three strategies are available: the owner of the equipment prefers an area where only the first mineral can be mined, in a place where only the second mineral can be mined, and in a place where both minerals can be mined. Provided that the owner of the equipment prefers an area where it can at least partially reassure customers with different interests. However, it may be the case that the place where the two minerals are associated contains fewer minerals than the convex shell. Thus, the preference of the owner implies that he or she, implicitly or explicitly, has a non-linear scalarization function.Here, we are looking at the case where linear is not possible and can be a common user."}, {"heading": "4.3 Deterministic versus Stochastic Policies", "text": "The third crucial factor that provides an optimal solution for a MOMDP is whether only deterministic policies are considered or stochastic policies are also permitted. While, in most applications, there is no reason to exclude stochastic policies from the outset, there may be cases where stochastic policies are clearly undesirable or even unethical. If, for example, politics determines the clinical treatment of a patient, as in the work of Lizotte, Bowling and Murphy (2010) and Shortreed, Laber, Lizotte, Stroup, Pineau and Murphy (2011), then it may be inappropriate to flip a coin to determine the course of action. We refer to the range of deterministic policies that are both inpatient and inpatient, as well as the range of inpatient policies that form part of all policies."}, {"heading": "4.3.1 Deterministic and Stochastic Policies with Linear Scalarization Functions", "text": "If f is linear, a result similar to theorem 1 applies to MOMDPs by the following logical conclusion: 1. For a MOMDP m, each CCS (\u0435mDS) is also a CCS (\u0435mDS).Proof. If f is linear, we can convert the MOMDP into a single target MDP for any W. This is done by treating the inner product of the reward vector and w as the new rewards and leaving the rest of the problem as it is. Since the inner product is distributed over the addition, the scaled yields remain additive (Eq.7).Therefore, for each W there is a translation into a single target MDP for which an optimal deterministic and stationary policy must exist, based on theorem 1. Therefore, for each W there is an optimal deterministic stationary policy (Eq.7)."}, {"heading": "4.3.2 Multiple Deterministic Policies with Monotonically Increasing Scalarization Functions", "text": "Theorem 2. In MOMDPs with an infinite horizon, deterministic non-stationary policies can pareto-dominate deterministic stationary policies that remain untouched by other deterministic stationary policies (White, 1982).To see why, consider the following MOMDPs, called m1, adapted to an example by White (1982).There is only one state and three actions a1, a2, and a3 that provide rewards (3, 0), (0, 3), and (1, 1) resp. If we allow only deterministic stationary policies, then there are three possible policies that are 1, 2, 3, and a3 that are non-stationary policies that are not stationary."}, {"heading": "4.3.3 Multiple Stochastic Policies with Monotonically Increasing Scalarization Functions", "text": "In fact, it is as if most of them are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is that they are not able to survive themselves. (...) It is that they are able to survive themselves. (...) It is that they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "4.3.4 Single Deterministic and Stochastic Policies with Monotonically Increasing Scalarization Functions", "text": "The type of optimal solution in this case arises directly from the reasons given for the multiple strategies. If only deterministic strategies are considered, then the individual policies being pursued cannot be stationary due to the risk of circular dependencies in the Q values, as shown in box (3) of Table 1. If stochastic strategies are allowed, the optimal policies can be stochastical, but this can be presented as a mixture of two or more deterministic stationary strategies, as reflected in box (4) of Table 1 for the same reasons cited in episode 2. In both cases, strategies can potentially benefit from the conditioning of the reward history."}, {"heading": "5. Planning in MOMDPs", "text": "In this section, we will examine some key approaches to planning in MOMDPs, i.e. calculating an optimal policy or coverage rate of unregulated policies using a complete model of MOMDP. Following the taxonomy presented in Section 4, we will first consider monopoly methods and then turn to multipolar methods for linear and monotonous increasing scalarization functions."}, {"heading": "5.1 Single-Policy Planning", "text": "In the eeisrmtlcehncS nvo nde nlrrsAe\u00fcgbtlrf\u00fc ide eeisrmtlrteeoiKe ni red eeirrlrmtlrteeeeeeVnlrrrrteeeeeeeeee\u00fcgBnlrteeu ni red eeirmnlrteeeeeeeeVnlrrteeeeeeeeeeee\u00fce ni red nllrrrrrrrrrrrrf\u00fc nlrrrrrrrrrf\u00fc nlrrrrrrrf\u00fc nlrrrrrlrf\u00fc nlrteeeeeeeeegl\u00fc ide ide nlrteeeeglrf\u00fc nlrteeegl\u00fc nlrf\u00fc nlrrrrrrrrrrf\u00fc nlrrrrrrrf\u00fc nlrrrrrf\u00fc nlrlrfteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.2 Multiple-Policy Planning with Linear Scalarization Functions", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.3 Multiple-Policy Planning with Monotonically Increasing Scalarization Functions", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves."}, {"heading": "6. Learning in MOMDPs", "text": "The methods examined in Section 5 assume that a model of the transition and reward dynamics of the MOMDP is known. In cases where such a model is not directly available, multi-objective reinforcement learning (MORL) can be used unreliably. One way to perform MORL is to take a model-based approach, i.e. to use the agent's interaction with the environment to learn a model of the transition and reward function of the MOMDP and then apply multi-objective planning methods such as those described in Section 8.1. Although such an approach seems well suited for MORL, few papers have considered it (e.g. Lizotte et al., 2010, 2012). We discuss possibilities for future work in model-based MORL in Section 8.1. Instead, most of the work in MORL focuses on model-free methods where a model of transition and reward function is never explicitly learned. In this section, we examine some central MORL approaches."}, {"heading": "6.1 Single-Policy Learning Methods", "text": "In the well-known scenario presented by authors such as Castelletti et al. (2002), Guo et al. (2008), it is a matter of learning a single policy that is optimal for the respective weight classes. As presented in Section 5.1, this is primarily the case if the individual methods used are not brought to the fore. However, even if there are no specific methods needed to address this attitude, it is still the most studied scenario for MORL. Linear scalarization with uniform weight proportions, i.e., all elements of W are equal, forms the basis of the work by Karlsson (1997), Bianchi, and Beldjilali, and Trentesaux (2009) and Shabani, with Shabani (2009) and Shabani among others, while non-uniform weights are used."}, {"heading": "6.2 Multiple-Policy Learning with Linear Scalarization Functions", "text": "In fact, it is a very selective and unpredictable group that sees itself in a position to put itself at the top of society. (...) It is not as if it is in a position to trump itself. (...) It is not as if it is in a position to trump itself. (...) It is as if it is in a position to trump itself. (...) It is not as if it is in a position to trump itself. (...) It is not as if it is in a position to trump itself. (...)"}, {"heading": "6.3 Multiple-Policy Learning with Monotonically Increasing Scalarization Functions", "text": "This year, the time has come for it to be only a matter of time before it is ready, until it is ready, until it comes into force."}, {"heading": "7. MOMDP Applications", "text": "Multi-objective methods of planning and learning have been used in a wide range of applications, both in simulation and real-world environments. In this section, we will examine these applications. For the sake of brevity, this list is not exhaustive, but aims to provide a number of illustrative examples. First, we will discuss the use of multi-objective methods in specific applications. Second, we will discuss research that has identified broader classes of problems in which multi-objective methods can play a useful role."}, {"heading": "7.1 Specific Applications", "text": "An important factor driving interest in multi-objective decision-making is the increasing social and political emphasis on environmental concerns, and several decisions need to be made to balance economic, social and environmental objectives, as reflected in the fact that a significant proportion of multi-objective methods applications have an ecological component. Perhaps the most researched application is the problem of water reservoir management, which is considered by Castelletti and others. (2002), Castelletti, Pianosi, and Soncini-Sessa (2008), Castelletti and Castelletti, Pianosi, and Restelli (2013). The general task is to find a control policy for the release of water from a dam while balancing multiple uses of the reservoir, including the production of hydroelectric power plants and flood mitigation. The management of hydroelectric power plants was also examined by Shabani (2009)."}, {"heading": "7.2 Applications within Broader Planning and Learning Tasks", "text": "In addition to the specific applications discussed above, several authors have identified more general classes of tasks where multi-objective sequential decision-making can be applied."}, {"heading": "7.2.1 Probabilistic and Risk-Aware Planning", "text": "Cheng, Subrahmanian, and Westerberg (2005) argue that decision-making under uncertainty is inherently multi-objective in nature. Even if there is only one reward to consider (such as profit), environmental uncertainty means that the expected value alone is not enough to support good decision-making; the decision-maker must also take into account the variance in return. Similarly, Bryce (2008) notes that probabilistic planning is inherently multi-objective, since there is a need to optimize both the cost and likelihood of success of the plan. He criticizes approaches that either summarize these factors or optimize one and then the other, arguing in favor of explicitly multi-objective methods. The aptly named \"probabilistic planning is multi-objective!\" paper by Bryce, Cushing, and Kambhampati (2007) shows how this could be achieved by describing a method based on multi-objective dynamic programming."}, {"heading": "7.2.2 Multi-Agent Systems", "text": "The use of MDPs within multi-agent systems has been extensively investigated (Bosoniu, Babuska, & Schutter, 2008), and several authors have suggested approaches that are strongly related to MOMDPs. In a multi-agent system, each agent has its own goal, but for effective overall performance, it must also take into account how his actions affect the other agents. If the agents are not entirely self-serving, this problem can be defined as MOMDP, by treating the effects on other agents as additional goals. Thus, Mouaddib (2006) uses multi-objective dynamic programming to facilitate collaboration between multiple agents whose underlying goals are at odds. For each pair of state shareholders, each agent stores three values: its local benefit, the gain of other agents, and the punishment it imposes on other agents. Policies for each agent are then determined by translating these vector values to apply the ratios and the ratios to these orders."}, {"heading": "7.2.3 Multi-Objective Optimization using Reinforcement Learning", "text": "Reinforcement learning is primarily applied to sequential decision-making tasks in a dynamic environment, but it can also be used effectively to control search mechanisms for static optimization tasks such as planning (Carchrae & Beck, 2005). Multi-objective optimization for static tasks such as design is a well-established field and, while the majority of this work has used mathematical or evolutionary approaches (Coello Coello et al., 2002), some authors have explored the application of attachment learning in such contexts. Mariano and Morales (1999, 2000b, 2000a) investigate the use of RL methods (Ant-Q and Q-Learning) as a search mechanism for the optimization of multi-objective design tasks. Values of decision variables are considered to be a current state, and measures are defined that alter the values of these variables. Multiple agents explore the state space in parallel."}, {"heading": "8. Future Work", "text": "In this section we list some of the possibilities for future research in the field of multi-objective planning and learning."}, {"heading": "8.1 Model-Based Methods", "text": "Given the breadth of planning methods for MOMDPs that could be used as sub-routines in model-based MORL methods, this is surprising. To our knowledge, the only work in this area is that of Lizotte et al. (2010, 2012), where a model of transition probabilities and reward function of the MOMDP is derived from historical data and then a spline-based multi-objective value iteration approach is applied to this model. Generally, learning such models appears to be only slightly more difficult than in the context of a single target, since estimates of each reward function can only be learned separately. The problem of learning the transition function, which is generally considered a hard part of modeling, is identical to setting a single target. In particular, in scenarios with multiple targets, model-based approaches to MORL could significantly reduce sample costs: once the model is learned, an entire CCS or PCS can be calculated offline without requiring additional samples."}, {"heading": "8.2 Learning Multiple Policies with Monotonically Increasing Scalarization Functions using Value Functions", "text": "If stochastic strategies are allowed, the problem is easier because we can learn CCS (see Section 4.3.3) and apply either blending strategies (Vamplew et al., 2009) or stationary randomizations (Wakuta, 1999) of strategies to this CCS. However, if only deterministic strategies are allowed, the problem is more difficult. An option might be to use an approximation of the finite horizon. By planning backwards from the planning horizon, the expected reward of t-time approaches the value of the finite horizon better and better. As mentioned in Section 5.2, similar approaches have been applied in the POMDP setting. Another way to find good approximations to non-stationary strategies might be to learn stationary strategies (perhaps by extending CON-MDP (Wiering & De Jong) to the period 2007)."}, {"heading": "8.3 Many-Objective Sequential Decision-Making", "text": "The majority of research reviewed in this article, both theoretical and applied, deals with MOMDPs with only a few objectives, reflecting the state of early evolutionary multi-objective research, which focused almost exclusively on problems with two or no more than three objectives. However, over the last decade, there has been a growing interest in evolutionary methods for so-called many objective problems that have at least four and sometimes more than fifty objectives (Ishibuchi, Tsukamoto, & Nojima, 2008).This research has shown that many algorithms that are good for some objectives scale poorly in the number of objectives that require special algorithms for the many objective settings. While many objective MDPs have received little consideration so far, there are numerous real-world control problems that can of course be modeled in this way."}, {"heading": "8.4 Expectation of Scalarized Return", "text": "In Section 3, we define the scaled value V \u03c0 w (s) to be the result of applying the scaling function f (s) to the multi-objective value V\u03c0 (s) to tow, i.e. V \u03c0 w (s) = f (V\u03c0 (s), w). Since V\u03c0 (s) itself is an expectation, this means that the scaling function is applied after calculating the expectation, i.e., V \u03c0 w (s) = f (V\u03c0 (s), w (E), w (E), w) = f (E [s), however, the scaling function is applied after calculating the expected return, i.e., V \u03c0 w (s) = f (s), w (s), w), w (E [s), w (E), w (E), w (E), w (E), w (E), w (E), w (E), w (E) is the only option that is possible. It is also possible to define V \u03c0 w (s) w (s) as the expectation value that we refer to in the literature as the expected return."}, {"heading": "9. Conclusions", "text": "This article presented an overview of algorithms designed for multi-objective sequential decision problems.In order to make explicit the circumstances under which specific methods are required to solve multi-objective problems, we have identified three different scenarios in which the transformation of such a problem into a single objective is impossible, unfeasible or undesirable. These scenarios not only provide motivation for the need for multi-objective methods, but also represent the three main ways in which these methods are applied in practice.We proposed a taxonomy that classifies multi-objective methods according to the applicable scenario, the scalarization function (which projects multi-objective values for scalar methods) and the type of policy that determines the way of determining an optimal solution that can be a single policy or coverage (convex or pareto)."}, {"heading": "Acknowledgments", "text": "We would like to thank Matthijs Spaan, Frans Oliehoek, Matthijs Snel, Marie D. Manner and Samy Sa \u0301 as well as the anonymous reviewers for their valuable feedback, which is supported by the DecisionTheoretic Control for Network Capacity Allocation Problems (# 612.001.109) project of the Dutch Organization for Scientific Research (NWO)."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}