{"id": "1402.0591", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Learning by Observation of Agent Software Images", "abstract": "Learning by observation can be of key importance whenever agents sharing similar features want to learn from each other. This paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task. This is possible because the proposed architecture displays information that is essential for observation, making it possible for software agents to observe each other. The agent architecture supports a learning process that covers all aspects of learning by observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge and evaluating the agents progress. The evaluation provides control over the decision to obtain new knowledge or apply the acquired knowledge to new problems. We combine two methods for learning from the observed information. The first one, the recall method, uses the sequence on which the actions were observed to solve new problems. The second one, the classification method, categorizes the information in the observed data and determines to which set of categories the new problems belong. Results show that agents are able to learn in conditions where common supervised learning algorithms fail, such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible. The results also show that our approach provides better results than other learning methods since it requires shorter learning periods.", "histories": [["v1", "Tue, 4 Feb 2014 01:45:26 GMT  (643kb)", "http://arxiv.org/abs/1402.0591v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["paulo roberto costa", "lu\\'is miguel botelho"], "accepted": false, "id": "1402.0591"}, "pdf": {"name": "1402.0591.pdf", "metadata": {"source": "CRF", "title": "Learning by Observation of Agent Software Images", "authors": ["Paulo Costa", "Luis Botelho"], "emails": ["paulo.costa@iscte.pt", "luis.botelho@iscte.pt"], "sections": [{"heading": null, "text": "The agent architecture supports a learning process that covers all aspects of learning through observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge, and evaluating the agent's progress. The evaluation gives control over the decision to acquire new knowledge or apply the acquired knowledge to new problems. We combine two methods to learn from the observed information. The first, the recall method, uses the sequence in which the actions were observed to solve new problems. The second, the classification method, categorizes the information in the observed data and determines to which categories the new problems belong. The results show that agents are able to learn under conditions where common supervised learning algorithms fail, e.g. when agents do not know the results of their actions from the outset or when not all the effects of the actions are visible.The results also show that our approach delivers better results than other learning methods because it requires shorter learning times."}, {"heading": "1. Introduction", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2. Literature Review", "text": "This section presents an overview of the approaches to learning through observation and to the visual representation of agents. It describes the important aspects of approaches that are related to learning through observation or that can help to solve the problems facing the application of learning through observation in software agents."}, {"heading": "2.1 The Visible Representation of Software Agents", "text": "In fact, it is so that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "2.2 Learning by Observation", "text": "The study on learning by observation shows that one of the most important aspects of an approach to learning by observation is the learning algorithm. In contrast to the learning algorithms of 2004, the learning algorithm is the best source of information. It defines how the knowledge gained from observation is stored and how it is used, that is, how the agent proposes actions that he can execute when confronted with new problems (Argall et al., 2009). One way for the learning algorithm is to follow the same sequence of actions as the expert who requires the agent to store the sequence on which he has observed the actions performed by the experts. This possibility for the learning algorithm, called sequencing, is one of the most commonly used methods of learning by observation approaches (Argall these approaches, 2009)."}, {"heading": "3. The Software Image", "text": "The section describes these new functionalities, explains the reasons for their inclusion in the software image and how they are advantageous for learning through observation. Our approach to learning through observation proposes a software image that allows software agents to learn by observing the actions of other agents. The act of observing a software image does not imply the use of computer vision; instead, agents use specialized sensors that read metadata about the observed agent. This metadata is what we call the software image defined by software objects and the relationships between them, as shown in Figure 1. Despite the current version of the software image that focuses learning through observation, we believe that this image may be useful for other purposes (for example, improving the agent's interaction with the environment through embodiment)."}, {"heading": "3.1 The Agent Sensors and the Snapshots of the Agent Activity", "text": "Machado and Botelho's (2006) version of the software image described software agents as a collection of parts with visible attributes and actors. The actors in turn described a collection of actions that the agent could perform. However, it is also important to include agent sensors in this description, especially if software agents only have access to a part of the state of the environment captured by their sensors. The information collected by the agent sensors represents the way the agent understands the world - the perspective of the agent of the world. If the sensors are not included in the static image, agents have no way of knowing whether the experts who observe them understand the world in the same way as they do. The ability to understand the world as the expert is important for a better understanding of the reasons behind the actions of the expert (Bandura, 1977; Ramachandran, 2000).Given the importance of the agent sensors they observe, our proposal for the software image image image image image image image image sequence image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image sequence image image image image image image image image image image image image sequence image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image sequence image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image and sequence image image image image image image image image agent image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image image"}, {"heading": "3.2 Additional Innovations on the Software Image", "text": "This section describes other innovations related to the software image. The most important innovation is the ability to store historical data. Historical data provide a limited amount of past snapshots, allowing observers to gather their knowledge much faster if they only observe the current actions. Without the perspective of the agent, it would be difficult to see the conditions of the agent performing the actions, that is, both the state of the environment and the instances of visible attributes he perceives."}, {"heading": "4. Learning by Observation", "text": "This section summarizes the approach to full learning through observation process, after previous work on the subject (Costa & Botelho, 2012), and provides a new insight into the learning process, focusing on important aspects such as the process of discovering and observing experts and the methods of learning from the information provided through observation; the section also describes the internal evaluation of the agent and how it affects the agent's behavior; the approach to learning through observation requires both the expert and the apprentice to have a software image, as it provides the means for comparing the agents and also the data for observation (see Section 3); and presents a global view of the learning process, which includes six activities that do not always take place in strict sequence."}, {"heading": "4.1 Discovering and Observing Expert Agents", "text": "In fact, most people are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "4.2 Learning from the Observed Snapshots", "text": "In fact, the fact is that most of them are able to determine themselves how they want, and that they are able to determine themselves."}, {"heading": "4.3 The Agent\u2019s Internal Evaluation", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5. Experimental Results", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process and a process in which there is a process, a process and a process in which there is a process, and a process in which there is and a process, and a process in which there is and a process in which there is a process, and a process in which there is a process, and a process in which there is a process in which there is a process, and a process in which there is a process in which there is a process, and a process in which there is a process in which there is a process in which there is a process and a process in which there is a process in which there is a process is a process in which there is a process and a process in which there is a process in which there is a process is a process and a process in which there is a process in which there is a process and a process in which there is a process is a process and a process in which there is a process in which there is a process is a process and a process in which there is a process in which there is a process and a process in which there is a process and a process in which there is a process and a process in which there is a process and a process and a process in which there is a process in which there is a process is a process and a process and a process in which there is a process and a process in which there is a process in which there is a process and a process and a process in which there is a process is a process and a process in which a process is a process is a process and a process and a process"}, {"heading": "5.1 Results of the Virtual Hand Scenario", "text": "In fact, most of them are able to play by the rules that they have shown in recent years, and they are able to play by the rules."}, {"heading": "5.2 Results of the Mountain Car Scenario", "text": "It is the time in which one sees oneself in a position to put oneself at the top of the world, and the time that one needs to put oneself at the top of the world, is the time that one needs to put oneself at the top of the world. (...) It is the time that one needs to put oneself at the top of the world. (...) It is the time that one needs to put oneself at the top of the world. (...) It is the time that one needs to change the world. (...) It is the time that one needs to change the world. (...) It is the time that one needs to change the world. (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (..., (...), (...), (...), (..., (...), time, (...), (...), (..., (...), time, (...), (..., (...), (...), (...), (...), (..., (...), (...), (...), (...), (..., (...), (...), (...), (...), (...), (...), (...,...,...,...,..., (...,...), (...,...,...,...,...), (...,...,...), (...,..., (...,...,...), (...,..., (...,...,...), (...,...), (...,...), (...,...,..., (...,...,...), (...), (..., (...), (...), (..., (..., (...), (...,...), (...), (..., (...), (...), (...), (..., (...,...), (...), (...,..., (...), (...), (...), (...),...,..., (...,...,...,...,...),..., (..., (..."}, {"heading": "6. Conclusions and Future Work", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "Acknowledgments", "text": "This paper reports on research carried out within the framework of the PhD programme in Computer Science and Technology of the ISCTE-Instituto Universitario de Lisboa. It is partly supported by Fundac and the FCT project PEst-OE / EEI / LA0008 / 2013 under number SFRH / BD / 44779 / 2008."}], "references": [{"title": "Imitation with ALICE: learning to imitate corresponding actions across dissimilar embodiments. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["A. Alissandrakis", "C. Nehaniv", "K. Dautenhahn"], "venue": "IEEE Transactions on,", "citeRegEx": "Alissandrakis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Alissandrakis et al\\.", "year": 2002}, {"title": "Social Learning Theory", "author": ["A. Bandura"], "venue": null, "citeRegEx": "Bandura,? \\Q1977\\E", "shortCiteRegEx": "Bandura", "year": 1977}, {"title": "Drama, a connectionist architecture for control and learning in autonomous robots", "author": ["A. Billard", "G. Hayes"], "venue": "Adaptive Behavior,", "citeRegEx": "Billard and Hayes,? \\Q1999\\E", "shortCiteRegEx": "Billard and Hayes", "year": 1999}, {"title": "Behavior Recognition for Learning from Demonstration", "author": ["E.A. Billing", "T. Hellstr\u00f6m", "L.E. Janlert"], "venue": "Proceedings of IEEE International Conference on Robotics and Automation, Alaska", "citeRegEx": "Billing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Billing et al\\.", "year": 2010}, {"title": "Simultaneous control and recognition of demonstrated behavior", "author": ["E. Billing", "T. Hellstr\u00f6m", "L.E. Janlert"], "venue": null, "citeRegEx": "Billing et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Billing et al\\.", "year": 2011}, {"title": "What your body and your living room tell my agent", "author": ["L.M. Botelho", "P. Figueiredo"], "venue": "In Proceedings of the AAMAS", "citeRegEx": "Botelho and Figueiredo,? \\Q2004\\E", "shortCiteRegEx": "Botelho and Figueiredo", "year": 2004}, {"title": "Imitation without intentionality. Using string parsing to copy the organization of behaviour", "author": ["R.W. Byrne"], "venue": "Animal Cognition,", "citeRegEx": "Byrne,? \\Q1999\\E", "shortCiteRegEx": "Byrne", "year": 1999}, {"title": "Confidence-based Robot Policy Learning from Demonstration", "author": ["S. Chernova"], "venue": null, "citeRegEx": "Chernova,? \\Q2009\\E", "shortCiteRegEx": "Chernova", "year": 2009}, {"title": "K*: An Instance-based Learner Using an Entropic Distance Measure", "author": ["J.G. Cleary", "L.E. Trigg"], "venue": "In 12th International Conference on Machine Learning,", "citeRegEx": "Cleary and Trigg,? \\Q1995\\E", "shortCiteRegEx": "Cleary and Trigg", "year": 1995}, {"title": "A Java Platform for Reinforcement Learning Experiments", "author": ["F.D. Comit\u00e9"], "venue": "Journe\u0301es Proble\u0300mes De\u0301cisionnels de Markov et Intelligence Artificielle PDMIA", "citeRegEx": "Comit\u00e9,? \\Q2005\\E", "shortCiteRegEx": "Comit\u00e9", "year": 2005}, {"title": "Software Image for Learning by Observation", "author": ["P.A.M. Costa", "L.M. Botelho"], "venue": "Proceedings of the 15th Portuguese Conference on Artificial Intelligence,", "citeRegEx": "Costa and Botelho,? \\Q2011\\E", "shortCiteRegEx": "Costa and Botelho", "year": 2011}, {"title": "Learning by Observation in Software Agents", "author": ["P.A.M. Costa", "L.M. Botelho"], "venue": "Proceedings of the 4th International Conference on Agents and Artificial Intelligence (ICAART 2012),", "citeRegEx": "Costa and Botelho,? \\Q2012\\E", "shortCiteRegEx": "Costa and Botelho", "year": 2012}, {"title": "The agent-based perspective on imitation", "author": ["K. Dautenhahn", "C.L. Nehaniv"], "venue": "Imitation in animals and artifacts,", "citeRegEx": "Dautenhahn and Nehaniv,? \\Q2002\\E", "shortCiteRegEx": "Dautenhahn and Nehaniv", "year": 2002}, {"title": "Imitation as a dual-route process featuring predictive and learning components: a biologically plausible computational model", "author": ["J. Demiris", "G. Hayes"], "venue": "Imitation in animals and artifacts (MIT Press edition).,", "citeRegEx": "Demiris and Hayes,? \\Q2002\\E", "shortCiteRegEx": "Demiris and Hayes", "year": 2002}, {"title": "Intelligence without Robots (A Reply to Brooks)", "author": ["O. Etzioni"], "venue": "AI MAGAZINE,", "citeRegEx": "Etzioni,? \\Q1993\\E", "shortCiteRegEx": "Etzioni", "year": 1993}, {"title": "Learning High-Level Behaviors from Demonstration through Semantic Networks", "author": ["B. Fonooni", "T. Hellstr\u00f6m", "Janlert", "L.-E"], "venue": "Proceedings of the 4th International Conference on Agents and Artificial Intelligence (ICAART)", "citeRegEx": "Fonooni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fonooni et al\\.", "year": 2012}, {"title": "Conceptual imitation learning: An application to human-robot interaction", "author": ["H. Hajimirsadeghi", "M. Ahmadabadi"], "venue": "Machine Learning,", "citeRegEx": "Hajimirsadeghi and Ahmadabadi,? \\Q2010\\E", "shortCiteRegEx": "Hajimirsadeghi and Ahmadabadi", "year": 2010}, {"title": "What is the significance of imitation in animals", "author": ["C. Heyes", "E. Ray"], "venue": "Advances in the Study of Behavior,", "citeRegEx": "Heyes and Ray,? \\Q2000\\E", "shortCiteRegEx": "Heyes and Ray", "year": 2000}, {"title": "Incremental learning of full body motion primitives and their sequencing through human motion observation", "author": ["D. Kulic", "C. Ott", "D. Lee", "J. Ishikawa", "Y. Nakamura"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kulic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulic et al\\.", "year": 2011}, {"title": "A developmental roadmap for learning by imitation in robots", "author": ["M. Lopes", "J. Santos-Victor"], "venue": "IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society,", "citeRegEx": "Lopes and Santos.Victor,? \\Q2007\\E", "shortCiteRegEx": "Lopes and Santos.Victor", "year": 2007}, {"title": "Software agents that learn through observation (short paper)", "author": ["J. Machado", "L. Botelho"], "venue": "Proceedings of the International Joint Conference on Autonomous Agents and MultiAgent Systems", "citeRegEx": "Machado and Botelho,? \\Q2006\\E", "shortCiteRegEx": "Machado and Botelho", "year": 2006}, {"title": "Imagem Visual do Corpo de Software: Aquisi\u00e7\u00e3o de Vocabul\u00e1rio por Observa\u00e7\u00e3o", "author": ["Machado", "J. a"], "venue": null, "citeRegEx": "Machado and a.,? \\Q2006\\E", "shortCiteRegEx": "Machado and a.", "year": 2006}, {"title": "Towards an Imitation System for Learning Robots", "author": ["G. Maistros", "G. Hayes"], "venue": "Methods and Applications of Artificial Intelligence,", "citeRegEx": "Maistros and Hayes,? \\Q2004\\E", "shortCiteRegEx": "Maistros and Hayes", "year": 2004}, {"title": "Instance-Based learning : Nearest Neighbor With Generalization", "author": ["B. Martin"], "venue": "Masters thesis,", "citeRegEx": "Martin,? \\Q1995\\E", "shortCiteRegEx": "Martin", "year": 1995}, {"title": "Studying the Role of Embodiment in Cognition", "author": ["M.J. Mataric"], "venue": "Cybernetics and Systems,", "citeRegEx": "Mataric,? \\Q1997\\E", "shortCiteRegEx": "Mataric", "year": 1997}, {"title": "Learning by observation in rhesus monkeys", "author": ["M. Meunier", "E. Monfardini", "D. Boussaoud"], "venue": "Neurobiology of learning and memory,", "citeRegEx": "Meunier et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meunier et al\\.", "year": 2007}, {"title": "Machine learning", "author": ["T. Mitchell"], "venue": null, "citeRegEx": "Mitchell,? \\Q1997\\E", "shortCiteRegEx": "Mitchell", "year": 1997}, {"title": "The essence of embodiment: A framework for understanding and exploiting structural coupling between system and environment", "author": ["T. Quick", "K. Dautenhahn", "C. Nehaniv", "G. Roberts"], "venue": "In AIP conference proceedings,", "citeRegEx": "Quick et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Quick et al\\.", "year": 2000}, {"title": "Mirror neurons and imitation learning as the driving force behind the great leap forward in human evolution", "author": ["V. Ramachandran"], "venue": "Edge Website article http://www. edge. org/3rd\\ culture/ramachandran/ramachandran\\", "citeRegEx": "Ramachandran,? \\Q2000\\E", "shortCiteRegEx": "Ramachandran", "year": 2000}, {"title": "The emerging mind: the Reith Lectures", "author": ["V. Ramachandran"], "venue": "Profile Books", "citeRegEx": "Ramachandran,? \\Q2003\\E", "shortCiteRegEx": "Ramachandran", "year": 2003}, {"title": "A Bayesian Model of Imitation in Infants and Robots. In Imitation and Social Learning in Robots, Humans, and Animals, 217\u2014-247", "author": ["R.P.N. Rao", "A.P. Shon", "A.N. Meltzoff"], "venue": null, "citeRegEx": "Rao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2004}, {"title": "Premotor cortex and the recognition of motor actions", "author": ["G. Rizzolatti", "L. Fadiga", "V. Gallese"], "venue": "Cognitive brain research,", "citeRegEx": "Rizzolatti et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Rizzolatti et al\\.", "year": 1996}, {"title": "Multiagent Hierarchical Learning from Demonstration", "author": ["K. Sullivan"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "Sullivan,? \\Q2011\\E", "shortCiteRegEx": "Sullivan", "year": 2011}, {"title": "Introduction to Sequence Learning", "author": ["R. Sun"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Sun,? \\Q2001\\E", "shortCiteRegEx": "Sun", "year": 2001}, {"title": "Reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Implementation of a Framework for Imitation Learning on a Humanoid Robot Using a Cognitive Architecture", "author": ["H. Tan"], "venue": "The Future of Humanoid Robots - Research and Applications,", "citeRegEx": "Tan,? \\Q2012\\E", "shortCiteRegEx": "Tan", "year": 2012}, {"title": "An agent-independent task learning framework", "author": ["M.A. Wood"], "venue": "Phd thesis, University of Bath", "citeRegEx": "Wood,? \\Q2008\\E", "shortCiteRegEx": "Wood", "year": 2008}], "referenceMentions": [{"referenceID": 28, "context": "Unlike natural selection, observation allows these capabilities to spread amongst individuals within the same generation (Ramachandran, 2000).", "startOffset": 121, "endOffset": 141}, {"referenceID": 7, "context": "This allows the artificial agents to directly know which actions are necessary to perform a specific task (Argall, Chernova, Veloso, & Browning, 2009; Chernova, 2009).", "startOffset": 106, "endOffset": 166}, {"referenceID": 20, "context": "As section 2 shows, to the best of our knowledge, the only approach to learning by observation in software agents is the one presented by Machado and Botelho (2006). However, in Machado and Botelho\u2019s proposal the agents were only capable of learning vocabulary whereas in the work described in this paper the agents are capable of learning control", "startOffset": 138, "endOffset": 165}, {"referenceID": 20, "context": "In addition, our approach also introduces several improvements to the software image (see section 3) initially proposed by Machado and Botelho (2006). The software image provides software agents with an accessible representation of their constituents and capabilities, the static image, and of the actions they perform, the dynamic image.", "startOffset": 123, "endOffset": 150}, {"referenceID": 20, "context": "In spite of the similarities with Machado and Botelho\u2019s (2006) approach, our approach introduced several improvements to the software image, in particular:", "startOffset": 34, "endOffset": 63}, {"referenceID": 26, "context": "It is used to compare our learning approach with a reinforcement learning (RL) algorithm in a situation in which this kind of learning algorithm has already shown to be a good approach (Mitchell, 1997).", "startOffset": 185, "endOffset": 201}, {"referenceID": 33, "context": "The second scenario presents the mountain car problem as described by Sutton and Barto (1998). It is used to compare our learning approach with a reinforcement learning (RL) algorithm in a situation in which this kind of learning algorithm has already shown to be a good approach (Mitchell, 1997).", "startOffset": 70, "endOffset": 94}, {"referenceID": 14, "context": "Because of this, software agents are not able to observe each other as tangible entities would be observed (Etzioni, 1993; Quick et al., 2000).", "startOffset": 107, "endOffset": 142}, {"referenceID": 27, "context": "Because of this, software agents are not able to observe each other as tangible entities would be observed (Etzioni, 1993; Quick et al., 2000).", "startOffset": 107, "endOffset": 142}, {"referenceID": 19, "context": "The literature overview regarding learning by observation shows that, with the exception of Machado and Botelho\u2019s (2006) approach, software agents are disregarded from the advances on learning by observation since they are usually related to robotics (Argall et al.", "startOffset": 92, "endOffset": 121}, {"referenceID": 27, "context": "Almost all software approaches for learning are constrained to observe the changes in the environment (the effects of agent actions) and the knowledge obtained to perform a task is limited to state change information (Quick et al., 2000; Argall et al., 2009).", "startOffset": 217, "endOffset": 258}, {"referenceID": 6, "context": "However, several authors have emphasized that not every action produces visible changes in the environment (Byrne, 1999; Dautenhahn & Nehaniv, 2002; Botelho & Figueiredo, 2004; Machado, 2006), thus, using state change information alone is not always a good option.", "startOffset": 107, "endOffset": 191}, {"referenceID": 6, "context": "of its actions a priori (Byrne, 1999; Dautenhahn & Nehaniv, 2002; Botelho & Figueiredo, 2004; Machado, 2006).", "startOffset": 24, "endOffset": 108}, {"referenceID": 24, "context": "To be able to learn by observing the actions of other agents, the software agent needs some kind of accessible representation of its body that displays the necessary visible features (Mataric, 1997; Botelho & Figueiredo, 2004; Machado, 2006).", "startOffset": 183, "endOffset": 241}, {"referenceID": 29, "context": "Research in neurology reveals that the human brain also uses a representation of the human body in its activities (Ramachandran, 2003).", "startOffset": 114, "endOffset": 134}, {"referenceID": 14, "context": "For Etzioni, a software agent can be situated in the software environment in the same way as a robot is situated in the physical world, if it is characterized by what it can do (its actions) and also by the kind of inputs it is able to collect from the software environment (Etzioni, 1993).", "startOffset": 274, "endOffset": 289}, {"referenceID": 19, "context": "Although Machado and Botelho\u2019s (2006) approach for the software image provides a description of the agent constituents and actions, it does not describe the kind of input the agent can collect from the software environment.", "startOffset": 9, "endOffset": 38}, {"referenceID": 14, "context": "Etzioni (1993) was one of the first authors to realize that the lack of a physical body was not an obstacle for the application of the principles of embodiment in software agents in the same way they are applied in robotics.", "startOffset": 0, "endOffset": 15}, {"referenceID": 33, "context": "It is also closely related to sequence learning in humans since it handles the same kind of problems, such as predicting the elements of a sequence based on the preceding element, finding the natural order of the elements in a sequence and selecting a sequence of actions to achieve a goal (Clegg, DiGirolamo, & Keele, 1998; Sun, 2001).", "startOffset": 290, "endOffset": 335}, {"referenceID": 6, "context": "The best way to maintain the temporal relations between the elements in the sequence consists of using the properties of the data structure where the sequences are stored (Byrne, 1999; Heyes & Ray, 2000; Kulic, Ott, Lee, Ishikawa, & Nakamura, 2011; Billing, Hellstr\u00f6m, & Janlert, 2011).", "startOffset": 171, "endOffset": 285}, {"referenceID": 6, "context": "Linear structures such as lists and vectors are the most commonly used (Byrne, 1999; Heyes & Ray, 2000).", "startOffset": 71, "endOffset": 103}, {"referenceID": 33, "context": "However these linear structures lack the ability to represent alternatives, which is an important aspect of sequential learning that opens the possibility of making choices inside the sequence (Sun, 2001).", "startOffset": 193, "endOffset": 204}, {"referenceID": 18, "context": "Each element of the sequence is represented as a node in the tree and the following element is chosen from one of the branches of that node (Kulic et al., 2011).", "startOffset": 140, "endOffset": 160}, {"referenceID": 32, "context": "This allows the agent to face future conditions that have never been observed, because in a real world situation it is almost always impossible to observe all possible conditions (Argall et al., 2009; Sullivan, 2011).", "startOffset": 179, "endOffset": 216}, {"referenceID": 30, "context": "Other hypothesis consist of using the observed conditions and actions to feed statistical approaches such as Bayesian algorithms and Hidden Markov models (HMMs) (Rao et al., 2004; Hajimirsadeghi & Ahmadabadi, 2010).", "startOffset": 161, "endOffset": 214}, {"referenceID": 7, "context": "The conditions and actions can also be used to train supervised learning algorithms, such as the classification algorithms (Argall et al., 2009; Chernova, 2009; Sullivan, 2011).", "startOffset": 123, "endOffset": 176}, {"referenceID": 32, "context": "The conditions and actions can also be used to train supervised learning algorithms, such as the classification algorithms (Argall et al., 2009; Chernova, 2009; Sullivan, 2011).", "startOffset": 123, "endOffset": 176}, {"referenceID": 2, "context": "One way of generalizing the acquired knowledge is using the observed conditions and actions to train neural networks, as described by Billard and Hayes (1999). Other hypothesis consist of using the observed conditions and actions to feed statistical approaches such as Bayesian algorithms and Hidden Markov models (HMMs) (Rao et al.", "startOffset": 134, "endOffset": 159}, {"referenceID": 35, "context": "also include the agent\u2019s motivation to learn, the discovery and observation of agents, the storage and interpretation of the information acquired in observation and the application of the newly acquired knowledge (Demiris & Hayes, 2002; Tan, 2012).", "startOffset": 213, "endOffset": 247}, {"referenceID": 35, "context": "One of the major flaws detected on the surveyed approaches, besides their focus on robot agents, was the fact that this global view is still missing (Tan, 2012).", "startOffset": 149, "endOffset": 160}, {"referenceID": 36, "context": "The evaluation allows the agent to measure how its performance is affected both when it is consolidating the knowledge acquired from observation or when executing actions (Wood, 2008; Hajimirsadeghi & Ahmadabadi, 2010).", "startOffset": 171, "endOffset": 218}, {"referenceID": 1, "context": "Their approach presents a learning process that, excluding motivation, is consistent with Bandura\u2019s (1977) social learning theory, which approximates their approach to learning by observation in humans and superior mammals.", "startOffset": 90, "endOffset": 107}, {"referenceID": 1, "context": "Their approach presents a learning process that, excluding motivation, is consistent with Bandura\u2019s (1977) social learning theory, which approximates their approach to learning by observation in humans and superior mammals. The inclusion of an internal evaluation to Demiris and Hayes\u2019s (2002) approach provides the learning process with a simple motivation mechanism.", "startOffset": 90, "endOffset": 294}, {"referenceID": 36, "context": "The agent can be intrinsically motivated to learn because it knows when it has acquired sufficient knowledge to perform a task on its own or because it detects that portions of its knowledge need improvement and thus require the agent to go back learning (Wood, 2008; Billing, Hellstr\u00f6m, & Janlert, 2010).", "startOffset": 255, "endOffset": 304}, {"referenceID": 36, "context": "The agent can be intrinsically motivated to learn because it knows when it has acquired sufficient knowledge to perform a task on its own or because it detects that portions of its knowledge need improvement and thus require the agent to go back learning (Wood, 2008; Billing, Hellstr\u00f6m, & Janlert, 2010). The ability to enhance the agent\u2019s knowledge through new observations is an important factor for learning by observation. According to Argall et al. (2009), one of the downsides of learning by observation is the fact that the agent\u2019s knowledge is limited to what it was able to observe.", "startOffset": 256, "endOffset": 462}, {"referenceID": 32, "context": "Several authors use specialized experts, or teachers, who monitor and reinforce the agent\u2019s actions (Sullivan, 2011; Hajimirsadeghi & Ahmadabadi, 2010; Chernova, 2009).", "startOffset": 100, "endOffset": 167}, {"referenceID": 7, "context": "Several authors use specialized experts, or teachers, who monitor and reinforce the agent\u2019s actions (Sullivan, 2011; Hajimirsadeghi & Ahmadabadi, 2010; Chernova, 2009).", "startOffset": 100, "endOffset": 167}, {"referenceID": 32, "context": "The teacher can also decide when the agent needs to acquire more knowledge (Sullivan, 2011).", "startOffset": 75, "endOffset": 91}, {"referenceID": 7, "context": "This allows the agent to request the teacher to perform a specific task (Chernova, 2009).", "startOffset": 72, "endOffset": 88}, {"referenceID": 28, "context": "This allows the agents to \u201dfeel\u201d like they are performing the actions they observe on an expert (Ramachandran, 2000), which greatly improves the easiness of identifying the observed actions, grounding them in the agent own actions.", "startOffset": 96, "endOffset": 116}, {"referenceID": 5, "context": "In this case, the agent\u2019s internal confidence builds on the successes and failures of the actions that were previously proposed instead of the metrics on the current actions, provided by the learning algorithm, as in the work of Chernova (2009), and Billing et al.", "startOffset": 229, "endOffset": 245}, {"referenceID": 3, "context": "In this case, the agent\u2019s internal confidence builds on the successes and failures of the actions that were previously proposed instead of the metrics on the current actions, provided by the learning algorithm, as in the work of Chernova (2009), and Billing et al. (2010).", "startOffset": 250, "endOffset": 272}, {"referenceID": 30, "context": "Several approaches use specific structures, such as the forward models, to emulate the behaviour of mirror neurons (Rizzolatti, Fadiga, & Gallese, 1996; Demiris & Hayes, 2002; Maistros & Hayes, 2004; Rao et al., 2004; Lopes & Santos-Victor, 2007).", "startOffset": 115, "endOffset": 246}, {"referenceID": 18, "context": "The main objective of these kind of structures is to create a distinction between the description of the action and its execution, that is, to create abstract representations of agent actions (Kulic et al., 2011).", "startOffset": 192, "endOffset": 212}, {"referenceID": 20, "context": "Like in Machado and Botelho\u2019s (2006) proposal, the elements of the agent software image are arranged in two categories, the static image and the dynamic image.", "startOffset": 8, "endOffset": 37}, {"referenceID": 20, "context": "The improvements on Machado and Botelho\u2019s (2006) proposal for the software image consist of including the agent sensors in the description of its components (the static image), combining the information on the state of the software environment (provided by the agent sensors) with information on important aspects of the agent\u2019s internal state with the observed actions and enabling the representation of composite actions, that is, actions composed of sequences of simpler actions.", "startOffset": 20, "endOffset": 49}, {"referenceID": 1, "context": "expert is important for a better understanding of the reasons behind the expert\u2019s actions (Bandura, 1977; Ramachandran, 2000).", "startOffset": 90, "endOffset": 125}, {"referenceID": 28, "context": "expert is important for a better understanding of the reasons behind the expert\u2019s actions (Bandura, 1977; Ramachandran, 2000).", "startOffset": 90, "endOffset": 125}, {"referenceID": 20, "context": "Unlike Machado and Botelho\u2019s (2006) proposal where agents could only observe the action being currently performed, in our proposal, agents acquire snapshots of the activity of the observed agent.", "startOffset": 7, "endOffset": 36}, {"referenceID": 1, "context": "2), the agents follow Bandura\u2019s (1977) social learning theory and learn by observing a similar expert, that is, an expert whose static image has the same structure and the same instances of the atomic elements as the agent\u2019s static image.", "startOffset": 22, "endOffset": 39}, {"referenceID": 1, "context": "2), the agents follow Bandura\u2019s (1977) social learning theory and learn by observing a similar expert, that is, an expert whose static image has the same structure and the same instances of the atomic elements as the agent\u2019s static image. The agent uses the comparison functionalities of the software image described by Costa and Botelho (2011) to compare its static image SIagent with the static image of the expert SIexpert.", "startOffset": 22, "endOffset": 345}, {"referenceID": 8, "context": "Previous experiments (Costa & Botelho, 2012) revealed that the most suited algorithms for the classification method are the KStar from Cleary and Trigg (1995) and the NNGE (Nearest Neighbour like algorithm using non-nested Generalized Exemplars from Martin, 1995).", "startOffset": 135, "endOffset": 159}, {"referenceID": 34, "context": "The second scenario compares our approach with a reinforcement learning approach in a typical reinforcement learning experiment, the mountain car experiment from Sutton and Barto (1998). Through this scenario we present a direct comparison between the solutions provided by a reinforcement learning algorithm and by our approach to learning by observation.", "startOffset": 162, "endOffset": 186}, {"referenceID": 32, "context": "The adoption of learning by observation solutions is relatively new in computer science and, as the latest approaches show, it is still under development (Sullivan, 2011; Kulic et al., 2011; Tan, 2012; Fonooni, Hellstr\u00f6m, & Janlert, 2012).", "startOffset": 154, "endOffset": 238}, {"referenceID": 18, "context": "The adoption of learning by observation solutions is relatively new in computer science and, as the latest approaches show, it is still under development (Sullivan, 2011; Kulic et al., 2011; Tan, 2012; Fonooni, Hellstr\u00f6m, & Janlert, 2012).", "startOffset": 154, "endOffset": 238}, {"referenceID": 35, "context": "The adoption of learning by observation solutions is relatively new in computer science and, as the latest approaches show, it is still under development (Sullivan, 2011; Kulic et al., 2011; Tan, 2012; Fonooni, Hellstr\u00f6m, & Janlert, 2012).", "startOffset": 154, "endOffset": 238}, {"referenceID": 18, "context": "The adoption of learning by observation solutions is relatively new in computer science and, as the latest approaches show, it is still under development (Sullivan, 2011; Kulic et al., 2011; Tan, 2012; Fonooni, Hellstr\u00f6m, & Janlert, 2012). With the exception of our work (Costa & Botelho, 2011, 2012) and Machado and Botelho\u2019s (2006) work, all the contributions for learning by observation are focused on robotic agents and their physical properties.", "startOffset": 171, "endOffset": 334}], "year": 2013, "abstractText": "Learning by observation can be of key importance whenever agents sharing similar features want to learn from each other. This paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task. This is possible because the proposed architecture displays information that is essential for observation, making it possible for software agents to observe each other. The agent architecture supports a learning process that covers all aspects of learning by observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge and evaluating the agent\u2019s progress. The evaluation provides control over the decision to obtain new knowledge or apply the acquired knowledge to new problems. We combine two methods for learning from the observed information. The first one, the recall method, uses the sequence on which the actions were observed to solve new problems. The second one, the classification method, categorizes the information in the observed data and determines to which set of categories the new problems belong. Results show that agents are able to learn in conditions where common supervised learning algorithms fail, such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible. The results also show that our approach provides better results than other learning methods since it requires shorter learning periods.", "creator": "TeX"}}}