{"id": "1402.0645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Local Gaussian Regression", "abstract": "Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently. Gaussian (process) regression, on the other hand, provides a fully generative model without significant formal requirements on the distribution of training data, but has much higher computational cost and usually works with one global scale per input dimension. Using a localising function basis and approximate inference techniques, we take Gaussian (process) regression to increasingly localised properties and toward the same computational complexity class as locally weighted regression.", "histories": [["v1", "Tue, 4 Feb 2014 07:35:48 GMT  (418kb,D)", "http://arxiv.org/abs/1402.0645v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["franziska meier", "philipp hennig", "stefan schaal"], "accepted": false, "id": "1402.0645"}, "pdf": {"name": "1402.0645.pdf", "metadata": {"source": "CRF", "title": "Local Gaussian Regression", "authors": ["Franziska Meier", "Philipp Hennig", "Stefan Schaal"], "emails": [], "sections": [{"heading": null, "text": "Locally weighted regression was developed as a non-parametric learning method that is computationally efficient, able to learn from very large amounts of data and gradually add data. An interesting feature of locally weighted regression is that it can work with spatially different length scales, an advantageous property for example in control problems. However, it does not offer a generative model for functional values and requires that training and test data be generated identically and independently. Gaussian (process) regression, on the other hand, provides a completely generative model without significant formal requirements for the distribution of training data, but has much higher compression costs and usually operates on a global scale per input dimension. Using a localization function base and approximate inference techniques, we take Gaussian (process) regression to increasingly localized properties and to the same compression complexity class as locally weighted regression."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to outdo themselves, and they are able to outdo themselves, \"he said.\" But it's not that they're able to outdo themselves. \"Indeed,\" It's not that they're able to outdo themselves, but that they're able to outdo themselves. \"Indeed,\" It's not that they're able to outdo themselves. \""}, {"heading": "2 Background", "text": "Both LWR and Gaussian regression have already been extensively studied, so we will give only brief introductions here. (generalized linear regression charts) Weights w-RF to the nonlinear function f: RD _ R via F functions \u03c6i (x): RD (x) = F-RS (generalized linear regression charts). (1) Use of function values at N locations xn (x) = F-RD, subsumed in the matrix X-RN \u00b7 D, are f (X) = local weighted regression (LWR), the elements of which are included in M local regression models. We assume that each local model has K functions (x), so that the m-th model is the prediction of x isfm (x) = K-RS (x)."}, {"heading": "3 Local Parametric Gaussian Regression", "text": "In Gaussian regression with RBF characteristics as described above, without changing anything, the characteristics \u03c6m (x) = \u03b7m (x) can be interpreted as M-constant function models (x) = 1, localized by the RBF function, \u03c6 (x) = [\u04451 (x) \u03b71 (x),.. This representation extends to more elaborate local models. Thus, for example, a local weighted regression with linear local models results. Supplemented to M-local models, each consisting of K parameters, the function mmmk combines the kth component of the local model \u043fkm (xn), localized by the m-weighting function."}, {"heading": "3.1 Learning in Local Bayesian Linear Regression", "text": "The exact inference in Gaussian regression with localized characteristic functions is related to the cubic costs of their non-local origins. However, due to localized characteristic functions, the correlation between distant local models is approximately 0, so the inference between local models is approximately independent. In this section, we intend to use this \"almost independence\" to derive a localized, approximate low-cost inference scheme, similar to the LWR. To arrive at this localized learning algorithm, we first introduce a latent variable for each local model m and each data point xn, similar to the probabilistic backfitting. [14] Intuitively, the f-form will form approximate targets, one for each local model against which the local regression parameters fit (see also Figure 1, center). This modified model motivates a factorizing limit of variation constructed in Section 3.1.1, whereby the theories Franziska Meier1, Philipp, and Stefan Schaalin are independent of the approximate magnitude 1.2."}, {"heading": "3.1.1 Variational Bound", "text": "The complete data probability of the modified model isp (y, f, w) q (TB = TB = TB = TB = TB (TB = TB = TB = TB = TB (TB = TB (TB = TB)) (16) N \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 W (W) W \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z (W). We treat w and f as probable variables and estimates."}, {"heading": "3.1.2 Optimizing Hyperparameters", "text": "We set the model parameters \u03b8 = {\u03b2y, {\u03b2fm, \u03bbm} Mm = 1, {\u03b1mk} to maximize the expected complete log probability below the limit of variation, Ef, w [log p (y, f, w, p, p, v, v, v, v)] = (24) Ef, w {N, n = 1 [logN (yn; M, m = 1 fnm, \u03b2 \u2212 1 y) + M, m = 1 logN (fnm; wTm\u03c6nm, \u03b2 \u2212 1fm)] + M, m = 1 logN (wm; 0, A \u2212 1m)} Setting the gradient of this expression to zero results in the following update equations for the variance \u03b2 \u2212 1y = 1N N N, n N (fnm; wTm\u03c6nm, \u03b2 \u2212 1f, \u03b2 \u2212 1fm)]] + 1\u00b5fn) 2 + 1Tvfn, \u03b2 \u2212 1 logN (wm, c; c, A \u2212 1m)}} Setting the gradient of this expression to zero results in the following actualization equations for the variance \u03b2 \u2212 1y = 1N, c N, c = 1N \u2212 N N, c \u2212 1N N N, c \u2212 1fm, c \u2212 1m), c N \u2212 1m), c N, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "3.1.3 Prediction", "text": "The prediction at a test point results from the marginalization of both w and f using \"N\" (y), \"1Tf,\" \"\u03b2\" (1y) N (f), \"W\" (x), \"df\" = N (y), \"mwTm\u03c6\" m, \"\u03b2\" \u2212 1 y + 1TB \u2212 11) (29) and \"N\" (y), \"wT\u03c6,\" \"\u03b2 \u2212 1y + 1TB \u2212 11) N (w; mwTm\u03c6\" m, \"m\" (y) dw = N (y), \"m\" m\u00b5Twm\u03c6 \"m,\" 2 (x) (30), \"\u03c32\" (x) = \u03b2 \u2212 1y + M \u00b2 m \u00b2 m = 1\u03b2 \u2212 1 fm + M \u00b2 m = 1\u03c6 \"m\" m \"m\" (31), which is linear in M and K."}, {"heading": "3.2 Incremental Adding of Local Models", "text": "An extension analogous to the incremental learning of the relevance vector machine [16] can be used to add local models to new, greedily selected locations cM + 1. The resulting algorithm starts with a local model, and per iteration one local model is added in the variation step. Conversely, for each selected location cM + 1 we check whether one of the existing local models c1: M produces a localizing weight (cM + 1) \u2265 wgen, where wgen is a parameter between 0 and 1, and regulates how many parameters are added."}, {"heading": "4 Extension to Finitely Many Local Nonparametric Models", "text": "An interesting component of the local Gaussian regression is that it easily extends to a model with a finite number of local non-parametric Gaussian process models. Marginalization of the weights wm in eq. (17) results in the marginal p (y, f, f, X, \u03b8) = N (y; 1 f, \u03b2 \u2212 1y IN) M \u0435m = 1N (fm; 0, Cm) (32), where p (fm) = N (fm; 0, Cm) is a Gaussian process before the function fm with a (degenerated) covariance function Cm = \u03b2 \u2212 1fmIN + inspTmA \u2212 1m\u03c6m. Replacing the finite number of local features greim with an infinite number of features results in the local non-parametric model with covariance function contrim (x, x) within the covariance function \u0441m (x, x) within the (x, x) within the variance (x)."}, {"heading": "5 Experiments", "text": "For the second comparison, we learn the inverse dynamics of an anthropomorphic SARCOS robot arm with seven degrees of freedom [8]. To allow comparison with the mixture of experts, we assume linear expert models. A before the form N (wm; 0, A \u2212 1m), each expert uses regression parameters and normalized Gaussian nuclei for the mixture components to make them as comparable as possible. To calculate the rear part in the e-step, a mean field approximation is used. In both experiments, LWPR performed several cycles through the data sets. Both the local Gaussian regression and the mixture of experts are executed on the basis of an additional convergence of 1000 algorithms."}, {"heading": "5.1 Data from the \u2018Cross Function\u2019", "text": "The test set is a regular grid of 1641 edges without noise and is used to evaluate how well the underlying function is covered. The initial longitudinal scale was set on Higgs = 0.3, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs Higgs, Higgs Higgs, Higgs Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs Higgs, Higgs Higgs Higgs Higgs, Higgs, Higgs Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs, Higgs,"}, {"heading": "5.2 Inverse Dynamics Learning Task", "text": "The SARCOS data contains 44, 484 training data points and 4,449 test data points. The 21 input variables represent joint positions, velocities and acceleration for the 7 joints. The task is to predict the 7 joint torques. In Table 2 we show the predictive power of LWPR, ME and LGR when trained with longitudinal learning methods and at wgen = 0.3. LGR performs better than LWPR and ME in terms of accuracy for almost all joints. However, the real advantage of LGR is that the number of hand-tuned parameters is reduced to specifying the learning rate for updating the gradient and setting the parameter."}, {"heading": "6 Conclusion", "text": "We have taken a top-down approach to develop a probable localized regression algorithm: We start with the generative model of Gaussian generalized linear regression, which amounts to a fully connected graph and therefore has cubic inference costs. To break down the calculation costs of the inference, we first introduce the idea of localized feature functions as local models that can be extended to local nonparametric models. In a second step, we argue that these local models are almost independent due to localization. We take advantage of this fact by a variation approach that reduces the complexity of compression to local compressions. Empirical evaluations suggest that LGR successfully solves the problem of adapting locally weighted regressions of inherent hyperparameters. A final step for future work is the formulation of our algorithm into an incremental version that can handle a continuous stream of incoming data Franziska Meier1, Stefan Schaalg2."}], "references": [{"title": "Constructive incremental learning from only local information", "author": ["Stefan Schaal", "Christopher G Atkeson"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Local learning algorithms", "author": ["L\u00e9on Bottou", "Vladimir Vapnik"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Bayesian methods for mixtures of experts", "author": ["Steve Waterhouse", "David MacKay", "Tony Robinson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["Lauren Hannah", "David M Blei", "Warren B Powell"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Bayesian Kernel Shaping for Learning Control", "author": ["Jo-Anne Ting", "Mrinal Kalakrishnan", "Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Kernel carpentry for online regression using randomly varying coefficient model", "author": ["Narayanan U Edakunni", "Stefan Schaal", "Sethu Vijayakumar"], "venue": "In Proceedings of the international joint conference on artificial intelligence (IJCAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Universal kernels", "author": ["C.A. Micchelli", "Y. Xu", "H. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Bayesian Gaussian processes for regression and classification", "author": ["M. N Gibbs"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Infinite mixtures of Gaussian process experts. In Advances in neural information processing systems", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Bayesian learning for neural networks", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "The bayesian backfitting relevance vector machine", "author": ["Aaron D\u2019Souza", "Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M.K. Titsias"], "venue": "In Artificial Intelligence & Statistics (AISTATS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Incremental gaussian processes", "author": ["Joaquin Quinonero-Candela", "Ole Winther"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Locally weighted projection regression: An o (n) algorithm for incremental real time learning in high dimensional space", "author": ["Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Locally weighted regression (LWR) [1] makes use of the popular and well-studied idea of local learning [2] to address the task of compressing large amounts of data into a small number of parameters.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Locally weighted regression (LWR) [1] makes use of the popular and well-studied idea of local learning [2] to address the task of compressing large amounts of data into a small number of parameters.", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "An initial candidate could be the mixture of experts model (ME) [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "Indeed, it has been argued [1], that LWR can be thought of as a mixture of experts model in which experts are trained independently of each other.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "The advantage of ME is that it comes with a full generative model [4, 5] allowing for principled learning of parameters and expert allocations, while LWR does not (see Figure 1).", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "The advantage of ME is that it comes with a full generative model [4, 5] allowing for principled learning of parameters and expert allocations, while LWR does not (see Figure 1).", "startOffset": 66, "endOffset": 72}, {"referenceID": 5, "context": "Previous work on probabilistic formulations of local regression [6, 7] has been focussed on bottom-up constructions, trying to find generative models for one local model at a time.", "startOffset": 64, "endOffset": 70}, {"referenceID": 6, "context": "Previous work on probabilistic formulations of local regression [6, 7] has been focussed on bottom-up constructions, trying to find generative models for one local model at a time.", "startOffset": 64, "endOffset": 70}, {"referenceID": 8, "context": "Gaussian, squareexponential) features from Equation (3), \u03c6i(x) = \u03b7i(x), (for F =M) enjoy particular popularity for various reasons, including algebraic conveniences and the fact that their associated reproducing kernel Hilbert space lies dense in the space of continuous functions [9].", "startOffset": 281, "endOffset": 284}, {"referenceID": 9, "context": "There are some special kernels of locally varying regularity [10], and mixture descriptions offer a more discretely varying model class [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "There are some special kernels of locally varying regularity [10], and mixture descriptions offer a more discretely varying model class [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Since it will become necessary to prune out unnecessary parts of the model, we adopt the classic idea of automatic relevance determination [12, 13] using a factorizing prior p(w\u2223A) = M \u220f m=1N (wm; 0,A\u22121 m ) with (14) Am = diag(\u03b1m1, .", "startOffset": 139, "endOffset": 147}, {"referenceID": 12, "context": "Since it will become necessary to prune out unnecessary parts of the model, we adopt the classic idea of automatic relevance determination [12, 13] using a factorizing prior p(w\u2223A) = M \u220f m=1N (wm; 0,A\u22121 m ) with (14) Am = diag(\u03b1m1, .", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "To arrive at this localised learning algorithm we first introduce a latent variable fnm for each local model m and data point xn, similar to probabilistic backfitting [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 14, "context": "[15]) that these distributions are Gaussian in both w and f .", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "An extension analogous to the incremental learning of the relevance vector machine [16] can be used to iteratively add local models at new, greedily selected locations cM+1.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "We evaluate and compare to mixture of experts and locally weighted projection regression (LWPR) \u2013 an extension of LWR suitable for regression in high dimensional space [17] \u2013 on two data sets.", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "For the second comparison we learn inverse dynamics of a SARCOS anthropomorphic robot arm with seven degrees of freedom [8].", "startOffset": 120, "endOffset": 123}], "year": 2014, "abstractText": "Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently. Gaussian (process) regression, on the other hand, provides a fully generative model without significant formal requirements on the distribution of training data, but has much higher computational cost and usually works with one global scale per input dimension. Using a localising function basis and approximate inference techniques, we take Gaussian (process) regression to increasingly localised properties and toward the same computational complexity class as locally weighted regression.", "creator": "LaTeX with hyperref package"}}}