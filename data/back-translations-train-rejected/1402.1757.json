{"id": "1402.1757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2014", "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited Communication", "abstract": "This paper investigates multi-agent frequencybased patrolling of intersecting, circle graphs under conditions where graph nodes have non-uniform visitation requirements and agents have limited ability to communicate. The task is modeled as a partially observable Markov decision process, and a reinforcement learning solution is developed. Each agent generates its own policy from Markov chains, and policies are exchanged only when agents occupy the same or adjacent nodes. This constraint on policy exchange models sparse communication conditions over large, unstructured environments. Empirical results provide perspectives on convergence properties, agent cooperation, and generalization of learned patrolling policies to new instances of the task. The emergent behavior indicates learned coordination strategies between heterogeneous agents for patrolling large, unstructured regions as well as the ability to generalize to dynamic variation in node visitation requirements.", "histories": [["v1", "Fri, 7 Feb 2014 20:51:00 GMT  (535kb)", "http://arxiv.org/abs/1402.1757v1", null]], "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["tao mao", "laura ray"], "accepted": false, "id": "1402.1757"}, "pdf": {"name": "1402.1757.pdf", "metadata": {"source": "CRF", "title": "Frequency-Based Patrolling with Heterogeneous Agents and Limited Communication", "authors": ["Tao Mao", "Laura E. Ray"], "emails": [], "sections": [{"heading": null, "text": "Key words: frequency-based patrols, multi-agent systems, decentralised reinforcement learning."}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "2 Problem definition", "text": "Definition 1 graph. [12] A graph is an ordering pair G = (V, E), consisting of a series of n corners (or nodes) V = {1, 2, 3... n} and a series of | E | edges: E V \u00b7 V. An edge e is a pair e = (i, j) E. A graph G is undirected if each patrol problem represents a patrolled node and an edge a possible route choice for the patrol problem. Definition 2 General patrols of agents patrol an area represented by an undirected graph G = (i, j) In the patrol problem, a vertex represents a patrolled node and an edge a possible way choice for the patrol problem. A group of r agents patrol an area represented by an undirected graph G = (V, E), an ungraced graph (V)."}, {"heading": "3 Reinforcement learning for multiagent Frequency based patrolling", "text": "The RL problem, which is Markov Decision Process, can be described as a tuple,, R S A p \u03c1, where R is the set of learning agents; S is the set of possible states; A is the set of measures from which the agents can choose; []: 0.1p S A S \u00b7 \u00b7 \u00b7 \u2192 is the state transition probability;: S A S\u03c1 \u00b7 \u00b7 \u00b7 \u2192 R is the reward function [10]. RL, Q-Learning in particular defines a state action value: Q S \u00b7 \u2192 R, which evaluates the value of a given action under a state. Q-Learning is an RL algorithm, which aims to find satisfactory approximations of Q values during the learning process and uses these Q values to obtain the best action policy for a given system. The state action value Q is updated at the end of each episode, and the learned value function Qt approaches the optimal value function as the algorithm, if the conditions of learning are met."}, {"heading": "4 Policy generation and exchange via inter-agent communication", "text": "Since communication is sparse, the information that is transmitted when communication takes place is important. The state that represents the environment is partially observable = X = X = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S"}, {"heading": "5 Results", "text": "In this section, empirical results are presented based on two patrol configurations - a benchmark configuration consisting of a single circular route with uniform requirements for the frequency of patrols (Fig. 1a) and a more general configuration consisting of three intersecting pie charts, each node having a different number of patrol nodes and different requirements for the frequency of patrols (Fig. 1b). Each node has a specified component frequency Ci, and11 nii C = \u2264 (6) equivalent 6 ensuring that the team has sufficient total capacity to perform the patrol task. Our visitor frequency notation assumes that each agent has a capacity of 100% and thus the total capacity is 100%, whereby r is the number of agents. Then the visitor frequency requirement for the node i is used as 100% ri iF = learning to act unselectively in a learning phase."}, {"heading": "5.1 Single-circle multi-agent patrolling", "text": "The benchmark configuration of Fig. 1a has n = 20 nodes, r = 2 patrol officers, component frequency Ci = 4.5% for each node and uniform patrol frequency requirements of Fi = 9% for each node; therefore, the system is overstaffed in the sense that agents overall have sufficient capacity to meet the visitor frequency requirements. This configuration is designed to test the learning algorithm in cases where there are several obvious solutions. Here, the range of measures that meet a uniform patrol frequency varies: agents that traverse the path in a single direction, the graphics being equally split between agents or agents who each take random steps with uniform probability, comprising solutions that meet that requirement. In the absence of additional criteria, all solutions are satisfactory, or acceptable solutions. An RL approach generally converges to more stodeterrent solutions when two ministerial measures are selected with equal probability."}, {"heading": "5.2 Multi-circle multi-agent patrolling", "text": "In the general configuration, 20 nodes are distributed in three circles, as in Fig. 1b. Some nodes are in two circles and appear twice in number. Component frequencies for the 20 nodes are not distributed evenly, while the requirements for consolidation are high."}, {"heading": "5.3 Dynamic multi-agent patrolling", "text": "Due to the number of learning steps required to establish a patrol, it is desirable that a learned policy is generalized to avoid discrepancies in node requirements and number of nodes. In the first variant, the frequency requirements for node 4 and node 14 are reversed, with node 4 having requirements that decrease from 8% to 19% and node 14, and in the second variant, a scenario for adding and removing a node is achieved by setting the frequency requirement for the node 2% to zero and adding a new node between nodes 19 and 20 with a frequency of 14%."}, {"heading": "6 Conclusions", "text": "We apply Q-Learning based on partially observable states to a patrol problem based on the frequency of multiple agents, where the frequency requirements for each node may be inconsistent and communication is limited. Transition matrices describing the likelihood of state transitions are derived from Markov chains, and policies are exchanged when two agents occupy the same or adjacent node. In both a benchmark configuration with uniform requirements for the frequency of patrols and a configuration with multiple diagrams and non-uniform requirements, multi-agent patrol strategies embodying cooperation are learned. Empirical results show that the patrol guidelines derived from static configurations can be applied to new instances of patrol task, with fluctuations in the number of nodes or changes in requirements for the frequency of patrols. The loosening of uniform frequency requirements and modeling of communication as a large-scale patrol model make the patrol model unrealistic for large-scale structured patrols."}, {"heading": "7 Acknowledgements", "text": "This work is supported by the Office of Naval Research as part of the Multi-University Research Initiative (MURI) with grant number N00014-08-1-0693."}, {"heading": "8 References", "text": "[1] Almeida, A., Ramalho, G., Santana, H., and et. al. [2] Elmaliach, Y., Shiloni, A., and Kaminka, G. [3] Elmaliach, Y., and Kaminka, G. \"A realistic model of frequency-based multi-robot patrol\"; Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent System, pp. 63-70, 2008. [3] Elmaliach, Y., Shiloni, A., and Kaminka, G. \"Frequency-based multi-robot distribution patrolling\"; Technical Report, Computer Science Department, Bar Ilan University, Israel, 2008. [4] Elmaliach, Y., Agmon, N., and Kaminka, G. \"Multirobot area control under frequency constraints.\""}], "references": [{"title": "Recent advances on multi-agent patrolling\u201d", "author": ["A. Almeida", "G. Ramalho", "H. Santana", "et. al"], "venue": "Advances in Artificial Intelligence-SBIA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A realistic model of frequency-based multi-robot polyline patrolling\u201d", "author": ["Y. Elmaliach", "A. Shiloni", "G. Kaminka"], "venue": "Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent System,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Frequency-based multi-robot fence patrolling\u201d", "author": ["Y. Elmaliach", "A. Shiloni", "G. Kaminka"], "venue": "Technical Report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Multirobot area control under frequency constraints\u201d", "author": ["Y. Elmaliach", "N. Agmon", "G. Kaminka"], "venue": "Proceedings of IEEE International Conference on Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "MSP algorithm: multi-robot patrolling based on territory allocation using balanced graph partitioning\u201d", "author": ["D. Portugal", "R. Rocha"], "venue": "Proceedings of 25th ACM Symposium on Applied Computing, Special Track on Intelligent Robotic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Theoretical analysis of the multi-agent patrolling problem\u201d", "author": ["Y. Chevaleyre"], "venue": "Proceedings of IEEE International Conference on Intelligent Agent Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A hybrid approach based on multi-agent geosimulation and reinforcment learning to solve a UAV partolling problem\u201d", "author": ["J. Perron", "B. Moulin", "J. Berger", "et. al"], "venue": "Proceedings of the 2008 Winter Simulation Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multi-agent movement coordination in patrolling\u201d", "author": ["A. Machado", "A. Almeida", "G. Ramaldo", "et. al"], "venue": "Proceedings of 3rd Intl\u2019 Conf. on Computers and Games,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Multi-robot uniform frequency coverage of significant locations in the environment\u201d", "author": ["M. Baglietto", "G. Cannata", "F. Capezio", "A. Sgorbissa"], "venue": "Distributed Autonomous Robotic Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Reinforcement Learning: An Introduction.", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Discrete Mathematics", "author": ["L. Lovasz", "J. Pelikan", "K. Vesztergombi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Graph Theory (translation of Graphentheorie)", "author": ["R. Diestel"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Nodes can have different visitation requirements owing to their prominence, patrolled site structure, or activity level within the region [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "In prior work, every point on the polygon is generally treated as having equal visitation requirements, and the objective is to either minimize the frequency variance or maximize the average/minimal frequency [2, 3].", "startOffset": 209, "endOffset": 215}, {"referenceID": 2, "context": "In prior work, every point on the polygon is generally treated as having equal visitation requirements, and the objective is to either minimize the frequency variance or maximize the average/minimal frequency [2, 3].", "startOffset": 209, "endOffset": 215}, {"referenceID": 1, "context": "further develop solutions to this uniform patrolling problem for open polygons [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "further develop solutions to this uniform patrolling problem for open polygons [2, 3].", "startOffset": 79, "endOffset": 85}, {"referenceID": 4, "context": "In graph-based patrolling with uniform patrol rates for each node, the objective is to either maximize visitation frequency for every point [5] or minimize idle time for every point [6~8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "For a single agent, the patrolling problem becomes a Travelling Salesman Problem [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 71, "endOffset": 77}, {"referenceID": 4, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "In multi-agent cases, Hamilton circuit finding is given first priority [5, 6]; if Hamilton circuits do not exist, the algorithms in [5] find the longest path and include outliers, while partition-based strategies are applied in [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "Reinforcement learning is applied in [7], which seeks an optimal graph partition for each member of the team to patrol.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "The graph model can be abstracted further in that patrolling efforts are focused on nodes, while edge lengths are ignored and synchronic node visits take place [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "As in [9], we focus on patrolled nodes, ignoring edge lengths and assuming synchronic node visitation.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "For this reason, the problem is cast into a reinforcement learning (RL) framework [10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "[12] A graph is an order pair G = (V, E) consisting of a set of n vertices (or nodes) V={1, 2, 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] An undirected graph G = (V, E) is considered a circle C = (Vc,Ec) if it has only the set of edges E= {(i1,i2), (i2,i3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "reward function [10].", "startOffset": 16, "endOffset": 20}], "year": 2011, "abstractText": "This paper investigates multi-agent frequencybased patrolling of intersecting, circle graphs under conditions where graph nodes have non-uniform visitation requirements and agents have limited ability to communicate. The task is modeled as a partially observable Markov decision process, and a reinforcement learning solution is developed. Each agent generates its own policy from Markov chains, and policies are exchanged only when agents occupy the same or adjacent nodes. This constraint on policy exchange models sparse communication conditions over large, unstructured environments. Empirical results provide perspectives on convergence properties, agent cooperation, and generalization of learned patrolling policies to new instances of the task. The emergent behavior indicates learned coordination strategies between heterogeneous agents for patrolling large, unstructured regions as well as the ability to generalize to dynamic variation in node visitation", "creator": "PDFCreator Version 0.9.6"}}}