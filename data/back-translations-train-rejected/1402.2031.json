{"id": "1402.2031", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2014", "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "abstract": "The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods.", "histories": [["v1", "Mon, 10 Feb 2014 04:15:23 GMT  (2312kb,D)", "http://arxiv.org/abs/1402.2031v1", "11 pages, 3 figures, 3 tables"]], "COMMENTS": "11 pages, 3 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["wen wang", "zhen cui", "hong chang", "shiguang shan", "xilin chen"], "accepted": false, "id": "1402.2031"}, "pdf": {"name": "1402.2031.pdf", "metadata": {"source": "CRF", "title": "Deeply Coupled Auto-encoder Networks for Cross-view Classification", "authors": ["Wen Wang", "Zhen Cui", "Hong Chang", "Shiguang Shan", "Xilin Chen"], "emails": ["xilin.chen}@vipl.ict.ac.cn"], "sections": [{"heading": null, "text": "In this paper, we propose a simple but effective coupled neural network called Deeply Coupled Autoencoder Networks (DCAN), which aims to build two deep neural networks that are coupled in all relevant layers. In DCAN, each deep structure is developed by stacking multiple discriminatory coupled auto-encoders, a denosing auto-encoder designed with maximum margin criterion, consisting of compactness within the class and punishment between classes. This single-layer component makes our model simultaneously consistent and improves its discriminatory capability. As the number of layers increases, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-sectional image classification tasks demonstrate the superiority of our method over modern methods."}, {"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if they are able to determine for themselves what they want. (...) It is not as if they are able to understand the rules that they have imposed on themselves. (...) It is as if they are able to determine for themselves what they want. (...) It is as if they want that they want that they want that they do what they want. (...)"}, {"heading": "2 Deeply Coupled Auto-encoder Networks", "text": "In this section, we first present the basic idea, and in the second part, we describe in detail the discriminatory coupled auto-encoder, then we describe how to stack multiple layers to build a deep network, and finally, we briefly describe the optimization of the model."}, {"heading": "2.1 Basic Idea", "text": "As shown in Fig.1, the Deeply Coupled Auto-Encoder Networks (DCAN) consists of two deep networks that are coupled together, and each is for one view. The network structures of the two deep networks are just like the leftmost and rightmost parts in Fig.1, where circles mean the units in each layer (pixels in an input image for the input layer and hidden representation in higher layers), and arrows denote the complete connection between adjacent layers. And the middle part of Fig.1 shows how the entire network projects samples in different views into a common space (pixels in an input image) and gradually increases the separability with increasing layers. The two deep networks are both built as blocks by stacking several similar coupled layers, because a single coupled layer may be insufficient, and the method of stacking multiple layers and training each layer is efficient in terms of the layer, as in many previous [6] works."}, {"heading": "2.2 Discriminative coupled auto-encoder", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world."}, {"heading": "2.3 Stacking coupled auto-encoder", "text": "Through the above training process, we model the map between the original sample space and a preliminary discriminatory subspace with a gap and build a hidden representation H, which is a compromise between approximating local consistency and distinguishing the projected data. However, since the real data is highly complicated, the use of a single coupled layer to model the huge and complex real scenes may be insufficient. Therefore, we opt for several such coupled network layers described in Section 2.2. With the increased number of layers, the entire network can compactly represent a significantly greater number of transformations than flat networks and gradually narrow the gap with improved discrimination capability."}, {"heading": "2.4 Optimization", "text": "We apply Lagrange's multiplier method to solve the objective function equation (1) with the constraints equation (2) as follows: min value (L (X, \u0432) + L (Y, \u0432))) + (G1 (H) \u2212 G2 (H)) + \u03b3 (12) Wx value (2) Wy value 2F) (10), the first term being the reconstruction error, the second term being the maximum margin criterion, and the last term the shrinkage constraints called Tikhonov regulators in [11], which are used to reduce the size of the weights and further to prevent overfitting. Equilibrium is the parameter between local consistency and empirical separability. And vice versa, the weight drop parameter is called and is normally set to a small value, e.g. 1.0e-4.To optimize the objective function (10), we use the Fagation function to solve the GS and subsequently the memory problems frequently."}, {"heading": "3 Experiments", "text": "In this section, the proposed DCAN is evaluated using two data sets: Multi-PIE [9] and CUHK Face Sketch FERET (CUFSF) [34, 31]."}, {"heading": "3.1 Databases", "text": "Multi-PIE data set [9] is used to evaluate facial recognition in all poses. Here, a subset of 337 subjects is selected in 7 poses (\u2212 45 \u0445, \u2212 30 \u0445, \u2212 15 \u0445, 0 \u0445, 15 \u0445, 15 \u0445, 30 \u0445, 45 \u0445), 3 expressions (Neutral, Smile, Disgust), no flush lighting from 4 sessions to validate our method. We randomly select 4 images for each pose of each subject and then randomly divide the data into two parts: the training set of 231 subjects (i.e. 231 x 7 x 4 = 6468 images) and the test set with the remaining subjects. The CUHK Face Sketch FERET (CUFSF) data set [34, 31] contains two types of facial images: photo and sketch. A total of 1,194 images (one image per subject) with illumination variations from the FERET data set [34, 31] were collected with each of the remaining data set."}, {"heading": "3.2 Settings", "text": "We compare the proposed DCAN method with several baselines and state-of-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15]. The first seven methods are paired methods for cross-view classification. MvDA collectively learns all transformations when multiple views can be used, and has achieved the state-of-the-art results in its reports [15]. Principal Component Analysis (PCA) [4] is used to reduce dimensions. In our experiments, we set the standard dimensionality as 100 with conservation of most energy except Deep CCA, PLS, CSR and CDFE."}, {"heading": "3.3 Face Recognition across Pose", "text": "First, to explicitly illustrate the learned mapping, we perform an experiment on Multi-PIE data sets by projecting the learned common features into a 2-D space with Principal Component Analysis (PCA). However, as in Fig.2. The classical method CCA can only roughly translate the data into the main directions and the state-of-the-art method MvDA [15] attempts to merge two types of data seem to fail. Thus, we argue that linear transformations are a little stiff to convert data from two views into an ideal common space. The three diagrams below show that DCAN gradually separates the samples from different classes with the increase of layers that we have described in the analyses above. Next, we compare our methods with several state-of-the-art methods for facial recognition taskon Multi-PIE data set."}, {"heading": "3.4 Photo-Sketch Recognition", "text": "Comparison results are shown in Table 3. As shown in this table, since in this case only two views can be used, MvDA deteriorates to a comparable performance with the previous two-view-based methods. Our proposed DCAN with three-layer networks can be achieved even better with more than 6% improvement, further indicating that DCAN benefits from the nonlinear and multi-layer structure. Discussion and Analysis: The above experiments show that our methods can work very well with a small sample size. The reasons lie in three folds: (1) The maximum marginal criterion makes the learned mapping discriminatory, which is a simple strategy in the supervised classification task. (2) The auto-encoder roughly preserves the local neighborhood structures, for which Alain et al theoretically proves that the learned representation can be obscured by auto-encoding local characteristics from 57% to 9700%."}, {"heading": "4 Conclusion", "text": "In this paper, we propose a method of deep learning, the Deeply Coupled Auto-encoder Networks (DCAN), which can gradually generate a coupled discriminant for the common object classification in cross-sectional views. At each level, we take into account both local consistency and the discrimination of the projected data. By stacking several such coupled network layers, DCAN can gradually improve the common characteristics learned in the common space. Furthermore, experiments in the tasks of cross-view classification show that our method is superior to other state-of-the-art methods."}], "references": [{"title": "A kernel method for canonical correlation", "author": ["S. Akaho"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["G. Alain", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.4246,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Predictive subspace learning for multi-view data: a large margin", "author": ["N. Chen", "J. Zhu", "E.P. Xing"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The cmu multi-pose, illumination, and expression (multipie) face", "author": ["R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1936}, {"title": "Multiview discriminant analysis", "author": ["M. Kan", "S. Shan", "H. Zhang", "S. Lao", "X. Chen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Discriminative learning and recognition of image set classes using canonical correlations", "author": ["T.-K. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "On optimization methods for deep", "author": ["Q.V. Le", "J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A.Y. Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Efficient backprop. In Neural networks: Tricks of the trade, pages 9\u201350", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Efficient sparse coding", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Coupled spectral regression for matching heterogeneous faces", "author": ["Z. Lei", "S.Z. Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Efficient and robust feature extraction by maximum margin criterion", "author": ["H. Li", "T. Jiang", "K. Zhang"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Inter-modality face recognition", "author": ["D. Lin", "X. Tang"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "The feret database and evaluation procedure for facerecognition algorithms", "author": ["P.J. Phillips", "H. Wechsler", "J. Huang", "P.J. Rauss"], "venue": "Image and vision computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Bypassing synthesis: Pls for face recognition with pose, low-resolution and sketch", "author": ["A. Sharma", "D.W. Jacobs"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["A. Sharma", "A. Kumar", "H. Daume", "D.W. Jacobs"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Feature extraction by maximizing the average neighborhood margin", "author": ["F. Wang", "C. Zhang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Semicoupled dictionary learning with applications to image super-resolution and photo-sketch synthesis", "author": ["S. Wang", "L. Zhang", "Y. Liang", "Q. Pan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Face photo-sketch synthesis and recognition", "author": ["X. Wang", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1955}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Graph embedding and extensions: a general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.-J. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Coupled informationtheoretic encoding for face photo-sketch recognition", "author": ["W. Zhang", "X. Wang", "X. Tang"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Maximum margin embedding", "author": ["B. Zhao", "F. Wang", "C. Zhang"], "venue": "In Data Mining,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "Unsupervised methods such as Canonical Correlation Analysis (CCA)[14] and Partial Least Square (PLS) [26] are employed to the task of cross-view recognition.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Unsupervised methods such as Canonical Correlation Analysis (CCA)[14] and Partial Least Square (PLS) [26] are employed to the task of cross-view recognition.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "And in [30], a semi-coupled dictionary is used to bridge two views.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "With label information available, many methods were further developed to learn a discriminant common space For instance, Discriminative Canonical Correlation Analysis (DCCA) [16] is proposed as an extension of CCA.", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "In [22], with an additional local smoothness constraints, two linear projections are simultaneously learnt for Common Discriminant Feature Extraction (CDFE).", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "There are also other such methods as the large margin approach [8] and the Coupled Spectral Regression (CSR) [20].", "startOffset": 63, "endOffset": 66}, {"referenceID": 16, "context": "There are also other such methods as the large margin approach [8] and the Coupled Spectral Regression (CSR) [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Recently, multi-view analysis [27, 15] is further developed to jointly learn multiple specific-view transforms when multiple views (usually more than 2 views) can be available.", "startOffset": 30, "endOffset": 38}, {"referenceID": 11, "context": "Recently, multi-view analysis [27, 15] is further developed to jointly learn multiple specific-view transforms when multiple views (usually more than 2 views) can be available.", "startOffset": 30, "endOffset": 38}, {"referenceID": 27, "context": "However, these linear discriminant analysis methods usually depend on the assumption that the data of each class agrees with a Gaussian distribution, while data in real world usually has a much more complex distribution [33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 3, "context": "See [5] for a recent review of Deep Learning algorithms.", "startOffset": 4, "endOffset": 7}, {"referenceID": 15, "context": "Lots of such basic building blocks have been proposed, including sparse coding [19], restricted Boltzmann machine (RBM) [12], autoencoder [13, 6], etc.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Lots of such basic building blocks have been proposed, including sparse coding [19], restricted Boltzmann machine (RBM) [12], autoencoder [13, 6], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Lots of such basic building blocks have been proposed, including sparse coding [19], restricted Boltzmann machine (RBM) [12], autoencoder [13, 6], etc.", "startOffset": 138, "endOffset": 145}, {"referenceID": 26, "context": "Specifically, the (stacked) autoencoder has shown its effectiveness in image denoising [32], domain adaptation [7], audio-visual speech classification [23], etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Specifically, the (stacked) autoencoder has shown its effectiveness in image denoising [32], domain adaptation [7], audio-visual speech classification [23], etc.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "As we all known, the kernel method, such as Kernel Canonical Correlation Analysis(Kernel CCA) [1], is also a widely used approach to learn nonlinear representations.", "startOffset": 94, "endOffset": 97}, {"referenceID": 22, "context": "The discriminative coupled auto-encoder has a similar input corrupted and reconstructive error minimized mechanism with the denoising auto-encoder proposed in [28], but is modified by adding a maximum margin criterion.", "startOffset": 159, "endOffset": 163}, {"referenceID": 17, "context": "This kind of criterion has been used in previous works, like [21, 29, 35], etc.", "startOffset": 61, "endOffset": 73}, {"referenceID": 23, "context": "This kind of criterion has been used in previous works, like [21, 29, 35], etc.", "startOffset": 61, "endOffset": 73}, {"referenceID": 29, "context": "This kind of criterion has been used in previous works, like [21, 29, 35], etc.", "startOffset": 61, "endOffset": 73}, {"referenceID": 9, "context": "The two deep networks are both built through stacking multiple similar coupled single layer blocks because a single coupled layer might be insufficient, and the method of stacking multiple layers and training each layer greedily has be proved efficient in lots of previous works, such as those in [13, 6].", "startOffset": 297, "endOffset": 304}, {"referenceID": 8, "context": "Training a deep network with coupled nonlinear transforms can be achieved by the canonical greedy layer-wise approach [12, 6].", "startOffset": 118, "endOffset": 125}, {"referenceID": 13, "context": "To optimize the objective function (10), we use backpropagation to calculate the gradient and then employ the limited-memory BFGS (L-BFGS) method [24, 17], which is often used to solve nonlinear optimization problems without any constraints.", "startOffset": 146, "endOffset": 154}, {"referenceID": 14, "context": "Obviously, the object function in (10) is differential to these parameters \u0398, and we use Back-propagation [18] method to derive the derivative of the overall cost function.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "In our setting, we find the objective function can achieve as fast convergence as described in [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "In this section, the proposed DCAN is evaluated on two datasets, Multi-PIE [9] and CUHK Face Sketch FERET (CUFSF) [34, 31].", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "In this section, the proposed DCAN is evaluated on two datasets, Multi-PIE [9] and CUHK Face Sketch FERET (CUFSF) [34, 31].", "startOffset": 114, "endOffset": 122}, {"referenceID": 25, "context": "In this section, the proposed DCAN is evaluated on two datasets, Multi-PIE [9] and CUHK Face Sketch FERET (CUFSF) [34, 31].", "startOffset": 114, "endOffset": 122}, {"referenceID": 6, "context": "Multi-PIE dataset [9] is employed to evaluate face recognition across pose.", "startOffset": 18, "endOffset": 21}, {"referenceID": 28, "context": "CUHK Face Sketch FERET (CUFSF) dataset [34, 31] contains two types of face images: photo and sketch.", "startOffset": 39, "endOffset": 47}, {"referenceID": 25, "context": "CUHK Face Sketch FERET (CUFSF) dataset [34, 31] contains two types of face images: photo and sketch.", "startOffset": 39, "endOffset": 47}, {"referenceID": 19, "context": "Total 1,194 images (one image per subject) were collected with lighting variations from FERET dataset [25].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "According to the configuration of [15], we use the first 700 subjects as the training data and the rest subjects as the testing data.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 142, "endOffset": 145}, {"referenceID": 18, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "We compare the proposed DCAN method with several baselines and stateof-the-art methods, including CCA [14], Kernel CCA [1], Deep CCA [3], FDA [4], CDFE [22], CSR [20], PLS [26] and MvDA [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 11, "context": "MvDA jointly learns all transforms when multiple views can be utilized, and has achieved the state-of-the-art results in their reports [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "The Principal Component Analysis (PCA) [4] is used for dimension reduction.", "startOffset": 39, "endOffset": 42}, {"referenceID": 10, "context": "CCA[14] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "KernelCCA[10] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "FDA[4] 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "CDFE[22] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "CSR[20] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "PLS[26] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "MvDA[15] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "The classical method CCA can only roughly align the data in the principal directions and the state-of-the-art method MvDA [15] attempts to merge two types of data but seems to fail.", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "The depicted samples are randomly chosen form Multi-PIE [9] dataset.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "Table 2: Results of CCA, FDA [4], CDFE [22], MvDA [15] and DCAN on MultiPIE dataset in terms of rank-1 recognition rate.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "Table 2: Results of CCA, FDA [4], CDFE [22], MvDA [15] and DCAN on MultiPIE dataset in terms of rank-1 recognition rate.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Table 2: Results of CCA, FDA [4], CDFE [22], MvDA [15] and DCAN on MultiPIE dataset in terms of rank-1 recognition rate.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "CCA[14] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "KernelCCA[10] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "CDFE[22] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "CSR[20] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "PLS[26] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "FDA[4] 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "MvDA[15] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "The sketch and photo images in CUFSF [34, 31] are respectively used for the gallery and probe set.", "startOffset": 37, "endOffset": 45}, {"referenceID": 25, "context": "The sketch and photo images in CUFSF [34, 31] are respectively used for the gallery and probe set.", "startOffset": 37, "endOffset": 45}, {"referenceID": 1, "context": "[2] theoretically prove that the learnt representation by auto-encoder can recover local properties from the view of manifold.", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}