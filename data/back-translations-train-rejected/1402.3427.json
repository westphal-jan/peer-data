{"id": "1402.3427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "Indian Buffet Process Deep Generative Models", "abstract": "Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings; they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications.", "histories": [["v1", "Fri, 14 Feb 2014 10:44:48 GMT  (189kb)", "http://arxiv.org/abs/1402.3427v1", null], ["v2", "Sun, 30 Aug 2015 18:41:41 GMT  (0kb,I)", "http://arxiv.org/abs/1402.3427v2", "This paper has been withdrawn by the author due to errors in the experiments (software bugs)"], ["v3", "Fri, 8 Jul 2016 19:37:54 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v3", null], ["v4", "Sat, 16 Jul 2016 14:11:47 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v4", null], ["v5", "Sun, 6 Aug 2017 21:27:48 GMT  (245kb,D)", "http://arxiv.org/abs/1402.3427v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sotirios p chatzis"], "accepted": false, "id": "1402.3427"}, "pdf": {"name": "1402.3427.pdf", "metadata": {"source": "CRF", "title": "Maximum Entropy Discrimination Denoising Autoencoders", "authors": ["Sotirios P. Chatzis"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 2.34 27v1 [cs.LG] 1 4Fe b20 14Index Terms - Auton coders, maximum entropy discrimination, meanfield, Gaussian process."}, {"heading": "1 INTRODUCTION", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 AUTOENCODER NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Traditional autoencoders", "text": "A traditional autoencoder has two different parts: an encoder and a decoder. Since the encoder is a deterministic representation of a d-dimensional input vector x to a hidden representation, it is usually considered an affine transformation, followed by a squeezing nonlinearity of the form y, f (x; \u03b8) = \u03c3 (Wx + b) (1), with W-Rd \u2032 \u00b7 d, b-Rd \u2032 \u00b7 1, and \u03b8 = {W, b}. From the resulting representation y, we can reconstruct its corresponding vector in the d-dimensional input space under consideration using the decoder of the AE-Rd distribution. Specifically, the obtained input reconstruction z is derived from an affine mapping, which is optionally followed by a squeezing nonlinearity, and reads z, x, g (y; target) x x x x x x x. \""}, {"heading": "2.2 Denoising autoencoders", "text": "It has been shown that the optimal reconstruction criterion of the AU cannot guarantee the extraction of useful features in several cases. Specifically, it has been shown that it can lead to the solution of simply copying the input to the output or other trivial factors that provide very low reconstruction errors in the training set in combination with extremely poor modeling and generalization services [8]. Using sparse representation strategies that impose dimensional constraints on the representation obtained is a solution to this problem (e.g., [7]). Denoizing autoencoders addresses this overmatch problem in a different way [8]. Rather than imposing constraints, DAs modify the applied reconstruction criterion, which specifies that it also denoizes partially corrupt inputs. This approach is based on the assumption that a higher level of representation should be fairly stable and robust, should include such inputs, and that the structure should be corrupted."}, {"heading": "3 PROPOSED APPROACH", "text": "We assume that it is a model in which we behave in the way we imagine it. (...) We assume that a model in which we apply such a model leads to a different model. (...) We assume that such a model leads to a different model. (...) We assume that it leads to a different model. (...) We assume that such a model leads to a different model. (...) We assume that it leads to a different model. (...) We assume that it leads to a different model. (...) We choose a new model. (...) D = 1, through which we obtain a corresponding corrupt distribution. (...) We choose a new model. (...) D = 1, through which a new model (...)."}, {"heading": "3.1 Inference algorithm", "text": "In order to draw conclusions about our model, we adopt the MED Conclusion Framework and expand it by introducing an additional q-probability concept into the optimized objective function based on the assumption (11) of our model. Specifically, the conventional MED Conclusion in the context of our model would include a solution to the following minimization problem: min q (Z, Z, H, E, E, E, E, E). Specifically, the conventional MED Conclusion in the context of our model would include the solution of the following minimization problem: min q (Z, Z, E, E, E, E)."}, {"heading": "3.1.1 Posterior Distribution", "text": "Let us start with the derivation of the expression of the rear distribution of our model. If we follow the midfield principle [28], we assume that the desired rear distribution is based on the z, z, z, c, and n, similar to the imposed previous one. Then, the optimization (15).r.t. returns q (n), we obtain the means by solving the primary problems (16).n, where the results (E) + 1D, d = 1E, d = 1E [zd, z, T \u00b7] (17) and the means are obtained by solving the primary problems."}, {"heading": "3.1.2 Pseudo-inputs selection", "text": "It is easy to show that the solution (15) of the problem relates to the optimization problem X-E [C-c = 1log p (z \u00b7 c | X, X-c, z-c \u00b7 c)] + E [C-c = 1log p (z-c-X)] (35) Since the optimization problem (35) does not provide closed-loop solutions, it is necessary to resort to an iterative algorithm to obtain the desired estimates. To this end, the limited memory variant of the BFGS algorithm (L-BFGS) [30] is a suitable candidate solution. However, this method entails an unaffordable combinatory search, especially if the observation space is high-dimensional, i.e. for large values of N. Alternatively, in this work we avoid learning the pseudo-inputs (which can potentially increase the training value of X-X-z-data to the actual use of X-x)."}, {"heading": "3.1.3 Hyperparameter estimation", "text": "Finally, we now turn to the estimates of the hyperparameters of our model, to which we have not imposed any prior distribution. Let's start with the noise variance \u03b42; solve (15) w.r.t. \u03b42 we get [36]) Furthermore, we consider the case that the core functions of the forced GP-Priors also involve some hyperparameters. In order to obtain their estimates, we optimize (15) above them. This is essentially reduced to the optimization problem max.: E [C-c-c-c-c, X-c, z-c-c)] + E [C-c-c-1log p (z-c-c-c-X-c)] (37), which is the hyperparameter we are looking for. Since the maximization problem (37) does not result in closed solutions, we can fall back to the L-form we are looking for."}, {"heading": "4 EXPERIMENTS", "text": "Here we conduct the experimental evaluation of our approach. First, we look at an image classification experiment using benchmark data well known in the literature of the neural networks of the AE. Our model is used to extract prominent features from these images, which are then presented to simple linear SVM classifiers to perform the classification task. Next, we look at an application of video mining, namely sports video mining in football videos. In this case, we use our model to extract prominent features from the raw video data and submit them to a simple linear SVM classifier to perform the classification task. Finally, we evaluate the performance of our method in a transfer learning experiment. In this case, our model is used to extract features from data that relate to different domains; then we train a simple binary classifier (linear SVM) with the extracted features that relate to data."}, {"heading": "4.1 Image classification benchmarks", "text": "In this experiment, we look at three benchmarks that address the problem of content-based image classification commonly used in deep learning literature. Specifically, we are experimenting with: 7 (i) The standard USPS dataset contains only 10 examples per class (digit) to perform a training, so we can assess whether our approach achieves the goal of high modeling performance by using scarce data, and how it compares with the competition. (ii) The small NORB datasets [32], which include stereo image pairs of fifty toys belonging to five generic categories. Each toy was mapped under six lighting conditions, nine elevations, and eighteenth azimuits. Originally, the objects were divided equally into test and training sets, each leading at a specific time."}, {"heading": "4.2 Sports Video Mining", "text": "In this experiment, we look at the problem of sports video mining in football videos. Specifically, we are interested in identifying four camera views: central, left, right, and in the end zone, and four game types: Long Play, Short Play, Kick, and Field Goal Play. To perform evaluations, we use a database of 30-minute American Football games. Frames of the videos were downloaded to 45-36 resolution and were pre-processed to remove commercials and replays. We keep only 100 examples from each category (i.e., game type or camera view) to perform a training, and use another 1000 to perform evaluations. In Figure 1, we offer some sample images from our video systems, i.e. SDA, mSDA, and DGP, in these experiments we compare with a baseline approach, namely the iM2EDM method of extraction is not presented."}, {"heading": "4.3 Transfer learning", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 CONCLUSIONS", "text": "This behavior of DA-Neural Networks is in contrast to the ability of humans to learn complex abstract concepts with very limited observations. In this paper, we investigated whether DA-Neural Networks can be reformulated to perform better when data is scarce. We assumed that the reason DA-performances decline in reducing the volume of training data lies in the frequency of their formulation. Based on this assumption, we postulated a novel, fully Bayesian treatment of DA using concepts derived from Bayesian non-parametric and large-margin principles. Specifically, our approach formulates the encoder modules by considering a Gaussian process to be impracticable before the latent input coding generated by taking into account the (corrupt) input coding."}], "references": [{"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large scale kernel machines, L. Bottou, O. Chapelle, D. De- Coste, and J. Weston, Eds. MIT press, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, pp. 504\u2013507, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, vol. 19, 2007, pp. 153\u2013160.  10", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in neural information processing systems, vol. 20, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Connectionist learning procedures", "author": ["G. Hinton"], "venue": "Artificial Intelligence, vol. 40, pp. 185\u2013234, 1989.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y. Boureau", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201907), vol. 20, 2007, pp. 1185\u20131192.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory-based Bayesian models of inductive learning and reasoning", "author": ["J.B. Tenenbaum", "C. Kemp", "P. Shafto"], "venue": "Trends in Cognitive Sciences, 2006, pp. 309\u2013318.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. Neural Information Processing Systems, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Nonparametric guidance of autoencoder representations using label information", "author": ["J. Snoek", "R.P. Adams", "H. Larochelle"], "venue": "Journal of Machine Learning Research, vol. 13, no. 1-48, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. Artificial Intelligence and Statistics, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Proc. Neural Information Processing Systems, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep Gaussian Processes", "author": ["A.C. Damianou", "N.D. Lawrence"], "venue": "Proc. AISTATS, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative, generative and imitative learning", "author": ["T. Jebara"], "venue": "Ph.D. dissertation, MIT, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "Proc. NIPS, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Extensions of the informative vector machine", "author": ["N.D. Lawrence", "J.C. Platt", "M.I. Jordan"], "venue": "Deterministic and Statistical Methods in Machine Learning, J. Winkler, N. D. Lawrence, and M. Niranjan, Eds. Berlin: Springer-Verlag, 2005, pp. 56\u201387.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems, Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, Eds., vol. 18. Cambridge, MA: The MIT Press, 2006, pp. 1259\u20131266.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic determination of the intrinsic structure in Bayesian factor analysis", "author": ["E. Fokoue"], "venue": "Statistical and Applied Mathematical Sciences Institute, Research Triangle Park, NC, Tech. Rep. TR- 2004-17, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Learning in Graphical Models, M. Jordan, Ed. Dordrecht: Kluwer, 1998, pp. 105\u2013162.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "Proc. ICML, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T. Jaakkola", "M. Jordan"], "venue": "Statistics and Computing, vol. 10, pp. 25\u201337, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods - Support Vector Learning, B. Sch\u00f6lkopf, C. Burges, and A. Smola, Eds. MIT-Press, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "On the limited memory method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming B, vol. 45, no. 3, pp. 503\u2013528, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z.E. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "Proc. ICML, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Proc. CVPR, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proc. ICML, 2007, pp. 473\u2013480.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Infinite Markov-switching maximum entropy discrimination machines", "author": ["S.P. Chatzis"], "venue": "Proc. ICML, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental hidden Markov models for view-based sport video analysis", "author": ["Y. Dingand", "G. Fan"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Qualitative estimation of camera motion parameters from video sequences", "author": ["M. Srinivasan", "S. Venkatesh", "R. Hosie"], "venue": "Pattern Recognit., vol. 30, pp. 593\u2013606, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. ICML, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proc. EMNLP, 2006, pp. 120\u2013128.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "It has recently become obvious that deep architectures allow for obtaining better modeling and representational capacities in the context of challenging pattern recognition applications [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": ", [6]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "In this work, we focus on autoencoder models [7]; specifically, we are interested in a recent variation of this method, namely denoising autoencoders [8].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "In this work, we focus on autoencoder models [7]; specifically, we are interested in a recent variation of this method, namely denoising autoencoders [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 213, "endOffset": 216}, {"referenceID": 6, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 223, "endOffset": 226}, {"referenceID": 4, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 238, "endOffset": 241}, {"referenceID": 9, "context": "stacked DAs (SDAs), require large amounts of data to perform learning [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "However, we know that humans are able to perform inductive reasoning (equivalent to concept generalization) with only a few examples [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "For example, [13] use restricted Boltzmann machines (RBMs) to extract features in an unsupervised manner, which are subsequently fed as inputs to the covariance kernel of a Gaussian process (GP) regressor or classifier [14].", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "For example, [13] use restricted Boltzmann machines (RBMs) to extract features in an unsupervised manner, which are subsequently fed as inputs to the covariance kernel of a Gaussian process (GP) regressor or classifier [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Similarly, [15] consider a classification model that imposes GP priors on the discriminative function that maps the latent encodings into class labels.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "The result of this choice is a Gaussian process latent variable model (GPLVM) [16] for the predicted class labels.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "While much more efficient compared to the approach of [13], neither does this method utilize the component nonparametric Bayesian model to perform learning of the generated input encodings.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "Further, [17] combined autoencoder training with neighborhood component analysis [18], which encouraged the model to encode similar latent representations for inputs belonging to the same class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "Further, [17] combined autoencoder training with neighborhood component analysis [18], which encouraged the model to encode similar latent representations for inputs belonging to the same class.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Finally, [19] recently proposed a deep Gaussian process (DGP) model.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "DGP is essentially a deep belief network (DBN) [2], [3] the component models of each layer of which are Gaussian processes instead of RBMs.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "DGP is essentially a deep belief network (DBN) [2], [3] the component models of each layer of which are Gaussian processes instead of RBMs.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "Specifically, we consider a nonlinear encoder module, obtained by imposing a Gaussian process prior [14] over the latent encodings of the observed (corrupt) data.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "To introduce the large-margin learning principle in the context of our hierarchical Bayesian model, we build upon the maximum entropy discrimination (MED) framework [20], [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "To introduce the large-margin learning principle in the context of our hierarchical Bayesian model, we build upon the maximum entropy discrimination (MED) framework [20], [21].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "This, in turn, gives rise to an associated reconstruction error minimized through model training, that reads [8]", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Similarly, in cases of binary observations x, or x \u2208 [0, 1], it is usually considered that", "startOffset": 53, "endOffset": 59}, {"referenceID": 7, "context": "Specifically, it has been shown that it may lead to the solution of just copying the input to the output or other trivial ones that yield very low reconstruction error in the training set combined with extremely poor modeling and generalization performance [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "Denoising autoencoders address this overfitting problem in a different way [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Some simple corruption process models often used in the literature are [8]: (i) additive isotropic Gaussian noise (GS), i.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Initializing deep architectures by stacking DAs is performed in a way fairly similar to traditional AEs [4], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Initializing deep architectures by stacking DAs is performed in a way fairly similar to traditional AEs [4], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 20, "context": "A drawback of such a modeling approach consists in the fact that performing inference for GP models entails inversion of the gram matrix of the input data [22], resulting in an O(D) complexity, which is rather inefficient.", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "To ameliorate this issue, we resort to utilization of a popular sparse pseudo-input Gaussian process (SPGP) modeling approach [23]: We impose a prior over the latent encodings {zd\u00b7}Dd=1 taking the form of a GP predictive distribution parameterized by a small pseudodataset X\u0304 = {x\u0304m\u00b7} M m=1, with corresponding latent encoding values {z\u0304m\u00b7}m=1 (pseudo-encodings).", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "This way, we eventually obtain a prior over the latent encodings {z\u00b7c}c=1 parameterized by the introduced pseudo-data, taking the form of a simple GP predictive density, reading [23]", "startOffset": 178, "endOffset": 182}, {"referenceID": 22, "context": "The selected form of the prior imposed over the model parameters \u03b7n allows for obtaining an exponentially tractable solution to the latent encodings dimensionality inference problem, by means of automatic relevance determination (ARD) [24].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches.", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "It has been shown that such an iterative consecutive updating procedure is guaranteed to monotonically optimize the objective function of our problem [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Following the meanfield principle [28], we assume that the sought posterior distribution factorizes over the z\u00b7c, z\u0304\u00b7c, and \u03b7n, similar to the imposed prior.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Indeed, in our work, we use the SVM-light toolbox of [29], which implements a highly-scalable working set selection algorithm, and provides both the primal", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "For this purpose, the limited memory variant of the BFGS algorithm (L-BFGS) [30] is a suitable candidate solution.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "mSDA generates an overcomplete representation by default, as described in [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "(ii) The small-NORB dataset [32], which comprises stereo image pairs of fifty toys belonging to five generic categories.", "startOffset": 28, "endOffset": 32}, {"referenceID": 30, "context": "(iii) The Convex/Nonconvex dataset, adopted from [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "SDA, mSDA, and DGP, in these experiments we also compare to a baseline approach, namely the iMEDM method of [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 31, "context": "The latter method is not presented with the raw video signals, but with appropriate feature vectors extracted as described in [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 32, "context": "Specifically, to capture camera view information, we use the color distribution and the yard line angle [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "To compute the two kinds of camera motion, we choose the optical flow-based method of [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Inspired from the recent work of [37], we use the encodings generated by our model as inputs to linear SVM classifiers, and evaluate the resulting classifiers on the basis of their capacity to effectively perform classification of data from a given domain while being trained on data from different domains.", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "Specifically, similar to [37], we consider a sentiment analysis application across different product domains.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": "Our datasets are derived from the Amazon reviews benchmark [38].", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "To ensure that our results will not be negatively affected from these imbalances, in our experiments we retain only four domains with similar numbers of samples, namely Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K), similar to [38].", "startOffset": 246, "endOffset": 250}, {"referenceID": 34, "context": "Similar to the work of [37], we formulate the recognition task as a binary classification problem: the trained models are required to recognize whether a review is positive (higher than 3 stars) or negative (3 stars or lower).", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Each review text is treated as a bag-of-words and transformed into binary vectors encoding the presence/absence of unigrams and bigrams, similar to [37].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "For computational reasons, only the 500 most frequent terms of the vocabulary of unigrams and bigrams are kept in the feature set, similar to [31].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "In this experiment, the SDA and mSDA models comprise 5 layers; this configuration was shown in [31] to yield the optimal result (we have confirmed this finding through our experiments).", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "Application of DGP to a transfer learning scenario is not straightforward, given the model formulation of [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 28, "context": "Note that mSDA takes two orders of magnitude less time (similar to the previous experiment), as also reported in [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 28, "context": "Borrowing ideas from the mSDA method of [31] might be a good starting point for this purpose.", "startOffset": 40, "endOffset": 44}], "year": 2017, "abstractText": "Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings; they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications.", "creator": "LaTeX with hyperref package"}}}