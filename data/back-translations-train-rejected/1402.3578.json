{"id": "1402.3578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2014", "title": "Learning-assisted Theorem Proving with Millions of Lemmas", "abstract": "Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of the lemmas in the HOL Light and Flyspeck libraries, adding up to millions of the best lemmas to the pool of statements that can be re-used in later proofs. We show that in combination with learning-based relevance filtering, such methods significantly strengthen automated theorem proving of new conjectures over large formal mathematical libraries such as Flyspeck.", "histories": [["v1", "Tue, 11 Feb 2014 03:08:02 GMT  (58kb)", "http://arxiv.org/abs/1402.3578v1", "journal version ofarXiv:1310.2797(which was submitted to LPAR conference)"]], "COMMENTS": "journal version ofarXiv:1310.2797(which was submitted to LPAR conference)", "reviews": [], "SUBJECTS": "cs.AI cs.DL cs.LG cs.LO", "authors": ["cezary kaliszyk", "josef urban"], "accepted": false, "id": "1402.3578"}, "pdf": {"name": "1402.3578.pdf", "metadata": {"source": "CRF", "title": "Learning-assisted Theorem Proving with Millions of Lemmas", "authors": ["Cezary Kaliszyk", "Josef Urban"], "emails": ["cezary.kaliszyk@uibk.ac.at", "josef.urban@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.35 78v1 [cs.AI] 1Large formal mathematical libraries consist of millions of atomic inference steps leading to a corresponding number of proven statements (lemmas). Similar to informal mathematical practice, only a tiny fraction of such statements are named by formal mathematicians and reused in later proofs. In this work, we propose and implement criteria that define the estimated usefulness of Homl Light lemmings for the detection of further theorems. We use these criteria to reduce the large lemmat inference graph in the Homl Light and Flyspeck libraries, and add millions of the best lemmats to the pool of statements that can be reused in later proofs. We show that such methods, in combination with learning-based relevance filtering, significantly strengthen the automated theorem evidence of new assumptions about large formal mathematical libraries such as flyspeck.Keywords: flyspeck, lemmine, minma, learning intelligence."}, {"heading": "1. Introduction: Automated Reasoning over Large Mathematical Libraries", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Using Lemmas for Theorem Proving in Large Theories", "text": "In fact, it is such that most people are able to understand themselves and to understand what it is all about, namely the question of how they are to behave and how they are to behave. (...) It is not so that they are able to behave. (...) It is not so that they are able to behave. (...) It is not so that they are able to behave. (...) It is as if they are doing it. (...). (...) It is as if they are doing it. (...) It is as if they are doing it. (...). (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.). (.).). (. (.). (.).). (.).). (. (.).). (.). (.).). (.). (. (.). (.).). (.). (.).). (.).). (. (.).). (.).). (.). (.).). (.).). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.).). ().).).)."}, {"heading": "3. Overview of Related Work and Ideas", "text": "It's about how you measure the quality of short selling and how you can use it for other purposes, especially in terms of the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it, it's about the way you use it."}, {"heading": "4. The Proof Data", "text": "Dre rf\u00fc nde nllrrrfEe\u00fceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "4.1. Initial Post-processing and Optimization of the Inference Traces", "text": "While taking evidence, only exact duplicates are easy to spot. As explained in the previous section, HOL does not use Light de Bruijn indices natively to represent variables, so the track may still contain alpha-convertible versions of the same theorems. Checking for alpha convertibility during evidence taking would be possible, but is not obvious, as in the LCF-like approach of HOL Light Alpha conversion itself leads to multiple core conclusions. To avoid performing term-level renaminations, we keep the original proof trace unaffected and implement its further optimizations as an external post-processing of the track. In particular, to merge alpha-convertible lemmats into a proof-trace T, we simply use the above-mentioned normalized variable representation of the lemmata as input to an external program that generates a new version of the proof-trace T."}, {"heading": "4.2. Obtaining Shorter Traces from the Tactic Calls", "text": "In fact, it is such that most people are able to surpass themselves, both in terms of the way they have come into the world and in terms of the way they have come into the world, and also in terms of the way they have come into the world, as well as in terms of the way they have come into the world, as they have come into the world. (Self and the people in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they"}, {"heading": "4.3. Other Possible Optimizations", "text": "The ATP experiments described in the following use only the four versions of the evidence tracking system (TRACE0, TRACE1, TRACE2 and the tactical track), but we have also investigated some other normalizations. A particularly interesting optimization from the point of view of ATP is the removal of subsumed lemmings. A first measurement with the (slightly modified) MoMM system on the clausified first order versions of approximately 200,000 core elements of the HOL Light lemmings has shown that about 33% of the clauses generated from the lemmings are subsumed. However, even here ATP operations such as subsumption interact in a non-trivial way with the degree of inferences recorded by the HOL Light kernel. It is an interesting task to define exactly how the original evidence graphic should be transformed in relation to such operations and how such detection transformations can be efficiently performed across the entire flyspectrum."}, {"heading": "5. Selecting Good Lemmas", "text": "There are a number of ideas that can be explored and combined in different ways, but the more complex methods (such as those used by AGIntRater) are not yet directly usable on the large ITP datasets we have. So far, we have mainly experimented with the following techniques: (1) Direct OCaml implementation of Lemma quality metrics based on the HOL lightproof recording data structures; (2) Schulz's Epkllemma and its minor changes; (3) PageRank, applied in different ways to the detection track; (4) Graph-cutting algorithms with modified weighting function."}, {"heading": "5.1. Direct Computation of Lemma Quality", "text": "The advantage of direct OCaml reference is that no extraction to external tools is necessary, and all the information gathered about the lemmas by the HOL-RL-RL-RL-RL-RL is: (D) The basic factors we use to define the high quality uses are: (I) Number of HOL symbols d (HOL) S (i) Number of recursive dependencies D (i) and D (i) Number of recursive uses U (i), and (iv) Number of HOL symbols d (HOL) S (i).If we recursively define U (i) and D (i) we assume that in general some lemmas are called, only some lemmas (k).Note that in HOL Light there are many lemmas that have no dependencies, but formally they are still derived."}, {"heading": "5.3. Lemma Quality via PageRank", "text": "PageRank (eigenvector centrality of a graph) is a method that assigns weights to the nodes in an arbitrarily directed graph (not just DAG) based on the weights of the adjacent nodes (\"incoming links\"). Specifically, the weights are calculated as the dominant eigenvector of the following equation set (PR1 (i) = 1 \u2212 fN + f \u00b2 i \u00b2 d (j) PR1 (j) | where N is the total number of nodes and f is a damping factor that is typically set to 0.85. The advantage of using PageRank is that there are fast approximative implementations that can process the entire flyspeck proof graph in about 10 minutes using 21 GB of RAM, and the weights of all nodes are calculated simultaneously in that period."}, {"heading": "5.4. Lemma Quality using Graph Cut", "text": "The approaches so far attempted to define what is a \"good\" dilemma that uses our intuitions coming from mathematics. Here, we will try to estimate the impact that the selection of certain terms will have on the last dependency graph used for the learning framework.Selecting a subset of potential intermediate lemmas can be seen as a variant of the graph theory problems of searching for an intersection with certain characteristics. We will only consider sections that respect the chronological order of theorems in the library. As many of the graph cut algorithms (for example, maximum intersection) are NP-complete, we decide to build the intersection by adding the nodes one by one. Given a graph in which certain nodes are already named (marked degrees in Figure 3) we will want to estimate the impact of choosing a new problem on the evaluation, we will calculate the dependencies that we will compute together with the newly selected nodes."}, {"heading": "5.5. Selecting Many Lemmas", "text": "Of the methods described above, only the different variants of the PageRank (PRi) produce the final ranking of all the Lemmata (Named) that have already been named. If the task is to select a predefined number of the best Lemmata, this naturally leads to the recursive Lemmata selection algorithm 1 (also used by epcllemma).There are two possible choices of the initial Lemmata Named0 in Algorithm 1: either the empty set or the set of all humanly named Theorems. This selection depends on whether we want to reorganize the library from scratch, or whether we want to select good Lemmata that complement the humanly named Lemmata Named0 that supplement the humanly named Theorems NltM."}, {"heading": "6. Evaluation Scenarios and Issues", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "7. Experiments", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7.3. Examples", "text": "UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU"}, {"heading": "8. Future Work and Conclusion", "text": "We have proposed, implemented and evaluated several approaches that attempt to efficiently find the best lemmas and reorganize a large corpus of computationally comprehensible human mathematical ideas, using the millions of logical interdependencies between the atomic elements of the corpus. We believe that such conceptual reorganization is a very interesting AI topic that can best be studied in the context of large, fully semantic corpora such as HOLLight and Flyback. The by-product of this work is the export and post-processing techniques that lead to the publicly available evidence curves that can serve as a basis for further research. The most conservative improvement in the strength of automated thinking that has been achieved so far through the core of the HOL Light thanks to Lemon Mining is about 5%. The improvement in the strength of automated thinking achieved through flyspeck problems is 21.4% compared to the methods developed in (Kaliszyk and Urban, 2012)."}, {"heading": "9. Acknowledgments", "text": "We would like to thank Stephan Schulz for his help in running epcllemma, Yury Puzis and Geoff Sutcliffe for their help with the AGInt tool and Ji r \u0131 Vyskoc il and Petr Pudla k for many discussions about obtaining interesting lemons from proofs."}], "references": [{"title": "Premise selection for mathematics by corpus analysis and kernel methods", "author": ["J. Alama", "T. Heskes", "D. K\u00fchlwein", "E. Tsivtsivadze", "J. Urban"], "venue": "J. Autom. Reasoning", "citeRegEx": "Alama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2014}, {"title": "Automated and Human Proofs in General Mathematics: An Initial Comparison", "author": ["J. Alama", "D. K\u00fchlwein", "J. Urban"], "venue": "LPAR. Vol. 7180 of LNCS. Springer,", "citeRegEx": "Alama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2012}, {"title": "Pagerank based clustering of hypertext document collections", "author": ["K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova"], "venue": null, "citeRegEx": "Avrachenkov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Avrachenkov et al\\.", "year": 2008}, {"title": "Mizar in a nutshell", "author": ["A. lowicz", "A. Naumowicz"], "venue": "Journal of Formalized Reasoning", "citeRegEx": "lowicz and Naumowicz,? \\Q2010\\E", "shortCiteRegEx": "lowicz and Naumowicz", "year": 2010}, {"title": "Introduction to the Flyspeck project", "author": ["T.C. Hales"], "venue": "Dagstuhl Seminar Proceedings. Internationales Begegnungs- und Forschungszentrum fu\u0308r Informatik (IBFI), Schloss Dagstuhl,", "citeRegEx": "Hales,? \\Q2006\\E", "shortCiteRegEx": "Hales", "year": 2006}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "FMCAD. Vol. 1166 of LNCS. Springer,", "citeRegEx": "Harrison,? \\Q1996\\E", "shortCiteRegEx": "Harrison", "year": 1996}, {"title": "Sine qua non for large theory reasoning", "author": ["K. Hoder", "A. Voronkov"], "venue": "CADE. Vol. 6803 of LNCS. Springer,", "citeRegEx": "Hoder and Voronkov,? \\Q2011\\E", "shortCiteRegEx": "Hoder and Voronkov", "year": 2011}, {"title": "Scalable LCF-style proof translation", "author": ["C. Kaliszyk", "A. Krauss"], "venue": "Proc. of the 4th International Conference on Interactive Theorem Proving (ITP\u201913). Vol. 7998 of LNCS", "citeRegEx": "Kaliszyk and Krauss,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Krauss", "year": 2013}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "CoRR abs/1211.7012", "citeRegEx": "Kaliszyk and Urban,? \\Q2012\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2012}, {"title": "Automated reasoning service for HOL Light", "author": ["C. Kaliszyk", "J. Urban"], "venue": "MKM/Calculemus/DML. Vol. 7961 of Lecture Notes in Computer Science", "citeRegEx": "Kaliszyk and Urban,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "HOL(y)Hammer: Online ATP service for HOL Light", "author": ["C. Kaliszyk", "J. Urban"], "venue": "CoRR abs/1309.4962,", "citeRegEx": "Kaliszyk and Urban,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "2013c. Lemma mining over HOL Light", "author": ["C. Kaliszyk", "J. Urban"], "venue": "LPAR. Vol. 8312 of Lecture Notes in Computer Science", "citeRegEx": "Kaliszyk and Urban,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "2013d. MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "CoRR abs/1310.2805", "citeRegEx": "Kaliszyk and Urban,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "2013e. Stronger automation for Flyspeck by feature weighting and strategy evolution", "author": ["C. Kaliszyk", "J. Urban"], "venue": "PxTP 2013. Vol. 14 of EPiC Series. EasyChair,", "citeRegEx": "Kaliszyk and Urban,? \\Q2013\\E", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV. Vol. 8044 of Lecture Notes in Computer Science. Springer,", "citeRegEx": "Kov\u00e1cs and Voronkov,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov", "year": 2013}, {"title": "MaSh: Machine learning for Sledgehammer", "author": ["D. K\u00fchlwein", "J.C. Blanchette", "C. Kaliszyk", "J. Urban"], "venue": "Proc. of the 4th International Conference on Interactive Theorem Proving (ITP\u201913)", "citeRegEx": "K\u00fchlwein et al\\.,? \\Q2013\\E", "shortCiteRegEx": "K\u00fchlwein et al\\.", "year": 2013}, {"title": "Overview and evaluation of premise selection techniques for large theory mathematics", "author": ["D. K\u00fchlwein", "T. van Laarhoven", "E. Tsivtsivadze", "J. Urban", "T. Heskes"], "venue": "IJCAR. Vol. 7364 of LNCS. Springer,", "citeRegEx": "K\u00fchlwein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "K\u00fchlwein et al\\.", "year": 2012}, {"title": "Prover9 and Mace4, http://www.cs.unm.edu/~mccune/ prover9", "author": ["W. McCune"], "venue": null, "citeRegEx": "McCune,? \\Q2005\\E", "shortCiteRegEx": "McCune", "year": 2005}, {"title": "Translating higher-order clauses to first-order clauses", "author": ["J. Meng", "L.C. Paulson"], "venue": "J. Autom. Reasoning", "citeRegEx": "Meng and Paulson,? \\Q2008\\E", "shortCiteRegEx": "Meng and Paulson", "year": 2008}, {"title": "The PageRank citation ranking: Bringing order to the Web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": null, "citeRegEx": "Page et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Page et al\\.", "year": 1998}, {"title": "Search for faster and shorter proofs using machine generated lemmas", "author": ["P. Pudl\u00e1k"], "venue": "Proceedings of the FLoC\u201906 Workshop on Empirically Successful Computerized Reasoning, 3rd International Joint Conference on Automated Reasoning", "citeRegEx": "Pudl\u00e1k,? \\Q2006\\E", "shortCiteRegEx": "Pudl\u00e1k", "year": 2006}, {"title": "Automated generation of interesting theorems", "author": ["Y. Puzis", "Y. Gao", "G. Sutcliffe"], "venue": "FLAIRS Conference", "citeRegEx": "Puzis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Puzis et al\\.", "year": 2006}, {"title": "Learning search control knowledge for equational deduction", "author": ["S. Schulz"], "venue": "Vol. 230 of DISKI. Infix Akademische Verlagsgesellschaft", "citeRegEx": "Schulz,? \\Q2000\\E", "shortCiteRegEx": "Schulz", "year": 2000}, {"title": "The Design and Implementation of a Compositional CompetitionCooperation Parallel ATP System", "author": ["G. Sutcliffe"], "venue": "Proceedings of the 2nd International Workshop on the Implementation of Logics. No. MPI-I-2001-2006 in Max-Planck-Institut fu\u0308r Informatik, Research Report", "citeRegEx": "Sutcliffe,? \\Q2001\\E", "shortCiteRegEx": "Sutcliffe", "year": 2001}, {"title": "SRASS - a semantic relevance axiom selection system", "author": ["G. Sutcliffe", "Y. Puzis"], "venue": "CADE. Vol. 4603 of LNCS. Springer,", "citeRegEx": "Sutcliffe and Puzis,? \\Q2007\\E", "shortCiteRegEx": "Sutcliffe and Puzis", "year": 2007}, {"title": "MPTP - Motivation, Implementation, First Experiments", "author": ["J. Urban"], "venue": "Journal of Automated Reasoning", "citeRegEx": "Urban,? \\Q2004\\E", "shortCiteRegEx": "Urban", "year": 2004}, {"title": "MoMM - fast interreduction and retrieval in large libraries of formalized mathematics", "author": ["J. Urban"], "venue": "Int. J. on Artificial Intelligence Tools", "citeRegEx": "Urban,? \\Q2006\\E", "shortCiteRegEx": "Urban", "year": 2006}, {"title": "BliStr: The Blind Strategymaker", "author": ["J. Urban"], "venue": "CoRR abs/1301.2683", "citeRegEx": "Urban,? \\Q2013\\E", "shortCiteRegEx": "Urban", "year": 2013}, {"title": "ATP and presentation service for Mizar formalizations", "author": ["J. Urban", "P. Rudnicki", "G. Sutcliffe"], "venue": "J. Autom. Reasoning", "citeRegEx": "Urban et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2013}, {"title": "MaLARea SG1 - Machine Learner for Automated Reasoning with Semantic Guidance", "author": ["J. Urban", "G. Sutcliffe", "P. Pudl\u00e1k", "J. Vysko\u010dil"], "venue": "IJCAR. Vol. 5195 of LNCS. Springer,", "citeRegEx": "Urban et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2008}, {"title": "Theorem proving in large formal mathematics as an emerging AI field", "author": ["J. Urban", "J. Vysko\u010dil"], "venue": "Automated Reasoning and Mathematics: Essays in Memory of William McCune. Vol. 7788 of LNAI. Springer,", "citeRegEx": "Urban and Vysko\u010dil,? \\Q2013\\E", "shortCiteRegEx": "Urban and Vysko\u010dil", "year": 2013}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["J. Urban", "J. Vysko\u010dil", "P. \u0160t\u011bp\u00e1nek"], "venue": "TABLEAUX. Vol. 6793 of LNCS. Springer,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "Using hints to increase the effectiveness of an automated reasoning program: Case studies", "author": ["R. Veroff"], "venue": "J. Autom. Reasoning", "citeRegEx": "Veroff,? \\Q1996\\E", "shortCiteRegEx": "Veroff", "year": 1996}, {"title": "The Isabelle framework", "author": ["M. Wenzel", "L.C. Paulson", "T. Nipkow"], "venue": "TPHOLs. Vol. 5170 of Lecture Notes in Computer Science", "citeRegEx": "Wenzel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wenzel et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 33, "context": ", 2010) (MML), Isabelle/HOL (Wenzel et al., 2008) and HOL Light (Harrison, 1996)/Flyspeck (Hales, 2006) have been translated to formats that", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": ", 2008) and HOL Light (Harrison, 1996)/Flyspeck (Hales, 2006) have been translated to formats that", "startOffset": 22, "endOffset": 38}, {"referenceID": 4, "context": ", 2008) and HOL Light (Harrison, 1996)/Flyspeck (Hales, 2006) have been translated to formats that", "startOffset": 48, "endOffset": 61}, {"referenceID": 25, "context": "allow easy experiments with external automated theorem provers (ATPs) and AI systems (Urban, 2004; Meng and Paulson, 2008; Kaliszyk and Urban, 2012).", "startOffset": 85, "endOffset": 148}, {"referenceID": 18, "context": "allow easy experiments with external automated theorem provers (ATPs) and AI systems (Urban, 2004; Meng and Paulson, 2008; Kaliszyk and Urban, 2012).", "startOffset": 85, "endOffset": 148}, {"referenceID": 8, "context": "allow easy experiments with external automated theorem provers (ATPs) and AI systems (Urban, 2004; Meng and Paulson, 2008; Kaliszyk and Urban, 2012).", "startOffset": 85, "endOffset": 148}, {"referenceID": 6, "context": "Several symbolic AI/ATP methods for reasoning in the context of a large number of related theorems and proofs have been suggested and tried already, including: (i) methods (often external to the core ATP algorithms) that select relevant premises (facts) from the thousands of theorems available in such corpora (Hoder and Voronkov, 2011; K\u00fchlwein et al., 2012), (ii) methods for internal guidance of ATP systems when reasoning in the large-theory setting (Urban et al.", "startOffset": 311, "endOffset": 360}, {"referenceID": 16, "context": "Several symbolic AI/ATP methods for reasoning in the context of a large number of related theorems and proofs have been suggested and tried already, including: (i) methods (often external to the core ATP algorithms) that select relevant premises (facts) from the thousands of theorems available in such corpora (Hoder and Voronkov, 2011; K\u00fchlwein et al., 2012), (ii) methods for internal guidance of ATP systems when reasoning in the large-theory setting (Urban et al.", "startOffset": 311, "endOffset": 360}, {"referenceID": 31, "context": ", 2012), (ii) methods for internal guidance of ATP systems when reasoning in the large-theory setting (Urban et al., 2011), (iii) methods that automatically evolve more and more efficient ATP strategies for the clusters of related problems from such corpora (Urban, 2013), and (iv) methods that learn which of such specialized strategies to use for a new problem (K\u00fchlwein et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 27, "context": ", 2011), (iii) methods that automatically evolve more and more efficient ATP strategies for the clusters of related problems from such corpora (Urban, 2013), and (iv) methods that learn which of such specialized strategies to use for a new problem (K\u00fchlwein et al.", "startOffset": 143, "endOffset": 156}, {"referenceID": 15, "context": ", 2011), (iii) methods that automatically evolve more and more efficient ATP strategies for the clusters of related problems from such corpora (Urban, 2013), and (iv) methods that learn which of such specialized strategies to use for a new problem (K\u00fchlwein et al., 2013).", "startOffset": 248, "endOffset": 271}, {"referenceID": 30, "context": "This setting reasonably corresponds to how large ITP libraries are constructed, and hopefully also emulates how human mathematicians work more faithfully than the classical scenario of a single hard problem consisting of isolated axioms and a conjecture (Urban and Vysko\u010dil, 2013).", "startOffset": 254, "endOffset": 280}, {"referenceID": 0, "context": "The pool of previously proved theorems ranges from thousands in large-theory ATP benchmarks such as MPTP2078 (Alama et al., 2014), to tens of thousands when working with the whole ITP libraries.", "startOffset": 109, "endOffset": 129}, {"referenceID": 15, "context": "So far, systems like HOLyHammer (similar systems include Sledgehammer/MaSh (K\u00fchlwein et al., 2013),MizAR (Urban et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 28, "context": ", 2013),MizAR (Urban et al., 2013; Kaliszyk and Urban, 2013d) and MaLARea (Urban et al.", "startOffset": 14, "endOffset": 61}, {"referenceID": 29, "context": ", 2013; Kaliszyk and Urban, 2013d) and MaLARea (Urban et al., 2008)) have only used the set of named library theorems for proving new conjectures and thus also for the premise-selection learning.", "startOffset": 47, "endOffset": 67}, {"referenceID": 26, "context": "The experiments with the MoMM system over the Mizar library (Urban, 2006) and with the recording of the Flyspeck library (Kaliszyk and Krauss, 2013) have shown", "startOffset": 60, "endOffset": 73}, {"referenceID": 7, "context": "The experiments with the MoMM system over the Mizar library (Urban, 2006) and with the recording of the Flyspeck library (Kaliszyk and Krauss, 2013) have shown", "startOffset": 121, "endOffset": 148}, {"referenceID": 1, "context": "Short alternative proofs: The experiments with AI-assisted ATP over the Mizar and Flyspeck libraries (Alama et al., 2012; Kaliszyk and Urban, 2012) have shown that the combined AI/ATP systems may sometimes find alternative proofs that are much shorter and very different from the human proofs, again turning some \u201chard\u201d named theorems into easy corollaries.", "startOffset": 101, "endOffset": 147}, {"referenceID": 8, "context": "Short alternative proofs: The experiments with AI-assisted ATP over the Mizar and Flyspeck libraries (Alama et al., 2012; Kaliszyk and Urban, 2012) have shown that the combined AI/ATP systems may sometimes find alternative proofs that are much shorter and very different from the human proofs, again turning some \u201chard\u201d named theorems into easy corollaries.", "startOffset": 101, "endOffset": 147}, {"referenceID": 32, "context": "5 To various extent, this problem might be remedied by the alternative learning/guidance methods (ii) and (iii) mentioned in Section 1: Learning of internal ATP guidance using for example Veroff\u2019s hint technique (Veroff, 1996), and learning of suitable ATP strategies using systems like BliStr (Urban, 2013).", "startOffset": 212, "endOffset": 226}, {"referenceID": 27, "context": "5 To various extent, this problem might be remedied by the alternative learning/guidance methods (ii) and (iii) mentioned in Section 1: Learning of internal ATP guidance using for example Veroff\u2019s hint technique (Veroff, 1996), and learning of suitable ATP strategies using systems like BliStr (Urban, 2013).", "startOffset": 294, "endOffset": 307}, {"referenceID": 14, "context": "State-of-the-art ATPs such as Vampire (Kov\u00e1cs and Voronkov, 2013), E (Schulz, 2002) and Prover9 (McCune, 2005\u20132010) implement various variants of the ANL loop (Wos et al.", "startOffset": 38, "endOffset": 65}, {"referenceID": 32, "context": "Veroff\u2019s hint technique (Veroff, 1996) extracts the best lemmas from the", "startOffset": 24, "endOffset": 38}, {"referenceID": 24, "context": ", in SRASS (Sutcliffe and Puzis, 2007) and MaLARea (Urban et al.", "startOffset": 11, "endOffset": 38}, {"referenceID": 29, "context": ", in SRASS (Sutcliffe and Puzis, 2007) and MaLARea (Urban et al., 2008).", "startOffset": 51, "endOffset": 71}, {"referenceID": 22, "context": "A similar lemma-extracting, generalizing and proof-guiding technique (called E Knowledge Base \u2013 EKB) was implemented by Schulz in E prover as a part of his PhD thesis (Schulz, 2000).", "startOffset": 167, "endOffset": 181}, {"referenceID": 21, "context": "AGIntRater (Puzis et al., 2006) is a tool that computes various characteristics of the lemmas that are part of the final refutational ATP proof and aggregates them into an overall interestingness rating.", "startOffset": 11, "endOffset": 31}, {"referenceID": 21, "context": "These characteristics include: obviousness, complexity, intensity, surprisingness, adaptivity, focus, weight, and usefulness, see (Puzis et al., 2006) for details.", "startOffset": 130, "endOffset": 150}, {"referenceID": 23, "context": "To interreduce the large number of such lemmas with respect to subsumption he used the CSSCPA (Sutcliffe, 2001) subsumption tool based on the E prover by Schulz and Sutcliffe.", "startOffset": 94, "endOffset": 111}, {"referenceID": 26, "context": "MoMM (Urban, 2006) adds a number of large-theory features to CSSCPA.", "startOffset": 5, "endOffset": 18}, {"referenceID": 19, "context": "Algorithms such as PageRank (Page et al., 1998) (eigenvector centrality) have today fast approximative implementations that easily scale to billions of nodes.", "startOffset": 28, "endOffset": 47}, {"referenceID": 19, "context": "Pudl\u00e1k (2006) has conducted experiments over several datasets with automated re-use of lemmas from many existing ATP proofs in order to find smaller proofs and also to attack unsolved problems.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "To obtain the full inference graph for Flyspeck we run the proof-recording version of HOL Light (Kaliszyk and Krauss, 2013) patched to additionally remember all the intermediate lemmas.", "startOffset": 96, "endOffset": 123}, {"referenceID": 5, "context": "In order to overcome the above three issues encountered in the first experiments, we followed by gathering data at the level of the HOL Light tactic steps (Harrison, 1996).", "startOffset": 155, "endOffset": 171}, {"referenceID": 5, "context": "In order to efficiently process large formal developments we decided to look at an intermediate level: only at the tactics that are composed using tactic combinators (Harrison, 1996).", "startOffset": 166, "endOffset": 182}, {"referenceID": 5, "context": "In order to overcome the above three issues encountered in the first experiments, we followed by gathering data at the level of the HOL Light tactic steps (Harrison, 1996). The execution of each HOL Light tactic produces a new goal state together with a justification function that produces an intermediate lemma. In this approach, instead of considering all kernel steps, we will consider only the lemmas produced by the justification functions of tactics. The HOL Light tactics work on different levels. The tactics executed by the user and visible in the proof script form the outermost layer. However most of the tactics are implemented as OCaml functions that inspect the goal and execute other (smaller) tactics. If we unfold such internal executions of tactics recursively, the steps performed are of a similar level of detail as in typical natural deduction proofs. This could give us a trace that is slightly smaller than the typical trace of the kernel inferences; however the size is still of the same order of magnitude. In order to efficiently process large formal developments we decided to look at an intermediate level: only at the tactics that are composed using tactic combinators (Harrison, 1996). In order to patch the tactic combinators present in HOL Light and Flyspeck it is enough to patch the three building blocks of tactic combinators: THEN, THENL, and by. Loading Flyspeck with these functions patched takes about 25% more time than the original and requires 6GB of memory to remember all the 20 million new intermediate theorems. This is significantly less than the patched kernel version and the produced graph can be reasonably optimized. The optimizations performed on the level of named theorems can be done here again: recursively splitting conjunctions and normalizing the quantifiers, as well as the premises we get 2,014,505 distinct conjuncts. After alpha-normalization this leaves a trace with 1,067,107 potential intermediate lemmas. In order to find dependencies between the potential intermediate lemmas we follow the approach by Kaliszyk and Krauss (2013) which needs a second dependency recording pass over the whole Flyspeck.", "startOffset": 156, "endOffset": 2099}, {"referenceID": 2, "context": "Modifications that use the initial PageRank scores for more advanced clustering exist (Avrachenkov et al., 2008) and perhaps could be used to mitigate this problem while still keeping the overall processing reasonably fast.", "startOffset": 86, "endOffset": 112}, {"referenceID": 8, "context": "Due to their size (the intermediate lemmas are often large implications), it takes 28 hours to extract and normalize (Kaliszyk and Urban, 2012) all the features.", "startOffset": 117, "endOffset": 143}, {"referenceID": 8, "context": "logic into FOF problems (Kaliszyk and Urban, 2012) and ATPs are run on them in the usual way to make the evaluations.", "startOffset": 24, "endOffset": 50}, {"referenceID": 8, "context": "In order to compare the new results with the extensive experimental results obtained over the previous versions of HOL Light and Flyspeck used in (Kaliszyk and Urban, 2012), we first detect the set of theorems that are preserved between the different versions.", "startOffset": 146, "endOffset": 172}, {"referenceID": 8, "context": "logic into FOF problems (Kaliszyk and Urban, 2012) and ATPs are run on them in the usual way to make the evaluations. In order to compare the new results with the extensive experimental results obtained over the previous versions of HOL Light and Flyspeck used in (Kaliszyk and Urban, 2012), we first detect the set of theorems that are preserved between the different versions. This is done by using the recursive content-based naming of symbols and theorems that we have developed for re-using as much information between different library versions in the HOLyHammer online service Kaliszyk and Urban (2013b). In case of HOL Light the complete set of 1954 core HOL Light theorems evaluated in previous evaluations of HOLyHammer has been preserved, only some of the names have been changed.", "startOffset": 25, "endOffset": 611}, {"referenceID": 8, "context": "When using only the original theorems, the success rate of the 14 most complementary AI/ATP methods developed in (Kaliszyk and Urban, 2012) run with 30s time limit each and restricted to the 1954 core HOL Light theorems is 63.", "startOffset": 113, "endOffset": 139}, {"referenceID": 8, "context": "When combined with the most useful old methods developed in (Kaliszyk and Urban, 2012), the performance of the best 14 methods is 44.", "startOffset": 60, "endOffset": 86}, {"referenceID": 8, "context": "First, the final 14-method HOLyHammer performance reported in (Kaliszyk and Urban, 2012) was 39%, while here it is only 36.", "startOffset": 62, "endOffset": 88}, {"referenceID": 8, "context": "4% improvement over (Kaliszyk and Urban, 2012) is valid, a full-scale evaluation of all the methods on the whole new Flyspeck 9 will likely show a smaller improvement due to the lemma-mining methods.", "startOffset": 20, "endOffset": 46}, {"referenceID": 8, "context": "4% in comparison to the methods developed in (Kaliszyk and Urban, 2012), however this improvement is not only due to the lemma-mining methods, but also due to some of the learning and strategy improvements introduced in (Kaliszyk and Urban, 2013e).", "startOffset": 45, "endOffset": 71}], "year": 2014, "abstractText": "Large formal mathematical libraries consist of millions of atomic inference steps that give rise to a corresponding number of proved statements (lemmas). Analogously to the informal mathematical practice, only a tiny fraction of such statements is named and re-used in later proofs by formal mathematicians. In this work, we suggest and implement criteria defining the estimated usefulness of the HOL Light lemmas for proving further theorems. We use these criteria to mine the large inference graph of the lemmas in the HOL Light and Flyspeck libraries, adding up to millions of the best lemmas to the pool of statements that can be re-used in later proofs. We show that in combination with learning-based relevance filtering, such methods significantly strengthen automated theorem proving of new conjectures over large formal mathematical libraries such as Flyspeck.", "creator": "LaTeX with hyperref package"}}}