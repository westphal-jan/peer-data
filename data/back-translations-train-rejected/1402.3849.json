{"id": "1402.3849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2014", "title": "Scalable Kernel Clustering: Approximate Kernel k-means", "abstract": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k-means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that its running time and memory requirements are significantly lower than those of kernel k-means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.", "histories": [["v1", "Sun, 16 Feb 2014 22:19:40 GMT  (114kb)", "http://arxiv.org/abs/1402.3849v1", "15 pages, 6 figures,extension of the work \"Approximate Kernel k-means: Solution to large scale kernel clustering\" published in KDD 2011"]], "COMMENTS": "15 pages, 6 figures,extension of the work \"Approximate Kernel k-means: Solution to large scale kernel clustering\" published in KDD 2011", "reviews": [], "SUBJECTS": "cs.CV cs.DS cs.LG", "authors": ["radha chitta", "rong jin", "timothy c havens", "anil k jain"], "accepted": false, "id": "1402.3849"}, "pdf": {"name": "1402.3849.pdf", "metadata": {"source": "CRF", "title": "Scalable Kernel Clustering: Approximate Kernel k -means", "authors": ["Radha Chitta", "Rong Jin", "Timothy C. Havens", "Anil K. Jain"], "emails": ["chittara@msu.edu,", "rongjin@cse.msu.edu,", "jain@cse.msu.edu,", "thavens@mtu.edu"], "sections": [{"heading": null, "text": "We show that the proposed method of reducing memory requirements means a smaller reduction in cluster performance than the accumulation of cluster data compared to traditional methods of reducing cluster performance: Approximate Kernel k -meansRadha Chitta, Rong Jin, Timothy C. Havens and Anil K. JainAbstract - Kernel-based cluster algorithms have the ability to capture the nonlinear structure in real data. Under different kernel-based cluster algorithms, kernel k -means has gained popularity due to its simple iterative nature and easy implementation. However, its runtime complexity and memory requirements increase quadratically in relation to the size of the dataset, and therefore large datasets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, referred to as Approximate Kernel-totals, which means we approach points between cluster centers among all the data centers."}, {"heading": "1 INTRODUCTION", "text": "Most algorithms capable of capturing large amounts of data assume that the data sets are used in a variety of applications such as web search, social networking, image retrieval, medical imaging, gene expression, recommendation systems, and market analytics. [2] Most algorithms capable of capturing large amounts of data assume that the data sets are more linear and grouped. [3] R. Chitta, R. Jin, and A.K. Jain are affiliated with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, 48824."}, {"heading": "2 BACKGROUND", "text": "We first briefly describe some of the related work on large-scale clustering and kernel-based clustering, and then sketch the k-means algorithm of the kernel."}, {"heading": "2.1 Large scale clustering", "text": "A number of methods have been developed to efficiently cluster large datasets. Incremental clustering [20], [21] and divide-and-conquer-based clustering algorithms [22], [23] are designed to operate across data points in a single pass, reducing the time needed for clustering. Sampling-based methods such as CLARA [24] and CURE [25] reduce computing time by locating cluster centers based on a small number of randomly selected datasets. Coreset algorithms [26] represent the dataset based on a small set of core datasets and find cluster centers only using these core datasets. Clustering algorithms such as BIRCH [27] and CLARANS [28] enhance cluster efficiency by grouping the dataset into data structures such as trees and graphs, allowing efficient data access to the cluster centers by enabling the development of the computing center to the small number of 30."}, {"heading": "2.2 Kernel-based clustering", "text": "Most of the existing large-scale clustering methods calculate the paired differences between data points using Euclidean distance measurement. As a result, they cannot precisely cluster data sets that are not linearly separable. Kernel-based cluster techniques address this constraint by using a nonlinear kernel removal function to capture the nonlinear structure in data [3]. Various kernel-based cluster algorithms have been developed, including kernel means [7], [8], spectral clustering [4], kernel SOM [5], and kernel neural gas [6]. Scalability is a major challenge for all kernel-based algorithms, since they require the calculation of the full kernel matrix, the size of which is square in the number of data points."}, {"heading": "2.3 Kernel k -means", "text": "Let X = {x1, x2,..., xn} be the input dataset consisting of n data points, where xi d, C is the number of clusters, and K n is the kernel matrix with Kij (xi, xj), and | \u00b7 H is the functional norm for the kernel function. Therefore, the kernel k mean is to minimize the cluster error, defined as the sum of the squared distances between the data points and the center of the cluster to which each point is assigned. Therefore, the kernel k mean problem can be ejected as the following optimization problem. The goal of the kernel mean is to minimize the cluster error, defined as the sum of the squared distances between the data points and the center of the cluster."}, {"heading": "3 APPROXIMATE KERNEL k -MEANS", "text": "A simple and naive approach to reducing the complexity of all data to be captured (see (3) the centers) is to randomly capture m points from the data set to be captured and find the cluster centers that are based only on the data collected; then assign each unsampled data point to the cluster whose center is closest to it. We refer to this two-step process as the two-step method that provides the data to a sufficient extent. We propose a superior approach to reducing the complexity of k means based on the fact that kernel k means requires the full n method of the kernel mean algorithms unless a sufficiently large sample of data points is provided. We propose a superior approach to reducing the complexity of k means based on the fact that kernel k means require the full n-level of the matrixK only because the clusters are represented as {cluster-binaries}."}, {"heading": "4 ENSEMBLE APPROXIMATE KERNEL k - MEANS", "text": "We improve the quality of the aKKm solution by group clustering.The objective of the ensemble clustering.U is to combine multiple partitions of the given dataset. It is based on a hypergraph partitioning that includes the average matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices matrices"}, {"heading": "5 ANALYSIS OF APPROXIMATE KERNEL k -MEANS", "text": "In this section we will first show that the computational complexity of the aKKm algorithm is lower than that of the kernel-k-means algorithm, and then deduce limits on the difference between the cluster error achieved by our algorithm and the kernel-k-means algorithm."}, {"heading": "5.1 Computational complexity", "text": "The aKKm algorithm consists of two parts: core computation and clustering. Since only a n \u00b7 m part of the kernel needs to be computed and stored, the cost of core computation is O (ndm), a dramatic reduction from the O (n2d) complexity of the classic k mean. Memory requirements are also reduced to O (mn). The most expensive cluster operation is matrix inversion K \u2212 1 and the calculation of T = KBK \u2212 1, which has computing costs of O (m3 + m2n). The cost of computing and updating the matrix U isO (mnCl), where l is the number of iterations required for convergence. Therefore, the total cost of clustering is O (m3 + m2n + mnCl). We can further reduce these costs by significantly optimizing the matrix problem K that we need for convergence."}, {"heading": "5.2 Clustering error", "text": "Let us leave the binary random variables for the construction of the subspace Hb, which corresponds to kernel mean usage, as a whole. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...) Let us leave the binary variables. (...) Let us leave the binary random variables. (...) Let us leave the variable. (...) Let us leave the binary random variables. (...). (...) Let us leave the binary random variables. (...) Let us leave the binary random variables. (...)"}, {"heading": "5.3 Parameter sensitivity", "text": "One of the important factors determining the performance of the approximate k mean algorithm is the sample size m. Sampling results in a trade-off between cluster quality and efficiency. As m increases, cluster quality improves, but the acceleration achieved by the algorithm suffers. The following theorem gives an estimate of m. Theorem 2. Let 1 = diag (\u03bb1,..,.,.), 2 = diag (\u03bbC + 1,.,.), Z1 = (z1,., zC) and Z = (zC + 1,., zn). Let\u03c4 = n max 1 \u2264 i \u2264 n | Z (i) | 2 (16) denote the coherence of the core matrix K (adjusted by [16])) and Z = (zC + 1,."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In this section, we will show that aKKm is an efficient and scalable variant of the kernel k-means algorithm. It has lower runtime and memory requirements, but is on a par with the kernel k-means in terms of cluster errors and cluster quality. We tested our algorithm on four sets of data of different sizes: Imagenet, MNIST, Forest Cover Type and Network Intrusion Datasets (Table 1). Using small and medium-sized datasets (Imagenet and MNIST), for which full kernel computation is possible on a single processor, we will show that the cluster performance of our algorithm is similar to that of the kernel k-means algorithm. We will then demonstrate scalability using the large Forest Cover type and network intrusion datasets. Finally, we will show that performance can be improved by using the aKKm ensemble algorithm. All algorithms have been limited to a MAT4 processor, 2.40 GHz, and 2.40 GHz."}, {"heading": "6.1 Performance comparison with kernel k - means", "text": "We use the Imagenet and MNIST datasets to show that the clustering performance of the approximate k-means algorithm is comparable to that of the k-means algorithm of the kernel."}, {"heading": "6.1.1 Datasets", "text": "\u2022 Imagenet: The Imagenet dataset [41] consists of over 1.2 million images organized according to the WordNet hierarchy. Each node in this hierarchy represents a concept (known as \"Synset\"). We selected 20,000 images from 12 synsets. We extracted key points from each image using the VLFeat library [42] and presented each key point as 128 Dimensional4. We used the k-means implementation in the MATLAB Statistics Toolbox and the Nystrom Approximation-based implementation of spectral clustering [40], available at http: / / alumni.cs.ucsb.edu / \u0445 wychen / sc.html. The remaining algorithms were implemented in-house. SIFT descriptor; an average of 3,055 key points were extracted from each image. \u2022 MNIST: The MNIST dataset 43 [is a subset of handwritten data base] is available from the IST."}, {"heading": "6.1.2 Experimental setup", "text": "We compare the approximate kernel k-means (aKKm) algorithm with the kernel k-means algorithm to show that it performs a similar function, then compare it with the two-stage kernel k-means (tKKm) algorithm, and show that the error is related to the kernel k-means algorithm (nysSC) [14], which strings together the upper C eigenvectors of a low order achieved by Nystrom approximation technology. (ii) The leaders are based on the k-means algorithm (lKm), which finds some representative patterns (called leaders) and then the kernel k-means on the leaders."}, {"heading": "6.1.3 Experimental results", "text": "The kernel computation time is common in both the aKKm and tKKm algorithms. We observe that a significant speedup (over 90%) is achieved by both algorithms in the kernel computation, if both the algorithms in the kernel computation and the kernel computation in the kernel computation are achieved in the kernel computation k-means. tKm is the most efficient in the aKm and tKKm algorithms. We observe that a significant acceleration (over 90%) is achieved by both algorithms in the kernel computation, if the algorithms in the kernel computation k-means. tKm is the most efficient in the clustering speedup. aKm takes longer than it needs to compute the inverse matrix K-1. However, if the algorithms are compared with the request, we will see later that the performance of clustering is more efficient."}, {"heading": "6.2 Performance of Approximate kernel k - means using different sampling strategies", "text": "Table 4 (a) and Figures 3 (a) and 3 (b) therefore compare the diagonal sampling, the standard sample and the kmeans sampling strategies with the uniform random sample. In Table 4 (a), we assume that the n \u00d7 n core matrix is calculated in advance and only includes the time for sorting the diagonal inputs (for diagonal sampling) or calculating the column norms (for the standard sample) and the time for selecting the first m indices in the sampling time. For the k mean sample, we show the time needed to execute the k mean sample and search for the representative sample. As expected, the 10 sampling times for all non-uniform sampling techniques are greater than the time required for the sample. Due to the high complexity of the split standard sampling and the k mean sample, the 10 sampling times for all non-uniform sampling techniques do not show significantly higher than the average values."}, {"heading": "6.3 Scalability", "text": "Using the large Forest Cover type and Network Intrusion datasets, we demonstrate that the proposed algorithm is scalable to large datasets, and the aKKm algorithm requires less than 40 GB of memory, dramatically reducing cluster storage requirements."}, {"heading": "6.3.1 Datasets", "text": "\u2022 Forest Cover Type: This dataset [46] consists of cartographic variables drawn from data from the US Geological Survey (USGS) and the US Forest Service (USFS). Each of the 581 012 datasets represents the attributes of a 30 x 30 metre forest floor cell. There are a total of 12 attributes, including 11 qualitative metrics such as soil type and wilderness, and quantitative metrics such as inclination, height and distance to hydrology. These 12 attributes are represented by 54 characteristics. Data are grouped into 7 classes, each representing a different forest type. The true coverage type was determined from data from the USFS Region 2 Resource Information System (RIS). \u2022 Network Intrusion: The Network Intrusion dataset [47] contains 4,898,431 50-dimensional patterns representing TCP dump data from seven weeks of local network traffic."}, {"heading": "6.3.2 Experimental setup", "text": "The aKKm algorithm facilitates this problem of complexity. We compare the performance of the aKKm algorithm on these datasets in terms of runtime, error reduction and NMI with that of the k mean, tKm and nysSC algorithms. We found that the lKKm algorithm takes longer than 24 hours to find the leaders for these large datasets, which clearly demonstrates its non-scalability. Therefore, we deleted this algorithm from the set of basic algorithms. We evaluate the efficiency of the aKKm algorithm for various sample sizes from 100 to 5000 000. On the network intrusion dataset, the value of m is only increased to 2 000 because larger values of m require more than 40 GB of memory. On the cover type 35, we use the parameter Blarity-3 in the same way as the kernel."}, {"heading": "6.3.3 Experimental results", "text": "Forest Cover Type: We compare the runtime of the algorithms in Table 2 (c). As in the MNIST dataset, the kernel computation time is minimal compared to the cluster time and the aKKm algorithm is less efficient than the tKKm algorithm in terms of cluster time. Compared to the nysSC algorithm, its runtime is higher if the sample size m is small, but as the time required by the nysSC algorithm increases cubic with m, the aKKm algorithm becomes more efficient when m is increased. It is faster than the k mean algorithm when m < 500. Fig. 4 (c) and 5 (c) show the effectiveness of our algorithm in terms of error reduction and NMI, 12 respectively. In Fig. 4 (c) we observe that a much higher error reduction is achieved by the aKKm algorithm than the Km algorithm."}, {"heading": "6.4 Ensemble aKKm", "text": "We use the ensemble aKKm algorithm to combine 10 ensemble partitions, and the times required to combine the partitions (averaged over 10 runs) for each of the data sets are shown in Table 5. We note that these times are small compared to the cluster times, and therefore have no significant impact on the total runtime. Figure 6 shows the improvement in NMI achieved by using ensembles on the four data sets. A significant improvement can be observed, especially if the sample size m is small. Figure 6 (b), for example, shows an NMI of about 0.48 is achieved on the MNIST data set, even with a small sample size of m = 100. The NMI increases by about 15% and is almost identical to the average NMI determined for a sample size m = 1,000. On the Cover Type and Network Intrusion data sets, there are significant improvements in the NMI values of the kernel, resulting in a good efficiency of the cluster value."}, {"heading": "7 CONCLUSIONS", "text": "We have proposed an efficient approximation for the kernel k-means algorithm, which is suitable for large datasets. The key idea is to avoid the calculation of the full kernel matrix by limiting cluster centers to a subspace spanned by a small set of randomly sampled datasets. We show theoretically and empirically that the proposed algorithm is efficient in terms of both computational complexity and memory requirements, and can deliver cluster results similar to those of the kernel k-means algorithm using the full kernel matrix. In most cases, the performance of our algorithm is better than that of other popular large-scale kernel cluster algorithms. By integrating ensemble clusters with the proposed algorithm, its efficiency will be further increased. In the future, we plan to further improve the scalability of kernel clustering by developing more efficient kernel approximation techniques."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was supported by the Office of Naval Research (ONR N00014-11-1-0100) < < < / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}], "references": [{"title": "Approximate kernel k-means: solution to large scale kernel clustering", "author": ["R. Chitta", "R. Jin", "T. Havens", "A. Jain"], "venue": "Proceedings of the SIGKDD conference on Knowledge Discovery and Data mining, 2011, pp. 895\u2013903.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of the performance of clustering algorithms in kernel-induced feature space", "author": ["D. Kim", "K. Lee", "D. Lee", "K. Lee"], "venue": "Pattern Recognition, vol. 38, no. 4, pp. 607\u2013 611, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The kernel self-organising map", "author": ["D. MacDonald", "C. Fyfe"], "venue": "Proceedings of the International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies, vol. 1, 2002, pp. 317\u2013320.  14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel neural gas algorithms with application to cluster analysis", "author": ["A.K. Qinand", "P.N. Suganthan"], "venue": "Pattern Recognition, vol. 4, pp. 617\u2013620, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Mercer kernel-based clustering in feature space", "author": ["M. Girolami"], "venue": "IEEE Transactions on Neural Networks, vol. 13, no. 3, pp. 780\u2013784, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Scholkopf", "A. Smola", "K. Muller"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1299\u20131314, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "A unified view of kernel k-means, spectral clustering and graph cuts", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "University of Texas at Austin, Tech. Rep., 2004, (Tech. rep. TR-04-25).", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["C. Ding", "X. He", "H. Simon"], "venue": "Proceedings of the SIAM Data Mining Conference, 2005, pp. 606\u2013610.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "C. Ding", "M. Gu", "X. He", "H. Simon"], "venue": "Advances in Neural Information Processing Systems, vol. 14, pp. 1057\u20131064, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 2, pp. 369\u2013374, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Nystrom method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 2153\u20132175, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral grouping using the Nystrom method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 214\u2013225, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Using the Nystrom method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "Advances in Neural Information Processing Systems, 2001, pp. 682\u2013688.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The spectral norm error of the naive nystrom extension", "author": ["A. Gittens"], "venue": "Arxiv preprint arXiv:1110.5305, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.-Y. Chen", "Y. Song", "H. Bai", "C.-J. Lin", "E. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 3, pp. 568 \u2013586, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation for SAR Image Based on a New Spectral Clustering Algorithm", "author": ["L. Liu", "X. Wen", "X. Gao"], "venue": "Life System Modeling and Intelligent Computing, pp. 635\u2013643, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M. Jordan"], "venue": "EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-45, 2009. [Online]. Available: http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-45.html", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental clustering for dynamic information processing", "author": ["F. Can"], "venue": "ACM Transactions on Information Systems, vol. 11, no. 2, pp. 143\u2013164, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Incremental clustering for very large document databases: Initial MARIAN experience", "author": ["F. Can", "E. Fox", "C. Snavely", "R. France"], "venue": "Information Sciences, vol. 84, no. 1-2, pp. 101\u2013 114, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "A framework for clustering evolving data streams", "author": ["C. Aggarwal", "J. Han", "J. Wang", "P. Yu"], "venue": "Proceedings of the International Conference on Very Large Databases, 2003, pp. 81\u201392.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 515\u2013528, 2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding Groups in Data: An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Cure: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "Information Systems, vol. 26, no. 1, pp. 35\u201358, 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Proceedings of the ACM Symposium on Theory of Computing, 2004, pp. 291\u2013300.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "BIRCH: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "ACM SIGMOD Record, vol. 25, no. 2, pp. 103\u2013114, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "CLARANS: A method for clustering objects for spatial data mining", "author": ["R. Ng", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 1003\u20131016, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the International Conference on World Wide Web, 2007, pp. 271\u2013280.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast clustering using mapreduce", "author": ["A. Ene", "S. Im", "B. Moseley"], "venue": "Proceedings of the International conference on Knowledge discovery and data mining, 2011, pp. 681\u2013689.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating mapreduce for multi-core and multiprocessor systems", "author": ["C. Ranger", "R. Raghuraman", "A. Penmetsa", "G. Bradski", "C. Kozyrakis"], "venue": "IEEE Symposium on High Performance Computer Architecture, 2007, pp. 13\u201324.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A large scale clustering scheme for kernel k-means", "author": ["R. Zhang", "A. Rudnicky"], "venue": "Proceedings of the International Conference on Pattern Recognition, 2002, pp. 289\u2013292.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Speeding-up the kernel k-means clustering method: A prototype based hybrid approach", "author": ["T. Sarma", "P. Viswanath", "B. Reddy"], "venue": "Pattern Recognition Letters, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast spectral clustering with random projection and sampling", "author": ["T. Sakai", "A. Imiya"], "venue": "Machine Learning and Data Mining in Pattern Recognition, pp. 372\u2013384, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized representer theorem", "author": ["B. Scholkopf", "R. Herbrich", "A. Smola"], "venue": "Proceedings of Computational Learning Theory, 2001, pp. 416\u2013426.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Schulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 25, no. 3, pp. 337\u2013 372, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Cluster ensembles - a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Entropy and correlation: Some comments", "author": ["T. Kvalseth"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 17, no. 3, pp. 517\u2013519, 1987.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1987}, {"title": "A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices", "author": ["G. Karypis", "V. Kumar"], "venue": "University of Minnesota (998), 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W. Chen", "Y. Song", "H. Bai", "C. Lin", "E. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 3, pp. 568\u2013586, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org, 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1998}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, vol. 2, 2006, pp. 2169\u20132178.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of Classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1985}, {"title": "Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables", "author": ["J. Blackard", "D. Dean"], "venue": "Computers and Electronics in Agriculture, vol. 24, no. 3, pp. 131\u2013152, 1999.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Cost-based modeling for fraud and intrusion detection:  15 Results from the jam project", "author": ["S. Stolfo", "W. Fan", "W. Lee", "A. Prodromidis", "P. Chan"], "venue": "Proceedings of DARPA Information Survivability Conference and Exposition, vol. 2, 2000, pp. 130\u2013144.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2000}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["E. Candes", "J. Romberg"], "venue": "Inverse Problems, vol. 23, no. 3, pp.  969\u2013985, 2007.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Geometry on probability spaces", "author": ["S. Smale", "D.-X. Zhou"], "venue": "Constructive Approximation, vol. 30, pp. 311\u2013323, 2009.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "It has found use in a multitude of applications such as web search, social network analysis, image retrieval, medical imaging, gene expression analysis, recommendation systems and market analysis [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "A previous version of this paper appeared as [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "Kernel clustering algorithms, therefore, have the ability to capture the non-linear structure in real world data sets and, thus, usually perform better than the Euclidean distance based clustering algorithms [3].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "A number of kernel-based clustering methods such as spectral clustering [4], kernel Self-", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Organizing Maps (SOM) [5] and kernel neural gas [6] have been proposed.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "Organizing Maps (SOM) [5] and kernel neural gas [6] have been proposed.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "In this study, we focus on kernel k-means [7], [8] due to its simplicity and efficiency.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "In this study, we focus on kernel k-means [7], [8] due to its simplicity and efficiency.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "In addition, several studies have established the equivalence of kernel k-means and other kernel-based clustering methods, suggesting that they yield similar results [9]\u2013[11].", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "In addition, several studies have established the equivalence of kernel k-means and other kernel-based clustering methods, suggesting that they yield similar results [9]\u2013[11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Kernel k-means [7], a non-linear extension of the classical k-means algorithm, replaces the Euclidean distance function d(xa, xb) = \u2016xa\u2212xb\u2016 employed in the k-means algorithm with a non-linear kernel distance function defined as d\u03ba(xa, xb) = \u03ba(xa, xa) + \u03ba(xb, xb)\u2212 2\u03ba(xa, xb), where xa \u2208 Rd and xb \u2208 Rd are two data points and \u03ba(.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "The Nystrom method for kernel approximation has been successfully employed in several learning problems [12]\u2013[15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The Nystrom method for kernel approximation has been successfully employed in several learning problems [12]\u2013[15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "The naive Nystrom approximation method [16] randomly samples a small number of points from the data set and computes a low rank approximation of the kernel matrix using the similarity between all the points and the sampled points.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Unlike the spectral clustering algorithm based on the naive Nystrom extension [14], our method uses information from all the eigenvectors of the approximate kernel matrix (without explicitly computing them), thereby yielding more accurate clustering results.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Sampling based methods, such as CLARA [24] and CURE [25], reduce the computation time by finding the cluster centers based on a small number of randomly selected data points.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Sampling based methods, such as CLARA [24] and CURE [25], reduce the computation time by finding the cluster centers based on a small number of randomly selected data points.", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "The coreset algorithms [26] represent the data set using a small set of core data points and find the cluster centers using only these core data points.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "Clustering algorithms such as BIRCH [27] and CLARANS [28] improve the clustering efficiency by summarizing the data set into data structures like trees and graphs, thus enabling efficient data access.", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "Clustering algorithms such as BIRCH [27] and CLARANS [28] improve the clustering efficiency by summarizing the data set into data structures like trees and graphs, thus enabling efficient data access.", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "With the evolution of cloud computing, parallel processing techniques for clustering are gaining popularity [29], [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "With the evolution of cloud computing, parallel processing techniques for clustering are gaining popularity [29], [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 29, "context": "For instance, in [30], the MapReduce framework [31] is employed to speedup the k-means and the kmedians clustering algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "For instance, in [30], the MapReduce framework [31] is employed to speedup the k-means and the kmedians clustering algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Kernel based clustering techniques address this limitation by employing a non-linear kernel distance function to capture the non-linear structure in data [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 31, "context": "In [32], the memory requirement is reduced by dividing the kernel matrix into blocks and using one block of the kernel matrix at a time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "The leaders clustering algorithm is integrated with kernel k-means to reduce its computational complexity in [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 150, "endOffset": 154}, {"referenceID": 17, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "In [34], random projection is combined with sampling to further improve the clustering efficiency.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Hence, the kernel k-means problem can be cast as the following optimization problem [11]:", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "The problem in (1) can be relaxed to the following optimization problem over U [11]:", "startOffset": 79, "endOffset": 83}, {"referenceID": 34, "context": "We propose a superior approach for reducing the complexity of kernel k-means based on the fact that kernel k-means requires the full n \u00d7 n kernel matrixK only because the the cluster centers {ck(\u00b7), k \u2208 [C]} are represented as linear combinations of all the data points to be clustered (see (3)) [35].", "startOffset": 296, "endOffset": 300}, {"referenceID": 35, "context": "The objective of ensemble clustering [36] is to combine multiple partitions of the given data set.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "A popular ensemble clustering algorithm is the MetaClustering algorithm (MCLA) [37], which maximizes the average normalized mutual information.", "startOffset": 79, "endOffset": 83}, {"referenceID": 37, "context": "where NMI(U, U ), the Normalized Mutual Information (NMI) [38] between two partitions a and b, represented by the membership matrices U and U b respectively, is defined by", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "NMI values lie in the range [0, 1].", "startOffset": 28, "endOffset": 34}, {"referenceID": 38, "context": "This meta-graph is partitioned using a graph partitioning algorithm such as METIS [39] to obtain C balanced meta-clusters \u03c01, \u03c02, .", "startOffset": 82, "endOffset": 86}, {"referenceID": 36, "context": "Ensemble aKKm: In the ensemble aKKm algorithm, an additional cost of O(nCr) is incurred for combining the partitions using MCLA [37].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "denote the coherence of the kernel matrix K (adapted from [16]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "This improves over the approximation error bound of the naive Nystrom method presented in [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "\u2022 Imagenet: The Imagenet data set [41] consists of over 1.", "startOffset": 34, "endOffset": 38}, {"referenceID": 41, "context": "We extracted keypoints from each image using the VLFeat library [42] and represented each keypoint as a 128 dimensional", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "We used the k-means implementation in the MATLAB Statistics Toolbox and the Nystrom approximation based spectral clustering implementation [40] available at http://alumni.", "startOffset": 139, "endOffset": 143}, {"referenceID": 42, "context": "\u2022 MNIST: The MNIST data set [43] is a subset of the database of handwritten digits available from NIST.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "We also gauge it\u2019s performance against that of (i) the Nystrom spectral clustering algorithm (nysSC) [14], which clusters the top C eigenvectors of a low rank approximate kernel obtained through the Nystrom approximation technique, (ii) the leaders based kernel k-means algorithm (lKKm) [33] which finds a few representative patterns (called leaders) based on a user-defined distance threshold and then runs kernel k-means on the leaders, and (iii) the k-means algorithm to show that it achieves a better clustering accuracy.", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "We also gauge it\u2019s performance against that of (i) the Nystrom spectral clustering algorithm (nysSC) [14], which clusters the top C eigenvectors of a low rank approximate kernel obtained through the Nystrom approximation technique, (ii) the leaders based kernel k-means algorithm (lKKm) [33] which finds a few representative patterns (called leaders) based on a user-defined distance threshold and then runs kernel k-means on the leaders, and (iii) the k-means algorithm to show that it achieves a better clustering accuracy.", "startOffset": 287, "endOffset": 291}, {"referenceID": 43, "context": "For the Imagenet data set, we employ the spatial pyramid kernel [44] to calculate the pairwise similarity with the number of pyramid levels set to be 4.", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "11 respectively, as suggested in [32].", "startOffset": 33, "endOffset": 37}, {"referenceID": 44, "context": "To evaluate the difference between the clustering results of the aKKm and tKKm algorithms, and the kernel k-means, we calculate the Adjusted Rand Index (ARI) [45], a measure of similarity between two data partitions.", "startOffset": 158, "endOffset": 162}, {"referenceID": 0, "context": "The adjusted Rand index value lies in [0, 1].", "startOffset": 38, "endOffset": 44}, {"referenceID": 45, "context": "\u2022 Forest Cover Type: This data set [46] is composed of cartographic variables obtained from the US Geological Survey (USGS) and the US Forest Service (USFS) data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 46, "context": "\u2022 Network Intrusion: The Network Intrusion data set [47] contains 4,898,431 50-dimensional patterns representing TCP dump data from seven weeks of local-area network traffic.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 57, "endOffset": 61}, {"referenceID": 47, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "(Theorem 1 from [16]) Let A be a n \u00d7 n positive semi-definite matrix and S \u2208 {0, 1}n\u00d7m be a random sampling matrix.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "2 from [48]) Let Z \u2208 Rn\u00d7n include the eigenvectors of a positive semi-definite A", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "(Lemma 2 from [49]) Let H be a Hilbert space and \u03be be a random variable on (Z, \u03c1) with values in H .", "startOffset": 14, "endOffset": 18}], "year": 2014, "abstractText": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k -means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that it\u2019s running time and memory requirements are significantly lower than those of kernel k -means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.", "creator": "LaTeX with hyperref package"}}}