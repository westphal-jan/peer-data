{"id": "1402.4084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2014", "title": "Selective Sampling with Drift", "abstract": "Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting.", "histories": [["v1", "Mon, 17 Feb 2014 17:53:57 GMT  (296kb,D)", "http://arxiv.org/abs/1402.4084v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["edward moroshko", "koby crammer"], "accepted": false, "id": "1402.4084"}, "pdf": {"name": "1402.4084.pdf", "metadata": {"source": "CRF", "title": "Selective Sampling with Drift", "authors": ["Edward Moroshko", "Koby Crammer"], "emails": ["edward.moroshko@gmail.com", "koby@ee.technion.ac.il"], "sections": [{"heading": null, "text": "Lately, a lot of work has been done on selective sampling, an online setting for active learning where algorithms work in rounds. In each round, an algorithm receives an input and makes a prediction, then can decide whether to query a label and, if so, update its model, otherwise the input will be discarded. However, most of this work focuses on the stationary case where there is a fixed target model and the algorithm's performance is compared with a fixed model. However, in many real-world applications, such as spam prediction, the best target function can drift over time or shift from time to time. We are developing a novel selective sampling algorithm for drifting setting, analyzing it under no assumptions about the mechanism that generates the sequence of instances, and deducing new error boundaries that depend on the amount of drift in the problem. Simulations on synthetic and real data sets show the superiority of our algorithms."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2 Problem setting", "text": "We consider the standard online learning model [1, 15] for binary classification, in which learning progresses in a sequence of rounds t = 1, 2,., T. (In round t, the algorithm observes an instance of xt-Rd and gives a prediction y = 1, + 1) for the yt label associated with xt. (We say that the algorithm made a prediction error when it exampled a sequence of errors in a sequence of T. The performance of an algorithm is measured by the total number of errors it makes on any sequence of examples.) In the standard performance model, the goal is to example this total number of errors in a sequence of T. The performance of an algorithm is measured by the total number of errors it makes on any sequence of examples."}, {"heading": "3 Algorithms", "text": "(D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D (D) (D) (D) (D) (D (D) (D) (D) (D) (D) (D) (D (D) (D) (D (D) (D) (D) (D (D) (D) (D (D) (D) (D) (D) (D) (D) (D (D) (D) (D (D) (D) (D) (D) (D) (D) (D) (D) (D) (D"}, {"heading": "4 Analysis", "text": "We now turn out to be reference persons for the number of errors of our algorithms. We provide an error for the fully monitored version (LASEC) and for the selective sampling version (LASEC-SS). Our limits depend on the total number of reference sequences Vm = 2). For the monitored setting, it is the amount of erroneous attempts (Mt = 1). For the selective sampling sequences, it is the set of indices, if Zt = 1 and Mt = 1.Theorem 2. Assume the LASEC algorithms (Fig. 1 with a Fig. 1 with a finite sequence of examples. Then for each reference sequence {ut} and the number of errors."}, {"heading": "5 Experimental Study", "text": "We evaluated our algorithms with synthetic and real-world data with displacement by using the average accuracy (total number of correct online classifications divided by the number of examples) of LASEC and LASEC-SS.Data: In our first experiment we used a synthetic dataset with 10,000 examples of dimension d = 50. The input from samplers is drawn by a zero-mean covariance Gaussian Distribution. This is u1 = u500, u501 = u1000, u501 =... The labels are set by yt = characters (x > t ut). Our second experiment uses the US Postal Service handwritten digital recognition corpus (USPS) [14]. It contains standardized grayscale images of size 16 x 16, divided into a training (test)."}, {"heading": "6 Conclusions", "text": "We proposed a novel second-order binary classification algorithm designed for non-stationary (drifting) selective sampling settings. Our algorithm is based on the last step of the min-max approach, and we showed how to solve the problem of minimum and maximum optimization directly for classification using square loss. To our knowledge, this is the first algorithm designed for selective sampling setting when there is a drift. We proved that our algorithm is error-bound in the monitored and selective sampling settings, and a limit to the expected number of errors in the selective sampling version of the algorithm. An interesting direction is to design algorithms that automatically detect the drift level and select sampling settings. In order for the algorithm to work well, the height of the drift V or a limit on the expected number of errors in the pre-selective version of the algorithm should be known from the sampling."}, {"heading": "A SUPPLEMENTARY MATERIAL", "text": "A.1 Detection of em. 7Detection. We follow the equivalence of the following inequalities: m \u2212 1 \u03b3 D \u2264 1 \u03b3 \u221a A (B + mC) m2 \u2212 2 \u03b3 mD + 1 \u03b32 D2 \u2264 1 \u03b32 A (B + mC) m2 \u2212 (2\u03b3 D + 1\u03b32 AC) m + 1\u03b32 D2 \u2212 1 \u03b32 AB \u2264 0m \u2264 (2\u03b3 D + 1\u03b32 AC + \u221a (2\u03b3 D + 1\u03b32 AC) 2 \u2212 4 (1\u03b32 D2 \u2212 1 \u03b32 AB) / 2 = 1\u03b3 D + 12\u03b32 AC + 1\u03b3 \u221a 1\u03b3 DAC + 14\u03b32 (AC) 2 + AB."}], "references": [{"title": "Queries and concept learning", "author": ["Dana Angluin"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["K.S. Azoury", "M.W. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "A polynomial-time algorithm for learning noisy linear threshold functions", "author": ["Avrim Blum", "Alan M. Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["Giovanni Cavallanti", "Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A second-order perceptron algorithm", "author": ["Nicol\u00f3 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "Siam Journal of Commutation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Robust bounds for classification via selective sampling", "author": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Francesco Orabona"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Worst-case analysis of selective sampling for linear classification", "author": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Adaptive regularization of weighted vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Regret minimization with concept drift", "author": ["Koby Crammer", "Yishay Mansour", "Eyal Even-Dar", "Jennifer Wortman Vaughan"], "venue": "In COLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Robust selective sampling from single and multiple teachers", "author": ["Ofer Dekel", "Claudio Gentile", "Karthik Sridharan"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "On relative loss bounds in generalized linear regression", "author": ["Jurgen Forster"], "venue": "In Fundamentals of Computation Theory (FCT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Selective sampling using the Query By Committee algorirhm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1987}, {"title": "A last-step regression algorithm for non-stationary online learning", "author": ["Edward Moroshko", "Koby Crammer"], "venue": "In AISTATS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Better algorithms for selective sampling", "author": ["Francesco Orabona", "Nicol\u00f2 Cesa-Bianchi"], "venue": "In ICML, pages 433\u2013440,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Least Squares Support Vector Machines", "author": ["J.A.K. Suykens", "T. van Gestel", "J. de Brabanter"], "venue": "World Scientific Publishing Company Incorporated,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Least squares support vector machine classifiers", "author": ["Johan A.K. Suykens", "Joos Vandewalle"], "venue": "Neural Processing Letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Re-adapting the regularization of weights for non-stationary regression", "author": ["Nina Vaits", "Koby Crammer"], "venue": "In The 22nd International Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Competitive on-line statistics", "author": ["Volodya Vovk"], "venue": "International Statistical Review,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}], "referenceMentions": [{"referenceID": 17, "context": "Following the pioneering work of Rosenblatt [18] many al-", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "For example, the second-order perceptron algorithm [5] extends the original perceptron algorithm and uses the spectral properties of the data to improve performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Another example is the AROW algorithm [8] which uses confidence as a secondorder information.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Despite the extensive and impressive guarantees that can be made for algorithms in such setting [5, 8], competing with the best fixed function is not always good enough.", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "Despite the extensive and impressive guarantees that can be made for algorithms in such setting [5, 8], competing with the best fixed function is not always good enough.", "startOffset": 96, "endOffset": 102}, {"referenceID": 12, "context": "For online regression, few algorithms were developed for this setting [13, 22, 16].", "startOffset": 70, "endOffset": 82}, {"referenceID": 21, "context": "For online regression, few algorithms were developed for this setting [13, 22, 16].", "startOffset": 70, "endOffset": 82}, {"referenceID": 15, "context": "For online regression, few algorithms were developed for this setting [13, 22, 16].", "startOffset": 70, "endOffset": 82}, {"referenceID": 3, "context": "Yet, for online classification, the Shifting Perceptron algorithm [4] is a first-order algorithm that shrinks the weight vector each iteration, and in this way weaken dependence on the past.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The Modified Perceptron algorithm [3] is another first-order algorithm that had been shown to work well in the drifting setting [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "The Modified Perceptron algorithm [3] is another first-order algorithm that had been shown to work well in the drifting setting [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Our algorithm extends the second-order perceptron algorithm [5], and we provide a performance bound in the mistake bound model.", "startOffset": 60, "endOffset": 63}, {"referenceID": 11, "context": "This setting is called selective sampling [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 37, "endOffset": 48}, {"referenceID": 1, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 37, "endOffset": 48}, {"referenceID": 10, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 37, "endOffset": 48}, {"referenceID": 15, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 49, "endOffset": 57}, {"referenceID": 21, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 49, "endOffset": 57}, {"referenceID": 4, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 73, "endOffset": 79}, {"referenceID": 7, "context": "Stationary Non-stationary Regression [23, 2, 11] [16, 22] Classification [5, 8] This work", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "This group includes the selective sampling versions of the perceptron and the second-order perceptron algorithms [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "This group includes the BBQ algorithm [6], where the threshold decays polynomially with t as t\u2212\u03ba, and more involved variants where the threshold depends on the margin of the RLS estimate [10, 17].", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "This group includes the BBQ algorithm [6], where the threshold decays polynomially with t as t\u2212\u03ba, and more involved variants where the threshold depends on the margin of the RLS estimate [10, 17].", "startOffset": 187, "endOffset": 195}, {"referenceID": 16, "context": "This group includes the BBQ algorithm [6], where the threshold decays polynomially with t as t\u2212\u03ba, and more involved variants where the threshold depends on the margin of the RLS estimate [10, 17].", "startOffset": 187, "endOffset": 195}, {"referenceID": 6, "context": "We build on the work of Cesa-Bianchi et al [7] that combined a randomized rule into the Perceptron algorithm, yielding a selective sampling algorithm.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "2 Problem setting We consider the standard online learning model [1, 15] for binary classification, in which learning proceeds in a sequence of rounds t = 1, 2, .", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "2 Problem setting We consider the standard online learning model [1, 15] for binary classification, in which learning proceeds in a sequence of rounds t = 1, 2, .", "startOffset": 65, "endOffset": 72}, {"referenceID": 6, "context": "Since finding u \u2208 R that minimizes the number of mistakes on a known sequence is a computationally hard problem, the performance of the best predictor in hindsight is often measured using the cumulative hinge loss L\u03b3,T (u) = \u2211T t=1 `\u03b3,t (u), where `\u03b3,t (u) = max { 0, \u03b3 \u2212 ytuxt } is Stationary Non-stationary [7] This work", "startOffset": 309, "endOffset": 312}, {"referenceID": 15, "context": "We follow Moroshko and Crammer [16] and design the prediction as a last-step min-max problem in the context of drifting.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "Lemma 1 ([16], Lemma 2).", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "To the best of our knowledge, this is the first application of the last-step min-max approach directly for classification, and not as a reduction from regression, which is possible by employing the square loss, as in least-squares We still use the squared loss in (1), as done for least-squares SVMs [21, 20], which allows us to compute all quantities analytically.", "startOffset": 300, "endOffset": 308}, {"referenceID": 19, "context": "To the best of our knowledge, this is the first application of the last-step min-max approach directly for classification, and not as a reduction from regression, which is possible by employing the square loss, as in least-squares We still use the squared loss in (1), as done for least-squares SVMs [21, 20], which allows us to compute all quantities analytically.", "startOffset": 300, "endOffset": 308}, {"referenceID": 20, "context": "SVMs [21, 20].", "startOffset": 5, "endOffset": 13}, {"referenceID": 19, "context": "SVMs [21, 20].", "startOffset": 5, "endOffset": 13}, {"referenceID": 15, "context": "Indeed, we showed that the optimal prediction for classification is the sign of the optimal prediction for regression [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "Our algorithm includes the second-order perceptron [5] algorithm as a special case when c = \u221e.", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "The second-order perceptron algorithm is indeed using the sign of the optimal min-max prediction for regression [11], which is in fact the prediction of the AAR algorithm [23] (aka \u201dforward algorithm\u201d [2]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "The second-order perceptron algorithm is indeed using the sign of the optimal min-max prediction for regression [11], which is in fact the prediction of the AAR algorithm [23] (aka \u201dforward algorithm\u201d [2]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 1, "context": "The second-order perceptron algorithm is indeed using the sign of the optimal min-max prediction for regression [11], which is in fact the prediction of the AAR algorithm [23] (aka \u201dforward algorithm\u201d [2]).", "startOffset": 201, "endOffset": 204}, {"referenceID": 17, "context": "Additionally, similar to other algorithms [18, 5], we update the algorithm only on mistaken rounds.", "startOffset": 42, "endOffset": 49}, {"referenceID": 4, "context": "Additionally, similar to other algorithms [18, 5], we update the algorithm only on mistaken rounds.", "startOffset": 42, "endOffset": 49}, {"referenceID": 4, "context": "The LASEC algorithm can be seen as an extension to the non-stationary setting of the second-order perceptron algorithm [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 6, "context": "This approach for deriving selective-algorithms from margin-based online algorithms is not new, and was used to design an algorithm for the non-drifting case [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Yet, unlike other selective sampling algorithms [7, 6, 10, 17], our algorithm is designed to work in the drifting setting.", "startOffset": 48, "endOffset": 62}, {"referenceID": 5, "context": "Yet, unlike other selective sampling algorithms [7, 6, 10, 17], our algorithm is designed to work in the drifting setting.", "startOffset": 48, "endOffset": 62}, {"referenceID": 9, "context": "Yet, unlike other selective sampling algorithms [7, 6, 10, 17], our algorithm is designed to work in the drifting setting.", "startOffset": 48, "endOffset": 62}, {"referenceID": 16, "context": "Yet, unlike other selective sampling algorithms [7, 6, 10, 17], our algorithm is designed to work in the drifting setting.", "startOffset": 48, "endOffset": 62}, {"referenceID": 6, "context": "Specifically, LASEC-SS is reduced for c =\u221e to the selective sampling version of the second-order perceptron algorithm [7], and as mentioned above, for a =\u221e it is reduced to LASEC, and the setting of both c = \u221e, a = \u221e reduces the algorithm to the second-order perceptron, which in turn reduces to the perceptron algorithm for b\u2192\u221e.", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "[19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For the stationary case, when uk = u \u2200k (Vm = 0) and we set c = \u221e for the LASEC algorithm we recover the second-order perceptron bound [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "As in other context [7]: Thm.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "For the stationary case, when uk = u \u2200k (Vm = 0) and we set c = \u221e for the LASEC-SS algorithm we recover the bound of the selective sampling version of the second-order perceptron algorithm [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 15, "context": "1) is the same as the prediction of the LASER algorithm for regression with drift, we can use the result proven by Moroshko and Crammer [16] (Theorem 4 therein), from where we have that for any sequence u1, .", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "Using Lemma 5 and Lemma 7 of Moroshko and Crammer [16] we have \u2211", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Next, we use corollary 8 from Moroshko and Crammer [16] to get the final bound for LASEC.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "As was shown [16] we have", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "Another bound for the drifting setting was shown by Cavallanti et al for the Shifting Perceptron [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "Again, using Lemma 5 and Lemma 7 from [16] we get,", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "This is an immediate application of corollary 8 from [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Our second experiment uses the US Postal Service handwritten digits recognition corpus (USPS) [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "1) to five other algorithms: the second-order perceptron algorithm (SOP) [5], the Perceptron algorithm [18], the Shifting Perceptron algorithm [4], the Modified Perceptron algorithm [3] and the Randomized Budget Perceptron algorithm [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "1) to five other algorithms: the second-order perceptron algorithm (SOP) [5], the Perceptron algorithm [18], the Shifting Perceptron algorithm [4], the Modified Perceptron algorithm [3] and the Randomized Budget Perceptron algorithm [4].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "1) to five other algorithms: the second-order perceptron algorithm (SOP) [5], the Perceptron algorithm [18], the Shifting Perceptron algorithm [4], the Modified Perceptron algorithm [3] and the Randomized Budget Perceptron algorithm [4].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "1) to five other algorithms: the second-order perceptron algorithm (SOP) [5], the Perceptron algorithm [18], the Shifting Perceptron algorithm [4], the Modified Perceptron algorithm [3] and the Randomized Budget Perceptron algorithm [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "1) to five other algorithms: the second-order perceptron algorithm (SOP) [5], the Perceptron algorithm [18], the Shifting Perceptron algorithm [4], the Modified Perceptron algorithm [3] and the Randomized Budget Perceptron algorithm [4].", "startOffset": 233, "endOffset": 236}, {"referenceID": 6, "context": "1 to several selective sampling algorithms: the selective sampling version of second-order perceptron algorithm (SOP-SS) [7], the selective sampling version of perceptron algorithm (Perceptron-SS) [7] and the BBQ algorithm [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "1 to several selective sampling algorithms: the selective sampling version of second-order perceptron algorithm (SOP-SS) [7], the selective sampling version of perceptron algorithm (Perceptron-SS) [7] and the BBQ algorithm [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 5, "context": "1 to several selective sampling algorithms: the selective sampling version of second-order perceptron algorithm (SOP-SS) [7], the selective sampling version of perceptron algorithm (Perceptron-SS) [7] and the BBQ algorithm [6].", "startOffset": 223, "endOffset": 226}, {"referenceID": 5, "context": "In addition to the BBQ algorithm [6], we also consider a variant of the BBQ algorithm, which we call BBQ-I.", "startOffset": 33, "endOffset": 36}, {"referenceID": 16, "context": "Before the first shift at round 500 the BBQ is the best as expected from previous results [17], but its performance significantly degrade after the first shift.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "This is because this algorithm performs query when the quantity rt = xt A \u22121 t xt is large enough (see [6]), while the matrix At grows each time a label is queried.", "startOffset": 103, "endOffset": 106}, {"referenceID": 16, "context": "We note that for stationary environment (when ut = u \u2200t), BBQ outperforms BBQ-I, as well as other selective sampling algorithms (see [17]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "In addition, we can see that unlike the stationary setting where it was shown [7] that a small fraction of labels are enough to get the accuracy of a fully supervised setting, in the drifting case much more labels are needed.", "startOffset": 78, "endOffset": 81}], "year": 2017, "abstractText": "Recently there has been much work on selective sampling, an online active learning setting, in which algorithms work in rounds. On each round an algorithm receives an input and makes a prediction. Then, it can decide whether to query a label, and if so to update its model, otherwise the input is discarded. Most of this work is focused on the stationary case, where it is assumed that there is a fixed target model, and the performance of the algorithm is compared to a fixed model. However, in many real-world applications, such as spam prediction, the best target function may drift over time, or have shifts from time to time. We develop a novel selective sampling algorithm for the drifting setting, analyze it under no assumptions on the mechanism generating the sequence of instances, and derive new mistake bounds that depend on the amount of drift in the problem. Simulations on synthetic and real-world datasets demonstrate the superiority of our algorithms as a selective sampling algorithm in the drifting setting.", "creator": "LaTeX with hyperref package"}}}