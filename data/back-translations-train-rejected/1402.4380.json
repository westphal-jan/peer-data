{"id": "1402.4380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2014", "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification", "abstract": "A Verbal Autopsy is the record of an interview about the circumstances of an uncertified death. In developing countries, if a death occurs away from health facilities, a field-worker interviews a relative of the deceased about the circumstances of the death; this Verbal Autopsy can be reviewed off-site. We report on a comparative study of the processes involved in Text Classification applied to classifying Cause of Death: feature value representation; machine learning classification algorithms; and feature reduction strategies in order to identify the suitable approaches applicable to the classification of Verbal Autopsy text. We demonstrate that normalised term frequency and the standard TFiDF achieve comparable performance across a number of classifiers. The results also show Support Vector Machine is superior to other classification algorithms employed in this research. Finally, we demonstrate the effectiveness of employing a \"locally-semi-supervised\" feature reduction strategy in order to increase performance accuracy.", "histories": [["v1", "Tue, 18 Feb 2014 16:02:05 GMT  (409kb)", "http://arxiv.org/abs/1402.4380v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel danso", "eric atwell", "owen johnson"], "accepted": false, "id": "1402.4380"}, "pdf": {"name": "1402.4380.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification", "authors": ["Samuel Danso", "Eric Atwell", "Owen Johnson"], "emails": ["scsod@leeds.ac.uk", "E.S.Atwell@leeds.ac.uk", "O.A.Johnson@leeds.ac.uk"], "sections": [{"heading": null, "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "1.1 Feature Value representations", "text": "It is a technique proposed by Salton et al. [10] to present documents as a feature vector generally used in information gathering and now applied to TC. A feature of a document could be either a word, phrase or other form used to identify the content of the document. Regardless of the scheme of representation, each feature must be associated with a value or weight, indicating the meaning of the feature in relation to its contribution to classification. As is argued, the weighting strategy has a major impact on the accuracy of classification than the choice of the learning algorithm used in the process of classification. There are numerous definitions proposed in the literature by various researchers."}, {"heading": "1.2 Machine Learning Classification techniques", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "1.3 Feature Reduction techniques", "text": "Reduction of traits is an important activity in the TC process, as it seeks to reduce the high dimensionality of the trait vectors, which usually lead to high computing costs and negatively affect the performance of learning algorithms. For example, it is suggested that the number of traits should not exceed the number of training examples, as this type can trigger an overadaptation to occur [15]. Furthermore, it was subsequently well established that a strategic removal of irrelevant and redundant traits tends to increase the efficiency and performance accuracy of a machine learning algorithm [26]. Consequently, this has led to the integration of trait reduction as part of the steps for many machine learning algorithms [12]. The decision tree is an example of a learning algorithm that identifies \"important\" traits in order to differentiate between categories."}, {"heading": "2.2 Pre-possessing and experimental setup.", "text": "The text was converted to lowercase letters and symbolized by whitespaces. All punctuations were also removed. However, although stopwords are removed during the pre-processing phase in most NLP tasks on the pretext that they are not informative and subsequently non-discriminatory, this resulted in mixed and inconclusive results [29]. Also [27] argues that stopwords tend to be domain specific, so the stopwords for this experiment were not removed from the data set. Separate data sets were created on the basis of characteristic value representations under investigation: binary, term frequency; normalized term frequency, expressed as term frequency divided by the total number of terms found in the given document (document length); and TFiDF. The files were stored in a format used by the WEKA Machine Learning Software [20] to conduct this experiment."}, {"heading": "2.3 Evaluation metrics", "text": "However, there are two types of measurements for the F1 score: micro averaging and macro averaging. The former is used when there is an even distribution of classes in the dataset. We use macro averaging to determine the overall performance based on the highly unequal distribution of the multi-class dataset used; it allows the calculation of equal weights for each CoD category [34]."}, {"heading": "3. Results and Discussion", "text": "The experiments used the 10-fold cross-validation method to allow a random division by category into training and test sets for 10 runs [35], followed by a weighted average over the 10 folds as shown in Table 1."}, {"heading": "3.1 Feature Value representation", "text": "The results in Table 1 show the variations that exist in performance between algorithms and attribute representations. As can be seen, Random Forest achieved the worst performance, while the SVM achieved the best performance, followed by Na\u00efve Bayes in all attribute representations under study. In contrast, the binary scheme achieved the worst performance in all three learning algorithms. This is followed by the Term Frequency Scheme. Normalized Term Frequency and TFiDF, however, achieved a comparable performance with SVM. In contrast, the normalized frequency reached about 2% higher than the TFiDF for the algorithm. Also, as seen, Na\u00efve Bayes achieved better performance over binary representation. The evidence from the results tend to show that the choice of attribute representation affects the performance of algorithms on algorithms and the poor performance of algorithms."}, {"heading": "3.2 Machine Learning classification algorithms", "text": "Although he successfully defended the classification of the encrypted part of the compound data [38], the results of this experiment indicate that he is not a suitable choice for classifying the free text. The differences between the features generated from the encrypted and the free text data can be derived from a controlled vocabulary with a limited number of features; possibly from a list of questions with \"yes\" and \"no\" options. In contrast, the uncontrolled vocabulary of the free text leads to a large number of features."}, {"heading": "3.3 Feature reduction", "text": "After identifying SVM as the best performing algorithm for this domain, the Feature Reduction Experiment is referred to only as SVM. Figure 4 shows the results it has achieved when experimenting with a number of features. Figure 4 shows the performance accuracy it has achieved at various thresholds, starting with the top 10 values of each logarithm. This trend starts with the number of features it raises. Figure 4 shows a reversal in the rate of increase between 100 and 300 peak functions, with the highest values of a marginal increase of 0.1%. This trend increases to decrease from the top features, which reach about 3.6% less than all features."}, {"heading": "Acknowledgments", "text": "Our special thanks go to Professor Betty Kirkwood of the London School of Hygiene and Tropical Medicine and the entire study team from the ObaapaVita and Newhints projects, who generated the dataset used to conduct this study, and to the Commonwealth Scholarship Commission for its financial support for this research."}], "references": [{"title": "Speech and Language Processing", "author": ["D Jurafsky", "JH Martin"], "venue": "Pearson Education India,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "A Smith: Automatic quality of life prediction using electronic medical records", "author": ["S Pakhomov", "N Shah", "P Hanson", "S. S Balasubramaniam"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Electronic medical records for clinical research: application to the identification of heart failure", "author": ["S Pakhomov", "S. A Weston", "S. J Jacobsen", "C.G Chute", "R Meverden", "V. L Roger"], "venue": "The American journal of managed care,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Hersh: A survey of current work in biomedical text mining", "author": ["A. M Cohen", "W. R"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A Semantically Annotated Corpus for Automatic Verbal Autopsy Analysis", "author": ["S Danso", "E Atwell", "O Johnson", "G ten Asbroek", "K Edmond", "C Hurt", "L Hurt", "C Zandoh", "C Tawiah", "J Fenty", "S. A Etego", "S. O Agyei B.R Kirkwood"], "venue": "ICAME journal of the international computer archive of morden English,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Verbal autopsy: current practices and challenges", "author": ["N Soleman", "D Chandramohan", "K Shibuya"], "venue": "Bulletin of the World Health Organization", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Validation of the symptom pattern method for analyzing verbal autopsy data", "author": ["C Murray", "A Lopez", "D Feean", "S Peter", "G Yang"], "venue": "PLOS Medicine", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Refining a probabilistic model for interpreting verbal autopsy data. Scandinavian journal of public health", "author": ["P Byass", "E Fottrell", "DL Huong", "Y Berhane", "T Corrah", "K Kahn", "L Muhe"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Buckley: Term-weighting approaches in automatic text retrieval", "author": ["C G Salton"], "venue": "Information processing & management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "Text categorization with support vector machines. how to represent texts in input space", "author": ["E Leopold", "J Kindermann"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Imbalanced text classification: A term weighting approach. Expert systems with Applications", "author": ["Y Liu", "HT Loh", "A Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Supervised and traditional term weighting methods for automatic text categorization", "author": ["Lan", "CL Tan", "J Su", "Y Lu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "A comprehensive comparative study on term weighting schemes for text categorization with support vector machines", "author": ["M Lan", "C. L Tan", "H.B Low", "SY Sung"], "venue": "Special interest tracks and posters of the 14th international conference on worldwide web. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Interpreting TF-IDF term weights as making relevance decisions", "author": ["HC Wu", "RWP Luk", "KF Wong", "KL Kwok"], "venue": "ACM Transactions on Information Systems (TOIS)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Supervised term weighting for automated text categorization", "author": ["F Debole", "F Sebastiani"], "venue": "In Proceedings of SAC-03,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Machine learning: a review of classification and combining techniques", "author": ["S Kotsiantis", "I Zaharakis", "P Pintelas"], "venue": "Artificial Intelligence Review", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Survey of improving naive bayes for classification. Advanced Data Mining and Applications 2007:134-145", "author": ["L Jiang", "D Wang", "Z Cai", "X Yan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Nigam: A comparison of event models for naive bayes text classification", "author": ["K A McCallum"], "venue": "In proceedings of AAAI-98 workshop on learning for Text Categorisation;", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I. H Witten", "E Frank"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Naive Bayes vs decision trees in intrusion detection systems", "author": ["NB Amor", "S Benferhat", "Z Elouedi"], "venue": "In proceedings of SAC '04 symposium on applied computing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "An overview of statistical learning theory", "author": ["VN Vapnik"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Machine learning in automated text categorization", "author": ["F Sebastiani"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Classification of premalignant pancreatic cancer massspectrometry data using decision tree ensembles", "author": ["G Ge", "GW Wong"], "venue": "BMC Bioinformatis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "AntConc: A learner and classroom friendly, multi-platform corpus analysis toolkit", "author": ["L Anthony"], "venue": "Proceedings of IWLeL Waseda. University,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "A framework of feature selection methods for text categorization", "author": ["S Li", "R Xia", "C Zong", "C-R Huang"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["G Forman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y Yang", "J.O Pedersen"], "venue": "In proceedings of  international conference on machine learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "Little words can make a big difference for text classification", "author": ["E Riloff"], "venue": "In SIGIR '95 proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1995}, {"title": "Estimating continuous distributions in Bayesian classifiers", "author": ["GH John", "P Langley"], "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "Improvements to Platt's SMO algorithm for SVM classifier design", "author": ["S S Keerthi", "S K Shevade", "C Bhattacharyya", "KRK Murthy"], "venue": "Neural Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["T Dunning"], "venue": "Computational Linguisicst", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1993}, {"title": "A pitfall and solution in multiclass feature selection for text classification", "author": ["G Forman"], "venue": "In ICML '04 proceedings of the 21st international conference on Machine Learning", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R Kohavi"], "venue": "In International joint conference on artificial intelligence,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", "author": ["M Gamon"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Random forests for verbal autopsy analysis: multisite validation study using clinical diagnostic gold standards", "author": ["AD Flaxman", "A Vahdatpour", "S Green", "SL James", "CJL Murray"], "venue": "Popul Health Metrics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["T Joachims"], "venue": "Machine Learning: In Proceedings of ECML-98", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1998}, {"title": "Tackling the poor assumptions of naive bayes text classifiers", "author": ["JD Rennie", "L Shih", "J Teevan", "D Karger"], "venue": "In proceedings of ICML.", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "given classifier, and these include: the data and domain; machine learning algorithm; and the features and their representation schemes employed in the process of building a classifier for the classification task[2].", "startOffset": 212, "endOffset": 215}, {"referenceID": 1, "context": "The biomedical domain is one area that is witnessing a high rate of growth in research in the application of TC technology [3-5].", "startOffset": 123, "endOffset": 128}, {"referenceID": 2, "context": "The biomedical domain is one area that is witnessing a high rate of growth in research in the application of TC technology [3-5].", "startOffset": 123, "endOffset": 128}, {"referenceID": 3, "context": "The biomedical domain is one area that is witnessing a high rate of growth in research in the application of TC technology [3-5].", "startOffset": 123, "endOffset": 128}, {"referenceID": 4, "context": "research has not been extended to Verbal Autopsy (VA) narratives, which are considered another subtype of biomedical genre [6] .", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "(WHO) recommended approach being applied in developing countries where the majority of deaths occur outside health facilities[7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "information [8, 9].", "startOffset": 12, "endOffset": 18}, {"referenceID": 7, "context": "information [8, 9].", "startOffset": 12, "endOffset": 18}, {"referenceID": 5, "context": "text as an alternative approach[7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "Danso et al [6] discuss the possible challenges associated with using machine learning approaches to classification of Verbal Autopsy text.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "It is a technique proposed by Salton et al [10] to represent", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "As argued by [11] the weighting strategy employed has major", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "There are numerous term weighting schemes proposed in the literature by various researchers [12-14] However, all these", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": "There are numerous term weighting schemes proposed in the literature by various researchers [12-14] However, all these", "startOffset": 92, "endOffset": 99}, {"referenceID": 12, "context": "There are numerous term weighting schemes proposed in the literature by various researchers [12-14] However, all these", "startOffset": 92, "endOffset": 99}, {"referenceID": 8, "context": "For example the DF and TF are mostly combined by the product of the TF and the inverse of DF (iDF) to form another widely used scheme known as TFiDF[10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "Also, the more documents a term occurs in, the less powerful it is in discriminating between a given set of documents [15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "combination of feature selections metrics such as information gain, chi-square, gain ratio and odd ratios with TF and DF have been explored[12, 13].", "startOffset": 139, "endOffset": 147}, {"referenceID": 11, "context": "combination of feature selections metrics such as information gain, chi-square, gain ratio and odd ratios with TF and DF have been explored[12, 13].", "startOffset": 139, "endOffset": 147}, {"referenceID": 12, "context": "due to the process employed in estimating the values [14].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "documents to ensure features found in both short and long documents are of equal importance[16].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "[6]", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Numerous machine learning techniques have been employed to tackle various classification problems[17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "Na\u00efve Bayes (NB ) is considered to be a relatively simple machine learning technique based on probability models- Bayesian theorem[18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "found in a given document of a given Class[19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "However NB has proved to be robust to noise and missing data as it has the ability of performing the probabilities without having any impact on the final outcome[20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "more popular than the majority of the classification techniques found in the literature[21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "technique is relatively the newest among the supervised machine learning techniques found in the literature[17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "the Structural Risk Minimisation principle[22]", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "in the feature space by this kernel provides a direct mapping to non-linear structure that exists within the feature space[17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Decision Tree (DT): DT has been employed successfully in many traditional applications in different domains [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "For example, DT has recently been employed as a machine learning technique to develop classification models that automatically classify pancreatic cancer data[24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 15, "context": "feature that best divides the feature space, and there are numerous approaches to identifying the best feature[17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "DT has been shown to have superior performance over other techniques with regard to some specific domains with datasets that have discrete/categorical data type attributes[25].", "startOffset": 171, "endOffset": 175}, {"referenceID": 13, "context": "example it is suggested that number of features should not exceed number of training examples as this nature may trigger over-fitting to occur[15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "and redundant features tends to increase efficiency and performance accuracy of a machine learning algorithm [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "Consequently, this has led to the integration of feature reduction as part of the steps for many machine learning algorithms[12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Lui et al[12] groups the feature reduction approaches into two: global and local.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "and disadvantages and their performance tends to largely depend on the dataset [12, 27, 28].", "startOffset": 79, "endOffset": 91}, {"referenceID": 25, "context": "and disadvantages and their performance tends to largely depend on the dataset [12, 27, 28].", "startOffset": 79, "endOffset": 91}, {"referenceID": 26, "context": "and disadvantages and their performance tends to largely depend on the dataset [12, 27, 28].", "startOffset": 79, "endOffset": 91}, {"referenceID": 4, "context": "See Danso et al [6] for a detailed description of the dataset.", "startOffset": 16, "endOffset": 19}, {"referenceID": 27, "context": "Even though stop-words are removed during the pre-processing stage in most NLP tasks under the pretext that they are not informative and subsequently non discriminative, this however has led to mixed and inconclusive results[29].", "startOffset": 224, "endOffset": 228}, {"referenceID": 25, "context": "[27] argues that stop-words tend to be domain specific, so the stop-words were therefore not removed from the dataset for this experiment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The files were stored in a format readable by the WEKA Machine Learning software[20] used in carrying out this experiment.", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "above, and thus employed in carrying out the experiments were: the Na\u00efve Bayes algorithm developed by [30]; the Platt\u2019s Sequential Minimal Optimisation(SMO), which is a variant of the standard SVM algorithm[31]; and the Random", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "above, and thus employed in carrying out the experiments were: the Na\u00efve Bayes algorithm developed by [30]; the Platt\u2019s Sequential Minimal Optimisation(SMO), which is a variant of the standard SVM algorithm[31]; and the Random", "startOffset": 206, "endOffset": 210}, {"referenceID": 30, "context": "superiority over the other metrics as pointed by [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "software[25] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "highly uneven distribution of the multi-class dataset being used; it allows equal weights to be computed for each CoD category[34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 32, "context": "validation evaluation method to allow a random split stratified by the categories into training and test sets for 10 runs[35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "For example Gamon[36] chose binary feature representation and yet achieved good results because of the brevity of the documents (which may result in a small number of unique words or limited vocabulary) used in", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "surprising considering the fact that the IDF normalisation factor tends to assign lower values to common terms that occur in several documents[37] such as the stopwords, and consequently resulting in a better performance accuracy[10], but these", "startOffset": 142, "endOffset": 146}, {"referenceID": 8, "context": "surprising considering the fact that the IDF normalisation factor tends to assign lower values to common terms that occur in several documents[37] such as the stopwords, and consequently resulting in a better performance accuracy[10], but these", "startOffset": 229, "endOffset": 233}, {"referenceID": 27, "context": "important in discriminating, and therefore reenforcing the statement \u201cLittle words can make a big difference for text classification\u201d by Rillof [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "applied to classify the coded part of Verbal Autopsy data[38], the results obtained from this experiment suggest that it is not an appropriate choice for classification of VA free text .", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "Since Decision Trees have generally been found to be susceptible to over-fitting with 500 or more features [39], there is the possibility that this", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "has been consistently shown to have relatively better performance in Text Classification experiments[20], and the results from this experiments are not an exception.", "startOffset": 100, "endOffset": 104}, {"referenceID": 36, "context": "dimensionality of the feature space, thus, the number of features tends not to be an issue; and SVMs are well designed to deal with sparseness found in feature vectors[39].", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "Danso et all\u2019s [6] description of the VA text seems to correlate with the taxonomy of issues outlined that the SVM", "startOffset": 15, "endOffset": 18}, {"referenceID": 25, "context": "training[27].", "startOffset": 8, "endOffset": 12}], "year": 2013, "abstractText": "A Verbal Autopsy is the record of an interview about the circumstances of an uncertified death. In developing countries, if a death occurs away from health facilities, a field-worker interviews a relative of the deceased about the circumstances of the death; this Verbal Autopsy can be reviewed offsite. We report on a comparative study of the processes involved in Text Classification applied to classifying Cause of Death: feature value representation; machine learning classification algorithms; and feature reduction strategies in order to identify the suitable approaches applicable to the classification of Verbal Autopsy text. We demonstrate that normalised term frequency and the standard TFiDF achieve comparable performance across a number of classifiers. The results also show Support Vector Machine is superior to other classification algorithms employed in this research. Finally, we demonstrate the effectiveness of employing a \u2019locally-semisupervised\u2019 feature reduction strategy in order to increase performance accuracy.", "creator": "Microsoft\u00ae Word 2010"}}}