{"id": "1402.4455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2014", "title": "Symbiosis of Search and Heuristics for Random 3-SAT", "abstract": "When combined properly, search techniques can reveal the full potential of sophisticated branching heuristics. We demonstrate this observation on the well-known class of random 3-SAT formulae. First, a new branching heuristic is presented, which generalizes existing work on this class. Much smaller search trees can be constructed by using this heuristic. Second, we introduce a variant of discrepancy search, called ALDS. Theoretical and practical evidence support that ALDS traverses the search tree in a near-optimal order when combined with the new heuristic. Both techniques, search and heuristic, have been implemented in the look-ahead solver march. The SAT 2009 competition results show that march is by far the strongest complete solver on random k-SAT formulae.", "histories": [["v1", "Tue, 18 Feb 2014 19:59:58 GMT  (258kb)", "http://arxiv.org/abs/1402.4455v1", "Proceedings of the Third International Workshop on Logic and Search (LaSh 2010)"]], "COMMENTS": "Proceedings of the Third International Workshop on Logic and Search (LaSh 2010)", "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["sid mijnders", "boris de wilde", "marijn heule"], "accepted": false, "id": "1402.4455"}, "pdf": {"name": "1402.4455.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Marijn J. H. Heule"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 2.44 55v1 [cs.DS] 18 Feb 2014"}, {"heading": "1 Introduction", "text": "It can be divided into decision euristics and correctness euristics. The former selects a decision variable for each node in the tree to branch to, the latter determining the size of the search tree. For each free value of the decision variable, a child node is created. Directional euristics provides the preferred sequence in which these child nodes should be visited. If a search tree contains solutions, the effective rectification of the heuristics can increase performance. Two techniques are developed to repair errors made by branching heuristics. First, restart strategies are used to compensate for ineffective decisions by decision euristics. A new search tree is created after each restart. Second, the discrepancy search [3], a technique used to restrict the programming of hayristics that largely follow the preference of the directness euristics."}, {"heading": "2 Look-ahead heuristics", "text": "Most work on the bifurcation of heuristics in the field of satisfaction focuses on predictive SAT solvers [1]. Unlike many other solvers, predictive SAT solvers follow various statistical measurements that make it possible to use quite complex heuristics. In this section, we will first give an overview of predictive SAT solvers. Afterwards, the branched heuristics in these solvers will be discussed. We conclude this section with the introduction of improved heuristics."}, {"heading": "2.1 Look-ahead SAT solvers", "text": "It is a complete solution method, which in each step includes a decision variable xdecision, and recursively a reduced formula for formula F, an unassigned variable x, an unassigned variable x, an assigned variable B, an assignment to B, an assignment to B, an assignment to B, an assignment to B, an assignment to B, an assignment to B, an assignment to B, an assignment to B, an assignment to F, an assignment to F, an assignment to F, an assignment to F, an assignment to F, an assignment to F, an assignment to F, an assignment to F, an assignment to F, and so on."}, {"heading": "2.2 Look-ahead evaluation", "text": "In fact, most of them will be able to follow the rules that they have applied in practice. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...)"}, {"heading": "2.3 Recursive weight heuristic", "text": "We developed a model for generalizing existing work on branched heuristics. We refer to this model as the recursive weight reduction heuristic. Let's leave VAR (F) to the series of variables in F and n = | VAR (F). The heuristic values hi (x) express for each iteration how much literal x is to be true by the clauses that contain x. First, for all letters x, h0 (x) are initialized to 1: h0 (x) = h0 (\u00ac x) = 1 (1) At each step, the heuristic values hi (x) are calculated using the average value \u00b5i = 12n x VAR (F) (hi (\u00ac x) x)))) = h0 (2) finally, in each next iteration, the heuristic values hi (x) in which letters y gain weight hi (\u00ac y) are calculated."}, {"heading": "3 Heuristic search", "text": "Let's take a step back from the SAT to consider how to capitalize on directional euristics in general. First, we will discuss the terminology of directional euristics and the ideas behind the most important heuristic search strategies, then we will present an alternative search strategy that is very powerful in combination with recursive weight euristics."}, {"heading": "3.1 Direction heuristics", "text": "This year, it's so far that it will be able to put itself at the top, \"he said.\" It's too early to believe that we'll be able to beat ourselves, \"he said.\" We're not yet able to put ourselves in the position, \"he said,\" but we haven't been able to put ourselves in the position yet. \""}, {"heading": "3.2 Depth first search", "text": "One of the most well-known search strategies is the depth first search (DFS). DFS branches from left to right until it reaches a leaf node, after which it traces chronologically. DFS traverses the order in which it visits leaf nodes in Fig. 4. DFS traverses the minimum number of edges required to explore the entire tree. If pheur (v) = 0.5 for all nodes v is in a binary tree, meaning that the direction of heuristic branches could also be selected at random, Pgoal (v) is the same for each node v at the same depth, then DFS is the cheapest strategy. If the directional euristics are stronger than the random selection, as in Fig. 3, alternative strategies traverse the tree more efficiently."}, {"heading": "3.3 Discrepancy search", "text": "If some leaf nodes are not target nodes, the intuition is that heuristics have only taken a small number of wrong branches on their way to this leaf node. Intuitively, tree search is best when paths with a small number of discrepancies are first explored. Limited Discrepancy Search (LDS) LDS [3] first explores those parts of the search tree that have a small number of discrepancies. In each iteration of LDS, the number of allowed discrepancies is incremented. Fig. 5 illustrates the iterations of LDS. LDS has a certain redundancy because it only sets an upper limit of discrepancies. Therefore, the iteration x examines the paths of previous iterations."}, {"heading": "3.4 Advanced Limited Discrepancy Search", "text": "The papers describing LDS [3], ILDS [7] and DDS [8] are more precise than the leaf nodes are explored at each of the iteration stages. However, aside from the pseudo codes, this means that the leaf nodes are explored within a single iteration stage. We assume that the strategies will be applied in the pseudo codes."}, {"heading": "4 Experiments", "text": "This section presents two types of results: theoretical and experimental results. The theoretical results are based on a probabilistic model of heuristic tree search. Experiments were performed using the look-ahead SAT Solver March [12], the fastest solver on random 3-SAT benchmarks. We compare several discretion-based search strategies based on the theoretical model and a dataset of random 3-SAT formulas. Additionally, two experiments were performed to determine how much ALDS could be improved."}, {"heading": "4.1 Theoretical results", "text": "Based on the increasing heuristic probability observation [12], we have created a model with only one target node. In this model, we assign the heuristic probability as follows (based on the observed Pheur (v) values in [12]: Pheur (v) = 0.56 + 0.015 \u00b7 Depth (v) (5) Each leaf node v at depth 12 is assigned a target node probability Pgoal (v) using the equations described in Section 3.1. This means that Pgoal is calculated by multiplying the heuristic probabilities of the left and right children leading to this leaf node, starting from the root with Pheur (root) = 1. Similar to the tree in Fig. 3, only using a much larger tree. In practice, search trees contain multiple target nodes, but no generality is lost by placing just one target node in the search tree of our model. This only results in the expected cost of one target node being a little larger and other numerical results."}, {"heading": "4.2 Satisfiability results", "text": "The data set for the experiments consisted of 20146 satisfactory random 3-SAT instances with 350 variables and 1491 clauses. The clause-to-variable ratio is 4.26, which is the ratio in which the probability of creating a satisfactory instance is about 50%, known as phase transition density. For each instance, the full search tree was examined to find out which of the 4096 subtrees contained 12 solutions at depth. A depth of 12 was chosen to keep the data compact sufficiently for practical use, but still perform discrepancy searches on a significant part of the search tree. On average, each instance contained 17.2 satisfactory subranges. Results from the experiments are in Fig. 10. The vertical axis shows the fraction of the problems that have not yet been resolved."}, {"heading": "4.3 Analysis", "text": "However, it is difficult to determine the performance of the optimal search strategy due to multiple satisfactory subtrees per instance. In order to approximate the optimal search strategy, we construct a greedy search strategy. The greedy search strategy is introduced in [12] and is constructed as follows: - Select the subtree in which most instances from the datasets have at least one solution. This subtree is next to be visited in this specific greedy search strategy. - Remove from consideration of all instances in which the selected subtree has at least one solution. - Repeat steps until all instances are removed from the dataset. - The subtrees that have not yet been ordered will end up in ALDS order.Building the greedy search strategy requires a series of inputs."}, {"heading": "5 Conclusion", "text": "We introduced recursive weight euristics, a branched heuristics for predictive SAT solvers. The solver march equipped with this heuristics works more strongly than any other solver on unsatisfactory random 3-SAT formulas. It won the gold medal in this category in the SAT 2009 competition. To capitalize on recursive weight euristics even on satisfactory instances, we presented Advanced Search for Limited Discrepancies (ALDS). Theoretical and practical results show that ADSL outperforms alternative complete search strategies on satisfying random 3-SAT formulas. We show that in these cases ALDS in combination with recursive weight euristics traverse the search tree in almost optimal order."}], "references": [{"title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems", "author": ["C.P. Gomes", "B. Selman", "N. Crato", "H. Kautz"], "venue": "J. Autom. Reason. 24(1-2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Limited discrepancy search", "author": ["W.D. Harvey", "M.L. Ginsberg"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Heuristics based on unit propagation for satisfiability problems", "author": ["C.M. Li", "Anbulagan"], "venue": "IJCAI (1).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "A constraint-based approach to narrow search trees for satisfiability", "author": ["C.M. Li"], "venue": "Information processing letters 71(2)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A backbone-search heuristic for efficient solving of hard 3-SAT formulae", "author": ["O. Dubois", "G. Dequen"], "venue": "In Nebel, B., ed.: IJCAI, Morgan Kaufmann", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Improved limited discrepancy search", "author": ["R.E. Korf"], "venue": "In Proceedings of AAAI-96, MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Depth-bounded discrepancy search", "author": ["T. Walsh"], "venue": "IJCAI\u201997: Proceedings of the Fifteenth international joint conference on Artifical intelligence, San Francisco, CA, USA, Morgan Kaufmann Publishers Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "A machine program for theorem-proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Commun. ACM 5(7)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1962}, {"title": "Investigating the behaviour of a SAT solver on random formulas", "author": ["O. Kullmann"], "venue": "Technical Report CSR 23-2002, University of Wales Swansea, Computer Science Report Series (http://www-compsci.swan.ac.uk/reports/2002.html)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Whose side are you on? finding solutions in a biased searchtree", "author": ["M.J.H. Heule", "H. van Maaren"], "venue": "Technical report, Proceedings of Guangzhou Symposium on Satisfiability In LogicBased Modeling", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Handbook of Satisfiability", "author": ["A. Biere", "M.J.H. Heule", "H. van Maaren", "T. Walsh", "eds."], "venue": "Volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "First, restart strategies [2] are used to compensate for ineffective choices made by decision heuristics.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Second, discrepancy search [3], a technique used in Constraint Programming, focuses on those parts of the search tree that mostly follow the preference of the direction heuristics.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "This heuristic is a generalization of earlier lookahead evaluation heuristics [4\u20136].", "startOffset": 78, "endOffset": 83}, {"referenceID": 3, "context": "This heuristic is a generalization of earlier lookahead evaluation heuristics [4\u20136].", "startOffset": 78, "endOffset": 83}, {"referenceID": 4, "context": "This heuristic is a generalization of earlier lookahead evaluation heuristics [4\u20136].", "startOffset": 78, "endOffset": 83}, {"referenceID": 5, "context": "To capitalize on this, we developed a new discrepancy search algorithm, called advanced limited discrepancy search (ALDS), which combines features of improved limited discrepancy search [7] and depth-bounded discrepancy search [8].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": "To capitalize on this, we developed a new discrepancy search algorithm, called advanced limited discrepancy search (ALDS), which combines features of improved limited discrepancy search [7] and depth-bounded discrepancy search [8].", "startOffset": 227, "endOffset": 230}, {"referenceID": 7, "context": "1 Look-ahead SAT solvers The look-ahead architecture for SAT solvers is based on the DPLL framework [9]: It is a complete solving method which selects in each step a decision variable xdecision and recursively calls DPLL for the reduced formula where xdecision is assigned to false (denoted by F [xdecision = 0]) and another where xdecision is assigned to true (denoted by F [xdecision = 1]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "The DIFF heuristic based on the number of newly created clauses was introduced by Li and Anbulagan [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "For this special case, Li [5] uses in satz weights based on the occurrences of variables.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "A slight variation but much more effective weight is used by Dubois and Dequen [6] in their solver kcnfs.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "In case of k-SAT instances, Kullmann [11] uses in the OKsolver weights based on the length of clauses in Cnew.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "For binary clause (y \u2228 z) \u2208 Cnew they compute a weight w i (y \u2228 z) = hi(\u00acy) \u2217 hi(\u00acz): \u2013 Li & Anbulagan 1997 [4]: w(y \u2228 z) = 1 = h0(\u00acy)\u00d7 h0(\u00acz), in short w \u00d7 0 .", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "\u2013 Li 1999 [5]: w(y \u2228 z) = #(\u00acy) + #(\u00acz) = h1(\u00acy) + h1(\u00acz), in short w + 1 .", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "\u2013 Dubois 2001 [6]: w(y \u2228 z) = #(\u00acy) \u00d7#(\u00acz) = h1(\u00acy)\u00d7 h1(\u00acz), in short w \u00d7 1 .", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Although [5,6] used \u03b3 = 5, we observed stronger performance using \u03b3 = 3.", "startOffset": 9, "endOffset": 14}, {"referenceID": 4, "context": "Although [5,6] used \u03b3 = 5, we observed stronger performance using \u03b3 = 3.", "startOffset": 9, "endOffset": 14}, {"referenceID": 9, "context": "We implementedw 0 , w + 1 , w \u00d7 1 , w \u00d7 2 , w \u00d7 3 and w \u00d7 4 in the look-ahead solver march ks [12] with \u03b3 = 3.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "It is observed [12], that heuristics tend to make more mistakes in the top of the search tree.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "Limited Discrepancy Search (LDS) LDS [3] explores first those parts of the search tree that have a small number of discrepancies.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Depth-bounded Discrepancy Search (DDS) By incrementally increasing the maximum depth up to which discrepancies are allowed to occur, DDS [8] differs from (I)LDS.", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "The papers describing LDS [3], ILDS [7] and DDS [8] are precise on which leaf nodes are explored in each of the iteration stages.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "The papers describing LDS [3], ILDS [7] and DDS [8] are precise on which leaf nodes are explored in each of the iteration stages.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "The papers describing LDS [3], ILDS [7] and DDS [8] are precise on which leaf nodes are explored in each of the iteration stages.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "This search strategy is inspired by an earlier experimental study on random 3-SAT instances, where we observed [12] two patterns regarding the goal node probabilities:", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "Experiments have been performed with the look-ahead SAT solver march [12], the fastest solver on random 3-SAT benchmarks.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Based on the increasing heuristic probability observation [12] we created a model with just one goal node.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "In this model we assign the heuristic probability as follows (based on the observed Pheur(v) values in [12]): Pheur(v) = 0.", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "2 Satisfiability results For the experimental results we used the look-ahead SAT solver march [12] with the recursive weight heuristic w 3 as direction heuristic (see Section 2.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "The Greedy search strategy is introduced in [12], and is constructed as follows:", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "When combined properly, search techniques can reveal the full potential of sophisticated branching heuristics. We demonstrate this observation on the well-known class of random 3-SAT formulae. First, a new branching heuristic is presented, which generalizes existing work on this class. Much smaller search trees can be constructed by using this heuristic. Second, we introduce a variant of discrepancy search, called ALDS. Theoretical and practical evidence support that ALDS traverses the search tree in a near-optimal order when combined with the new heuristic. Both techniques, search and heuristic, have been implemented in the look-ahead solver march. The SAT 2009 competition results show that march is by far the strongest complete solver on random k-SAT formulae.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}