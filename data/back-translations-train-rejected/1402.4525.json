{"id": "1402.4525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2014", "title": "Off-Policy General Value Functions to Represent Dynamic Role Assignments in RoboCup 3D Soccer Simulation", "abstract": "Collecting and maintaining accurate world knowledge in a dynamic, complex, adversarial, and stochastic environment such as the RoboCup 3D Soccer Simulation is a challenging task. Knowledge should be learned in real-time with time constraints. We use recently introduced Off-Policy Gradient Descent algorithms within Reinforcement Learning that illustrate learnable knowledge representations for dynamic role assignments. The results show that the agents have learned competitive policies against the top teams from the RoboCup 2012 competitions for three vs three, five vs five, and seven vs seven agents. We have explicitly used subsets of agents to identify the dynamics and the semantics for which the agents learn to maximize their performance measures, and to gather knowledge about different objectives, so that all agents participate effectively and efficiently within the group.", "histories": [["v1", "Tue, 18 Feb 2014 23:01:13 GMT  (1220kb,D)", "http://arxiv.org/abs/1402.4525v1", "18 pages, 8 figures"]], "COMMENTS": "18 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["saminda abeyruwan", "andreas seekircher", "ubbo visser"], "accepted": false, "id": "1402.4525"}, "pdf": {"name": "1402.4525.pdf", "metadata": {"source": "CRF", "title": "Off-Policy General Value Functions to Represent Dynamic Role Assignments in RoboCup 3D Soccer Simulation", "authors": ["Saminda Abeyruwan", "Andreas Seekircher", "Ubbo Visser"], "emails": ["saminda@cs.miami.edu", "aseek@cs.miami.edu", "visser@cs.miami.edu"], "sections": [{"heading": null, "text": "Keywords: Dynamic role assignment function, Reinforcement Learning, GQ (\u03bb), Greedy-GQ (\u03bb), Off-PAC, Off-Policy Prediction and Control and RoboCup 3D Soccer Simulation."}, {"heading": "1 Introduction", "text": "The simulated actors formalize their objectives in two levels: 1. the physical levels where controls related to walking, kicking, etc.; and 2. the decision layers where high-level actions are taken to detect behaviors. In this paper, we examine a mechanism that is suitable for decision layers to consider recently introduced Off-Policy Gradient Decent Algorithms in Reinforcement Leaning (RL) that illustrate learnable knowledge representations in order to learn about a dynamic role allocation. To learn about an effective dynamic role allocation function, agents must consider the dynamics of agent-environment interactions. We consider these interactions as the knowledge of the agent. When this knowledge is represented, Xiv: 140 2.45 25v1 [cs.AI] 18 Feb 20142 Saminda Abeyruwan, Andreas, and Ubbo in a predictable environment."}, {"heading": "2 Related Work", "text": "One goal of multi-agent systems research is to examine the prospects for efficient collaboration between a number of agents in real-time environments. In our research, we focus on the collaboration of a number of agents in a real-time football simulation environment, where agents learn about optimal or near-optimal role mapping within a particular formation using GVFs. This sub-task is particularly difficult compared to other simulation leagues that take into account environmental constraints, i.e. limited locomotion capabilities, limited communication bandwidth or crowd management rules. Role mapping is part of the hierarchical machine learning paradigm [20,19] where a formation defines the role space. Homogeneous agents can switch roles within a formation to maximize a given reward function."}, {"heading": "3 Learnable knowledge representation for Robotic Soccer", "text": "Recently, within the framework of the RL framework [21], an expressive knowledge representation language, which can be learned through sensory motor data, was introduced that is directly usable for robot football, since the interactions with the environment are performed by perceptors and actuators. In this approach, knowledge is presented as a large number of approximate value functions, each with its own policy; 2. pseudo-reward function; 3. pseudo-terminal reward function [22]. In continuous state spaces, approximate value functions are learned by means of function approximation and more efficient non-political learning algorithms. Firstly, we briefly present some of the important concepts related to the GVFs. Full information on the GVFs is available in [22,8,9,7]. Secondly, we show their direct application to simulated robotic soccer.5"}, {"heading": "3.1 Interpretation", "text": "The interpretation of the approximate value function as a knowledge representation language based on information from perceptors and actuators is defined as: Definition 1. Knowledge expressed as an approximate value is true or accurate if its numerical values coincide with those of the mathematically defined value function, it is approximate. Therefore, according to definition (1), a value function poses a question, and an approximate function is the answer to this question. Based on the previous interpretation, the standard RL framework extends to present learnable knowledge as follows. In the standard RL framework [21], the agent and the world are to be represented in discrete time steps = 1, 2, 3,... The agent recognizes the state at each time step St-S and selects a ShareAt-A. One time step later, the agent receives a scalar reward Rt + 1, and recognizes the state St + 1, which terminates the state S. The rewards are rewarded according to the reward function: St + S on the level of the Selection function (Objective \u2192 1 is the reward function)."}, {"heading": "3.2 Off-Policy Action-Value Methods for GVFs", "text": "The first way to learn about GVFs is to use action value functions. Let Gt complete the return of State St at time t, then the sum of the rewards (transient plus terminal) until the completion of the function T is: Gt = T \u2211 k = t + 1 r (Sk) + z (ST). The action value function is: Q\u03c0 (s, a) = E (Gt | St = s, At = a, At + 1: T \u2212 1), where, Q\u03c0: S \u00b7 A \u2192 R. This is the expected return on an action that emanates from State s, and the action a, and selects actions according to the policy approach, until the termination of the action value function with Q: S \u00b7 A \u2192 R. Therefore, the action value function is a precisely grounded question, 6 Saminda Abeyruwan, Andreas Seekircher, and Ubbo Visseralgorithm 1."}, {"heading": "3.3 Off-Policy Policy Gradient Methods for GVFs", "text": "The second method to learn something about the GVFs is the use of off-policy gradient methods with actor-critical architecture that use a state-value function for learning GVFs. It is defined as: V \u03c0, \u03b3, r, z (s) = E (Gt | St = s, At: T \u2212 1 \u03c0, T vs v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v"}, {"heading": "4 Dynamic Role Assignment", "text": "A role is the specification of an agent's internal or external behavior. In our football domain, roles select the behavior of agents based on different reference criteria: The agent near the ball becomes the striker. In a role space of size n, one gets the cooperation between m \u2264 n agents, A = {a1,..., am}, through formations. The role space consists of active and reactive roles. For example, the striker is an active role and the defender could be a reactive role. In a reactive role, there is a function, R 7 \u2192 T, which maps the roles on target positions, T, on the field. These target positions are calculated in terms of a reference position (e.g. the ball position) and other auxiliary criteria such as the rules of crowd management."}, {"heading": "4.1 Target Positions with the Primary Formation", "text": "Within our framework, an agent can choose a role from among thirteen roles. These roles are part of a primary formation, and an agent calculates the respective target positions according to his belief in the absolute ball position and the rules imposed by the 3D football simulation server. We have labeled the role space to describe the behaviors associated with it. Figure (1) shows the target positions for the role space before kick-off. The agent closest to the ball assumes the striker role (SK), which is the only active role. Suppose the agent has given the belief in the absolute ball position by (xb, yb). Target positions are shifted forward left (FL) and forward right (FR). Target positions are shifted by (xb, yb) \u00b1 (0, 2). The extended forward role to the left (EX1L) and the extended forward role to the right (EX1R) target positions are given by (xb, yb). The stopposition is extended by (2,0) BBW (1ST) and (EX1ST) block position is given by (1)."}, {"heading": "4.2 Roles to RL Action Mapping", "text": "The agent closest to the ball becomes the striker, and only one agent is allowed to become the striker. The other agents besides the goalkeeper are allowed to choose between twelve roles. We rank the available roles according to individual actions of the RL algorithm. To use Algorithm 1, an agent must formulate a question function based on a value function, and the answer function provides the solution as an approximate value function. All agents formulate the same question: What role do I play in this formation to maximize future rewards? All agents learn independently of the question, while helping each other to maximize their future reward. We assume that the agents do not communicate their current role. Therefore, at a certain step, multiple agents can commit to the same role. We discourage this condition by modifying the question: What is my role in this formation to maximize future rewards, while maintaining a completely different role from all teammates in all time steps 11?"}, {"heading": "4.3 State Variables Representation", "text": "Figure 2 shows the schematic diagram of the representation of the state variables. All points and vectors in Figure 2 are defined in relation to a global coordinate system. h is the center of the home goal, while o is the center of the opposing goal. b is the ball position."}, {"heading": "5 Question and Answer Functions", "text": "All agents communicate their belief in other agents. Based on their belief, all agents calculate a cost function and assign the nearest agent as a striker. We have formulated a cost function based on the relative distance to the ball, the angle of the agent, the number of teammates and opponents within a region near the ball and the activity of the agents. In our formulation, there is a natural termination condition: scoring goals. In terms of assigning the striker role, we define a pseudo-termination condition. When an agent becomes a striker, a pseudo-termination occurs and the striker does not participate in the learning process unless he chooses a different role. We define the questions and answers as follows:"}, {"heading": "5.1 GVF Definitions for State-Action Functions", "text": "Question functions: 1. \u03c0 = greedy w.r.t. Q, 2. \u03b3 (.) = 0.8, 3. r (.) = (a) the change of the x-value of the absolute ball position; (b) a small negative reward of 0.01 for each cycle; (c) a negative reward of 5 for all players within a radius of 1.5 meters; 4. z (.) = (a) + 100 for scoring against the opponent; (b) \u2212 100 for scoring the opponent; and 5. time step = 2 seconds. response functions: 12 Saminda Abeyruwan, Andreas Seekircher and Ubbo Visser1. \u03c0b = -greedy w.r.t. target action function, 2. = 0.05, 3. It (.) = 1, 4. \u03c6 (.,.) = (a) we use tile coding to formulate the attribute vector. nstart = 2 and nend = 3, 5, 7. mmax = 3, 5, 7. implementation."}, {"heading": "5.2 GVF for Gradient Descent Functions", "text": "Question functions: 1. \u03c0 = Gibbs distribution, 2. \u03b3 (.) = 0.9, 3. r (.) = (a) the change of the x-value of the absolute ball position; (b) a small negative reward of 0.01 for each cycle; (c) a negative reward of 5 is given to all players within a radius of 1.5 meters; 4. z (.) = (a) + 100 for scoring against the opponent; (b) \u2212 100 for scoring the opponent; and 5. time step = 2 seconds. Response functions: 1. \u03c0b = the Gibbs distribution learned is used with a small error. To ensure the exploration with the probability 0.01, the Gibbs distribution is disturbed with an \u03b2-value. In our experiments we use \u03b2 = 0.5. Therefore, we use a behavioral policy: e uT\u03c6 (s, a) + \u03b2 b e uT\u03c6 (s, b)."}, {"heading": "6 Experiments", "text": "We conducted experiments against the Boldhearts and MagmaOffenburg teams, both semi-finalists of the RoboCup 3D Soccer Simulation competition in Mexico 20124. We conducted knowledge learning according to the configuration specified in Section (5). Section (6.1) describes the performance of the algorithm (1), and Section (6.2) describes the performance of the algorithm (2) for the experimental setup."}, {"heading": "6.1 GVFs with Greedy-GQ(\u03bb)", "text": "The first experiments were conducted with a team size of five people using the RL agents against Boldhearts. After 140 games, our RL agent increased the chance from 30% to 50%. This number does not increase in the next games, but after 260 games the number of lost games (initially 35%) is reduced to 15%. In the further experiments, we used the goal difference to compare the performance of the RL agent. Figure (3) shows the average goal difference, which exceeds the hand-distributed role assignment and the RL agent archive in games against Boldhearts and MagmaOffenburg with different team sizes. With only three agents per team, the RL agent only needs 40 games to learn a policy that exceeds the hand-encrypted role selection (Figure (3 (a))). Even with five agents per team, the learning agent is able to increase the goal difference against both opponents (Figure (3 (b))."}, {"heading": "6.2 GVFs with Off-PAC", "text": "With Off-PAC, we used a similar environment to that of Subsection (6.1), but with a different learning situation. Instead of learning individual strategies for teams separately, we learned a single policy for both teams. We ran the opposing teams in a round robin mode for 200 games and repeated complete runs for several times. The first experiments were conducted with a team size of three with RL agents against both teams. Figure (4 (a) shows the results of containers of 20 games on average between two studies. The RL agents have learned a stable policy compared to the hand-refined policy, but the learned policy is via hand-tuned role assignment function. The second experiments were conducted with a team size of five with the RL agents against the opposing teams. Figure (4 (b) shows the results of the bins of 20 games averaged between three studies."}, {"heading": "7 Conclusions", "text": "We have developed and experimented RL agents that learn to assign roles to maximize expected future rewards. All agents in the team ask the question, \"What is my role in this formation to maximize future rewards while maintaining a completely different role from all teammates in all time steps?\" This is a targeted question. We use Greedy-GQ (\u03bb) and Off-PAC to learn experientially grounded knowledge encoded in GVFs. Dynamic Role Assignment Function is abstracted from all other low components, obstacle avoidance, object tracking, and so on. If the Role Assignment Function selects a passive role and assigns a destination, the lower layers will deal with this requirement. If the lower layers do not meet this requirement, for example, this feedback will not apply to the RL Assignment Function."}], "references": [{"title": "Scaling-up Knowledge for a Cognizant Robot", "author": ["T. Degris", "J. Modayily"], "venue": "In Notes of the AAAI Spring Symposium on Designing Intelligent Robots: Reintegrating AI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Bridging the Gap: Learning in the Robocup Simulation and Midsize League", "author": ["T. Gabel", "S. Lange", "M. Lauer", "M. Riedmiller"], "venue": "Proceedings of the 7th Portuguese Conference on Automatic Control (Controlo)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Market-Based Dynamic Task Allocation using Heuristically Accelerated Reinforcement Learning", "author": ["Gurzoni", "J.A. Jr.", "F. Tonidandel", "R.A.C. Bianchi"], "venue": "Proceedings of the 15th Portugese Conference on Progress in Artificial Intelligence. pp. 365\u2013376. EPIA\u201911, Springer-Verlag, Berlin, Heidelberg", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Reinforcement Learning in Robotics: A Survey", "author": ["J. Kober", "J.A.D. Bagnell", "J. Peters"], "venue": "International Journal of Robotics Research", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Q-Learning Based Market-Driven Multi-Agent Collaboration in Robot Soccer", "author": ["H. K\u00f6se", "U. Tatladede", "C. Mericli", "K. Kaplan", "H.L. Akan"], "venue": "Proceedings of the Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN). pp. 219\u2013228", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "UT Austin Villa 2011: A Champion Agent in the RoboCup 3D Soccer Simulation Competition", "author": ["P. MacAlpine", "D. Urieli", "S. Barrett", "S. Kalyanakrishnan", "F. Barrera", "A. LopezMobilia", "N. \u015etiurc\u0103", "V. Vu", "P. Stone"], "venue": "Proceedings of 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "PhD Thesis, University of Alberta", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "GQ(\u03bb): A General Gradient Algorithm for TemporalDifference Prediction Learning with Eligibility Traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proceedings of the 3rd Conference on Artificial General Intelligence (AGI-10) pp. 1\u20136", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward Off-Policy Learning Control with Function Approximation", "author": ["H.R. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML 2010). pp. 719\u2013726", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Acquiring a Broad Range of Empirical Knowledge in Real Time by Temporal-Difference Learning", "author": ["J. Modayil", "A. White", "P.M. Pilarski", "R.S. Sutton"], "venue": "Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics (SMC). pp. 1903\u20131910. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-timescale Nexting in a Reinforcement Learning Robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "From Animals to Animats 12 - 12th International Conference on Simulation of Adaptive Behavior (SAB). pp. 299\u2013309", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping", "author": ["A.Y. Ng", "D. Harada", "S.J. Russell"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning (ICML). pp. 278\u2013287. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Dynamic Switching and Real-Time Machine Learning for Improved Human Control of Assistive Biomedical Robots", "author": ["P. Pilarski", "M. Dawson", "T. Degris", "J. Carey", "R. Sutton"], "venue": "4th IEEE RAS EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob). pp. 296 \u2013302", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "On Experiences in a Complex and Competitive Gaming Domain: Reinforcement Learning Meets RoboCup", "author": ["M. Riedmiller", "T. Gabel"], "venue": "Third IEEE Symposium on Computational Intelligence and Games. pp. 17\u201323. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement Learning for Robot Soccer", "author": ["M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange"], "venue": "Autonomous Robots 27, 55\u201373", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Accurate Ball Tracking with Extended Kalman Filters as a Prerequisite for a High-Level Behavior with Reinforcement Learning", "author": ["A. Seekircher", "S. Abeyruwan", "U. Visser"], "venue": "The 6th Workshop on Humanoid Soccer Robots at Humanoid Conference, Bled (Slovenia)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Roboviz: Programmable Visualization for Simulated Soccer", "author": ["J. Stoecker", "U. Visser"], "venue": "R\u00f6fer, T., Mayer, N.M., Savage, J., Saranli, U. (eds.) RoboCup. pp. 282\u2013 293. Lecture Notes in Computer Science, Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Reinforcement Learning for RoboCupSoccer Keepaway", "author": ["P. Stone", "R.S. Sutton", "G. Kuhlmann"], "venue": "Adaptive Behavior 13(3), 165\u2013188", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Layered Learning", "author": ["P. Stone", "M. Veloso"], "venue": "Proceedings of the Eleventh European Conference on Machine Learning. pp. 369\u2013381. Springer Verlag", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Task Decomposition, Dynamic Role Assignment, and LowBandwidth Communication for Real-Time Strategic Teamwork", "author": ["P. Stone", "M. Veloso"], "venue": "Artificial Intelligence 110(2), 241\u2013273", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Horde: A Scalable Real-Time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems. pp. 761\u2013768. AAMAS \u201911, International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "A Convergent O(N) Algorithm for OffPolicy Temporal-Difference Learning with Linear Function Approximation", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems (NIPS). pp. 1609\u20131616. MIT Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Off-Policy Actor-Critic", "author": ["Thomas Degris", "R.S.S. Martha White"], "venue": "Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Scaling Life-Long Off-Policy Learning", "author": ["A. White", "J. Modayil", "R. Sutton"], "venue": "International Conference on Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE. pp. 1\u20136", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "The knowledge representational forms show different degrees of computational complexities and expressiveness [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "We learn the role assignment function using a framework that is developed based on the concepts of Horde, the real-time learning methodology, to express knowledge using General Value Functions (GVFs) [22].", "startOffset": 200, "endOffset": 204}, {"referenceID": 7, "context": "with action-value methods, a prediction question uses GQ(\u03bb) algorithm [8], and a control or a goal-oriented question uses Greedy-GQ(\u03bb) algorithm [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "with action-value methods, a prediction question uses GQ(\u03bb) algorithm [8], and a control or a goal-oriented question uses Greedy-GQ(\u03bb) algorithm [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 23, "context": "with policy-gradient methods, a goal-oriented question can be answered using Off-Policy Actor-Critic algorithm [24], with an extended state-value function, GTD(\u03bb) [7], for GVFs.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "with policy-gradient methods, a goal-oriented question can be answered using Off-Policy Actor-Critic algorithm [24], with an extended state-value function, GTD(\u03bb) [7], for GVFs.", "startOffset": 163, "endOffset": 166}, {"referenceID": 22, "context": "convergent to a local optimum or equilibrium point [23,9].", "startOffset": 51, "endOffset": 57}, {"referenceID": 8, "context": "convergent to a local optimum or equilibrium point [23,9].", "startOffset": 51, "endOffset": 57}, {"referenceID": 19, "context": "The role assignment is a part of the hierarchical machine learning paradigm [20,19], where a formation defines the role space.", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "The role assignment is a part of the hierarchical machine learning paradigm [20,19], where a formation defines the role space.", "startOffset": 76, "endOffset": 83}, {"referenceID": 3, "context": ", [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "Within the domain of robotic soccer, RL has been successfully applied in learning the keep-away subtask in the RoboCup 2D [18] and 3D [16] Soccer Simulation Leagues.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "Within the domain of robotic soccer, RL has been successfully applied in learning the keep-away subtask in the RoboCup 2D [18] and 3D [16] Soccer Simulation Leagues.", "startOffset": 134, "endOffset": 138}, {"referenceID": 1, "context": "Also, in other RoboCup leagues, such as the Middle Size League, RL has been applied successfully to acquire competitive behaviors [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "One of the noticeable impact on RL is reported by the Brainstormers team, the RoboCup 2D Simulation League team, on learning different subtasks [14].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "A comprehensive analysis of a general batch RL framework for learning challenging and complex behaviors in robot soccer is reported in [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "Despite convergence guarantees, Q(\u03bb) [21] with linear function approximation has been used in role assignment in robot soccer [5] and faster learning is observed with the introduction of heuristically accelerated methods [3].", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "Despite convergence guarantees, Q(\u03bb) [21] with linear function approximation has been used in role assignment in robot soccer [5] and faster learning is observed with the introduction of heuristically accelerated methods [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Despite convergence guarantees, Q(\u03bb) [21] with linear function approximation has been used in role assignment in robot soccer [5] and faster learning is observed with the introduction of heuristically accelerated methods [3].", "startOffset": 221, "endOffset": 224}, {"referenceID": 5, "context": "The dynamic role allocation framework based on dynamic programming is described in [6] for real-time soccer environments.", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "[22] have introduced a real-time learning architecture, Horde, for expressing knowledge using General Value Functions (GVFs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In addition, following researches describe methods and components to build strategic agents: [1] describes a methodology to build a cognizant robot that possesses vast amount of situated, reversible and expressive knowledge.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "[11] presents a methodology to \u201cnext\u201d in real time predicting thousands of features of the world state, and [10] presents methods predict about temporally extended consequences of a robot\u2019s behaviors in general forms of knowledge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] presents a methodology to \u201cnext\u201d in real time predicting thousands of features of the world state, and [10] presents methods predict about temporally extended consequences of a robot\u2019s behaviors in general forms of knowledge.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": ", [13,25]) for switching and prediction tasks in assistive biomedical robots.", "startOffset": 2, "endOffset": 9}, {"referenceID": 24, "context": ", [13,25]) for switching and prediction tasks in assistive biomedical robots.", "startOffset": 2, "endOffset": 9}, {"referenceID": 20, "context": "Recently, within the context of the RL framework [21], a knowledge representation language has been introduced, that is expressive and learnable from sensorimotor data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "pseudo-terminal-reward function [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "The complete information about the GVFs are available in [22,8,9,7].", "startOffset": 57, "endOffset": 67}, {"referenceID": 7, "context": "The complete information about the GVFs are available in [22,8,9,7].", "startOffset": 57, "endOffset": 67}, {"referenceID": 8, "context": "The complete information about the GVFs are available in [22,8,9,7].", "startOffset": 57, "endOffset": 67}, {"referenceID": 6, "context": "The complete information about the GVFs are available in [22,8,9,7].", "startOffset": 57, "endOffset": 67}, {"referenceID": 20, "context": "In the standard RL framework [21], let the agent and the world interact in discrete time steps t = 1, 2, 3, .", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "The objective of the standard RL framework is to learn the stochastic action-selection policy \u03c0 : S\u00d7A \u2192 [0, 1], that gives the probability of selecting each action in each state, \u03c0(s, a) = \u03c0(s|a) = P(At = a|St = s), such that the agent maximizes rewards summed over the time steps.", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "This factor is generalized to a termination function \u03b3 : S \u2192 [0, 1], where 1\u2212 \u03b3(s) is the probability of termination at state s, and a terminal reward z(s) is generated.", "startOffset": 61, "endOffset": 67}, {"referenceID": 6, "context": "Algorithm 1 Greedy-GQ(\u03bb) with linear function approximation for GVFs learning [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": ") \u2208 [0, 1].", "startOffset": 4, "endOffset": 10}, {"referenceID": 20, "context": "We use a feature extractor \u03c6 : St \u00d7 At \u2192 {0, 1} , N \u2208 N, built on tile coding [21] to generate feature vectors from state variables and actions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "Weights are learned using the gradientdescent temporal-difference Algorithm (1) [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Algorithm 2 Off-PAC with linear function approximation for GVFs learning [7,24].", "startOffset": 73, "endOffset": 79}, {"referenceID": 23, "context": "Algorithm 2 Off-PAC with linear function approximation for GVFs learning [7,24].", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": ") \u2208 [0, 1].", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "\u03c0 : St \u00d7At \u2192 [0, 1] (target policy is greedy w.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "\u03b3 : St \u2192 [0, 1] (termination function); 3.", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "\u03c0b : St \u00d7At \u2192 [0, 1] (behavior policy); 2.", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "It : St \u00d7At \u2192 [0, 1] (interest function); 3.", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "\u03bb : St \u2192 [0, 1] (eligibility-trace decay-rate function).", "startOffset": 9, "endOffset": 15}, {"referenceID": 5, "context": "Static role assignments often provide inferior performance in robot soccer [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 16, "context": "1: Primary formation, [17]", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "These are part of reward shaping [12].", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "It is not a uniformly distributed policy as used in [22].", "startOffset": 52, "endOffset": 56}], "year": 2014, "abstractText": "Collecting and maintaining accurate world knowledge in a dynamic, complex, adversarial, and stochastic environment such as the RoboCup 3D Soccer Simulation is a challenging task. Knowledge should be learned in real-time with time constraints. We use recently introduced Off-Policy Gradient Descent algorithms within Reinforcement Learning that illustrate learnable knowledge representations for dynamic role assignments. The results show that the agents have learned competitive policies against the top teams from the RoboCup 2012 competitions for three vs three, five vs five, and seven vs seven agents. We have explicitly used subsets of agents to identify the dynamics and the semantics for which the agents learn to maximize their performance measures, and to gather knowledge about different objectives, so that all agents participate effectively and efficiently within the group.", "creator": "LaTeX with hyperref package"}}}