{"id": "1402.4645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2014", "title": "A Survey on Semi-Supervised Learning Techniques", "abstract": "Semisupervised learning is a learning standard which deals with the study of how computers and natural systems such as human beings acquire knowledge in the presence of both labeled and unlabeled data. Semisupervised learning based methods are preferred when compared to the supervised and unsupervised learning because of the improved performance shown by the semisupervised approaches in the presence of large volumes of data. Labels are very hard to attain while unlabeled data are surplus, therefore semisupervised learning is a noble indication to shrink human labor and improve accuracy. There has been a large spectrum of ideas on semisupervised learning. In this paper we bring out some of the key approaches for semisupervised learning.", "histories": [["v1", "Wed, 19 Feb 2014 12:40:31 GMT  (203kb)", "http://arxiv.org/abs/1402.4645v1", "5 Pages, 3 figures, Published with International Journal of Computer Trends and Technology (IJCTT)"]], "COMMENTS": "5 Pages, 3 figures, Published with International Journal of Computer Trends and Technology (IJCTT)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["v jothi prakash", "dr l m nithya"], "accepted": false, "id": "1402.4645"}, "pdf": {"name": "1402.4645.pdf", "metadata": {"source": "CRF", "title": "A Survey On Semi-Supervised Learning Techniques", "authors": ["V. Jothi Prakash", "L.M. Nithya"], "emails": [], "sections": [{"heading": null, "text": "ISSN: 2231-2803 www.internationaljournalssrg.org Page 25Keywords - semi-supervised learning, generative mixture models, self-training, graph-based models"}, {"heading": "1. INTRODUCTION", "text": "This ability to learn from experience, analytical observation, and other means leads to a system that can improve endlessly, providing greater efficiency. It's about generality. There are two types of learning, called inductive learning. In inductive learning, the mission is to build a good classifier on the training data set, with the ability to simplify invisible data. During the training, the student has no knowledge of the test data set. In transductive learning, the student is aware that the test data set at the time of the training has a good classifier that generalizes these known test data sets."}, {"heading": "2. GENERATIVE MODELS", "text": "Generative models may be the oldest semisupervised learning method. It starts from a model, p (x, y) = p (y) p (x | y) where p (x | y) is a recognizable mixture distribution. with a large amount of unlabeled data, the mixture components can be detected; then we only need one labeled example per component to fully determine the mixture distribution. as a generative model, a mixture becomes obviously inductive, and of course, aISSN: 2231-2803 www.internationaljournalssrg.org has a relatively fewer number of parameters. A sample binary classification problem [9] is shown in the figure, 1 binary classification probleIn [1], a new model for bias correction similar to the generative model used for education. The training samples help in estimating the parameters needed for bias correction. This generative model is extended by combining bias correction parameters with maximum discriminatory categories used in education."}, {"heading": "3. SELF-TRAINING", "text": "In fact, most of them will be able to abide by the rules they have imposed on themselves. (...) In fact, it is as if they are able to change the rules. (...) In fact, it is as if they are able to change the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to break the rules. (...)"}, {"heading": "4. CO-TRAINING", "text": "Co-training [5] is a semi-supervised learning method that requires two views of the data. It assumes that each example is defined on the basis of two unequal sets of characteristics that provide different, complementary information about the instance. Preferably, the two views are conditionally independent in the sense that the two sets of characteristics in each case are conditionally independent and each view is sufficient. An instance's class can be accurately predicted from each view alone. Co-training begins by learning a separate classifier for each view by using each of the designated samples. The most reliable predictions of each classifier on the unlabeled data set are used to build iteratively additional labeled training data. A semi-supervised co-training style regression algorithm, i.e. COREG, is proposed [6]. This method uses two regressors in which one regressor labels the unlabeled data on the unlabeled data set, while the unlabeled data is labeled for the other."}, {"heading": "5. MULTIVIEW LEARNING", "text": "Multiview learning models require multiple hypotheses with different inductive biases, e.g. decision trees, na\u00efve bayes, SVMs, etc., to be trained from the same marked data set, and are necessary to make similar predictions about any unmarked data instance. In [7], semi-supervised learning methods are developed using two discriminatory sequence learning algorithms - the Hidden Markov (HM) perceptron and Support Vector Machines (SVM) a Multiview HM perceptron and a Multiview 1-standard and 2-standard HM SVMs are developed by consuming the principle of consensus maximization between propositions. Figure 2 Errors for multiple splits of characteristics intoviewsAccording to the consensus maximization principle, the sequence is presented as a non-uniform sequence."}, {"heading": "6. GRAPH-BASED MODELS", "text": "Graph-based semi-supervised methods define a graph in which the nodes are represented as labeled and blank samples in the dataset and the edges represent the similarity between the samples. These methods usually assume a smooth label over the graph. Graph-based methods do not require a parameter. These methods are discriminatory and also transductive in their nature. In [8] a general framework for semi-supervised learning on a directed graph is proposed. The structure of the graph along with the direction of the edges is taken into account. The algorithm takes input as directed graphs and the label set. The blank instances are classified by the steps. (1) A random passage over the graph with a probability matrix for the transition is defined so that it exhibits a unique stationary distribution such as the teleported random path. (2) Calculate the matrix by using the diagonal function of classifying the graph with its stationary distribution (3)."}, {"heading": "7. CONCLUSION", "text": "As already mentioned, marked data is expensive and hard to obtain. On the other hand, unmarked data is comparatively easy to collect. Semi-supervised learning can be used to classify the unmarked data, and it can also be used to develop better classifiers. Semi-supervised learning requires less human work and performs better than its unsupervised and supervised counterparts. Due to this advantage, semi-supervised learning is of great interest both in theory and practice."}], "references": [{"title": "Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning\u201d,Proc", "author": ["Xiaojin Zhu", "John Lafferty"], "venue": "of the 22nd Int\u2019l Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Semi-Supervised Self-Training of Object Detection Models", "author": ["C. Rosenberg", "M. Hebert", "H. Schneiderman"], "venue": "Proc. Seventh Workshop Applications of Computer Vision, vol. 1, pp. 29-36, Jan.2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning subjective nouns using extraction pattern bootstrapping.", "author": ["E. Riloff", "J. Wiebe", "T. Wilson"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning CoNLL-2003", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training\" COLT", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Semi-Supervised Regression with Co-Training Style Algorithms", "author": ["Zhi-Hua Zhou", "Ming Li"], "venue": "IEEE Transactions On Knowledge And Data Engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Multiview discriminative sequential learning", "author": ["U. Brefeld", "C. B \u0308uscher", "T. Scheffer"], "venue": "European Conference on Machine Learning \u201cProc. of the European Conference on Machine Learning (ECML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Semi-Supervised Learning Literature Survey\u201d, Computer Sciences TR 1530 University of Wisconsin, Madison", "author": ["Xiaojin Zhu"], "venue": "Last modified on July", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "A sample binary classification problem [9] is shown in Fig.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "In [2], Mixture models and graph-based algorithms are combined and spontaneously enforce the smoothness assumption.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [3], a semi-supervised approach for training object detection systems based on self-training is discussed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [4], two bootstrapping algorithms, MetaBootstrapping and Basilisk are discussed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "CO-TRAINING Co-training [5] is a semi-supervised learning technique that needs two views of the data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "COREG, is proposed [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "In [7], semi-supervised learning methods by using two discriminative sequence learning algorithms \u2013 the Hidden Markov (HM) perceptron and Support Vector Machines (SVM) a multi-view HM perceptron as well as multi-view 1-norm and 2-norm HM SVMs are developed by consuming the principle of consensus maximization between propositions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The error [7] for several splits of features into views is shown in the Fig.", "startOffset": 10, "endOffset": 13}], "year": 2014, "abstractText": "Semi-supervised learning is a learning standard which deals with the study of how computers and natural systems such as human beings acquire knowledge in the presence of both labeled and unlabeled data. Semi\u2013supervised learning based methods are preferred when compared to the supervised and unsupervised learning because of the improved performance shown by the semi-supervised approaches in the presence of large volumes of data. Labels are very hard to attain while unlabeled data are surplus, therefore semi-supervised learning is a noble indication to shrink human labor and improve accuracy. There has been a large spectrum of ideas on semi-supervised learning. In this paper we bring out some of the key approaches for semi-supervised learning.", "creator": null}}}