{"id": "1402.4845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2014", "title": "Diffusion Least Mean Square: Simulations", "abstract": "In this technical report we analyse the performance of diffusion strategies applied to the Least-Mean-Square adaptive filter. We configure a network of cooperative agents running adaptive filters and discuss their behaviour when compared with a non-cooperative agent which represents the average of the network. The analysis provides conditions under which diversity in the filter parameters is beneficial in terms of convergence and stability. Simulations drive and support the analysis.", "histories": [["v1", "Wed, 19 Feb 2014 22:59:14 GMT  (78kb)", "http://arxiv.org/abs/1402.4845v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.MA", "authors": ["jonathan gelati", "sithan kanna"], "accepted": false, "id": "1402.4845"}, "pdf": {"name": "1402.4845.pdf", "metadata": {"source": "CRF", "title": "DIFFUSION LEAST MEAN SQUARE: SIMULATIONS", "authors": ["Jonathan Gelati", "Sithan Kanna"], "emails": ["1jonathan@softwareengineer.it", "2ssk08@ic.ac.uk"], "sections": [{"heading": null, "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "II. BACKGROUND", "text": "In the following, we use X to specify the matrix of characteristics with the size M x L, where M is the number of characteristics and L is the number of points in time. We use y for the vector of the measured signals with the size L and w for the vector of the parameters with the size M. The positive scalar i is used to mark time instances (MS). The algorithm iteratively calculates the value of w at the time i as follows: w (i) \u2212 Gradient parentage algorithm applied to J (i) may find the values of the vector parameter w, which minimizes the value of J (i). The algorithm iteratively calculates the value of w at the time i \u2212 Gradient as follows: w (i \u2212 1) \u2212 Byte (i \u2212 1) -Byte J (i \u2212 1))))) v-filter w (1). The parameter \u00b5 is called the learning rate and is used to control the step size of the value of the algorithm \u2212 converting algorithm to the value of the smaller algorithm \u2212 p."}, {"heading": "III. DIFFUSION LMS", "text": "Diffusion LMS (DLMS) is used in settings in which multiple filters are run simultaneously to estimate the same optimal vector parameter. It extends LMS by introducing an additional step in which the estimates of the filters are combined. The combination step can be performed before or after the execution of the in Equation (4).A modeling For the purpose of our study we model a distributed machine learning environment as a set of N agents similar to [4].An agent is an independent computing unit that perceives the input signal with a certain degree of noise and iteratively applies the gradient derivation algorithm to calculate the parameter vector w. The goal of such an agent is to find the parameter vector choose that minimizes the cost function J (i). While we are computationally independent, an agent can share information about its estimates with other agents."}, {"heading": "IV. HOW DLMS OUTPERFORMS THE AVERAGE FILTER", "text": "This year is the highest in the history of the country."}, {"heading": "V. CONCLUSION", "text": "A network of agents running DLMS filters and sharing estimates for each iteration may actually perform better than the agent, which on average forms a homogeneous adaptive network, if there is diversity in the configuration of agents. Diversity is expressed by a different combination of parameters such as initial vector parameters, learning rate, trust coefficients or perceived signal. We can summarize the effects of each parameter as follows: \u2022 that agents with different initial vector parameters w0 determine what average configuration the agent network will behave after the second iteration; \u2022 that agents with different \u00b5 values imply that the agent network consists of faster and slower agents that align their estimates with each iteration; the net effect is a faster convergence compared to the behavior of the average agent in a non-cooperative network; \u2022 that agents who begin to trust their own assessments more than they trust the timing of other agents."}], "references": [{"title": "Diffusion Least-Mean Squares Over Adaptive Networks", "author": ["C. Lopes", "A. Sayed"], "venue": "Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Adaptive filters, Chapter 4 Stochastic Gradient", "author": ["V John Mathews", "Scott C Douglas"], "venue": "Adaptive filters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Diffusion strategies for Adaptation and Learning over Networks", "author": ["Ali H. Sayed", "Sheng-Yuan Tu", "Jianshu Chen", "Xiaochuan Zhao", "Zaid J. Towfic"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Diffusion Adaptation over Networks, CoRR", "author": ["A. Sayed"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A Note on the Generation of Random Normal Deviates", "author": ["G.E.P. Box", "Mervin E. Muller"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1958}], "referenceMentions": [{"referenceID": 0, "context": "To face these challenges, ML naturally evolves into distributed ML (DML) where we use a network of nodes, typically organized in neighbourhoods where each neighbourhood uses a diffusion adaptation strategy [2]: a node executes an ML algorithm, cooperates with others by sharing its estimations and combines the estimations of its neighbourhood using weighting coefficients.", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "Such an adaptive filter is called stochastic gradient adaptive filter as it makes use of the instantaneous gradient which according to [3] \u201cis an unbiased estimate of the true gradient.", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "For the purpose of our investigation, we model a distributed machine learning environment as a set of N agents similarly to [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "In general, a network of agents is an undirected graph where agents may change the neighbourhoods they belong to over time [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "This strategy is also called combine-then-adapt (CTA) in [4] and is summarized in Algorithm 1.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "In our simulations we used the Box Muller transformation [6] to randomly generate the two signals with a given mean and standard deviation.", "startOffset": 57, "endOffset": 60}], "year": 2014, "abstractText": "In this technical report we analyse the performance of diffusion strategies applied to the Least-Mean-Square adaptive filter. We configure a network of cooperative agents running adaptive filters and discuss their behaviour when compared with a non-cooperative agent which represents the average of the network. The analysis provides conditions under which diversity in the filter parameters is beneficial in terms of convergence and stability. Simulations drive and support the analysis.", "creator": "LaTeX with hyperref package"}}}