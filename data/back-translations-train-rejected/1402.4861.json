{"id": "1402.4861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "A Quasi-Newton Method for Large Scale Support Vector Machines", "abstract": "This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors.", "histories": [["v1", "Thu, 20 Feb 2014 01:44:33 GMT  (90kb,D)", "http://arxiv.org/abs/1402.4861v1", "5 pages, To appear in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2014"]], "COMMENTS": "5 pages, To appear in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aryan mokhtari", "alejandro ribeiro"], "accepted": false, "id": "1402.4861"}, "pdf": {"name": "1402.4861.pdf", "metadata": {"source": "CRF", "title": "A QUASI-NEWTON METHOD FOR LARGE SCALE SUPPORT VECTOR MACHINES", "authors": ["Aryan Mokhtari", "Alejandro Ribeiro"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Given a training set of known class points, the goal of a supporting vector machine (SVM) is to find a hyperplane that best separates the training set. If future samples are statistically identical to the training set, this hyperplane provides the best classification accuracy. Calculation of the separating hyperplane involves solving a convex optimization problem that can be implemented without great difficulty for medium-sized problems [1]. Large problems where the dimension of the points to be classified is large require a correspondingly large training set. In these situations, the calculation of the gradients required for the numerical determination of the separating hyperplanes is impracticable and motivates the use of stochastic gradient descending methods that build unbiased gradient estimates based on small sample data [1-4]. In practice, however, stochastic departure methods that require a large number of iterations to conform."}, {"heading": "2. STOCHASTIC QUASI-NEWTON METHOD", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "2.1. Regularized stochastic BFGS support vector machines", "text": "(1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1"}, {"heading": "6. REFERENCES", "text": "[1] L. Bottou, \"Large-scale machine learning with stochastic gradient descent],\" In Proceedings of COMPSTAT '2010, pp. 177-186, Physica-Verlag HD, 2010. [2] S. Shalev-Shwartz, Y. Singer, and N. Srebro, \"Pegasos: Primal estimated sub-gradient solver for svm,\" In Proceedings of the 24th International Conference on Machine learning, pp. 807-814, ACM, 2007. [3] T. Zhang, \"Solving large scale linear problems using stochastic gradient descent descent algorithms,\" in Proceedings of the twenty-first international conference on Machine learning, pp. 919926, ACM, 2004. [4] N. LeRoux, M. Schmidt, and F. Bach, \"A stochastic gradient method with an exponential convergence."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pp. 177\u2013186, Physica-Verlag HD, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 807\u2013814, ACM, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference on Machine learning, p. 919926, ACM, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets", "author": ["N. LeRoux", "M. Schmidt", "F. Bach"], "venue": "arXiv preprint arXiv, 1202.6258, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "On the local and superlinear convergence of quasi-newton methods", "author": ["C.G. Broyden", "J.E.D. Jr.", "Wang", "J.J. More"], "venue": "IMA J. Appl. Math, vol. 12, no. 3, pp. 223\u2013245, June 1973.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Sgd-qn: Careful quasinewton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1737\u20131754, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Global convergence of a class of quasi-newton methods on convex problems", "author": ["R.H. Byrd", "J. Nocedal", "Y. Yuan"], "venue": "SIAM J. Numer. Anal., vol. 24, no. 5, pp. 1171\u20131190, October 1987.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1987}, {"title": "A dual stochastic dfp algorithm for optimal resource allocation in wireless systems", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "Proc. IEEE 14th Workshop on Signal Process. Advances in Wireless Commun. (SPAWC). pp. 21-25, Darmstadt Germany, June 16-19 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized stochastic bfgs algorithm", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Global Conf. on Signal and Inform. Process. pp. 1109-1112, Austin Texas, Dec. 3-5 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical optimization, 2nd ed", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Some global convergence properties of a variable metric algorithm for minimization without exact line search, 2nd ed", "author": ["M.J.D. Powell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1971}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. Gnter"], "venue": "Proc. 11th Intl. Conf. on Artificial Intelligence and Statistics (AIstats), p. 433 440, Soc. for Artificial Intelligence and Statistics, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "The nature of statistical learning theory, 2nd ed", "author": ["V. Vapnik"], "venue": "springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "arXiv preprint arXiv, 1401.7625, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A characterization of super linear convergence and its application to quasi-newton methods", "author": ["J.J.E. Dennis", "J.J. More"], "venue": "Mathematics of computation, vol. 28, no. 126, pp. 549\u2013560, 1974.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}], "referenceMentions": [{"referenceID": 0, "context": "Computation of the separating hyperplane entails solution of a convex optimization problem that can be implemented without much difficulty in problems of moderate size [1].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 1, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 2, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 3, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 4, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 9, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 10, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 11, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "In particular, we adapt a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) method [9] for the solution of SVM classification problems (Section 2).", "startOffset": 137, "endOffset": 140}, {"referenceID": 12, "context": "distances to the separating hyperplane, as measured by the loss function l((x, y);w), with the minimization of the L2 norm \u2016w\u20162 to enforce desirable properties in w\u2217 [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The following lemma shows that solutions of (7) can be computed by a simple algebraic formula (see [14] for proofs of results in this paper).", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 9, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 10, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 14, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 8, "context": "In order to handle large scale problems with reasonable convergence times we adapted a regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method [9].", "startOffset": 192, "endOffset": 195}], "year": 2014, "abstractText": "This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors.", "creator": "LaTeX with hyperref package"}}}