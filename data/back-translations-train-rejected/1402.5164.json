{"id": "1402.5164", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "Distribution-Independent Reliable Learning", "abstract": "We study several questions in the reliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than others. A positive reliable classifier is one that makes no false positive errors. The goal in the positive reliable agnostic framework is to output a hypothesis with the following properties: (i) its false positive error rate is at most $\\epsilon$, (ii) its false negative error rate is at most $\\epsilon$ more than that of the best positive reliable classifier from the class. A closely related notion is fully reliable agnostic learning, which considers partial classifiers that are allowed to predict \"unknown\" on some inputs. The best fully reliable partial classifier is one that makes no errors and minimizes the probability of predicting \"unknown\", and the goal in fully reliable learning is to output a hypothesis that is almost as good as the best fully reliable partial classifier from a class.", "histories": [["v1", "Thu, 20 Feb 2014 22:41:39 GMT  (24kb)", "http://arxiv.org/abs/1402.5164v1", "20 pages"]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CC cs.DS", "authors": ["varun kanade", "justin thaler"], "accepted": false, "id": "1402.5164"}, "pdf": {"name": "1402.5164.pdf", "metadata": {"source": "CRF", "title": "Distribution-Independent Reliable Learning", "authors": ["Varun Kanade", "Justin Thaler"], "emails": ["vkanade@eecs.berkeley.edu", "jthaler@seas.harvard.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.51 64v1 [cs.LG] 2 0Fe b20 14For distributional independent learning, the most well-known algorithms for PAC learning typically use polynomial threshold representations, while state-of-the-art agnostic learning algorithms use pointwise polynomial approximations. We show that one-sided polynomial approximations, an intermediate between polynomial threshold representations and point-by-point polynomial approximations, are sufficient for learning in reliable agnostic environments. We then show that majorities can be learned completely reliably, and disjunctions of majorities can be learned positively reliably by constructing suitable one-sided polynomial approximations. Our absolutely reliable algorithm for majorities provides the first proof that absolutely reliable learning can be strictly easier than agnostic learning. Our algorithms also meet strong characteristics in terms of attribute efficiency, and in many cases offer seamless compromises between runtime and completeness."}, {"heading": "1 Introduction", "text": "In many learning tasks, one type of error is more costly than other types. For example, if the detection of spam messages, an important email marked as spam (a false positive) is a major problem, while false negatives are only a minor nuisance. On the other hand, in constellations such as the detection of errors in an electrical network, false negatives can be very harmful. In other constellations, it may be better to refrain from predicting at all than to make a wrong mistake, for example, in the detection of medical illnesses. According to Kalai et al. (2012), this type of task is called reliable learning. The closely related tasks have been extensively studied in statistics and machine learning literature. We discuss some of this work later than wrong. We simply notice that the work of Kalai et al. and the current work distracts from much of the existing literature."}, {"heading": "1.1 Our Contributions", "text": "In this work, we focus on distributive reliable learning methods, and our most important technical contribution is to provide new reliable learning algorithms for a variety of concept classes. (We now place our reliable learning algorithms in the context of previous work on PAC and agnostic learning.) However, the approximate degree of a Boolean function f: (1) n is the lowest degree of real polynomia, which points to errors in most areas of learning. (1) The approximate degree (with error parameters) of f is the smallest degree of real polynomia, which approaches errors at points. (It is known that low-threshold concept classes can be learned efficiently in Valiant's PAC model; in fact, threshold upper is below the fastest known PAC learning algorithms for a variety of basic concept classes, including DNF and reading formulations (Klivans and Servans, 2004)."}, {"heading": "1.2 Related Work", "text": "The problem of the reliable one-one-one-one-one can be expressed as minimizing a loss function with different costs for false negative and false positive errors (see e.g. Domingos, 1999, Elkan, 2001). Reliable learning is also possible with respect to the Neyman Pearson criterion from classical statistics - where it has been shown that the optimal strategy for minimizing one type of error that is subject to the other type is due to the threshold of the ratio between the probabilities (Neyman and Pearson, 1933). However, the main problem is that the loss functions with different costs from these previous works are unreliable and the resulting optimization problems are insolvable. The work of Kalai et al. (2012) and the current work of algorithms in which we rely on algorithms with both verifiable guarantees of their distribution errors in relation to the zero one-one-one-one-one-one-one-one-one"}, {"heading": "2 Preliminaries and Definitions", "text": "Let us use the convention that + 1 is TRUE and \u2212 1 FALSE.1 To simplify the notation, let us implicitly discuss the parameter n, which corresponds to the length of the input vectors. Let c, h: X \u2192 {\u2212 1, 1} be Boolean functions. Let EX (c, \u00b5, c) = Prx \u0445 \u00b5 [c (x) 6 = h (x)] be the error of the hypothesis h with respect to the concept c and the distribution \u00b5. Let EX (c, \u00b5) be the example oracle that when queriing a pair (x, c (x) yields \u00b5 [c) the ability c, and c is a concept in C. Since the algorithms presented in this paper are not typically executed explicitly........... n, we will not make these functions appear contrary to the usual analysis of 1algorithmic."}, {"heading": "2.1 Reliable Learning", "text": "We check the various concepts of reliable agnostic learning proposed by Kalai et al. (2012) (false). As in the case of agnostic learning (2012), the data comes from an arbitrary common distribution D over X \u00d7 {\u2212 \u2212 1, 1}. For a boolean function, h: X \u2192 {\u2212 1, 1}, we define the false positive error (false +) and the false negative error (false \u2212) with respect to D as follows: false + (h, D) = Pr (x) = x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x (x, x, x, x, x, x, x, x, x, x, x), x (x, x, x, x), x (x, x, x, x, x, x, x, x, x, x, x, x, x, x, x (x, x, x, x, x, x, x, x, x x, x, x, x, x, x, x, x x, x, x x, x, x, x x x, x x (x x x x, x, x x x x, x, x x, x x, x x x x, x, x x x x, x x x x, x, x x x x x, x x x x, x x x, x x x, x x x x x, x, x x x x, x x x x x x, x x x x x, x x x x x, x x x, x x x x x, x x x, x x, x x, x x x, x x, x x, x x x x x, x x x x x, x x x x, x, x x x x, x, x x x x, x, x x x x x, x, x, x x x x x x, x x x x"}, {"heading": "2.2 Approximating Polynomials", "text": "If p: {\u2212 1, 1} n \u2192 R is a real polynomial, deg (p) will denote the total degree of p. Let f: {\u2212 1, 1} n \u2192 {\u2212 1, 1} n be a Boolean function. We say that a polynomial p: {\u2212 1, 1} n \u2192 {\u2212 1, 1} for f if | p (x) \u2212 f (x) | \u2264 \u0432 for all x {\u2212 1, 1} n. Let d: g: g (f) stand for the least degree of alignment with f. We define d: eg (f) = d: eg1 / 3 (f) and refer to the approximate degree of f without restriction. The constant 1 / 3 is arbitrary and is chosen by convention."}, {"heading": "2.3 One-sided Approximating Polynomials", "text": "The definitions as presented here appeared essentially in earlier work by Bun and Thaler (2013a) (see also Sherstov (2014)), which required only the term we call positive one-sided approximate degree. Here, we explicitly distinguish between positive and negative one-sided approximate weight. We say that a polynomial p is a positive one-sided approximate approximation to f if p meets the following two conditions.1. For all x-f \u2212 1 (1), p (x) n \u2192 {\u2212 1, 1} n can be a Boolean function. We say that a polynomial p is a positive one-sided approximation to f if p \u2212 c meets the following two conditions.1. For all x-f \u2212 1, p (x) an approximate approximation to p \u2212 1 (x)."}, {"heading": "2.4 Additional Notation", "text": "In this thesis, we use O to hide polylogarithmic factors in n and log (1 / B). We also define sgn (t) = \u2212 1 if t \u2264 0 and 1 otherwise."}, {"heading": "2.5 Generalization Bounds", "text": "Let F: X \u2192 R be a function class. Let's take 1,. \u2212 n independently of each other values in {\u2212 1, + 1} with equal probability and select the variables x1,.., xn i.i.d. from some distribution \u00b5 over X. Then the Rademacher complexity of F, as Rm (F) is defined with equal probability as: Rm (F) = E [sup f: F 1 n m,."}, {"heading": "3 Learning Algorithms", "text": "In Section 3.1, we first present a very simple algorithm for positive reliable learning disjunctions. However, it is unlikely that there are such simple algorithms for reliable learning for richer classes; in Section 3.2, we present our main result, which derives reliable learning algorithms from one-sided polynomic approaches."}, {"heading": "3.1 A Simple Algorithm for Positive Reliably Learning Disjunctions", "text": "The learning algorithm (shown in Figure 1) ignores all positive examples and finds a disjunction that is maximally positive and correctly classifies all negative examples (see also Kearns and Vazirani (1994, Fig. 1). Theorem 4. The algorithm in Fig. 1 Positive reliably learns the class of disjunctions for some m in O (n / 2), where m is the number of designated examples that the algorithm calls Input.Proof. Let DISJ designate the class of disjunctions and let D be the distribution over X \u00d7 {\u2212 1, 1}. It is known that VC-DIM (DISJ) = n and thus for some m = O (n / 2) the following applies to each c-DISJ: | false + (c; D) \u2212 1m \u00b2 c: yi = \u2212 1 I (c (xi) = + 1) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 h, false (c; J) \u2212 J \u00b2 = all DISJ-DISJ = 1 = 1.0 = (1.0) = (1.D) is reliable."}, {"heading": "3.2 From One-Sided Approximations to Reliable Learning", "text": "In this section we demonstrate our most important learning outcomes. We describe a generic algorithm that analogous only the loss of punishment. We reliably learn any concept class that can be unilaterally approved by degree d and weight W polynomials. Weight W controls the sample complexity of the learning algorithm, and degree d controls the running time. For many natural classes, the resulting algorithm has strong attribute-efficient properties, since the weight of the approximate polynomial typically depends only on the number of relevant attributes. Our algorithm extends the L1 regression technology of Kalai et al al al al al al. (2005) For agnostic learning, we need a more detailed analysis, as positive-reliable learning typically depends only on the number of relevant attributes. Our algorithm extends to the L1 regression technology of Kalai et al al al al al al al al al al al al. (2005) For agnostic learning, we need a more detailed analysis, since positive learning requires a more detailed analysis of the negative algorithm in case of the false algorithm."}, {"heading": "4 One-sided Polynomial Approximations", "text": "In this section we construct both positive and negative one-sided polynomial approximations for low hemispheres (and positive (both negative) one-sided approximations for disjunctions (both negative conjunctions) of low hemispheres."}, {"heading": "5 Trading off Runtime for Sample Complexity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Standard Agnostic Learning of Conjunctions", "text": "The result is based on the presence of log-n polynomials for the n-variable AND function of degree O (1 / 2). Theorem 6 gives an avenue for obtaining better sample complexity, to the cost of increased runtime: if we can show that any connection to n-variable variables by a degree d polynomial of weight W (1 / 3). n Log 6 gives an avenue for obtaining better sample complexity, at the cost of increased complexity: if we can show that any connection to n variables by a degree d polynomial of weight W (1 / 3), then the L1 regression algorithm becomes only poly (d, W)."}, {"heading": "5.2 Positive Reliable Learning of DNFs", "text": "As discussed in Section 1.2, the reductions of Kalai et al. (2012), combined with the agnostic learning algorithm for conjunctions based on Kalai et al. (2005), imply that DNFs can be reliably learned in time 2 (O). However, the sample complexity of the resulting algorithm may be as large as its runtime. Here, we give an algorithm for positive reliable learning of DNFs that has smaller sample complexity at the expense of longer runtime. Theorem 10. For each DNF of size m and width (i.e., maximum runtime) at most w, and each d > log protocol of size (1 / 2) exists an (explicit) positive unilateral polynomial polynomial of size m and weight 2O (w) most."}, {"heading": "6 Limitations of Our Techniques", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 On Halfspaces", "text": "Theorem 7 specifies that all light weight semispaces (i.e. the weight o (n2 \u2212 \u03b4) for some \u043c > 0) can be learned reliably in time 2o (n). It is reasonable to ask whether we can reliably learn all semispaces in time 2o (n) with our techniques. Unfortunately, the answer is no. Theorem 11. There is a semispace h for which d like +, 1 / 8 (h) and d like \u2212, 1 / 8 (h) are both in time 2o (n). Proof. We prove the statement about d like \u2212, 1 / 4 like in the case of d like \u2212, 1 / 4 similar. In the face of a Boolean function h: {\u2212 1, 1} n \u2192 {\u2212 1, 1} we leave gh: {\u2212 1} 2n \u2192 {\u2212 1, 1} to imply the statement about l like the function g (x, y) = h (x2), where x2, 1 \u2212 b, the copies of the section of two (the) fragments are composed."}, {"heading": "6.2 On DNFs", "text": "All polynomial-sized DNFs can be reliably learned positively in time and sample complexity. It is natural to ask whether DNFs can be reliably learned negatively with our techniques with similar efficiency. Unfortunately, this is not the case. Bun and Thaler (2013a), extending a ground-breaking lower limit of Aaronson and Shi (2004), showed that there is a polynomial-sized DNF f (more precisely, f is the negation of the ELEMENT-DISTINCTNESS function) that fulfills d-eg \u2212 (f) = (n / log n) 2 / 3; therefore, our techniques cannot reliably learn polynomial DNFs better in time than exp (O-DN2 / 3)."}, {"heading": "7 Discussion", "text": "We have shown that low one-sided approach classes can be learned efficiently in the reliable agnostic model. As we have seen, one-sided approximation is an intermediate concept that lies between half-sided degree and approximate degree; we have identified important concept classes, such as majorities and intersections of majorities whose one-sided approximation degree is strictly smaller than its approximate degree. Consequently, we have obtained reliable (in some cases even completely reliable) agnostic learning algorithms that are strictly more efficient than the most rapidly known agnostic ones. We have thus provided the first proof that even fully reliable agnostic learning can be strictly easier than agnostic learning. The notion of one-sided approximate polynomic approach was only recently introduced (Bun and Thaler (2013a), and before that it was only used to prove lower limits."}, {"heading": "Acknowledgments", "text": "UK is supported by a Simons postdoctoral fellowship. JT is supported by a Simons Research Fellowship. This research was conducted while the authors were at the Simons Institute for the Theory of Computing at the University of California, Berkeley."}, {"heading": "A Missing Details For Theorem 7", "text": "For all k > 0, Kahn et al. (1996) define the polynomial Sk (t) as follows (below): a, b, and r are parameters that Kahn et al. \u2212 \u2212 \u2212 \u2212 ultimately set to a =. (k / logW), b =. (k2 / (W logW), and r = k \u2212 a \u2212 b). Sk (t) = C \u2212 1 \u00b7 a \u2212 i = 0 (t \u2212 i) \u00b7 W \u2212 j = W \u2212 b (t \u2212 j) \u00b7 Tr (t \u2212 a W \u2212 b), (b \u2212 a logW logW), (15) where C = (a = 0 (W \u2212 i) \u00b7 b \u2212 b (W \u2212 i) \u00b7 W \u2212 b \u2212 b (t \u2212 j) is a normalization constant chosen so that Sk (W) = 1, and as usual Tr is the r'th Chebyshev polynomial."}], "references": [{"title": "Quantum lower bounds for the collision and the element distinctness problems", "author": ["Scott Aaronson", "Yaoyun Shi"], "venue": "J. ACM,", "citeRegEx": "Aaronson and Shi.,? \\Q2004\\E", "shortCiteRegEx": "Aaronson and Shi.", "year": 2004}, {"title": "Any and-or formula of size n can be evaluated in time n1/2+o(1) on a quantum computer", "author": ["Andris Ambainis", "Andrew M. Childs", "Ben Reichardt", "Robert Spalek", "Shengyu Zhang"], "venue": "SIAM J. Comput.,", "citeRegEx": "Ambainis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ambainis et al\\.", "year": 2010}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Bounds for small-error and zero-error quantum algorithms. In FOCS, pages 358\u2013368", "author": ["Harry Buhrman", "Richard Cleve", "Ronald de Wolf", "Christof Zalka"], "venue": "IEEE Computer Society,", "citeRegEx": "Buhrman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Buhrman et al\\.", "year": 1999}, {"title": "Hardness amplification and the approximate degree of constant-depth circuits", "author": ["Mark Bun", "Justin Thaler"], "venue": null, "citeRegEx": "Bun and Thaler.,? \\Q2013\\E", "shortCiteRegEx": "Bun and Thaler.", "year": 2013}, {"title": "Dual lower bounds for approximate degree and markov-bernstein inequalities", "author": ["Mark Bun", "Justin Thaler"], "venue": null, "citeRegEx": "Bun and Thaler.,? \\Q2013\\E", "shortCiteRegEx": "Bun and Thaler.", "year": 2013}, {"title": "Introduction to Approximation Theory", "author": ["E.W. Cheney"], "venue": "AMS Chelsea Publishing Series. AMS Chelsea Pub.,", "citeRegEx": "Cheney.,? \\Q1982\\E", "shortCiteRegEx": "Cheney.", "year": 1982}, {"title": "Metacost: A general method for making classifiers cost-sensitive", "author": ["Pedro Domingos"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Domingos.,? \\Q1999\\E", "shortCiteRegEx": "Domingos.", "year": 1999}, {"title": "The foundations of cost-sensitive learning", "author": ["Charles Elkan"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Elkan.,? \\Q2001\\E", "shortCiteRegEx": "Elkan.", "year": 2001}, {"title": "Representation, approximation and learning of submodular functions using low-rank decision trees", "author": ["Vitaly Feldman", "Pravesh Kothari", "Jan Vondr\u00e1k"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Feldman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2013}, {"title": "A separation of np and conp in multiparty communication", "author": ["Dmitry Gavinsky", "Alexander A. Sherstov"], "venue": "complexity. Theory of Computing,", "citeRegEx": "Gavinsky and Sherstov.,? \\Q2010\\E", "shortCiteRegEx": "Gavinsky and Sherstov.", "year": 2010}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["David Haussler"], "venue": "Information and Computation,", "citeRegEx": "Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Haussler.", "year": 1992}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["Sham M. Kakade", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Agnostically learning halfspaces", "author": ["Adam Tauman Kalai", "Adam R. Klivans", "Yishay Mansour", "Rocco A. Servedio"], "venue": "In FOCS,", "citeRegEx": "Kalai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2005}, {"title": "Reliable agnostic learning", "author": ["Adam Tauman Kalai", "Varun Kanade", "Yishay Mansour"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Toward efficient agnostic learning", "author": ["Michael Kearns", "Robert E. Schapire", "Linda M. Sellie"], "venue": "In Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "An Introduction to Computational Learning Theory", "author": ["Michael J. Kearns", "Umesh Vazirani"], "venue": null, "citeRegEx": "Kearns and Vazirani.,? \\Q1994\\E", "shortCiteRegEx": "Kearns and Vazirani.", "year": 1994}, {"title": "Lower bounds for agnostic learning via approximate rank", "author": ["Adam R. Klivans", "Alexander A. Sherstov"], "venue": "Computational Complexity,", "citeRegEx": "Klivans and Sherstov.,? \\Q2010\\E", "shortCiteRegEx": "Klivans and Sherstov.", "year": 2010}, {"title": "On the problem of the most efficient tests for statistical hypotheses", "author": ["J. Neyman", "E.S. Pearson"], "venue": "Philos. Trans. R. So. Lond. Ser. A Contain. Pap. Math. Phys. Character,", "citeRegEx": "Neyman and Pearson.,? \\Q1933\\E", "shortCiteRegEx": "Neyman and Pearson.", "year": 1933}, {"title": "An Introduction to the Approximation of Functions. Blaisdell book in numerical analysis and computer science", "author": ["T.J. Rivlin"], "venue": "Dover Publications,", "citeRegEx": "Rivlin.,? \\Q1981\\E", "shortCiteRegEx": "Rivlin.", "year": 1981}, {"title": "Attribute-efficient learning and weight-degree tradeoffs for polynomial threshold functions", "author": ["Rocco A. Servedio", "Li-Yang Tan", "Justin Thaler"], "venue": "JMLR Proceedings,", "citeRegEx": "Servedio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Servedio et al\\.", "year": 2012}, {"title": "Breaking the Minsky-Papert barrier for constant-depth circuits", "author": ["A.A. Sherstov"], "venue": "In STOC,", "citeRegEx": "Sherstov.,? \\Q2014\\E", "shortCiteRegEx": "Sherstov.", "year": 2014}, {"title": "Approximating the and-or tree", "author": ["Alexander A. Sherstov"], "venue": "Theory of Computing,", "citeRegEx": "Sherstov.,? \\Q2013\\E", "shortCiteRegEx": "Sherstov.", "year": 2013}, {"title": "Optimal bounds for sign-representing the intersection of two halfspaces by polynomials", "author": ["Alexander A. Sherstov"], "venue": null, "citeRegEx": "Sherstov.,? \\Q2013\\E", "shortCiteRegEx": "Sherstov.", "year": 2013}, {"title": "A theory of the learnable", "author": ["Leslie G. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}], "referenceMentions": [{"referenceID": 13, "context": "Abstract We study several questions in the reliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than other types.", "startOffset": 83, "endOffset": 103}, {"referenceID": 13, "context": "Following Kalai et al. (2012), we call these kinds of tasks reliable learning.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "Following Kalai et al. (2012), we call these kinds of tasks reliable learning. Closely related tasks have been widely studied in the statistics and machine learning literature. We discuss some of this work later; here, we simply note that the work of Kalai et al. and the present work depart from much of the extant literature by emphasizing computational considerations, i.e., by focusing on \u201cfast\u201d algorithms, and guarantees with respect to the zero-one loss. Kalai et al. (2012) introduced a formal framework to study reliable learning in the agnostic setting, which is a challenging model that captures the problem of learning in the presence of adversarial classification noise.", "startOffset": 10, "endOffset": 482}, {"referenceID": 13, "context": "Following Kalai et al. (2012), we call these kinds of tasks reliable learning. Closely related tasks have been widely studied in the statistics and machine learning literature. We discuss some of this work later; here, we simply note that the work of Kalai et al. and the present work depart from much of the extant literature by emphasizing computational considerations, i.e., by focusing on \u201cfast\u201d algorithms, and guarantees with respect to the zero-one loss. Kalai et al. (2012) introduced a formal framework to study reliable learning in the agnostic setting, which is a challenging model that captures the problem of learning in the presence of adversarial classification noise. In particular, the goal of an agnostic learning algorithm is to produce a hypothesis that has error that is at most \u01eb higher than the best from a certain class. A false positive error occurs when the true label is negative, but the hypothesis predicts positive. Analogously, a false negative error occurs when the true label is positive, but the hypothesis predicts negative. The best positive reliable classifier from a class is one that make no false positive errors and minimizes false negative errors. In the positive reliable learning setting, the goal of a learning algorithm is to output a hypothesis with the following properties: (i) its false positive error rate is at most \u01eb, (ii) its false negative error rate is at most \u01eb more than that of the best positive reliable classifier from the class. The notion of negative reliable learning is identical with the roles of false positive and false negatives reversed. Kalai et al. (2012) also introduced the notion of full reliability.", "startOffset": 10, "endOffset": 1628}, {"referenceID": 13, "context": "Meanwhile, concept classes with low approximate degree can be efficiently learned in the agnostic model, a connection that has yielded the fastest known algorithms for distribution-independent agnostic learning (Kalai et al., 2005).", "startOffset": 211, "endOffset": 231}, {"referenceID": 21, "context": "imate degree was introduced in its present form by Bun and Thaler (2013a) (see also (Sherstov, 2014)), though equivalent dual formulations had been used in several prior works (Gavinsky and Sherstov, 2010, Sherstov, 2013a, Bun and Thaler, 2013b).", "startOffset": 84, "endOffset": 100}, {"referenceID": 4, "context": "imate degree was introduced in its present form by Bun and Thaler (2013a) (see also (Sherstov, 2014)), though equivalent dual formulations had been used in several prior works (Gavinsky and Sherstov, 2010, Sherstov, 2013a, Bun and Thaler, 2013b).", "startOffset": 51, "endOffset": 74}, {"referenceID": 4, "context": "imate degree was introduced in its present form by Bun and Thaler (2013a) (see also (Sherstov, 2014)), though equivalent dual formulations had been used in several prior works (Gavinsky and Sherstov, 2010, Sherstov, 2013a, Bun and Thaler, 2013b). Our learning algorithm is similar to the L1 regression algorithm of Kalai et al. (2005); however, the analysis of our algorithm is more delicate.", "startOffset": 51, "endOffset": 335}, {"referenceID": 18, "context": "Reliable learning is also related to the Neyman-Pearson criterion from classical statistics \u2014 where it has been shown that the optimal strategy to minimize one type of errors, subject to the other type being bounded, is to threshold the ratio of the likelihoods (Neyman and Pearson, 1933).", "startOffset": 262, "endOffset": 288}, {"referenceID": 7, "context": ", (Domingos, 1999, Elkan, 2001)). Reliable learning is also related to the Neyman-Pearson criterion from classical statistics \u2014 where it has been shown that the optimal strategy to minimize one type of errors, subject to the other type being bounded, is to threshold the ratio of the likelihoods (Neyman and Pearson, 1933). However, the main problem is computational; in general the loss functions with different costs from these prior works are not convex and the resulting optimization problems are intractable. The work of Kalai et al. (2012) and the present work departs from the prior work in that we focus on algorithms with both provable guarantees on their generalization error with respect to the zero-one loss, and bounds on their computational complexity, rather than focusing purely on statistical efficiency.", "startOffset": 3, "endOffset": 546}, {"referenceID": 7, "context": ", (Domingos, 1999, Elkan, 2001)). Reliable learning is also related to the Neyman-Pearson criterion from classical statistics \u2014 where it has been shown that the optimal strategy to minimize one type of errors, subject to the other type being bounded, is to threshold the ratio of the likelihoods (Neyman and Pearson, 1933). However, the main problem is computational; in general the loss functions with different costs from these prior works are not convex and the resulting optimization problems are intractable. The work of Kalai et al. (2012) and the present work departs from the prior work in that we focus on algorithms with both provable guarantees on their generalization error with respect to the zero-one loss, and bounds on their computational complexity, rather than focusing purely on statistical efficiency. Kalai et al. (2012) showed that any concept class that is agnostically learnable under a fixed distribution is also learnable in the reliable agnostic learning models under the same distribution.", "startOffset": 3, "endOffset": 842}, {"referenceID": 10, "context": "Using these general reductions, Kalai et al. showed that the class of polynomial-size DNF formulae is positive reliable learnable under the uniform distribution in polynomial time with membership queries (it also follows from their reductions and the agnostic learning algorithm of Kalai et al. (2005) described below that DNF formulae can be positive reliably learned in the distribution-independent setting in time 2\u00d5( \u221a n)).", "startOffset": 32, "endOffset": 302}, {"referenceID": 10, "context": "Using these general reductions, Kalai et al. showed that the class of polynomial-size DNF formulae is positive reliable learnable under the uniform distribution in polynomial time with membership queries (it also follows from their reductions and the agnostic learning algorithm of Kalai et al. (2005) described below that DNF formulae can be positive reliably learned in the distribution-independent setting in time 2\u00d5( \u221a n)). Agnostically learning DNFs under the uniform distribution remains a notorious open problem, and thus their work gave the first indication that positive (or negative) reliable learning may be easier than agnostic learning. Kalai et al. (2005) put forth an algorithm for agnostic learning based on L1-regression.", "startOffset": 32, "endOffset": 670}, {"referenceID": 10, "context": "Using these general reductions, Kalai et al. showed that the class of polynomial-size DNF formulae is positive reliable learnable under the uniform distribution in polynomial time with membership queries (it also follows from their reductions and the agnostic learning algorithm of Kalai et al. (2005) described below that DNF formulae can be positive reliably learned in the distribution-independent setting in time 2\u00d5( \u221a n)). Agnostically learning DNFs under the uniform distribution remains a notorious open problem, and thus their work gave the first indication that positive (or negative) reliable learning may be easier than agnostic learning. Kalai et al. (2005) put forth an algorithm for agnostic learning based on L1-regression. Our reliable learning algorithms based on one-sided approximate degree upper bounds is inspired by and generalizes their work. Klivans and Sherstov (2010) subsequently established strong limitations on the L1-regression approach of Kalai et al.", "startOffset": 32, "endOffset": 894}, {"referenceID": 10, "context": "Using these general reductions, Kalai et al. showed that the class of polynomial-size DNF formulae is positive reliable learnable under the uniform distribution in polynomial time with membership queries (it also follows from their reductions and the agnostic learning algorithm of Kalai et al. (2005) described below that DNF formulae can be positive reliably learned in the distribution-independent setting in time 2\u00d5( \u221a n)). Agnostically learning DNFs under the uniform distribution remains a notorious open problem, and thus their work gave the first indication that positive (or negative) reliable learning may be easier than agnostic learning. Kalai et al. (2005) put forth an algorithm for agnostic learning based on L1-regression. Our reliable learning algorithms based on one-sided approximate degree upper bounds is inspired by and generalizes their work. Klivans and Sherstov (2010) subsequently established strong limitations on the L1-regression approach of Kalai et al. (2005), proving lower bounds on the size of any set of \u201cfeature functions\u201d that can point-wise approximate the concept classes of majorities and conjunctions.", "startOffset": 32, "endOffset": 991}, {"referenceID": 24, "context": "Definition 1 (PAC Learning (Valiant, 1984)).", "startOffset": 27, "endOffset": 42}, {"referenceID": 13, "context": "We review the various notions of reliable agnostic learning proposed by Kalai et al. (2012). As in the case of agnostic learning, the data comes from an arbitrary joint distribution D over X\u00d7{\u22121, 1}.", "startOffset": 72, "endOffset": 92}, {"referenceID": 14, "context": "Definition 3 (Positive Reliable Learning (Kalai et al., 2012)).", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "Definition 4 (Negative Reliable Learning (Kalai et al., 2012)).", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "Definition 5 (Fully Reliable Learning (Kalai et al., 2012)).", "startOffset": 38, "endOffset": 58}, {"referenceID": 14, "context": "Theorem 1 ((Kalai et al., 2012)).", "startOffset": 11, "endOffset": 31}, {"referenceID": 4, "context": "The definitions as they are presented here essentially appeared in prior work of Bun and Thaler (2013a) (see also Sherstov (2014)), who only required the notion we refer to as positive one-sided approximate degree.", "startOffset": 81, "endOffset": 104}, {"referenceID": 4, "context": "The definitions as they are presented here essentially appeared in prior work of Bun and Thaler (2013a) (see also Sherstov (2014)), who only required the notion we refer to as positive one-sided approximate degree.", "startOffset": 81, "endOffset": 130}, {"referenceID": 2, "context": "Bartlett and Mendelson (2002) proved the following result: Theorem 2 ((Bartlett and Mendelson, 2002)).", "startOffset": 70, "endOffset": 100}, {"referenceID": 2, "context": "Bartlett and Mendelson (2002) proved the following result: Theorem 2 ((Bartlett and Mendelson, 2002)).", "startOffset": 0, "endOffset": 30}, {"referenceID": 12, "context": "(2008) proved the following result: Theorem 3 ((Kakade et al., 2008)).", "startOffset": 47, "endOffset": 68}, {"referenceID": 12, "context": "Kakade et al. (2008) proved the following result: Theorem 3 ((Kakade et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Our algorithm extends the L1-regression technique of Kalai et al. (2005) for agnostic learning, but we require a more detailed analysis.", "startOffset": 53, "endOffset": 73}, {"referenceID": 13, "context": "This program is similar to one used in the L1-regression algorithm for agnostic learning introduced by Kalai et al. (2005). The variables of the program are the \u2211d j=0 (n j ) = O(nd) coefficients of the polynomial p(x).", "startOffset": 103, "endOffset": 123}, {"referenceID": 9, "context": ", (Feldman et al., 2013)); we state this as a theorem for completeness.", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": ", (Feldman et al., 2013)); we state this as a theorem for completeness. Instead of the mathematical program described in the proof of Theorem 5, to obtain Theorem 6, the L1regression algorithm of Kalai et al. (2005) is directly applied, with the added constraint that the weight of the approximating polynomial is at most W .", "startOffset": 3, "endOffset": 216}, {"referenceID": 3, "context": "Buhrman et al. (1999)), the \u00d5( \u221a n log (1/\u01eb)) upper bound on d\u0303eg+,\u01eb(MAJ) is easily seen to be tight up to factors hidden by the \u00d5 notation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "the standard texts of Cheney (1982) and Rivlin (1981)).", "startOffset": 22, "endOffset": 36}, {"referenceID": 6, "context": "the standard texts of Cheney (1982) and Rivlin (1981)).", "startOffset": 22, "endOffset": 54}, {"referenceID": 13, "context": "1 Standard Agnostic Learning of Conjunctions Kalai et al. (2005) showed how to use L1-regression to agnostically learn conjunctions on n variables in time 2 \u221a n .", "startOffset": 45, "endOffset": 65}, {"referenceID": 18, "context": "In fact, this question is already well-understood in the case of constant \u01eb: letting ANDn denote the AND function on n variables, Servedio et al. (2012) implicitly showed that for any \u221a n < d and any \u01eb = \u0398(1), there exists an \u01eb-approximating polynomial for the ANDn function of degree d and weight poly(n)\u00b72\u00d5(n/d).", "startOffset": 130, "endOffset": 153}, {"referenceID": 4, "context": "In fact, this construction is essentially optimal, matching a lower bound for constant \u01eb proved in the same paper (see also (Bun and Thaler, 2013a, Lemma 20)). We now extend the ideas of Servedio et al. (2012) to handle subconstant values of \u01eb.", "startOffset": 125, "endOffset": 210}, {"referenceID": 13, "context": "2, the reductions of Kalai et al. (2012), combined with the agnostic learning algorithm for conjunctions due to Kalai et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 13, "context": "2, the reductions of Kalai et al. (2012), combined with the agnostic learning algorithm for conjunctions due to Kalai et al. (2005), imply that DNFs can be positive reliably learned in time 2(\u00d5( \u221a n)).", "startOffset": 21, "endOffset": 132}, {"referenceID": 21, "context": "Sherstov (2013b) proved that there exists a halfspace h such that deg\u00b1(g) = \u03a9(n).", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Bun and Thaler (2013a), extending a seminal lower bound of Aaronson and Shi (2004), showed that there is a polynomial-sized DNF f (more specifically, f is the negation of the ELEMENT DISTINCTNESS function) satisfying d\u0303eg\u2212(f) = \u03a9((n/ log n) 2/3); thus, our techniques cannot negative reliably learn polynomial-sized DNFs in time better than exp ( \u00d5 ( n2/3 )) .", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bun and Thaler (2013a), extending a seminal lower bound of Aaronson and Shi (2004), showed that there is a polynomial-sized DNF f (more specifically, f is the negation of the ELEMENT DISTINCTNESS function) satisfying d\u0303eg\u2212(f) = \u03a9((n/ log n) 2/3); thus, our techniques cannot negative reliably learn polynomial-sized DNFs in time better than exp ( \u00d5 ( n2/3 )) .", "startOffset": 59, "endOffset": 83}, {"referenceID": 4, "context": "The notion of one-sided polynomial approximation has only been introduced very recently (Bun and Thaler (2013a)), and previously had only been used to prove lower bounds.", "startOffset": 89, "endOffset": 112}, {"referenceID": 4, "context": "The notion of one-sided polynomial approximation has only been introduced very recently (Bun and Thaler (2013a)), and previously had only been used to prove lower bounds. By giving the first algorithmic application of one-sided polynomial approximations, our work lends further credence to the notion that these approximations are fundamental objects worthy of further study in their own right. Just as threshold degree and approximate degree have found applications (both positive and negative) in many domains outside of learning theory, we hope that one-sided approximate degree will as well. Identifying such applications is a significant direction for further work. Our work does raise several open questions specific to one-sided polynomial approximations. Here we highlight two. We have shown that halfspaces of weight at most W have one-sided approximate degree \u00d5(W 1/2), and yet there exist halfspaces with one-sided approximate degree \u03a9(n). However, the (nonexplicit) halfspace from Sherstov (2013b) that we used to demonstrate the \u03a9(n) lower bound has weight 2\u03a9(n).", "startOffset": 89, "endOffset": 1010}], "year": 2017, "abstractText": "We study several questions in the reliable agnostic learning framework of Kalai et al. (2009), which captures learning tasks in which one type of error is costlier than other types. A positive reliable classifier is one that makes no false positive errors. The goal in the positive reliable agnostic framework is to output a hypothesis with the following properties: (i) its false positive error rate is at most \u01eb, (ii) its false negative error rate is at most \u01eb more than that of the best positive reliable classifier from the class. A closely related notion is fully reliable agnostic learning, which considers partial classifiers that are allowed to predict \u201cunknown\u201d on some inputs. The best fully reliable partial classifier is one that makes no errors and minimizes the probability of predicting \u201cunknown\u201d, and the goal in fully reliable learning is to output a hypothesis that is almost as good as the best fully reliable partial classifier from a class. For distribution-independent learning, the best known algorithms for PAC learning typically utilize polynomial threshold representations, while the state of the art agnostic learning algorithms use pointwise polynomial approximations. We show that one-sided polynomial approximations, an intermediate notion between polynomial threshold representations and point-wise polynomial approximations, suffice for learning in the reliable agnostic settings. We then show that majorities can be fully reliably learned and disjunctions of majorities can be positive reliably learned, through constructions of appropriate onesided polynomial approximations. Our fully reliable algorithm for majorities provides the first evidence that fully reliable learning may be strictly easier than agnostic learning. Our algorithms also satisfy strong attribute-efficiency properties, and in many cases they provide smooth tradeoffs between sample complexity and running time. University of California, Berkeley. Email: vkanade@eecs.berkeley.edu The Simons Institute for the Theory of Computing at UC Berkeley. Email: jthaler@seas.harvard.edu", "creator": "LaTeX with hyperref package"}}}