{"id": "1402.5380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2014", "title": "Godseed: Benevolent or Malevolent?", "abstract": "It is known that benign looking AI objectives may result in powerful AI drives that may pose a risk to the human society. We examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing AI agents. We follow a design approach that tries to exclude malevolent motivations from AI's, however, we see that even objectives that seem benevolent at first may pose significant risk to humanity. We also discuss various solution approaches including selfless goals, hybrid designs, universal constraints, and generalization of robot laws.", "histories": [["v1", "Sat, 1 Feb 2014 17:35:53 GMT  (12kb)", "https://arxiv.org/abs/1402.5380v1", null], ["v2", "Mon, 10 Oct 2016 21:43:51 GMT  (22kb)", "http://arxiv.org/abs/1402.5380v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eray \\\"ozkural"], "accepted": false, "id": "1402.5380"}, "pdf": {"name": "1402.5380.pdf", "metadata": {"source": "CRF", "title": "Godseed: Benevolent or Malevolent?", "authors": ["Eray \u00d6zkural"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 2.53 80v2 [cs.AI] 10 Oct 201 6"}, {"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "3 Is the concept of malevolence universal?", "text": "This year, the time has come for us to find a solution that is capable, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "3.1 Meta-Rules for God-level Autonomous Artificial Intelligence", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "4 Selfish vs. Selfless", "text": "It could be argued that some of the problems of given meta-rules could be avoided by shifting the benefit from selfish to selfless. For example, the surviving artificial intelligence could be modified to seek the maximum survival of all, so it would try to bring peace to the galaxies. Capitalist artificial intelligence could be modified to ensure that the prosperity of all increases, or perhaps is balanced, gets a fair share. Control freak artificial intelligence could be transformed into a Nietzsche artificial intelligence, which would increase the number of deliberate individuals. As such, some obviously catastrophic consequences can be prevented, and almost always an unselfish goal is better. For example, maximizing wisdom: if it tries to gather wisdom in its galaxy-sized scientific intellect, it may have undesirable side effects, but if it tries to build a fair society of trans-capable people with a non-destructive and non-totalitarian goal, it could then be collectively useful in the long run."}, {"heading": "5 Hybrid Meta-rules and Cybernetic Darwinism", "text": "We have many instincts and emotions; we have predetermined desires and fears, hunger and compassion, pride and love, shame and remorse to accomplish the myriad tasks that will prolong the human species. However, this multi-species fitness function is a result of Darwinian evolution with red claws and sharp teeth. Such an evolution could eventually stabilize itself with very advanced and excellent cybernetic life forms, or it could not. However, such Darwinian systems would have an advantage: they would not stick to a meta goal. To prevent this apparent obsession, one strategy could be to give several coherent goals to AI, goals that are not so much conflicting as balancing their behavior."}, {"heading": "6 Universal Constraints and Semi-Autonomous AI", "text": "The easiest way to ensure that no AI agent is ever excluded from much control is to add limitations to the optimization problems that AI solves in the real world. For example, because the scientific deities are quite dangerous, they may be limited to operating in a particular space-time region, physically and precisely designated. Such physical limitations give the agent a kind of mortality that modifies the behavior of many universal actors. If such universal limitations are given, the AGI agent will have only a limited budget of physical resources, i.e., space / time, and energy to make major changes to the entire environment."}, {"heading": "7 Scenarios for semi-autonomous AGI agents", "text": "There are many beneficial ways in which we can employ a semi-autonomous agent. Autonomy is absolutely helpful for space exploration, and I have suggested sending trans-apient AGI probes to search for life in exoplanets. [8] We could use semi-autonomy to first explore Mars and the solar system, there are several important applications for this, including prospecting for water and minerals, mining, construction, agriculture, maintenance and so on, which can help solve space problems and exploration. Entire industries and traditional government functions can be replaced by AGI agents. One AGI system can take care of producing and maintaining enough power for humans; another could take care of obtaining clean water and irrigation; another system could produce large amounts of reliable, healthy food for millions of people."}, {"heading": "8 Conclusion and Future Work", "text": "In fact, it is that we are able to assert ourselves, that we are able to be able to be in a position, that we are able to be in a position, and that we are able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}], "references": [{"title": "GOLEM: towards an AGI meta-architecture enabling both goal preservation and radical self-improvement", "author": ["Ben Goertzel"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Superintelligence: Fears, promises and potentials", "author": ["Ben Goertzel"], "venue": "Journal of Evolution and Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind", "author": ["Marvin Minsky"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gene expression dynamics in the macrophage exhibit criticality", "author": ["Matti Nykter", "Nathan D. Price", "Maximino Aldana", "Stephen A. Ramsey", "Stuart A. Kauffman", "Leroy E. Hood", "Olli Yli-Harja", "Ilya Shmulevich"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1897}, {"title": "The basic ai drives", "author": ["Stephen M. Omohundro"], "venue": "AGI, volume 171 of Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Universal knowledge-seeking agents", "author": ["Laurent Orseau"], "venue": "editors, ALT,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Artificial intelligence and brain simulation probes for interstellar expeditions", "author": ["Eray Ozkural"], "venue": "Year Starship Symposium Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Artificial General Intelligence - 4th International Conference, AGI", "author": ["J\u00fcrgen Schmidhuber", "Kristinn R. Th\u00f3risson", "Moshe Looks", "editors"], "venue": "Mountain View, CA, USA, August 3-6,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Coherent extrapolated volition", "author": ["Eliezer Yudkowsky"], "venue": "Technical report, Singularity Institute for Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}], "referenceMentions": [{"referenceID": 8, "context": "If human society degenerated as a whole, would this mean that all resources would be spent on petty pursuits? If a coherent extrapolated volition [11] were realized with an AGI agent, would this set our sights on exploring other star systems, or spending our resources on such unessential trivialities as luxury homes and sports cars? Would the humans at one point feel that they have had enough and order the AGI to dismantle itself? The human society is governed mostly by the irrational instincts of apes trapped in a complex technological life, and unfortunately not always with clear goals; will it ever be possible to refine our culture so that only significant ideas take the lead? That sounds more like a debate of social theory, than AGI design.", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "AI eschatology literature mainly blows the conclusions of Omohundro\u2019s philosophical article [5] out of proportion which argues for AI drives that will result from specifying a benign looking goal, such as maximizing paperclips in the world.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Autonomous robots with beneficial goal systems have been discussed by Ben Goertzel [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "However, even if we assumed there were no bottlenecks (and according to my projections that would mean a singularity by 2035 [8]), the theory concerns the whole globe, not a small subset of it.", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Goertzel reviews the problems in the AI eschatology folklore in a lucid paper that distills the problem with the eschatological stance to its essence: that it is an informal rather than a scientific argument [2].", "startOffset": 208, "endOffset": 211}, {"referenceID": 4, "context": "Previously, Omohundro identified basic AI drives in reinforcement learning agents with open ended benign looking AI objectives [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "Orseau discusses the construction of such advanced AGI agents, in particular knowledge seeking agents[6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": ", [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "A minimal model of a reinforcement learning agent that maximizes its knowledge may be found in [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Marvin Minsky hypothesized in his last book The Emotion Machine that attachment learning plays a key role in the cognitive development of higher intelligence [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "For space exploration, autonomy is absolutely helpful, and I have proposed sending trans-sapient AGI equipped probes to look for life in exoplanets [8].", "startOffset": 148, "endOffset": 151}], "year": 2016, "abstractText": "It is hypothesized by some thinkers that benign looking AI objectives may result in powerful AI drives that may pose an existential risk to human society. We analyze this scenario and find the underlying assumptions to be unlikely, as well as the premises of the argument. We argue that the AI eschatology stance is not scientifically plausible, more intelligence helps avoiding accidents and learning about ethics, and we also argue for the rights of brain simulations. We may still conceive of logical use cases for autonomy. We examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing AI agents. We follow a design approach that tries to exclude malevolent motivations from AI agents, however, we see that objectives that seem benevolent may pose significant risk. We consider the following meta-rules: preserve and pervade life and culture, maximize the number of free minds, maximize intelligence, maximize wisdom, maximize energy production, behave like human, seek pleasure, accelerate evolution, survive, maximize control, and maximize capital. We also discuss various solution approaches for benevolent behavior including selfless goals, hybrid designs, Darwinism, universal constraints, semi-autonomy, and generalization of robot laws. A \u201cprime directive\u201d for AI may help in formulating an encompassing constraint for avoiding malicious behavior. We hypothesize that social instincts for autonomous robots may be effective such as attachment learning. We mention multiple beneficial scenarios for an advanced semi-autonomous AGI agent in the near future including space exploration, automation of industries, state functions, and cities. We conclude that a beneficial AI agent with intelligence beyond human-level is possible and has many practical use cases.", "creator": "LaTeX with hyperref package"}}}