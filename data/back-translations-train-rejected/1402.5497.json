{"id": "1402.5497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2014", "title": "Efficient Semidefinite Spectral Clustering via Lagrange Duality", "abstract": "We propose an efficient approach to semidefinite spectral clustering (SSC), which addresses the Frobenius normalization with the positive semidefinite (p.s.d.) constraint for spectral clustering. Compared with the original Frobenius norm approximation based algorithm, the proposed algorithm can more accurately find the closest doubly stochastic approximation to the affinity matrix by considering the p.s.d. constraint. In this paper, SSC is formulated as a semidefinite programming (SDP) problem. In order to solve the high computational complexity of SDP, we present a dual algorithm based on the Lagrange dual formalization. Two versions of the proposed algorithm are proffered: one with less memory usage and the other with faster convergence rate. The proposed algorithm has much lower time complexity than that of the standard interior-point based SDP solvers. Experimental results on both UCI data sets and real-world image data sets demonstrate that 1) compared with the state-of-the-art spectral clustering methods, the proposed algorithm achieves better clustering performance; and 2) our algorithm is much more efficient and can solve larger-scale SSC problems than those standard interior-point SDP solvers.", "histories": [["v1", "Sat, 22 Feb 2014 09:39:52 GMT  (2342kb,D)", "http://arxiv.org/abs/1402.5497v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yan yan", "chunhua shen", "hanzi wang"], "accepted": false, "id": "1402.5497"}, "pdf": {"name": "1402.5497.pdf", "metadata": {"source": "CRF", "title": "Efficient Semidefinite Spectral Clustering via Lagrange Duality", "authors": ["Yan Yan", "Chunhua Shen", "Hanzi Wang"], "emails": ["yanyan@xmu.edu.cn)", "chunhua.shen@adelaide.edu.au)", "hanzi.wang@xmu.edu.cn)"], "sections": [{"heading": null, "text": "In fact, most people who fight for the rights of women and men are fighting for equality between men and women."}, {"heading": "II. NORMALIZATION OF THE AFFINITY MATRIX", "text": "In this section, we briefly present the connection between kernel k means and spectral clusters (18), [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering. We begin with the introduction of the notation ai = > used in this paper. We refer to the space of the real matrices as S. Similarly, we interpret the space of the M \u00b7 M symmetric matrices of SM and positive semidefinitive matrices of SM +. For a matrix X, the following statements are equivalent: (1) X < 0 (X) All eigenvalues of X are not negative (X)."}, {"heading": "III. SEMIDEFINITE SPECTRAL CLUSTERING", "text": "In this section we present an efficient algorithm with two versions, which aims at the effective normalization of the affinity matrix for semi-defined spectral clustering."}, {"heading": "A. Frobenius Normalization with the P.S.D. Constraint", "text": "Empirical studies [18], [19] have shown that the normalization of affinity matrix K has significant effects on the final cluster results. Compared to L1 normalization and relative entropy normalization, Frobenius normalization has proven to be very practical and can significantly increase cluster performance. In fact, it is natural to find a double stochastic approach that meets the limitations (5) on K under the Frobenius norm, which is the extension of the common euclidean vector norm. \u00b7 A simple derivation leads to F being a p.s.d. matrix (i.e. F < 0), since F = WW > and.Rn, \u00b5 > F\u00b5 > WW > p > p > p = p = p = p.s = p.s matrix is an explosive matrix."}, {"heading": "B. Semidefinite Spectral Clustering via Lagrange Duality", "text": "The Lagrange duality accepts the condition that there will be a \"double problem\": < Q = > Q = > Q = > Q = > Q = > Q = [1] Q = [2] Q = [3] K < K [3] K + [4] K + [4] K + 3 (5) K \u2212 4 (5) K \u2212 4 (5) K \u2212 4 (5) K \u2212 5 (5) K \u2212 4 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) S (5).4 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5) K \u2212 5 (5 (5) K \u2212 5) K \u2212 5 (5 (5"}, {"heading": "C. Discussions", "text": "There are a few important issues on the proposed LD-SSC1 and LD-SSC2 algorithms. \u2022 First, LD-SSC and the original Frobenius normalizing spectral clustering [19] are intrinsically different in the formulated optimization problems and therefore the solutions are different. On the one hand, the optimization problem in [19] is formulated as a square programming problem. On the other hand, our formulation is an SDP problem. Compared to the work in [19], which only tries to find a cabinet of double stochastic matrix, the proposed LD-SSC emphasizes the importance of the p.s.d. property of the normalization matrix, which makes the input affinity matrix more precise."}, {"heading": "IV. EXPERIMENTS", "text": "In order to evaluate the proposed LD-SSC algorithm (two versions: LD-SSC1 and LD-SSC2), we perform a series of cluster experiments across common data sets. The following subsections describe the details of the experiments and results."}, {"heading": "A. Data Sets", "text": "We use several well-studied datasets from the UCI Machine Learning Repository1 (including SPECTF Heart, Wine, Pima, Hayes-Roth, Iris, and BUPA), cancer datasets 2 (including leukemia and lung), two public facial datasets (including 1UCI Repository: http: / / www.ics.uci.edu / \u0445 mlearn / MLRepository.html 2Cancer Data Sets: http: / / datam.i2r.a-star.edu.sg / datasets / krbd / ORL face database3 and Yale face database4), and two object image datasets (including COIL-20 [32] 5 and the handwritten binary alphadigits set6), respectively. The UCI Repository is well established and widely used for benchmarking various cluster algorithms. The cancer variation datasets are the demanding yardsticks in the cancer community. The last four datasets are the alphabet and the alphabet expressions commonly used in the alphabet 6."}, {"heading": "B. Parameter Settings and Evaluation Metric", "text": "In this section, we evaluate the multi-class spectral clustering function described in [31], which iteratively solves a discrete solution by using an alternating optimization method that incorporates the most important eigenvectors. Other methods (such as [2]) can also be used, but these methods yield similar results [19]. Therefore, we use the framework of [31] while replacing different normalization algorithms in our experiments. Note that the results of all clustering algorithms depend on initialization to reduce statistical variations, we repeat all clustering algorithms 10 times with random initialization and report the results that correspond to the best objective values (similar to [31]). Two types of kernels used to construct the affinity matrix are the Gaussian kernel and the polynomial function, which is the similarity function for the Gaussian class."}, {"heading": "C. Comparisons with State-of-the-art Algorithms", "text": "We perform a comparison between the proposed LD-SSC1, LD-SSC2 and the least error rate achieved with three different states of the way of normalization, including: 1) Normalizedcut (NC), 2), 3), 4), 5), 5), 6), 7), 7, 7, 7, \"7,\" 7, \"7,\" 8, \"8,\" 8, \"9,\" 9, \"9,\" 9, \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \"\" \",\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\", \"\", \",\", \"\" \",\", \"\" \",\""}, {"heading": "D. Computational Complexity", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "V. CONCLUSION AND DISCUSSION", "text": "The normalization of the affinity matrix is a critical factor for spectral clustering. Existing normalization algorithms can be considered a double stochastic approach to the affinity matrix under different error scales. In this paper, an efficient and scalable normalization algorithm is presented with two versions (i.e. LD-SSC1 and LD-SSC2) for semidefinitive spectral clustering. Both versions are equivalent to the SSC problem, but differ only in their optimization step, where LD-SSC1 requires less memory usage, while LDSSC2 has a faster convergence rate. We show that having the double stochastic restriction and the p.s.d. restriction during the normalization steps is more desirable than having the standard SDP solvers."}], "references": [{"title": "Data clustering: a review", "author": ["A.K. Jain", "M. Murty", "P.J. Flynn"], "venue": "ACM Comput. Surv., vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "On spectral clustering: analysis and algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2001, pp. 849\u2013856.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Partitioning sparse matrices with eigenvectors of graph", "author": ["A. Pothen", "H.D. Simon", "K.P. Liou"], "venue": "SIAM Journal of Matrix Anal. Appl., vol. 11, pp. 430\u2013452, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "New spectral methods for ratio cut partioning and clustering", "author": ["L. Hagen", "A. Kahng"], "venue": "IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 11, no. 9. pp. 1074\u20131085, 1992.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["P.K. Chan", "M.D.F. Schlag", "J.Y. Zien"], "venue": "IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 13, no. 9. pp. 1088\u20131096, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Spectral graph theory", "author": ["F. Chung"], "venue": "AMS publication, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "A min-max cut algorithm for graph partitioning and data clustering", "author": ["C. Ding", "X. He", "H. Zha", "M. Gu", "H. Simon"], "venue": "Proc. IEEE Int. Conf. Data Mining, Washington, D.C., USA, 2001, pp. 107\u2013114.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning spectral clustering with application to speech separation", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Research, vol. 7, pp. 1963\u20132001, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1963}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2002, pp. 585\u2013591.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F.P. Nie", "S.C. Yan", "Y.T. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132772, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A tutorial on spectral clustering", "author": ["C. Ding"], "venue": "Talk presented at ICML 2004. (slides available at http://ranger.uta.edu/ chqding/Spectral/)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Improved minmax cut graph clustering with nonnegative relaxation", "author": ["F. Nie", "C. Ding", "D. Luo", "H. Huang"], "venue": "Proc. European Conf. on Mach. Learn. / Principles and Pract. of Know. Discov. in Databases, Barcelona, Spain, 2010, pp. 451\u2013466", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral embedded clustering: a framework for in-sample and out-of-sample spectral clustering", "author": ["F. Nie", "Z. Zeng", "I.W. Tsang", "D. Xu", "C. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 11, pp. 1796-1808, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1808}, {"title": "Consensus spectral clustering in nearlinear time", "author": ["D. Luo", "C. Ding", "H. Huang"], "venue": "Proc. IEEE Conf. Data Eng., Hannover, Germany, pp. 1079-1090, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Forging the graphs: a low rank and positive semidefinite graph learning approach", "author": ["D. Luo", "C. Ding", "H. Huang", "F. Nie"], "venue": "Proc. Adv. Neural Inf. Process. Syst., Lake Tahoe, Nevada, USA, 2012, pp. 2969\u20132977,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.Y. Chen", "Y. Song", "H. Bai", "C.J. Lin", "E.Y. Chang"], "venue": "IEEE Pattern Anal. Mach. Intell., vol. 33, no. 3, pp. 568\u2013586, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "A unifying approach to hard and probabilistic clustering", "author": ["R. Zass", "A. Shashua"], "venue": "Proc. IEEE Int. Conf. Comp. Vis., Beijing, China, 2005, pp. 294\u2013301.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Doubly stochastic normalization for spectral clustering", "author": ["R. Zass", "A. Shashua"], "venue": "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, 2006, pp. 1569\u20131576.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust multi-class transductive learning with graphs", "author": ["W. Liu", "S. Chang"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,Miami, Florida, USA, 2009, pp. 381\u2013388.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "J. Nocedal"], "venue": "ACM Trans. Mathematical Software, vol. 23, no. 4, pp. 550\u2013560, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Kernel k-means, spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Proc. ACM Int. Conf. Knowledge & Discovery Data Mining, Seattle, USA, 2004, pp. 555\u2013556.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Concerning non-negative matrices and doubly stochastic matrices", "author": ["R. Sinkhorn", "P. Knopp"], "venue": "Pacific J. Math., vol. 21, pp. 343\u2013348, 1967.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1967}, {"title": "A convex programming procedure", "author": ["D D\u2019Esopo."], "venue": "Naval Logistics Quarterly, vol. 6, no. 1, pp. 33\u201342, 1959.  YAN et al.: EFFICIENT SEMIDEFINITE SPECTRAL CLUSTERING VIA LAGRANGE DUALITY  13", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1959}, {"title": "Functional Operators Vol. II", "author": ["J. Von Neumann"], "venue": "Princeton University Press, 1950.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1950}, {"title": "On semidefinite relaxation for normalized k-cut and connections to spectral clustering", "author": ["E.P. Xing", "M.I. Jordan"], "venue": "Division of Computer Science, University of California, Berkeley, Technical Report CSD-03- 1265, 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph Laplacian regularization for large-scale semidefinite programming", "author": ["K.Q. Weinberger", "F. Sha", "Q. Zhu", "L. Saul"], "venue": "Proc. Adv. Neural Inf. Process. Syst., Vancouver, B. C., Canada, pp. 1489\u20131496, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "A distributed SDP approach for largescale noisy anchor-free graph realization with applications to molecular conformation", "author": ["P. Biswas", "K. Toh", "Y. Ye"], "venue": "SIAM Journal on Scien. Comput., vol. 30, no. 3, pp. 1251\u20131277, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiclass spectral clsutering", "author": ["S.X. Yu", "J. Shi"], "venue": "Proc. IEEE Int. Conf. Comp. Vis., Pittsburgh, PA, USA, 2003, pp. 313\u2013319.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Columbia object image library (COIL-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Columbia Univ., Technical Report CUCS-006-96, Feb. 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Document clustering using locality preserving indexing", "author": ["D. Cai", "X.F. He", "J. Han"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 17, no. 12, pp. 1624\u20131637, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "J. Cogn. Neurosci., vol. 3, no. 1, pp. 71\u201386, 1991.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1991}, {"title": "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hepanha", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1997}, {"title": "Document retrieval and clustering: from principal component analysis to self-aggregation networks", "author": ["C. Ding"], "venue": "Proc. Int. Workshop on Artificial Intelligence and Statistics, Key West, Florida, 2003.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "CVX: MATLAB software for disciplined convex programming. Version 1.22", "author": ["M. Grant", "S. Boyd"], "venue": "http://cvxr.com/cvx/", "citeRegEx": "37", "shortCiteRegEx": null, "year": 0}, {"title": "Computational models of perceptual organization", "author": ["S.X. Yu"], "venue": "Ph.D. thesis, Carnegie Mellon University, May 2003.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION CLUSTERING is one of the most popular techniques for statistical data analysis with various applications, including image analysis, pattern recognition, machine learning, and information retrieval [1].", "startOffset": 210, "endOffset": 213}, {"referenceID": 0, "context": "Numerous clustering algorithms have been developed in the literature [1], such as k-means, single linkage, and fuzzy clustering.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "In recent years, spectral clustering [2]\u2013[16], a class of clustering algorithms based on the spectrum analysis of the affinity matrix, has emerged as an effective clustering technique.", "startOffset": 37, "endOffset": 40}, {"referenceID": 15, "context": "In recent years, spectral clustering [2]\u2013[16], a class of clustering algorithms based on the spectrum analysis of the affinity matrix, has emerged as an effective clustering technique.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "Compared with the traditional algorithms [1], such as k-means or single linkage, spectral clustering has many fundamental advantages.", "startOffset": 41, "endOffset": 44}, {"referenceID": 16, "context": "For example, it is easy to implement and reasonably fast, especially for large sparse matrices [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "Therefore, the three critical factors that affect the final performance of spectral clustering are [18], [19]: 1) the construction of the affinity matrix, 2) the normalization of the affinity matrix, and 3) the simple clustering algorithm, as shown in Fig.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Therefore, the three critical factors that affect the final performance of spectral clustering are [18], [19]: 1) the construction of the affinity matrix, 2) the normalization of the affinity matrix, and 3) the simple clustering algorithm, as shown in Fig.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "There are several popular ways [12] to construct the affinity matrix, such as the k-nearest neighbor graph and the fully connected graph.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The normalization of the affinity matrix is achieved by finding the closest doubly stochastic matrix to the affinity matrix under a certain error measure [18]\u2013[20], while the simple clustering algorithm (e.", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "The normalization of the affinity matrix is achieved by finding the closest doubly stochastic matrix to the affinity matrix under a certain error measure [18]\u2013[20], while the simple clustering algorithm (e.", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "Empirical studies [19] indicate that the first two critical factors have a greater impact on the final clustering performance compared with the third critical factor (i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "We briefly review some related work [3]\u2013[7] before presenting our work.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "We briefly review some related work [3]\u2013[7] before presenting our work.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "Given a similarity graph with the affinity matrix, the simplest way to construct a partition of the graph is to solve the mincut problem [3], which aims to minimize the weights of edges (i.", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Ratio-cut [4], [5] and Normalized-cut [7] are the two most common algorithms.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "In Ratio-cut [4], [5], the size of the subgraph is measured by the number of vertices, whereas, the size is measured by the weight of the edges attached to a subgraph in Normalized-cut.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "In Ratio-cut [4], [5], the size of the subgraph is measured by the number of vertices, whereas, the size is measured by the weight of the edges attached to a subgraph in Normalized-cut.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]\u2013[10].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]\u2013[10].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "An effective approach is to consider the continuous relaxation versions of these problems [2], [7]\u2013[10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "Minmax-cut was proposed in [8] and showed more balanced partitions than Normalized-cut and Ratio-cut.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "[13] applied an additional nonnegative constraint into Minmax-cut to obtain more accurate clustering results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Recently, a spectral embedding clustering framework [14] was developed to incorporate the linear property of the cluster assignment matrix.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "In [18], [19], it has been shown that the key difference between Ratio-cut and Normalized-cut is the error measure used to find the closest doubly stochastic approximation of the input affinity matrix during the normalization step.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [18], [19], it has been shown that the key difference between Ratio-cut and Normalized-cut is the error measure used to find the closest doubly stochastic approximation of the input affinity matrix during the normalization step.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "[19] developed a scheme for finding the optimal doubly stochastic matrix under the Frobenius norm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Experimental results [19] have demonstrated that the Frobenius normalization based spectral clustering achieves better performance on various standard data sets than the traditional normalization based algorithms, such as the L1 normalization and the relative entropy normalization based spectral clustering methods.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": ", L-BFGSB [22]) to solve the SSC problem in a simple manner.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "In this section, we briefly introduce the connection between kernel k-means and spectral clustering [18], [19], [23] and reveal the role of the normalization of the affinity matrix for spectral clustering.", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "However, a major disadvantage of the k-means algorithm is that it can only find linearly-separable clusters in the input space [23].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Since Ki,j = \u03ba(ai,aj), (2) can be converted into the following matrix form [18]:", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Based on (3), F satisfies the following constraints [24]: F \u2265 0,F1 = 1,F = F>.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "In [18], it has been proved that for any non-negative symmetric matrix K, the iterative process K = D\u22121/2K(t)D\u22121/2 with D = diag(K1) converges to a doubly stochastic matrix under the relative entropy measure (using the symmetric version of the iterative proportional fitting procedure [24]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [18], it has been proved that for any non-negative symmetric matrix K, the iterative process K = D\u22121/2K(t)D\u22121/2 with D = diag(K1) converges to a doubly stochastic matrix under the relative entropy measure (using the symmetric version of the iterative proportional fitting procedure [24]).", "startOffset": 285, "endOffset": 289}, {"referenceID": 18, "context": "[19] have shown that it is more natural to find the doubly stochastic matrix under the Frobenius error norm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Constraint Empirical studies [18], [19] have shown that the normalization of the affinity matrix K has significant effects on the final clustering results.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Constraint Empirical studies [18], [19] have shown that the normalization of the affinity matrix K has significant effects on the final clustering results.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "constraint is neglected during the normalization step in [19] due to the simplification of the computational complexity.", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "By taking the idea of the cyclic coordinate ascent technique [25] (which seeks for the optimum of the objective function by repeatedly optimizing each of the coordinate directions), we can efficiently solve (13).", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "L-BFGS-B [22]) since it does not have the matrix variables.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "To be specific, the number of iterations for the inner loop (L-BFGS-B [22] is employed in our case in Step 2) is 5\u223c10, while the number of iterations for the outer loop (Step 2 to Step 4) is 50\u223c100.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "\u2022 First, LD-SSC and the originial Frobenius normalization based spectral clustering [19] are intrinsically different in the formulated optimization problems and hence the solutions are different.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "On one hand, the optimization problem in [19] is formulated as a quadratic programming problem.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "Compared with the work in [19] that only tries to find a closet doubly stochastic matrix, the proposed LD-SSC emphasizes the importance of the p.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "On the other hand, in [19], the Von-Neumann\u2019s successive projections lemma [27] is applied to solve the quadratic problem.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "On the other hand, in [19], the Von-Neumann\u2019s successive projections lemma [27] is applied to solve the quadratic problem.", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "[28] proposed the semidefinite relaxation for k Normalized-cut.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Several methods [29], [30] have also been proposed to solve large-scale SDP problems.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Several methods [29], [30] have also been proposed to solve large-scale SDP problems.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "For example, in [29], matrix factorization is used to approximate a large-scale SDP problem with a smaller one.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "[16] developed a graph learning algorithm by solving a convex optimization problem with the low rank and p.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "An efficient algorithm based on augmented Lagrangian multiplier was proposed to attain the global optimum in [16], while LD-SSC takes advantages of the Lagrange duality property.", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "ORL face database3 and Yale face database4), and two object image data sets (including COIL-20 [32]5 and the handwritten binary Alphadigits data set6), respectively.", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Parameter Settings and Evaluation Metric In this section, we evaluate the multi-class spectral clustering described in [31] which iteratively solves a discrete solution by using an alternating optimization procedure taking the k principal eigenvectors.", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "Other methods (such as [2]) can also be used, but these methods give similar results [19].", "startOffset": 23, "endOffset": 26}, {"referenceID": 18, "context": "Other methods (such as [2]) can also be used, but these methods give similar results [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "Hence, we employ the framework of [31] while replacing different normalization algorithms in our experiments.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "To reduce statistical variation, we repeat all the clustering algorithms for 10 times with random initialization, and report the results corresponding to the best objective values (similar to [31]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 32, "context": "All the images in Yale are normalized to 128\u00d7128 [35].", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "In this paper, we use the principal component analysis (PCA) [34] to perform dimensionality reduction.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "The COIL-20 data set [32] has 1, 440 images of 20 object categories.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "Given that ri and si are the obtained cluster label and the ground truth label, respectively, the error rate is defined as follows [33]:", "startOffset": 131, "endOffset": 135}, {"referenceID": 18, "context": "We choose the lowest error rate [19] and the mean error rate across different kernel parameters (\u03b4 in the Gaussian kernel and d in the polynomial kernel) as the evaluation metric in our experiments.", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "Comparisons with State-of-the-art Algorithms We perform a comparison between the proposed LD-SSC1, LD-SSC2 and spectral clustering with three different stateof-the-art normalization algorithms, including: 1) Normalizedcut (NC) [7], [18] which is based on the relative entropy", "startOffset": 227, "endOffset": 230}, {"referenceID": 17, "context": "Comparisons with State-of-the-art Algorithms We perform a comparison between the proposed LD-SSC1, LD-SSC2 and spectral clustering with three different stateof-the-art normalization algorithms, including: 1) Normalizedcut (NC) [7], [18] which is based on the relative entropy", "startOffset": 232, "endOffset": 236}, {"referenceID": 3, "context": "normalization; 2) Ratio-cut (RC) [4] which is based on the L1 normalization; and 3) the Frobenius normalization based spectral clustering (FSC) [19].", "startOffset": 33, "endOffset": 36}, {"referenceID": 18, "context": "normalization; 2) Ratio-cut (RC) [4] which is based on the L1 normalization; and 3) the Frobenius normalization based spectral clustering (FSC) [19].", "startOffset": 144, "endOffset": 148}, {"referenceID": 32, "context": "4, it shows that LD-SSC1 and LD-SSC2 try to align all the points in a line so that the two classes are well separated, which is similar to the idea of the linear discriminant analysis (LDA) [35] for the two-classes case in the supervised learning.", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "The connections between different clusters are suppressed while the connections within the same clusters are enhanced, which has a similar effect as the aggregation network [36].", "startOffset": 173, "endOffset": 177}, {"referenceID": 34, "context": "To show the efficiency of the proposed algorithm, we also compare the computational time between our scalable LD-SSC algorithm (LD-SSC1 and LD-SSC2) and the SSC using CVX (CVX-SSC) [37], which is a standard package for convex optimization.", "startOffset": 181, "endOffset": 185}, {"referenceID": 28, "context": "The framework of [31] is used to perform different clustering algorithms for real image segmentation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "The pixel affinity matrix K is measured based on the maximum magnitude of edges across the line between two pixels [38].", "startOffset": 115, "endOffset": 119}], "year": 2014, "abstractText": "We propose an efficient approach to semidefinite spectral clustering (SSC), which addresses the Frobenius normalization with the positive semidefinite (p.s.d.) constraint for spectral clustering. Compared with the original Frobenius norm approximation based algorithm, the proposed algorithm can more accurately find the closest doubly stochastic approximation to the affinity matrix by considering the p.s.d. constraint. In this paper, SSC is formulated as a semidefinite programming (SDP) problem. In order to solve the high computational complexity of SDP, we present a dual algorithm based on the Lagrange dual formalization. Two versions of the proposed algorithm are proffered: one with less memory usage and the other with faster convergence rate. The proposed algorithm has much lower time complexity than that of the standard interior-point based SDP solvers. Experimental results on both UCI data sets and real-world image data sets demonstrate that 1) compared with the state-of-the-art spectral clustering methods, the proposed algorithm achieves better clustering performance; and 2) our algorithm is much more efficient and can solve larger-scale SSC problems than those standard interior-point SDP solvers.", "creator": "TeX"}}}