{"id": "1402.5634", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2014", "title": "To go deep or wide in learning?", "abstract": "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-layered supervised learning model, or learn the features directly using a deep (multi-layered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.", "histories": [["v1", "Sun, 23 Feb 2014 16:51:51 GMT  (28kb,D)", "http://arxiv.org/abs/1402.5634v1", "9 pages, 1 figure, Accepted for publication in Seventeenth International Conference on Artificial Intelligence and Statistics"]], "COMMENTS": "9 pages, 1 figure, Accepted for publication in Seventeenth International Conference on Artificial Intelligence and Statistics", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gaurav pandey", "ambedkar dukkipati"], "accepted": false, "id": "1402.5634"}, "pdf": {"name": "1402.5634.pdf", "metadata": {"source": "CRF", "title": "To go deep or wide in learning?", "authors": ["Gaurav Pandey"], "emails": [], "sections": [{"heading": null, "text": "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a two-tiered supervised learning model, or learn the characteristics directly from a deep (multi-tiered) model. While the first approach is very problematic, the second approach involves computational effort in multi-tiered learning and fine-tuning the model. In this paper, we propose an approach called wide learning, which relies on arc cosine cores learning a single layer of infinite width. We propose accurate and inaccurate learning strategies for broad learning and show that broad learning with one layer exceeds both single tiers and deep architectures of finite width for some benchmark datasets."}, {"heading": "1 INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "2 PRELIMINARIES AND BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Restricted Boltzmann Machine (RBM)", "text": "An RBM [Hinton, 2002] is a complete two-part Markov random field with a layer of visible units (x) and another layer of finite latent units (h). The visible units correspond to the characteristics of the observed sample, for example pixels in an image. Each visible unit is connected to each hidden unit by an edge. As the graph is split in two, the cliques of the model correspond to the edges and are size 2. The potential function of an edge (vi, hj) is given by \u2212 (wijvihj + bjhj), where wij, ai, 1 \u2264 i \u2264 d, 1 \u2264 j \u2264 K of the parameters of the model. The energy function, which is the sum of the potential function across all edges, is given by us (x, h) = \u2212 we wijvihj + aivi hj + aivi j j j (1) = \u2212 bTWh."}, {"heading": "2.2 Single Layer Threshold Networks And Corresponding Kernels", "text": "In deep learning, the weights of a multi-layer neural network (except those that connect the output units) are initialized using RBMs and fine-tuned using a back-propagation algorithm. Instead of learning the weights in advance using RBM, it is possible to randomly extract the weights from a fixed distribution and feed the results of the hidden units directly to a linear classifier such as SVM. Contrary to intuition, it has been observed that when weights are sampled from the normal standard distribution and the number of hidden units is much greater than the number of visible units, the resulting classifier performs well in many classification tasks [Huang et al., 2004]. In addition, performance improves with the number of hidden units. It is possible to tracably perform learning if the number of hidden units in a randomly weighted neural unit is x x."}, {"heading": "3 COVARIANCE ARC-COSINE KERNELS", "text": "Instead of scanning the entries of the matrix W from the standard normal distribution, if we scan the columns of a multivariate Gaussian distribution with zero mean and covariance \u03a3, we obtain a modified arc cosine core of formK\u03a3, n (x, y) = 1 (2\u03c0) d 2 | \u03a3 | 12 \u0432 w cosine kernel H (wTx) H (wTy) (wTx) n.... (wTy) n exp (\u2212 1w2) dw, which we call the covariance cosine kernel. If we apply a change in the variable u = 1 2w in the above equation, we get getK\u0440, n (x, y) = | 12 (2\u03c0) d 2 | 12 \u0445 u-Rd H (uT) normal (uT \u03a3 1 2x) Kill (Till) n, n (uT \u03a3 1 2x) n) n exp (2x b (b) b (b) = 2,0 b = 2,0 b (p)."}, {"heading": "4 WIDE LEARNING", "text": "It is well known [Bengio, 2009] that in the case of natural image fields, the characteristics learned from an RBM are Gabor-like, that is, they correspond to the output of a Gabor filter with a certain fixed frequency and orientation. Since the set of all possible frequencies and orientations has innumerable elements, an RBM tries to extract a subset of these frequencies / orientations that best capture the existing invariances in the data. On the other hand, a covariance arc cosine nucleus tries to find a distribution via the weight vectors that best capture the inventories in the data, allowing an infinite number of characteristics to be used for arbitrary data. This is also why we call distribution learning for arc cosine nuclei broad learning. In the rest of the paper, whenever we refer to an arcosine nucleus, we imply the nucleus with reflected linear units, that is, the nucleus corresponding to 1 (11)."}, {"heading": "4.1 Exact Wide Learning", "text": "In order to derive an algorithm for the formation of the covariance matrix (15), we have to rewrite the equation for rectified linear units into Equation (7) (13), where x0 and h0 are the initial values for the visible and hidden units and xp and hp are the values for the visible and hidden units after p iteration of Gibbs sampling. In our case, we assume that W t has an infinite number of columns from any distribution with covariance matrix, and we are interested in calculating the covariance matrix."}, {"heading": "4.2 Inexact Wide Learning", "text": "Instead of learning the covariance matrix from the data, one can use an RBM to learn the weight matrix W. Then, assuming that the columns of the weight matrix were taken from a multivariate Gaussian distribution with a mean of zero, one can estimate the covariance matrix W with maximum probability. The corresponding covariance matrix is given by \u03a3 = 1M WWWT, (18) where W is the weight matrix learned from the RBM and M is the number of columns in W. In our experiments, we found that training the covariance matrix with the first approach took much more time than training an RBM. Second, for exact training, we had to perform core calculations and matrix products, as mentioned in Equation (17), which made each iteration of the exact training much slower than the RBM iterations."}, {"heading": "5 DEEP-WIDE LEARNING", "text": "In deep learning, multiple RBMs are stacked on top of each other, so that the output of the previous layer RBM is fed as input into the next layer RBM. Thus, the weight matrix in the second layer is learned on the basis of the output of the first layer. Likewise, several covariance matrices of arc cosine cores can be stacked on top of each other. However, as already mentioned, the characteristic representation in the second layer has an infinite width. At first glance, it seems that exact learning of the covariance matrix in the second layer will have an infinite number of rows and columns. Therefore, when learning the covariance matrix in the second layer, one cannot directly use the equation (17). At first glance, it seems that exact learning of the covariance matrix in the second layer is not possible."}, {"heading": "5.1 Exact Deep-Wide Learning", "text": "Given the kernel matrix between the characteristic representation of the previous layer using the covariance matrix, i.e. the K + K matrix, we calculate the covariance matrix K + K matrix K + K matrix K + K matrix K + K matrix K + K matrix K + K matrix K + K matrix K (x, y) = K matrix (x, y) = K + K matrix (x, y) = K + K matrix (x, y) K + K matrix (x, y) K + K matrix K + K matrix (x, y). To calculate the covariance matrix matrix over the characteristics of the previous layer, we use the Equation (17). Let h (a) and h (b) be matrix) be matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix of the previous layer."}, {"heading": "5.2 Inexact Deep-Wide Learning", "text": "Since unattended feature learning only works when the number of instances is huge, this means that the kernel matrix will also be very huge. Therefore, learning with such a huge kernel matrix will be unfeasible in terms of both memory and processing time requirements. Therefore, we have tried imprecise approaches to extend the architecture to multiple layers. In the first approach, we learn an end-dimensional first layer using RBM. Next, we learn a covariance arc cosine kernel in addition to the first-level RBM activities mentioned in the previous section. However, we found that this approach resulted in a decrease in accuracy for all data sets we have tried. For example, for the MNIST number recognition task, the accuracy decreased from 99.05% to 97.85%. We have also tried to apply a subset of features to the kernel PCA for the covariance reduction of this detection method."}, {"heading": "6 DISCUSSION", "text": "In fact, if we are able to hide, we are able to hide, if we are able to hide."}, {"heading": "7 EXPERIMENTS", "text": "We tested the resulting covariance core for many datasets commonly used to compare deep learning architectures."}, {"heading": "7.1 MNIST", "text": "The MNIST classification is a margin of error of 0.95% after 0.20% after 0.20% after 0.20% after 0.20% after 0.20% after 0.20% after 0.20% after 0.25% after 0.25% after 0.25% after 0.25% after 0.25% after 0.25% after 0.25% after 0.25% after 0.50% after 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20acafter 1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1.50 \u20ac1."}, {"heading": "7.2 Tall and wide rectangles", "text": "This dataset [Larochelle et al., 2007] consists of rectangles of different widths and heights, the goal being to classify the rectangles into classes in which the rectangles in one class have more height than width, while the rectangles in the other class have more width than height. We trained an RBM on the pixel values and calculated the covariance matrix from the learned weight matrix W. This covariance matrix was used in the first layer in the covariant arc cosine core. For this dataset, we found that the use of a first layer of covariant arc cosine kernel, followed by several layers of arc cosine cores in which the covariance matrix is set to identity, resulted in improved performance."}, {"heading": "8 Conclusion", "text": "In this paper, we have proposed the term \"wide learning,\" which eliminates the fine-tuning common in deep architectures. We have developed precise and inaccurate methods for learning in such models. We have found that replacing a layer of finite width with a layer of infinite width leads to a drastic improvement in performance for the data sets under consideration. Furthermore, using a single layer significantly reduces the number of hyper parameters to be estimated, saving time in computationally expensive web searches. Further experiments with more complicated data sets, such as natural image fields, are required to test their suitability for general AI tasks."}], "references": [{"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["Bach et al", "F.R. 2004] Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "In Proceedings of the 21st International Conference on Machine learning,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Algorithms for hyper-parameter optimization", "author": ["Bergstra et al", "J. 2011] Bergstra", "R. Bardenet", "Y. Bengio", "B K\u00e9gl"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Blitzer et al", "J. 2005] Blitzer", "K.Q. Weinberger", "L.K. Saul"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Kernel descriptors for visual recognition", "author": ["Bo et al", "L. 2010] Bo", "X. Ren", "D. Fox"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Large-margin classification in infinite neural networks", "author": ["Cho", "Saul", "Y. 2010] Cho", "L.K. Saul"], "venue": "Neural Computation,", "citeRegEx": "Cho et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan et al", "D. 2012] Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Triggs", "N. 2005] Dalal", "B. Triggs"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Training invariant support vector machines", "author": ["Decoste", "Sch\u00f6lkopf", "D. 2002] Decoste", "B. Sch\u00f6lkopf"], "venue": "Machine Learning,", "citeRegEx": "Decoste et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Decoste et al\\.", "year": 2002}, {"title": "An introduction to restricted Boltzmann machines", "author": ["Fischer", "Igel", "A. 2012] Fischer", "C. Igel"], "venue": "In Progress in Pattern Recognition,", "citeRegEx": "Fischer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fischer et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "G.E. 2006] Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["Huang et al", "2004] Huang", "G.-B", "Zhu", "Q.-Y", "Siew", "C.-K"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Important gains from supervised finetuning of deep architectures on large labeled sets", "author": ["Lamblin", "Bengio", "P. 2010] Lamblin", "Y. Bengio"], "venue": "NIPS*", "citeRegEx": "Lamblin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamblin et al\\.", "year": 2010}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle et al", "H. 2007] Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the 24th International Conference on Ma-", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton", "V. 2010] Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan et al", "L. 2013] Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.", "creator": "LaTeX with hyperref package"}}}