{"id": "1402.5766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2014", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "abstract": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.", "histories": [["v1", "Mon, 24 Feb 2014 09:49:04 GMT  (263kb)", "http://arxiv.org/abs/1402.5766v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["adriana romero", "petia radeva", "carlo gatta"], "accepted": false, "id": "1402.5766"}, "pdf": {"name": "1402.5766.pdf", "metadata": {"source": "META", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "authors": ["Adriana Romero", "Petia Radeva"], "emails": ["ADRIANA.ROMERO@UB.EDU", "PETIA.IVANOVA@UB.EDU", "CGATTA@CVC.UAB.ES"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.57 66v1 [cs.LG] 2 4"}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. State-of-the-art", "text": "In recent years, the number of people who have settled in the city has multiplied in recent years, and the number of people who have settled in the city has doubled, the number of people who have settled in the city has doubled, the number of people who have settled in the city has doubled, the number of people who have settled in the city has doubled, the number of people who have settled in the city has doubled, the number of people who have settled in the city has doubled, the number of people who have settled in the city has increased, the number of people who have settled in the city has increased, and the number of people who have settled in the city has increased."}, {"heading": "3. Method", "text": "In this section, we describe how the proposed method learns a sparse feature representation of the data in terms of population and lifetime spareness, iteratively building an ideally sparse target and optimizing the dictionary by minimizing the error between the system output and the ideally sparse target, Section 3.1 highlights the algorithm to enforce lifetime and population spareness in the ideally sparse target, Section 3.2 provides implementation details on the system and optimization strategies used to minimize the error between the system output and the ideally sparse target."}, {"heading": "3.1. Enforcing Population and Lifetime Sparsity by defining an ideal target", "text": "We define population and lifetime economy as characteristics of an ideal sparse output. In view of N training samples and an output of dimensionality Nh, we define the first property of output as: 1. Strong lifetime economy: The output vectors must be composed exclusively of active and inactive units (however, no intermediate values between two fixed scalars are permitted) and all outputs must be activated for an equal number of inputs. Activation is exactly distributed among the Nh outputs. Our Strong lifetime economy is a stricter requirement than the high dispersion concept introduced in (Ngiam et al., 2011), as it only requires that \"the mean square activations of each feature (output) are roughly equal (outputs). While high dispersion attempts to diversify the learned basics, the output distribution of the outputs is not guaranteed (Ngiam et al., 2011), it only requires that the average square activations of each feature (s) be activated (s)."}, {"heading": "3.1.1. IDEAL TARGET GENERATION: THE ENFORCING POPULATION AND LIFETIME SPARSITY (EPLS) ALGORITHM", "text": "Suppose we have a system that generates a row output vector h. We use the notation hj to refer to an element of h. We define an output matrix H consisting of Nb output vectors of dimensionality Nh so that Nb \u2264 N. Similarly, we define an ideal target output matrix T of the same size. Algorithm 1 describes the EPLS algorithm to generate the ideal target T from H. For simplicity, each step of the algorithm in which the subscript j appears must be applied."}, {"heading": "3.2. System and Optimization strategies", "text": "Suppose we have a system with the activation function f that uses a data vector d as input and generates an output vector h = f (d). We use the same notation as in Section 3 and define a data matrix D consisting of N rows and Nd columns, where Nd is the input dimension. To compare our training strategy with previous known systems, we tested our algorithm using H = f (DW + b), (1) where f is a logistic nonlinearity."}, {"heading": "3.2.1. OPTIMIZATION STRATEGY", "text": "The system could be trained using a bar-derived mini-batch method (SGD) with adaptive learning rates such as the variance-based SGD (vSGD) (Schaul et al., 2013). Algorithm 2 details the latter training process. (line 1) The mini-batch size Nb can be set to any value in all the experiments we have set, Nb = Nh. (line 1), in any epoch we process the samples of the training collection (line 3), reset the EPLS inhibitor a to a flat activation (line 4) and process all mini-batches. (line 1) Samples D (b) are selected (line 6), the output H (b) is calculated (line 7) and the EPLS inhibitor a to a flat activation (line 4)."}, {"heading": "4. Experiments", "text": "The performance of training and encoding strategies in single-layer networks has been extensively analyzed in the literature (Coates et al., 2011; Coates & Ng, 2011; Ngiam et al., 2011) on STL-101 datasets. STL-10 dataset consists of 96x96 pixels of color images belonging to 10 different classes. The dataset is divided into a large, unlabeled training series with 100K images and smaller labeled training and test sets containing 5000 and 8000 images. It must be noted that in STL-10, the primary challenge is to make use of the unlabeled data (100K images) that is 100 times greater than the labeled data used to train the classifiers (1000 images per fold). In this case, the revised training must rely heavily on the ability of the unlabeled method to learn discriminatory features."}, {"heading": "5. Computational complexity", "text": "The EPLS algorithm requires the calculation of T associated with O (NNh) costs, and therefore scales linearly to both N and Nh. Since we can use vSGD for optimization, the method scales linearly to N given a specified number of epochs. Finally, the cost of calculating the derivative with Nd is linear, since we use a closed form for XI-E elements. Storage complexity is related to the minibatch size Nb. Consequently, the method can be gracefully scaled to very large datasets: in theory, the mini-batch input data D (b) (NbNd elements), the output H (b) (NbNh elements), the target T (b) (NbNh elements), and the system parameters for optimizing the Nh (Nd + 1) elements must be stored in memory; a total of Nh (Nd + 1) elements + Nd (Nd)."}, {"heading": "6. Discussion", "text": "In fact, the fact is that most of them are able to decide whether they are able or not, will be able to play by the rules."}, {"heading": "7. Conclusion", "text": "The algorithm provides a meta-parameter-free, standard, simple, and computationally efficient approach to unattended, sparse feature learning. It explicitly seeks both longevity and population sparseness to learn discriminatory traits, thereby avoiding dead results. The results show that the method significantly exceeds all modern methods of the STL-10 dataset with lower standard deviations between the wrinkles, suggesting a more robust message between training sets."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "IEEE TPAMI,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the difference between orthogonal matching pursuit and orthogonal least squares", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Unpublished manuscript,", "citeRegEx": "Blumensath and Davies,? \\Q2007\\E", "shortCiteRegEx": "Blumensath and Davies", "year": 2007}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS, pp", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew"], "venue": "In ICML, pp", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Courville", "Aaron", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In AISTATS,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "What is the goal of sensory coding", "author": ["D.J. Field"], "venue": "Neural Computation,", "citeRegEx": "Field,? \\Q1994\\E", "shortCiteRegEx": "Field", "year": 1994}, {"title": "Unsupervised and supervised visual codes with restricted boltzmann machines", "author": ["Goh", "Hanlin", "Thome", "Nicolas", "Cord", "Matthieu", "Lim", "Joo-Hwee"], "venue": "In ECCV,", "citeRegEx": "Goh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2012}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G.E. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Hinton,? \\Q2010\\E", "shortCiteRegEx": "Hinton", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "YeeWhye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen and Oja,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "Independent component analysis", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "Wiley Interscience,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["Kavukcuoglu", "Koray", "Sermanet", "Pierre", "Boureau", "Y-Lan", "Gregor", "Karol", "Mathieu", "Micha\u00ebl", "LeCun", "Yann"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "The hungarian method for the assignment problem", "author": ["Kuhn", "Harold W"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Kuhn and W.,? \\Q1955\\E", "shortCiteRegEx": "Kuhn and W.", "year": 1955}, {"title": "Exploring strategies for training deep neural networks", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Lamblin", "Pascal"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Efficient backprop. In Neural Networks: Tricks of the Trade, pp. 9\u201350", "author": ["LeCun", "Yann", "Bottou", "Leon", "Orr", "Genevieve", "M\u00fcller", "Klaus"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by v1", "author": ["B. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaifar", "P.S. Krishnaprasad"], "venue": "In ACSSC,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Self-taught learning: Transfer learning from unlabeled data", "author": ["Raina", "Rajat", "Battle", "Alexis", "Lee", "Honglak", "Packer", "Benjamin", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Raina et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2007}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M.A. Ranzato", "C. Poultney", "S. Chopra", "Y. Lecun"], "venue": "In NIPS,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "No More Pesky Learning Rates", "author": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"], "venue": "In ICML,", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In NIPS, pp", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In IEEE CVPR,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 26, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 4, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 10, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 1, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 15, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 6, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 2, "context": "\u201d In addition to that, one of the main criticisms to state-of-the-art methods is that they require a significant amount of metaparameters (Bengio et al., 2013).", "startOffset": 138, "endOffset": 159}, {"referenceID": 25, "context": "As stated in (Snoek et al., 2012), the tuning of these meta-parameters is a laborious task that requires expert knowledge, rules of thumb or extensive search and, whose setting can vary for different tasks.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": ", 2011) and automatic approaches to optimize the performance of learning algorithms (Snoek et al., 2012).", "startOffset": 84, "endOffset": 104}, {"referenceID": 12, "context": "To the best of our knowledge, work in this direction includes ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000) and sparse filtering (Ngiam et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 16, "context": "Although ICA provides good results at object recognition tasks (Le et al., 2011; Ngiam et al., 2011), the method scales poorly to large datasets and high input dimensionality.", "startOffset": 63, "endOffset": 100}, {"referenceID": 18, "context": "Significant amount of work has been done in order to overcome this limitation (Lee et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Significant amount of work has been done in order to overcome this limitation (Lee et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2010) is a successful variant of sparse coding, which uses a predictor to approximate the sparse representation and solves the sparse coding computationally expensive encoding step.", "startOffset": 38, "endOffset": 64}, {"referenceID": 10, "context": "Sparse RBM (Hinton et al., 2006; Lee et al., 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al.", "startOffset": 11, "endOffset": 50}, {"referenceID": 19, "context": "Sparse RBM (Hinton et al., 2006; Lee et al., 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al.", "startOffset": 11, "endOffset": 50}, {"referenceID": 23, "context": ", 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al., 2006) weight decay, sparseness constant, sparsity penalty Sparse Coding (Olshausen & Field, 1997) sparsity penalty RICA (Le et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 16, "context": ", 2006) weight decay, sparseness constant, sparsity penalty Sparse Coding (Olshausen & Field, 1997) sparsity penalty RICA (Le et al., 2011) reconstruction penalty PSD (Kavukcuoglu et al.", "startOffset": 122, "endOffset": 139}, {"referenceID": 13, "context": ", 2011) reconstruction penalty PSD (Kavukcuoglu et al., 2010) sparsity penalty, prediction penalty OMP-k (Pati et al.", "startOffset": 35, "endOffset": 61}, {"referenceID": 21, "context": ", 2010) sparsity penalty, prediction penalty OMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) k (non-zero elements) ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al.", "startOffset": 51, "endOffset": 116}, {"referenceID": 12, "context": ", 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) k (non-zero elements) ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000) Sparse Filtering (Ngiam et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 10, "context": "Commonly used algorithms for unsupervised feature learning include Restricted Boltzmann Machines (RBM) (Hinton et al., 2006), auto-encoders (Bengio et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": ", 2006), auto-encoders (Bengio et al., 2006), sparse coding (Raina et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": ", 2006), sparse coding (Raina et al., 2007) and hybrids such as PSD (Kavukcuoglu et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": ", 2007) and hybrids such as PSD (Kavukcuoglu et al., 2010).", "startOffset": 32, "endOffset": 58}, {"referenceID": 12, "context": "Many other methods such as ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000), Reconstruction ICA (RICA) (Le et al.", "startOffset": 31, "endOffset": 78}, {"referenceID": 16, "context": ", 2000), Reconstruction ICA (RICA) (Le et al., 2011), Sparse Filtering (Ngiam et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 21, "context": ", 2011) and methods related to vector quantization such as Orthogonal Matching Pursuit (OMP-k) (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) have also been used in the literature to extract unsupervised feature representations.", "startOffset": 95, "endOffset": 160}, {"referenceID": 23, "context": "Sparse auto-encoders (Ranzato et al., 2006), sparse RBM (Hinton et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 10, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 19, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 9, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 8, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 13, "context": ", 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al., 2010), OMP-k (Pati et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 21, "context": ", 2010), OMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) and Reconstruction ICA (RICA) (Le et al.", "startOffset": 15, "endOffset": 80}, {"referenceID": 16, "context": ", 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) and Reconstruction ICA (RICA) (Le et al., 2011) explicitly model the data distribution by minimizing the reconstruction error.", "startOffset": 84, "endOffset": 101}, {"referenceID": 7, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 23, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 19, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 16, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 2, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 7, "context": "There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs (Field, 1994; Willmore & Tolhurst, 2001; Ranzato et al., 2006; Ngiam et al., 2011).", "startOffset": 121, "endOffset": 203}, {"referenceID": 23, "context": "There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs (Field, 1994; Willmore & Tolhurst, 2001; Ranzato et al., 2006; Ngiam et al., 2011).", "startOffset": 121, "endOffset": 203}, {"referenceID": 23, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 19, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 16, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 24, "context": "The system might be trained by means of an off-the-shelf mini-batch Stochastic Gradient Descent (SGD) method with adaptive learning rates such as variance-based SGD (vSGD) (Schaul et al., 2013).", "startOffset": 172, "endOffset": 193}, {"referenceID": 17, "context": "Starting with \u0393 set to small random numbers as in (LeCun et al., 1998) (line 1), at each epoch we shuffle the samples of the training set (line 3), reset the EPLS inhibitor a to a flat activation (line 4) and process all mini-batches.", "startOffset": 50, "endOffset": 70}, {"referenceID": 24, "context": "After that, the gradient of the error is computed (line 9) and the learning rate \u03b7 is estimated as in (Schaul et al., 2013) (line 10).", "startOffset": 102, "endOffset": 123}, {"referenceID": 24, "context": "9: G = \u2207\u0393||H \u2212T||2 10: Estimate learning rate \u03b7 as in (Schaul et al., 2013) 11: \u0393 = \u0393\u2212 \u03b7G 12: end for 13: Limit the bases W in \u0393 to have unit norm 14: until stop condition verified", "startOffset": 54, "endOffset": 75}, {"referenceID": 4, "context": "The performance of training and encoding strategies in single layer networks has been extensively analyzed in the literature (Coates et al., 2011; Coates & Ng, 2011; Ngiam et al., 2011) on STL-101 dataset.", "startOffset": 125, "endOffset": 185}, {"referenceID": 4, "context": "To validate our method, we follow the experimental pipeline of (Coates et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 16, "context": "Single-Layer with meta-parameters RICA (Le et al., 2011) (1600/Natural) 52.", "startOffset": 39, "endOffset": 56}, {"referenceID": 16, "context": "Our results show that simultaneously enforcing both population and lifetime sparsity helps in learning discriminative dictionaries, which reflect in better performance, especially when compared to meta-parameter free methods (Ngiam et al., 2011; Le et al., 2011).", "startOffset": 225, "endOffset": 262}, {"referenceID": 16, "context": "Our algorithm has several advantages over OMP-1: (1) It can use any activation function; (2) by enforcing lifetime sparsity it does not suffer of the dead output problem, thus not requiring ad-hoc tricks to avoid it; (3) it does not require whitening, which can be a problem if the input dimensionality is large (Le et al., 2011).", "startOffset": 312, "endOffset": 329}, {"referenceID": 13, "context": "Fifth, there is an interest in the literature in avoiding redundancy in the image representation by using the algorithms in a convolutional fashion (Kavukcuoglu et al., 2010).", "startOffset": 148, "endOffset": 174}], "year": 2014, "abstractText": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.", "creator": "LaTeX with hyperref package"}}}