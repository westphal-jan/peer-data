{"id": "1402.6013", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2014", "title": "Open science in machine learning", "abstract": "We present OpenML and mldata, open science platforms that provides easy access to machine learning data, software and results to encourage further study and application. They go beyond the more traditional repositories for data sets and software packages in that they allow researchers to also easily share the results they obtained in experiments and to compare their solutions with those of others.", "histories": [["v1", "Mon, 24 Feb 2014 23:12:42 GMT  (24kb)", "http://arxiv.org/abs/1402.6013v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DL", "authors": ["joaquin vanschoren", "mikio l braun", "cheng soon ong"], "accepted": false, "id": "1402.6013"}, "pdf": {"name": "1402.6013.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Cheng Soon Ong"], "emails": ["joaquin@liacs.nl", "mikio.braun@tu-berlin.de", "chengsoon.ong@unimelb.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 140 2.60 13v1 [cs.LG] 2 4Fe bKeywords: machine learning, open science"}, {"heading": "1 Introduction", "text": "Machine learning and data mining research can be greatly accelerated by \"shifting empirical research out of the minds of people and laboratories and into tools that help us structure and change information.\" [3] The huge flows of experiments that are conducted to name new algorithms, test hypotheses, or new data sets have much more utility than their original purpose, but are often discarded or their details are lost over time. In this paper, we present recently developed infrastructures that aim to make machine learning research more open, going beyond the more traditional repositories by ensuring that researchers can share detailed results from experiments and compare their solutions with those of others."}, {"heading": "2 OpenML", "text": "In fact, most people who are able to move are able to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move."}, {"heading": "3 mldata", "text": "mldata (http: / / mldata.org) is a community-based website for sharing machine learning data sets. Data sets can be either raw data files or collections of data, or use one of the supported file formats such as HDF5 or ARFF. In this case, mldata considers metadata contained in the files to display more information. mldata can also define learning tasks based on data sets, with mldata currently focusing on monitored learning data. mldata identifies which features are used for input and output and also what score is used to evaluate function. mldata also allows learning tasks to be jointly created by grouping learning tasks, and lets users submit results in the form of predicted labels, which are then automatically evaluated.mldata.org supports four types of information: raw data sets, learning tasks, learning methods, learning methods and challenges. A raw data set is just a part of the data to be used during the learning task, and also for the evaluation and the cost function."}, {"heading": "4 Related work", "text": "There are also platforms for providing reproducible benchmarks. DELVE (http: / / www.cs.utoronto.ca / \u02dc delve) was the first, but is currently in limbo. MLComp (http: / / mlcomp.org) allows users to upload their algorithms and evaluate them on known datasets (or vice versa) on MLComp servers. RunMyCode (http: / / runmycode.org) allows researchers to create publication companion websites by uploading code and building an interface. Users can then fill in all input online and get the result of the algorithm. Compared to these systems, OpenML and mldata offer users more flexibility in conducting experiments: New tasks can be introduced for novel experiments and experiments can be performed in any environment. OpenML also provides clean integration into data mining platforms that researchers already use in day-to-to-day research, and more direct data integration that researchers can do."}, {"heading": "Acknowledgments", "text": "This work is supported by a grant of 600.065.120.12N150 from the Dutch Fund for Scientific Research (NWO) and the IST programme of the European Community within the PASCAL2 Network of Excellence, IST-2007-216886."}], "references": [{"title": "Classifier technology and the illusion of progress", "author": ["D. Hand"], "venue": "Statistical Science", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Data mining research: Current status and future opportunities", "author": ["H. Hirsh"], "venue": "Statistical Analysis and Data Mining 1(2), 104\u2013107", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "The future of science: Building a better collective memory", "author": ["M. Nielsen"], "venue": "APS Physics 17(10)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved dataset characterisation for meta-learning", "author": ["Y. Peng", "P. Flach", "C. Soares", "P. Brazdil"], "venue": "Lecture Notes in Computer Science 2534, 141\u2013152", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Experiment databases", "author": ["J. Vanschoren", "H. Blockeel", "B. Pfahringer", "G. Holmes"], "venue": "A new way to share, organize and learn from experiments. Machine Learning 87(2), 127\u2013158", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Research in machine learning and data mining can be speeded up tremendously by moving empirical research results \u201cout of people\u2019s heads and labs, onto the network and into tools that help us structure and alter the information\u201d [3].", "startOffset": 228, "endOffset": 231}, {"referenceID": 0, "context": "This facilitates much larger-scale machine learning studies, yielding more generalizable results [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Last but not least, these infrastructures keep track of experiment details, ensuring that we can easily reproduce them later on, and confidently build upon earlier work [2].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "Moreover, for all data sets, it calculates meta-data about the features and the data distribution[4], and for all implementations, meta-data is stored about their (hyper)parameters and properties such as what input data they can handle, what tasks they can solve and, if possible, advanced properties such bias-variance profiles.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "OpenML also offers clean integration in data mining platforms that researchers already use in daily research, and closer data integration so that researchers can reuse results in many ways beyond direct benchmark comparisons, such as meta-learning studies [5].", "startOffset": 256, "endOffset": 259}], "year": 2014, "abstractText": "We present OpenML and mldata, open science platforms that provides easy access to machine learning data, software and results to encourage further study and application. They go beyond the more traditional repositories for data sets and software packages in that they allow researchers to also easily share the results they obtained in experiments and to compare their solutions with those of others.", "creator": "LaTeX with hyperref package"}}}