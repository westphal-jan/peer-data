{"id": "1402.7025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2014", "title": "Exploiting the Statistics of Learning and Inference", "abstract": "When dealing with datasets containing a billion instances or with simulations that require a supercomputer to execute, computational resources become part of the equation. We can improve the efficiency of learning and inference by exploiting their inherent statistical nature. We propose algorithms that exploit the redundancy of data relative to a model by subsampling data-cases for every update and reasoning about the uncertainty created in this process. In the context of learning we propose to test for the probability that a stochastically estimated gradient points more than 180 degrees in the wrong direction. In the context of MCMC sampling we use stochastic gradients to improve the efficiency of MCMC updates, and hypothesis tests based on adaptive mini-batches to decide whether to accept or reject a proposed parameter update. Finally, we argue that in the context of likelihood free MCMC one needs to store all the information revealed by all simulations, for instance in a Gaussian process. We conclude that Bayesian methods will remain to play a crucial role in the era of big data and big simulations, but only if we overcome a number of computational challenges.", "histories": [["v1", "Wed, 26 Feb 2014 10:47:09 GMT  (80kb,D)", "http://arxiv.org/abs/1402.7025v1", "Proceedings of the NIPS workshop on \"Probabilistic Models for Big Data\""], ["v2", "Tue, 4 Mar 2014 21:12:43 GMT  (81kb,D)", "http://arxiv.org/abs/1402.7025v2", "Proceedings of the NIPS workshop on \"Probabilistic Models for Big Data\""]], "COMMENTS": "Proceedings of the NIPS workshop on \"Probabilistic Models for Big Data\"", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["max welling"], "accepted": false, "id": "1402.7025"}, "pdf": {"name": "1402.7025.pdf", "metadata": {"source": "CRF", "title": "Exploiting the Statistics of Learning and Inference", "authors": ["Max Welling"], "emails": ["m.welling@uva.nl"], "sections": [{"heading": "1 Statistical Learning", "text": "If we learn a parametric model from the data we extract from the data and store it in the parameter values of the model, it seems to be more than a matter of time before we know all the decimated parameters of the model. However, if the model has a large capacity to store information, there may be an overstretching of the parameters, which are usually too large to prevent the learning algorithm from storing information in the most significant parts of the parameters. Bayesian methods determine a posterior distribution of the parameters in which the previous time is usually prevented from becoming too large (similar to regulation) and integration by the posterior effectively destroys the information, the insignificant decimal locations of the parameter values. Further data usually implies that further bits of our parameters are recruited."}, {"heading": "2 Statistical Optimization", "text": "There is a growing tendency to do what you have to do to do it."}, {"heading": "3 Data Redundancy", "text": "As discussed in the previous section, our proposed methods of accelerating learning algorithms will be based on the idea that far from convergence, only a few data cases are needed to determine a reasonably accurate update. So, if we want to learn the mean of a one-dimensional normal distribution, and the current mean is still far from the sample data, then we only need to query a few data cases to know the direction of the update. We can say that the information in the data relevant for the parameter update is highly redundant. It is interesting to stress that redundancy is not a property of the data, but the relationship between the model and the data. For example, we recommend updating the data cases close to convergence, while the other half recommends updating them to the right. In the ML estimation, redundancy is precisely zero.We will try to capture this new measurement, namely the \"learning signal to noise\" ratio."}, {"heading": "4 Statistical MCMC Inference", "text": "The reason for this is that every MCMC procedure often begins a detailed phase of combustion. \"Unfortunately, the answer is an unqualified no. Bayesian inference requires (an approximation of the entire posterior distribution of parameters comprising given data), but hardly any of the common methods that use statistical properties to improve the computational efficiency of inference (with the exception of [12]). So let's look at the workhorse of Bayesian inference, MCMC sampling. Any MCMC algorithm that assumes the posterior distribution p (D) must take into account all data positions to consider the imaginary infinite dataset before it comes to a grinfinite standstill, even before it begins. Even if for an infinite dataset the posterior distribution is a single point (the\" maximum posterior \"value)."}, {"heading": "5 Big Simulations", "text": "The question of the sense and nonsense of the way in which we express these simulation methods and neural networks, scientists such as astronomers, meteorologists, biologists and so on, who express their expertise in terms of extremely complex simulations. To give an example of how extremely complex these simulations can be: the SCEC \"shakeout simulation\" of 360 minutes of earthquake simulation along the San Andreas fault in California took 220 teraflops per second. The typical simulation has a number of free parameters to adjust the observations well."}, {"heading": "6 Conclusions", "text": "Our contention is that in the context of large data sets and complex simulations, it is imperative that we use the statistical properties of the learning and inference process. Intuitively, we want to maximize the amount of information we transfer from data or simulations to parameters per unit of computation, and we conclude that computation must be an essential part of the overall goal. However, the traditional view of learning and inference could be maintained because the \"old days\" data sets were small enough and the \"old days\" simulations were simple enough so that relatively expensive optimization or inference procedures would still converge within hours. However, when you are faced with 1 billion cases of data or a simulation running 24 hours on a supercomputer, the equation changes dramatically and you are forced to rethink traditional inference methods such as MCMC. We predict that Bayesian methods will play an important role in managing just a number of computational tasks in an era when data volume and simulation complexity are growing exponentially."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Distributed stochastic gradient mcmc", "author": ["S. Ahn", "B. Shahbaba", "M. Welling"], "venue": "Technical report, University of California Irvine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A Bayesian framework for graphical models", "author": ["H. Attias"], "venue": "In Advances in Neural Information Processing Systems \u2013 NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures", "author": ["M.J. Beal", "Z. Ghahramani"], "venue": "In Bayesian Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Predictability, complexity and learning", "author": ["W. Bialek", "I. Nemenman", "N. Tishby"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Statistical tests for optimization efficiency", "author": ["L. Boyles", "A. Korattikara", "D. Ramanan", "M. Welling"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "Technical report, University of Washington,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Approximate slice sampling for bayesian posterior inference", "author": ["C. DuBois", "A. Korattikara", "M. Welling", "P. Smyth"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical Report abs/1207.0580,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Online learning for latent dirichlet allocation", "author": ["M. Hoffman", "D. Blei", "F. Bach"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Statistical optimization of nonnegative matrix factorization", "author": ["A. Korattikara", "L. Boyles", "M. Welling", "J. Kim", "H. Park"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Gps-abc: Gaussian process surrogate approximate bayesian computation", "author": ["T. Meeds", "M. Welling"], "venue": "arXiv 1401.2838,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T. Minka"], "venue": "In Proc. of the Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "A view of the EM algorithm that justifies incremental, sparse and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Practi- cal bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.A. Adams"], "venue": "In Neural Information Processing Systems,,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Statistical inference for noisy nonlinear ecological dynamic systems", "author": ["S.N. Wood"], "venue": "Nature, 466(7310):1102\u20131104,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "At the same time, the amount of predictive information in data grows slower than the amount of Shannon information [5] namely as N with \u03b1 < 1.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "We have seen some evidence of this recently when [11] introduced dropout in an attempt to combat overfitting for deep neural networks.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "There are essentially two classes of big data Bayesian methods and both are based on stochastic minibatch updates: stochastic gradient variational Bayes [12] and stochastic gradient MCMC [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "There are essentially two classes of big data Bayesian methods and both are based on stochastic minibatch updates: stochastic gradient variational Bayes [12] and stochastic gradient MCMC [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "In [6,13] we have proposed methods to increase the minibatch size (instead of decreasing the stepsize), based on statistical hypothesis tests that estimate the probability that a proposed update is more than 180 degrees in the wrong direction, leading to a learning procedure that auto-tunes its optimization hyper-parameters.", "startOffset": 3, "endOffset": 9}, {"referenceID": 10, "context": "In [6,13] we have proposed methods to increase the minibatch size (instead of decreasing the stepsize), based on statistical hypothesis tests that estimate the probability that a proposed update is more than 180 degrees in the wrong direction, leading to a learning procedure that auto-tunes its optimization hyper-parameters.", "startOffset": 3, "endOffset": 9}, {"referenceID": 5, "context": "As discussed in the previous section, our proposed methods [6,13] for speeding up learning algorithms are based on the notion that far away from convergence only few data-cases are needed to determine a reasonably accurate update.", "startOffset": 59, "endOffset": 65}, {"referenceID": 10, "context": "As discussed in the previous section, our proposed methods [6,13] for speeding up learning algorithms are based on the notion that far away from convergence only few data-cases are needed to determine a reasonably accurate update.", "startOffset": 59, "endOffset": 65}, {"referenceID": 9, "context": "Almost none of the standard posterior inference methods make use of statistical properties to improve the computational efficiency of inference (with the exception of [12]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "It should be mentioned that machine learning practitioners have already embraced this view some time ago in the context of posterior inference through biased methods such as variational Bayesian inference [4,3,16]and expectation propagation [15].", "startOffset": 205, "endOffset": 213}, {"referenceID": 2, "context": "It should be mentioned that machine learning practitioners have already embraced this view some time ago in the context of posterior inference through biased methods such as variational Bayesian inference [4,3,16]and expectation propagation [15].", "startOffset": 205, "endOffset": 213}, {"referenceID": 13, "context": "It should be mentioned that machine learning practitioners have already embraced this view some time ago in the context of posterior inference through biased methods such as variational Bayesian inference [4,3,16]and expectation propagation [15].", "startOffset": 205, "endOffset": 213}, {"referenceID": 12, "context": "It should be mentioned that machine learning practitioners have already embraced this view some time ago in the context of posterior inference through biased methods such as variational Bayesian inference [4,3,16]and expectation propagation [15].", "startOffset": 241, "endOffset": 245}, {"referenceID": 15, "context": "One can show [18] that when the stepsize \u03b5t is annealed to zero using the same conditions as the ones imposed for stochastic gradient descent in order to guarantee its convergence, namely \u2211 t \u03b5t =\u221e, \u2211 t \u03b5 2 t <\u221e, as T \u2192\u221e this dynamical system samples from the correct distribution.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "This effect can be further enhanced by using the empirical inverse Fisher information as a preconditioning matrix turning gradient descent into a form of stochastic Fisher scoring [1].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "However, SGLD is able to make multiple parameter updates per server by subsampling the data on a single server without any communication [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Indeed, [10] have developed a slice sampler variant based on these principles while [8] have developed a minibatch Hamiltonian Monte Carlo algorithm.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Indeed, [10] have developed a slice sampler variant based on these principles while [8] have developed a minibatch Hamiltonian Monte Carlo algorithm.", "startOffset": 84, "endOffset": 87}, {"referenceID": 14, "context": "The recent advances in Bayesian optimization [17] should prove to be a huge help in replacing grid search by a smarter form of parameter exploration However, scientists", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "In [14] we proposed the following procedure based on the notion of a synthetic likelihood [19].", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [14] we proposed the following procedure based on the notion of a synthetic likelihood [19].", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "When dealing with datasets containing a billion instances or with simulations that require a supercomputer to execute, computational resources become part of the equation. We can improve the efficiency of learning and inference by exploiting their inherent statistical nature. We propose algorithms that exploit the redundancy of data relative to a model by subsampling data-cases for every update and reasoning about the uncertainty created in this process. In the context of learning we propose to test for the probability that a stochastically estimated gradient points more than 180 degrees in the wrong direction. In the context of MCMC sampling we use stochastic gradients to improve the efficiency of MCMC updates, and hypothesis tests based on adaptive mini-batches to decide whether to accept or reject a proposed parameter update. Finally, we argue that in the context of likelihood free MCMC one needs to store all the information revealed by all simulations, for instance in a Gaussian process. We conclude that Bayesian methods will remain to play a crucial role in the era of big data and big simulations, but only if we overcome a number of computational challenges. 1 Statistical Learning When we learn a parametric model from data we extract the useful information from the data and store it in the parameter-values of the model. Naive algorithms learn all decimal places of the model parameters (up to machine precision) by optimizing some cost function (e.g. the log-likelihood). If the model has a large capacity to store information this might lead to overfitting. Regularization typically avoids parameters to become too large, preventing the learning algorithm to store information in the most significant bits of the parameters. Bayesian methods determine a posterior distribution over parameters, where the prior usually prevents the parameters from becoming too large (similar to regularization) and the integration over the parameters weighted by the posterior effectively destroys the information in the insignificant decimal places of the parameter values. More data usually implies that more bits of our parameters are recruited to store information. In the context of big data, there is the general perception that A) learning is computationally more expensive implying that given a fixed amount of computational resources it takes longer to train good predictive models and B) that with more data overfitting is becoming less of a concern. However, both statements need not necessarily be true. To see why A) may not be true, one can simply imagine subsampling the large dataset into ar X iv :1 40 2. 70 25 v1 [ cs .L G ] 2 6 Fe b 20 14 a smaller dataset and train a model model fast on the smaller dataset. Any algorithm that takes much longer to reach the same predictive power as this naive subsampling approach clearly does something wrong if one cares at all about the amount of learning per unit time. Before the advent of big data one may not have cared much about reaching the optimal possible predictive performance given some fixed amount of time (or more generally any amount of time unknown in advance). However, one can no longer afford this luxury when there is so much data that most algorithms will simply not completely converge within any reasonable amount of time. In other words, we should be interested in algorithms that when interrupted at any arbitrary time should be optimally predictive, and not with algorithms that perform well only after a very long time. A typical example of the former class of algorithms is \u201dstochastic gradient descent\u201d (SGD), while typical examples of the latter are \u201dbatch learning\u201d and standard Markov chain Monte Carlo sampling algorithms (MCMC). SGD can be expected to do as well as possible for any fixed amount of training time (when the annealing schedule for the stepsize is set wisely), while batch learning and MCMC might return disastrous results when the amount of compute time is limited. As an extreme example, for a very large data-set a batch gradient update might not have done a single update while SGD might have already arrived at a reasonable model. To see why B) is not necessarily true we only have to remember that while data volume grows exponentially, Moore\u2019s law also allows us to train models that grow exponentially in their capacity. In the field of deep learning, this is exactly what seems to be happening: Google and Yahoo! currently train models with billions of parameters. At the same time, the amount of predictive information in data grows slower than the amount of Shannon information [5] namely as N with \u03b1 < 1. This law of diminishing returns of data thus implies that our models are increasing their capacity faster than the amount of predictive information that we need to fill them with. The surprising conclusion is thus that regularization to prevent overfitting is increasingly important rather then increasingly irrelevant. We have seen some evidence of this recently when [11] introduced dropout in an attempt to combat overfitting for deep neural networks. It seems that currently our best models are the ones that overfit to some degree and require regularization or bagging to avoid the overfitting. If, in the context of big data, you are training models that do not straddle the boundary between underand overfitting, then it is likely that by increasing the capacity of your model (assuming that you have the computational resources to do so) you can reach better predictive performance. We thus argue forcefully that computationally efficient Bayesian methods are becoming increasingly relevant in the big data era: they are relevant because our best high capacity models need them as a protection against overfitting and they need to be computationally efficient in order to deal with the large number of data cases involved. There are essentially two classes of big data Bayesian methods and both are based on stochastic minibatch updates: stochastic gradient variational Bayes [12] and stochastic gradient MCMC [18]. I will say more about the latter later in this paper. 2 Statistical Optimization There is an increasing tendency to cast learning as an optimization problem of some loss function. For example, an SVM is often taught as a quadratic program over Lagrange multipliers and neural network training is taught as an exercise in minimizing weights of some loss function defined on the output units of the network. New powerful tools from the\u201cconvex optimization\u201d literature have encouraged this myopic view of learning to the point that some researchers now hold the view that every learning problem should be cast as a (preferably) convex optimization problem. The tendency to view all learning problems as \u201cmere optimization\u201d which can be successfully attacked with the modern blessings of convex optimization ignores some of the unique properties of learning problems. In other words: learning can indeed be formulated as an optimization problem, but a rather special one that has important features which should not be ignored. So what are these special properties of learning problems that distinguish them from plain vanilla optimization? The main difference lies in the loss function being a function of the data and the data being a random draw from some underlying distribution P . It is thus useful to think of the loss function as a random entity itself, i.e. one that fluctuates under resampling of the data-items from P . One aspect of this randomness is well appreciated by most machine learning researchers, namely that it contains the information necessary to avoid overfitting. Most researchers understand that simply fitting a model by minimizing some loss based on the training data alone might lead to overfitting. The fluctuations caused by resampling the training data will cause the optimal parameter-values to fluctuate and hence determine a distribution of parameters rather than a single value (the frequentist equivalent of the posterior distribution). It is less appreciated that taking the statistical properties of optimization into account during the entire learning process (and not just close to the point of convergence) can be very helpful to increase its computational efficiency. Perhaps the point can be made most forcefully by considering the extreme case of an infinite dataset. Any learning procedure that uses all data at every iteration is doomed to not do anything at all! During the initial stages of learning, when we are trying to the determine the most significant digits of our parameters, the information in the data is highly redundant. In other words: most data items agree on how they recommend changing the parameter values and as a result, one only has to query a small subset of them to get reliable information on how to update parameters. In contrast, close to convergence, most data items disagree on how to change the parameters and the update direction will strongly depend on \u201cwho you ask\u201d. Stochastic gradient descent (SGD) exploits precisely this idea, but does not tie it directly to the statistical properties of the optimization problem, implying that the annealing schedule of the stepsize has to be set by hand. In [6,13] we have proposed methods to increase the minibatch size (instead of decreasing the stepsize), based on statistical hypothesis tests that estimate the probability that a proposed update is more than 180 degrees in the wrong direction, leading to a learning procedure that auto-tunes its optimization hyper-parameters. These methods exploit the redundancy in data in relation to a partially trained model.", "creator": "LaTeX with hyperref package"}}}