{"id": "1403.0598", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2014", "title": "The Structurally Smoothed Graphlet Kernel", "abstract": "A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.", "histories": [["v1", "Mon, 3 Mar 2014 21:20:14 GMT  (291kb,D)", "http://arxiv.org/abs/1403.0598v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pinar yanardag", "s v n vishwanathan"], "accepted": false, "id": "1403.0598"}, "pdf": {"name": "1403.0598.pdf", "metadata": {"source": "CRF", "title": "The Structurally Smoothed Graphlet Kernel", "authors": ["Pinar Yanardag", "S.V. N. Vishwanathan"], "emails": ["ypinar@purdue.edu", "vishy@stat.purdue.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people living in the US are able to know themselves and understand how they have behaved. (...) It is not as if they are able to understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. \"(...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "A graph is a pair of G = (V, E), where V = {v1, v2,..., v | V |} is an ordered set of vertices or nodes and E V \u00b7 V is a series of edges. G = (V, E) and H = (VH, EH) given, H is a partial graph of G iff, there is an injective figure \u03b1: VH \u2192 V such that (v, w) and EH iff (\u03b1 (v), \u03b1 (w)). Two graphs G = (V, E) and G \u2032 = (V, E \u2032) are isomorphic if there is a bijective figure g: V \u2192 V \u2032, so that (vi, vj) and E iff (vi), g (vj))."}, {"heading": "2.2 The Graphlet kernel", "text": "Let Gk = {g1, g2,.., gnk} be the set of graphs of magnitude k, where nk denotes the number of unique graphs of magnitude k. Considering a graph G, we define fG as a normalized length vector nk, the i-th component of which corresponds to the frequency of occurrence of gi in G: fG = (c1 \u2211 nk j cj, \u00b7 \u00b7 \u00b7, cnk \u2211 nk j cj) T. (1) Here ci denotes the number of Gi events as a subgraph of G. Considering two graphs G and G \u2032, the graph core kg is defined as: kg (G, G \u2032): = f > G fG \u2032, (2) which is simply the point product between the normalized graph frequency vectors."}, {"heading": "2.3 Smoothing multinomial distributions", "text": "In this section, we will briefly review the distribution techniques for multinomial distributions and show that graph nuclei are actually based on the estimation of a multinomial distribution. Suppose we observe a sequence of 1, 2,.., en containing n discrete events from a sediment of size M and we want to estimate the probability P (ei) of observing each event. (3), where the number of events in the observed sequence and the number of events observed are based on a total number of events observed, it is easy to see that the representation in graph nuclei in section 2.2 is actually an MLE estimate based on the observed sequences of the graph. However, MLE estimates of the multinomial distribution are spiky, which is that they assign zero probability to events used in graph nuclei in section 2.2, in fact an MLE estimate on the observed sequences of the graph distribution."}, {"heading": "3. DEFINING A BASE DISTRIBUTION", "text": "The space of the graph has an inherent structure, so you can construct a direct acyclic diagram (DAG) to show how different graphics relate to each other. A node in depth k denotes a graph of size k, and only if it does, can we use the node of the DAG and a graph interchangeably. We will first discuss how we can use this graph of size k and then discuss how we can use this graph to define a graph of size k + 1. The first step towards constructing our DAG is to obtain all the unique graph types of size k. Therefore, we will first create all possible graphs of size k (this includes a timeO (2k), and use Nauty [25] to obtain its canonically described isominated representations."}, {"heading": "3.1 Kneser-Ney Smoothing with a structural distribution", "text": "We now have all the components necessary to explain our Kneser Structural Ney (SKN) framework. Given an arbitrary graph Gj of size k + 1, we estimate the probability that this graphite will be observed as follows: PSKN (gj) = max (cj \u2212 d, 0) \u2211 gj, \"Gk + 1 cj\" + d, \"gj,\" gj, \"Gk + 1 cj,\" gj, \"Gk + 1 | {gj\": cj \"> d.\" As can be seen from the equation, we first discount the number of all graphs by d and then distribute this mass to all other graphs. The amount of mass that a graph receives is controlled by the base distribution. To automatically adjust the discount parameter d, we use the Pitman-Yor method (a Bayesian approximation of Kneser-Ney) in the next section."}, {"heading": "4. PITMAN-YOR PROCESS", "text": "We will only give a very high overview of a Pitman-Yor process and refer the reader to the excellent papers of Teh [35] and [12] for more details. A Pitman-Yor process P on a floor Gk + 1 of size- (k + 1) graphs is then defined viaPk + 1 of customers PY (dk + 1, Pk + 1, Pk), (11) where dk + 1 is a discount parameter 0 \u2264 dk + 1 < 1 of customers is a strength parameter, and Pk is a basic distribution. The most intuitive way to understand draws from the Pitman-Yor process is via the Chinese restaurant process (see also Figure 6). Consider a restaurant with an infinite number of tables. Customers enter the restaurant one by one. The first customer sits at the first table and will sit at the first table."}, {"heading": "5. RELATED WORK", "text": "The problem of estimating multinomial distributions is a classic problem. However, in natural language processing, they occur in the following context: Suppose we find a sequence of dictionaries,.. wk, and one is interested in asking what the probability of observing words is next. Estimating this probability lies at the heart of language models, and many complex smoothing techniques have been proposed. This is a classic multinomial estimation problem that suffers from thrift, since the event space is infinite. Moreover, natural language exhibits behavior dominated by a small number of frequently occurring words. In the extensive empirical evaluation, it was found that Kneser-Ney smoothing is very effective."}, {"heading": "6. EXPERIMENTS", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "7. DISCUSSION", "text": "We presented a new framework for smoothing normalized graph frequency vectors inspired by smoothing techniques from natural language processing. Although our models are inspired by working in language models, they differ fundamentally in the way they define a fallback-base distribution. We believe that our framework is applicable beyond graph cores and can be used in any structural environment where one can naturally define a relationship, such as the DAG we defined in Figure 4. We are currently investigating the applicability of our framework to string cores [22, 23, 37] and tree cores [8]. It is also interesting to investigate whether our method can be extended to other graph cores such as Random Walk cores."}], "references": [{"title": "Gephi: an open source software for exploring and manipulating networks", "author": ["Mathieu Bastian", "Sebastien Heymann", "Mathieu Jacomy"], "venue": "In ICWSM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "editors", "author": ["D. Bonchev", "D.H. Rouvray"], "venue": "Chemical Graph Theory: Introduction and Fundamentals, volume 1. Gordon and Breach Science Publishers, London, UK", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Protein function prediction via graph kernels", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Sch\u00f6nauer", "S.V.N. Vishwanathan", "A.J. Smola", "H.-P. Kriegel"], "venue": "Proceedings of Intelligent Systems in Molecular Biology (ISMB), Detroit, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Protein function prediction via graph kernels", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Schonauer", "S.V.N. Vishwanathan", "A.J. Smola", "H.P. Kriegel"], "venue": "Bioinformatics (ISMB),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Shortest-path kernels on graphs", "author": ["Karsten M. Borgwardt", "Hans-Peter Kriegel"], "venue": "In Proc. Intl. Conf. Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "In Proceedings  of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Convolution kernels for natural language", "author": ["M. Collins", "N. Duffy"], "venue": "T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 625\u2013632, Cambridge, MA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "R", "author": ["A.K. Debnath"], "venue": "L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J Med Chem, 34:786\u2013797", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "Distinguishing enzyme structures from non-enzymes without alignments", "author": ["P.D. Dobson", "A.J. Doig"], "venue": "J Mol Biol,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P.A. Flach", "S. Wrobel"], "venue": "B. Sch\u00f6lkopf and M. K. Warmuth, editors, Proc. Annual Conf. Computational Learning Theory, pages 129\u2013143. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["Sharon Goldwater", "Tom Griffiths", "Mark Johnson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Producing power-law distributions and damping word frequencies with two-stage language models", "author": ["Sharon Goldwater", "Thomas L Griffiths", "Mark Johnson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Exploring network structure, dynamics, and function using networkx", "author": ["Aric Hagberg", "Pieter Swart", "Daniel S Chult"], "venue": "Technical report, Los Alamos National Laboratory (LANL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Convolution kernels on discrete structures", "author": ["David Haussler"], "venue": "Technical Report UCS-CRL-99-10, UC Santa Cruz,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Cyclic pattern kernels for predictive graph mining", "author": ["T. Horvath", "T. G\u00e4rtner", "S. Wrobel"], "venue": "Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 158\u2013167", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Reducing kernel matrix diagonal dominance using semi-definite programming", "author": ["Jaz Kandola", "Thore Graepel", "John Shawe-Taylor"], "venue": "In Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Kernels for graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "K. Tsuda, B. Sch\u00f6lkopf, and J.P. Vert, editors, Kernels and Bioinformatics, pages 155\u2013170, Cambridge, MA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J. Kleinberg"], "venue": "Journal of the ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Improved backing-off for M-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proc. ICASSP", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The skew spectrum of graphs", "author": ["Risi Kondor", "Karsten Borgwardt"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Fast kernels for inexact string matching", "author": ["C. Leslie", "R. Kuang"], "venue": "Proc. Annual Conf. Computational Learning Theory", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "W.S. Noble"], "venue": "Proceedings of the Pacific Symposium on Biocomputing, pages 564\u2013575, Singapore", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Nauty user\u00e2\u0102\u0179s guide (version 2.4)", "author": ["Brendan D McKay"], "venue": "Computer Science Dept., Australian National University,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical report,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "The two-parameter poisson-dirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor"], "venue": "Annals of Probability, 25(2):855\u2013900", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "Biological network comparison using graphlet degree distribution", "author": ["N. Przulj"], "venue": "European Conference on Computational Biology (ECCB),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J. Ramon", "T. G\u00e4rtner"], "venue": "Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD\u201903)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling cellular machinery through biological network comparison", "author": ["R. Sharan", "T. Ideker"], "venue": "Nature Biotechnology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Fast subtree kernels on graphs", "author": ["Nino Shervashidze", "Karsten Borgwardt"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["Nino Shervashidze", "S.V.N. Vishwanathan", "Tobias Petri", "Kurt Mehlhorn", "Karsten Borgwardt"], "venue": "Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels", "author": ["Nino Shervashidze", "Pascal Schweitzer", "Erik Jan Van Leeuwen", "Kurt Mehlhorn", "Karsten M Borgwardt"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Hash kernels", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.J. Smola", "A. Strehl", "S.V.N. Vishwanathan"], "venue": "M. Welling and D. van Dyk, editors, Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Yee Whye Teh"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Statistical evaluation of the predictive toxicology challenge 2000-2001", "author": ["H. Toivonen", "A. Srinivasan", "R.D. King", "S. Kramer", "C. Helma"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}, {"title": "Fast kernels for string and tree matching", "author": ["S.V.N. Vishwanathan", "A.J. Smola"], "venue": "S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 569\u2013576. MIT Press, Cambridge, MA", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Karsten Borgwardt", "author": ["S.V.N. Vishwanathan"], "venue": "and Nicol N. Schraudolph. Fast computation of graph kernels. In B. Sch\u00f6lkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing Systems 19, Cambridge MA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Nicol N", "author": ["S.V.N. Vishwanathan"], "venue": "Schraudolph, Imre Risi Kondor, and Karsten M. Borgwardt. Graph kernels. Journal of Machine Learning Research", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison of descriptor spaces for chemical compound retrieval and classification", "author": ["Nikil Wale", "Ian A Watson", "George Karypis"], "venue": "Knowledge and Information Systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "State of the art of graphbased data mining", "author": ["Takashi Washio", "Hiroshi Motoda"], "venue": "SIGKDD Explorations,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2003}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Trans. Inf. Syst., 22(2):179\u2013214", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 31, "context": "[32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors.", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "In this paper, we are interested in comparing graphs by computing a kernel between graphs [39].", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.", "startOffset": 93, "endOffset": 100}, {"referenceID": 29, "context": "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.", "startOffset": 120, "endOffset": 123}, {"referenceID": 40, "context": "Graph kernels are popular because many datasets from diverse domains such as bio-informatics [3, 30], chemo-informatics [2], and web data mining [41] naturally can be represented as graphs.", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "The kernels proposed by [21] are a notable exception.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "[3] (see Vishwanathan et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "[39] for an efficient algorithm for computing this kernel).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].", "startOffset": 59, "endOffset": 62}, {"referenceID": 15, "context": "Other popular motifs include subtrees [31], shortest paths [5], and cycles [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The motif used in this kernel is the set of unique sub-graphs of size k, which were christened as graphlets by Przulj [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "This is well known as the diagonal dominance problem in the machine learning community [17], and the resulting kernel matrix is close to the identity matrix.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "First, we propose a new smoothing technique for graphlets which is inspired by Kneser-Ney smoothing [20] used for language models in natural language processing.", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "Second, we provide a novel Bayesian version of our model that is extended from the Hierarchical Pitman-Yor process of Teh [35].", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "They were introduced by Przulj [28] to design a new measure of local structural similarity between biological networks.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Plots are generated with NetworkX library [14].", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "This issue occurs in a number of different domains and therefore, unsurprisingly, has received significant research attention [42]; smoothing methods are typically used to address this problem.", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Kneser-Ney smoothing is a fallback based smoothing method which has been identified as the state-of-the-art smoothing in natural language processing by several studies [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 24, "context": "Therefore, we first exhaustively generate all possible graphs of size k (this involves a one timeO(2) effort), and use Nauty [25] to obtain their canonically-labelled isomorphic representations.", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "Image is generated by Gephi [1].", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "For example, in the case of structured graphs such as social networks, one might benefit from weighting the edges according to the PageRank [26] score of the nodes.", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "Similarly, other link analysis algorithms such as Hubs or Authority given by HITS algorithm [19] can be used in order to exploit the domain knowledge.", "startOffset": 92, "endOffset": 96}, {"referenceID": 34, "context": "We will only give a very high level overview of a Pitman-Yor process and refer the reader to the excellent papers by Teh [35] and [12] for more details.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "We will only give a very high level overview of a Pitman-Yor process and refer the reader to the excellent papers by Teh [35] and [12] for more details.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "models [7], [24].", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": "models [7], [24].", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "[13] and Teh [35] independently showed that Kneser-Ney can be explained in a Bayesian setting by using the Pitman-Yor Process (PYP) [27].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Graph kernels can be considered as special cases of convolutional kernels proposed by Haussler [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 115, "endOffset": 118}, {"referenceID": 15, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 166, "endOffset": 170}, {"referenceID": 30, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 172, "endOffset": 176}, {"referenceID": 33, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 178, "endOffset": 182}, {"referenceID": 28, "context": "In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths [11], [18],[5], graph kernels based on limited-size subgraphs [16], [31], [34] and graph kernels based on subtree patterns [29].", "startOffset": 227, "endOffset": 231}, {"referenceID": 32, "context": "[33] performs a relaxation on the vertices and exploit labeling information embedded in the graphs to derive their so-called Weisfeiler-Lehman kernels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] addresses the sparsity problem by applying a sparse projection into a lower dimensional space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 160, "endOffset": 164}, {"referenceID": 33, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 211, "endOffset": 215}, {"referenceID": 17, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 217, "endOffset": 221}, {"referenceID": 37, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 223, "endOffset": 227}, {"referenceID": 4, "context": "To compare the efficacy of our approach, we compare our KneserNey and Pitman-Yor smoothed kernels with state-of-the-art graph kernels namel the graphlet kernel [32], the hash kernel [34], the random walk kernel [11], [18], [38], and the shortest path kernel [5].", "startOffset": 258, "endOffset": 261}, {"referenceID": 34, "context": "We adopted Markov chain Monte Carlo sampling based inference scheme for the hierarchical Pitman-Yor language model from [35] and modified the open source implementation of HPYP from https://github.", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "MUTAG [9] is a binary data set of 188 mutagenic aromatic and heteroaromatic nitro compounds, labeled whether they have mutagenicity in Salmonella typhimurium.", "startOffset": 6, "endOffset": 9}, {"referenceID": 35, "context": "The Predictive Toxicology Challenge (PTC) [36] dataset is a chemical compound dataset that reports the carcinogenicity for male and female rats.", "startOffset": 42, "endOffset": 46}, {"referenceID": 39, "context": "NCI1 and NCI109 [40], (http://pubchem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Enzymes is a data set of protein tertiary structures obtained from [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "DD [10] is a data set of protein structures where each protein is represented by a graph and nodes are amino acids that are connected by an edge if they are less than 6 Angstroms apart.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Therefore, we use random sampling, as advocated by [32], in order to obtain an empirical distribution of graphlet counts that is close to the actual distribution of graphlets in the graph.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": ", 8} we randomly sampled 10,000 sub-graphs, and used Nauty [25] to get canonically-labeled isomorphic representations which are then used to construct the frequency representation.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "We performed 5-fold cross-validation with C-Support Vector Machine Classification using LibSVM [6], using 4 folds for training and 1 for testing.", "startOffset": 95, "endOffset": 98}, {"referenceID": 34, "context": "Teh [35] shows that Pitman-Yor yields a better performance if one tune the hyperparameters.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].", "startOffset": 84, "endOffset": 96}, {"referenceID": 22, "context": "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].", "startOffset": 84, "endOffset": 96}, {"referenceID": 36, "context": "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].", "startOffset": 84, "endOffset": 96}, {"referenceID": 7, "context": "We are currently investigating the applicability of our framework to string kernels [22, 23, 37] and tree kernels [8].", "startOffset": 114, "endOffset": 117}], "year": 2014, "abstractText": "A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.", "creator": "LaTeX with hyperref package"}}}