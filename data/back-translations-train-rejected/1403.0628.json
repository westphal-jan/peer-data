{"id": "1403.0628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2014", "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations", "abstract": "We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of $\\mathcal{O}\\Big(U \\sqrt{T \\log(U \\sqrt{T} \\log^2 T +1)}\\Big)$ where $U$ is the $L_2$ norm of a comparator, and both $T$ and $U$ are unknown to the player. This bound is optimal up to $\\sqrt{\\log \\log T}$ terms. When $T$ is known, we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown $T$ case, a Normal approximation to the conditional value of the game proves to be the key analysis tool.", "histories": [["v1", "Mon, 3 Mar 2014 23:06:24 GMT  (23kb)", "https://arxiv.org/abs/1403.0628v1", null], ["v2", "Wed, 21 May 2014 16:17:09 GMT  (23kb)", "http://arxiv.org/abs/1403.0628v2", "Proceedings of the 27th Annual Conference on Learning Theory (COLT 2014)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan", "francesco orabona"], "accepted": false, "id": "1403.0628"}, "pdf": {"name": "1403.0628.pdf", "metadata": {"source": "CRF", "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations", "authors": ["H. Brendan McMahan", "Francesco Orabona"], "emails": ["mcmahan@google.com", "orabona@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 3.06 28v2 [cs.LG] an algorithm that provides a limit of repentance of O (U \u221a T log (U \u221a T log2 T + 1)), where U is the L2 standard of any comparator and both T and U are unknown to the player. This limit is optimal up to \u221a log T terms. If T is known, we derive an algorithm with an optimal limit of repentance (up to constant factors). For both the known and unknown T case, a Normal approach to the conditional value of the game proves to be the most important analysis tool."}, {"heading": "1 Introduction", "text": "The online learning framework provides a scalable and flexible approach to modeling a wide range of prediction problems, including classification, regression, ranking and portfolio management. Online algorithms work in rounds where a new instance is given in each round and the algorithm makes a prediction. Then, the environment reveals the instance label, and the learning algorithm updates its internal hypothesis. The goal of the learners is to minimize the cumulative loss they suffer as a result of their prediction errors. Research in this area focuses mainly on developing new prediction strategies and testing theoretical guarantees for them. However, a minimax analysis has recently been proposed as a general tool to design optimal prediction strategies [Rakhlin et al, 2012, McMahan and Abernethy, 2013]. The problem is considered a sequential zero-sum game between the player (the learner) and an opponent (the environment provides optimal strategies for both)."}, {"heading": "2 Notation and Problem Formulation", "text": "It is the only option we have. < p > This is the only option we have. < p (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (.S (0).S (0).S).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S (0).S \".S\".S (.S).S).S (.S).S).S (.S).S (0).S \".S\".S \"(.S).S\" (.S).S \"S\" (.S).S \"S\" (.S).S \"S\" (.S).S \"S\" (.S).S \"S (.S).S (.S\" S).S (.S \"S).S (.S\" S).S (.S \"S).S (.S\" S).S \"(.S).S (.S\" S).S (.S \"S\" S).S \"(.S).S\" (.S \"S).S\" (.S).S (.S).S \"(.S\" S \"S\" (.S).S (.S).S (.S (.S).S (.S).S \"S (.S).S\" S (.S \"S (.S).S (.S\" S (.S).S (.S).S (.S).S (.S \"S).S\").S (.S (.S (.S \"S).S (.S\").S).S (.S (.S).S \""}, {"heading": "3 Potential Functions and the Duality of Reward and Regret", "text": "In the current section, we will review some existing results in online learning theory and provide a number of mild generalizations for our purposes. (...) Potential functions play a significant role in the definition and analysis of online learning algorithms (...). (...) We will consider the potential and key assumptions that should depend exclusively on cumulative gradients g1: 2, and that these functions are convex in this reasoning. (...) Since our goal is adaptive algorithms, we often look at a sequence of changing potential functions q1,. (...) qT, each of which takes as an argument \u2212 g1 and is convex. These functions have appeared with different interpretations in many papers, with different emphasis. They can be considered as 1) the conjugation of an (implicit) time-varying regulator in a mirror descent or follow-the-the-regularized leader (FTL)."}, {"heading": "4 Minimax Analysis Approaches for Known-Horizon Games", "text": "In general, the problem of calculating the conditional value of a game is Vt (\u03b8) = conservation. And even for a known potential, deriving an optimal solution via (14) is generally a tough problem. If the player is not restricted, we can simplify the calculation of Vt and deriving optimal strategies. < The following ideas from McMahan and Abernethy [2013], as well as the number of probability distributions on G. McMahan and Abernethy [2013] show that in some cases it is possible to easily calculate this maximum, especially if G = [\u2212 G] d and qt decompose the function on a pro-coordinate field (i.e. if the problem is essentially d independent, one-dimensional problems)."}, {"heading": "4.1 The case of the orthogonal adversary", "text": "Let B (270) = f (270) for an increasing convex function f > > > 2 (2), and define (2) = f (2) for a constant C > 0. Also note that ft (2) = B (3). Our first key result is the following: Theorem 4. Let the opponent of G = {g: 0) play, and assume that all ft satisfaction is in w max g), G < w, g > + ft + 1 (3). Our first key result is the following: Theorem 4. Let the opponent of G = {g: 2) play, and take all ft satisfaction in w max g, G < w, g > + ft + 1 (3)."}, {"heading": "4.2 The case of the parallel adversary, and Normal approximations", "text": "The results of McMahan and Abernethy [2013] may be considered a special case for the results in this section. First, we introduce a notation. We write \"T \u2212 t,\" if T and t are clear from the context. We write \"r,\" around \"r\" is a Rademacher random variable, and \"r\" is the sum of the IID random variables. We write \"r,\" \"r\" and \"r\" is a Rademacher random variable, and \"r.\" (\u2212 1) is the sum of the IID random variables. (Let \"s\"), \"s\" and \"s,\" \"s\" and \"s,\" \"\" s \"and.\" (T \"t\") is the sum of the IID random variables with the distribution N (0, \u03c32) and \"s.\" (T \"t\"), \"s\" and \"s.\""}, {"heading": "5 A Power Family of Minimax Algorithms", "text": "We analyze a family of algorithms based on the potentials B (\u03b8) = f (\u03b8) = q), where f (x) = Wp | p for the parameters W > 0 and p [1, 2], if the dimension is at least two. This is reminiscent of p-standard algorithms [Gentiles, 2003], but the connection is superficial - the standard we use to measure the value of our Hilbert space is always the norm of our Hilbert space. Our main result is: Corollary 9. Let d > 1 and W > 0, and let f and B be defined as above. Define ft (x) = Wp (x2 + (T) G) p / 2. Then ft (T) is the conditional value of the game, and the optimal strategy is as in Theorem 4. If p (1, 2), let q \u00b2 2 so that we allow it."}, {"heading": "6 Tight Bounds for Unconstrained Learning", "text": "This type of regulation is particularly interesting because it is related to parameter-free algorithms; a similar potential function has been used for a parameter-free algorithm. (...) This type of regulation is particularly interesting because it is related to parameter-free algorithms. (...) The algorithms are based on parameter-free algorithms. (...) This type of regulation is particularly interesting because it is based on parameter-free algorithms. (...) The algorithms are based on parameter-free algorithms. (...) The algorithms are based on parameter-free algorithms. (...) This type of regulation is particularly interesting because it is based on parameter-free algorithms. (...) The algorithms are based on parameter-free algorithms. (...) The algorithms are based on parameter-free algorithms. (...) This type of regulation is particularly interesting because it is based on parameter-free algorithms."}, {"heading": "6.2 AdaptiveNormal: an adaptive algorithm for unknown T", "text": "Our techniques suggest the following recipe for developing adaptive algorithms: Analyze the known T-case, define a potential qt (\u03b8) \u2248 VT (\u03b8), and then analyze the incremental-optimal algorithm for thipotentiential (14) via theorem 1. We follow this recipe in the current section. Consider again the game in which an opponent of G = {g-H-G-G-G-G-G-G-G. Define the function ft asft (x) = \u03b2t exp (x2at), where a > 3\u03c0G 24 is a decreasing sequence specified below. From this we define the potential qt-G-2 = ft-H-2). Provided we play the incremental-optimal algorithm of (14). Using Lemma 8, we can write the minimax value for the one-round game and the minimum game value X-write."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "Suppose the algorithm offers the reward guarantee (4). First, we note that for each comparator we have Regret (u) = \u2212 Reward \u2212 < g1: T, u >. (21) Then, when applying the definitions of Reward, Regret and the fennel conjugate, we have Regret (u) = \u03b8T \u00b7 u \u2212 Reward By (21) \u2264 \u03b8T \u00b7 u \u2212 qT (\u03b8T) + 1: T Assuming (5) we have u, Reward = successT \u00b7 u \u2212 Regret (u) By (21) = maxv \u2212 Regret (v) versus max v \u2212 qT (v) \u2212 Alternative provided (5)."}, {"heading": "A.2 Proof of Lemma 3", "text": "Proof: We have B \u0445 (u) = sup\u03b8 < u, \u03b8 > \u2212 f (\u043a \u03b8). If we consider the given equality to be correct, this applies in factB \u0445 (u) = sup \u03b8 \u2212 f (\u043a \u03b8) = sup \u03b1 \u2265 0 \u2212 f (\u03b1) = sup \u03b1 R \u2212 f (\u03b1) = f \u0445 (0). Therefore, we can assume that we consider the equality to be 6 = 0, and by inspection we can make the assumption that both values are equal."}, {"heading": "A.3 Proof of Theorem 4", "text": "First, we show that if f meets the condition on the derivatives, the same conditions are met by ft, for all t. We have that all ft have the form h (x) = f (\u221a x2 + a), where a \u2265 0. Therefore, we must prove that xh \"(x) h\" (x) h \"(x) \u2264 1. We have that h\" (x) = xf \"(\u221a x2 + a) \u221a x2 + a, and h\" (x) = x2f \"(x2 + af\") \u2032 (\u221a x2 + a) + a \"(\u221a x2 + a) x2 + a, soxh\" (x) h \"(x) = x2f\" \u2212 \u2212 (\u221a x2 + a) \u221a x2 + af \"(\u221a x2 + af\") \u2032 (\u04452 + a) (x2 + a)."}, {"heading": "A.4 Proof of Theorem 6", "text": "Proof that we are the functions ft and f of (18) are even (1). Let us be a --random variable -2 (2) -2 (2) -2 (2) -2 (2) -2 (2) -2 (2) -3 (2) -3 (2) -3 (2) -3 (2) -4 (2) -4 (2) -4 (2) -4 (2) -4 (2) -4 (2) -4 (2) -3 (2) -4 (2) -4 (2) -3 (2) -4 (2) -4 (2) -3 (2) -3) -4 (2) -2 (2) -4 (2) -4 (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (5) -5 (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4 (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) ("}, {"heading": "A.6 Lemma 14", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "A.7 Lemma 17", "text": "Lemma 15. Let f (x) = b exp (x2a) \u2212 exp (x2c) If a \u2265 c > 0, b \u2265 0, and b c \u2264 a, then the function f (x) is decrease for x \u2265 0.Proof. then f (t) \u2264 1 for any t \u2265 0.Proof. Let f (t) = a 3 2 t \u221a t + 1 (a (t + 1) \u2212 b) 3 2, with a \u2265 3 / 2b > 0. Then f (t) \u2264 1 for any t \u2265 0.Proof. The character of the first derivative of the function has the same sign of (2 a \u2212 3b) (t + 1) + b, therefore from the hypothesis on a and b the function is strictly increasing. Also, the asymptote for t \u2192 \u043c 1, therefore we have the given upper limit of (1).Lemma 17. Let (x) \u2212 \u2212 \u2212 exp (x), \u03b2t ofof.ft, if."}], "references": [{"title": "Repeated games against budgeted adversaries", "author": ["J. Abernethy", "M.K. Warmuth"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Abernethy and Warmuth.,? \\Q2010\\E", "shortCiteRegEx": "Abernethy and Warmuth.", "year": 2010}, {"title": "Continuous experts and the Binning algorithm", "author": ["J. Abernethy", "J. Langford", "M.K. Warmuth"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT06),", "citeRegEx": "Abernethy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2007}, {"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["J. Abernethy", "P.L. Bartlett", "A. Rakhlin", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Optimal strategies from random walks", "author": ["J. Abernethy", "M.K. Warmuth", "J. Yellin"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Euler\u2019s constant, Taylor\u2019s formula, and slowly converging series", "author": ["J.V. Baxley"], "venue": "Mathematics Magazine,", "citeRegEx": "Baxley.,? \\Q1992\\E", "shortCiteRegEx": "Baxley.", "year": 1992}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A parameter-free hedging algorithm", "author": ["K. Chaudhuri", "Y. Freund", "D. Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "The robustness of the p-norm algorithms", "author": ["C. Gentile"], "venue": "Machine Learning,", "citeRegEx": "Gentile.,? \\Q2003\\E", "shortCiteRegEx": "Gentile.", "year": 2003}, {"title": "General convergence results for linear discriminant updates", "author": ["A.J. Grove", "N. Littlestone", "D. Schuurmans"], "venue": "Machine Learning,", "citeRegEx": "Grove et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Grove et al\\.", "year": 2001}, {"title": "Relative loss bounds for multidimensional regression problems", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Kivinen and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 2001}, {"title": "Minimax optimal algorithms for unconstrained linear optimization", "author": ["H.B. McMahan", "J. Abernethy"], "venue": "In NIPS,", "citeRegEx": "McMahan and Abernethy.,? \\Q2013\\E", "shortCiteRegEx": "McMahan and Abernethy.", "year": 2013}, {"title": "Dimension-free exponentiated gradient", "author": ["F. Orabona"], "venue": "In NIPS,", "citeRegEx": "Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Orabona.", "year": 2013}, {"title": "A generalized online mirror descent with applications to classification and regression", "author": ["F. Orabona", "K. Crammer", "N. Cesa-Bianchi"], "venue": null, "citeRegEx": "Orabona et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2013}, {"title": "Lecture notes on online learning", "author": ["A. Rakhlin"], "venue": "Technical report,", "citeRegEx": "Rakhlin.,? \\Q2009\\E", "shortCiteRegEx": "Rakhlin.", "year": 2009}, {"title": "Localization and adaptation in online learning", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In AISTATS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2013}, {"title": "Relax and randomize: From value to algorithms", "author": ["S. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In NIPS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Online learning: Theory, algorithms, and applications", "author": ["S. Shalev-Shwartz"], "venue": "Technical report, The Hebrew University,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["M. Streeter", "H.B. McMahan"], "venue": "In NIPS,", "citeRegEx": "Streeter and McMahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and McMahan.", "year": 2012}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "In NIPS,", "citeRegEx": "Xiao.,? \\Q2009\\E", "shortCiteRegEx": "Xiao.", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": ", 2008a], in others upper bounds on the value of the game (often based on the sequential Rademacher complexity) are used to construct efficient algorithms with theoretical guarantees [Rakhlin et al., 2012].", "startOffset": 183, "endOffset": 205}, {"referenceID": 1, "context": "In some cases the value of the game can be calculated exactly in an efficient way [Abernethy et al., 2008a], in others upper bounds on the value of the game (often based on the sequential Rademacher complexity) are used to construct efficient algorithms with theoretical guarantees [Rakhlin et al., 2012]. While most of the work in this area has focused on the setting where the player is constrained to a bounded convex set [Abernethy et al., 2008a] (with the notable exception of McMahan and Abernethy [2013]), in this work we are interested in the general setting of unconstrained online learning with", "startOffset": 83, "endOffset": 511}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game.", "startOffset": 36, "endOffset": 65}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game. In particular, we show that under these conditions the optimal play of the adversary is always orthogonal or always parallel to the sum of his previous plays, while the optimal play of the player is always parallel. On the other hand, for some cases where the exact minimax strategy is hard to characterize, we introduce a new relaxation procedure based on a Normal approximation. In the particular application of interest, we show the relaxation is strong enough to yield an optimal regret bound, up to constant factors. In Section 5, we use our new tools to recover and extend previous results on minimax strategies for linear online learning, including results for bounded domains. In fact, we show how to obtain a family of minimax strategies that smoothly interpolates between the minimax algorithm for a bounded feasible set and a minimax optimal algorithm in fact equivalent to unconstrained gradient descent. We emphasize that all the algorithms from this family are exactly minimax optimal,1 in a sense we will make precise in the next section. Moreover, if you are allowed to play outside of the comparator set, we show that some members of this family have a non-vacuous regret bound for the unconstrained setting, while remaining optimal for the constrained one. When studying unconstrained problems, a natural question is how small we can make the dependence of the regret bound on U , the L2 norm of an arbitrary comparator point, while still maintaining a \u221a T dependency on the time horizon. The best algorithm from the above family achieves Regret(U) \u2264 1 2 (U + 1) \u221a T . Streeter and McMahan [2012] and Orabona [2013] show it is possible to reduce the dependence on U to O(U logUT ).", "startOffset": 36, "endOffset": 1854}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game. In particular, we show that under these conditions the optimal play of the adversary is always orthogonal or always parallel to the sum of his previous plays, while the optimal play of the player is always parallel. On the other hand, for some cases where the exact minimax strategy is hard to characterize, we introduce a new relaxation procedure based on a Normal approximation. In the particular application of interest, we show the relaxation is strong enough to yield an optimal regret bound, up to constant factors. In Section 5, we use our new tools to recover and extend previous results on minimax strategies for linear online learning, including results for bounded domains. In fact, we show how to obtain a family of minimax strategies that smoothly interpolates between the minimax algorithm for a bounded feasible set and a minimax optimal algorithm in fact equivalent to unconstrained gradient descent. We emphasize that all the algorithms from this family are exactly minimax optimal,1 in a sense we will make precise in the next section. Moreover, if you are allowed to play outside of the comparator set, we show that some members of this family have a non-vacuous regret bound for the unconstrained setting, while remaining optimal for the constrained one. When studying unconstrained problems, a natural question is how small we can make the dependence of the regret bound on U , the L2 norm of an arbitrary comparator point, while still maintaining a \u221a T dependency on the time horizon. The best algorithm from the above family achieves Regret(U) \u2264 1 2 (U + 1) \u221a T . Streeter and McMahan [2012] and Orabona [2013] show it is possible to reduce the dependence on U to O(U logUT ).", "startOffset": 36, "endOffset": 1873}, {"referenceID": 1, "context": "Regret bounds for known-T algorithms (A) Minimax Regret \u221a T for u \u2208 W , O(T ) otherwise Abernethy et al. [2008a] (B) OGD, fixed \u03b7 1 2 (1 + U) \u221a T E.", "startOffset": 88, "endOffset": 113}, {"referenceID": 1, "context": "Regret bounds for known-T algorithms (A) Minimax Regret \u221a T for u \u2208 W , O(T ) otherwise Abernethy et al. [2008a] (B) OGD, fixed \u03b7 1 2 (1 + U) \u221a T E.g., Shalev-Shwartz [2012] (C) pq-Algorithm (", "startOffset": 88, "endOffset": 174}, {"referenceID": 16, "context": "Theorem 11 Regret bounds for adaptive algorithms for unknown T (G) Adaptive FTRL/RDA (1 + 1 2 U) \u221a T Shalev-Shwartz [2007], Xiao [2009] (H) Dim.", "startOffset": 101, "endOffset": 123}, {"referenceID": 16, "context": "Theorem 11 Regret bounds for adaptive algorithms for unknown T (G) Adaptive FTRL/RDA (1 + 1 2 U) \u221a T Shalev-Shwartz [2007], Xiao [2009] (H) Dim.", "startOffset": 101, "endOffset": 136}, {"referenceID": 10, "context": "Following McMahan and Abernethy [2013], we generalize the game in terms of a generic convex benchmark function B : H \u2192 R, instead of using the definition (1).", "startOffset": 10, "endOffset": 39}, {"referenceID": 5, "context": "and analysis of online learning algorithms [Cesa-Bianchi and Lugosi, 2006].", "startOffset": 43, "endOffset": 74}, {"referenceID": 15, "context": "They can be viewed as 1) the conjugate of an (implicit) time-varying regularizer in a Mirror Descent or Follow-the-Regularized-Leader (FTRL) algorithm [Cesa-Bianchi and Lugosi, 2006, Shalev-Shwartz, 2007, Rakhlin, 2009], 2) as proxy for the conditional value of the game in a minimax setting [Rakhlin et al., 2012], or 3) a potential giving a bound on the amount of reward we want the algorithm to have obtained at the end of round t [Streeter and McMahan, 2012, McMahan and Abernethy, 2013].", "startOffset": 292, "endOffset": 314}, {"referenceID": 11, "context": "6] and Orabona [2013], the key is showing the sum of these \u01eb\u0302t terms is always bounded by a constant.", "startOffset": 7, "endOffset": 22}, {"referenceID": 11, "context": "where we view q\u2217 t as a time-varying regularizer (see Orabona et al. [2013] and references therein).", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": "When the regularizer q\u2217 is fixed, that is, qt = q for all t for some convex function q, we get the approach pioneered by Grove et al. [2001] and Kivinen and Warmuth [2001]: \u01ebt(\u03b8t, gt) = q(\u03b8t)\u2212 q(\u03b8t\u22121) + \u3008wt, gt\u3009 = q(\u03b8t)\u2212 ( q(\u03b8t\u22121) + \u3008\u2207q(\u03b8t\u22121), gt\u3009 ) = Dq(\u03b8t, \u03b8t\u22121), where Dq is the Bregman Divergence with respect to q, and we predict with wt = \u2207q(\u03b8t\u22121).", "startOffset": 121, "endOffset": 141}, {"referenceID": 8, "context": "When the regularizer q\u2217 is fixed, that is, qt = q for all t for some convex function q, we get the approach pioneered by Grove et al. [2001] and Kivinen and Warmuth [2001]: \u01ebt(\u03b8t, gt) = q(\u03b8t)\u2212 q(\u03b8t\u22121) + \u3008wt, gt\u3009 = q(\u03b8t)\u2212 ( q(\u03b8t\u22121) + \u3008\u2207q(\u03b8t\u22121), gt\u3009 ) = Dq(\u03b8t, \u03b8t\u22121), where Dq is the Bregman Divergence with respect to q, and we predict with wt = \u2207q(\u03b8t\u22121).", "startOffset": 121, "endOffset": 172}, {"referenceID": 13, "context": "Admissible relaxations and potentials We extend the notion of relaxations of the conditional value of the game of Rakhlin et al. [2012] to the present setting.", "startOffset": 114, "endOffset": 136}, {"referenceID": 13, "context": "(4) of Rakhlin et al. [2012] if we force all \u01eb\u0302t = 0, but if we allow some slack \u01eb\u0302t, (13) corresponds exactly to (8) and (9).", "startOffset": 7, "endOffset": 29}, {"referenceID": 13, "context": "following Rakhlin et al. [2012, Eq. (5)], Rakhlin et al. [2013], and Streeter and McMahan [2012, Eq.", "startOffset": 10, "endOffset": 64}, {"referenceID": 10, "context": "For example, following ideas from McMahan and Abernethy [2013],", "startOffset": 34, "endOffset": 63}, {"referenceID": 10, "context": "McMahan and Abernethy [2013] shows that in some cases is possible to easily calculate this maximum, in particular when G = [\u2212G,G]d and qt decomposes on a per-coordinate spaces (that is, when the problem is essentially d independent, one-dimensional problems).", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "The results of McMahan and Abernethy [2013] can be viewed as a special case of the results in this section.", "startOffset": 15, "endOffset": 44}, {"referenceID": 7, "context": "This is reminiscent of p-norm algorithms [Gentile, 2003], but the connection is superficial\u2014the norm we use to measure \u03b8 is always the norm of our Hilbert space.", "startOffset": 41, "endOffset": 56}, {"referenceID": 1, "context": "The p = 1 case in fact exactly recaptures the result of Abernethy et al. [2008a] for linear functions, extending it also to spaces of dimension equal to two.", "startOffset": 56, "endOffset": 81}, {"referenceID": 11, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al.", "startOffset": 125, "endOffset": 140}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009].", "startOffset": 214, "endOffset": 238}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009]. The lower bound for this game was proven in Streeter and McMahan [2012] for 1-dimensional spaces, and Orabona [2013] extended it to Hilbert spaces and improved the leading constant.", "startOffset": 215, "endOffset": 312}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009]. The lower bound for this game was proven in Streeter and McMahan [2012] for 1-dimensional spaces, and Orabona [2013] extended it to Hilbert spaces and improved the leading constant.", "startOffset": 215, "endOffset": 357}, {"referenceID": 4, "context": "Thus, choosing \u03b2t = \u01eb/ log (t + 1), for example, is sufficient to prove that \u01eb1:T is bounded by \u01eb 2 a [Baxley, 1992].", "startOffset": 102, "endOffset": 116}], "year": 2014, "abstractText": "We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of O (", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}