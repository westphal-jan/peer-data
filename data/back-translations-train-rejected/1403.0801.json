{"id": "1403.0801", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring", "abstract": "Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach.", "histories": [["v1", "Tue, 4 Mar 2014 14:45:56 GMT  (33kb,D)", "https://arxiv.org/abs/1403.0801v1", null], ["v2", "Wed, 5 Mar 2014 14:50:06 GMT  (33kb,D)", "http://arxiv.org/abs/1403.0801v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["derrick higgins", "chris brew", "michael heilman", "ramon ziai", "lei chen", "aoife cahill", "michael flor", "nitin madnani", "joel tetreault", "daniel blanchard", "diane napolitano", "chong min lee", "john blackmore"], "accepted": false, "id": "1403.0801"}, "pdf": {"name": "1403.0801.pdf", "metadata": {"source": "CRF", "title": "Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring", "authors": ["Derrick Higgins", "Chris Brew", "Michael Heilman", "Ramon Ziai", "Lei Chen", "Aoife Cahill", "Michael Flor", "Nitin Madnani", "Joel Tetreault", "Daniel Blanchard", "Diane Napolitano", "Chong Min Lee", "John Blackmore"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves by going in search of themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2 Previous Work", "text": "Like the field of automated essay scoring, research on methods of automated short answer assessment has a history that spans several decades. As early as 1988, Carlson & Ward investigated the potential use of natural language processing for the \"formulation of hypotheses\" task, a new type of item taking into account the GRE test, which would require students to list all possible explanations they could think of if some observed phenomena (for example, a steady decrease in mortality rates for a particular population group) were taken into account. While this is a somewhat unique type of item, it is quite similar in its basic scoring characteristics (the fact that it is evaluated according to the accuracy or semantic appropriateness of a short textual unit), which were taken into account for many other \"short answer tasks.\" Research on automated short answer scoring continued in the Educational Testing Service during the early 1990s (Kaplan, 1991; Kaplan and 1994, and Bennett)."}, {"heading": "2.1 The Automated Student Assessment Prize", "text": "The idea behind it is that the idea is a project, which is a project, which is about how to give yourself and others a chance. (...) The idea behind it is that it is a project, which is about developing an idea. (...) The idea behind it is that it is a project, which is a project. (...) The idea behind it is that it is a project, which is a project. (...) The idea behind it is that it is the idea behind it, the idea behind it, and the idea behind it, and the idea behind it, and the idea behind it. (...) The idea behind it is that it is the idea behind it, the idea behind it is the idea behind it, and the idea behind it is the idea behind it, and the idea behind it is the idea behind it, and the idea behind it is the idea behind it. (...) The idea behind it is the idea behind it, the idea behind it is the idea behind it. \""}, {"heading": "3 Data", "text": "The data used in this study were the same as those used in the ASAP short answer scoring challenge described above. Students \"questions and answers in this data set were provided by several state education departments, although the names of participating states were not provided by the contest organizers. Organizers reported that students\" answers to some questions were entered on a computer, while answers to other questions were provided in handwritten form (and later transcribed).The data set contains ten different short answer questions, which differ in features such as topic area, average response length, and scoring scale, as shown in Table 2. All ASAP questions were administered in the U.S. in Grade 10, with the exception of question 10, which was administered in Grade 8. For each question, a section is also provided outlining the criteria for assigning results to answers. 55More detailed information about the ASAP questions and scoring guidelines are available on the ASAP website for the ASAP / aggle contest: http: / / www.askagg.com."}, {"heading": "4 Model", "text": "The features described in the following section have been used to train a variety of different regression models to estimate the score to be assigned to each answer: \u2022 simple linear regression with the fewest squares; \u2022 burr regression; \u2022 support for vector regression (with an RBF core); \u2022 random forests; and \u2022 gradient enhancement. Finally, it should be noted that the human correlation given here is significantly higher for some tasks than was observed in previously reported studies using short-answer questions. Further information on the scoring practices of the states would help to clarify whether the two human ratings of these answers were really provided independently in all cases; the parameters used in the model training were determined by cross-validation within the training setup.This structure is consistent with the popular stacking modeling approach used in the ASAP contest, where individual predictive features used by the top-level model are the same as the results of the classification of the 5 or the regression section itself."}, {"heading": "5 Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 BASE Features", "text": "The four classes of features described in this section form the \"BASE\" set, which is intended to represent the features commonly found in high-performance models of the ASAP challenge."}, {"heading": "5.1.1 Bag of Words Features", "text": "The first set of basic features included in our Batch Classifier is a feature that is itself the output of a model trained to predict human values based on specific words. Such a bag-of-words model with random forest regression was provided by the contest organizers as the basis for the ASAP challenge. Using the same methodology as the simple bag-of-words models, we have also included two features based on bag-of-words ngrams. Characteristics tend to be useful for this task as they capture lexical information in a way that is impervious to misspelling.Finally, we include two bag-of-words features that have been trained using the same method as above but pre-processed with the Porter Stemming Algorithm for all words. These three features together are hereinafter referred to as Bag-of-Words features (BOW)."}, {"heading": "5.1.2 LDA Features", "text": "For each question, two LDA theme rooms were constructed using the MALLET toolkit (McCallum, 2002), one with 30 dimensions and one with only ten. As a feature, the weight of each of these 40 topics was used for a given answer, which should be roughly comparable to the reduced lexical characteristics of the ASAP participants."}, {"heading": "5.1.3 Well-formedness Features", "text": "Five features were used to represent the degree to which each answer consisted of well-edited English grammatical text, based on the number of errors detected in four different categories, which were automatically identified by a grammatical error detection system (Attali and Burstein, 2006), and on a feature representing the sum of all error categories."}, {"heading": "5.1.4 Length Features", "text": "Three characteristics were used to represent length: the number of characters, words and sentences in each answer."}, {"heading": "5.2 Syntactically-informed Features", "text": "The four additional categories of characteristics described in this section are more \"syntactically informed\" than those in the BASE group, as they encode information about sequences of words, syntactical units, or units of discourse."}, {"heading": "5.2.1 Ngram Features", "text": "This feature class comprised three \"bag-of-ngram\" characteristics analogous to the Bag-of-Word characteristics described above. Each of these characteristics comprised the 1000 most common unagrams, bigrams, and trigrams in a regression model designed to predict human value. Random forests, support vector machines, and burr regression were used in the three feature variants."}, {"heading": "5.2.2 Language Model Features", "text": "Based on language models created with the IRST Language Modeling Toolkit (Federico et al., 2008), three characteristics were included: language models were trained based on the answers to each question that received the highest score, one of the two highest scores, and the score of zero. As a feature, the helplessness of an answer in relation to each of these models was used."}, {"heading": "5.2.3 Dependency Features", "text": "This feature class included six \"bag of dependency\" characteristics. Each of these characteristics included the 1000 most common dependency characteristics (and possibly other items; see below) derived from responses in the training set using the Stanford parser (Klein and Manning, 2003) in a regression model designed to predict the human score. Feature variants included models that included only dependencies, dependencies, and single words, and dependencies combined with \"partial\" dependencies (a lexical head associated with a dependency relationship, but not the other head associated with it). Each of the three variants was implemented with both random forests and support vector machines, yielding a total of six characteristics."}, {"heading": "5.2.5 Discourse Segment Features", "text": "This last category of characteristics comprised five characteristics based on a system for segmenting short answers into meaningful subunits based on regular expressions. This model analyzes sentences or sentences triggered by bullets, numbering or discourse connectors, as follows or so. The characteristics used are the number of total identified discourse units, the length of a numbered list, the highest number of identified discourse units associated with a numbered sphere, and the number of discourse units led by discourse markers, in two categories (the final category indicating a conclusion, and the other category indicating supplementary information)."}, {"heading": "6 Experiments", "text": "First, to demonstrate the strong performance of the ensemble model on this set of data relative to individual top-level regression models, we trained each model on the BASE feature list (see Section 5.1). As Table 3 shows, the ensemble of regressors exceeds each model on two of the three measures and is hardly trumped by the increase in gradients on the third measure. Table 3 aggregates the results across all ten ASAP questions using three different measures; the average correlation of the non-rounded predicted values with human results provides the most accurate measure of model performance because information is lost in the rounding process and in many test contexts, a non-rounded Item Point number could be directly used to construct the overall test results. \"The average square weighted kappa, calculated with rounded predictions, calculated using rounded predictions, is also used in the official measurement form, which we will use for comparison purposes."}, {"heading": "7 Conclusions and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Performance relative to ASAP systems", "text": "The first point to note in connection with the above-mentioned results is that the predictive accuracy of the final ensemble model on the ASAP ranking data is lower than the highest values achieved during the competition itself. The highest final weighted kappa value on the ASAP ranking was 0.772, comparing7As participants made their code available as open source, we were able to reproduce their models and generate more detailed statistical assessments.Square weighted kappa by question our result of 0.768. There are a number of reasons for this. First, stack models such as our ensemble model and the models used by the ASAP leaders can have a number of free parameters that can be iteratively manipulated to improve the performance of the models. Base level learning parameters and top level regression models can be adjusted, functions can be added or removed, and feature variants of the ASAP model can be introduced per day to optimize the ASAP results, as the two pre-results for each system have been published."}, {"heading": "7.2 Role of syntactically-informed features", "text": "The results presented in Table 4 show that syntactically better informed traits can actually contribute to improved short-response evaluation. Taken together, the use of all the new traits improves the aggregated weighted kappa by about 0.009, with the greatest increase being achieved by the inclusion of syntactic dependency information, the trait class with the clearest link to the syntax. This is particularly noteworthy because many student responses contain numerous spelling and grammar errors, making it difficult to reliably analyze them. However, the incremental predictive value of these traits was modest, but there may be motivation to include such traits in short-response evaluation systems that go beyond their empirical impact on accuracy. As the assessment rubrics for these vocabularies claim to be sensitive to characteristics of responses that go beyond simple word choice, including higher traits that reduce the risk that knowledge will actually be reduced to the degree of validity that the student's results are based on the evaluation alone."}, {"heading": "7.3 Future work", "text": "There are a number of areas where this work could be expanded. First, the use of syntactic dependencies does not, of course, come close to exhausting the space of trait types that more directly reflect the syntactic-semantic relationships that are intended to encode short answers. Features based on, for example, semantic role labeling and paraphrase recognition can offer additional benefits. There may also be some benefits when traits are tailored to a specific content area (such as biology or reading comprehension). Another important question is how the specific setting of the ASAP task affects the results obtained. Parameters such as the size of the training sample, the number of simultaneous tasks to be modeled, and the content areas to be evaluated could significantly influence the results."}], "references": [{"title": "Automated essay scoring with e-rater v.2", "author": ["Attali", "Burstein2006] Yigal Attali", "Jill Burstein"], "venue": "Journal of Technology, Learning, and Assessment,", "citeRegEx": "Attali et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Attali et al\\.", "year": 2006}, {"title": "Performance of a generic approach in automated essay scoring", "author": ["Attali et al.2010] Yigal Attali", "Brent Bridgeman", "Cathy Trapani"], "venue": "Journal of Technology, Learning, and Assessment,", "citeRegEx": "Attali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Attali et al\\.", "year": 2010}, {"title": "GE FRST evaluation report: How well does a statistically-based natural language processing system score natural language constructed responses", "author": ["Burstein", "Kaplan1995] Jill Burstein", "Randy M. Kaplan"], "venue": "ETS Research Report No. RR-95-29", "citeRegEx": "Burstein et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Burstein et al\\.", "year": 1995}, {"title": "CAA of short non-MCQ answers", "author": ["Jenny Jerrams-Smith", "Victor Soh"], "venue": "In Proceedings of the 5th International CAA Conference,", "citeRegEx": "Callear et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Callear et al\\.", "year": 2001}, {"title": "A new look at formulating hypotheses items", "author": ["Carlson", "Ward1988] Sybil B. Carlson", "William C. Ward"], "venue": "GRE Board Professional Report No. 85-14P; ETS Research Report", "citeRegEx": "Carlson et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 1988}, {"title": "Towards effective tutorial feedback for explanation questions: A dataset and baselines", "author": ["Rodney D. Nielsen", "Chris Brew"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter", "citeRegEx": "Dzikovska et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dzikovska et al\\.", "year": 2012}, {"title": "Intellimetric: From here to validity", "author": ["Scott Elliot"], "venue": null, "citeRegEx": "Elliot.,? \\Q2003\\E", "shortCiteRegEx": "Elliot.", "year": 2003}, {"title": "IRSTLM: an open source toolkit for handling large scale language models", "author": ["Nicola Bertoldi", "Mauro Cettolo"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Federico et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2008}, {"title": "AI scoring MSA science. Paper presented at the meeting of the Maryland Assessment Group. Ocean City, MD", "author": ["Foltz", "Lochbaum2010] Peter Foltz", "Karen Lochbaum"], "venue": null, "citeRegEx": "Foltz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 2010}, {"title": "Analysis of student writing for a large scale implementation", "author": ["Foltz et al.2011] Peter Foltz", "Karen Lochbaum", "Mark Rosenstein"], "venue": null, "citeRegEx": "Foltz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 2011}, {"title": "Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach", "author": ["Hahn", "Meurers2012] Michael Hahn", "Detmar Meurers"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,", "citeRegEx": "Hahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2012}, {"title": "Using the free-response scoring tool to automatically score the formulatinghypotheses item", "author": ["Kaplan", "Bennett1994] Randy M. Kaplan", "Randy E. Bennett"], "venue": "ETS Research Report No", "citeRegEx": "Kaplan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan et al\\.", "year": 1994}, {"title": "Using a trainable pattern-directed computer program to score natural language item responses", "author": ["Randy M. Kaplan"], "venue": "ETS Research Report No", "citeRegEx": "Kaplan.,? \\Q1991\\E", "shortCiteRegEx": "Kaplan.", "year": 1991}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Automated scoring and annotation of essays with the Intelligent Essay Assessor", "author": ["Darrell Laham", "Peter W. Foltz"], "venue": null, "citeRegEx": "Landauer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 2003}, {"title": "C-rater: Scoring of short-answer questions", "author": ["Leacock", "Chodorow2003] Claudia Leacock", "Martin Chodorow"], "venue": "Computers and the Humanities,", "citeRegEx": "Leacock et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 2003}, {"title": "Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu", "author": ["Andrew Kachites McCallum"], "venue": null, "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure", "author": ["Ramon Ziai", "Niels Ott", "Janina Kopp"], "venue": "In Proceedings of the TextInfer 2011 Workshop on Tex-", "citeRegEx": "Meurers et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Meurers et al\\.", "year": 2011}, {"title": "Towards robust computerized marking of free-text responses", "author": ["Terry Russell", "Peter Broomhead", "Nicola Aldridge"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2002}, {"title": "Learning to grade short answer questions using semantic similarity measures and dependency graph alignments", "author": ["Razvan Bunescu", "Rada Mihalcea"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Compu-", "citeRegEx": "Mohler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohler et al\\.", "year": 2011}, {"title": "Classification errors in a domain-independent assessment system", "author": ["Wayne Ward", "James H. Martin"], "venue": "In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Nielsen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2008}, {"title": "The imminence of grading essays by computer", "author": ["Ellis B. Page"], "venue": "Phi Delta Kappan,", "citeRegEx": "Page.,? \\Q1966\\E", "shortCiteRegEx": "Page.", "year": 1966}, {"title": "The use of the computer in analyzing student essays", "author": ["Ellis B. Page"], "venue": "International Review of Education,", "citeRegEx": "Page.,? \\Q1968\\E", "shortCiteRegEx": "Page.", "year": 1968}, {"title": "About the effects of combining latent semantic analysis with natural language processing techniques for free-text assessment", "author": ["P\u00e9rez et al.2005] Diana P\u00e9rez", "Enrique Alfonseca", "Pilar Rodr\u0131\u0301guez", "Alfio Gliozzo", "Carlo Strapparava", "Bernardo Magnini"], "venue": null, "citeRegEx": "P\u00e9rez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "P\u00e9rez et al\\.", "year": 2005}, {"title": "The William and Flora Hewlett Foundation Automated Student Assessment Prize (ASAP). ASAP Short Answer Scoring Competition System Description", "author": ["Peters", "Jankiewicz2012] Jonathan Peters", "Pawe\u0142 Jankiewicz"], "venue": null, "citeRegEx": "Peters et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2012}, {"title": "A human-computer collaborative approach to the marking of free text answers", "author": ["Mary McGee Wood", "Stuart M. Anderson"], "venue": "In Proceedings of the 8th International CAA Conference,", "citeRegEx": "Sargeant et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sargeant et al\\.", "year": 2004}, {"title": "Contrasting state-of-the-art automated scoring of essays: Analysis", "author": ["Shermis", "Hamner2012] Mark D. Shermis", "Ben Hamner"], "venue": "In Paper presented at Annual Meeting of the National Council on Measurement in Education,", "citeRegEx": "Shermis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shermis et al\\.", "year": 2012}, {"title": "Information extraction and machine learning: Automarking short free text responses to science questions", "author": ["Sukkarieh", "Pulman2005] Jana Sukkarieh", "Stephen Pulman"], "venue": "In Proceedings of the 12th International Conference on Artificial", "citeRegEx": "Sukkarieh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sukkarieh et al\\.", "year": 2005}, {"title": "A framework for evaluation and use of automated scoring", "author": ["Xiaoming Xi", "F. Jay Breyer"], "venue": "Educational Measurement: Issues and Practice,", "citeRegEx": "Williamson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2012}, {"title": "A new dataset and method for automatically grading ESOL texts", "author": ["Ted Briscoe", "Ben Medlock"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Yannakoudakis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Research on using computers to score open-ended student responses has a long history, dating back to Ellis Page\u2019s work on automated scoring of essays (Page, 1966; Page, 1968).", "startOffset": 150, "endOffset": 174}, {"referenceID": 22, "context": "Research on using computers to score open-ended student responses has a long history, dating back to Ellis Page\u2019s work on automated scoring of essays (Page, 1966; Page, 1968).", "startOffset": 150, "endOffset": 174}, {"referenceID": 9, "context": "Landauer et al. (2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 4, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al. (2011); Foltz et al.", "startOffset": 51, "endOffset": 101}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al. (2011); Foltz et al. (2011)).", "startOffset": 51, "endOffset": 122}, {"referenceID": 12, "context": "tasks continued at the Educational Testing Service during the early 1990s (Kaplan, 1991; Kaplan and Bennett, 1994; Burstein and Kaplan, 1995), and received broader attention in the early 2000s, when a number of short answer scoring systems were developed, including ETS\u2019 c-rater (Leacock", "startOffset": 74, "endOffset": 141}, {"referenceID": 18, "context": "and Chodorow, 2003), AutoMark (Mitchell et al., 2002), the Intelligent Essay Assessor (Landauer et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 14, "context": ", 2002), the Intelligent Essay Assessor (Landauer et al., 2003), the Oxford-UCLES system (Sukkarieh and Pulman, 2005), and applications developed at the University of Portsmouth (Callear et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 3, "context": ", 2003), the Oxford-UCLES system (Sukkarieh and Pulman, 2005), and applications developed at the University of Portsmouth (Callear et al., 2001)", "startOffset": 122, "endOffset": 144}, {"referenceID": 25, "context": "and the University of Manchester (Sargeant et al., 2004).", "startOffset": 33, "endOffset": 56}, {"referenceID": 14, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 23, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 19, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 17, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 16, "context": "For instance, the tasks addressed by Mitchell et al. (2002) required answers to include specific welldefined concepts (see Figure 1), and were therefore more amenable to a knowledge engineering approach, whereas those addressed by Foltz et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 8, "context": "(2002) required answers to include specific welldefined concepts (see Figure 1), and were therefore more amenable to a knowledge engineering approach, whereas those addressed by Foltz et al. (2011) elicited longer, less-constrained responses (see Figure 2), and were scored according to the evidence students gave of their \u201cdepth of knowledge\u201d,", "startOffset": 178, "endOffset": 198}, {"referenceID": 5, "context": "to the set of textual entailment evaluation corpora (Dzikovska et al., 2012).", "startOffset": 52, "endOffset": 76}, {"referenceID": 0, "context": "Attali et al. (2010); Williamson et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Attali et al. (2010); Williamson et al. (2012)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 16, "context": "constructed using the MALLET toolkit (McCallum, 2002), one with 30 dimensions, and one with only ten.", "startOffset": 37, "endOffset": 53}, {"referenceID": 7, "context": "Three features were included based on language models built using the IRST Language Modeling Toolkit (Federico et al., 2008).", "startOffset": 101, "endOffset": 124}], "year": 2014, "abstractText": "Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach.", "creator": "LaTeX with hyperref package"}}}