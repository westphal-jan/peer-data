{"id": "1403.1169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "A proof challenge: multiple alignment and information compression", "abstract": "These notes pose a \"proof challenge\": a proof, or disproof, of the proposition that \"For any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, a multiple alignment concept, described in the document, provides a means of encoding all the redundancy that may exist in I. Aspects of the challenge are described.", "histories": [["v1", "Tue, 4 Mar 2014 17:00:19 GMT  (7kb)", "http://arxiv.org/abs/1403.1169v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j gerard wolff"], "accepted": false, "id": "1403.1169"}, "pdf": {"name": "1403.1169.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jgw@cognitionresearch.org;"], "sections": [{"heading": null, "text": "ar Xiv: 140 3.11 69v1 [cs.AI] 4M arconcept, described in the document, provides a means of encoding all the redundancies that may exist in I. Aspects of the challenge are described."}, {"heading": "1 Introduction", "text": "For several years, I have been developing the SP Theory of Intelligence, which aims to simplify and integrate concepts between artificial intelligence, mainstream computers, and human perception and cognition, with information compression as a unifying theme [3, 2]. A central idea of SP Theory is a concept of multidimensional alignment that achieves the effect of compressing information. In this context, it would be good to have a formal proof or contradiction for the following sentence: For each given body of information, expressed as a one-dimensional sequence of atomic symbols, the concept of multiple alignment provides a means of coding all the redundancies described in I. The following sections describe relevant concepts and aspects of this \"evidential challenge\" Dr. Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP)."}, {"heading": "2 The matching and unification of patterns", "text": "To understand some of the complexities in this area, I found it useful to focus on a fairly simple idea: that we can identify repetitions or \"redundancies\" in information by looking for patterns that match each other, and that we can reduce this redundancy and thus compress information by making two or more copies. As just described, the principle loses information on the positions of all but one of the original patterns, but this can be fixed with one of three variants of the idea: \u2022 Chunking-with-codes. Here, the uniform pattern is named relatively briefly, identified or \"code,\" which is used as an abbreviation for the pattern or \"chunk.\" For example, if the words \"Treaty on the Functioning of the European Union\" appear in several different places in a document, we can save the expression \"S\" by giving it a short name, such as \"TFEU\" and then using that name as an expression, it is equated with a general unit."}, {"heading": "3 SP patterns and the multiple alignment con-", "text": "Concept In the SP system, all kinds of knowledge are presented step by step with SP patterns: arrays of atomic symbols in one or two dimensions. So far, the emphasis has been on 1D patterns, but it is intended that the system is generalized at a certain point in time to work with 2D patterns. In the SP system, all types of processing are performed by building multiple alignments such as the one shown in Figure 1. This and other examples in this document are created by the SP computer model, based on the SP theory.In the illustration, each line contains an SP pattern. Conventionally, line 0 contains a new pattern that represents incoming information, while the remaining lines contain old patterns that represent already stored information.Multiple alignments such as those shown in the illustration are built step by step, similar to bioinformatics programs that each generate multiple alignments in multiple phases, with the multiple alignment of DNA patterns being well compressed."}, {"heading": "4 Unsupervised learning", "text": "In the example just shown, the old patterns were created manually, but it is assumed that when the system is more mature, most of the old patterns would be generated by the system itself, as described in [3, Section 5] and [2, Chapter 9]. Unsupervised learning in the SP system means compressing a given information body (I) to generate a grammar (G), and an encoding (E) of I in the term G. In accordance with the principle of minimal length encoding [1], the system aims to compress the total size of G and E. For current purposes, many details of how learning is achieved are not important, but a brief summary can be useful: \u2022 When an incoming (\"new\") pattern is received, the system looks for good complete or partial matches with pre-stored (\"old\") patterns when a new pattern does not match. \""}, {"heading": "5 How a New pattern may be encoded eco-", "text": "This section outlines how an encoding, E, can be derived from new and old patterns in multi-alignment. Let's look at the multi-alignment shown in Figure 1. From this multi-alignment one can derive a code pattern in the following way: 1. Scan the multi-alignment from left to right and look for columns that alone contain an ID symbol and are not aligned with another symbol.2. Copy these symbols into a code pattern in the same order in which they appear in multi-alignment, and the result in this case is the code pattern \"S PL 0a 17 6 11 21 # S.\" This is actually a compressed representation of the sentence \"t h e a p l e s a r e e e t.\" As a rough measure of compression, 17 symbols in the encoding were reduced to 8 symbols. If we reduce the number of bits in each sentence to 54 bits, the bits were used in the encoding."}, {"heading": "6 Kinds of redundancy in sequential informa-", "text": "This section looks at some types of redundancy that can occur in a 1D sequence of atomic symbols I, and how they can be encoded in the multiple alignment framework. The first three correspond to the three encoding techniques outlined in Section 2: Chunking-with-codes, Schema-plus-correction, and Run-length coding. As a rough generalization, any symbol or string that occurs 2 or more times in me represents redundancy in I. More specifically, a symbol or string represents redundancy when it occurs more often in me than you would expect by chance."}, {"heading": "6.1 Chunks", "text": "Assuming that the old patterns in rows 1 to 8 of Figure 1 were derived through unattended learning from a relatively large corpus of texts in natural language (I), sequences of symbols such as \"t h e\" in \"D 17 t h e # D,\" a p p p l e \"in\" N Nr 6 a p l e # N \"and\" s w e e \"in\" A 21 s w e e e e t # A \"can be considered bits of information, each with associated ID symbols or\" codes \"such as\" D, \"17\" and \"# D\" in \"D 17 t h e # D,\" according to the chunkking-with-codes technique for information compression."}, {"heading": "6.2 Schemata", "text": "Examples of this are patterns such as \"S Num; NP # NP V # V A # A # S 'and\" NP 0a D # D N # N # NP' in Figure 1. In the first case, the slots are \"NP # NP ', V # V' and 'A # A'; in the second case, the slots are\" D # D 'and' N # N '. An example of the schema plus correction method for information compression is the way in which the sequence \"t h e a p p l e s a r e s w e e t' can be economically encoded as\" S PL 0a 17 6 11 21 # S '(Section 5). \"S... # S\" here is the schema and the symbols \"PL 0a 17 6 11 21' are\" corrections \"to the schema on more than one abstraction level."}, {"heading": "6.3 Runs", "text": "Any sequence of two or more blocks, each one of which, with the exception of the first one, follows immediately after its predecessor, can be described as a \"run.\" In the context of multiple alignment, a sequence such as \"a b c a b c a b c a b c c $\" containing repeated instances of the piece \"a b c\" can be encoded recursively, as in Figure 2. This can be seen as an example of encoding the run length."}, {"heading": "6.4 Discontinous dependencies", "text": "A well-known feature of natural languages is that there can be grammatical \"similarities\" or \"dependencies\" between one part of a sentence and another. For example, if the subject of the sentence is singular, then the main verb must also be singular, and if the subject is plural, the main verb must be plural. Within a sentence, there can be dependencies that are completely independent of each other, such as number dependence and gender dependence in the French sentence Les plumes sont vertes (\"The feathers are green\"): P P P P Number of dependenciesLes plume s sont vert e s"}, {"heading": "F F Gender dependencies", "text": "This type of agreement or dependency is often described as \"discontinuous,\" because it can arbitrarily skip large amounts of intermediate structure. For example, there is a plural agreement between subject and verb in the sentence The winds from the west are strong, although subject and verb are separated by the phrase from the west. This phrase can be replaced by one or more subordinate sentences, which can be arbitrarily complex. Dependencies like these can be encoded using the multiple alignment system, as in Figure 1. Here, the multiple dependence between subject and verb in line 8 is marked with the pattern \"Num PL; Np Vp.\" The symbol \"Np\" is aligned with the appropriate symbol in line 2, while the symbol \"Vp\" is aligned with its twin in line 5."}, {"heading": "6.5 Mirror images", "text": "The last form of redundancy to be considered in this section is when one sequence is a mirror image of the other. For example, the sequence \"i n f o r m a t i o n\" corresponds to the sequence \"n o i t a m r o f n i,\" provided that the synchronization process reverses the sequence of symbols in one of the sequences relative to the other and that the symbols are treated as atomic, with no internal structure or left-to-right asymmetry. As it currently stands, the SP computer model does not perform this type of reverse synchronization, but it could be generalized to do so. In addition to such a generalization, a reform of the representation of SP patterns would be necessary to facilitate the formation of multiple arrangements in which any sequence can appear in its left-to-right or right-to-left order. To make this possible, it would be necessary to ensure symmetry between the symbols at each end of the ID."}, {"heading": "7 Towards a proof", "text": "The challenge raised in the introduction is to prove or disprove the assertion that: For each given information body I, expressed as a one-dimensional sequence of atomic symbols, the concept of multiple alignment is a means of encoding all the redundancies that can exist in I. It seems that the proposition can be proved or disproved by answering the following questions: \u2022 Do the redundancies described in Section 6 exhaust the possibilities? Are there other redundancies that can be found in I? \u2022 Can the multiple alignment system for each of the types of redundancies described in Section 6 encode all of those redundancies, if any, that exist in I? Another possible way to approach the problem is in the following statements (from Section 6): A symbol or sequence of symbols represents redundancy if it occurs more frequently in I than one would expect by chance."}], "references": [{"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Parts I and II. Information and Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1964}, {"title": "Unifying Computing and Cognition: the SP Theory and Its Applications. CognitionResearch.org", "author": ["J.G. Wolff"], "venue": "Menai Bridge,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "The SP theory of intelligence: an overview", "author": ["J.G. Wolff"], "venue": "Information, 4(3):283\u2013341,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "For several years, I have been developing the SP theory of intelligence, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme [3, 2].", "startOffset": 256, "endOffset": 262}, {"referenceID": 1, "context": "For several years, I have been developing the SP theory of intelligence, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, with information compression as a unifying theme [3, 2].", "startOffset": 256, "endOffset": 262}, {"referenceID": 0, "context": "In accordance with the principle of minimum length encoding [1], the system aims to minimise the overall size of G and E.", "startOffset": 60, "endOffset": 63}], "year": 2014, "abstractText": "These notes pose a \u201cproof challenge\u201d: a proof, or disproof, of the proposition that For any given body of information, I, expressed as a one-dimensional sequence of atomic symbols, a multiple alignment concept, described in the document, provides a means of encoding all the redundancy that may exist in I. Aspects of the challenge are described.", "creator": "LaTeX with hyperref package"}}}