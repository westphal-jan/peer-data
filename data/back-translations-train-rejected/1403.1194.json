{"id": "1403.1194", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2014", "title": "Latent Semantic Word Sense Disambiguation Using Global Co-occurrence Information", "abstract": "In this paper, I propose a novel word sense disambiguation method based on the global co-occurrence information using NMF. When I calculate the dependency relation matrix, the existing method tends to produce very sparse co-occurrence matrix from a small training set. Therefore, the NMF algorithm sometimes does not converge to desired solutions. To obtain a large number of co-occurrence relations, I propose to use co-occurrence frequencies of dependency relations between word features in the whole training set. This enables us to solve data sparseness problem and induce more effective latent features. To evaluate the efficiency of the method of word sense disambiguation, I make some experiments to compare with the result of the two baseline methods. The results of the experiments show this method is effective for word sense disambiguation in comparison with the all baseline methods. Moreover, the proposed method is effective for obtaining a stable effect by analyzing the global co-occurrence information.", "histories": [["v1", "Wed, 5 Mar 2014 17:20:01 GMT  (120kb)", "http://arxiv.org/abs/1403.1194v1", "6 pages, 2figures"]], "COMMENTS": "6 pages, 2figures", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["minoru sasaki"], "accepted": false, "id": "1403.1194"}, "pdf": {"name": "1403.1194.pdf", "metadata": {"source": "CRF", "title": "LATENT SEMANTIC WORD SENSE DISAMBIGUATION USING GLOBAL CO-OCCURRENCE INFORMATION", "authors": ["Minoru Sasaki"], "emails": ["msasaki@mx.ibaraki.ac.jp"], "sections": [{"heading": null, "text": "KEYWORDS Literature disambiguation, information about global co-occurrences, dependency relationships, non-negative matrix factorization"}, {"heading": "1. INTRODUCTION", "text": "In natural language processing, the acquisition of sensory examples from a sample sentence containing a specific target word is used to construct a large dataset of tagged examples to demonstrate a wide range of semantic analysis. For example, we can use the obtained dataset to construct a classifier that identifies its meaning by analyzing coexistence statistics for a target word. Furthermore, it is increasingly important to choose the most appropriate meaning of the ambiguous word and construct thesaurus for each meaning of an ambiguous word. To construct large-scale training data, it is important to improve the use of speech words and thesaurus to determine the meaning of the ambiguous word."}, {"heading": "2. NON-NEGATIVE MATRIX FACTORIZATION", "text": "Non-negative matrix factorization (NMF) is a popular decomposition method for multivariate data [4]. NMF splits non-negative matrix X into matrix W and k \u00b7 n matrix H, while these matrices have no negative elements. k is usually used as a smaller value than n and m.WHX (1) Using NMF for a term document matrix X, matrix H represents the cluster result with k-Topics. To quantify the quality of this approximation, cost functions based on the Kullback-Leibler divergence are used and minimized using iterative updating rules as follows: ijT ij ijij WHH XH WW (2) ijT ij T ijij T ijij WHW HX HH, (3) where Wij and Hij are the i-th row and j-th column elements respectively. These matrices are applied randomly and with this H or congenz."}, {"heading": "3. WSD USING GLOBAL CO-OCCURRENCE INFORMATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Latent Semantic WSD Using Local Co-occurrence Information", "text": "In previous research [3], a novel WSD method of certain word instances using the automatically extracted sense information suggests inducing latent characteristics for three matrices; the first matrix A contains co-occurrences frequencies of words with which the target word occurs in common; the second matrix B contains term frequencies of words appearing in the context window; the third matrix C contains co-occurrences of words with which the occurring context words of the target word occur in common; and then NMF is applied to the three matrices to factorize each matrix into two non-negative matrices, while the previous results are used to initialize the next factorization, as shown in Figure. In the face of a non-negative matrix A, B, and C at the beginning of this method, matrix W, H, G, and F are initialized as matrix d."}, {"heading": "3.2. Latent Semantic WSD Using Global Co-occurrence Information", "text": "This latent-semantic WSD method is efficient in finding a reduced semantic space. However, problems arise when we use this method. If we calculate the third matrix C, this method tends to generate a very sparse matrix from a small training set. To obtain a large number of parallelism relationships, I suggest to use the frequency of dependency relationships between word characteristics throughout the training set. This allows us to solve the problem of data economy and induce more effective latent characteristics. Like the method proposed above, the proposed method requires three matrices, but the third matrix differs from the previous method. The third matrix D contains the frequency of parallel occurrences of contextual words occurring in dependence relationships to contextual words in a large document set."}, {"heading": "4. EXPERIMENT", "text": "In order to evaluate the efficiency of the proposed WSD method on the basis of the global co-occurence information, I am conducting some experiments to compare them with the results of the existing methods. In this section, I will describe a sketch of the experiments."}, {"heading": "4.1. Data", "text": "I used the Japanese Semeval-2010 WSD task set, which contains 50 target words made up of 22 nouns, 23 verbs and 5 adjectives [2]. In this set, there are 50 training instances and 50 test instances for each target word. In addition, I use 22,832 documents selected from the Japanese BCCWJ corpus1 to obtain a large number of parallel relationships."}, {"heading": "4.2. Evaluation Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1. Baseline System 1", "text": "As the first basic method, I use only the first matrix A, which is described in Section 3.1. To construct the matrix A, I represent each sentence with the target word in the training set as a high-dimensional vector, each component representing the frequency of the target word in the sentence. Then, NMF is applied to matrix A to factoring each matrix into two non-negative matrices W and H. Each vector is marked with the meaning of the target word in this sentence: T ii Hcb (6) For the input example of the target word, its context words are extracted to construct a vector f and the vector f is also mapped with the matrix H into the same semantic space: Tfd Hcb (6)."}, {"heading": "4.2.2. Baseline System 2", "text": "In the second base system, I use the latent semantic WSD using local co-occurrence information as described in Section 3.1. I construct the three matrices A, B, and C to generate latent semantic dimensions using NMF.1 http: / / www.ninjal.ac.jp / english / products / bccwj /."}, {"heading": "5. EXPERIMENTAL RESULTS", "text": "In practice, the algorithms are executed several times from different starting points, and the NMF is selected as a practicable solution. In our experiments, each method is executed three times and the average precision of all executions is calculated. In this Figure 2, the proposed method shows a higher precision than the other basic methods, so that this approach is effective for word disambiguation. Compared to base system 1, the proposed method can achieve a better precision, so that it is effective for WSD to use context information and co-event information. Compared to base system 2, the proposed method offers a slightly better precision than the base system 2. As shown in Table 1, the proposed method can achieve the highest precision and be stable at a high precision value. However, compared to base system 2, the proposed method cannot achieve a stable precision, because the number of courinal information that is effective for the common occurrence is not stable."}, {"heading": "6. CONCLUSION", "text": "In this paper, I propose a novel method of meaning disambiguation based on global coexistence information using NMF. To evaluate the efficiency of the method of meaning disambiguation, I am conducting some experiments to compare it with the results of the two basic methods. Results of the experiments show that this method is effective for meaning disambiguation compared to all basic methods. Furthermore, the proposed method is effective to achieve a stable effect by analyzing the global coexistence information. Further work would be needed to consider larger training data in order to obtain a large amount of coexistence information."}], "references": [{"title": "Support-Vector Networks", "author": ["Corinna Cortes", "Vladimir Vapnik"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "SemEval- 2010 task: Japanese WSD", "author": ["Okumura", "Manabu", "Shirai", "Kiyoaki", "Komiya", "Kanako", "Yokono", "Hikaru"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Latent Semantic Word Sense Induction and Disambiguation", "author": ["Van de Cruys", "Tim", "Apidianaki", "Marianna"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Algorithms for Non-negative Matrix Factorization", "author": ["Lee", "Daniel D", "Seung", "Sebastian"], "venue": "Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Managing Gigabytes (second edition): Compressing and Indexing Documents and Images\u201d, Morgan Kaufmann Publishers Inc. Authors Minoru Sasaki: received his B.E., M.E. and D.Eng. degrees in information science and intelligent systems from the University of Tokushima in 1996, 1998 and 2000. He is now a lecturer in the department of computer and information sciences", "author": ["Witten", "Ian H", "Moffat", "Alistair", "Bell", "Timothy C"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}], "referenceMentions": [{"referenceID": 4, "context": "A typical method for this approach is the classical bag-of-words (BOW) approach [5], where each document is represented as a feature vector counting the number of occurrences of different words as features.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "By using such features, it becomes easy to adapt many existing supervised learning methods such as Support Vector Machine (SVM) [1] for the WSD task.", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "In previous research, [3] proposes a novel WSD method of particular word instances using the automatically extracted sense information.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "NON-NEGATIVE MATRIX FACTORIZATION Non-Negative Matrix Factorization (NMF) is a popular decomposition method for multivariate data [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "Latent Semantic WSD Using Local Co-occurrence Information In previous research, [3] proposes a novel WSD method of particular word instances using the automatically extracted sense information.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Data I used the Semeval-2010 Japanese WSD task data set, which includes 50 target words comprising 22 nouns, 23 verbs, and 5 adjectives [2].", "startOffset": 136, "endOffset": 139}], "year": 2014, "abstractText": "In this paper, I propose a novel word sense disambiguation method based on the global co-occurrence information using NMF. When I calculate the dependency relation matrix, the existing method tends to produce very sparse co-occurrence matrix from a small training set. Therefore, the NMF algorithm sometimes does not converge to desired solutions. To obtain a large number of co-occurrence relations, I propose to use co-occurrence frequencies of dependency relations between word features in the whole training set. This enables us to solve data sparseness problem and induce more effective latent features. To evaluate the efficiency of the method of word sense disambiguation, I make some experiments to compare with the result of the two baseline methods. The results of the experiments show this method is effective for word sense disambiguation in comparison with the all baseline methods. Moreover, the proposed method is effective for obtaining a stable effect by analyzing the global co-occurrence information.", "creator": "PScript5.dll Version 5.2.2"}}}