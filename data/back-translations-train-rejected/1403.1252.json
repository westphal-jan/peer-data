{"id": "1403.1252", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Inducing Language Networks from Continuous Space Word Representations", "abstract": "Recent advancements in unsupervised feature learning have developed powerful latent representations of words. However, it is still not clear what makes one representation better than another and how we can learn the ideal representation. Understanding the structure of latent spaces attained is key to any future advancement in unsupervised learning. In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure.", "histories": [["v1", "Thu, 6 Mar 2014 01:36:53 GMT  (4879kb,D)", "https://arxiv.org/abs/1403.1252v1", "14 pages"], ["v2", "Fri, 27 Jun 2014 17:36:43 GMT  (4879kb,D)", "http://arxiv.org/abs/1403.1252v2", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SI", "authors": ["bryan perozzi", "rami al-rfou", "vivek kulkarni", "steven skiena"], "accepted": false, "id": "1403.1252"}, "pdf": {"name": "1403.1252.pdf", "metadata": {"source": "CRF", "title": "Inducing Language Networks from Continuous Space Word Representations", "authors": ["Bryan Perozzi", "Rami Al-Rfou", "Vivek Kulkarni", "Steven Skiena"], "emails": ["bperozzi@cs.stonybrook.edu", "ralrfou@cs.stonybrook.edu", "vvkulkarni@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In this thesis we introduce a new view of continuous word representation in space as language networks. We examine two techniques for creating language networks from learned characteristics by stimulating them for two popular word representation methods and examining the characteristics of their resulting networks. We find that the induced networks differ from other methods for creating language networks and that they contain a meaningful community structure."}, {"heading": "1 Introduction", "text": "The integration of knowledge in this format is the secret behind many recent breakthroughs in machine learning-based applications such as speech recognition, computer vision and natural language processing (NLP). Here, we focus on word representations (word embeddings), in which each word consists of a dense, real-valued vector. During the preparation phase, the representations acquire the desirable property that similar words have a smaller distance to each other than disjointed words [15], allowing the representations to use the abundance of raw text to learn characteristics and knowledge required for supervised learning applications such as language labeling, called entity recognition, language modeling, sensory analysis, etc. We have several methods we are learning to understand them."}, {"heading": "2 Continuous Space Language Models", "text": "The goal of a language model is to assign a probability to any word sequence that estimates the probability of observing such a sequence. Normally, the training goal maximizes the common probability of the training corpus. A continuous spatial probabilistic language model aims to estimate this probability distribution by first learning continuous representations for the words and phrases observed in the language. Such mapping is useful to deal with the curse of dimensionality in cases where the data distribution as a natural language is sparse. Furthermore, these representations could be used as characteristics for natural language processing applications, domain adaptation, and learning scenarios that include text or language. Specifically, a sequence of words S = [w1.. wk] we want to maximize P (w1.., wk) and learn representations for words. During the training process, the continuous spatial language model learns a mapping of words to points in Rd, where d lies between rule 20 and d."}, {"heading": "2.1 Polyglot", "text": "The Polyglot project offers word representations for each language in Wikipedia [22]. For sufficiently large Wikipedias, the vocabulary consists of the most common 100,000 words. The representations are learned by a procedure similar to that of Collobert et al. [11]. For a given word sequence, St = [wt \u2212 k.. wt.... wt + k] is observed in the corpus T, constructing a damaged sequence S \u2032 t by replacing the word in the middle with a word wj randomly selected from the vocabulary V. Once the vectors are found, we assemble the sequence representation by concatenating the vectors to a vector called a projection layer St. The model is punished by the hinged loss function, 1T = T \u0445 t = 1 | 1 \u2212 score (S \u2032 t) + score (St) | + score is computed by a hidden layer neural networkscore (St) = Wings2 (Wlot = 1), which is most frequently displayed in the (1th) work."}, {"heading": "2.2 SkipGram", "text": "While the polyglot embedding takes into account the order of the words to form the representation of any word order, the SkipGram model proposed by Mikolov et al. [16] maximizes the average embedding probability of the context words regardless of their order. This allows the model to scale to larger context windows. In our case, we train a SkipGram model2 on the English Wikipedia corpus provided by the Polyglot project for the most common 350,000 words with context size k to 5 and the embedding vector size to 64.1 polyglot embedding and corpus available at http: / / bit.ly / embeddings 2 SkipGram training tool available at https: / / code.google.com / p / word2vec /"}, {"heading": "2.3 Random", "text": "In order to have a baseline, we also create random embeddings for the most common 100,000 words. The initial position of the words in the polyglot embeddings was scanned from a uniform distribution, so we create the random embeddings vectors by scanning from U (m \u00b2 \u2212 \u03c3, m \u00b2 + \u03c3), where m \u00b2 and \u03c3 are the mean or standard deviation of the trained polyglot embeddings. From this baseline, we can see how the language networks we build differ from networks caused by randomly initialized dots."}, {"heading": "3 Word Embedding Networks", "text": "We look at the problem of constructing a meaningful network in the face of a continuous spatial language model. Since there are a variety of ways in which such a network could be induced, we start by developing a list of desirable properties for a language network. In particular, we try to build a network that: 1. In a connected diagram, all words can be related to each other. 3. Understand that the community structure in the network reflects the syntactic and semantic information encoded in the word embedding. We also require a method to calculate the distance in the embedding space. While there are a variety of metrics that could be used, we found that Euclidean distance works well."}, {"heading": "3.3 Discussion", "text": "To answer this question, we will use the properties mentioned at the beginning of this section: 1. Networking - networks generated by the K-NN method connect much faster (as a function of the edges) than those produced by the D-proximity. 2. Networking - with only 100K channels (2a). 3. Mixing - we would like our resulting networks to be modular. 4. We would prefer to add the channels between the members of a community rather than connecting the communities together."}, {"heading": "4 Related Work", "text": "Here we discuss the relevant work in language networks and word embedding. There are also related work on the theoretical properties of the nearest neighboring graphs, consult Eppstein, Paterson and Yao [12] for some basic research."}, {"heading": "4.1 Language Networks", "text": "Word Co-occurrences. A branch of the study of language as networks attempts to form networks directly from a corpus of raw text. Cancho and Sole \"[9] examine random co-occurence graphs as a method of analyzing language. In their diagram, edges connect words that appear below a fixed threshold (d \u2264 2) from each other in sentences. They find that networks constructed in this way exhibit both a small world structure and a distribution of the laws of power. Language networks based on word co-occurrences have been used in a variety of natural language processing tasks, including the motive analysis of semantics [7], text summary [1] and the resolution of disambiguity of word usages [26]. Another approach to the study of language networks is based on the study of the relationships between words exposed by a written language reference. Motter et al. [21] use a thesaurus to show a network of synonyms to construct them."}, {"heading": "4.2 Word Embeddings", "text": "Distributed representations were first proposed by Hinton [14] to learn how to map symbolic data to continuous space.These representations 3 Our induced networks, available at http: / / bit.ly / inducing _ language _ networksare able to capture fine grain structures and regularities in the data [18]. With recent advances in hardware performance, Bengio et al. [4] used the distributed representations to create a state-of-the-art probabilistic language model.The model maps each word in a predefined V vocabulary to a point in the Rd space (word embeddings).The model was trained for days on a cluster of machines.Further applications followed, Collobert et al. [11] developed SENNA, a system that displays word representations, word embeddings, minimal recognition costs, synchronizing training and synchronizing models."}, {"heading": "5 Conclusions", "text": "Despite their usefulness, understanding the mechanisms that give them their characteristics is still a tough problem. In this work, we presented an approach to looking at word embedding as a language network. We investigated the characteristics of induced networks and their community structure, and with the help of this analysis, we were able to develop a process that develops a connected graph with meaningful clusters. We believe that this work will create the conditions for progress in both NLP techniques that use distributed word representation, and in understanding the characteristics of the machine learning processes that generate it. Much remains to be done. In the future, we would like to focus on comparing word embedding with other known distributed representation techniques (e.g. LDA / LSA), investigating the effects of different types of vocabulary (e.g. topic words, units) on induced graphs and the stability of graph properties as a function of network size."}, {"heading": "Acknowledgments", "text": "This research was partially supported by NSF grants DBI-1060572 and IIS-1017181, with additional support from TASC Inc and a Google Faculty Research Award."}], "references": [{"title": "A complex network approach to text summarization", "author": ["L. Antiqueira", "O.N.O. Jr.", "L. da Fontoura Costa", "M. das Gra\u00e7as Volpe Nunes"], "venue": "Information Sciences", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Gephi: An Open Source Software for Exploring and Manipulating Networks", "author": ["M. Bastian", "S. Heymann", "M. Jacomy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Y. Bengio", "J.-S. Senecal"], "venue": "Neural Networks, IEEE Transactions on 19.4", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "When is \u201cnearest neighbor\u201d meaningful?", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "Database Theory\u2014ICDT\u201999. Springer,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Quantifying Semantics using Complex Network Analysis", "author": ["C. Biemann", "S. Roos", "K. Weihe"], "venue": "Proceedings of COLING", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "The small world of human language", "author": ["R.F. i. Cancho", "R.V. Sol\u00e9"], "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "The Expressive Power of Word Embeddings", "author": ["Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "ICML 2013 Workshop on Deep Learning for Audio, Speech, and Language Processing. Vol. abs/1301.3226. Atlanta,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On nearest-neighbor graphs", "author": ["D. Eppstein", "M.S. Paterson", "F.F. Yao"], "venue": "Discrete & Computational Geometry", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In: vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Learning distributed representations of concepts", "author": ["G.E. Hinton"], "venue": "Proceedings of the eighth annual conference of the cognitive science society. Amherst,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "In: Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "Proceedings of NAACL- HLT", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Topology of the conceptual network of language", "author": ["A.E. Motter", "A.P.S. de Moura", "Y.-C. Lai", "P. Dasgupta"], "venue": "In: Phys. Rev. E", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Polyglot: Distributed Word Representations for Multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Sofia, Bulgaria: Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Learning internal representation by back propagation", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1986}, {"title": "Large, pruned or continuous space language models on a GPU for statistical machine translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "Proceedings of the NAACL-HLT", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Global organization of the Wordnet lexicon", "author": ["M. Sigman", "G.A. Cecchi"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "HyperLex: lexical cartography for information retrieval", "author": ["J. V\u00e9ronis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Integrating knowledge in this format is the secret behind many recent breakthroughs in machine learning based applications such as speech recognition, computer vision, and natural language processing (NLP) [3].", "startOffset": 206, "endOffset": 209}, {"referenceID": 14, "context": "During the pre-training stage, the representations acquire the desirable property that similar words have lower distance to each other than to unrelated words [15].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 12, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 16, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 23, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 9, "context": "Several methods and algorithms have been proposed to learn word representations along different benchmarks for evaluation [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "The Polyglot project offers word representations for each language in Wikipedia [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] maximizes the average log probability of the context words independent of their order", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This tendency to bridge new clusters together, rather than just expand existing ones, may be related to the instability of the nearest neighbor [6] in high dimensional spaces.", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "In (2a), C shown for k = [2,30] and d = [0.", "startOffset": 25, "endOffset": 31}, {"referenceID": 17, "context": "Tighter local clustering may explain some of the interesting regularities observed in the SkipGram embedding [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "In order to understand the differences between the language networks better, we conducted an examination of the clusters found using the Louvain method [8] for modularity maximization.", "startOffset": 152, "endOffset": 155}, {"referenceID": 11, "context": "There is also related work on the theoretical properties of nearest neighbor graphs, consult Eppstein, Paterson, and Yao [12] for some basic investigations.", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Sol\u00e9 [9] examine word co-occurrence graphs as a method to analyze language.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 174, "endOffset": 177}, {"referenceID": 25, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 1, "context": ") Images created with Gephi [2].", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "[21] use a thesaurus to construct a network of synonyms, which they find to find to exhibit small world structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In [25], Sigman and Cecchi investigate the graph structure of the Wordnet lexicon.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Much of the previous work in language networks build networks that are prone to noise from spurious correlations in word co-occurrence or infrequent word senses [9, 25].", "startOffset": 161, "endOffset": 168}, {"referenceID": 24, "context": "Much of the previous work in language networks build networks that are prone to noise from spurious correlations in word co-occurrence or infrequent word senses [9, 25].", "startOffset": 161, "endOffset": 168}, {"referenceID": 8, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 20, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 24, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 8, "context": "|V | |E| C Crandom pl plrandom \u03b3 Cancho and Sol\u00e9 [9](UWN) 478, 773 1.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "70 Cancho and Sol\u00e9 [9](RWN) 460, 902 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "[21] 30, 244 \u2212 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Distributed representations were first proposed by Hinton [14], to learn a mapping of symbolic data to continuous space.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "are able to capture fine grain structures and regularities in the data [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Usually, these models are trained using back-propagation algorithm [23] which requires large amount of computational resources.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "[4] used the distributed representations to produce a state-of-the-art probabilistic language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] developed SENNA, a system that offers part of speech tagger, chunker, named entity recognizer, semantic role labeler and discriminative syntactic parser using the distributed word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 18, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 92, "endOffset": 100}, {"referenceID": 19, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 92, "endOffset": 100}, {"referenceID": 21, "context": "Al-Rfou, Perozzi, and Skiena [22] trained word embeddings for more than a hundred languages and showed that the representations help building multilingual applications with minimal human effort.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "[16] as simpler and faster alternatives to neural network based models.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Recent advancements in unsupervised feature learning have developed powerful latent representations of words. However, it is still not clear what makes one representation better than another and how we can learn the ideal representation. Understanding the structure of latent spaces attained is key to any future advancement in unsupervised learning. In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure.", "creator": "LaTeX with hyperref package"}}}