{"id": "1403.1497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Active Learning for Autonomous Intelligent Agents: Exploration, Curiosity, and Interaction", "abstract": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications.", "histories": [["v1", "Thu, 6 Mar 2014 17:12:30 GMT  (2786kb,D)", "http://arxiv.org/abs/1403.1497v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["manuel lopes", "luis montesano"], "accepted": false, "id": "1403.1497"}, "pdf": {"name": "1403.1497.pdf", "metadata": {"source": "CRF", "title": "Active Learning for Autonomous Intelligent Agents: Exploration, Curiosity, and Interaction", "authors": ["Manuel Lopes", "Luis Montesano"], "emails": ["manuel.lopes@inria.fr", "luis.montesano@unizar.es"], "sections": [{"heading": null, "text": "In this study, we present different approaches that allow an intelligent actor to explore his or her environment autonomously in order to gather information and learn multiple tasks. Different communities proposed different solutions that are in many cases similar and / or complementary. These solutions include active learning, exploration / exploitation, online learning and social learning. The common aspect of all these approaches is that it is the actor who selects and decides what information to collect next. Applications of these approaches already include tutor systems, autonomous capture of learning, navigation and mapping, and human-robot interaction. We discuss how these approaches relate, explain their similarities and their differences in terms of problem-solving and performance metrics. We believe that such an integrated discussion improves interdisciplinary research and application.1"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1 Exploration", "text": "Exploration by an agent (or a team of agents) is at the heart of rover missions, search and rescue operations, environmental monitoring, surveillance and safety, best teaching strategies, online advertising, etc. In all of these situations, the time and resources required to complete a task are limited or unknown. In addition, there are often trade-offs between different tasks such as surviving in a hostile environment, communicating with other agents, gathering more information to mitigate risk, collecting and analyzing samples. All of these tasks must be completed at the end, but the sequence is relevant in that it helps subsequent tasks. For example, gathering geological samples for analysis and communicating the results will be easier if the robot already has a map of the environment. Active strategies are paramount to selecting the right tasks and actively performing the task by maximizing the operational benefit while minimizing the resources or time needed to achieve the goal."}, {"heading": "1.2 Curiosity", "text": "A more open perspective on learning should take into account cases in which the task itself is not defined. Humans develop and grow in an indeterminate environment without predefined goals. Due to this uncertainty, we cannot assume that all situations are apriori and that the agent himself has to adapt and learn new tasks. More problematic is that the tasks are so complex that learning new skills is required (Gottlieb et al., 2013). Similar to animals with complex behaviors, an initial phase of immaturity dedicated to playing and learning may allow the development of such skills, which is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman et al., 1997; Lungarella et al., 2003; Oudeyer, 2011)."}, {"heading": "1.3 Interaction", "text": "Learning agents have worked intensively on the problem of acquiring robust and adaptable skills and behaviors for complex tasks from two different perspectives: programming by demonstration (also known as imitation learning) and learning by experience. From an AL perspective, the main difference between these two approaches is the source of the new data. Programming by demonstration is based on examples provided by an external actor (usually a human). Learning by experience takes advantage of the embodiment of the actor to collect examples of itself acting in the world. In abstract AL from machine learning, the new data / labels are used to come from an oracle, and no special consideration is given to what exactly the oracle is, along with well-educated traits such as bias and consistency. More recently, data and labels may come from people who lead to bias and inconsistencies, and this is also the case for actors who interact with people, taking into account applications that might come from other sources of information."}, {"heading": "1.4 Organization", "text": "We will first explain the AL principles for autonomous intelligent agents in paragraph 2. Then the core review will be organized into three main parts: paragraph 3 AL during self-exploration; paragraph 4 autonomous discovery / creation of targets; and finally paragraph 5 AL in humans."}, {"heading": "2 Active Learning for Autonomous", "text": "In this section we offer an integrated perspective on the many approaches to active learning. The name active learning has mostly been used in machine learning, but here we look at each situation in which a learning agent uses its current learning task hypothesis to choose what / where / how to learn next. Figure 1 shows the three main perspectives for active learning. Exploration in the area of enhanced learning (Sutton and Barto, 1998), Bayesian optimization et al., learning algorithms and decisions about what can be selected. Figure 1 shows the three main perspectives for active learning. Exploration in the area of enhanced learning."}, {"heading": "2.1 Optimal Exploration Problem", "text": "To justify the discussion, let us consider a robot whose mission is to create a map of some physical quantities of interest over a region (e.g. obstacles, air pollution, traffic density, presence of diamonds...).The actor will have a series of problems on board to act in the environment, which include movement along a path or to a certain position and use his sensors to obtain measurements of the quantity of interest. Furthermore, it may be possible to make decisions about other problems, such as which algorithms should be used to process the measurements obtained or to adapt the model to the environment.The amount of all possible decisions will define the space of exploration policy.In order to derive an active algorithm for this task, we must model the costs and the loss function related to the actions of a specific exploration policy. Among the most common costs are the cost of using each of the sensors on board (e.g. energy consumption, time needed, changes in the payment load and location)."}, {"heading": "2.2 Learning Setups", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Function approximation", "text": "Regression and classification, problems are the most common problems with machine learning methods. In both cases, the goal is to approximate the input-output ratio g: x \u2192 y. Typical loss functions are the square mean error L = | h = | y | 2 = | g (x) \u2212 y | 2 for regression and the 0 \u2212 1 loss L0 \u2212 1 = I (g (x) = y) for classification, where I refer to the indicator function. In this setup, the cost function directly measures the cost of obtaining measurements (e.g. taking the measurement or moving to the next point), if it exists. The perspective of active learning corresponds to the decision for which input x it is more relevant to request the corresponding label y. Some other limitations may be included, such as limiting the number of input points (pool-based active learning) or deciding whether the points arrive sequentially and decide to request the appropriate label (see below)."}, {"heading": "2.2.2 Multi-Armed Bandits", "text": "An alternative formalism that is normally applied to discrete selection problems is Multi-Armed Bandit (MAB) formalism (Gittins, 1979; Bubeck and CesaBianchi, 2012). Multi-Armed Bandits define a problem where a player can select an arm from among a number of n possible ones in each round. After playing the selected arm, the player receives a reward. In most cases, the goal of the player is to find a strategy that allows him to receive the maximum cumulative reward possible. Loss in bandit problems is usually based on the concept of regret, that is, the difference between the reward collected and the reward that would have been collected if the player had known which was the best arm since the beginning (Auer et al., 2003). Many algorithms have been proposed for various variants of the problems where the player does not regret after a learning phase, but must either explain what is the best arm (carillon et al, 2011 or 2011)."}, {"heading": "2.2.3 MDP", "text": "The most common and well-known formalities for modelling sequential decision-making processes are Markov decision-making processes (MDP) (Bellman, 1952). If there is no knowledge of the environmental model and an actor needs to optimize a reward function while interacting with the environment, the problem is called reinforcement learning (RL) (Sutton and Barto, 1998). A sequential problem occurring is modeled as a series of states S. The goal of the actor is to find a policy that allows the system to switch between state A and the rewards that the system receives at each step of time R. The temporal evolution of the system is considered dependent on the current state and the action chosen, i.e. p (st + 1 | st, at). The goal of the actor is to find a policy, i.e., a (a) = p (a | s) that maximizes the total discounted reward of the system, J (s0) = Thematic function, not the Environmental Exploration Function, for 2011, irical Exploration Possibility)."}, {"heading": "2.3 Space of Exploration Policies", "text": "Political space is defined by all the possible action sequences that can be taken by the agent, or alternatively by all the different closed-loop strategies that generate such action sequences. The simplest approach is to select a single data point from the environmental database and use it to improve the model g. In this case, all possible sequences of data points (or the algorithm or sensor that is used to select them) are defined. Another case is when autonomous agents collect information by moving around. In this case, the actions usually include all the trajectories necessary to try out specific locations (or the motion commands that lead the agent to them). However, the formulation of Equation 1 is much more general in nature and can incorporate any other possible decision that the agent wants to make. An agent could try to select specific locations to maximize information, or could choose on a more abstract level between different regions, for example by starting off the idea or driving the beach."}, {"heading": "2.4 Cost", "text": "The term C (\u03c0) represents the cost of the policy and we assume that any measure taken after \u03c0 incurs costs that are independent of future actions. However, the cost of a measure may depend on the history of the actions and states of the actor. In fact, modelling this dependence is an important design decision, especially for autonomous actors. Figure 3 illustrates the implications of this dependence. In the first example, the cost of an action C (at) only depends on the action. This is usually the case for costs associated with capturing the environment. In the second case, the cost C (at | at \u2212 1) depends on the previous action as it implies a non-cost movement. Naturally, this type of cost appears for autonomous actors who need to move from one location to another. In many cases, the costs will consist of a combination of various costs that may or may not depend individually on previous actions."}, {"heading": "2.5 Loss and Active Learning Tasks", "text": "V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V V-V-V-V-V-V V-V-V V V V-V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V"}, {"heading": "2.6 Measures of Information", "text": "The calculation of the information gain of a given sample is a difficult task, which can be very expensive or unsolvable in arithmetical terms. Furthermore, it can be implemented in many ways, depending on how the information is defined and on the assumptions and decisions regarding losses, costs and representation. Furthermore, we note that in some cases, due to interdependencies between all points, the order in which the samples are obtained may be relevant. The following classification follows the one proposed in (Settles, 2009) (see also MacKay, 1992; Settles, 2009) and supplements it by incorporating empirical measures as a different method for assessing the information gain of a sample. The second class of measures aims to take into account those cases in which there is no single model covering the entire state or in which the actors lack the knowledge to choose which is the best (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007)."}, {"heading": "2.6.1 Uncertainty sampling and Entropy", "text": "Formally, this can be modelled as entropy of output. Uncertainty samples, where the query is made where the classifier is most uncertain (Lewis and Gale, 1994), are still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Hungarian, 2007), to name but a few."}, {"heading": "2.6.2 Minimizing the version space", "text": "The version space defines the subset of all possible models (or parameters of a model) that are consistent with the current samples, and therefore provides a number of hypotheses about which we are still undecided; this space generally cannot be calculated; it has been approached in many different ways. An initial model considered selective sampling (Cohn et al., 1994), where a pool or stream of unlabeled examples exists and the student can request the labels to an oracle, with the aim of minimizing the amount of labeled data in order to learn the concepts with fixed accuracy. A query by a committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of discrepancy between the committee. A different perspective was proposed by (Angluin, 1988) to find the correct hypothesis by means of member queries. In this method, the learner is considered to be a hypotheses of a class and must be identified."}, {"heading": "2.6.3 Variance reduction", "text": "The reduction of variance is aimed at selecting the sample (s) that minimizes the variance of the estimate for unmarked samples (Cohn et al., 1996). For some specific regression problems (e.g. linear regression or Gaussian mixture models), solutions are available in closed form. In other cases, the variance is calculated using a number of possible unmarked examples that can be computationally expensive. Finally, there are other decision-theoretical measures such as the expected model change (Settles et al., 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007), which select the sample that is expected to result in the largest change in the model parameters or the largest reduction in the generalization error."}, {"heading": "2.6.4 Empirical Measures", "text": "In fact, it is such that most of them are able to outdo themselves. (...) Indeed, it is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) It is such that they are able to outdo themselves. (...) () () () () () () (() () () () () () () () () () () () () () () () () () () () () () (()) () () () () ()) () () () () () () () ()) () () () () () () () () ()) () () () () () () ()) () () () ()) () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() () () (() () () (() (() (() () (() (() () ((() () () (() () ((() () () () () () (() () () ((() (() ((() () () (((() () ((((() () (((())) ((((("}, {"heading": "2.7 Solving strategies", "text": "The optimal exploration problem defined in Eq.1 is mathematically insoluble in its most general case. Note that our aim is to find an exploration policy or algorithm that is able to minimize the amount of data needed while minimizing the loss. In Figure 1, this would amount to selecting the one that best fits among all possible paths with equivalent costs. Furthermore, common statistical learning theory does not directly apply to most active learning algorithms, and it is difficult to obtain theoretical guarantees about their properties, the main reason being that most theory of learning is based on the assumption that data is acquired randomly, i.e. the training data comes from one distribution rather than the real data, while in active learning the agents themselves select the next data point."}, {"heading": "2.7.1 Theoretical guarantees for binary search", "text": "Despite previous observations, there are several cases where it is possible to show that active learning brings benefits and provides some guarantees. (Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can bring in different problem classes. For example, (Dasgupta, 2005; Dasgupta, 2011) investigated the problem of actively finding the optimal threshold in line with a separable classification problem. A binary search applied to this problem leads to an exponential increase in sample efficiency. Under what conditions and for what problems this gain is currently in force. As discussed by the authors, in the worst case, it may still be necessary to classify the entire dataset in order to identify the best possible classifier. However, if we consider the average case and expected learning quality for limited sample sizes, the results show that we can achieve exponential improvements over random exploration. In fact, other authors have shown that general search algorithms can be derived for 2011."}, {"heading": "2.7.2 Greedy methods", "text": "Many practical solutions are greedy, i.e. they only look at maximizing directly a function. We take the difference between a greedy approach that directly maximizes a function and a short-sighted approach that ignores the long-term effects of these decisions. As we are discussing now, there are cases where greedy methods are not short-sighted. The question is how far greedy solutions are from the optimal exploration strategy. Generally, this is a complex combinatorial problem. If the loss function that is minimized exhibits some structural problems, then some guarantees can be found that correlate the sample complexity of a given algorithm with the possible best polynomial time algorithms. In this approach, the submodular property has been widely used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012). Submodular functions are functions that observe the decreasing yield."}, {"heading": "2.7.3 Approximate Exploration", "text": "Two of the most influential papers on these issues are E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003), both of which take into account how often a state-action pair has been consulted to decide whether further exploration is required or whether the model (in a PAC environment) can be trusted enough to be used for planning purposes. With different technical details, both algorithms guarantee that the system is highly likely to learn policies whose value is close to optimum. Some other approaches consider limited forward-looking planning to come close to solving this problem (Sim and Roy, 2005; Krause and Guestrin, 2007)."}, {"heading": "2.7.4 No-regret", "text": "In the field of multi-armed bandits, several algorithms have been developed that can solve the problem of optimization (Victor Gabillon et al., 2011) or learning (Carpentier et al., 2011) with the best possible regret, sometimes taking into account specific knowledge of the statistical properties of each arm, but in many cases adopting a non-distributive approach (Auer et al., 2003)."}, {"heading": "3 Exploration", "text": "In this section, we will present the main approaches to active learning, especially in systems with physical limitations, i.e. where costs depend on the state. In this section, we will break down the literature according to what is chosen as an exploratory policy. Distinctions are not clear in some cases, and some papers contain aspects of more than one problem or can be considered from different perspectives. We will consider three different parts: greedy selection of points where C (at | at \u2212 1) = C (at) and consideration of a choice between an infinite number of points or a finite set, the last part taking into account cases where the selection explicitly takes into account C (at \u2212 1) and longer time horizons. There is already a wide variety of approaches, but mainly corresponding to the division of classical active learning, multi-armed bandits and exploratory exploitation in the field of enhanced learning. We are interested in applications related to generally autonomous agents and will only consider approaches that focus on active classical learning methods when they provide a novel idea."}, {"heading": "3.1 Single-Point Exploration", "text": "This section describes work that selects at each step which observation point is best explored without any explicit long-term planning. It is the most common environment for active learning in functional approximation problems (Settles, 2009), with examples ranging from vehicle recognition (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al., 2007), and others. Note that, as seen in Section 2.7, in some cases measurements of information have been defined where a greedy choice (quasi) is optimal. Figure 5 provides an example of this setting in which a robot can attempt to capture an object at any point to determine the likelihood of success; with each new attempt, the robot can still choose between the same (infinite) stops."}, {"heading": "3.1.1 Learning reliability of actions", "text": "Another example of the use of active learning under these conditions and with a particular interest in physical systems is to learn the reliability of actions (Saxena et al., 2006).This process requires a large database of synthetically generated capture points (as originally proposed by (Saxena et al., 2006)) or alternatively to actively search and select where capture actions are to be applied to measure their success (Salganicoff et al., 1996; Morales et al., 2004).Another approach proposed by (Montesano et al., 2009; Montesano and Lopes, 2012) is the active search and selection of capture actions to measure their success (Salganicoff et al., 1996; Morales et al., 2004)."}, {"heading": "3.1.2 Learning general input-output relations", "text": "A simple case is learning forward-looking kinematic or dynamic models of robots, but it can also be the effects of time-extended strategies such as walking. In order to learn the dynamic model of a robot (MartinezCantin et al., 2010), the authors considered which metric to take next to improve the model. They consider a model parameterized by the location and orientation of a rigid body, and their goal is to learn such parameters as quickly as possible, relying on uncertainty measures such as an optimal one. In non-parametric models, several papers learn different models of robotic kinematics by using either close neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al., 2011). Empirical measures of learning progress have been applied by (Baranes and Oudeyer, 2012) and (Rolf et al., 2011)."}, {"heading": "3.1.3 Policies", "text": "This setting is discussed in detail in Section 5.3. (Chernova and Veloso, 2009), using support vector machines as a classification method. Confidence in predicting SVM is considered by the authors, and as the robot moves, it interrogates the teacher when this confidence is low. As part of the formalism of inverse reinforcement learning, requests are made to a user to determine the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012)."}, {"heading": "3.2 Multi-Armed Bandits", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2.1 Multiple (Sub-)Tasks", "text": "In this case, we considered a number of possible choices that need to be made to learn another (partial) task, which can be predefined or acquired autonomously (see Section 4) in order to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002). Multi-task problems have been taken into account in classification tasks (Qi et al., 2008; Reichart et al., 2008), where active learning methods are used to improve not just one task, but the overall quality of the various tasks. More interesting for our discussion are the work of (Singh et al., 2005; Oudeyer et al., 2007). The authors share the problem of learning complex agentarian tasks in terms of learning a macroeconomic action, or predictive models in which an autonomous manner emerges (see Section 4)."}, {"heading": "3.2.2 Multiple Strategies", "text": "The other big perspective is to take into account that decisions are the different methods that can be used to learn from the task, in which case a single task is often considered. This learning approach makes it clear that a learning problem is extremely dependent on the method of collecting the data and the algorithm used to learn the task. Other approaches include choosing between the different teachers that can be observed (Price and Boutilier, 2003), where some of them may not even be cooperative (Shon et al., 2007), or even choosing whether to look for / ask for a teacher demonstration or do self-research (Nguyen and Oudeyer, 2012).Another approach looks at the problem of having a different representation and choosing the best one. The representation that brings more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).The aforementioned work by (Lopes andOudeyer, 2012) also showed that the same algorithm that can be used most quickly to select the exploration strategy."}, {"heading": "3.3 Long-term exploration", "text": "An example from the real world is the selection of informative paths for environmental monitoring, see Figure 7. We divide this section into two parts: a first part entitled Exploration in dynamic systems with regard to exploration, which takes into account the dynamic constraints of the system, and another, which takes into account similar aspects specific to Reinforcement Learning and Markov decision-making processes. We make this distinction based on the different communities, formalities and metrics commonly used in each area."}, {"heading": "3.3.1 Exploration in Dynamical Systems", "text": "The most representative example of such a problem is one of the best-studied problems in robotics: Simultaneous Localization and Mapping (SLAM), which aims to create a map of an unknown environment while tracking the position of the robot within it. Early work focuses on more active localization, with the goal of actively moving the robot to obtain a better localization. In (Fox et al., 1998) belief in the robot position and orientation is gained by means of a Monte Carlo algorithm. Actions are selected based on the expected entropy of the robot location. A number of predefined relative movements are taken into account and only the moving costs are taken into account. Initial attempts to actively explore the environment during SLAM are aimed at maximizing the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss, 2003 and Stachard, al.)."}, {"heading": "3.3.2 Exploration / Exploitation", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to determine themselves. (...) In fact, it is the case that they are able to determine themselves. (...) In fact, it is the case that they are able to determine themselves. (...) In fact, it is the case that they are able to determine themselves. (...) It is the case that they are able to determine themselves. (...) In fact, it is the case that they are able to determine themselves. (...) It is as if they are able to determine themselves what they want. (...)"}, {"heading": "3.4 Others", "text": "There are other exploration methods that do not fit well into the previously defined structure, in most cases because they do not explicitly model uncertainty. Relevant examples relate to political search and active vision. Other cases combine different methods to achieve different goals."}, {"heading": "3.4.1 Mixed Approaches", "text": "There are several methods that involve multiple levels of active learning to accomplish complex tasks, see Figure 8.In (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010), the authors would like to learn a dynamic model of a robot arm or a good map of the environment with minimal data. To do this, it is necessary to find a trajectory consisting of a sequence of via points that reduces uncertainty on the estimator as quickly as possible. Another example is the SAGG-RIAC architecture (Baranes and Oudeyer, 2012). In this system, a hierarchy of forward models is learned, and for this purpose, it actively makes decisions on two levels: in a target space, it selects which topic / commands are to test (i.e., which one to pursue), and in order to achieve higher goals."}, {"heading": "3.4.2 Implicit exploration", "text": "Even if such a process of data collection is explicit in many cases, other situations, although strongly dependent on the same process, are approached only implicitly or as a side effect of an optimization process (Deisenroth et al., 2013). The most noteworthy example is all political gradient methods and similar approaches (Sutton et al., 2000; Kober et al., 2013), in which the learner attempts to directly optimize a given policy and the reward associated with it. Some methods consider stochastic strategies and the noise of policy is used for data exploration and collection (Peters et al., 2005). Exploration is reduced under the same process that adjusts the parameters to improve the expected reward. Another line of research is to use more classical optimization methods to find the best parameters that maximize a reward function (Stulp and Sigaud, 2012)."}, {"heading": "3.4.3 Active Perception", "text": "Another common use of the word active is active perception. Originally, it was introduced because many problems with vision become easier when more than one image or even a video stream is available. An active movement of the camera can detect such additional information much more easily. More recently, it has been motivated by the possibilities that open up when a robot acts in the environment to discover world characteristics. This idea has been applied, among other things, to segment objects and their properties (Fitzpatrick et al., 2003), unique and modeled articulated objects (Katz et al., 2008), unique sound (Berglund und und Morte, 2005), etc. Attention can also be seen as an example of active perception (Meger et al., 2008), which represents an attention system and learning in a real environment to learn about objects with SIFTs, and finally, an active approach in highly confusing environments can also offer significant benefits (van Hoof et al, 2012)."}, {"heading": "3.5 Open Challenges", "text": "Under the Exploration label, we looked at several areas, including standard active learning, exploration and exploitation problems, multi-armed bandits, and general online learning problems. All of these problems already have a large research facility, but there are still many open challenges. Obviously, a lot of work is still needed to expand the problem classes that can be actively amplified in an efficient manner. In all the contexts we have described, there are already many different approaches, many of which come with formal guarantees. However, for each individual problem case, it is not clear which method is the most efficient in practice, or how to synthesize exploration strategies from a problem domain description. Some of the heuristics and methods, but also many of the hypotheses and models proposed in the development communities, can be natural extensions of the active learning environment. For example, there is very limited research on active learning for more complex models such as time heterogenesis (see table 4) and noise differences in many areas."}, {"heading": "4 Curiosity", "text": "In fact, most of them are in a position to decide whether they will be able to leave the country, or whether they will be able to leave it."}, {"heading": "4.1 Creating Representations", "text": "In many cases (most?), the success of a learning algorithm depends crucially on the representations selected. Any variant of feature selection is the most common approach to the problem, and it is believed that there is a large feature bank, and the learning algorithm selects a good subset of it, taking scarcity or other criteria into account. Nevertheless, the problem is not trivial, and most heuristics will inevitably fail in most cases (Guyon and Elisseeff, 2003). Some work focuses solely on the perceptual abilities of agents. For example (Meng and Lee, 2008), basic radial functions grow to learn mapping between sensory modalities by sampling sites with a high error. For discussion of this document, especially in this section, the most relevant works are those that consider not only the best representation for a particular task, but those that have a mithierarchy."}, {"heading": "4.2 Bounded Rationality", "text": "There are several models of artificial curiosity or intrinsic motivation systems that generally guide the behavior of the agent into novel situations. These models offer exploration bonuses, sometimes referred to as intrinsic rewards, to draw attention to such novel situations. In many situations, the advantages of such models for an autonomous agent are not clear. An interesting perspective may be that of limited rationality. Even if agents would be able to see the entire environment in which they may not be able to behave optimally, another way to see these works is to take into account that the agent is living in a POMDP problem and in some cases it is possible to find another reward function that mitigates part of the part of the observation problem. A very interesting perspective has been addressed in defining the optimal reward problem (Sorg et al., 2010a). In this case, the authors believe that the learning agent is limited in his reasoning skills and that if he tries to submit the additional year that he can optimize the observed task, the additional one that is found in 2010a."}, {"heading": "4.3 Creating Skills", "text": "When an animal is faced with a new environment, there are an infinite number of different tasks that it could try to accomplish, such as learning the characteristics of all objects or understanding its own dynamics in that new environment. It can be argued that there is the only goal of survival and that each subdivision is an arbitrary construct. We agree with this view, but we consider that such a subdivision will create a number of reusable targets that can offer advantages within the single main goal.Table 4: Active Learning vs. Artificial CuriosityActive Learning Artificial CuriosityLearning with Reduced Time / Data Learning with Reduced Time / Data Changing Fixed Tasks and are Selected by Agents Everywhere parts cannot be learnable Everything can be learned within the boundary Not everything can be learned during a lifetime. Reduce the uncertainty Improve the progress of (sub) goal creation becomes one of the earliest computational models that are hierarchically motivated in 1999 (Singal);"}, {"heading": "4.3.1 Regression Models", "text": "In recent years, it has been shown that people are able to determine for themselves what they want and what they do not want. (...) In recent years, it has been shown that people are able to determine for themselves what they want. (...) In recent years, it has been shown that people are able to decide what they want. (...) In recent years, it has been shown that people are able to determine for themselves. (...) In recent years, the situation has changed. (...) In recent years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the situation has changed. (...) In the last ten years, the world has changed. (...) In the last ten years, the world has changed. (...) In the last ten years, the world has changed."}, {"heading": "4.3.2 MDP", "text": "In the case of problems formulated as MDPs, several researchers have defined automatic measures to create options or other equivalent governmental action abstractions, see (Barto and Mahadevan, 2003) for an early discussion. (Mannor et al., 2004) considered approaches such as online clustering of the governmental action space using metrics of connectivity and variance of reward values. Such a connectivity measure was introduced by (McGovern and Barto, 2001), where states that exist on multiple paths to the goals are considered partial goals and an option is initiated to achieve them. These states can be considered \"doors\" connecting between strongly connected portions of the state space. Other metrics of connectivity were proposed by (Menache et al, 2002; S-ims-ek and Barto, 2004; S-ims-ek et al, 2005; Simsek and Barto, 2008). Even before the introduction of optionalism formalism (Digney et al, the methodology of the neighboring states is subordinated at the level, the subordinated at the level of the hierarchical)."}, {"heading": "4.4 Diversity and Competence", "text": "For many learning problems, we can define multiple spaces of parameters, usually the input parameters and the resulting behaviors are trivial concepts. Most of the previous concepts can be applied in different spaces and in many cases, and depending on the metric of learning, there is a decision as to which of these spaces is better to use when leading the exploration. The robot could detect outstanding events in the perception space, or generate new references, in the control room of a robot or in the environment. Although it comes from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavioral space could be more efficient and relevant than in the space of parameters generating this behavior. The first perspective proposed by (Lehman and Stanley, 2011) is that many different genetic controllers encodings could lead to very similar behaviors, and even if the morphological and environmental constraints are considered much smaller in the beta space."}, {"heading": "4.5 Development", "text": "Several authors argue that data-driven approaches need to be combined with pre-structured information. Examples of artificial development are that the learning process is guided not only by the environment and the data it gathers, but also by the \"genetic information\" of the system (Elman, 1997; Lungarella et al., 2003).In the living organism, maturity constraints are thought to help reduce the complexity of learning in the early stages, leading to better and more efficient learning in the long term by structuring the perception and movement space (Nagai et al., 2006; Lee et al., 2007; Lopes et al., 2007; Lapeyre et al., 2011; Baranes and Oudeyer et al., 2011; Oudeyer et al., 2013) or by developing intrinsic rewards that draw attention to informative experiences (Baldassarre, 2011; Singh et al., 2011; Bares and Oudeyer et al, 2013)."}, {"heading": "4.6 Open Challenges", "text": "Evolutionary models (Singh et al., 2010b) and more recent neuroscience studies (Gottlieb et al., 2013) are beginning to give a clearer picture of whether and why curiosity is an essential driver in many animals. A clear understanding of why this drive exists, what triggers the drive to learn new tasks, and why agents seek complex situations will provide many insights into human cognition and the development of autonomous and robust agents. A related discussion is that a purely data-driven approach cannot take into account such long-term learning problems. If we look at large domain problems that vary over time, the need for prior information that provides exploration limitations will be a fundamental aspect for any algorithm. These developmental limitations and all genetic information will be fundamental to any such endeavor."}, {"heading": "5 Interaction", "text": "The previous sections, which are considered active learning, where the agents act or query, and either the environment or an oracle provide more data. Such abstract formalism might not be the best model if the oracle is a person with specific reasoning skills. Humans have an enormous amount of prior knowledge, conclusions that allow them to solve very complex problems and thus find a benevolent teacher who provides information for learning (Ekvall and Kragic, 2004), information about the task set (Lopes et al., 2007), information about the task set (Calinon et al., 2007), information about affortations (Ekvall and Kragic, 2004), information about representation (Lopes et al.), explains this process where the world state, the signals produced by the teacher, and the signal needed for the learners."}, {"heading": "5.1 Interactive Learning Scenarios", "text": "The type of feedback a person gives depends on the task of describing human knowledge, how easy it is to provide any kind of information that is able to provide information. In some cases, a combination of both is also required, for example, when it comes to making a numerical assessment of the various strategies."}, {"heading": "5.2 Human Behavior", "text": "Humans change their behavior when demonstrating other actions (Nagai and Rohlfing, 2009), which may help the learner by drawing attention to the relevant parts of the actions, but it also shows that humans will change the way a task is performed (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012). It is now clear that when teaching robots, there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011). Importantly, feedback is often ambiguous and diverges from the mathematical interpretation of a reward or a sample of a policy. For example, teachers (Thomaz and Breazeal, 2008) have often given a reward for exploratory actions, even if the signal was used as the default reward. In some problems, we can define an optimal teaching sequence, but Breazeal people do not adhere to developed strategies (Thomaz and Thomlfing, 2008)."}, {"heading": "5.3 Active Learning by Demonstration", "text": "Typically, the burden of selecting informative demonstrations is entirely on the teacher's side. Active learning approaches give the student the power to choose which demonstrations the teacher should conduct. Several criteria have been suggested: game theory approaches (Shon et al., 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), committee query (Judah et al., 2012), membership queries (Melo andLopes, 2013), maximum classification uncertainty (Chernova et al., 2009), expected short-sighted increase (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al., 2008). A common goal is to find the right behavior, defined as that corresponds to the teacher, by repeatedly asking for the right behavior in a given situation."}, {"heading": "5.3.1 Learning Policies", "text": "Another learning task of interest is to acquire political strategies by querying an oracle. (Chernova and Veloso, 2009) This uncertainty is applied online taking into account the sampling perspective, and therefore only questions are asked in states that are actually encountered by the robot. One problem with this approach is that information about the dynamics of the environment is not taken into account when learning policy. To address this problem, (Melo and Lopes, 2010) a method has been proposed that calculates a kernel based on MDP metrics (Taylor et al., 2008), which includes information about environmental dynamics. In this way, the topology of the dynamics is better preserved and thus better results can be obtained, then with just a simple classifier with a na\u00efve core. They use the method proposed by (Montesano and Lopes, 2012) to ask questions where there is less trust."}, {"heading": "5.4 Online Feedback and Guidance", "text": "Another approach is to take into account that the robot always performs the reward function, and that a teacher / user could interrupt it at any time and accept the robot's command. These corrections will act as new demonstrations that are incorporated into the learning process. Interestingly, MDP reward is low in information, but it is sampled from the process, while human amplification is rich in information, but may have greater distortions. Knox (Knox and Stone, 2009; Knox and Stone, 2010) presents the initial framework within which the user learns to predict human feedback and then selects measures to maximize the expected reward from humans. After learning to predict such behavior during the agent's learning, the reward from the environment is also observed, and the combination of both allows the robot to better deal with information given by the user."}, {"heading": "5.5 Ambiguous Protocols and Teacher Adaptation", "text": "In most previous discussions, we considered that the teacher's feedback signals have a semantic meaning known to the learner. However, in many cases, the teacher's signals may be too loud or have an unknown meaning. Several of these works fall within the framework of communication (Klingspor et al., 1997), where a common understanding between the robot and the teacher is essential to facilitate good interactive learning.The system in (Mohammad and Nishida, 2010) automatically learns different interaction protocols for navigation tasks, where the robot learns the actions it should perform and which gestures correspond to these actions. In (Lopes et al., 2011; Grizou et al., 2013), the authors introduce a new algorithm for inverse reinforcement learning among multiple instructions with unknown symbols. At each step, the learner performs an action and waits for the teacher's feedback."}, {"heading": "5.6 Open Challenges", "text": "There are two major challenges in interactive systems: one is to clearly understand the theoretical preconditions of such systems; empirical results seem to suggest that an interactive approach is more sample efficient than any individual approach; another is the relationship between active learning and optimal teaching, where there is no clear understanding of the problems that can be efficiently learned but not taught, and vice versa; and the second challenge is to accurately model the cognitive-representative differences between teacher and learner during the interactive learning process, including how to create a common representation of the problem; how to create interaction protocols and physical interfaces that enable such a common understanding; and how to use the multimodal stimuli that humans provide during teaching and interaction."}, {"heading": "6 Final Remarks", "text": "In this document, we have outlined a general perspective on actors who aim to learn quickly and search for the most important information needed. To our knowledge, this is the first time that a unified view has been taken of the methods and objectives of different communities. In all these areas, several further developments are still needed, but there is already an opportunity for a more multidisciplinary perspective that can lead to more advanced methods."}, {"heading": "1 Introduction 1", "text": "1.1 Exploration.........................................................................................................."}, {"heading": "2 Active Learning for Autonomous Intelligent Agents 4", "text": "2.1 Optimal Exploration Problem.. 4 2.2 Learning Setups.. 4 2.2 Learning Setups. 6 2.2.3 MDP. 7 2.5 Loss and Active Learning Tasks. 6 2.2.3 Space of Exploration Policies. 6 2.2.2 Multi-Armed Bandits. 6. 6. 6. 6. 6. 6. 7. 8. 6. 8. 6. 8. 8. 8. 6. 6. 7. 7. 2.5 Loss and Active Learning Tasks. 7. 2.6. Information Policy. 8. 6. 6. Uncertainty Sampling and Entropy 8 2.6.2 Minimizing the version space. 8 2.6.3. Deviation. 7. 2.5. Loss and Active Learning Tasks. 9. 9. 2.6.4. Empirical Measures."}, {"heading": "3 Exploration 11", "text": "3.1 Single point exploration..... 123.1.1 Learning reliability of actions. 12 3.1.2 Learning general input-output relationships.......... 12 3.1.3 Politics..................... 133.2 Multi-armed bandits.......... 133.2.1 Multiple (sub-) tasks................. 14. 3.2.2 Multiple strategies............. 143.3 Long-term exploration........... Mixed approaches........ 17 3.3.2 Exploration / exploitation........................................ 163.4 Others........................"}, {"heading": "4 Curiosity 19", "text": "4.1 Creation of representations...... 20 4.2 Limited rationality........ 20 4.3 Creation of skills................ 204.3.1 Regression models......... 21 4.3.2 MDP................. 224.4 Diversity and competence......... 22 4.5 Development.............. 23. 4.6 Open challenges..........."}, {"heading": "5 Interaction 23", "text": "5.1 Interactive learning scenarios...... 25 5.2 Human behaviour............ 25 5.3 Active learning through demonstration... 265.3.1 Learning policies......... 27 5.4 Online feedback and guidance...... 27 5.5 Ambiguous protocols and teacher adaptation................ 28 5.6 Open challenges.............."}], "references": [{"title": "Models for autonomously motivated exploration in reinforcement learning", "author": ["P. Auer", "S.H. Lim", "C. Watkins"], "venue": "Proceedings of the 22nd international conference on Algorithmic learning theory, ALT\u201911, pages 14\u201317, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "Auer et al\\.,? 2011", "shortCiteRegEx": "Auer et al\\.", "year": 2011}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "Proc. of the 8-th Conf. on Intelligent Autonomous Systems, pages 438\u2013445.", "citeRegEx": "Bakker and Schmidhuber,? 2004", "shortCiteRegEx": "Bakker and Schmidhuber", "year": 2004}, {"title": "The true sample complexity of active learning", "author": ["M.F. Balcan", "S. Hanneke", "J. Wortman"], "venue": "Conf. on Learning Theory (COLT).", "citeRegEx": "Balcan et al\\.,? 2008", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "What are intrinsic motivations? a biological perspective", "author": ["G. Baldassarre"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Baldassarre,? 2011", "shortCiteRegEx": "Baldassarre", "year": 2011}, {"title": "Online choice of active learning algorithms", "author": ["Y. Baram", "R. El-Yaniv", "K. Luz"], "venue": "The Journal of Machine Learning Research, 5:255\u2013291.", "citeRegEx": "Baram et al\\.,? 2004", "shortCiteRegEx": "Baram et al\\.", "year": 2004}, {"title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ Inter. Conf. on, pages 1766\u20131773.", "citeRegEx": "Baranes and Oudeyer,? 2010", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2010}, {"title": "The interaction of maturational constraints and intrinsic motivations in active motor development", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Baranes and Oudeyer,? 2011", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2011}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Robotics and Autonomous Systems.", "citeRegEx": "Baranes and Oudeyer,? 2012", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2012}, {"title": "R-iac: Robust intrinsically motivated exploration and active learning", "author": ["A. Baran\u00e8s", "Oudeyer", "P.-Y."], "venue": "Autonomous Mental Development, IEEE Transactions on, 1(3):155\u2013169.", "citeRegEx": "Baran\u00e8s et al\\.,? 2009", "shortCiteRegEx": "Baran\u00e8s et al\\.", "year": 2009}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems, 13(4):341\u2013379.", "citeRegEx": "Barto and Mahadevan,? 2003", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A. Barto", "S. Singh", "N. Chentanez"], "venue": "Inter. Conf. on development and learning (ICDL\u201904), San Diego, USA.", "citeRegEx": "Barto et al\\.,? 2004", "shortCiteRegEx": "Barto et al\\.", "year": 2004}, {"title": "Neural net algorithms that learn in polynomial time from examples and queries", "author": ["E.B. Baum"], "venue": "Neural Networks, IEEE Transactions on, 2(1):5\u201319.", "citeRegEx": "Baum,? 1991", "shortCiteRegEx": "Baum", "year": 1991}, {"title": "On the theory of dynamic programming", "author": ["R. Bellman"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 38(8):716. 29", "citeRegEx": "Bellman,? 1952", "shortCiteRegEx": "Bellman", "year": 1952}, {"title": "Sound source localisation through active audition", "author": ["E. Berglund", "J. Sitte"], "venue": "Intelligent Robots and Systems, 2005.(IROS 2005). 2005 IEEE/RSJ Inter. Conf. on, pages 653\u2013658.", "citeRegEx": "Berglund and Sitte,? 2005", "shortCiteRegEx": "Berglund and Sitte", "year": 2005}, {"title": "Conflict, arousal, and curiosity", "author": ["D. Berlyne"], "venue": "McGraw-Hill Book Company.", "citeRegEx": "Berlyne,? 1960", "shortCiteRegEx": "Berlyne", "year": 1960}, {"title": "A formalism for learning from demonstration", "author": ["E. Billing", "T. Hellstr\u00f6m"], "venue": "Paladyn. Journal of Behavioral Robotics, 1(1):1\u201313.", "citeRegEx": "Billing and Hellstr\u00f6m,? 2010", "shortCiteRegEx": "Billing and Hellstr\u00f6m", "year": 2010}, {"title": "Information based adaptive robotic exploration", "author": ["F. Bourgault", "A. Makarenko", "S. Williams", "B. Grocholsky", "H. Durrant-Whyte"], "venue": "IEEE/RSJ Conf. on Intelligent Robots and Systems (IROS).", "citeRegEx": "Bourgault et al\\.,? 2002", "shortCiteRegEx": "Bourgault et al\\.", "year": 2002}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research, 3:213\u2013231.", "citeRegEx": "Brafman and Tennenholtz,? 2003", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2003}, {"title": "Local utility elicitation in gai models", "author": ["D. Braziunas", "C. Boutilier"], "venue": "Twenty-first Conf. on Uncertainty in Artificial Intelligence, pages 42\u201349.", "citeRegEx": "Braziunas and Boutilier,? 2005", "shortCiteRegEx": "Braziunas and Boutilier", "year": 2005}, {"title": "Tutelage and collaboration for humanoid robots", "author": ["C. Breazeal", "A. Brooks", "J. Gray", "G. Hoffman", "J. Lieberman", "H. Lee", "A.L. Thomaz", "D. Mulanda."], "venue": "Inter. Journal of Humanoid Robotics, 1(2).", "citeRegEx": "Breazeal et al\\.,? 2004", "shortCiteRegEx": "Breazeal et al\\.", "year": 2004}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V. Cora", "N. De Freitas"], "venue": "Arxiv preprint arXiv:1012.2599.", "citeRegEx": "Brochu et al\\.,? 2010", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Active preference learning with discrete choice data", "author": ["E. Brochu", "N. de Freitas", "A. Ghosh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Brochu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2007}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends R", "citeRegEx": "Bubeck and Cesa.Bianchi,? 2012", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Seeing actions as hierarchically organised structures: great ape manualskills", "author": ["R.W. Byrne"], "venue": "The Imitative Mind. Cambridge University Press.", "citeRegEx": "Byrne,? 2002", "shortCiteRegEx": "Byrne", "year": 2002}, {"title": "Designing interactions for robot active learners", "author": ["M. Cakmak", "C. Chao", "A. Thomaz"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2):108\u2013 118.", "citeRegEx": "Cakmak et al\\.,? 2010a", "shortCiteRegEx": "Cakmak et al\\.", "year": 2010}, {"title": "Exploiting social partners in robot learning", "author": ["M. Cakmak", "N. DePalma", "R. Arriaga", "A. Thomaz"], "venue": "Autonomous Robots.", "citeRegEx": "Cakmak et al\\.,? 2010b", "shortCiteRegEx": "Cakmak et al\\.", "year": 2010}, {"title": "Algorithmic and human teaching of sequential decision tasks", "author": ["M. Cakmak", "M. Lopes"], "venue": "AAAI Conference on Artificial Intelligence (AAAI\u201912), Toronto, Canada.", "citeRegEx": "Cakmak and Lopes,? 2012", "shortCiteRegEx": "Cakmak and Lopes", "year": 2012}, {"title": "Optimality of human teachers for robot learners", "author": ["M. Cakmak", "A. Thomaz"], "venue": "Inter. Conf. on Development and Learning (ICDL).", "citeRegEx": "Cakmak and Thomaz,? 2010", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2010}, {"title": "Active learning with mixed query types in learning from demonstration", "author": ["M. Cakmak", "A. Thomaz"], "venue": "Proc. of the ICML Workshop on New Developments in Imitation Learning.", "citeRegEx": "Cakmak and Thomaz,? 2011", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2011}, {"title": "Designing robot learners that ask good questions", "author": ["M. Cakmak", "A. Thomaz"], "venue": "7th ACM/IEE Inter. Conf. on Human-Robot Interaction.", "citeRegEx": "Cakmak and Thomaz,? 2012", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2012}, {"title": "On learning, representing and generalizing a task in a humanoid robot", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part B. Special issue on robot learning by observation, demonstration and imitation, 37(2):286\u2013", "citeRegEx": "Calinon et al\\.,? 2007", "shortCiteRegEx": "Calinon et al\\.", "year": 2007}, {"title": "Upper confidence bounds algorithms for active learning in multi-armed bandits", "author": ["A. Carpentier", "M. Ghavamzadeh", "A. Lazaric", "R. Munos", "P. Auer"], "venue": "Algorithmic Learning Theory.", "citeRegEx": "Carpentier et al\\.,? 2011", "shortCiteRegEx": "Carpentier et al\\.", "year": 2011}, {"title": "Minimax bounds for active learning", "author": ["R. Castro", "R. Novak"], "venue": "IEEE Trans. on Information Theory, 54(5):2339\u20132353.", "citeRegEx": "Castro and Novak,? 2008", "shortCiteRegEx": "Castro and Novak", "year": 2008}, {"title": "Learning exploration/exploitation strategies for single trajectory reinforcement learning", "author": ["M. Castronovo", "F. Maes", "R. Fonteneau", "D. Ernst"], "venue": "10th European Workshop on Reinforcement Learning (EWRL 2012).", "citeRegEx": "Castronovo et al\\.,? 2012", "shortCiteRegEx": "Castronovo et al\\.", "year": 2012}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "National Conf. on Artificial Intelligence, pages 363\u2013 369. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.", "citeRegEx": "Chajewska et al\\.,? 2000", "shortCiteRegEx": "Chajewska et al\\.", "year": 2000}, {"title": "Transparent active learning for robots", "author": ["C. Chao", "M. Cakmak", "A. Thomaz"], "venue": "Human-Robot Interaction (HRI), 2010 5th ACM/IEEE Inter. Conf. on, pages 317\u2013324.", "citeRegEx": "Chao et al\\.,? 2010", "shortCiteRegEx": "Chao et al\\.", "year": 2010}, {"title": "Interactive policy learning through confidence-based autonomy", "author": ["S. Chernova", "M. Veloso"], "venue": "J. Artificial Intelligence Research, 34:1\u201325.", "citeRegEx": "Chernova and Veloso,? 2009", "shortCiteRegEx": "Chernova and Veloso", "year": 2009}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning, 15(2):201\u2013221. 30", "citeRegEx": "Cohn et al\\.,? 1994", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research, 4:129\u2013145.", "citeRegEx": "Cohn et al\\.,? 1996", "shortCiteRegEx": "Cohn et al\\.", "year": 1996}, {"title": "Comparing action-query strategies in semi-autonomous agents", "author": ["R. Cohn", "E. Durfee", "S. Singh"], "venue": "Inter. Conf. on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Cohn et al\\.,? 2011", "shortCiteRegEx": "Cohn et al\\.", "year": 2011}, {"title": "Planning delayed-response queries and transient policies under reward uncertainty", "author": ["R. Cohn", "E. Durfee", "S. Singh"], "venue": "Seventh Annual Workshop on Multiagent Sequential Decision Making Under Uncertainty (MSDM-2012), page 17.", "citeRegEx": "Cohn et al\\.,? 2012", "shortCiteRegEx": "Cohn et al\\.", "year": 2012}, {"title": "Selecting Operator Queries using Expected Myopic Gain", "author": ["R. Cohn", "M. Maxim", "E. Durfee", "S. Singh"], "venue": "2010 IEEE/WIC/ACM Inter. Conf. on Web Intelligence and Intelligent Agent Technology, pages 40\u201347.", "citeRegEx": "Cohn et al\\.,? 2010", "shortCiteRegEx": "Cohn et al\\.", "year": 2010}, {"title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning", "author": ["O. \u015eim\u015fek", "A.G. Barto"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "\u015eim\u015fek and Barto,? 2004", "shortCiteRegEx": "\u015eim\u015fek and Barto", "year": 2004}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 337\u2013344.", "citeRegEx": "Dasgupta,? 2005", "shortCiteRegEx": "Dasgupta", "year": 2005}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "Theoretical computer science, 412(19):1767\u20131781.", "citeRegEx": "Dasgupta,? 2011", "shortCiteRegEx": "Dasgupta", "year": 2011}, {"title": "Bayesian q-learning", "author": ["R. Dearden", "N. Friedman", "S. Russell"], "venue": "AAAI Conf. on Artificial Intelligence, pages 761\u2013768.", "citeRegEx": "Dearden et al\\.,? 1998", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "A survey on policy search for robotics", "author": ["M. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics, 21.", "citeRegEx": "Deisenroth et al\\.,? 2013", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Learning object-specific grasp affordance densities", "author": ["R. Detry", "E. Baseski", "M. P", "Y. Touati", "N. Kruger", "O. Kroemer", "J. Peters", "J. Piater"], "venue": "In IEEE 8TH Inter. Conf. on Development and Learning", "citeRegEx": "Detry et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Detry et al\\.", "year": 2009}, {"title": "Learning hierarchical control structures for multiple tasks and changing environments", "author": ["B. Digney"], "venue": "fifth Inter. Conf. on simulation of adaptive behavior on From animals to animats, volume 5, pages 321\u2013330.", "citeRegEx": "Digney,? 1998", "shortCiteRegEx": "Digney", "year": 1998}, {"title": "Teaching and learning of robot tasks via observation of human performance", "author": ["R. Dillmann"], "venue": "Robotics and Autonomous Systems, 47(2):109\u2013116.", "citeRegEx": "Dillmann,? 2004", "shortCiteRegEx": "Dillmann", "year": 2004}, {"title": "Learning robot behaviour and skills based on human demonstration and advice: the machine learning paradigm", "author": ["R. Dillmann", "O. Rogalla", "M. Ehrenmann", "R. Zollner", "M. Bordegoni"], "venue": "Inter. Symposium on Robotics Research (ISRR), volume 9, pages 229\u2013238.", "citeRegEx": "Dillmann et al\\.,? 2000", "shortCiteRegEx": "Dillmann et al\\.", "year": 2000}, {"title": "Interactive natural programming of robots: Introductory overview", "author": ["R. Dillmann", "R. Z\u00f6llner", "M. Ehrenmann", "O Rogalla"], "venue": "In Tsukuba Research Center, AIST. Citeseer", "citeRegEx": "Dillmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dillmann et al\\.", "year": 2002}, {"title": "Active learning for outdoor obstacle detection", "author": ["C. Dima", "M. Hebert"], "venue": "Robotics Science and Systems Conf., Cambridge, MA.", "citeRegEx": "Dima and Hebert,? 2005", "shortCiteRegEx": "Dima and Hebert", "year": 2005}, {"title": "Enabling learning from large datasets: Applying active learning to mobile robotics", "author": ["C. Dima", "M. Hebert", "A. Stentz"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE Inter. Conf. on, volume 1, pages 108\u2013114.", "citeRegEx": "Dima et al\\.,? 2004", "shortCiteRegEx": "Dima et al\\.", "year": 2004}, {"title": "Robot shaping: Developing autonomous agents through learning", "author": ["M. Dorigo", "M. Colombetti"], "venue": "Artificial intelligence, 71(2):321\u2013370.", "citeRegEx": "Dorigo and Colombetti,? 1994", "shortCiteRegEx": "Dorigo and Colombetti", "year": 1994}, {"title": "Reinforcement learning with limited reinforcement: using bayes risk for active learning in pomdps", "author": ["F. Doshi", "J. Pineau", "N. Roy"], "venue": "25th Inter. Conf. on Machine learning (ICML\u201908), pages 256\u2013263.", "citeRegEx": "Doshi et al\\.,? 2008", "shortCiteRegEx": "Doshi et al\\.", "year": 2008}, {"title": "Design for an optimal probe", "author": ["M. Duff"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "Duff,? 2003", "shortCiteRegEx": "Duff", "year": 2003}, {"title": "Interactive grasp learning based on human demonstration", "author": ["S. Ekvall", "D. Kragic"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE Inter. Conf. on, volume 4, pages 3519\u20133524.", "citeRegEx": "Ekvall and Kragic,? 2004", "shortCiteRegEx": "Ekvall and Kragic", "year": 2004}, {"title": "Rethinking innateness: A connectionist perspective on development, volume 10", "author": ["J. Elman"], "venue": "The MIT press.", "citeRegEx": "Elman,? 1997", "shortCiteRegEx": "Elman", "year": 1997}, {"title": "Interactive machine learning", "author": ["J. Fails", "D. Olsen Jr"], "venue": "8th Inter. Conf. on Intelligent user interfaces, pages 39\u201345.", "citeRegEx": "Fails and Jr,? 2003", "shortCiteRegEx": "Fails and Jr", "year": 2003}, {"title": "Adaptive mobile robot navigation and mapping", "author": ["H.J.S. Feder", "J.J. Leonard", "C.M. Smith"], "venue": "International Journal of Robotics Research, 18(7):650\u2013668.", "citeRegEx": "Feder et al\\.,? 1999", "shortCiteRegEx": "Feder et al\\.", "year": 1999}, {"title": "Learning about objects through action: Initial steps towards artificial cognition", "author": ["P. Fitzpatrick", "G. Metta", "L. Natale", "S. Rao", "G. Sandini."], "venue": "IEEE Inter. Conf. on Robotics and Automation, Taipei, Taiwan.", "citeRegEx": "Fitzpatrick et al\\.,? 2003", "shortCiteRegEx": "Fitzpatrick et al\\.", "year": 2003}, {"title": "Robot, asker of questions", "author": ["T. Fong", "C. Thorpe", "C. Baur"], "venue": "Robotics and Autonomous systems, 42(3):235\u2013243.", "citeRegEx": "Fong et al\\.,? 2003", "shortCiteRegEx": "Fong et al\\.", "year": 2003}, {"title": "Active markov localization for mobile robots", "author": ["D. Fox", "W. Burgard", "S. Thrun"], "venue": "Robotics and Autonomous Systems, 25(3):195\u2013207.", "citeRegEx": "Fox et al\\.,? 1998", "shortCiteRegEx": "Fox et al\\.", "year": 1998}, {"title": "A reinforcement learning algorithm with polynomial interaction complexity for only-costly-observable mdps", "author": ["R. Fox", "M. Tennenholtz"], "venue": "National Conf. on Artificial Intelligence (AAAI).", "citeRegEx": "Fox and Tennenholtz,? 2007", "shortCiteRegEx": "Fox and Tennenholtz", "year": 2007}, {"title": "Real-time hand gesture detection and recognition using boosted classifiers and active learning", "author": ["H. Francke", "J. Ruiz-del Solar", "R. Verschae"], "venue": "Advances in Image and Video Technology, pages 533\u2013547.", "citeRegEx": "Francke et al\\.,? 2007", "shortCiteRegEx": "Francke et al\\.", "year": 2007}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine learning, 28(2):133\u2013168.", "citeRegEx": "Freund et al\\.,? 1997", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Preference learning: An introduction", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier"], "venue": "Preference Learning, page 1.", "citeRegEx": "F\u00fcrnkranz and H\u00fcllermeier,? 2010", "shortCiteRegEx": "F\u00fcrnkranz and H\u00fcllermeier", "year": 2010}, {"title": "Efficient spacetime modeling for informative sensing", "author": ["S. Garg", "A. Singh", "F. Ramos"], "venue": "Sixth Inter. Workshop on Knowledge Discovery from Sensor Data, pages 52\u201360.", "citeRegEx": "Garg et al\\.,? 2012", "shortCiteRegEx": "Garg et al\\.", "year": 2012}, {"title": "Following a moving target?onte carlo inference for dynamic bayesian models", "author": ["W. Gilks", "C. Berzuini"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(1):127\u2013146.", "citeRegEx": "Gilks and Berzuini,? 2002", "shortCiteRegEx": "Gilks and Berzuini", "year": 2002}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J. Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 148\u2013177.", "citeRegEx": "Gittins,? 1979", "shortCiteRegEx": "Gittins", "year": 1979}, {"title": "Online distributed sensor selection", "author": ["D. Golovin", "M. Faulkner", "A. Krause"], "venue": "Proc. ACM/IEEE Inter. Conf. on Information Processing in Sensor Networks (IPSN).", "citeRegEx": "Golovin et al\\.,? 2010a", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Proc. Inter. Conf. on Learning Theory (COLT).", "citeRegEx": "Golovin and Krause,? 2010", "shortCiteRegEx": "Golovin and Krause", "year": 2010}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Proc. Neural Information Processing Systems (NIPS).", "citeRegEx": "Golovin et al\\.,? 2010b", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Attention, learning, and the value of information", "author": ["J. Gottlieb"], "venue": "Neuron, 76(2):281\u2013295.", "citeRegEx": "Gottlieb,? 2012", "shortCiteRegEx": "Gottlieb", "year": 2012}, {"title": "Information seeking, curiosity and attention: computational and empirical mechanisms", "author": ["J. Gottlieb", "Oudeyer", "P.-Y.", "M. Lopes", "A. Baranes"], "venue": "Trends in Cognitive Sciences.", "citeRegEx": "Gottlieb et al\\.,? 2013", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "Robot Learning Simultaneously a Task and How to Interpret Human Instructions", "author": ["J. Grizou", "M. Lopes", "Oudeyer", "P.-Y."], "venue": "Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics (ICDL-EpiRob), Osaka, Japan.", "citeRegEx": "Grizou et al\\.,? 2013", "shortCiteRegEx": "Grizou et al\\.", "year": 2013}, {"title": "Dogged learning for robots", "author": ["D. Grollman", "O. Jenkins"], "venue": "Robotics and Automation, 2007 IEEE Inter. Conf. on, pages 2483\u20132488.", "citeRegEx": "Grollman and Jenkins,? 2007a", "shortCiteRegEx": "Grollman and Jenkins", "year": 2007}, {"title": "Learning robot soccer skills from demonstration", "author": ["D. Grollman", "O. Jenkins"], "venue": "Development and Learning, 2007. ICDL 2007. IEEE 6th Inter. Conf. on, pages 276\u2013281.", "citeRegEx": "Grollman and Jenkins,? 2007b", "shortCiteRegEx": "Grollman and Jenkins", "year": 2007}, {"title": "Sparse incremental learning for interactive robot control policy estimation", "author": ["D. Grollman", "O. Jenkins"], "venue": "Robotics and Automation, 2008. ICRA 2008. IEEE Inter. Conf. on, pages 3315\u20133320.", "citeRegEx": "Grollman and Jenkins,? 2008", "shortCiteRegEx": "Grollman and Jenkins", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research, 3:1157\u20131182.", "citeRegEx": "Guyon and Elisseeff,? 2003", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Intrinsically motivated affordance discovery and modeling", "author": ["S. Hart", "R. Grupen"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems, pages 279\u2013300. Springer.", "citeRegEx": "Hart and Grupen,? 2013", "shortCiteRegEx": "Hart and Grupen", "year": 2013}, {"title": "Intrinsically motivated hierarchical manipulation", "author": ["S. Hart", "S. Sen", "R. Grupen"], "venue": "2008 IEEE Conf. on Robots and Automation (ICRA), Pasadena, California.", "citeRegEx": "Hart et al\\.,? 2008", "shortCiteRegEx": "Hart et al\\.", "year": 2008}, {"title": "Discovering hierarchy in reinforcement learning with hexq", "author": ["B. Hengst"], "venue": "MACHINE LEARNING-Inter. WORKSHOP THEN Conf.-, pages 243\u2013250. Citeseer.", "citeRegEx": "Hengst,? 2002", "shortCiteRegEx": "Hengst", "year": 2002}, {"title": "Learning exploration strategies", "author": ["T. Hester", "M. Lopes", "P. Stone"], "venue": "AAMAS, USA.", "citeRegEx": "Hester et al\\.,? 2013", "shortCiteRegEx": "Hester et al\\.", "year": 2013}, {"title": "Reinforcement Learning: State-of-the-Art, chapter Learning and Using Models", "author": ["T. Hester", "P. Stone"], "venue": "Springer.", "citeRegEx": "Hester and Stone,? 2011", "shortCiteRegEx": "Hester and Stone", "year": 2011}, {"title": "Intrinsically motivated model learning for a developing curious agent", "author": ["T. Hester", "P. Stone"], "venue": "AAMAS Workshop on Adaptive Learning Agents.", "citeRegEx": "Hester and Stone,? 2012", "shortCiteRegEx": "Hester and Stone", "year": 2012}, {"title": "Portfolio allocation for bayesian optimization", "author": ["M. Hoffman", "E. Brochu", "N. de Freitas"], "venue": "In Uncertainty in artificial intelligence,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "J. Mach. Learn. Res., 11:1563\u20131600.", "citeRegEx": "Jaksch et al\\.,? 2010", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Learning task space control through goal directed exploration", "author": ["L. Jamone", "L. Natale", "K. Hashimoto", "G. Sandini", "A. Takanishi"], "venue": "Inter. Conf. on Robotics and Biomimetics (ROBIO\u201911).", "citeRegEx": "Jamone et al\\.,? 2011", "shortCiteRegEx": "Jamone et al\\.", "year": 2011}, {"title": "Active learning in partially observable markov decision processes", "author": ["R. Jaulmes", "J. Pineau", "D. Precup"], "venue": "NIPS Workshop on Value of Information in Inference, Learning and Decision-Making.", "citeRegEx": "Jaulmes et al\\.,? 2005", "shortCiteRegEx": "Jaulmes et al\\.", "year": 2005}, {"title": "Causal graph based decomposition of factored mdps", "author": ["A. Jonsson", "A. Barto"], "venue": "The Journal of Machine Learning Research, 7:2259\u20132301.", "citeRegEx": "Jonsson and Barto,? 2006", "shortCiteRegEx": "Jonsson and Barto", "year": 2006}, {"title": "Active imitation learning via reduction to iid active learning", "author": ["K. Judah", "A. Fern", "T. Dietterich"], "venue": "UAI. 32", "citeRegEx": "Judah et al\\.,? 2012", "shortCiteRegEx": "Judah et al\\.", "year": 2012}, {"title": "Reinforcement learning via practice and critique advice", "author": ["K. Judah", "S. Roy", "A. Fern", "T. Dietterich"], "venue": "AAAI Conf. on Artificial Intelligence (AAAI-10).", "citeRegEx": "Judah et al\\.,? 2010", "shortCiteRegEx": "Judah et al\\.", "year": 2010}, {"title": "Gaussian processes for sample efficient reinforcement learning with rmax-like exploration", "author": ["T. Jung", "P. Stone"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 601\u2013616.", "citeRegEx": "Jung and Stone,? 2010", "shortCiteRegEx": "Jung and Stone", "year": 2010}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence, 101(1):99\u2013 134.", "citeRegEx": "Kaelbling et al\\.,? 1998", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. Artificial Intelligence Research, 4:237\u2013285.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Towards understanding how humans teach robots", "author": ["T. Kaochar", "R. Peralta", "C. Morrison", "I. Fasel", "T. Walsh", "P. Cohen"], "venue": "User Modeling, Adaption and Personalization, pages 347\u2013352.", "citeRegEx": "Kaochar et al\\.,? 2011", "shortCiteRegEx": "Kaochar et al\\.", "year": 2011}, {"title": "Active learning with gaussian processes for object categorization", "author": ["A. Kapoor", "K. Grauman", "R. Urtasun", "T. Darrell"], "venue": "IEEE 11th Inter. Conf. on Computer Vision.", "citeRegEx": "Kapoor et al\\.,? 2007", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Interactive classifier system for real robot learning", "author": ["D. Katagami", "S. Yamada"], "venue": "Robot and Human Interactive Communication, 2000. RO-MAN 2000. Proceedings. 9th IEEE Inter. Workshop on, pages 258\u2013 263.", "citeRegEx": "Katagami and Yamada,? 2000", "shortCiteRegEx": "Katagami and Yamada", "year": 2000}, {"title": "Learning to manipulate articulated objects in unstructured environments using a grounded relational representation", "author": ["D. Katz", "Y. Pyuro", "O. Brock"], "venue": "RSS - Robotics Science and Systems IV, Zurich, Switzerland.", "citeRegEx": "Katz et al\\.,? 2008", "shortCiteRegEx": "Katz et al\\.", "year": 2008}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, 49(2):209\u2013232.", "citeRegEx": "Kearns and Singh,? 2002", "shortCiteRegEx": "Kearns and Singh", "year": 2002}, {"title": "Functional genomic hypothesis generation and experimentation by a robot scientist", "author": ["R. King", "K. Whelan", "F. Jones", "P. Reiser", "C. Bryant", "S. Muggleton", "D. Kell", "S. Oliver"], "venue": "Nature, 427(6971):247\u2013252.", "citeRegEx": "King et al\\.,? 2004", "shortCiteRegEx": "King et al\\.", "year": 2004}, {"title": "Humanrobot communication and machine learning", "author": ["V. Klingspor", "J. Demiris", "M. Kaiser"], "venue": "Applied Artificial Intelligence, 11(7):719\u2013746.", "citeRegEx": "Klingspor et al\\.,? 1997", "shortCiteRegEx": "Klingspor et al\\.", "year": 1997}, {"title": "Navigation planning in probabilistic roadmaps with uncertainty", "author": ["M. Kneebone", "R. Dearden"], "venue": "ICAPS. AAAI.", "citeRegEx": "Kneebone and Dearden,? 2009", "shortCiteRegEx": "Kneebone and Dearden", "year": 2009}, {"title": "How humans teach agents: A new experimental perspective", "author": ["W. Knox", "B. Glass", "B. Love", "W. Maddox", "P. Stone"], "venue": "Inter. Journal of Social Robotics, Special Issue on Robot Learning from Demonstration.", "citeRegEx": "Knox et al\\.,? 2012", "shortCiteRegEx": "Knox et al\\.", "year": 2012}, {"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W. Knox", "P. Stone"], "venue": "fifth Inter. Conf. on Knowledge capture, pages 9\u201316.", "citeRegEx": "Knox and Stone,? 2009", "shortCiteRegEx": "Knox and Stone", "year": 2009}, {"title": "Combining manual feedback with subsequent mdp reward signals for reinforcement learning", "author": ["W. Knox", "P. Stone"], "venue": "9th Inter. Conf. on Autonomous Agents and Multiagent Systems (AAMAS\u201910), pages 5\u201312.", "citeRegEx": "Knox and Stone,? 2010", "shortCiteRegEx": "Knox and Stone", "year": 2010}, {"title": "Reinforcement learning from simultaneous human and mdp reward", "author": ["W. Knox", "P. Stone"], "venue": "11th Inter. Conf. on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Knox and Stone,? 2012", "shortCiteRegEx": "Knox and Stone", "year": 2012}, {"title": "Reinforcement learning in robotics: a survey", "author": ["J. Kober", "D. Bagnell", "J. Peters"], "venue": "Inter. Journal of Robotics Research, 32(11):12361272.", "citeRegEx": "Kober et al\\.,? 2013", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Grounding verbs of motion in natural language commands to robots", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "Inter. Symposium on Experimental Robotics (ISER), New Delhi, India.", "citeRegEx": "Kollar et al\\.,? 2010", "shortCiteRegEx": "Kollar et al\\.", "year": 2010}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J. Kolter", "A. Ng"], "venue": "26th Annual Inter. Conf. on Machine Learning, pages 513\u2013520.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Sensorimotor abstraction selection for efficient, autonomous robot skill acquisition", "author": ["G. Konidaris", "A. Barto"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201908).", "citeRegEx": "Konidaris and Barto,? 2008", "shortCiteRegEx": "Konidaris and Barto", "year": 2008}, {"title": "Instructing a reinforcement learner", "author": ["V.N. Korupolu", "P.", "M. Sivamurugan", "B. Ravindran"], "venue": "TwentyFifth Inter. FLAIRS Conf.", "citeRegEx": "Korupolu et al\\.,? 2012", "shortCiteRegEx": "Korupolu et al\\.", "year": 2012}, {"title": "A robot that learns to communicate with human caregivers", "author": ["H. Kozima", "H. Yano"], "venue": "First Inter. Workshop on Epigenetic Robotics, pages 47\u201352.", "citeRegEx": "Kozima and Yano,? 2001", "shortCiteRegEx": "Kozima and Yano", "year": 2001}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "Uncertainty in AI.", "citeRegEx": "Krause and Guestrin,? 2005", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Nonmyopic active learning of gaussian processes: an explorationexploitation approach", "author": ["A. Krause", "C. Guestrin"], "venue": "24th Inter. Conf. on Machine learning.", "citeRegEx": "Krause and Guestrin,? 2007", "shortCiteRegEx": "Krause and Guestrin", "year": 2007}, {"title": "Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9:235\u2013284.", "citeRegEx": "Krause et al\\.,? 2008", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Interactive learning of world model information for a service robot", "author": ["S. Kristensen", "V. Hansen", "S. Horstmann", "J. Klandt", "K. Kondak", "F. Lohnert", "A. Stopp"], "venue": "Sensor Based Intelligent Robots, pages 49\u201367. 33", "citeRegEx": "Kristensen et al\\.,? 1999", "shortCiteRegEx": "Kristensen et al\\.", "year": 1999}, {"title": "Active learning using mean shift optimization for robot grasping", "author": ["O. Kroemer", "R. Detry", "J. Piater", "J. Peters"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ Inter. Conf. on, pages 2610\u2013 2615.", "citeRegEx": "Kroemer et al\\.,? 2009", "shortCiteRegEx": "Kroemer et al\\.", "year": 2009}, {"title": "Combining active learning and reactive control for robot grasping", "author": ["O. Kroemer", "R. Detry", "J. Piater", "J. Peters"], "venue": "Robotics and Autonomous Systems, 58(9):1105\u20131116.", "citeRegEx": "Kroemer et al\\.,? 2010", "shortCiteRegEx": "Kroemer et al\\.", "year": 2010}, {"title": "Active learning for teaching a robot grounded relational symbols", "author": ["J. Kulick", "M. Toussaint", "T. Lang", "M. Lopes"], "venue": "Inter. Joint Conference on Artificial Intelligence (IJCAI\u201913), Beijing, China.", "citeRegEx": "Kulick et al\\.,? 2013", "shortCiteRegEx": "Kulick et al\\.", "year": 2013}, {"title": "Exploration in relational worlds", "author": ["T. Lang", "M. Toussaint", "K. Kersting"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 178\u2013194.", "citeRegEx": "Lang et al\\.,? 2010", "shortCiteRegEx": "Lang et al\\.", "year": 2010}, {"title": "Maturational constraints for motor learning in high-dimensions: the case of biped walking", "author": ["M. Lapeyre", "O. Ly", "P. Oudeyer"], "venue": "Inter. Conf. on Humanoid Robots (Humanoids\u201911), pages 707\u2013714.", "citeRegEx": "Lapeyre et al\\.,? 2011", "shortCiteRegEx": "Lapeyre et al\\.", "year": 2011}, {"title": "Mobile robot programming using natural language", "author": ["S. Lauria", "G. Bugmann", "T. Kyriacou", "E. Klein"], "venue": "Robotics and Autonomous Systems, 38(34):171\u2013181.", "citeRegEx": "Lauria et al\\.,? 2002", "shortCiteRegEx": "Lauria et al\\.", "year": 2002}, {"title": "Online, interactive learning of gestures for human/robot interfaces", "author": ["C. Lee", "Y. Xu"], "venue": "Robotics and Automation, 1996. Proceedings., 1996 IEEE Inter. Conf. on, volume 4, pages 2982\u20132987.", "citeRegEx": "Lee and Xu,? 1996", "shortCiteRegEx": "Lee and Xu", "year": 1996}, {"title": "Staged competence learning in developmental robotics", "author": ["M. Lee", "Q. Meng", "F. Chao"], "venue": "Adaptive Behavior, 15(3):241\u2013255.", "citeRegEx": "Lee et al\\.,? 2007", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Abandoning objectives: Evolution through the search for novelty alone", "author": ["J. Lehman", "K. Stanley"], "venue": "Evolutionary Computation, 19(2):189\u2013223.", "citeRegEx": "Lehman and Stanley,? 2011", "shortCiteRegEx": "Lehman and Stanley", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["D. Lewis", "W. Gale"], "venue": "17th annual Inter. ACM SIGIR Conf. on Research and development in information retrieval, pages 3\u201312. Springer-Verlag New York, Inc.", "citeRegEx": "Lewis and Gale,? 1994", "shortCiteRegEx": "Lewis and Gale", "year": 1994}, {"title": "Autonomous exploration for navigating in mdps", "author": ["S. Lim", "P. Auer"], "venue": "JMLR.", "citeRegEx": "Lim and Auer,? 2012", "shortCiteRegEx": "Lim and Auer", "year": 2012}, {"title": "Facilitating active learning with inexpensive mobile robots", "author": ["S. Linder", "B. Nestrick", "S. Mulders", "C. Lavelle"], "venue": "Journal of Computing Sciences in Colleges, 16(4):21\u201333.", "citeRegEx": "Linder et al\\.,? 2001", "shortCiteRegEx": "Linder et al\\.", "year": 2001}, {"title": "Tutelage and socially guided robot learning", "author": ["A. Lockerd", "C. Breazeal"], "venue": "Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ Inter. Conf. on, volume 4, pages 3475\u2013 3480.", "citeRegEx": "Lockerd and Breazeal,? 2004", "shortCiteRegEx": "Lockerd and Breazeal", "year": 2004}, {"title": "Simultaneous acquisition of task and feedback models", "author": ["M. Lopes", "T. Cederborg", "Oudeyer", "P.-Y."], "venue": "IEEE - International Conference on Development and Learning (ICDL\u201911), Frankfurt, Germany.", "citeRegEx": "Lopes et al\\.,? 2011", "shortCiteRegEx": "Lopes et al\\.", "year": 2011}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["M. Lopes", "T. Lang", "M. Toussaint", "P.Y. Oudeyer"], "venue": "Neural Information Processing Systems (NIPS\u201912), Tahoe, USA.", "citeRegEx": "Lopes et al\\.,? 2012", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "A computational model of social-learning mechanisms", "author": ["M. Lopes", "F. Melo", "B. Kenward", "J. Santos-Victor"], "venue": "Adaptive Behavior, 467(17).", "citeRegEx": "Lopes et al\\.,? 2009a", "shortCiteRegEx": "Lopes et al\\.", "year": 2009}, {"title": "Abstraction levels for robotic imitation: Overview and computational approaches", "author": ["M. Lopes", "F. Melo", "L. Montesano", "J. Santos-Victor"], "venue": "Sigaud, O. and Peters, J., editors, From Motor to Interaction Learning in Robots, volume 264 of Studies in Compu-", "citeRegEx": "Lopes et al\\.,? 2010", "shortCiteRegEx": "Lopes et al\\.", "year": 2010}, {"title": "Affordance-based imitation learning in robots", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\u201907), USA.", "citeRegEx": "Lopes et al\\.,? 2007", "shortCiteRegEx": "Lopes et al\\.", "year": 2007}, {"title": "Active learning for reward estimation in inverse reinforcement learning", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "Machine Learning and Knowledge Discovery in Databases (ECML/PKDD\u201909).", "citeRegEx": "Lopes et al\\.,? 2009b", "shortCiteRegEx": "Lopes et al\\.", "year": 2009}, {"title": "The strategic student approach for life-long exploration and learning", "author": ["M. Lopes", "Oudeyer", "P.-Y."], "venue": "IEEE International Conference on Development and Learning (ICDL), San Diego, USA.", "citeRegEx": "Lopes et al\\.,? 2012", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "A developmental roadmap for learning by imitation in robots", "author": ["M. Lopes", "J. Santos-Victor"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 37(2).", "citeRegEx": "Lopes and Santos.Victor,? 2007", "shortCiteRegEx": "Lopes and Santos.Victor", "year": 2007}, {"title": "Artificial curiosity with planning for autonomous perceptual and cognitive development", "author": ["M. Luciw", "V. Graziano", "M. Ring", "J. Schmidhuber"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Luciw et al\\.,? 2011", "shortCiteRegEx": "Luciw et al\\.", "year": 2011}, {"title": "Developmental robotics: a survey", "author": ["M. Lungarella", "G. Metta", "R. Pfeifer", "G. Sandini"], "venue": "Connection Science, 15(40):151\u2013190.", "citeRegEx": "Lungarella et al\\.,? 2003", "shortCiteRegEx": "Lungarella et al\\.", "year": 2003}, {"title": "The curious robot-structuring interactive robot learning", "author": ["I. Lutkebohle", "J. Peltason", "L. Schillingmann", "B. Wrede", "S. Wachsmuth", "C. Elbrechter", "R. Haschke"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE Inter. Conf. on, pages 4156\u20134162.", "citeRegEx": "Lutkebohle et al\\.,? 2009", "shortCiteRegEx": "Lutkebohle et al\\.", "year": 2009}, {"title": "Information-based objective functions for active data selection", "author": ["D. MacKay"], "venue": "Neural computation, 4(4):590\u2013604.", "citeRegEx": "MacKay,? 1992", "shortCiteRegEx": "MacKay", "year": 1992}, {"title": "Hierarchical optimistic region selection driven by curiosity", "author": ["O. Maillard"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Maillard,? 2012", "shortCiteRegEx": "Maillard", "year": 2012}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["O.A. Maillard", "R. Munos", "D. Ryabko"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Maillard et al\\.,? 2011", "shortCiteRegEx": "Maillard et al\\.", "year": 2011}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["S. Mannor", "I. Menache", "A. Hoze", "U. Klein"], "venue": "Inter. Conf. on Machine Learning, page 71.", "citeRegEx": "Mannor et al\\.,? 2004", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Extraction of reward-related feature space using correlation-based and reward-based learning methods", "author": ["P. Manoonpong", "F. W\u00f6rg\u00f6tter", "J. Morimoto"], "venue": "17th Inter. Conf. on Neural information processing: theory and algorithms - Volume Part I, ICONIP\u201910,", "citeRegEx": "Manoonpong et al\\.,? 2010", "shortCiteRegEx": "Manoonpong et al\\.", "year": 2010}, {"title": "Bayesian optimisation for intelligent environmental monitoring", "author": ["R. Marchant", "F. Ramos"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ Inter. Conf. on, pages 2242\u20132249.", "citeRegEx": "Marchant and Ramos,? 2012", "shortCiteRegEx": "Marchant and Ramos", "year": 2012}, {"title": "A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot", "author": ["R. Martinez-Cantin", "N. de Freitas", "E. Brochu", "J. Castellanos", "A. Doucet"], "venue": "Autonomous Robots - Special Issue on Robot", "citeRegEx": "Martinez.Cantin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2009}, {"title": "Active policy learning for robot planning and exploration under uncertainty", "author": ["R. Martinez-Cantin", "N. de Freitas", "A. Doucet", "J. Castellanos"], "venue": "In Robotics: Science and Systems (RSS)", "citeRegEx": "Martinez.Cantin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2007}, {"title": "Body schema acquisition through active learning", "author": ["R. Martinez-Cantin", "M. Lopes", "L. Montesano"], "venue": "IEEE International Conference on Robotics and Automation (ICRA\u201910), Alaska, USA.", "citeRegEx": "Martinez.Cantin et al\\.,? 2010", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2010}, {"title": "Robot self-initiative and personalization by learning through repeated interactions", "author": ["M. Mason", "M. Lopes"], "venue": "6th ACM/IEEE International Conference on Human-Robot Interaction (HRI\u201911).", "citeRegEx": "Mason and Lopes,? 2011", "shortCiteRegEx": "Mason and Lopes", "year": 2011}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["A. McGovern", "A.G. Barto"], "venue": "Inter. Conf. on Machine Learning (ICML\u201901), San Francisco, CA, USA.", "citeRegEx": "McGovern and Barto,? 2001", "shortCiteRegEx": "McGovern and Barto", "year": 2001}, {"title": "Curious george: An attentive semantic robot", "author": ["D. Meger", "P. Forss\u00e9n", "K. Lai", "S. Helmer", "S. McCann", "T. Southey", "M. Baumann", "J. Little", "D. Lowe"], "venue": "Robotics and Autonomous Systems, 56(6):503\u2013511.", "citeRegEx": "Meger et al\\.,? 2008", "shortCiteRegEx": "Meger et al\\.", "year": 2008}, {"title": "A unified framework for imitation-like behaviors", "author": ["F. Melo", "M. Lopes", "J. Santos-Victor", "M.I. Ribeiro"], "venue": "4th International Symposium on Imitation in Animals and Artifacts, Newcastle, UK.", "citeRegEx": "Melo et al\\.,? 2007", "shortCiteRegEx": "Melo et al\\.", "year": 2007}, {"title": "Learning from demonstration using mdp induced metrics", "author": ["F.S. Melo", "M. Lopes"], "venue": "Machine learning and knowledge discovery in databases (ECML/PKDD\u201910).", "citeRegEx": "Melo and Lopes,? 2010", "shortCiteRegEx": "Melo and Lopes", "year": 2010}, {"title": "Multi-class generalized binary search for active inverse reinforcement learning", "author": ["F.S. Melo", "M. Lopes"], "venue": "submitted to Machine Learning.", "citeRegEx": "Melo and Lopes,? 2013", "shortCiteRegEx": "Melo and Lopes", "year": 2013}, {"title": "Qcutdynamic discovery of sub-goals in reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "Machine Learning: ECML 2002, pages 187\u2013 195.", "citeRegEx": "Menache et al\\.,? 2002", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Error-driven active learning in growing radial basis function networks for early robot learning", "author": ["Q. Meng", "M. Lee"], "venue": "Neurocomputing, 71(7):1449\u20131461.", "citeRegEx": "Meng and Lee,? 2008", "shortCiteRegEx": "Meng and Lee", "year": 2008}, {"title": "Autonomous development of a grounded object ontology by a learning robot", "author": ["J. Modayil", "B. Kuipers"], "venue": "National Conf. on Artificial Intelligence (AAAI).", "citeRegEx": "Modayil and Kuipers,? 2007", "shortCiteRegEx": "Modayil and Kuipers", "year": 2007}, {"title": "Learning interaction protocols using Augmented Bayesian Networks applied to guided navigation", "author": ["Y. Mohammad", "T. Nishida"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ Inter. Conf. on, pages 4119\u20134126.", "citeRegEx": "Mohammad and Nishida,? 2010", "shortCiteRegEx": "Mohammad and Nishida", "year": 2010}, {"title": "Safe exploration in markov decision processes", "author": ["T.M. Moldovan", "P. Abbeel"], "venue": "CoRR, abs/1205.4810.", "citeRegEx": "Moldovan and Abbeel,? 2012", "shortCiteRegEx": "Moldovan and Abbeel", "year": 2012}, {"title": "Learning grasping affordances from local visual descriptors", "author": ["L. Montesano", "M. Lopes"], "venue": "IEEE International Conference on Development and Learning (ICDL\u201909), China.", "citeRegEx": "Montesano and Lopes,? 2009", "shortCiteRegEx": "Montesano and Lopes", "year": 2009}, {"title": "Active learning of visual descriptors for grasping using nonparametric smoothed beta distributions", "author": ["L. Montesano", "M. Lopes"], "venue": "Robotics and Autonomous Systems, 60(3):452\u2013462.", "citeRegEx": "Montesano and Lopes,? 2012", "shortCiteRegEx": "Montesano and Lopes", "year": 2012}, {"title": "An active learning approach for assessing robot grasp reliability", "author": ["A. Morales", "E. Chinellato", "A. Fagg", "A. del Pobil"], "venue": "In IEEE/RSJ Inter. Conf. on Intelligent Robots and Systems (IROS", "citeRegEx": "Morales et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Morales et al\\.", "year": 2004}, {"title": "Improving the detection of unknown computer worms activity using active learning", "author": ["R. Moskovitch", "N. Nissim", "D. Stopel", "C. Feher", "R. Englert", "Y. Elovici"], "venue": "KI 2007: Advances in Artificial Intelligence, pages 489\u2013493. Springer.", "citeRegEx": "Moskovitch et al\\.,? 2007", "shortCiteRegEx": "Moskovitch et al\\.", "year": 2007}, {"title": "Encouraging behavioral diversity in evolutionary robotics: an empirical study", "author": ["J. Mouret", "S. Doncieux"], "venue": "Evolutionary Computation.", "citeRegEx": "Mouret and Doncieux,? 2011", "shortCiteRegEx": "Mouret and Doncieux", "year": 2011}, {"title": "Learning for joint attention helped by functional development", "author": ["Y. Nagai", "M. Asada", "K. Hosoda"], "venue": "Advanced Robotics, 20(10):1165\u20131181.", "citeRegEx": "Nagai et al\\.,? 2006", "shortCiteRegEx": "Nagai et al\\.", "year": 2006}, {"title": "Computational analysis of motionese toward scaffolding robot action learning", "author": ["Y. Nagai", "K. Rohlfing"], "venue": "Autonomous Mental Development, IEEE Transactions on, 1(1):44\u201354.", "citeRegEx": "Nagai and Rohlfing,? 2009", "shortCiteRegEx": "Nagai and Rohlfing", "year": 2009}, {"title": "Nine billion correspondence problems", "author": ["C.L. Nehaniv"], "venue": "Cambridge University Press.", "citeRegEx": "Nehaniv,? 2007", "shortCiteRegEx": "Nehaniv", "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming, 14(1):265\u2013294.", "citeRegEx": "Nemhauser et al\\.,? 1978", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Interactive learning gives the tempo to an intrinsically motivated robot learner", "author": ["M. Nguyen", "Oudeyer", "P.-Y."], "venue": "IEEE-RAS Inter. Conf. on Humanoid Robots.", "citeRegEx": "Nguyen et al\\.,? 2012", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "Bootstrapping intrinsically motivated learning with human demonstration", "author": ["S. Nguyen", "A. Baranes", "P. Oudeyer"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Nguyen et al\\.,? 2011", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Model learning for robot control: a survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, 12(4):319\u2013340.", "citeRegEx": "Nguyen.Tuong and Peters,? 2011", "shortCiteRegEx": "Nguyen.Tuong and Peters", "year": 2011}, {"title": "Learning and interacting in human-robot domains", "author": ["M. Nicolescu", "M. Mataric"], "venue": "Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 31(5):419\u2013430.", "citeRegEx": "Nicolescu and Mataric,? 2001", "shortCiteRegEx": "Nicolescu and Mataric", "year": 2001}, {"title": "Natural methods for robot task learning: Instructive demonstrations, generalization and practice", "author": ["M. Nicolescu", "M. Mataric"], "venue": "second Inter. joint Conf. on Autonomous agents and multiagent systems, pages 241\u2013248.", "citeRegEx": "Nicolescu and Mataric,? 2003", "shortCiteRegEx": "Nicolescu and Mataric", "year": 2003}, {"title": "Social learning mechanisms compared in a simple environment", "author": ["J. Noble", "D.W. Franks"], "venue": "Artificial Life VIII: Eighth Inter. Conf.on the Simulation and Synthesis of Living Systems, pages 379\u2013385. MIT Press.", "citeRegEx": "Noble and Franks,? 2002", "shortCiteRegEx": "Noble and Franks", "year": 2002}, {"title": "Dimension reduction and its application to model-based exploration in continuous spaces", "author": ["A. Nouri", "M. Littman"], "venue": "Machine learning, 81(1):85\u201398.", "citeRegEx": "Nouri and Littman,? 2010", "shortCiteRegEx": "Nouri and Littman", "year": 2010}, {"title": "The geometry of generalized binary search", "author": ["R. Nowak"], "venue": "Information Theory, Transactions on, 57(12):7893\u20137906.", "citeRegEx": "Nowak,? 2011", "shortCiteRegEx": "Nowak", "year": 2011}, {"title": "Interactive learning in human-robot collaboration", "author": ["T. Ogata", "N. Masago", "S. Sugano", "J. Tani"], "venue": "Intelligent Robots and Systems, 2003.(IROS 2003). Proceedings. 2003 IEEE/RSJ Inter. Conf. on, volume 1, pages 162\u2013167.", "citeRegEx": "Ogata et al\\.,? 2003", "shortCiteRegEx": "Ogata et al\\.", "year": 2003}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P.A.R. Ortner"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Ortner,? 2007", "shortCiteRegEx": "Ortner", "year": 2007}, {"title": "What is intrinsic motivation? a typology of computational approaches", "author": ["P. Oudeyer", "F. Kaplan"], "venue": "Frontiers in Neurorobotics, 1.", "citeRegEx": "Oudeyer and Kaplan,? 2007", "shortCiteRegEx": "Oudeyer and Kaplan", "year": 2007}, {"title": "Developmental Robotics", "author": ["Oudeyer", "P.-Y."], "venue": "Seel, N., editor, Encyclopedia of the Sciences of Learning, Springer Reference Series. Springer.", "citeRegEx": "Oudeyer and P..Y.,? 2011", "shortCiteRegEx": "Oudeyer and P..Y.", "year": 2011}, {"title": "Intrinsically motivated learning of real world sensorimotor skills with developmental constraints", "author": ["Oudeyer", "P.-Y.", "A. Baranes", "F. Kaplan"], "venue": "Baldassarre, G. and Mirolli, M., editors, Intrinsically Motivated Learning in Natural and Artificial Systems. Springer.", "citeRegEx": "Oudeyer et al\\.,? 2013", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2013}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["Oudeyer", "P.-Y.", "F. Kaplan", "V. Hafner"], "venue": "IEEE Transactions on Evolutionary Computation, 11(2):265\u2013286.", "citeRegEx": "Oudeyer et al\\.,? 2007", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "The playground experiment: Taskindependent development of a curious robot", "author": ["Oudeyer", "P.-Y.", "F. Kaplan", "V. Hafner", "A. Whyte"], "venue": "AAAI Spring Symposium on Developmental Robotics, pages 42\u201347.", "citeRegEx": "Oudeyer et al\\.,? 2005", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2005}, {"title": "Natural Actor-Critic", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "Proc. 16th European Conf. Machine Learning, pages 280\u2013291.", "citeRegEx": "Peters et al\\.,? 2005", "shortCiteRegEx": "Peters et al\\.", "year": 2005}, {"title": "Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning", "author": ["M. Pickett", "A. Barto"], "venue": "MACHINE LEARNING-Inter. WORKSHOP THEN Conf.-, pages 506\u2013513.", "citeRegEx": "Pickett and Barto,? 2002", "shortCiteRegEx": "Pickett and Barto", "year": 2002}, {"title": "Learning to explore and build maps", "author": ["D. Pierce", "B. Kuipers"], "venue": "National Conf. on Artificial Intelligence, pages 1264\u20131264.", "citeRegEx": "Pierce and Kuipers,? 1995", "shortCiteRegEx": "Pierce and Kuipers", "year": 1995}, {"title": "Neural network perception for mobile robot guidance", "author": ["D. Pomerleau"], "venue": "Technical report, DTIC Document.", "citeRegEx": "Pomerleau,? 1992", "shortCiteRegEx": "Pomerleau", "year": 1992}, {"title": "An analytic solution to discrete bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Inter. Conf. on Machine learning, pages 697\u2013704.", "citeRegEx": "Poupart et al\\.,? 2006", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Accelerating reinforcement learning through implicit imitation", "author": ["B. Price", "C. Boutilier"], "venue": "J. Artificial Intelligence Research, 19:569\u2013629.", "citeRegEx": "Price and Boutilier,? 2003", "shortCiteRegEx": "Price and Boutilier", "year": 2003}, {"title": "Two-dimensional active learning for image classification", "author": ["G. Qi", "X. Hua", "Y. Rui", "J. Tang", "H. Zhang"], "venue": "Computer Vision and Pattern Recognition (CVPR\u201908).", "citeRegEx": "Qi et al\\.,? 2008", "shortCiteRegEx": "Qi et al\\.", "year": 2008}, {"title": "Eliciting additive reward functions for markov decision processes", "author": ["K. Regan", "C. Boutilier"], "venue": "Inter. Joint Conf. on Artificial Intelligence (IJCAI\u201911), Barcelona, Spain.", "citeRegEx": "Regan and Boutilier,? 2011", "shortCiteRegEx": "Regan and Boutilier", "year": 2011}, {"title": "Multi-task active learning for linguistic annotations", "author": ["R. Reichart", "K. Tomanek", "U. Hahn", "A. Rappoport"], "venue": "ACL08.", "citeRegEx": "Reichart et al\\.,? 2008", "shortCiteRegEx": "Reichart et al\\.", "year": 2008}, {"title": "Online goal babbling for rapid bootstrapping of inverse models in high dimensions", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "Development and Learning (ICDL), 2011 IEEE Inter. Conf. on.", "citeRegEx": "Rolf et al\\.,? 2011", "shortCiteRegEx": "Rolf et al\\.", "year": 2011}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "J.A.D. Bagnell"], "venue": "13th Inter. Conf. on Artificial Intelligence and Statistics (AISTATS).", "citeRegEx": "Ross and Bagnell,? 2010", "shortCiteRegEx": "Ross and Bagnell", "year": 2010}, {"title": "Learning independent causes in natural images explains the spacevariant oblique effect", "author": ["C.A. Rothkopf", "T.H. Weisswange", "J. Triesch"], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u20136.", "citeRegEx": "Rothkopf et al\\.,? 2009", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2009}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "ICML, Williamstown.", "citeRegEx": "Roy and McCallum,? 2001", "shortCiteRegEx": "Roy and McCallum", "year": 2001}, {"title": "Evolving predictive visual motion detectors", "author": ["J. Ruesch", "A. Bernardino"], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u2013", "citeRegEx": "Ruesch and Bernardino,? 2009", "shortCiteRegEx": "Ruesch and Bernardino", "year": 2009}, {"title": "Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices", "author": ["M. Salganicoff", "L. Ungar"], "venue": "MACHINE LEARNINGInter. WORKSHOP THEN Conf.-, pages 480\u2013487.", "citeRegEx": "Salganicoff and Ungar,? 1995", "shortCiteRegEx": "Salganicoff and Ungar", "year": 1995}, {"title": "Active learning for vision-based robot grasping", "author": ["M. Salganicoff", "L.H. Ungar", "R. Bajcsy"], "venue": "Machine Learning, 23(2).", "citeRegEx": "Salganicoff et al\\.,? 1996", "shortCiteRegEx": "Salganicoff et al\\.", "year": 1996}, {"title": "Iterative learning of grasp adaptation through human corrections", "author": ["E. Sauser", "B. Argall", "G. Metta", "A. Billard"], "venue": "Robotics and Autonomous Systems.", "citeRegEx": "Sauser et al\\.,? 2011", "shortCiteRegEx": "Sauser et al\\.", "year": 2011}, {"title": "Robotic grasping of novel objects", "author": ["A. Saxena", "J. Driemeyer", "J. Kearns", "A.Y. Ng"], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Saxena et al\\.,? 2006", "shortCiteRegEx": "Saxena et al\\.", "year": 2006}, {"title": "Learning motor dependent crutchfield\u2019s information distance to anticipate changes in the topology of sensory body maps", "author": ["T. Schatz", "Oudeyer", "P.-Y."], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u20136.", "citeRegEx": "Schatz et al\\.,? 2009", "shortCiteRegEx": "Schatz et al\\.", "year": 2009}, {"title": "Active learning for logistic regression: an evaluation", "author": ["A. Schein", "L.H. Ungar"], "venue": "Machine Learning, 68:235\u2013265.", "citeRegEx": "Schein and Ungar,? 2007", "shortCiteRegEx": "Schein and Ungar", "year": 2007}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "Inter. Joint Conf. on Neural Networks, pages 1458\u20131463.", "citeRegEx": "Schmidhuber,? 1991a", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "From Animals to Animats: First Inter. Conf. on Simulation of Adaptive Behavior, pages 222 \u2013 227, Cambridge, MA, USA.", "citeRegEx": "Schmidhuber,? 1991b", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "On learning how to learn learning strategies", "author": ["J. Schmidhuber"], "venue": "Technical Report FKI-198-94, Fakultaet fuer Informatik, Technische Universitaet Muenchen.", "citeRegEx": "Schmidhuber,? 1995", "shortCiteRegEx": "Schmidhuber", "year": 1995}, {"title": "Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science, 18(2):173 \u2013 187.", "citeRegEx": "Schmidhuber,? 2006", "shortCiteRegEx": "Schmidhuber", "year": 2006}, {"title": "Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem", "author": ["J. Schmidhuber"], "venue": "Technical report, http://arxiv.org/abs/1112.5309.", "citeRegEx": "Schmidhuber,? 2011", "shortCiteRegEx": "Schmidhuber", "year": 2011}, {"title": "Reinforcement learning with self-modifying policies", "author": ["J. Schmidhuber", "J. Zhao", "N. Schraudolph"], "venue": "Learning to learn, 293:309.", "citeRegEx": "Schmidhuber et al\\.,? 1997", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1997}, {"title": "Global versus local search in constrained optimization of computer models", "author": ["M. Schonlau", "W. Welch", "D. Jones"], "venue": "Flournoy, N., Rosenberger, W., and Wong, W., editors, New Developments and Applications in Experimental Design, volume 34, pages 11\u201325.", "citeRegEx": "Schonlau et al\\.,? 1998", "shortCiteRegEx": "Schonlau et al\\.", "year": 1998}, {"title": "Emerging social awareness: Exploring intrinsic motivation in multiagent learning", "author": ["P. Sequeira", "F. Melo", "R. Prada", "A. Paiva"], "venue": "IEEE Inter. Conf. on Developmental Learning.", "citeRegEx": "Sequeira et al\\.,? 2011", "shortCiteRegEx": "Sequeira et al\\.", "year": 2011}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical Report CS Tech. Rep. 1648, University of Wisconsin-Madison.", "citeRegEx": "Settles,? 2009", "shortCiteRegEx": "Settles", "year": 2009}, {"title": "Multipleinstance active learning", "author": ["B. Settles", "M. Craven", "S. Ray"], "venue": "Advances in neural information processing systems, pages 1289\u20131296.", "citeRegEx": "Settles et al\\.,? 2007", "shortCiteRegEx": "Settles et al\\.", "year": 2007}, {"title": "Query by committee", "author": ["H. Seung", "M. Opper", "H. Sompolinsky"], "venue": "fifth annual workshop on Computational learning theory, pages 287\u2013294.", "citeRegEx": "Seung et al\\.,? 1992", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Active imitation learning", "author": ["A.P. Shon", "D. Verma", "R.P.N. Rao"], "venue": "AAAI Conf. on Artificial Intelligence (AAAI\u201907).", "citeRegEx": "Shon et al\\.,? 2007", "shortCiteRegEx": "Shon et al\\.", "year": 2007}, {"title": "Global a-optimal robot exploration in slam", "author": ["R. Sim", "N. Roy"], "venue": "IEEE Inter. Conf. on Robotics and Automation (ICRA).", "citeRegEx": "Sim and Roy,? 2005", "shortCiteRegEx": "Sim and Roy", "year": 2005}, {"title": "An intrinsic reward mechanism for efficient exploration", "author": ["\u00d6. \u015eim\u015fek", "A. Barto"], "venue": "Inter. Conf. on Machine learning, pages 833\u2013840.", "citeRegEx": "\u015eim\u015fek and Barto,? 2006", "shortCiteRegEx": "\u015eim\u015fek and Barto", "year": 2006}, {"title": "Skill characterization based on betweenness", "author": ["O. Simsek", "A. Barto"], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Simsek and Barto,? 2008", "shortCiteRegEx": "Simsek and Barto", "year": 2008}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["\u00d6. \u015eim\u015fek", "A. Wolfe", "A. Barto"], "venue": "Inter. Conf. on Machine learning, pages 816\u2013823.", "citeRegEx": "\u015eim\u015fek et al\\.,? 2005", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Efficient planning of informative paths for multiple robots", "author": ["A. Singh", "A. Krause", "C. Guestrin", "W. Kaiser", "M. Batalin"], "venue": "Proc. of the Int. Joint Conf. on Artificial Intelligence.", "citeRegEx": "Singh et al\\.,? 2007", "shortCiteRegEx": "Singh et al\\.", "year": 2007}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S. Singh", "A. Barto", "N. Chentanez"], "venue": "Advances in neural information processing systems (NIPS), volume 17, pages 1281\u20131288.", "citeRegEx": "Singh et al\\.,? 2005", "shortCiteRegEx": "Singh et al\\.", "year": 2005}, {"title": "Where do rewards come from? In Annual Conf", "author": ["S. Singh", "R. Lewis", "A. Barto"], "venue": "of the Cognitive Science Society.", "citeRegEx": "Singh et al\\.,? 2009", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "On Separating Agent Designer Goals from Agent Goals: Breaking the Preferences\u2013Parameters Confound", "author": ["S. Singh", "R. Lewis", "J. Sorg", "A. Barto", "A. Helou"], "venue": "Citeseer.", "citeRegEx": "Singh et al\\.,? 2010a", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning: an evolutionary perspective", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2).", "citeRegEx": "Singh et al\\.,? 2010b", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "A general activelearning framework for on-road vehicle recognition and tracking", "author": ["S. Sivaraman", "M. Trivedi"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, 11(2):267\u2013276.", "citeRegEx": "Sivaraman and Trivedi,? 2010", "shortCiteRegEx": "Sivaraman and Trivedi", "year": 2010}, {"title": "Internal rewards mitigate agent boundedness", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Int. Conf. on Machine Learning (ICML).", "citeRegEx": "Sorg et al\\.,? 2010a", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Reward design via online gradient ascent", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Advances of Neural Information Processing Systems, volume 23.", "citeRegEx": "Sorg et al\\.,? 2010b", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Variance-based rewards for approximate bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "26th Conf. on Uncertainty in Artificial Intelligence.", "citeRegEx": "Sorg et al\\.,? 2010c", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Optimal rewards versus leaf-evaluation heuristics in planning agents", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Twenty-Fifth AAAI Conf. on Artificial Intelligence.", "citeRegEx": "Sorg et al\\.,? 2011", "shortCiteRegEx": "Sorg et al\\.", "year": 2011}, {"title": "Exploring unknown environments with mobile robots using coverage maps", "author": ["C. Stachniss", "W. Burgard"], "venue": "AAAI Conference on Artificial Intelligence.", "citeRegEx": "Stachniss and Burgard,? 2003", "shortCiteRegEx": "Stachniss and Burgard", "year": 2003}, {"title": "Information gain-based exploration using rao-blackwellized particle filters", "author": ["C. Stachniss", "G. Grisetti", "W. Burgard"], "venue": "Robotics: Science and Systems.", "citeRegEx": "Stachniss et al\\.,? 2005", "shortCiteRegEx": "Stachniss et al\\.", "year": 2005}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M. Littman"], "venue": "J. of Machine Learning Research.", "citeRegEx": "Strehl et al\\.,? 2009", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "J. Comput. Syst. Sci., 74(8):1309\u20131331.", "citeRegEx": "Strehl and Littman,? 2008", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "Policy improvement methods: Between black-box optimization and episodic reinforcement learning", "author": ["F. Stulp", "O. Sigaud"], "venue": "ICML.", "citeRegEx": "Stulp and Sigaud,? 2012", "shortCiteRegEx": "Stulp and Sigaud", "year": 2012}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "author": ["Y. Sun", "F. Gomez", "J. Schmidhuber"], "venue": "Artificial General Intelligence, pages 41\u201351.", "citeRegEx": "Sun et al\\.,? 2011", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Adv. Neural Information Proc. Systems (NIPS), volume 12, pages 1057\u20131063.", "citeRegEx": "Sutton et al\\.,? 2000", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Reinforcement learning algorithms for mdps", "author": ["C. Szepesv\u00e1ri"], "venue": "Wiley Encyclopedia of Operations Research and Management Science.", "citeRegEx": "Szepesv\u00e1ri,? 2011", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2011}, {"title": "Bounding performance loss in approximate mdp homomorphisms", "author": ["J. Taylor", "D. Precup", "P. Panagaden"], "venue": "Advances in Neural Information Processing Systems, pages 1649\u20131656.", "citeRegEx": "Taylor et al\\.,? 2008", "shortCiteRegEx": "Taylor et al\\.", "year": 2008}, {"title": "Expensive function optimization with stochastic binary outcomes", "author": ["M. Tesch", "J. Schneider", "H. Choset"], "venue": "Inter. Conf. on Machine Learning (ICML\u201913).", "citeRegEx": "Tesch et al\\.,? 2013", "shortCiteRegEx": "Tesch et al\\.", "year": 2013}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence, 172(67):716\u2013737.", "citeRegEx": "Thomaz and Breazeal,? 2008", "shortCiteRegEx": "Thomaz and Breazeal", "year": 2008}, {"title": "Learning about objects with human teachers", "author": ["A. Thomaz", "M. Cakmak"], "venue": "ACM/IEEE Inter. Conf. on Human robot interaction, pages 15\u201322.", "citeRegEx": "Thomaz and Cakmak,? 2009", "shortCiteRegEx": "Thomaz and Cakmak", "year": 2009}, {"title": "Reinforcement learning with human teachers: Understanding how people want to teach robots", "author": ["A. Thomaz", "G. Hoffman", "C. Breazeal"], "venue": "Robot and Human Interactive Communication, 2006. ROMAN 2006. The 15th IEEE Inter. Symposium on, pages 352\u2013357.", "citeRegEx": "Thomaz et al\\.,? 2006", "shortCiteRegEx": "Thomaz et al\\.", "year": 2006}, {"title": "Efficient exploration in reinforcement learning", "author": ["S. Thrun"], "venue": "Technical Report CMU-CS-92-102, CarnegieMellon University.", "citeRegEx": "Thrun,? 1992", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Exploration in active learning", "author": ["S. Thrun"], "venue": "Handbook of Brain Science and Neural Networks, pages 381\u2013384.", "citeRegEx": "Thrun,? 1995", "shortCiteRegEx": "Thrun", "year": 1995}, {"title": "Finding structure in reinforcement learning", "author": ["S. Thrun", "A Schwartz"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Thrun and Schwartz,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1995}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research, 2:45\u201366.", "citeRegEx": "Tong and Koller,? 2001", "shortCiteRegEx": "Tong and Koller", "year": 2001}, {"title": "Theory and Principled Methods for Designing Metaheuristics, chapter The Bayesian Search Game", "author": ["M. Toussaint"], "venue": "Springer.", "citeRegEx": "Toussaint,? 2012", "shortCiteRegEx": "Toussaint", "year": 2012}, {"title": "Curiosity-driven learning of traversability affordance on a mobile robot", "author": ["E. Ugur", "M.R. Dogar", "M. Cakmak", "E. Sahin"], "venue": "Development and Learning, 2007. ICDL 2007. IEEE 6th Inter. Conf. on, pages 13\u201318.", "citeRegEx": "Ugur et al\\.,? 2007", "shortCiteRegEx": "Ugur et al\\.", "year": 2007}, {"title": "Maximally informative interaction learning for scene exploration", "author": ["H. van Hoof", "O. Kr\u00f6mer", "H. Amor", "J. Peters"], "venue": "In IEEE/RSJ Inter. Conf. on Intelligent Robots and Systems (IROS)", "citeRegEx": "Hoof et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoof et al\\.", "year": 2012}, {"title": "Optimal bayesian recommendation sets and myopically optimal choice query sets", "author": ["P. Viappiani", "C. Boutilier"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Viappiani and Boutilier,? 2010", "shortCiteRegEx": "Viappiani and Boutilier", "year": 2010}, {"title": "Multibandit best arm identification", "author": ["Victor Gabillon", "Alessandro Lazaric", "Mohammad Ghavamzadeh", "S. Bubeck"], "venue": "Neural Information Processing Systems (NIPS\u201911).", "citeRegEx": "Gabillon et al\\.,? 2011", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Bayesian reinforcement learning", "author": ["N. Vlassis", "M. Ghavamzadeh", "S. Mannor", "P. Poupart"], "venue": "Reinforcement Learning, pages 359\u2013386.", "citeRegEx": "Vlassis et al\\.,? 2012", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Active learning in multimedia annotation and retrieval: A survey", "author": ["M. Wang", "X. Hua"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(2):10.", "citeRegEx": "Wang and Hua,? 2011", "shortCiteRegEx": "Wang and Hua", "year": 2011}, {"title": "Autonomous mental development by robots and animals", "author": ["J. Weng", "J. McClelland", "A. Pentland", "O. Sporns", "I. Stockman", "M. Sur", "E. Thelen"], "venue": "Science, 291:599 \u2013 600.", "citeRegEx": "Weng et al\\.,? 2001", "shortCiteRegEx": "Weng et al\\.", "year": 2001}, {"title": "Efficient modelbased exploration", "author": ["M. Wiering", "J. Schmidhuber"], "venue": "Inter. Conf. on Simulation of Adaptive Behavior: From Animals to Animats 6, pages 223\u2013228.", "citeRegEx": "Wiering and Schmidhuber,? 1998", "shortCiteRegEx": "Wiering and Schmidhuber", "year": 1998}, {"title": "Policy teaching through reward function learning", "author": ["H. Zhang", "D. Parkes", "Y. Chen"], "venue": "ACM Conf. on Electronic commerce, pages 295\u2013304.", "citeRegEx": "Zhang et al\\.,? 2009", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 250, "context": "However, the embodiment in this type of systems provides a unique opportunity to exploit an active learning (AL) 2 approach (AL)(Angluin, 1988; Thrun, 1995; Settles, 2009) to guide the robot actions towards", "startOffset": 128, "endOffset": 171}, {"referenceID": 216, "context": "However, the embodiment in this type of systems provides a unique opportunity to exploit an active learning (AL) 2 approach (AL)(Angluin, 1988; Thrun, 1995; Settles, 2009) to guide the robot actions towards", "startOffset": 128, "endOffset": 171}, {"referenceID": 130, "context": "Active learning can also be used to describe situations where the student is involved in the learning process as opposed to passively listening to lectures, see for instance (Linder et al., 2001).", "startOffset": 174, "endOffset": 195}, {"referenceID": 209, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 260, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 141, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 216, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 43, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 44, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 180, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 194, "context": "perspective on the applicability of AL for real application, and indeed there already many examples: image classification (Qi et al., 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 122, "endOffset": 139}, {"referenceID": 252, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 29, "endOffset": 52}, {"referenceID": 259, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 65, "endOffset": 85}, {"referenceID": 216, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 110, "endOffset": 125}, {"referenceID": 102, "context": "the robot scientist (King et al., 2004) eliminates redundant experiments based on inductive logic programming.", "startOffset": 20, "endOffset": 39}, {"referenceID": 207, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 252, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 98, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al., 2007), neural networks (Cohn et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 38, "context": ", 2007), neural networks (Cohn et al., 1996), mixture models (Cohn et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 38, "context": ", 1996), mixture models (Cohn et al., 1996), inverse reinforcement learning (Lopes et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 137, "context": ", 1996), inverse reinforcement learning (Lopes et al., 2009b), among many others.", "startOffset": 40, "endOffset": 61}, {"referenceID": 216, "context": "Classical Active Learning(AL), refers to a set of approaches in which a learning algorithm is able to interactively query a source of information to obtain the desired outputs at new data points (Settles, 2009).", "startOffset": 195, "endOffset": 210}, {"referenceID": 214, "context": "Optimal Experimental Design(OED), an early perspective on active learning where the design of the experiments is optimal according to some statistical criteria (Schonlau et al., 1998).", "startOffset": 160, "endOffset": 183}, {"referenceID": 20, "context": "Bayesian Optimization, class of methods to solve an optimization problem that use statistical measures of uncertainty about the target function to guide exploration (Brochu et al., 2010).", "startOffset": 165, "endOffset": 186}, {"referenceID": 56, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 221, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 72, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 253, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 75, "context": "Results seem to indicate that curiosity is an intrinsic drive in most animals (Gottlieb et al., 2013).", "startOffset": 78, "endOffset": 101}, {"referenceID": 260, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 58, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 141, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 10, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 209, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 187, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 225, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 211, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 186, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 228, "context": ", 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al., 2010b).", "startOffset": 126, "endOffset": 147}, {"referenceID": 240, "context": "Exploration in reinforcement learning (Sutton and Barto, 1998), bayesian optimization (Brochu et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 20, "context": "Exploration in reinforcement learning (Sutton and Barto, 1998), bayesian optimization (Brochu et al., 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 183, "context": ", 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al.", "startOffset": 71, "endOffset": 97}, {"referenceID": 19, "context": ", 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al., 2004) or", "startOffset": 128, "endOffset": 151}, {"referenceID": 216, "context": "active learning for classification and regression problems (Settles, 2009), all these share many properties and face similar challenges.", "startOffset": 59, "endOffset": 74}, {"referenceID": 20, "context": "I don\u2019t like the last sentence with the last changes in the section then be used to find the best point with a minimum of function evaluations (Brochu et al., 2010).", "startOffset": 143, "endOffset": 164}, {"referenceID": 144, "context": "Another example, still for regression, is to decompose complex regression functions to a set of local regressions and then rely on multi-armed bandit algorithms to balance exploration in a more efficient way (Maillard, 2012).", "startOffset": 208, "endOffset": 224}, {"referenceID": 221, "context": "Clearly this problem is, in general, intractable and the following sections describe particular instantiations, approximations and models of this optimal exploration problem (\u015eim\u015fek and Barto, 2006).", "startOffset": 174, "endOffset": 198}, {"referenceID": 56, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 221, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 72, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 253, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 216, "context": "Some other restrictions can be included such as being restricted to a finite set of input points (pool-based active learning) or having the points arriving sequentially and having to decide to query or not (online learning)(see (Settles, 2009) for a comprehensive discussion on the different settings).", "startOffset": 228, "endOffset": 243}, {"referenceID": 70, "context": "An alternative formalism that is usually applied to discrete selection problems is the multi-armed bandit (MAB) formalism (Gittins, 1979; Bubeck and CesaBianchi, 2012).", "startOffset": 122, "endOffset": 167}, {"referenceID": 31, "context": ", 2011) or the value of all the arms (Carpentier et al., 2011).", "startOffset": 37, "endOffset": 62}, {"referenceID": 12, "context": "The most general, and well known, formalism to model sequential decision processes are markov-decision process (MDP)(Bellman, 1952).", "startOffset": 116, "endOffset": 131}, {"referenceID": 240, "context": "When there is no knowledge about the model of the environment and an agent has to optimize a reward function while interacting with the environment the problem is called reinforcement learning (RL) (Sutton and Barto, 1998).", "startOffset": 198, "endOffset": 222}, {"referenceID": 96, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 240, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 243, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 109, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 85, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al.", "startOffset": 72, "endOffset": 127}, {"referenceID": 175, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al.", "startOffset": 72, "endOffset": 127}, {"referenceID": 46, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al., 2013).", "startOffset": 190, "endOffset": 215}, {"referenceID": 95, "context": "Partial-observable markov decision processes (POMDP) generalize the concept for cases where the state is not directly observable (Kaelbling et al., 1998).", "startOffset": 129, "endOffset": 153}, {"referenceID": 20, "context": "Optimization Learning Point Bayesian Optimization (Brochu et al., 2010) Classical Active Learning (Settles, 2009) Discrete tasks Multi-armed bandits (Auer et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 216, "context": ", 2010) Classical Active Learning (Settles, 2009) Discrete tasks Multi-armed bandits (Auer et al.", "startOffset": 34, "endOffset": 49}, {"referenceID": 31, "context": ", 2003) AL for MAB (Carpentier et al., 2011) Trajectory Exploration/Exploitation (Kaelbling et al.", "startOffset": 19, "endOffset": 44}, {"referenceID": 96, "context": ", 2011) Trajectory Exploration/Exploitation (Kaelbling et al., 1996) Exploration", "startOffset": 44, "endOffset": 68}, {"referenceID": 31, "context": "In this pure learning problem of multi-armed bandits regret bounds on the simple regret can also be made (Carpentier et al., 2011; Victor Gabillon et al., 2011).", "startOffset": 105, "endOffset": 160}, {"referenceID": 88, "context": "also been established (Jaksch et al., 2010).", "startOffset": 22, "endOffset": 43}, {"referenceID": 216, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 53, "endOffset": 68}, {"referenceID": 143, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 84, "endOffset": 113}, {"referenceID": 216, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 84, "endOffset": 113}, {"referenceID": 209, "context": "The latter class of measures aims to consider those cases where there is no single model that covers the whole state-space, or if the agents lacks the knowledge to select which is the best one (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007).", "startOffset": 193, "endOffset": 239}, {"referenceID": 183, "context": "The latter class of measures aims to consider those cases where there is no single model that covers the whole state-space, or if the agents lacks the knowledge to select which is the best one (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007).", "startOffset": 193, "endOffset": 239}, {"referenceID": 128, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 90, "endOffset": 112}, {"referenceID": 252, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 152, "endOffset": 175}, {"referenceID": 207, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 197, "endOffset": 221}, {"referenceID": 37, "context": "An initial model considered Selective Sampling (Cohn et al., 1994) where a pool, or stream, of unlabeled examples exists and the learner may request the labels to an or-", "startOffset": 47, "endOffset": 66}, {"referenceID": 218, "context": "Query by committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of disagreement between the committee.", "startOffset": 19, "endOffset": 60}, {"referenceID": 66, "context": "Query by committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of disagreement between the committee.", "startOffset": 19, "endOffset": 60}, {"referenceID": 37, "context": "Perhaps the best-studied approach of this kind is learning by queries (Angluin, 1988; Cohn et al., 1994; Baum, 1991).", "startOffset": 70, "endOffset": 116}, {"referenceID": 11, "context": "Perhaps the best-studied approach of this kind is learning by queries (Angluin, 1988; Cohn et al., 1994; Baum, 1991).", "startOffset": 70, "endOffset": 116}, {"referenceID": 180, "context": "Under this setting approaches have generalized methods based on binary search (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 78, "endOffset": 113}, {"referenceID": 157, "context": "Under this setting approaches have generalized methods based on binary search (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 78, "endOffset": 113}, {"referenceID": 252, "context": "Also, active learning in support vector machines can be seen in a version space perspective or as the uncertainty of the classifier (Tong and Koller, 2001).", "startOffset": 132, "endOffset": 155}, {"referenceID": 38, "context": "Variance reduction aims to select the sample(s) that will minimize the variance of the estimation for unlabeled samples (Cohn et al., 1996).", "startOffset": 120, "endOffset": 139}, {"referenceID": 217, "context": "Finally, there are other decisiontheoretic based measures such as the expected model change (Settles et al., 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al.", "startOffset": 92, "endOffset": 114}, {"referenceID": 200, "context": ", 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007) which select the sample that, in expectation, will result in the largest change in the model parameters or in the largest reduction in the generalization error, respectively.", "startOffset": 40, "endOffset": 89}, {"referenceID": 166, "context": ", 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007) which select the sample that, in expectation, will result in the largest change in the model parameters or in the largest reduction in the generalization error, respectively.", "startOffset": 40, "endOffset": 89}, {"referenceID": 208, "context": "The first attempt to develop empirical measures was made by (Schmidhuber, 1991a; Schmidhuber, 1991b) in which an agent could model its own expectation about how future experiences can improve model learning.", "startOffset": 60, "endOffset": 100}, {"referenceID": 209, "context": "The first attempt to develop empirical measures was made by (Schmidhuber, 1991a; Schmidhuber, 1991b) in which an agent could model its own expectation about how future experiences can improve model learning.", "startOffset": 60, "endOffset": 100}, {"referenceID": 186, "context": "After this seminal paper, several measures to empirically estimate how can data improve task learning have been proposed and a integrated view can be seen in (Oudeyer et al., 2007).", "startOffset": 158, "endOffset": 180}, {"referenceID": 133, "context": "nenholtz, 2003) to the use of empirical measures in (Lopes et al., 2012).", "startOffset": 52, "endOffset": 72}, {"referenceID": 186, "context": "From (Oudeyer et al., 2007).", "startOffset": 5, "endOffset": 27}, {"referenceID": 186, "context": "Using the change in loss they may gain robustness by becoming independent of the loss\u2019 absolute value and can potentially detect timevarying conditions (Oudeyer et al., 2007; Lopes et al., 2012).", "startOffset": 152, "endOffset": 194}, {"referenceID": 133, "context": "Using the change in loss they may gain robustness by becoming independent of the loss\u2019 absolute value and can potentially detect timevarying conditions (Oudeyer et al., 2007; Lopes et al., 2012).", "startOffset": 152, "endOffset": 194}, {"referenceID": 133, "context": "To note that finding a good estimator for the expected loss is not trivial and resampling methods might be required (Lopes et al., 2012).", "startOffset": 116, "endOffset": 136}, {"referenceID": 186, "context": "See also (Oudeyer et al., 2007) for different definitions of learning progress.", "startOffset": 9, "endOffset": 31}, {"referenceID": 32, "context": "(Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can give in different classes of problems.", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "(Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can give in different classes of problems.", "startOffset": 0, "endOffset": 45}, {"referenceID": 43, "context": "For instance, (Dasgupta, 2005; Dasgupta, 2011) studied the problem of actively finding the optimal threshold on a line for a separable classification problem.", "startOffset": 14, "endOffset": 46}, {"referenceID": 44, "context": "For instance, (Dasgupta, 2005; Dasgupta, 2011) studied the problem of actively finding the optimal threshold on a line for a separable classification problem.", "startOffset": 14, "endOffset": 46}, {"referenceID": 180, "context": "authors have shown that generalized binary search algorithms can be derived for more complex learning problems (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 111, "endOffset": 146}, {"referenceID": 157, "context": "authors have shown that generalized binary search algorithms can be derived for more complex learning problems (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 111, "endOffset": 146}, {"referenceID": 115, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 73, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 72, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 144, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 171, "context": "A theorem from (Nemhauser et al., 1978) says that for monotonic submodular functions, the value of the function for the set obtained with the greedy algorithm G(Dg) is close, (1 \u2212 1/e), to the value of the optimal set G(DOPT ).", "startOffset": 15, "endOffset": 39}, {"referenceID": 101, "context": "Two of the most influential works on the topics are: E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 56, "endOffset": 80}, {"referenceID": 17, "context": "Two of the most influential works on the topics are: E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 91, "endOffset": 122}, {"referenceID": 220, "context": "Some other approaches consider limited look-ahead planning to approximately solve this problem (Sim and Roy, 2005; Krause and Guestrin, 2007).", "startOffset": 95, "endOffset": 141}, {"referenceID": 116, "context": "Some other approaches consider limited look-ahead planning to approximately solve this problem (Sim and Roy, 2005; Krause and Guestrin, 2007).", "startOffset": 95, "endOffset": 141}, {"referenceID": 31, "context": ", 2011) or the learning (Carpentier et al., 2011) problem with the best possible regret sometime taking into account specific knowledge about the statistical properties of each arm, but in many cases taken a distribution free approach (Auer et al.", "startOffset": 24, "endOffset": 49}, {"referenceID": 164, "context": "From (Montesano and Lopes, 2012).", "startOffset": 5, "endOffset": 32}, {"referenceID": 216, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al.", "startOffset": 87, "endOffset": 102}, {"referenceID": 229, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al.", "startOffset": 149, "endOffset": 178}, {"referenceID": 98, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al., 2007) among others.", "startOffset": 199, "endOffset": 220}, {"referenceID": 205, "context": "with the probability of grasp success when a robot tries to grasp at those points (Saxena et al., 2006).", "startOffset": 82, "endOffset": 103}, {"referenceID": 205, "context": "This process requires a large database of synthetically generated grasping points (as initially suggested by (Saxena et al., 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 203, "context": ", 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al., 1996; Morales et al., 2004).", "startOffset": 115, "endOffset": 163}, {"referenceID": 165, "context": ", 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al., 1996; Morales et al., 2004).", "startOffset": 115, "endOffset": 163}, {"referenceID": 163, "context": "Another approach, proposed by (Montesano and Lopes, 2009; Montesano and Lopes, 2012) (see also Figure 5), derived a kernel based algorithm to predict the probability of a successful grasp together with its uncertainty based on Beta priors.", "startOffset": 30, "endOffset": 84}, {"referenceID": 164, "context": "Another approach, proposed by (Montesano and Lopes, 2009; Montesano and Lopes, 2012) (see also Figure 5), derived a kernel based algorithm to predict the probability of a successful grasp together with its uncertainty based on Beta priors.", "startOffset": 30, "endOffset": 84}, {"referenceID": 47, "context": "Another approach used Gaussian process to model directly probability densities of successful grasps (Detry et al., 2009).", "startOffset": 100, "endOffset": 120}, {"referenceID": 119, "context": "Clearly such success probabilities depend on the grasping policy is being applied, and a combination of the two will be required to learn the best grasping strategy (Kroemer et al., 2009; Kroemer et al., 2010).", "startOffset": 165, "endOffset": 209}, {"referenceID": 120, "context": "Clearly such success probabilities depend on the grasping policy is being applied, and a combination of the two will be required to learn the best grasping strategy (Kroemer et al., 2009; Kroemer et al., 2010).", "startOffset": 165, "endOffset": 209}, {"referenceID": 53, "context": "(Dima et al., 2004) use active learning to request human users the correct labels of extensive datasets acquired by robots using density measures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 52, "context": "Also using multiview approaches (Dima and Hebert, 2005).", "startOffset": 32, "endOffset": 55}, {"referenceID": 254, "context": "Another property exploited by other authors is the traversability of given regions (Ugur et al., 2007).", "startOffset": 83, "endOffset": 102}, {"referenceID": 245, "context": "A final example considers how to optimize the parameters of a controller whose results can only be evaluated as success or failure (Tesch et al., 2013).", "startOffset": 131, "endOffset": 151}, {"referenceID": 7, "context": "ferent models of the robotic kinematic, using either nearest-neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al.", "startOffset": 71, "endOffset": 98}, {"referenceID": 197, "context": "ferent models of the robotic kinematic, using either nearest-neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al., 2011).", "startOffset": 120, "endOffset": 139}, {"referenceID": 7, "context": "Empirical measures of learning progress were used by (Baranes and Oudeyer, 2012) and (Rolf et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 197, "context": "Empirical measures of learning progress were used by (Baranes and Oudeyer, 2012) and (Rolf et al., 2011).", "startOffset": 85, "endOffset": 104}, {"referenceID": 36, "context": "(Chernova and Veloso, 2009) considering support vector machines as the classification method.", "startOffset": 0, "endOffset": 27}, {"referenceID": 137, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 156, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 41, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 39, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 92, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 157, "context": "Initial sample complexity results show that this approaches can indeed provide gains on the average case (Melo and Lopes, 2013).", "startOffset": 105, "endOffset": 127}, {"referenceID": 4, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 87, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 84, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 10, "context": ", 2013), or being interested in learning from among a set of discrete tasks (Barto et al., 2004).", "startOffset": 76, "endOffset": 96}, {"referenceID": 117, "context": "Examples include environmental sensing where the state is partitioned for computational purposes (Krause et al., 2008), or learning dynamical models of robots where the partition is created online based on the similarities of the function properties", "startOffset": 97, "endOffset": 118}, {"referenceID": 187, "context": "at each location (Oudeyer et al., 2005; Baran\u00e8s and Oudeyer, 2009) (see Figure 6).", "startOffset": 17, "endOffset": 66}, {"referenceID": 4, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 87, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 7, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 71, "context": "When this is not the case then a MAB algorithm must be used to ensure proper exploration of all the arms (Lopes and Oudeyer, 2012; Golovin et al., 2010a).", "startOffset": 105, "endOffset": 153}, {"referenceID": 209, "context": "These results confirms the heuristics of learning progress given by (Schmidhuber, 1991b; Oudeyer et al., 2007).", "startOffset": 68, "endOffset": 110}, {"referenceID": 186, "context": "These results confirms the heuristics of learning progress given by (Schmidhuber, 1991b; Oudeyer et al., 2007).", "startOffset": 68, "endOffset": 110}, {"referenceID": 183, "context": "From (Oudeyer and Kaplan, 2007).", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "This set can be pre-defined, or acquired autonomously (see Section 4), to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002)", "startOffset": 195, "endOffset": 228}, {"referenceID": 23, "context": "This set can be pre-defined, or acquired autonomously (see Section 4), to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002)", "startOffset": 195, "endOffset": 228}, {"referenceID": 194, "context": "Multi-task problems have been considered in classification tasks (Qi et al., 2008; Reichart et al., 2008).", "startOffset": 65, "endOffset": 105}, {"referenceID": 196, "context": "Multi-task problems have been considered in classification tasks (Qi et al., 2008; Reichart et al., 2008).", "startOffset": 65, "endOffset": 105}, {"referenceID": 225, "context": "More interestingly for our discussion are the works from (Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 57, "endOffset": 99}, {"referenceID": 186, "context": "More interestingly for our discussion are the works from (Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 57, "endOffset": 99}, {"referenceID": 186, "context": "(Oudeyer et al., 2007) initially considered that each parameter region gave a different learning", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Taking into account the previous discussion we know that a better exploration strategy must be applied and the authors considered more robust measures and created a stochastic policy to provide efficient results in high-dimensional problems (Baranes and Oudeyer, 2012).", "startOffset": 241, "endOffset": 268}, {"referenceID": 144, "context": "More recently (Maillard, 2012) introduce a new formulation of the problem and a new algorithm with specific regret bounds.", "startOffset": 14, "endOffset": 30}, {"referenceID": 225, "context": "The initial work of (Singh et al., 2005) lead to further improvements.", "startOffset": 20, "endOffset": 40}, {"referenceID": 221, "context": "The measures of progress that guide the selection of the macro action that is to be chosen started to consider the change in value function during learning (\u015eim\u015fek and Barto, 2006).", "startOffset": 156, "endOffset": 180}, {"referenceID": 81, "context": "Similar ideas were applied to learn affordances (Hart and Grupen, 2013) where different controllers and their validity regions are learned following their learning progress.", "startOffset": 48, "endOffset": 71}, {"referenceID": 116, "context": "When using a gaussian process as function approximation it is important to consider exploration to find the property of the kernel and then, for known parameters of the kernel, a simple offline policy provides optimal results (Krause and Guestrin, 2007).", "startOffset": 226, "endOffset": 253}, {"referenceID": 117, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 72, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 71, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 219, "context": "and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), or even choose between looking/asking for a teacher demonstration or doing self-exploration (Nguyen and Oudeyer, 2012).", "startOffset": 70, "endOffset": 89}, {"referenceID": 112, "context": "The representation that gives more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).", "startOffset": 73, "endOffset": 123}, {"referenceID": 145, "context": "The representation that gives more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).", "startOffset": 73, "endOffset": 123}, {"referenceID": 33, "context": "A similar approach was suggested by (Castronovo et al., 2012) where a list of possible exploration reward is proposed and a given arm bandit is assigned to each one.", "startOffset": 36, "endOffset": 61}, {"referenceID": 84, "context": "This limitation was recently improved by considering that the agent can evaluate and select online the best exploration strategies (Hester et al., 2013).", "startOffset": 131, "endOffset": 152}, {"referenceID": 86, "context": "In this work the authors relied on a factored representation of an MDP (Hester and Stone, 2012) and using many different exploration bonuses they were able to define a large set of exploration strategies.", "startOffset": 71, "endOffset": 95}, {"referenceID": 148, "context": "Courtesy from (Marchant and Ramos, 2012).", "startOffset": 14, "endOffset": 40}, {"referenceID": 63, "context": "In (Fox et al., 1998) the belief over the robot position and orientation was obtained using a Monte Carlo algorithm.", "startOffset": 3, "endOffset": 21}, {"referenceID": 60, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 16, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 234, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 235, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 60, "context": "For instance, in (Feder et al., 1999) an EKF was used to represent the robot location and the map features measured using sonar.", "startOffset": 17, "endOffset": 37}, {"referenceID": 234, "context": "For grid maps, similar ideas have been developed using mutual information (Stachniss and Burgard, 2003) and it is even possible to combine both representations (Bourgault et al.", "startOffset": 74, "endOffset": 103}, {"referenceID": 16, "context": "For grid maps, similar ideas have been developed using mutual information (Stachniss and Burgard, 2003) and it is even possible to combine both representations (Bourgault et al., 2002) using a weighted crite-", "startOffset": 160, "endOffset": 184}, {"referenceID": 5, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 7, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 10, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 187, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 186, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 7, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 89, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 197, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 4, "context": ", 2011) mdp Exploration strategies 1 Environment (Baram et al., 2004; Krause et al., 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al.", "startOffset": 49, "endOffset": 115}, {"referenceID": 117, "context": ", 2011) mdp Exploration strategies 1 Environment (Baram et al., 2004; Krause et al., 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al.", "startOffset": 49, "endOffset": 115}, {"referenceID": 193, "context": ", 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al., 2007) reg.", "startOffset": 62, "endOffset": 108}, {"referenceID": 219, "context": ", 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al., 2007) reg.", "startOffset": 62, "endOffset": 108}, {"referenceID": 112, "context": "Teacher,self-exploration 1 Function (Nguyen and Oudeyer, 2012) mdp n Representations 1 Environment (Konidaris and Barto, 2008; Maillard et al., 2011)", "startOffset": 99, "endOffset": 149}, {"referenceID": 145, "context": "Teacher,self-exploration 1 Function (Nguyen and Oudeyer, 2012) mdp n Representations 1 Environment (Konidaris and Barto, 2008; Maillard et al., 2011)", "startOffset": 99, "endOffset": 149}, {"referenceID": 150, "context": "The work in (Martinez-Cantin et al., 2007) directly aims to estimate the trajectory (i.", "startOffset": 12, "endOffset": 42}, {"referenceID": 150, "context": "a policy) in a continuous action-state space taking into account the cost to go there and all the information gathered in the path (Martinez-Cantin et al., 2007).", "startOffset": 131, "endOffset": 161}, {"referenceID": 104, "context": "For instance, (Kneebone and Dearden, 2009) uses a POMDP framework to incorporate uncertainty into Rapid Random Trees planning.", "startOffset": 14, "endOffset": 42}, {"referenceID": 116, "context": "time models (Krause and Guestrin, 2007; Garg et al., 2012).", "startOffset": 12, "endOffset": 58}, {"referenceID": 68, "context": "time models (Krause and Guestrin, 2007; Garg et al., 2012).", "startOffset": 12, "endOffset": 58}, {"referenceID": 224, "context": "By exploiting submodularity it is possible to compute efficient paths for multiple robots assuring that they will gather information in a set of regions (Singh et al., 2007).", "startOffset": 153, "endOffset": 173}, {"referenceID": 148, "context": "Without relying on a particular division into regions, but without any proven bounds, (Marchant and Ramos, 2012) used Bayesian optimization tools to find an informative path in a space-time model.", "startOffset": 86, "endOffset": 112}, {"referenceID": 249, "context": "Another setting where the learner actively plans its actions to improve learning is in reinforcement learning (see an early review on the topic (Thrun, 1992)).", "startOffset": 144, "endOffset": 157}, {"referenceID": 237, "context": "probability of finding a solution that is approximately correct, following the standard probably approximately correct learning (PAC) (Strehl and Littman, 2008; Strehl et al., 2009).", "startOffset": 134, "endOffset": 181}, {"referenceID": 236, "context": "probability of finding a solution that is approximately correct, following the standard probably approximately correct learning (PAC) (Strehl and Littman, 2008; Strehl et al., 2009).", "startOffset": 134, "endOffset": 181}, {"referenceID": 17, "context": "2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 16, "endOffset": 47}, {"referenceID": 17, "context": "Specifically, for the case of Rmax (Brafman and Tennenholtz, 2003), the algorithm divides the states into known and unknown based on the number of visits made.", "startOffset": 35, "endOffset": 66}, {"referenceID": 237, "context": "For a further analysis an more recent algorithm see the discussion in (Strehl and Littman, 2008).", "startOffset": 70, "endOffset": 96}, {"referenceID": 202, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 182, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 88, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 45, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 192, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 258, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 232, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 111, "context": "Bayesian exploration bonus (BEB) approach (Kolter and Ng, 2009) mixes the ideas of Bayesian RL with R-max where the state are not explicitly separated between known and un-", "startOffset": 42, "endOffset": 63}, {"referenceID": 133, "context": "generalized for the case where each different state might have different statistical properties (Lopes et al., 2012).", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "As a generalization of exploration methods in reinforcement learning, such as (Brafman and Tennenholtz, 2003), ideas have been suggested such as planning to be surprised (Sun et al.", "startOffset": 78, "endOffset": 109}, {"referenceID": 239, "context": "As a generalization of exploration methods in reinforcement learning, such as (Brafman and Tennenholtz, 2003), ideas have been suggested such as planning to be surprised (Sun et al., 2011) or the combination of empirical learning progress with visit counts (Hester and Stone, 2012).", "startOffset": 170, "endOffset": 188}, {"referenceID": 86, "context": ", 2011) or the combination of empirical learning progress with visit counts (Hester and Stone, 2012).", "startOffset": 76, "endOffset": 100}, {"referenceID": 64, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 90, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 55, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 94, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 86, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 87, "endOffset": 136}, {"referenceID": 179, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 87, "endOffset": 136}, {"referenceID": 122, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al., 2010).", "startOffset": 162, "endOffset": 181}, {"referenceID": 162, "context": "Safe exploration techniques have started to be developed (Moldovan and Abbeel, 2012).", "startOffset": 57, "endOffset": 84}, {"referenceID": 149, "context": "In (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010) the authors want to learn a dynamical model of a robot arm, or a good map of the environment, with the minimum amount of data.", "startOffset": 3, "endOffset": 63}, {"referenceID": 151, "context": "In (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010) the authors want to learn a dynamical model of a robot arm, or a good map of the environment, with the minimum amount of data.", "startOffset": 3, "endOffset": 63}, {"referenceID": 7, "context": "From (Baranes and Oudeyer, 2012).", "startOffset": 5, "endOffset": 32}, {"referenceID": 20, "context": "problem, and if it is to be used in real time, then efficient Bayesian optimization techniques must be used (Brochu et al., 2010).", "startOffset": 108, "endOffset": 129}, {"referenceID": 7, "context": "Another examples is the SAGG-RIAC architecture (Baranes and Oudeyer, 2012).", "startOffset": 47, "endOffset": 74}, {"referenceID": 119, "context": "We can also view the works of (Kroemer et al., 2009; Kroemer et al., 2010) as having a level of active explo-", "startOffset": 30, "endOffset": 74}, {"referenceID": 120, "context": "We can also view the works of (Kroemer et al., 2009; Kroemer et al., 2010) as having a level of active explo-", "startOffset": 30, "endOffset": 74}, {"referenceID": 46, "context": "in many cases, other situations, even if strongly dependent on that same process, address it only in an implicit way or as a side-effect of an optimization process (Deisenroth et al., 2013).", "startOffset": 164, "endOffset": 189}, {"referenceID": 241, "context": "The most noteworthy example are all policy gradient methods and similar approaches (Sutton et al., 2000; Kober et al., 2013).", "startOffset": 83, "endOffset": 124}, {"referenceID": 109, "context": "The most noteworthy example are all policy gradient methods and similar approaches (Sutton et al., 2000; Kober et al., 2013).", "startOffset": 83, "endOffset": 124}, {"referenceID": 188, "context": "Some methods consider stochastic policies and the noise on the policy is used to perform exploration and collect data (Peters et al., 2005).", "startOffset": 118, "endOffset": 139}, {"referenceID": 238, "context": "Another line of research is to use more classical methods of optimization to find the best set of parameters that maximize a reward function (Stulp and Sigaud, 2012).", "startOffset": 141, "endOffset": 165}, {"referenceID": 245, "context": "Recently, and using a more accurate model of uncertainty it is possible to use Bayesian optimization methods to search for the best policy parameters that result in the highest success rate (Tesch et al., 2013).", "startOffset": 190, "endOffset": 210}, {"referenceID": 61, "context": "This idea has been applied to segment object and learn about their properties (Fitzpatrick et al., 2003), disambiguate and model articulated objects (Katz et al.", "startOffset": 78, "endOffset": 104}, {"referenceID": 100, "context": ", 2003), disambiguate and model articulated objects (Katz et al., 2008), disambiguate sound (Berglund and Sitte, 2005), among others.", "startOffset": 52, "endOffset": 71}, {"referenceID": 13, "context": ", 2008), disambiguate sound (Berglund and Sitte, 2005), among others.", "startOffset": 28, "endOffset": 54}, {"referenceID": 154, "context": "Attention can also be seen as an instance of active perception, (Meger et al., 2008) presents an attention system and learning in a real environment to learn about object using SIFTs and finally, in highly cluttered environments active approach can also provide significant gains (van Hoof et al.", "startOffset": 64, "endOffset": 84}, {"referenceID": 14, "context": "This spontaneous motivation to explore and intrinsic curiosity to novelty (Berlyne, 1960) challenged utilitarian perspectives on behavior.", "startOffset": 74, "endOffset": 89}, {"referenceID": 3, "context": "ture situations (Baldassarre, 2011; Singh et al., 2009), only after going through school will that knowledge have some practical benefit.", "startOffset": 16, "endOffset": 55}, {"referenceID": 226, "context": "ture situations (Baldassarre, 2011; Singh et al., 2009), only after going through school will that knowledge have some practical benefit.", "startOffset": 16, "endOffset": 55}, {"referenceID": 228, "context": "Some hypothesis can be made that this stage results from an evolutionary process that leads to a better performance in a class of problems (Singh et al., 2010b).", "startOffset": 139, "endOffset": 160}, {"referenceID": 227, "context": "Or that intrinsic motivation is a way to deal with bounded agents where maximizing the objective reward would be too difficult (Singh et al., 2010a; Sorg et al., 2010a).", "startOffset": 127, "endOffset": 168}, {"referenceID": 230, "context": "Or that intrinsic motivation is a way to deal with bounded agents where maximizing the objective reward would be too difficult (Singh et al., 2010a; Sorg et al., 2010a).", "startOffset": 127, "endOffset": 168}, {"referenceID": 74, "context": "Even for very limited time spans where an agent wants to select a single action, there are many somewhat contradictory mechanisms for attention and curiosity (Gottlieb, 2012).", "startOffset": 158, "endOffset": 174}, {"referenceID": 0, "context": "This work, suggested by (Auer et al., 2011; Lim and Auer, 2012), shows that it is possible to address such problem and still ensure formal regret bounds.", "startOffset": 24, "endOffset": 63}, {"referenceID": 129, "context": "This work, suggested by (Auer et al., 2011; Lim and Auer, 2012), shows that it is possible to address such problem and still ensure formal regret bounds.", "startOffset": 24, "endOffset": 63}, {"referenceID": 212, "context": "Under different formalisms we can also see the POWERPLAY system as a way to increasingly augment the complexity of already explained problems (Schmidhuber, 2011).", "startOffset": 142, "endOffset": 161}, {"referenceID": 7, "context": "The approach from (Baranes and Oudeyer, 2012) can also be seen in this perspective where the space of policy parameters is explored in an increasing order of complexity.", "startOffset": 18, "endOffset": 45}, {"referenceID": 209, "context": "One of the earliest works that tried to operationalize these concepts was made by (Schmidhuber, 1991b).", "startOffset": 82, "endOffset": 102}, {"referenceID": 210, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 211, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 225, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 186, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 7, "context": "Research in this field has considered new problems such as: situations where parts of the state space are unlearnable (Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012); guide exploration in different", "startOffset": 118, "endOffset": 172}, {"referenceID": 7, "context": "spaces (Baranes and Oudeyer, 2012); environmental changes (Lopes et al.", "startOffset": 7, "endOffset": 34}, {"referenceID": 133, "context": "spaces (Baranes and Oudeyer, 2012); environmental changes (Lopes et al., 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 211, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 186, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 7, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 84, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 133, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 227, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 230, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 215, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 225, "context": ", 2011); open-ended problems (Singh et al., 2005; Oudeyer et al., 2007); autonomous discovery of good representations (Luciw et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 186, "context": ", 2011); open-ended problems (Singh et al., 2005; Oudeyer et al., 2007); autonomous discovery of good representations (Luciw et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 140, "context": ", 2007); autonomous discovery of good representations (Luciw et al., 2011);", "startOffset": 54, "endOffset": 74}, {"referenceID": 84, "context": "and selecting efficient exploration policies (Lopes and Oudeyer, 2012; Hester et al., 2013).", "startOffset": 45, "endOffset": 91}, {"referenceID": 80, "context": "Nevertheless, the problem is not trivial and most heuristics are bound to fail in most cases (Guyon and Elisseeff, 2003).", "startOffset": 93, "endOffset": 120}, {"referenceID": 159, "context": "For instance, (Meng and Lee, 2008) grows radial basis functions to learn mappings between sensory modalities by sampling locations with an high error.", "startOffset": 14, "endOffset": 34}, {"referenceID": 201, "context": "For instance (Ruesch and Bernardino, 2009; Schatz and Oudeyer, 2009; Rothkopf et al., 2009) study what is the relation be-", "startOffset": 13, "endOffset": 91}, {"referenceID": 199, "context": "For instance (Ruesch and Bernardino, 2009; Schatz and Oudeyer, 2009; Rothkopf et al., 2009) study what is the relation be-", "startOffset": 13, "endOffset": 91}, {"referenceID": 190, "context": "Early works considered how a finite-automaton and an hierarchy could be learned from data (Pierce and Kuipers, 1995).", "startOffset": 90, "endOffset": 116}, {"referenceID": 160, "context": "Generalizations of those ideas consider how to detect regularities that identify non-static world objects and thus allowing to infer actions that change the world in the desired ways (Modayil and Kuipers, 2007).", "startOffset": 183, "endOffset": 210}, {"referenceID": 230, "context": "A very interesting perspective was approached with the definition of the optimal reward problem (Sorg et al., 2010a).", "startOffset": 96, "endOffset": 116}, {"referenceID": 231, "context": "The authors have extended their initial approach to have a more practical algorithm using reward gradient (Sorg et al., 2010b) and by comparing different search methods (Sorg et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 233, "context": ", 2010b) and by comparing different search methods (Sorg et al., 2011).", "startOffset": 51, "endOffset": 70}, {"referenceID": 215, "context": "Such search for an extra reward signal can also be used to improve coordination in a multi-agent scenario (Sequeira et al., 2011).", "startOffset": 106, "endOffset": 129}, {"referenceID": 10, "context": "This perspective on (sub) goal creation motivated one of the earliest computational models on intrinsic motivated systems (Barto et al., 2004; Singh et al., 2005), see Figure 9.", "startOffset": 122, "endOffset": 162}, {"referenceID": 225, "context": "This perspective on (sub) goal creation motivated one of the earliest computational models on intrinsic motivated systems (Barto et al., 2004; Singh et al., 2005), see Figure 9.", "startOffset": 122, "endOffset": 162}, {"referenceID": 242, "context": "There the authors, using the theory of options (Sutton et al., 1999), construct new goals (as options) every time the agent finds a new \u201dsalient\u201d stimuli.", "startOffset": 47, "endOffset": 68}, {"referenceID": 225, "context": "From (Singh et al., 2005).", "startOffset": 5, "endOffset": 25}, {"referenceID": 144, "context": "An initial theoretical study frames such model as a multi-armed bandits over a pre-defined hierarchical partition of the space (Maillard, 2012).", "startOffset": 127, "endOffset": 143}, {"referenceID": 117, "context": "An example is the optimization setting of (Krause et al., 2008).", "startOffset": 42, "endOffset": 63}, {"referenceID": 186, "context": "The aforementioned works of (Oudeyer et al., 2007; Baranes and Oudeyer, 2012) consider initially a single region (a prediction task in the former and a control task in the latter) but then automatically and continuously constructs new region,", "startOffset": 28, "endOffset": 77}, {"referenceID": 7, "context": "The aforementioned works of (Oudeyer et al., 2007; Baranes and Oudeyer, 2012) consider initially a single region (a prediction task in the former and a control task in the latter) but then automatically and continuously constructs new region,", "startOffset": 28, "endOffset": 77}, {"referenceID": 82, "context": "ways to manipulate them, (Hart et al., 2008) introduces an intrinsic reward that motivates the system to explore changes in the perceptual space.", "startOffset": 25, "endOffset": 44}, {"referenceID": 7, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al.", "startOffset": 56, "endOffset": 83}, {"referenceID": 212, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al., 2011; Lim and Auer, 2012), explores the policy space in an increasing order of complexity of learning each behavior.", "startOffset": 150, "endOffset": 189}, {"referenceID": 129, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al., 2011; Lim and Auer, 2012), explores the policy space in an increasing order of complexity of learning each behavior.", "startOffset": 150, "endOffset": 189}, {"referenceID": 9, "context": "In the case of problems formulated as MDPs several researchers have defined automatic measures to create options or other equivalent state-action abstractions, see (Barto and Mahadevan, 2003) for an early discussion.", "startOffset": 164, "endOffset": 191}, {"referenceID": 146, "context": "(Mannor et al., 2004) considered approaches such as online clustering of the state-action space using measures of connectivity, and variance of reward values.", "startOffset": 0, "endOffset": 21}, {"referenceID": 153, "context": "One such connectivity measure was introduced by (McGovern and Barto, 2001) where states that are present in multiple paths to the goals are considered sub-goals and an option is initiated to reach them.", "startOffset": 48, "endOffset": 74}, {"referenceID": 48, "context": "Even before the introduction of the options formalism, (Digney, 1998) introduced a method that would create skills based on reward gradients.", "startOffset": 55, "endOffset": 69}, {"referenceID": 83, "context": "(Hengst, 2002) exploited the factored structure of the problem to create the hierar-", "startOffset": 0, "endOffset": 14}, {"referenceID": 91, "context": "A more recent approach models the problem as a dynamic bayesian network that explains the relation between different tasks (Jonsson and Barto, 2006).", "startOffset": 123, "endOffset": 148}, {"referenceID": 1, "context": "By ensuring that neighbor states at the lower level are clustered in the higher level, it is possible to create efficient hierarchies of behavior (Bakker and Schmidhuber, 2004).", "startOffset": 146, "endOffset": 176}, {"referenceID": 189, "context": "An alternative perspective on the creation of a set of reusable macro actions is to exploit commonalities in collections of policies (Thrun et al., 1995; Pickett and Barto, 2002).", "startOffset": 133, "endOffset": 178}, {"referenceID": 7, "context": "Although coming from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavior space might be more efficient and relevant than in the space of the parameters that generate that behavior.", "startOffset": 68, "endOffset": 95}, {"referenceID": 127, "context": "Although coming from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavior space might be more efficient and relevant than in the space of the parameters that generate that behavior.", "startOffset": 125, "endOffset": 151}, {"referenceID": 127, "context": "The first perspective proposed by (Lehman and Stanley, 2011) is that many different genetic controller encodings might lead to very similar behaviors, and when considering also the morphological and environmental restrictions, the space of behaviors is much smaller than the space of controller encodings.", "startOffset": 34, "endOffset": 60}, {"referenceID": 167, "context": "The notion of diversity is not clear due to the redundancy in the control parameters, see (Mouret and Doncieux, 2011) for a discussion.", "startOffset": 90, "endOffset": 117}, {"referenceID": 69, "context": "and improve efficiency (Gilks and Berzuini, 2002).", "startOffset": 23, "endOffset": 49}, {"referenceID": 5, "context": "From a robot controller point of view we can see a similar idea as proposed by (Baranes and Oudeyer, 2010), see Figure 10.", "startOffset": 79, "endOffset": 106}, {"referenceID": 89, "context": "(Jamone et al., 2011) and (Rolf et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 197, "context": ", 2011) and (Rolf et al., 2011).", "startOffset": 12, "endOffset": 31}, {"referenceID": 212, "context": "We can refer again to the works of (Schmidhuber, 2011; Lim and Auer, 2012) and see that they also consider as criteria having access to the more diversified set of policies possible.", "startOffset": 35, "endOffset": 74}, {"referenceID": 129, "context": "We can refer again to the works of (Schmidhuber, 2011; Lim and Auer, 2012) and see that they also consider as criteria having access to the more diversified set of policies possible.", "startOffset": 35, "endOffset": 74}, {"referenceID": 58, "context": "For examples artificial development considers that the learning process is guided not only by the environment and the data it is collect but also by the \u201dgenetic information\u201d of the system (Elman, 1997; Lungarella et al., 2003).", "startOffset": 189, "endOffset": 227}, {"referenceID": 141, "context": "For examples artificial development considers that the learning process is guided not only by the environment and the data it is collect but also by the \u201dgenetic information\u201d of the system (Elman, 1997; Lungarella et al., 2003).", "startOffset": 189, "endOffset": 227}, {"referenceID": 3, "context": ", 2013) or by developing intrinsic rewards that focus attention to informative experiences (Baldassarre, 2011; Singh et al., 2010b), pre-dispositions to detect meaningful salient events, among many other aspects.", "startOffset": 91, "endOffset": 131}, {"referenceID": 228, "context": ", 2013) or by developing intrinsic rewards that focus attention to informative experiences (Baldassarre, 2011; Singh et al., 2010b), pre-dispositions to detect meaningful salient events, among many other aspects.", "startOffset": 91, "endOffset": 131}, {"referenceID": 228, "context": "Evolutionary models (Singh et al., 2010b) and recent studies in neurosciences (Gottlieb et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 75, "context": ", 2010b) and recent studies in neurosciences (Gottlieb et al., 2013) are starting to provide a more clear picture on if, and why, curiosity is an intrinsic drive in many animals.", "startOffset": 45, "endOffset": 68}, {"referenceID": 177, "context": "(Nicolescu and Mataric, 2003), information about the task solution (Calinon et al.", "startOffset": 0, "endOffset": 29}, {"referenceID": 30, "context": "(Nicolescu and Mataric, 2003), information about the task solution (Calinon et al., 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 57, "context": ", 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 136, "context": ", 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al., 2007), among others.", "startOffset": 108, "endOffset": 128}, {"referenceID": 76, "context": "From (Grizou et al., 2013).", "startOffset": 5, "endOffset": 26}, {"referenceID": 50, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 51, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 177, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 19, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 131, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 49, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 246, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 77, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al., 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 100, "endOffset": 150}, {"referenceID": 137, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al., 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 100, "endOffset": 150}, {"referenceID": 36, "context": ", 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 39, "endOffset": 66}, {"referenceID": 93, "context": "Lopes, 2011), iterations between practice and user feedback sessions (Judah et al., 2010; Korupolu et al., 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 69, "endOffset": 112}, {"referenceID": 113, "context": "Lopes, 2011), iterations between practice and user feedback sessions (Judah et al., 2010; Korupolu et al., 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 69, "endOffset": 112}, {"referenceID": 106, "context": ", 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 61, "endOffset": 105}, {"referenceID": 107, "context": ", 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 61, "endOffset": 105}, {"referenceID": 176, "context": "Most approaches will just ask to user whenever the information is needed (Nicolescu and Mataric, 2001) or when there is high uncertainty (Chernova and Veloso, 2009).", "startOffset": 73, "endOffset": 102}, {"referenceID": 36, "context": "Most approaches will just ask to user whenever the information is needed (Nicolescu and Mataric, 2001) or when there is high uncertainty (Chernova and Veloso, 2009).", "startOffset": 137, "endOffset": 164}, {"referenceID": 55, "context": "A more advanced situation considers making queries only when it is too risky to try experiments (Doshi et al., 2008).", "startOffset": 96, "endOffset": 116}, {"referenceID": 24, "context": "(Cakmak et al., 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": ", 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 234, "endOffset": 284}, {"referenceID": 29, "context": ", 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 234, "endOffset": 284}, {"referenceID": 198, "context": "One justification for the need and expected gain of using such systems is discussed by (Ross and Bagnell, 2010).", "startOffset": 87, "endOffset": 111}, {"referenceID": 191, "context": "Such observation, as the authors refer, was already given by (Pomerleau, 1992) without a proof.", "startOffset": 61, "endOffset": 78}, {"referenceID": 181, "context": "See the work from (Ogata et al., 2003) for a study on this subject.", "startOffset": 18, "endOffset": 38}, {"referenceID": 62, "context": "certainty on the task being learned (Fong et al., 2003; Chao et al., 2010).", "startOffset": 36, "endOffset": 74}, {"referenceID": 35, "context": "certainty on the task being learned (Fong et al., 2003; Chao et al., 2010).", "startOffset": 36, "endOffset": 74}, {"referenceID": 118, "context": "face to provide information to the system during teleoperation (Kristensen et al., 1999), or it is the system that initiates questions based on perceptual saliency (Lutkebohle et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 142, "context": ", 1999), or it is the system that initiates questions based on perceptual saliency (Lutkebohle et al., 2009).", "startOffset": 83, "endOffset": 108}, {"referenceID": 65, "context": "to learn a better gesture classification the system is able to ask the user to provide more examples of a given class (Francke et al., 2007) even if for human-robot interfaces (Lee and Xu, 1996)).", "startOffset": 118, "endOffset": 140}, {"referenceID": 125, "context": ", 2007) even if for human-robot interfaces (Lee and Xu, 1996)).", "startOffset": 43, "endOffset": 61}, {"referenceID": 178, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 155, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 170, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 134, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 25, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 15, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 193, "context": "and the learner chooses which one to observe (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 219, "context": "and the learner chooses which one to observe (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), and even choose between looking at a demonstrator or just learn by self-exploration (Nguyen et al.", "startOffset": 122, "endOffset": 141}, {"referenceID": 174, "context": ", 2007), and even choose between looking at a demonstrator or just learn by self-exploration (Nguyen et al., 2011).", "startOffset": 93, "endOffset": 114}, {"referenceID": 169, "context": "Humans change the way they act when they are demonstrating actions to others (Nagai and Rohlfing, 2009).", "startOffset": 77, "endOffset": 103}, {"referenceID": 193, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al.", "startOffset": 64, "endOffset": 91}, {"referenceID": 135, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al., 2009; Lopes et al., 2010; Calinon et al., 2007) active Section 5.", "startOffset": 98, "endOffset": 161}, {"referenceID": 30, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al., 2009; Lopes et al., 2010; Calinon et al., 2007) active Section 5.", "startOffset": 98, "endOffset": 161}, {"referenceID": 29, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al.", "startOffset": 11, "endOffset": 60}, {"referenceID": 26, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al.", "startOffset": 11, "endOffset": 60}, {"referenceID": 99, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 93, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 246, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 77, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 106, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 152, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 76, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al., 2013)", "startOffset": 142, "endOffset": 163}, {"referenceID": 121, "context": "From (Kulick et al., 2013).", "startOffset": 5, "endOffset": 26}, {"referenceID": 247, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 97, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 105, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 248, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 246, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 97, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 246, "context": "For instance, in the work of (Thomaz and Breazeal, 2008) the teachers frequently gave a reward to exploratory actions even if the signal was used as a standard reward.", "startOffset": 29, "endOffset": 56}, {"referenceID": 27, "context": "Also, in some problems we can define an optimal teaching sequence but humans do not behave according to those strategies (Cakmak and Thomaz, 2010).", "startOffset": 121, "endOffset": 146}, {"referenceID": 97, "context": "(Kaochar et al., 2011) developed a GUI to observe the teaching patterns of humans when teaching an electronic learner to achieve a complex sequential task ( e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 219, "context": "Several criteria have been proposed: game theoretic approaches (Shon et al., 2007), entropy (Lopes et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 137, "context": ", 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), query by committee (Judah et al.", "startOffset": 17, "endOffset": 60}, {"referenceID": 156, "context": ", 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), query by committee (Judah et al.", "startOffset": 17, "endOffset": 60}, {"referenceID": 92, "context": ", 2009b; Melo and Lopes, 2010), query by committee (Judah et al., 2012), membership queries (Melo and", "startOffset": 51, "endOffset": 71}, {"referenceID": 36, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 41, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al.", "startOffset": 95, "endOffset": 133}, {"referenceID": 39, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al.", "startOffset": 95, "endOffset": 133}, {"referenceID": 55, "context": ", 2011) and risk minimization (Doshi et al., 2008).", "startOffset": 30, "endOffset": 50}, {"referenceID": 137, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 41, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 39, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 156, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 36, "context": ", 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 137, "context": ", 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al., 2009b).", "startOffset": 104, "endOffset": 125}, {"referenceID": 36, "context": "(Chernova and Veloso, 2009) used support-vector machine classifiers to make queries to the teacher when it is uncertain about the action to execute as measured by the uncertainty of the classifier.", "startOffset": 0, "endOffset": 27}, {"referenceID": 156, "context": "To address this issue, (Melo and Lopes, 2010) proposed a method that computes a kernel based on MDP metrics (Taylor et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 244, "context": "To address this issue, (Melo and Lopes, 2010) proposed a method that computes a kernel based on MDP metrics (Taylor et al., 2008) that includes the information of the environment dynamics.", "startOffset": 108, "endOffset": 129}, {"referenceID": 164, "context": "They use the method proposed by (Montesano and Lopes, 2012) to make queries where there is lower confidence of the estimated policy.", "startOffset": 32, "endOffset": 59}, {"referenceID": 137, "context": "by (Lopes et al., 2009b).", "startOffset": 3, "endOffset": 24}, {"referenceID": 41, "context": "This work was latter extended by considering not just the uncertainty on the policy but the expected reduction in the global uncertainty (Cohn et al., 2010; Cohn et al., 2011).", "startOffset": 137, "endOffset": 175}, {"referenceID": 39, "context": "This work was latter extended by considering not just the uncertainty on the policy but the expected reduction in the global uncertainty (Cohn et al., 2010; Cohn et al., 2011).", "startOffset": 137, "endOffset": 175}, {"referenceID": 195, "context": "The teacher can directly ask about the reward value at a given location (Regan and Boutilier, 2011) and it has been shown that reward queries can be combined with action queries (Melo and Lopes, 2013).", "startOffset": 72, "endOffset": 99}, {"referenceID": 157, "context": "The teacher can directly ask about the reward value at a given location (Regan and Boutilier, 2011) and it has been shown that reward queries can be combined with action queries (Melo and Lopes, 2013).", "startOffset": 178, "endOffset": 200}, {"referenceID": 67, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 34, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 18, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 256, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 21, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 106, "context": "The TAMMER framework, and its extensions, considers how signals from humans can speed up exploration and learning in reinforcement learning tasks (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 146, "endOffset": 190}, {"referenceID": 107, "context": "The TAMMER framework, and its extensions, considers how signals from humans can speed up exploration and learning in reinforcement learning tasks (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 146, "endOffset": 190}, {"referenceID": 106, "context": "Knox (Knox and Stone, 2009; Knox and Stone, 2010) presented the initial framework where the agent learns to predict the human feedback and then selects actions to maximize the expected reward from the human.", "startOffset": 5, "endOffset": 49}, {"referenceID": 107, "context": "Knox (Knox and Stone, 2009; Knox and Stone, 2010) presented the initial framework where the agent learns to predict the human feedback and then selects actions to maximize the expected reward from the human.", "startOffset": 5, "endOffset": 49}, {"referenceID": 108, "context": "Recently this process was improved to allow both processes to occur simultaneously (Knox and Stone, 2012).", "startOffset": 83, "endOffset": 105}, {"referenceID": 262, "context": "(Zhang et al., 2009) introduced a method were the teacher is able to provide extra rewards to change the behavior", "startOffset": 0, "endOffset": 20}, {"referenceID": 147, "context": "Other approaches considered that the learner can train by self-exploration and have several periods where the teacher is able to criticize its progress (Manoonpong et al., 2010; Judah et al., 2010).", "startOffset": 152, "endOffset": 197}, {"referenceID": 93, "context": "Other approaches considered that the learner can train by self-exploration and have several periods where the teacher is able to criticize its progress (Manoonpong et al., 2010; Judah et al., 2010).", "startOffset": 152, "endOffset": 197}, {"referenceID": 77, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 78, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 79, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 152, "context": "A similar approach was followed in the work of (Mason and Lopes, 2011).", "startOffset": 47, "endOffset": 70}, {"referenceID": 204, "context": "be made where the corrections are provided directly by moving the robot arm (Sauser et al., 2011).", "startOffset": 76, "endOffset": 97}, {"referenceID": 40, "context": "asynchronous behavior exist then the agent must decide how to act while waiting for the feedback (Cohn et al., 2012).", "startOffset": 97, "endOffset": 116}, {"referenceID": 103, "context": "Several of these works fall under the learning from communication framework (Klingspor et al., 1997), where a shared understanding between the robot and the teacher is fundamental to allow good interactive learning sessions.", "startOffset": 76, "endOffset": 100}, {"referenceID": 161, "context": "The system in (Mohammad and Nishida, 2010) automatically learns different interaction protocols for navigation tasks where the robot learns the actions it should make and which gestures correspond to those actions.", "startOffset": 14, "endOffset": 42}, {"referenceID": 132, "context": "In (Lopes et al., 2011; Grizou et al., 2013) the authors introduce a new algorithm for inverse reinforcement learning under multiple instructions with unknown symbols.", "startOffset": 3, "endOffset": 44}, {"referenceID": 76, "context": "In (Lopes et al., 2011; Grizou et al., 2013) the authors introduce a new algorithm for inverse reinforcement learning under multiple instructions with unknown symbols.", "startOffset": 3, "endOffset": 44}, {"referenceID": 114, "context": "An early work consider such process in isolation and considered that learning the meaning of communication can be simplified by using the expectation from the already known task model (Kozima and Yano, 2001).", "startOffset": 184, "endOffset": 207}, {"referenceID": 124, "context": "Other works, such as (Lauria et al., 2002; Kollar et al., 2010), consider the case of learning new instructions and guidance signals for already known tasks, thus providing more efficient commands for instructing the robot.", "startOffset": 21, "endOffset": 63}, {"referenceID": 110, "context": "Other works, such as (Lauria et al., 2002; Kollar et al., 2010), consider the case of learning new instructions and guidance signals for already known tasks, thus providing more efficient commands for instructing the robot.", "startOffset": 21, "endOffset": 63}, {"referenceID": 121, "context": "In (Kulick et al., 2013) the authors take an active learning approach allowing the", "startOffset": 3, "endOffset": 24}], "year": 2014, "abstractText": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications.1", "creator": "TeX"}}}