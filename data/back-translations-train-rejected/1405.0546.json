{"id": "1405.0546", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2014", "title": "Kaggle LSHTC4 Winning Solution", "abstract": "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.", "histories": [["v1", "Sat, 3 May 2014 01:41:27 GMT  (243kb,D)", "http://arxiv.org/abs/1405.0546v1", "Kaggle LSHTC winning solution description"], ["v2", "Fri, 9 May 2014 04:57:19 GMT  (243kb,D)", "http://arxiv.org/abs/1405.0546v2", "Kaggle LSHTC winning solution description"]], "COMMENTS": "Kaggle LSHTC winning solution description", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["antti puurula", "jesse read", "albert bifet"], "accepted": false, "id": "1405.0546"}, "pdf": {"name": "1405.0546.pdf", "metadata": {"source": "CRF", "title": "Kaggle LSHTC4 Winning Solution", "authors": ["Antti Puurula", "Jesse Read", "Albert Bifet"], "emails": [], "sections": [{"heading": null, "text": "Kaggle LSHTC4 Winning SolutionAntti Puurula1, Jesse Read2, and Albert Bifet31 Department of Computer Science, The University of Waikato, Private Bag 3105, Hamilton 3240, New Zealand2 Department of Information and Computer Science, Aalto University, FI-00076 Aalto, Espoo, Finland 3 Huawei Noah's Lab, Hong Kong Science Park, Shatin, Hong Kong, China"}, {"heading": "1 Overview", "text": "Our winning submission to the 2014 Kaggle Competition for Large-Scale Hierarchical Text Classification (LSHTC) consists largely of an interplay of sparse generative models spanning the Multinomial Naive Bayes. Basic classifiers consist of hierarchically smoothed models that combine document, label, and hierarchy levels, with pre-processing of functions using variants of TF-IDF and BM25. Additional diversification occurs through different types of folds and random search optimizations for different measures. macroFscore's ensemble algorithm optimizes macroFscore by predicting the documents for each label instead of the usual prediction of labels per document. Document values are predicted by weighted matching of base classifiers with a variant of feature-weighted linear stacking. The number of documents per label is selected based on label priorities and thresholds for voting results per document. This document describes the document and the models used to build our solution."}, {"heading": "2 Data Segmentation", "text": "Source files: MAKE FILES, nfold sample corpus.py, fast sample corpus.py, shuffle data.py, count labelsets2.pyThe segmentation of the training data is done by the script MAKE FILES included in the code package.4 https: / / kaggle2.blob.core.windows.net / competitions / kaggle / 3634 / media / LSHTC4 winner solution.zip 5 https: / / kaggle2.blob.core.windows.net / competitions / 3634 / media / LSHTC4 winner solution not resultsfiles.zipar Xiv: 140 5.05 46v1 [cs.AI] 3M ay2 014I2.341.782 portion of the dried training documents for the previous part 23,54 and 23,54 segments."}, {"heading": "3 Base-classifiers", "text": "The models are stored in an economical precomputing format, and the inference complexity is reduced according to the frugality of the model. Models are stored in an economical precomputing format, and inferencing with inverted indices is used to reduce the complexity of the model."}, {"heading": "4 Ensemble Model", "text": "This year it is more than ever before that it is a reactionary project, in which it is a reactionary project."}, {"heading": "5 How to Generate the Solution", "text": "The programs and scripts described above can be run to generate the winning submission file. Some of the programs require considerable computing resources, and both the optimization of the basic classifier parameters and the classification of the 452k document test set may take several days or longer, depending on the model. We used a handful of Quadcore i7-2600 CPU processors with 16GB of RAM over the competition period to develop and optimize the models. At least 16GB of RAM is required to store the word numbers that reach 100M parameters, and the ensemble combination requires less than 8GB of memory and can be calculated from the provided.results files. The base classifier result files are included in the distribution, as their calculation takes considerable time.To optimize the base classifiers, compile SGM-Tests.java with javac, configure Make devlateVS and run an existing V.UN template or.x."}, {"heading": "6 What Gave us the First Place?", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7 Acknowledgements", "text": "We would like to thank Kaggle and the organizers of the LSHTC for their work to make the competition a success, and the Machine Learning Group at the University of Waikato for the computers we used for our solution."}], "references": [{"title": "Scalable text classification with sparse generative modeling", "author": ["A. Puurula"], "venue": "Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelligence. PRICAI\u201912,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Integrated instance- and class-based generative modeling for text classification", "author": ["A. Puurula", "S.H. Myaeng"], "venue": "Australasian Document Computing Symposium", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Combining modifications to multinomial naive bayes for text classification", "author": ["A. Puurula"], "venue": "Information Retrieval Technology. Volume 7675 of Lecture Notes in Computer Science", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Feature-weighted linear stacking", "author": ["J. Sill", "G. Takcs", "L. Mackey", "D. Lin"], "venue": "CoRR abs/0911.0460", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The weka data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explor. Newsl", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Cumulative progress in language models for information retrieval", "author": ["A. Puurula"], "venue": "Australasian Language Technology Workshop", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "A detailed description of basic models of this type are given in [1, 2].", "startOffset": 65, "endOffset": 71}, {"referenceID": 1, "context": "A detailed description of basic models of this type are given in [1, 2].", "startOffset": 65, "endOffset": 71}, {"referenceID": 2, "context": "py performs a Gaussian Random Search [3] for the chosen parameters, constrained and transformed according to the configuration file.", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "idf weighting from TF-IDF that has been used earlier [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "The Multinomials use hierarchical smoothing with a uniform background distribution [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "This constructs document-conditional models, and computes label-conditional probabilities using the document-conditionals as kernel densities [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "The ensemble model is built on our earlier LSHTC3 ensemble [3], but performs classification by predicting instances per label.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "The classifier vote weight prediction is a case of Feature-Weighted Linear Stacking [4], but the regression models are trained separately for each base-classifier, using reference weights that approximate optimal weights per label in a development set.", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "The most useful metafeatures in the LSHTC3 submission used labelset correlation features between the base-classifiers for each document instance [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "java show how the features are constructed as Weka [5] Instances.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc.", "startOffset": 150, "endOffset": 153}], "year": 2014, "abstractText": "Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores.", "creator": "LaTeX with hyperref package"}}}