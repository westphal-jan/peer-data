{"id": "1405.1005", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "Comparing apples to apples in the evaluation of binary coding methods", "abstract": "We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means).", "histories": [["v1", "Mon, 5 May 2014 19:26:58 GMT  (795kb)", "http://arxiv.org/abs/1405.1005v1", null], ["v2", "Sat, 27 Sep 2014 18:35:35 GMT  (570kb)", "http://arxiv.org/abs/1405.1005v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mohammad rastegari", "shobeir fakhraei", "jonghyun choi", "david jacobs", "larry s davis"], "accepted": false, "id": "1405.1005"}, "pdf": {"name": "1405.1005.pdf", "metadata": {"source": "CRF", "title": "Comparing apples to apples in the evaluation of binary coding methods", "authors": ["Mohammad Rastegari", "Shobeir Fakhraei", "Jonghyun Choi", "David Jacobs", "Larry S. Davis"], "emails": ["@cs.umd.edu", "@umiacs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Experimental Settings and Notations", "text": "To assess the accuracy of binary codes, we measure the accuracy and remember the task of retrieving the closest neighbors to r. First, we divide the data into two groups: Train and Test. On the training portion, we learn the binary codes, and on the test partition, we use this model to extract the binary codes. We take the actual closest neighbors in the original attribute space as truth. Then, we use the binary codes to find the closest neighbors in the binary space by hamming removal. Considering a query sample, we collect the samples around the query in the hamming space in the radius r. For each r, we calculate the precision and the retrieval. Therefore, we construct the precision callback curve by variing r."}, {"heading": "2.1 Datasets", "text": "ImageNet20K [2] ImageNet comprises 17000 object categories. We used the benchmark of ImageNET, which is called ISLVRC2010 and has 1000 categories; for each category we randomly selected 20 examples. In total we have 20K images. This setting follows [19]. We used BoW (with 1000 code words of SIFT functions), SPHoG, GIST and LBP as visual characteristics; we concatenated them to a long feature vector and used PCA to reduce dimensionality to 1000. 1MGist [7] This data set contains 1M Internet images and their GIST characteristics; it has 960 dimensions. This data set was created by Je'gou et al. [7] and was used for approximate dimensions in the search for neighbors. SUN14K [25] This data set has 700 categories of images. Unlike ImageNet, this data set was used for the detection of Hageet [7] features."}, {"heading": "2.2 Methods", "text": "The methods we are examining are: Iterative Quantization (ITQ) [5], Orthogonal K-Means (OK Means) [16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Local Sensitive Hashing (LSH) [4]; we compare two variants of LSH - one with the bias term (LSHbias) and the other without (LSHnobias). We present ITQ with a bias term enhanced by Predictable Hashing (PH), using the software available online for ITQ, MDSH and OK means provided by their authors."}, {"heading": "2.3 Notation", "text": "We call the data matrix X, RD, and N, where D is the dimensionality of the space and N is the number of data points. A data point is represented by xi, which is the ith column of X. The output binary codes are represented by B, where K is the number of bits and bi is the ith column of B."}, {"heading": "3 Preserving Cosine Similarity", "text": "It is about the question of whether and to what extent it is actually about a way in which it is about the dissemination of data, which is about the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is disseminated in which it is disseminated in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in which it is dissemination in the way in which it is dissemination in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemination in the way in which it is dissemin"}, {"heading": "4 Preserving Euclidean Distance", "text": "In this scenario, the performance of LSH is significantly better than the results published in conjunction with recently developed data-driven approaches suggest. If the length of binary code is sufficiently large, LSH outperforms these data-driven methods, largely because these data-driven methods require a mechanism to avoid redundant hyperplanes - which encode training data in a very similar way - by ensuring that vectors of encoded bits are uncorrelated across hyper-planes. We will explain why this limitation of correlation is too strong and limits the performance of data-driven methods. In addition, we study different data distributions and illustrate and justify the cases where LSH performs better. We propose a simple coding scheme based on the notion of predictability that allows ITQ to learn binary codes that model the correlation between the original characteristics. We show that the predictability of the method allows the length to be increased."}, {"heading": "4.1 In Defense of LSH", "text": "This year it is more than ever before."}, {"heading": "4.2 Predictable Embedding: Enforcing bias term in ITQ", "text": "It is the question that arises whether this is a way in which people move in the most different parts of the world. (...) It is the question to what extent it is a way in which people move in the most different parts of the world. (...) It is the question of whether it is a way in which people move in the most different parts of the world. (...) It is the question to what extent people move in the most different parts of the world. (...) It is the question to what extent they move in the most different parts of the world. (...) It is the question to what extent they are in the most different parts of the world. (...) It is the question to what extent do they move in the most different parts of the world. (...) It is the question of how far do people differ in the most different parts of the world. (...) It is the question to what extent do they exist in the most different parts of the world."}, {"heading": "5 Conclusion", "text": "We demonstrated that the appropriate method for comparing different binary code methods depends on the metric that binary mapping is intended to maintain. To maintain cosinal similarity, the data should be mapped to the hypersphere of the entity. This allows a fair comparison between Euclidean distance preservation methods and methods for maintaining cosinal similarity. We showed that, contrary to what has been reported in recent publications, ITQ outperforms all other methods. On the other hand, capturing the correlation between data plays a crucial role if the goal is to maintain the Euclidean distance. Applying an LSH with a bias term can achieve the best performance with a large budget of bitcodes. We demonstrated that including the bias helps to model the correlation between the data in the original feature space. If ITQ is extended by a bias term, it achieves better performance by a large margin. We also presented a geometric intuition for it based on predictable cobinary coding."}], "references": [{"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S Mirrokni"], "venue": "In Proceedings of the twentieth annual symposium on Computational geometry,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Optimized product quantization for approximate nearest neighbor search", "author": ["Tiezheng Ge", "Kaiming He", "Qifa Ke", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Similarity search in high dimensions via hashing", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the international conference on very large data bases,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "K-means hashing: An affinitypreserving quantization method for learning binary compact codes", "author": ["Kaiming He", "Fang Wen", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Searching with quantization: approximate nearest neighbor search using short codes and distance estimators", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid"], "venue": "Technical Report RR-7020,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Complementary projection hashing", "author": ["Zhongming Jin", "Yao Hu", "Yue Lin", "Debing Zhang", "Shiding Lin", "Deng Cai", "Xuelong Li"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Brian Kulis", "Trevor Darrell"], "venue": "In The Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A general two-step approach to learning-based hashing", "author": ["Guosheng Lin", "Chunhua Shen", "David Suter", "Anton van den Hengel"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Compressed hashing", "author": ["Yue Lin", "Rong Jin", "Deng Cai", "Shuicheng Yan", "Xuelong Li"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Supervised hashing with kernels", "author": ["W Liu", "J Wang", "R Ji", "YG Jiang", "SF Chang"], "venue": "Proceedings of Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Hash bit selection: A unified solution for selection problems in hashing", "author": ["Xianglong Liu", "Junfeng He", "Bo Lang", "Shih-Fu Chang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Minimal Loss Hashing for Compact Binary Codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Cartesian k-means", "author": ["Mohammd Norouzi", "David Fleet"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["Genevieve Patterson", "James Hays"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Scalable objectclass retrieval with approximate and top-k ranking", "author": ["Mohammad Rastegari", "Chen Fang", "Lorenzo Torresani"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["Mohammad Rastegari", "Ali Farhadi", "David Forsyth"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Dual-view predictable hashing", "author": ["Mohammad Rastegari", "Jonghyun Choi", "Shobeir Fakhraei", "Hal Daume", "Larry Davis"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Kernel principal component analysis. In Advances in kernel methods-support vector learning", "author": ["Bernhard Scholkopf", "Alexander Smola", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Inductive hashing on manifolds", "author": ["Fumin Shen", "Chunhua Shen", "Qinfeng Shi", "Anton van den Hengel", "Zhenmin Tang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Spectral Hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Multidimensional spectral hashing", "author": ["Yair Weiss", "Rob Fergus", "Antonio Torralba"], "venue": "In Proceedings of the 12th European conference on Computer Vision (ECCV),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A Ehinger", "Aude Oliva", "Antonio Torralba"], "venue": "In Computer vision and pattern recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "[15] proposed a multi-index hashing method, and Rastegari et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] introduced a branch and bound approach to perform exact k-nearest neighbors search in sub-linear time with long binary codes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] introduced locality sensitivity hashing (LSH) and provide theoretical guarantees on retrieval in sublinear time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[23] introduced Spectral Hashing (SPH) and [24] Multidimensional Spectral Hashing (MDSH); they formulate the problem as an optimization that reduces to an eigenvalue problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[23] introduced Spectral Hashing (SPH) and [24] Multidimensional Spectral Hashing (MDSH); they formulate the problem as an optimization that reduces to an eigenvalue problem.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "Furthermore, Gong and Lazebnik [5](ITQ) and Norouzi and Fleet [16] (CK-means) proposed methods to minimize the quantization error of mapping the data to a binary hypercube via rotation.", "startOffset": 31, "endOffset": 34}, {"referenceID": 15, "context": "Furthermore, Gong and Lazebnik [5](ITQ) and Norouzi and Fleet [16] (CK-means) proposed methods to minimize the quantization error of mapping the data to a binary hypercube via rotation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "Product Quantization [7] is an instance of Cartesian Kmeans[16] that does not optimize for rotation and Orthogonal K-means is the binary version of CK-means that number of subspaces are equal to the number of bits.", "startOffset": 21, "endOffset": 24}, {"referenceID": 15, "context": "Product Quantization [7] is an instance of Cartesian Kmeans[16] that does not optimize for rotation and Orthogonal K-means is the binary version of CK-means that number of subspaces are equal to the number of bits.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "There is another class of binary code learning methods that are supervised [12, 14, 19].", "startOffset": 75, "endOffset": 87}, {"referenceID": 13, "context": "There is another class of binary code learning methods that are supervised [12, 14, 19].", "startOffset": 75, "endOffset": 87}, {"referenceID": 18, "context": "There is another class of binary code learning methods that are supervised [12, 14, 19].", "startOffset": 75, "endOffset": 87}, {"referenceID": 13, "context": "We experimentally observed that given a good supervision, MLH[14] performs similar to ITQ.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 5, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 7, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 9, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 10, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 12, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 15, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 21, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 23, "context": "Recent papers [3, 6, 8, 10, 11, 13, 16, 22, 24] showed comparisons without taking these factors into account.", "startOffset": 14, "endOffset": 47}, {"referenceID": 4, "context": "In particular, some methods preserve cosine similarity [5], while others preserve Euclidean distance [8, 24].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "In particular, some methods preserve cosine similarity [5], while others preserve Euclidean distance [8, 24].", "startOffset": 101, "endOffset": 108}, {"referenceID": 23, "context": "In particular, some methods preserve cosine similarity [5], while others preserve Euclidean distance [8, 24].", "startOffset": 101, "endOffset": 108}, {"referenceID": 15, "context": "We show that the state-of-the-art technique OK-means [16] and MDSH [24] most of the times, in first scenario, performs worse than Iterative Quantization (ITQ)[5], which is an older method.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "We show that the state-of-the-art technique OK-means [16] and MDSH [24] most of the times, in first scenario, performs worse than Iterative Quantization (ITQ)[5], which is an older method.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "We show that the state-of-the-art technique OK-means [16] and MDSH [24] most of the times, in first scenario, performs worse than Iterative Quantization (ITQ)[5], which is an older method.", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "LSH [1] most of the time performs better than every other method in the second scenario for long binary codes.", "startOffset": 4, "endOffset": 7}, {"referenceID": 18, "context": "Inspired by [19], we propose a geometrical intuition on learning Iterative Quantization (ITQ)[5] augmented by a bias term.", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "Inspired by [19], we propose a geometrical intuition on learning Iterative Quantization (ITQ)[5] augmented by a bias term.", "startOffset": 93, "endOffset": 96}, {"referenceID": 18, "context": "[19, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[19, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19, 20] use predictability to represent nearest neighbor preservation via a max-margin formulation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[19, 20] use predictability to represent nearest neighbor preservation via a max-margin formulation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "ImageNet20K [2] ImageNet includes 17000 categories of objects.", "startOffset": 12, "endOffset": 15}, {"referenceID": 18, "context": "This setting is followed by [19].", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "1MGist [7] This dataset contains 1M internet images and their GIST features; it has 960 dimensions.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "[7] and has been used for approximate nearest neighbor search in the vision community.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "SUN14K [25] This dataset has 700 categories of images.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "We used the portion of this dataset used for attribute based recognition by Patterson and Hays [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "It has 14K images and we used the visual features extracted by Patterson and Hays [17] which have 19K dimensions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "The methods we evaluate are: Iterative Quantization (ITQ) [5], Orthogonal K-means(OK-means)[16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Locality Sensitive Hashing (LSH) [4]; we compare with two variant of LSH \u2013 one with the bias term (LSHbias) and the other without (LSHnobias).", "startOffset": 58, "endOffset": 61}, {"referenceID": 15, "context": "The methods we evaluate are: Iterative Quantization (ITQ) [5], Orthogonal K-means(OK-means)[16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Locality Sensitive Hashing (LSH) [4]; we compare with two variant of LSH \u2013 one with the bias term (LSHbias) and the other without (LSHnobias).", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "The methods we evaluate are: Iterative Quantization (ITQ) [5], Orthogonal K-means(OK-means)[16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Locality Sensitive Hashing (LSH) [4]; we compare with two variant of LSH \u2013 one with the bias term (LSHbias) and the other without (LSHnobias).", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "The methods we evaluate are: Iterative Quantization (ITQ) [5], Orthogonal K-means(OK-means)[16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Locality Sensitive Hashing (LSH) [4]; we compare with two variant of LSH \u2013 one with the bias term (LSHbias) and the other without (LSHnobias).", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "The methods we evaluate are: Iterative Quantization (ITQ) [5], Orthogonal K-means(OK-means)[16], Spectral Hashing (SPH) [23], Multidimensional Spectral Hashing (MDSH) [24], Locality Sensitive Hashing (LSH) [4]; we compare with two variant of LSH \u2013 one with the bias term (LSHbias) and the other without (LSHnobias).", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "Gong and Lazebnik [5] did not explicitly show that ITQ preserves cosine similarity.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "This is contrary to the results presented in [16] and [24] that claimed that OK-means and MDSH outperform ITQ.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "This is contrary to the results presented in [16] and [24] that claimed that OK-means and MDSH outperform ITQ.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "The power of LSH has been underestimated in many recent papers [5, 14, 24].", "startOffset": 63, "endOffset": 74}, {"referenceID": 13, "context": "The power of LSH has been underestimated in many recent papers [5, 14, 24].", "startOffset": 63, "endOffset": 74}, {"referenceID": 23, "context": "The power of LSH has been underestimated in many recent papers [5, 14, 24].", "startOffset": 63, "endOffset": 74}, {"referenceID": 0, "context": "[1], the chance of preserving nearest neighbors in the Hamming space by LSH will be higher when the random values of wk come from a normal distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "codes in many binary hashing papers for image retrieval [5] [9] [14] [24], reveals that they do not include a bias term for random hyperplanes in LSH when comparing their methods with LSH for preserving Euclidean distance.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "codes in many binary hashing papers for image retrieval [5] [9] [14] [24], reveals that they do not include a bias term for random hyperplanes in LSH when comparing their methods with LSH for preserving Euclidean distance.", "startOffset": 60, "endOffset": 63}, {"referenceID": 13, "context": "codes in many binary hashing papers for image retrieval [5] [9] [14] [24], reveals that they do not include a bias term for random hyperplanes in LSH when comparing their methods with LSH for preserving Euclidean distance.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "codes in many binary hashing papers for image retrieval [5] [9] [14] [24], reveals that they do not include a bias term for random hyperplanes in LSH when comparing their methods with LSH for preserving Euclidean distance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "There may be nonlinear correlations between the data but capturing nonlinear correlations is not trivial, and, in fact, is an area of current research; one can use Kernel-PCA[21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "The concept of predictability in binary codes was introduced for attribute discovery [19] and dual-view hashing [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "The concept of predictability in binary codes was introduced for attribute discovery [19] and dual-view hashing [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": "[19, 20] proposed that a bit value produced by a hyperplane is predictable when the corresponding hyperplane has max-margin from data points.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[19, 20] proposed that a bit value produced by a hyperplane is predictable when the corresponding hyperplane has max-margin from data points.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "In [19], their goal was to produce binary codes that preserve the classification performance", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [20], their goal was to learn a shared Hamming space between two modalities (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Again, this is contrary to results published in previous papers [16, 24] on these methods.", "startOffset": 64, "endOffset": 72}, {"referenceID": 23, "context": "Again, this is contrary to results published in previous papers [16, 24] on these methods.", "startOffset": 64, "endOffset": 72}, {"referenceID": 3, "context": "As discussed in [4], the parameters of a random projection should come from a p-stable distribution to preserve \ufffdp-norm neighbors in the original feature space.", "startOffset": 16, "endOffset": 19}], "year": 0, "abstractText": "We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means).", "creator": "cairo 1.10.2 (http://cairographics.org)"}}}