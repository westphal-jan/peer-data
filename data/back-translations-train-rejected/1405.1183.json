{"id": "1405.1183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2014", "title": "Some thoughts about benchmarks for NMR", "abstract": "The NMR community would like to build a repository of benchmarks to push forward the design of systems implementing NMR as it has been the case for many other areas in AI. There are a number of lessons which can be learned from the experience of other communi- ties. Here are a few thoughts about the requirements and choices to make before building such a repository.", "histories": [["v1", "Tue, 6 May 2014 08:09:13 GMT  (19kb)", "http://arxiv.org/abs/1405.1183v1", "Proceedings of the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014)"]], "COMMENTS": "Proceedings of the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel le berre"], "accepted": false, "id": "1405.1183"}, "pdf": {"name": "1405.1183.pdf", "metadata": {"source": "CRF", "title": "Some thoughts about benchmarks for NMR", "authors": ["Daniel Le Berre"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 5.11 83v1 [cs.AI] 6M ay2 01"}, {"heading": "What to expect", "text": "Over the past two decades, a huge number of municipalities have built up repositories of benchmarks, mainly with the idea of evaluating running systems on a common set of issues. The oldest common input format for AI benchmarks is probably STRIPS (Fictions and Nilsson 1971), for planning systems. One of the oldest and most compelling is TPTP (\"Thousands of Problems for Theorem Provers\") (Sutcliffe 2009), the benchmarks library for First Order and Higher Order Theorem Provers. Such a repository was built in 1993 and has since been developed as a companion to the CADE ATP System Competition (CASC) (Sutcliffe and Suttner 2006), the benchmarks library for First Order and Higher Order Theorem Provers."}, {"heading": "Modeling versus Benchmarking", "text": "One of the first questions that arises when creating a benchmark format is the goal of the format. There are mainly two possibilities: to satisfy the end user by providing a format that simplifies modeling in this format; and to promote the formats designed by the SAT community to read such a format. High-level input formats such as PDDL, TPTP, ASP and Minizinc are clearly modeled; formats designed by the SAT community (SAT, MAXSAT, QBF, MUS) are clearly solvable."}, {"heading": "Data versus Protocol", "text": "Another question raised when designing an input format is whether the benchmark represents data or whether it represents a complete protocol; the problem is orthogonal to the abstraction level of the input format: it is guided by the nature of the issues to be solved; in many cases, benchmarks represent data in one or more files (e.g. rules and facts, domain and instance), and the system responds to a single query; there are other cases where some interaction with the system is required: for example, the SMT LIB 2 format (Barrett, Stump and Tinelli 2010) defines a protocol for communicating with the system in order to solve problems step by step, meaning that the system is governmental in this case; and the Aiger format used in the competition to verify hardware models (Biere and Jussila 2007) also provides some incremental capabilities that correspond to the process of Bounded Model Checking."}, {"heading": "Checkable queries", "text": "In this case, the aforementioned people will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be to be able to be able to be to be able to be able to be to be to be able to be to be able to be to be to be able to be to be to be able to be to be to be to be able to be to be to be to be to be able to be to be to be to be to be to be able to be to be to be to be to be to be able to be to be to be to be to be to be to be to be to be to be to be to be to be to be able to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be able to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to to be to be to be to"}, {"heading": "Chicken and egg problem", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "The bias of benchmarking systems", "text": "It should also be clear that the benchmarks used to evaluate the systems in some sense determine the systems developed or improved by the community. Anyone who looks at the winners of the various SAT competitions7 can check whether the solvers behave differently based on randomly generated benchmarks and benchmarks from real-world applications or hard combinatorial problems, for each community. Randomly generated benchmarks are interesting for two reasons: they are easy to generate and can be generally formally defined. Combinatorial benchmarks are important because they usually force the system to behave badly. Application benchmarks are interesting because they provide some clues to the practical complexity of the problem."}, {"heading": "Benchmarks libraries", "text": "Benchmarks are normally made available to the community through a library: CSPLIB, SATLIB, PBLIB, SMTLIB, etc. However, managing these libraries over the long term is a problem. A good example is SATLIB (Hoos and Sttzle 2000). It was designed in 1999 to host the benchmarks made available to the SAT community. It has done a good job in collecting the benchmarks created during the 1990s. However, the huge increase in the benchmarks (and their size!) in early 2000 made it difficult to catch up after 2001, so the SAT competition websites have since been providing the benchmarks used in the competitions. The situation is not ideal because there is no longer a central place in the SAT community to access the benchmarks. Some of the benchmarks made available to the research community by IBM (Zarpas 2006) cannot be distributed anymore."}, {"heading": "Conclusion", "text": "Many communities have set up central repositories with benchmarks to compare the performance of their systems, and the success of these repositories depends, first, on the adoption of this format by the community, and, second, on the availability of benchmarks for which some information is provided: difficulty, expected response, runtime of existing systems, etc. 9http: / / www.csplib.org / 10http: / / www.tptp.org / For a community like NMR, which addresses a wide range of different issues, the first step is to decide what issues will require an initial effort at standardization. Heuristics can be either the maturity of existing systems in the community or the importance of the problem to the community. In both cases, the choice of the format for the benchmarks is important: should it be user-oriented or system-oriented? Data or protocol-oriented, should data or benchmarks be used."}], "references": [{"title": "and Treinen", "author": ["P. Abate"], "venue": "R.", "citeRegEx": "Abate and Treinen 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "The SMT-LIB Standard: Version 2.0", "author": ["Stump Barrett", "C. Tinelli 2010] Barrett", "A. Stump", "C. Tinelli"], "venue": "Proceedings of the 8th International Workshop on Satisfiability Modulo Theories (Edinburgh, UK)", "citeRegEx": "Barrett et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2010}, {"title": "and Jussila", "author": ["A. Biere"], "venue": "T.", "citeRegEx": "Biere and Jussila 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated testing and debugging of sat and qbf solvers", "author": ["Lonsing Brummayer", "R. Biere 2010] Brummayer", "F. Lonsing", "A. Biere"], "venue": "Computer Science,", "citeRegEx": "Brummayer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brummayer et al\\.", "year": 2010}, {"title": "N", "author": ["R. Fikes", "Nilsson"], "venue": "J.", "citeRegEx": "Fikes and Nilsson 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "The first answer set programming system competition", "author": ["Gebser"], "venue": "Computer Science,", "citeRegEx": "Gebser,? \\Q2007\\E", "shortCiteRegEx": "Gebser", "year": 2007}, {"title": "W", "author": ["Heule, M.", "Jr."], "venue": "A. H.; and Wetzler, N.", "citeRegEx": "Heule. Jr.. and Wetzler 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Sttzle", "author": ["H.H. Hoos"], "venue": "T.", "citeRegEx": "Hoos and Sttzle 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Trick", "author": ["D. Johnson"], "venue": "M., eds.", "citeRegEx": "Johnson and Trick 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Promoting robust black-box solvers through competitions", "author": ["Roussel Lecoutre", "C. van Dongen 2010] Lecoutre", "O. Roussel", "M.R.C. van Dongen"], "venue": null, "citeRegEx": "Lecoutre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lecoutre et al\\.", "year": 2010}, {"title": "Evaluating and certifying qbfs: A comparison of state-of-the-art tools", "author": ["Narizzano"], "venue": "AI Commun", "citeRegEx": "Narizzano,? \\Q2009\\E", "shortCiteRegEx": "Narizzano", "year": 2009}, {"title": "and Strichman", "author": ["V. Ryvchin"], "venue": "O.", "citeRegEx": "Ryvchin and Strichman 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Chatalic", "author": ["L. Simon"], "venue": "P.", "citeRegEx": "Simon and Chatalic 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "E", "author": ["L. Simon", "D. Le Berre", "Hirsch"], "venue": "A.", "citeRegEx": "Simon. Le Berre. and Hirsch 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "P", "author": ["Stuckey"], "venue": "J.; Becket, R.; and Fischer, J.", "citeRegEx": "Stuckey. Becket. and Fischer 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Suttner", "author": ["G. Sutcliffe"], "venue": "C.", "citeRegEx": "Sutcliffe and Suttner 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "The TPTP Problem Library and Associated Infrastructure: The FOF and CNF Parts, v3.5.0", "author": ["G. Sutcliffe"], "venue": "Journal of Automated Reasoning", "citeRegEx": "Sutcliffe,? \\Q2009\\E", "shortCiteRegEx": "Sutcliffe", "year": 2009}, {"title": "Producing and verifying extremely large propositional refutations - have your cake and eat it too", "author": ["Van Gelder"], "venue": "Ann. Math. Artif. Intell", "citeRegEx": "Gelder,? \\Q2012\\E", "shortCiteRegEx": "Gelder", "year": 2012}], "referenceMentions": [], "year": 2014, "abstractText": "The NMR community would like to build a repository of benchmarks to push forward the design of systems implementing NMR as it has been the case for many other areas in AI. There are a number of lessons which can be learned from the experience of other communities. Here are a few thoughts about the requirements and choices to make before building such a repository.", "creator": "LaTeX with hyperref package"}}}