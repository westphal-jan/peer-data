{"id": "1405.1379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device", "abstract": "This paper addresses the challenging scenario for the distant-talking control of a music playback device, a common portable speaker with four small loudspeakers in close proximity to one microphone. The user controls the device through voice, where the speech-to-music ratio can be as low as -30 dB during music playback. We propose a speech enhancement front-end that relies on known robust methods for echo cancellation, double-talk detection, and noise suppression, as well as a novel adaptive quasi-binary mask that is well suited for speech recognition. The optimization of the system is then formulated as a large scale nonlinear programming problem where the recognition rate is maximized and the optimal values for the system parameters are found through a genetic algorithm. We validate our methodology by testing over the TIMIT database for different music playback levels and noise types. Finally, we show that the proposed front-end allows a natural interaction with the device for limited-vocabulary voice commands.", "histories": [["v1", "Mon, 5 May 2014 14:37:47 GMT  (105kb,D)", "http://arxiv.org/abs/1405.1379v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["ramin pichevar", "jason wung", "daniele giacobello", "joshua atkins"], "accepted": false, "id": "1405.1379"}, "pdf": {"name": "1405.1379.pdf", "metadata": {"source": "CRF", "title": "Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device", "authors": ["Ramin Pichevar", "Jason Wung", "Daniele Giacobello"], "emails": ["ramin.pichevar@beatsbydre.com"], "sections": [{"heading": "1. Introduction", "text": "In this context, it should be noted that these two are very complex matters, and that this is a very complex and complex matter."}, {"heading": "2. Speech Enhancement System", "text": "Let y [n] be the near-range microphone signal consisting of the near-range language s [n] and noise v [n] mixed with the acoustic echo d [n] = h [n] \u0445 x [n], where h [n] is the impulse response of the system, x [n] is the distant reference signal and \u043a is the convolution operator. The overall block diagram of the speech enhancement algorithm is shown in Figure 1, which consists of two robust acoustic echo cancellators (RAECs), a double-sided Xiv: 140 5.13 79v1 [cs.SD] on May 5, 201 4talk probability (DTP) estimators, two residual power estimators (RPEs), a noise power estimator (NPE), a noise suppressor (NS) and a voice activity detector (VAD)."}, {"heading": "2.1. Robust Acoustic Echo Canceler", "text": "Since strong fast-end interference can corrupt the error signal of the acoustic echo suppressor (AEC) and lead to a deviation of the adaptive filter, the RAEC system [6, 8] is used, in which the error-restored non-linearity and robust adaptive step size control allow continuous tracking of the echo path during double calls. To reduce the delay of the adaptive frequency range [16], the retardable adaptive filter structure [17] is used. A cascaded structure similar to the system approach of [7] is used: The output of the first RAEC is fed to the input of the second RAEC, which differs from the original system approach in [7], where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work). The tuning parameters for each of the RAEC consist of the frame size of the NAEC, the number of the number of the AEC distributed blocks of the number of the AEC, the number of the number of the Aitations of the number of the number of the AEC distributed by the number of the AEC."}, {"heading": "2.2. Residual Echo Power Estimator", "text": "Since the AEC cannot cancel the entire echo signal due to modelling inconsistency, a further improvement of the residual echo suppressor (RES) is required to improve speech quality. A coherence-based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar [20] is used for a more accurate estimate of residual echo power. As shown in Figure 1, the DTP estimator differs from the one in [20] in that the coherence between the estimated RAEC echo signal d and the microphone signal y is calculated, rather than between the speaker signal x and the microphone signal y. This is possible because the estimated echo signal d is due to the robust echo path traceability of the RAEC. In this work, we propose to minimize the residual echo compatibility of the signal by using the output of the dual echo suppressor."}, {"heading": "2.3. Noise Suppressor", "text": "In this paper, we combine RPE and NPE for residual echo and noise suppression with a single noise suppressor, as shown in Figure 1. For combined residual echo and noise suppression, the Ephraim and Malah log amplitude estimator [9] is used. (2) The estimation of the a priori speech-to-noise ratio (SNR) is based on the decision-oriented (DD) approach [22]: The estimation of the residual echo and noise threshold (DD) is based on the decision-directed (DD) approach [22]: The estimation of residual and resistant tuners (m \u2212 1), the estimate of frequency and noise threshold (SNR) based on the decision-directed (DD) approach [22]."}, {"heading": "2.4. Generation of Speech Enhancement Mask", "text": "In view of our application scenario, we propose to combine the direct masking approach, which is particularly effective at overall low SNRs, with the NS output mask GLSAk, as shown in Figure 1. Specifically, we use the estimated gain from the LSA imaging to determine the type of masking to apply to the spectrum. However, given an accurate estimate of the binary mask, it is very difficult for very low SNRs to use the estimated gain from the LSA imaging for these cases. Our masking will then be: \"k\" = [(1 \u2212 Gmin) GLSAk [m] + Gmin], \"k\" [m] [m] output models that can be used in the proposed Figure 1 < < < < < < < < m] output models."}, {"heading": "3. The Tuning Problem", "text": "The matching problem can be formalized as an optimization problem. In our case, the objective function to maximize is the ASR recognition rate R (s [n]), where s [n] is the processed language, i.e. the output of the language enhancement system. To narrow the search range, we can impose inequality constraints on the variables that simply set the lower and upper limit for the components of the solution vector. Our optimization problem will then be: Maximize R (s [n, p]))) subject to U \u2264 p \u2264 L, (4) where p is now the vector of the parameters that need to be matched, s [n, p] is the output of the language enhancement system that is achieved with these parameters, and L and U represent respectively lower and upper limits for the values of each variable. The basic concept of a GA is to use genetic operators as mutation and crossover to develop a set of M-function, k (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K) (K)) (K) (K) (K)) (K) (K) (K) (K) (K) (K) (K (K))) (K (K (K))) (K (K (K))) (K (K ()))) ()) (K (K ())) (K ())) (K ())) () ())) ()) () ())) () ()) () ()) ()) ()) ())) () () ()) () ()) () ()) ()) () ()) ()) () ())) () () ())) () ())) ()) ()) () ()) ()) ()) ())) ()) ()) ()) ()))) ()) ()) ())) ()) ()))) ()) ()))) () ())) () ())))) ()))"}, {"heading": "4. Experimental Results", "text": "In this section, we present the results of our designed front-enhancement front-end for speech enhancement with the tuned parameters using those in Section 3. To obtain the set of parameters that maximize detection rate, we optimized and optimized the system on a noisy TIMIT database. The set of tuned parameters is then used in the ASR front-end for telephony control of our music player with limited vocabulary, as illustrated in Figure 3."}, {"heading": "4.1. Speech Recognition on TIMIT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. Noisy Database", "text": "The database was created by simulating the interaction between the user and the playback device. In this scenario, music is played from a portable device with four loudspeakers and an embedded microphone placed approximately one centimetre from the nearest loudspeaker, and the user expresses speech in a resonant environment with continuous music playback. The microphone signal y [n] was then generated according to: y [n] = s [n] + \u03c31d [n] + \u03c32v2 [n] + \u03c33v3 [n], consisting of the language s [n], the acoustic echo from music d [n], the background noise v2 [n] (Babbel, Factory and Music) and a pink noise introduced to simulate a mild broadband constant electrical noise and electromagnetic radiation v3 [n]. For each file in the TIMIT database, the SER and SNR were selected from equal distributions, the values of 10 \u2212 B and \u2212 B \u2212 the target values, the target values, the target values, the target values, the target values, the target values, and the target values were selected from 10 \u2212 15 \u2212 B and \u2212 the responses."}, {"heading": "4.1.2. Training of the Speech Recognizer", "text": "We used the HTK toolkit [25] to train an acoustic model on the noisy TIMIT database of 61 telephones [27]. A set of 13 Mel Frequency Cepstral Coefficients (MFCCs) with their first and second derivatives, i.e. a total of 39 coefficients, is generated and used as characteristics for our experimental analysis. We normalized the variance and mean of the MFCCs as proposed in [10] to apply the direct masking correctly. We used 5-state HMMs with an 8-mix GMM for each phone. We trained our HMMs with the loud language processed by our front-end."}, {"heading": "4.1.3. Recognition of the noisy TIMIT database", "text": "After obtaining the HNRM in the acoustic model, we optimized the parameters of our front end. We raised the problem as in Section 3. For the initial population, we chose a set of reasonably well-manually optimized parameters and reasonable limits that allow us to use only three generations to achieve convergence; the genetic algorithm had a population of M = 40 possible candidates, and the best N = 10 were migrated to the next generation; these values were empirically selected by balancing the complexity and accuracy of the results; the Telephone Accuracy Rate (PAR) using a Bigram model increased from 35% to 40% after our optimization of the training data, proving the validity of our method; and, to allow a fair comparison, we also adjusted the parameters to maximize the mean opinion value (MOS) by comparing the test results with the target alignment of the TES database (POLQA)."}, {"heading": "4.2. Limited Vocabulary Speech Recognition", "text": "We used the tuned parameters and HMMs obtained from our analysis of the TIMIT database to investigate the feasibility of speech recognition with limited vocabulary under extremely difficult conditions."}, {"heading": "4.2.1. Recognition of limited-size Vocabulary Speech", "text": "We used the system to recognize four commands: \"PLAY,\" \"NEXT,\" \"BACK\" and \"PAUSE.\" The commands were generated by changing the TIMIT language model accordingly. As shown in Figure 1, we used a standard VAD applied frame-by-frame, after direct masking, to isolate the commands [31, 32]: \u2211 k [\u03b3k\u043ek 1 + \u0412k \u2212 log (1 + \u0412k)] > \u03b7, (6) where \u0442k and \u03b3k are the a priori and a posteriori SNRs and \u03b7 a fixed threshold. Figure 4 shows an example of a loud command before and after processing. The command is inaudible to human listeners before processing, while the language structure is well preserved after processing."}, {"heading": "4.2.2. Recording of Real-World Commands", "text": "We used eight subjects (male / female, native / non-native) who uttered the command list at a distance of about 1 m from the microphone of the Beats PillTM portable speaker while the music was being played. We used four different music tracks in the echo path, selecting the starting point of the track at random, and the subjects gave the following commands to the speakers: \"PLAY,\" \"NEXT,\" \"BACK,\" \"PAUSE\" (as shown in Figure 3) The playback level for the experiments was set to three different levels of 95 dB SPL, 90 dB SPL and 85 dB SPL. We estimated the range of the SERs for the different setups to be approximately \u2212 35 to \u2212 30 dB, \u2212 30 dB and \u2212 25 dB, and \u2212 20 dB for the three levels of SPL, or the tuning of the SERs based on language optimization."}, {"heading": "5. Conclusion", "text": "The proposed front-end for speech enhancement consists of a cascaded robust AEC, an echo performance estimate based on a double call probability estimate, and a novel quasi-binary masking that uses the classic MMSE-based method at very low SNRs. Tuning greatly improves the speech recognition rate on the TIMIT database, and then tests the optimized front-end in realistic environments for remote control of a music player with a limited command vocabulary, resulting in a relatively high recognition rate for voice commands at a speech-to-music ratio of \u2212 35 dB, scenarios rarely encountered in literature."}, {"heading": "6. References", "text": "In fact, it is such that it is a matter of a way in which people who are able to understand the world and understand what it is all about: a world in which people who are able to understand the world, and around a world in which people who are able to understand the world, understand the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}], "references": [{"title": "An overview of noise-robust automatic speech recognition", "author": ["J. Li", "L. Deng", "Y. Gong", "R. Haeb-Umbach"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745\u2013777, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Distant speech recognition", "author": ["M. W\u00f6lfel", "J. McDonough"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A natural acoustic front-end for interactive TV in the EU-project DICIT", "author": ["L. Marquardt", "P. Svaizer", "E. Mabande", "A. Brutti", "C. Zieger", "M. Omologo", "W. Kellermann"], "venue": "Proc. IEEE Pacific Rim Conf. on Comm., Computers and Signal Processing, pp. 894\u2013899, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Microphone array processing for robust speech recognition", "author": ["M.L. Seltzer"], "venue": "Ph.D. dissertation, Carnegie Mellon University, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Acoustic echo cancellation based on independent component analysis and integrated residual echo enhancement", "author": ["T.S. Wada", "B.-H. Juang"], "venue": "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 205\u2013208, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "A system approach to acoustic echo cancellation in robust hands-free teleconferencing", "author": ["J. Wung", "T.S. Wada", "B.-H. Juang", "B. Lee", "M. Trott", "R.W. Schafer"], "venue": "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 101\u2013104, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhancement of residual echo for robust acoustic echo cancellation", "author": ["T.S. Wada", "B.-H. Juang"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 175\u2013189, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using a minimum mean-square error log-spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 33, no. 2, pp. 443\u2013445, 1985.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1985}, {"title": "A direct masking approach to robust ASR", "author": ["W. Hartmann", "A. Narayanan", "E. Fosler-Lussier", "D. Wang"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 21, pp. 1993\u20132005, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Genetic algorithms in search, optimization, and machine learning, Addison-Wesley", "author": ["D.E. Goldberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Tuning methodology for speech enhancement algorithms using a simulated conversational database and perceptual objective measures", "author": ["D. Giacobello", "J. Wung", "R. Pichevar", "J. Atkins"], "venue": "accepted for publication in Proc. 4th Joint Workshop on Hands-free Speech Communication and Microphone Arrays, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Results on automated tuning of a voice quality enhancement system using objective quality measures", "author": ["D. Giacobello", "J. Atkins", "J. Wung", "R. Prabhu"], "venue": "Proc. 135th Audio Engineering Society Convention, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "TIMIT: acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo"], "venue": "Linguistic Data Consortium,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Effective acoustic adaptation for a distant-talking interactive TV system.", "author": ["J. Huang", "M. Epstein", "M. Matassoni"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Frequency-domain and multirate adaptive filtering", "author": ["J.J. Shynk"], "venue": "IEEE Signal Processing Magazine, vol. 9, no. 1, pp. 14\u201337, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Multidelay block frequency domain adaptive filter", "author": ["J.S. Soo", "K.K. Pang"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, , vol. 38, no. 2, pp. 373\u2013376, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Unbiased residual echo power estimation for hands-free telephony", "author": ["G. Enzner", "R. Martin", "P. Vary"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 2, pp. 1893\u20131896, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1893}, {"title": "Residual echo power spectral density estimation based on an optimal smoothed misalignment for acoustic echo cancelation", "author": ["S. Goetze", "M. Kallinger", "K.-D. Kammeyer"], "venue": "Proc. Intl. Workshop on Acoustic Echo and Noise Control, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Coherence based double talk detector with soft decision", "author": ["I.J. Tashev"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 165\u2013168, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Unbiased MMSEbased noise power estimation with low complexity and low tracking delay", "author": ["T. Gerkmann", "R.C. Hendriks"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 4, pp. 1383\u20131393, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Robust automatic speech recognition with missing and unreliable acoustic data", "author": ["M. Cooke", "P. Green", "L. Josifovski", "A. Vizinho"], "venue": "Speech communication, vol. 34, pp. 267\u2013285, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Reconstruction of missing features for robust speech recognition", "author": ["B. Raj", "M. Seltzer", "R.M. Stern"], "venue": "Speech Communication, vol. 43, pp. 275\u2013296, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Joint optimization of LCMV beamforming and acoustic echo cancellation for automatic speech recognition", "author": ["W. Herbordt", "S. Nakamura", "W. Kellerman"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 3, pp. 77\u201380, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint noise reduction and acoustic cancellation using the transfer-function generalized sidelobe canceller", "author": ["G. Reuven", "S. Gannot", "I. Cohen"], "venue": "Speech communication, vol. 49, pp. 623\u2013635, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Voice Activity Detection. Fundamentals and Speech Recognition System Robustness, Robust Speech Recognition and Understanding", "author": ["J. Ramirez", "J. Gorriz", "J. Segura"], "venue": "Michael Grimm and Kristian Kroschel (Eds.),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A statistical model-based voice activity detection", "author": ["J. Sohn", "N. Kim", "W. Sung"], "venue": "IEEE Signal Processing Letters, vol. 6, no. 1, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": ", reverberation [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "In particular, due to the severe degradation of the input signal, the ASR performance drops significantly when the distance between the user and the microphone increases [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": ", the DICIT project [4].", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "However, to the authors\u2019 knowledge, the available solutions rely heavily on large microphone arrays [5], which may be infeasible for handheld portable device.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 5, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 6, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 7, "context": "Secondly, we propose a novel noise reduction method, where we combine a traditional minimum mean-squared error (MMSE) speech enhancement approach [9] with an estimate of the ideal binary mask [10].", "startOffset": 146, "endOffset": 149}, {"referenceID": 8, "context": "Secondly, we propose a novel noise reduction method, where we combine a traditional minimum mean-squared error (MMSE) speech enhancement approach [9] with an estimate of the ideal binary mask [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "The parameters of the algorithm are tuned for maximum recognition rate by casting the tuning problem as a nonlinear program, solved efficiently through a genetic algorithm (GA) [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 10, "context": "A similar approach was used in [12, 13] to maximize the objective perceptual quality of a speech enhancement system for fullduplex communication.", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "A similar approach was used in [12, 13] to maximize the objective perceptual quality of a speech enhancement system for fullduplex communication.", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "The training and evaluation corpora are generated through a synthetic mixture of clean speech (from the TIMIT database [14]) and music, both convolved with separate impulse responses, and further mixed with a background noise to cover as many deployment scenarios as possible.", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "The acoustic models of the ASR are trained by the front-end enhanced speech, an effective way to learn and exploit the typical distortions of the system itself [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "Since strong near-end interference may corrupt the error signal of the acoustic echo canceler (AEC) and cause the adaptive filter to diverge, the RAEC system [6, 8] is used, where the error recovery nonlinearity and robust adaptive step-size control allows for continuous tracking of the echo path during double talk.", "startOffset": 158, "endOffset": 164}, {"referenceID": 6, "context": "Since strong near-end interference may corrupt the error signal of the acoustic echo canceler (AEC) and cause the adaptive filter to diverge, the RAEC system [6, 8] is used, where the error recovery nonlinearity and robust adaptive step-size control allows for continuous tracking of the echo path during double talk.", "startOffset": 158, "endOffset": 164}, {"referenceID": 14, "context": "To reduce the delay of the frequency-domain adaptive filter [16], the multi-delay adaptive filter structure [17] is used.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "To reduce the delay of the frequency-domain adaptive filter [16], the multi-delay adaptive filter structure [17] is used.", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "A cascaded structure similar to the system approach of [7] is used: the output of the first RAEC is fed to the input of the second RAEC, which is different from the original system approach in [7] where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work).", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "A cascaded structure similar to the system approach of [7] is used: the output of the first RAEC is fed to the input of the second RAEC, which is different from the original system approach in [7] where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work).", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 36, "endOffset": 44}, {"referenceID": 17, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 36, "endOffset": 44}, {"referenceID": 18, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "As shown in Figure 1, the DTP estimator differs from that in [20] since the coherence is calculated between the RAEC estimated echo signal d\u0302 and the microphone signal y rather than between the loudspeaker signal x and the microphone signal y.", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "The low complexity MMSE noise power estimator [21] is used for the NPE, and the Ephraim and Malah logspectral amplitude (LSA) estimator [9] is used for the combined residual echo and noise suppression:", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "The low complexity MMSE noise power estimator [21] is used for the NPE, and the Ephraim and Malah logspectral amplitude (LSA) estimator [9] is used for the combined residual echo and noise suppression:", "startOffset": 136, "endOffset": 139}, {"referenceID": 20, "context": "The estimation of the a priori speech-to-noise ratio (SNR) \u03bek is done using the decision-directed (DD) approach [22]:", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 170, "endOffset": 174}, {"referenceID": 22, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "where Gmin is the minimum suppression gain [13], and the output is then: \u015ck[m] = \u03b6k[m]Ek[m].", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": ",M} in order to find the solution that maximizes the cost function [11, 26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 8, "context": "We normalized the variance and mean of the MFCCs, as suggested in [10] for properly applying the direct masking.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "In order to provide a fair comparison, we also tuned the parameters to maximize the mean opinion score (MOS) using the Perceptual Objective Listening Quality Assessment (POLQA) [28], as done in [12], through the same GA setup and the same noisy TIMIT database.", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "Although used in a different setup, the results obtained with the proposed method compare favorably to some prior results [29, 30], where authors investigated joint echo cancellation and speech enhancement at higher SERs and SNRs.", "startOffset": 122, "endOffset": 130}, {"referenceID": 24, "context": "Although used in a different setup, the results obtained with the proposed method compare favorably to some prior results [29, 30], where authors investigated joint echo cancellation and speech enhancement at higher SERs and SNRs.", "startOffset": 122, "endOffset": 130}, {"referenceID": 25, "context": "As shown in Figure 1, we used a standard VAD, applied on a frame-by-frame basis, after the direct masking to isolate the commands [31, 32]: \u2211", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "As shown in Figure 1, we used a standard VAD, applied on a frame-by-frame basis, after the direct masking to isolate the commands [31, 32]: \u2211", "startOffset": 130, "endOffset": 138}, {"referenceID": 10, "context": "Also in this case, we compared with the set of parameters obtained by optimization through POLQA [12].", "startOffset": 97, "endOffset": 101}], "year": 2014, "abstractText": "This paper addresses the challenging scenario for the distanttalking control of a music playback device, a common portable speaker with four small loudspeakers in close proximity to one microphone. The user controls the device through voice, where the speech-to-music ratio can be as low as \u221230 dB during music playback. We propose a speech enhancement front-end that relies on known robust methods for echo cancellation, doubletalk detection, and noise suppression, as well as a novel adaptive quasi-binary mask that is well suited for speech recognition. The optimization of the system is then formulated as a large scale nonlinear programming problem where the recognition rate is maximized and the optimal values for the system parameters are found through a genetic algorithm. We validate our methodology by testing over the TIMIT database for different music playback levels and noise types. Finally, we show that the proposed front-end allows a natural interaction with the device for limited-vocabulary voice commands.", "creator": "LaTeX with hyperref package"}}}