{"id": "1405.2262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2014", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "abstract": "We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.", "histories": [["v1", "Fri, 9 May 2014 15:23:06 GMT  (273kb,D)", "http://arxiv.org/abs/1405.2262v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["michael s gashler", "stephen c ashmore"], "accepted": false, "id": "1405.2262"}, "pdf": {"name": "1405.2262.pdf", "metadata": {"source": "CRF", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "authors": ["Michael S. Gashler", "Stephen C. Ashmore"], "emails": ["scashmor}@uark.edu,"], "sections": [{"heading": null, "text": "Keywords: neural networks, time series, curve fit, extrapolation, Fourier decomposition"}, {"heading": "1 Introduction", "text": "An example of this is the fact that most of them are people who are able to decide for themselves what they want to do and what they want to do."}, {"heading": "2 Intuitive-level Algorithm Description", "text": "Our approach uses a deep artificial neural network with a mixture of activation functions. We train the network with stochastic gradients descent [40]. Each unit in the artificial neural network uses one of three activation functions, represented in Figure 1, sinusoid: f (x) = sin (x), softplus: f (x) = loge (1 + ex), or identity: f (x) = x. Using the \"identity\" activation function creates a linear unit that is only able to model linear components in the data. Nonlinear components in the data require a nonlinear activation function. The softplus units allow the network to fit non-repeated nonlinearity in the training sequence. The sinusoid units allow the network to repeat nonlinearity in the data."}, {"heading": "2.1 Regularization", "text": "In order to achieve a better nonlinear extrapolation, it is necessary to simplify the model. Occam's shaver suggests that the simplest model that is able to adapt to the training data most likely provides the best generalizing predictions. The simplest unit in our model is the linear unit. Therefore, it is desirable to shift so much of the total weight in the network away from sinusoidal units and toward the linear units while the model is still adapted to the training data. Where non-recurring nonlinearity occurs in the training data, linear units will not be able to match the training data. In such cases, we want the weight to shift toward the softplus units. Where repetitive nonlinearity occurs in the training data, the sinusoidal units should retain some of their original weight shift. We achieve this weight shift during training using a combination of control techniques."}, {"heading": "2.2 Training procedure", "text": "This works well with the logistical activation function because its derived results give a value close to zero anywhere except for input values close to zero. This gives it a very local region of influence, so any pattern presentation will often make significant changes to the few weights relevant to predicting this pattern. However, the three activation functions we use in our algorithm have non-local regions of influence. In other words, any pattern presentation will often significantly affect many or all weights in the network, which has the effect of giving the network a strong tendency to divergence unless a very small learning rate is used. In our experiments, we found that the values of appropriate learning rates vary significantly from training. Small learning rates, such as 10 \u2212 4, would quickly lead to divergences."}, {"heading": "3 Related Works", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4 Technical Algorithm Description", "text": "In this section, we describe implementation details necessary to reproduce our results. Intuition for this algorithm is described in Section 2. To support further research, we have integrated the most important parts of our implementation into the Waffles Machine Learning Toolkit [15]."}, {"heading": "4.1 Initializing Weights", "text": "Our neural network contains 4 layers. Layer 4 (the initial layer) contains a single linear unit. Layer 3 contains k sinusoid units, where k has a power of 2. In our implementation, layer 3 also contains 12 softplus units and 12 linear units. (These quantities can be adjusted, but due to our use of L1 regularization there is little need to optimize them.) We consider an explanation of k complex values as input. The real component is given by the values in the training sequence. The imaginary component consists of all zeros. The output of the fast Fourier form is also a sequence of k complex values as input. {r1, i1}, {r2}, {r3}, ik, ik."}, {"heading": "4.2 Training", "text": "To start with the training, we initialize the learning rate \u03b7 to 10 \u2212 9 and the regularization term \u03bb to 1. (Note that these values are dynamically adjusted during the training.) Each training epoch presents the k-time series values in the training sequence to the network in a random order. A time value is fed into each presentation, the weights are regulated, and then the weights are updated through stochastic weight loss by means of regular back propagation. During the first half of the training epochs, the L2 regularization is used. In the second half of the training epochs, the L1 regularization is used. In each of our experiments, we performed 107 training epochs. At the end of each training epoch, the values \u03b7 and \u03bb are dynamically adjusted. This is done by measuring the root mean of the squared prediction recommendation over the training values. If < 0.1 points, then the optimization process = Great Britain / 1.001 is taken back to the training, we can only copy these values by a factor of 1, if we can actually validate the result."}, {"heading": "5 Validation", "text": "In this section, we present visual results that show that our method is capable of modelling nonlinear trends into the future. (Intentionally not used in this section, quantitative comparisons with corresponding results using recursive neural network methods are essential. However, if our intention was to establish our method as a new state of the art in time series computation, such comparisons would be indispensable. If the research community focused only on improving the method with the highest precision, it risks getting stuck in a local optimum. By advancing this alternative approach, we want to open the way for more research interest in this area, which could potentially lead to other use cases or a long-term shift in predictions."}, {"heading": "6 Summary of Contributions", "text": "To facilitate better generalization, our method inserts several units with simpler activation functions into the network and then trains in a way that allows the weight to shift toward these simpler units while simultaneously matching the training data. We use a simple method of dynamic parameter setting to train efficiently while ensuring that the model always fits well into the training sequence. We presented results from several experiments that show that our method is effective in predicting nonlinear trends in time series data. In particular, the deeper layers provide a mechanism to significantly \"reduce\" the time in the sinoid region by providing new theoretical intuition as to why deep neural networks can actually facilitate the search for simpler prediction models than they can be found with flat networks. In particular, the deeper layers provide a mechanism to significantly \"decrease\" the time in the sinoid region by simultaneously shifting the subsequent weight units during training in such a way that they are conserved by both the weight units and the time difference."}], "references": [{"title": "The analysis of observed chaotic data in physical systems", "author": ["H.D. Abarbanel", "R. Brown", "J.J. Sidorowich", "L.S. Tsimring"], "venue": "Reviews of modern physics 65(4), 1331", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Combining neural network forecasts on wavelettransformed time series", "author": ["A. Aussem", "F. Murtagh"], "venue": "Connection Science 9(1), 113\u2013122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "A real-coded genetic algorithm for training recurrent neural networks", "author": ["A. Blanco", "M. Delgado", "M.C. Pegalajar"], "venue": "Neural Networks 14(1), 93\u2013105", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Predicting chaotic time series with wavelet networks", "author": ["L. Cao", "Y. Hong", "H. Fang", "G. He"], "venue": "Physica D: Nonlinear Phenomena 85(1), 225\u2013238", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Time-series prediction using a local linear wavelet neural network", "author": ["Y. Chen", "B. Yang", "J. Dong"], "venue": "Neurocomputing 69(4), 449\u2013465", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A committee of neural networks for traffic sign classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on. pp. 1918\u20131921. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "New life for neural networks", "author": ["G.W. Cottrell"], "venue": "Science 313(5786), 454\u2013455", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An application of non-linear programming to train recurrent neural networks in time series prediction problems", "author": ["M.P. Cu\u00e9llar", "M. Delgado", "M.C. Pegalajar"], "venue": "Enterprise Information Systems VII pp. 95\u2013102", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems 2(4), 303\u2013314", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "25 years of time series forecasting", "author": ["J.G. De Gooijer", "R.J. Hyndman"], "venue": "International journal of forecasting 22(3), 443\u2013473", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks for time series processing", "author": ["G. Dorffner"], "venue": "Neural Network World. Citeseer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning deep face representation", "author": ["H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin", "C. Doudou"], "venue": "arXiv preprint arXiv:1403.2802", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic creation of an autonomous agent: Genetic evolution of a neural-network driven robot", "author": ["D. Floreano", "F. Mondada"], "venue": "From animals to animats 3, 421\u2013430", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Time series prediction and neural networks", "author": ["R.J. Frank", "N. Davey", "S.P. Hunt"], "venue": "Journal of Intelligent and Robotic Systems 31(1-3), 91\u2013103", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Waffles: A machine learning toolkit", "author": ["M.S. Gashler"], "venue": "Journal of Machine Learning Research MLOSS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Temporal nonlinear dimensionality reduction", "author": ["M.S. Gashler", "T.R. Martinez"], "venue": "Proceedings of the International Joint Conference on Neural Networks. pp. 1959\u2013 1966. IEEE Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalenet-multiscale neural-network architecture for time series prediction", "author": ["A.B. Geva"], "venue": "Neural Networks, IEEE Transactions on 9(6), 1471\u20131482", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "Mohamed", "A.r.", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. pp. 6645\u20136649. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Forecasting stock market movement direction with support vector machine", "author": ["W. Huang", "Y. Nakamori", "S.Y. Wang"], "venue": "Computers & Operations Research 32(10), 2513\u20132522", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing 10(3), 215\u2013236", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25. pp. 1106\u20131114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Using support vector machines for long-term discharge prediction", "author": ["J.Y. Lin", "C.T. Cheng", "K.W. Chau"], "venue": "Hydrological Sciences Journal 51(4), 599\u2013612", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Fourier neural networks: An approach with sinusoidal activation functions", "author": ["L. Mingo", "L. Aslanyan", "J. Castellanos", "M. Diaz", "V. Riazanov"], "venue": "International Journal ITA 11(1)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "A focused backpropagation algorithm for temporal pattern recognition", "author": ["M.C. Mozer"], "venue": "Backpropagation: theory, architectures, and applications pp. 137\u2013169", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "A focused back-propagation algorithm for temporal pattern recognition", "author": ["M.C. Mozer"], "venue": "Complex systems 3(4), 349\u2013381", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Training recurrent neural networks: Why and how ? an illustration in dynamical process modeling", "author": ["O. Nerrand", "P. Roussel-Ragot", "D. Urbani", "L. Personnaz", "G. Dreyfus"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Gpu implementation of neural networks", "author": ["K.S. Oh", "K. Jung"], "venue": "Pattern Recognition 37(6), 1311\u20131314", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Tech. Rep. CUED/F-INFENG/TR.1, Cambridge University, Engineering Department", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1987}, {"title": "Time series prediction using support vector machines: a survey", "author": ["N.I. Sapankevych", "R. Sankar"], "venue": "Computational Intelligence Magazine, IEEE 4(2), 24\u201338", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Fourier neural networks", "author": ["A. Silvescu"], "venue": "Neural Networks, 1999. IJCNN\u201999. International Joint Conference on. vol. 1, pp. 488\u2013491. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonlinear black-box modeling in system identification: a unified overview", "author": ["J. Sj\u00f6berg", "Q. Zhang", "L. Ljung", "A. Benveniste", "B. Deylon", "P.Y. Glorennec", "H. Hjalmarsson", "A. Juditsky"], "venue": "Automatica 31, 1691\u20131724", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural networks for control", "author": ["E. Sontag"], "venue": "Essays on Control: Perspectives in the Theory and its Applications 14, 339\u2013380", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1993}, {"title": "Wavelet neural network classification of eeg signals by using ar model with mle preprocessing", "author": ["A. Subasi", "A. Alkan", "E. Koklukaya", "M.K. Kiymik"], "venue": "Neural Networks 18(7), 985\u2013997", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2014", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Fourier neural networks and generalized single hidden layer networks in aircraft engine fault diagnostics", "author": ["H. Tan"], "venue": "Journal of engineering for gas turbines and power 128(4), 773\u2013782", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks 1(4), 339\u2013356", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Networks 16(10), 1429\u20131451", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Time series forecasting using a hybrid arima and neural network model", "author": ["G.P. Zhang"], "venue": "Neurocomputing 50, 159\u2013175", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2003}, {"title": "Forecasting with artificial neural networks:: The state of the art", "author": ["G. Zhang", "B. Eddy Patuwo", "M. Y Hu"], "venue": "International journal of forecasting 14(1), 35\u201362", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Wavelet networks", "author": ["Q. Zhang", "A. Benveniste"], "venue": "Neural Networks, IEEE Transactions on 3(6), 889\u2013898", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1992}, {"title": "Fourier-neural-network-based learning control for a class of nonlinear systems with flexible components", "author": ["W. Zuo", "Y. Zhu", "L. Cai"], "venue": "Neural Networks, IEEE Transactions on 20(1), 139\u2013151", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 24, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 37, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 43, "context": "Fourier decompositions provide a mechanism to make neural networks with sinusoidal activation functions fit to a training sequence [25,38,44], but it is one thing to fit a curve to a training sequence, and quite another to make it extrapolate effectively to predict future nonlinear trends.", "startOffset": 131, "endOffset": 141}, {"referenceID": 8, "context": "Indeed, a neural network with only one hidden layer has been shown to be a universal function approximator [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 22, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 21, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 11, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 36, "context": "For example, they have been very effective in the domain of visual recognition [6,23,22,12,37].", "startOffset": 79, "endOffset": 94}, {"referenceID": 39, "context": "We train the network using stochastic gradient descent [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Since softplus units are a softened variant of rectified linear units, this intuition is consistent with published empirical results showing that rectified linear units can outperform logistic units [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 24, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 37, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 43, "context": "(Similar approaches have been previously proposed [25,38,44].", "startOffset": 50, "endOffset": 60}, {"referenceID": 10, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 20, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 41, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 13, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 40, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 9, "context": "Many papers have surveyed the various techniques for using neural networks and related approaches to model and forecast time-series data [11,21,42,14,41,10].", "startOffset": 137, "endOffset": 156}, {"referenceID": 13, "context": "The most common, and perhaps simplest, methods for time-series prediction involve feeding sample values from the past into the model to predict sample values in the future [14,1].", "startOffset": 172, "endOffset": 178}, {"referenceID": 0, "context": "The most common, and perhaps simplest, methods for time-series prediction involve feeding sample values from the past into the model to predict sample values in the future [14,1].", "startOffset": 172, "endOffset": 178}, {"referenceID": 28, "context": "A more sophisticated group of methods involves neural networks with recurrent connections [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 33, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 2, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 206, "endOffset": 215}, {"referenceID": 26, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 308, "endOffset": 315}, {"referenceID": 38, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 308, "endOffset": 315}, {"referenceID": 30, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 348, "endOffset": 355}, {"referenceID": 25, "context": "Existing methods for training the weights of recurrent neural networks can be broadly divided into two categories: Those based on nonlinear global optimization techniques, such as evolutionary optimization [13,34,3], and those based on descending a local error gradient, such as Backpropagation Through Time [27,39] or Real-Time Recurrent Learning [31,26].", "startOffset": 348, "endOffset": 355}, {"referenceID": 34, "context": "Unfortunately, in practice, evolutionary optimization tends to be extremely slow, and it is unable to yield good results with many difficult problems [35,34].", "startOffset": 150, "endOffset": 157}, {"referenceID": 33, "context": "Unfortunately, in practice, evolutionary optimization tends to be extremely slow, and it is unable to yield good results with many difficult problems [35,34].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "With recurrent neural networks, local optima are a more significant problem than with regular feed-forward networks [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 18, "context": "Long Short Term Memory architectures address this problem by using only linear units with the recurrent loops [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 6, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 15, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "Recent advances in deep neural network learning have also helped to improve the training of recurrent neural networks [30,7,16,18].", "startOffset": 118, "endOffset": 130}, {"referenceID": 32, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 177, "endOffset": 184}, {"referenceID": 24, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 177, "endOffset": 184}, {"referenceID": 37, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 251, "endOffset": 258}, {"referenceID": 43, "context": "The idea of using a neural network that can combine basis functions to reconstruct a signal, and initializing its weights with a Fourier transform, has been previously proposed [33,25], and more recently methods for training them have begun to emerge [38,44].", "startOffset": 251, "endOffset": 258}, {"referenceID": 42, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 3, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 1, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 16, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 35, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 4, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 49, "endOffset": 65}, {"referenceID": 19, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 23, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 31, "context": "Some popular approaches include wavelet networks [43,4,2,17,36,5], and support vector machines [20,24,32].", "startOffset": 95, "endOffset": 105}, {"referenceID": 14, "context": "To assist further research, we have integrated the major parts of our implementation into the Waffles machine learning toolkit [15].", "startOffset": 127, "endOffset": 131}], "year": 2014, "abstractText": "We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.", "creator": "LaTeX with hyperref package"}}}