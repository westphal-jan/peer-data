{"id": "1405.2420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2014", "title": "Optimal Learners for Multiclass Problems", "abstract": "The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of \"how to learn\"? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).", "histories": [["v1", "Sat, 10 May 2014 11:23:08 GMT  (58kb)", "http://arxiv.org/abs/1405.2420v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "shai shalev-shwartz"], "accepted": false, "id": "1405.2420"}, "pdf": {"name": "1405.2420.pdf", "metadata": {"source": "CRF", "title": "Optimal Learners for Multiclass Problems", "authors": ["Amit Daniely"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 5.24 20v1 [cs.LG] 1 0M ay2 01"}, {"heading": "1. Introduction", "text": "It is a fundamental problem in machine learning that deals with a variety of different areas, including object recognition, speech recognition, document categorization, and much more. Over the years, multicultural classification has undergone intensive studies, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Keshet et al., 2005; Torralba et al., 2007). Many methods have been developed to address this problem, starting with the na\u00efve one-on-one-all method, to more complex methods such as structured predictions (Collins, 2002; Taskar 2001)."}, {"heading": "2. No optimal learner can be proper", "text": "Our first result shows that surprisingly, any learning algorithm with near-optimal sample complexity must be inadequate. (Theorem 1) For each 1 \u2264 d \u00b2, there is a hypotheses category Hd, with 2 d + 1 terms such as: \u2022 The PAC sample complexity of Hd is O (log (1 / \u03b4). \u2022 The PAC sample complexity of a proper learning algorithm for Hd is inadequate (d + log (1 / \u03b4). \u2022 In particular, H \u00b2 is a learnable class that cannot be learned by a proper algorithm. (Detailed evidence is given in the appendix, and here we sketch the main idea of the proof. Let X be a finite set and let Y = 2X \u00b2.) For each A X, we define hA \u2192 Yby hA (x) = {A x \u00b2 A \u00b2 A \u00b2."}, {"heading": "3. An optimal learner for general classes", "text": "In this section, we describe and analyze a generic optimal learning algorithm. Let's start with an algorithm for a transductive learning environment in which the algorithm \u2212 1 should observe labeled examples and an additional unlabeled example and output the labeling of the unlabeled example. Later, in Section 3.3, we show a generic reduction from the transductive setting to the usual inductive learning model (i.e., the vanilla PAC model). Formally, in the transductive model, the algorithm looks at a series of m unlabeled examples, S-Xm, and then one of them is uniformly randomly selected, x-U (S). The algorithm observes the labeling of all examples except the selected example and should predict the labeling of the selected example. That is, the input of the algorithm, A, is the set S-Xm, and the limitation of some h-H to S\\ x, the labeling of the selected example. The algorithm should predict the labeling of the selected example."}, {"heading": "3.1. The one-inclusion algorithm", "text": "It is not only a question of time, but also a question of time until we go into the world. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to this question. (...) It is a question of time until we find an answer to the question. (...) It is a question of time until we ask it. (...) It is a question of time until we ask it. (...) It is a question of time until we ask it. (...) It is a question of time until we ask it."}, {"heading": "3.2. Analysis", "text": "The main result of this section is a new analysis of the one inclusion algorithm that shows its optimality in the transductive model (up to a constant factor of 1 / 2. In the next section, we will deal with the PAC model. In order to present our results, we need a few definitions. Let G = (V, E) be a hypergraph. Throughout the process, we will consider only hypergraphs for which E is an antique (i.e., there is no e1, e2, E that e1 is strictly contained in e2. Since U V, we will define the induced hypergraph, G [U] as a hypergraph whose vertex theorem is U and whose marginal theorem is e U, so that e E-2, and e is maximum w.r.t. These conditions are us. The degree of a vertex v in a hypergraph G = (V, E) is the number of the hyperedges, e E, so that we e-2, and e is maximum w.r.t."}, {"heading": "3.3. PAC optimality: from transductive to inductive learning", "text": "In the previous section, we analyzed the optimal error rate of learning in transductive learning. We now turn to the inductive PAC model. By a simple reduction from inductive to transductive learning, we will show that a variant of the one-inclusion algorithm in the PAC model is essentially optimal. First, any transductive algorithm A can be interpreted naturally as an inductive algorithm that we call Ai. Specifically, after seeing the sample S = {(xi, yi)} m \u2212 1 i = 1, the hypothesis h: X \u2192 Y so that h (x) is the designation A for x after looking at the designated sample S. It is understood that (see Appendix) the (worst) expectation of the error of the hypothesis returned by Ai in the m-point sample is the same (up to a factor of e up to and including H (m)."}, {"heading": "4. Efficient optimal learning and gaps for linear classes", "text": "In this section, we examine the family of linear hypotheses classes that is widely used in practice and has received much attention in the literature - see, for example, Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that even with such simple classes, there can be gaps between ERM sample complexity and PAC sample complexity, which negatively responds to an open question raised by Collins (2005). In addition, we derive computationally efficient optimal learners for linear classes based on the concept of compression schemes. This is in contrast to the inclusion algorithm from the previous section, which is generally inefficient. Due to the lack of space, most evidence is moved to the annex."}, {"heading": "4.1. Linear hypothesis classes", "text": "First, we define the different hypotheses classes of the linear multi-class classifiers we study. All of these classes depend on a class-specific feature mapping, i.e. X \u00d7 Y \u2192 Rd. We will provide some examples of feature mappings that are widely used in practice."}, {"heading": "4.1.1. Dimension based linear classifiers (denoted H\u03a8)", "text": "For w-Rd and x-X, define the multi-class predictor hw (x) = ArgmaxyY < w, c-R (x, y) >. In case of a tie, hw (x) is assumed to be a \"don't know label,\" and the corresponding hypotheses class is defined as H\u0442 = {hw | w-R d}. Example 1 (Multivector construction) If the labels are unstructured, a canonical choice of \"i\" is the so-called multivector construction. In this case, Y = [k], X = Rd, and the prediction on an instance x-Rd is the index of the d-k matrix whose y'th column is x, while the rest is 0. In this case, each classifier corresponds to a matrix W, and the prediction on an instance x-Rd is the index of the column that maximizes the inner product with x."}, {"heading": "4.1.2. Large margin linear classifiers (denoted H\u03a8,R)", "text": "The second type of hypothesis that we apply in classification is based on the question of where the number of possible spaces lies, but the second type of hypothesis that we apply in classification is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply is the margin. (...) The second type of hypothesis that we apply. (...) The second type of hypothesis. (...) The second type of hypothesis. (...) The second type of hypothesis. (...) The second type of hypothesis."}, {"heading": "4.2. Results", "text": "We start with linear predictors without margin (1 / 2).To put the results in the relevant context, it was necessary that the (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM) -group (ERM (ERM) -group (ERM) -group (ERM (ERM) -group (ERM) -group (ERM (ERM) -group (ERM group) -group (ERM (ERM) -group (ERM) -group (ERM group (ERM) -group (ERM) -group (ERM group) -group (ERM (ERM) -group (ERM group (ERM) -group (ERM) -group) -group (ERM group (ERM) -group (ERM group) -group (ERM group (ERM) -group (ERM) -group) -group (ERM group) -group (ERM group) -group (ERM group) -group (ERM group) -group (ERM group (ERM group) -group (ERM group (ERM) ERM group) -group (ERM group) -group (ERM group) -group (ERM group) ERM group (ERM group) ERM group (ERM group) -group (ERM group (ERM group) ERM group (ERM"}, {"heading": "4.3. The compression-based optimal learners", "text": "Each of theorems 5, 6, 8 and 9 consists of two statements: the first asserts that some algorithms have a certain sample complexity, while the second asserts that there is an ERM whose sample complexity is worse than the sample complexity of the algorithm in the first part. As explained in this section, the first parts of these theorems are established by the development of (efficient) compression schemes. In the next section, we will discuss the evidence of the second parts (the lower limits of specific ERMs). Unfortunately, we have to be very short due to the lack of space. We now show that it is possible for linear classes to derive optimal learners who are also compatibly efficient. In the case of margin-based classes, this result is not new - an efficient algorithm based on the multicultural perceptions proposed in Collins (2002)."}, {"heading": "4.4. Lower bounds for specific ERMs", "text": "Next, we will explain how to implement the second parts of theorems 5, 6, 8 and 9. For theorems 5 and 6, the idea is to start with the first Cantor class (introduced in section 2) and to realize it through a geometric construction by a linear class. This realization allows us to extend the \"bad ERM\" for the first Cantor class to a \"bad ERM\" for that linear class. The idea behind the lower limits of theorems 8 and 9 is similar but more technically involved. Instead of the first Cantor class, we will introduce a new discrete class, the second Cantor class, which may be of independent interest. This class, which can be considered dual to the first Cantor class, will be defined as follows. Let Y be a non-empty finite set. Let X = 2Y class and let Y = Y dimension be recognized."}, {"heading": "5. A new dimension", "text": "Consider again the question of characterizing the sample complexity of a class H. Theorem 2 shows that the sample complexity of a class H is characterized by the sequence of sample complexity H (m). A better characterization would be an idea of a dimension that assigns a single number, dim (H), which controls the growth of H (m), and consequently the sample complexity of H (m). To achieve a plausible generalization, let us return for a moment to binary classification and examine the relationships between the VC dimension and the sequence of H (m). It is not difficult to see that the VC dimension of H is the maximum number d, which is H (d) = d.Furthermore, a nice result from Haussler et al. (1988) shows that \u2022 If we are Y | = 2, then VCdim (H)."}, {"heading": "Appendix A. Agnostic learning and further directions", "text": "In this thesis, we focused on learning in the feasible setting. Therefore, for general hypotheses classes, it remains an open question to find an optimal algorithm for the agnostic setting. However, for linear classes, our upper limits are achieved by compression schemes. Therefore, as stated in Theorem 18, our results can be extended to the agnostic setting, resulting in algorithms for the ranges H and H, R, whose sample complexity is O (d / E) + Log (1 / E) + Log (1 / E) + Log (1 / E) + Log (1 / E) + Log (1 / E) resp. 2. We find that these upper limits are optimal, up to the factors Log (d / E) and Log (1 / E) + Log (1 / E) + Log (1) + Log (E) + Log (E) + Log (E)."}, {"heading": "Appendix B. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. The Natarajan and Graph Dimensions", "text": "We remember two of the most important generalizations of the VC dimension to the multi-class hypothesis. (Definition 14 (graph dimension) Let H (Y) x be a hypothesis class. (Definition 14 (graph dimension) Let H (Y) x be a hypothesis class. (Definition 14 (graph dimension) Let H (A) x be a hypothesis class. We say that A (X) X (x) G-class is disjointed if there is h (x). (Definition 14 (graph dimension) Let H be with h (A) x-class. (X) The graph dimension of H is the maximum cardinality of a G-disjointed class. (The following theory shows that the graph dimension essentially characterizes the ERM sample complexity.) Theorem 15 (Daniely et al. (2011) The graph dimension of class x is a hypothesis."}, {"heading": "B.2. Compression Schemes", "text": "A compression scheme of size d for a class H is a pair of functions: Comm: 1 \u00b2 m = d (X \u00b7 Y) m \u2192 (X \u00b7 Y) d and DeCom: (X \u00b7 Y) d \u2192 YX, with the property that for each realizable sample S = (x1, y1),.., (xm, ym) applies that if h = DeCom \u00b2 Com (S) then 1 \u2264 i \u2264 m, yi = h (xi).Each compression scheme yields a learning algorithm, namely DeCom \u00b2 Com. It is known that the sample complexity of this algorithm is limited upwards by the size of the compression scheme. To be precise, we have: Theorem 18 (Littlestone and Warmuth (1986) Suppose that there is a compression scheme of size d for a class H."}, {"heading": "Appendix C. The Cantor classes", "text": "The first Cantor class follows directly from Lemma 20. The second Cantor class follows directly from Lemma 20. The second Cantor class follows directly from Lemma 20. The first Cantor class follows directly from Lemma 20. The second Cantor class follows directly from Lemma 20. The first Cantor class follows directly from Lemma 20. The second Cantor class is the second Cantor class. The second Cantor class is the second Cantor class. The second Cantor class is the first Cantor class. The second Cantor class is the second Cantor class. The second Cantor class is the second Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor-Cantor"}, {"heading": "Appendix D. Proofs", "text": "D.1. Some lemmas and additional notationsLet X, \"Y\" be another instance and label spaces. Let \": X\" \u00b7 X \"\u00b7 X\" \u00b7 X \"\u00b7 X\" \u00b7 H. \"If\" X \"is the identity function, we simplify the above notation to\" H \"(or\" H \"). We say that a hypothesis of class\" H \"(Y\") \"X\" (H \")\" X \"(Y\") \"X\" (H \")\" X \"(H\") is feasible if \"H\" (Y \"),\" H \"(Y\"), \"H\" (H \"), the different terms of example complexity in relation to\" H \"are never greater than the corresponding terms in relation to\" Let \"H\" (Y \"), H\" (Y \"), H\" (Y \"), H\" (Y \"), H\" (Y \"), H\" (\")."}, {"heading": "D.2. Proof of Theorem 1", "text": "We will apply the following version of Chernoff's theories: \"We will apply the theories of Chernoff's theories.\" \"We will apply the theories of Chernoff.\" \"We will apply the theories of Chernoff.\" \"We will apply the theories of Chernoff.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\". \"\" We. \"\" \"We.\". \"We.\" \"\" We. \".\" \"We.\" \"\" We. \".\" We. \"\". \"We.\" \"\" We. \".\" We. \"\" \"We.\". \"We..\" We.. \"We.\" \"We..\" We. \"\" We.. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\". \"We.\" \"We..\" We. \".\" We.. \"\" \"\" We. \".\" We. \".\" We. \".\" We. \".\" We. \".\" We. \".\" We. \".\". \"We.\" We. \".\" \".\" We. \"We.\". \"We.\" We. \"We.\". \".\" We. \"We.\" We. \".\". \"We.\" \"We.\" We. \"We.\". \"\" \"\" We. \"We.\" We. \"We.\" \"We.\" We. \".\" We. \"We..\" We. \"\" We.. \"We..\" We. \"\" We.. \"\" We. \"We.\" We. \"We..\" We. \"We..\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We..\" We. \"We..\" We. \"We.\" We. \"We.\" We. \""}, {"heading": "D.3. Proof of Lemma 3", "text": "We use the induction on the number of vertices. Assuming d (G) \u2264 d. Therefore, there is v0 \u0445 V with d (v0) \u2264 d. Let us leave G \u2032 = (V \u2032, E \u2032) = G [V\\ {v0}]. Through the induction hypothesis there is an orientation h \u2032: E \u2032 \u2192 V \u2032 with maximum out degree d. We define an orientation h: E \u2192 V byh (e) = {v e = {v0, v} h \u2032 (e\\ {v0}) otherThe problem extends to the case where Y is infinite by a standard application of the compactness theorem."}, {"heading": "D.4. Proof of theorem 4", "text": "Suppose we run A on mA, H (2, 2) examples to obtain a hypothesis and project h (x) on a new example. < M (1, 2) error probability if we apply the one hypothesis to m (2, 2). < M (2, 2) error probability if we apply the one hypothesis to m (2, 2), it follows that the hypothesis that returns it is a new example. < M (m) m (m) m (2) m (2) m (2).M (2) error probability that the returned hypothesis is."}, {"heading": "D.7. Proof of Theorem 6", "text": "In order to prove the first part of Theorem 6, we will again rely on Theorem 18. (...) We will show a compression scheme of magnitude O (R) based on multilateral perception. (...) We will only briefly review it. (...) We will remember that multicultural perception relies on a number of examples that are feasible. (...) At each step, it receives an instance and tries to project its label onto the observed past. (...) The two crucial properties of the preceptron that we will rely on are the following: \u2022 If the perctrons depend on a sequence of examples that is feasible, then it makes most O (R) errors. (...) The predictions of the perceptron algorithms are influenced only by previous erroneous predictions."}, {"heading": "D.8. Proof of Theorem 9", "text": "The first part of the theorem follows directly from the first part of theorem 6. We move on to the second part. First, we note that Hd, t, 2, R of Hd, t, R of Hd, t, R of Hd, t, R of [2] q of Lemma 23 and Lemma 21 of Hd, t, 2, R of Hd, t, R. Also the label space will be of Hd, t, R of Hd, t of [2] q of [2] q. To simplify the notations, it is sufficient to show that a fragmented union of the copies of HY, Cantor, with | Y of Hd, t, R of d, R of d (t + 1) R. of Lemma 22, is sufficient to show that for any universal constant C > 0, HY, Cantor of HY, with | Y can be realized."}, {"heading": "D.9. Proof of Theorem 8", "text": "The first part of the theorem follows directly from the first part of theorem 5. We move on to the second part. First, it is sufficient to limit ourselves to the case q = 2."}, {"heading": "D.10. Proof of Theorem 27", "text": "Theorem 27 for each x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": "Wiley-Interscience, second edition,", "citeRegEx": "Alon and Spencer.,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2000}, {"title": "Characterizations of learnability for classes", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long"], "venue": "n}-valued functions. Journal of Computer and System Sciences,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Discriminative reranking for natural language parsing", "author": ["M. Collins"], "venue": "In Machine Learning,", "citeRegEx": "Collins.,? \\Q2000\\E", "shortCiteRegEx": "Collins.", "year": 2000}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods", "author": ["Michael Collins"], "venue": "In New developments in parsing technology,", "citeRegEx": "Collins.,? \\Q2005\\E", "shortCiteRegEx": "Collins.", "year": 2005}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer.,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "multiclass learning approaches: A theoretical comparision with implications", "author": ["A. Daniely", "S. Sabato", "S. Shalev-Shwartz"], "venue": "In NIPS,", "citeRegEx": "Daniely et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2012}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Predicting {0, 1}-functions on randomly drawn points", "author": ["David Haussler", "Nick Littlestone", "Manfred K. Warmuth"], "venue": "In FOCS,", "citeRegEx": "Haussler et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1988}, {"title": "Phoneme alignment based on discriminative learning", "author": ["J. Keshet", "S. Shalev-Shwartz", "Y. Singer", "D. Chazan"], "venue": "In Interspeech,", "citeRegEx": "Keshet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Relating data compression and learnability", "author": ["N. Littlestone", "M. Warmuth"], "venue": "Unpublished manuscript,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1986\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1986}, {"title": "Lectures on discrete geometry, volume 212", "author": ["J. Matousek"], "venue": null, "citeRegEx": "Matousek.,? \\Q2002\\E", "shortCiteRegEx": "Matousek.", "year": 2002}, {"title": "On learning sets and functions", "author": ["B.K. Natarajan"], "venue": "Mach. Learn.,", "citeRegEx": "Natarajan.,? \\Q1989\\E", "shortCiteRegEx": "Natarajan.", "year": 1989}, {"title": "Shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds", "author": ["Benjamin I Rubinstein", "Peter L Bartlett", "J Hyam Rubinstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rubinstein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2006}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2010}, {"title": "Let H \u2282 YX be a hypothesis class with the following property: There is a label \u2217 \u2208 Y such that, for every f \u2208 H and x \u2208 X , either f(x) = \u2217 or f is the only function in H whose value at x is f(x)", "author": ["Daniely"], "venue": "Multiclass Learning Lemma", "citeRegEx": "Daniely,? \\Q2011\\E", "shortCiteRegEx": "Daniely", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "This brings back the fundmamental question of \u201chow to learn\u201d? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al. (2006) showing that its sample complexity is essentially optimal.", "startOffset": 173, "endOffset": 198}, {"referenceID": 2, "context": "Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).", "startOffset": 170, "endOffset": 185}, {"referenceID": 14, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 15, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 4, "context": "(Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)).", "startOffset": 0, "endOffset": 88}, {"referenceID": 10, "context": "(Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)).", "startOffset": 0, "endOffset": 88}, {"referenceID": 11, "context": "Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others.", "startOffset": 166, "endOffset": 260}, {"referenceID": 8, "context": ", 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others.", "startOffset": 39, "endOffset": 68}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity.", "startOffset": 112, "endOffset": 1714}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs.", "startOffset": 112, "endOffset": 1934}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al.", "startOffset": 112, "endOffset": 2974}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al. (1988), see also Simon and Sz\u00f6r\u00e9nyi (2010)), showing that it is optimal up to a constant factor of 2 in a transductive setting.", "startOffset": 112, "endOffset": 3006}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al. (1988), see also Simon and Sz\u00f6r\u00e9nyi (2010)), showing that it is optimal up to a constant factor of 2 in a transductive setting.", "startOffset": 112, "endOffset": 3042}, {"referenceID": 5, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al.", "startOffset": 146, "endOffset": 172}, {"referenceID": 11, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), and others.", "startOffset": 203, "endOffset": 297}, {"referenceID": 2, "context": "Departing general theory, we turn our focus to investigate hypothesis classes that are used in practice, in light of the above results and the result of Daniely et al. (2011). We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al.", "startOffset": 153, "endOffset": 175}, {"referenceID": 2, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), and others. Arguably, the two most natural questions in this context are: (i) is the ERM rule still sub-optimal even for such classes? and (ii) If yes, are there efficient optimal learnears for these classes? Regarding the first question, we show that even though the sample complexity of these classes is upper bounded in terms of the dimension or the margin, there are sub-optimal ERMs whose sample complexity has additional multiplicative factor that depends on the number of labels. This settles in negative an open question due to Collins (2005). Regarding the second question above, as opposed to the one-inclusion algorithm, which is in general inefficient, for linear classes we derive computationally efficient learners (provided that the hypotheses can be evaluated efficiently), that enjoy optimal sample complexity.", "startOffset": 204, "endOffset": 850}, {"referenceID": 6, "context": "This class is due to Daniely et al. (2011) and we call it the first Cantor class due to the resemblance to the construction used for proving the famous theorem of Cantor from set theory (e.", "startOffset": 21, "endOffset": 43}, {"referenceID": 6, "context": "This class is due to Daniely et al. (2011) and we call it the first Cantor class due to the resemblance to the construction used for proving the famous theorem of Cantor from set theory (e.g., http://en.wikipedia.org/wiki/Cantor\u2019s_theorem). Daniely et al. (2011) employed this class to establish gaps between the sample complexity of different ERM learners.", "startOffset": 21, "endOffset": 263}, {"referenceID": 6, "context": "Since Hd \u2282 HXd,Cantor, we can apply the \u201cgood\u201d ERM learner described in Daniely et al. (2011) with respect to the class HXd,Cantor and obtain an algorithm for Hd whose sample complexity is \u2264 ln(1/\u03b4) \u01eb .", "startOffset": 72, "endOffset": 94}, {"referenceID": 15, "context": "The one-inclusion algorithm We next describe the one-inclusion transductive learning algorithm of Rubinstein et al. (2006). Let S = {x1, .", "startOffset": 98, "endOffset": 123}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al. (2006), the proof of the lower bound relies on a new argument.", "startOffset": 85, "endOffset": 137}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al. (2006), the proof of the lower bound relies on a new argument. As opposed to Rubinstein et al. (2006) who lower bounded \u01ebH(m) using the Natarajan dimension, we give a direct analysis.", "startOffset": 85, "endOffset": 232}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al.", "startOffset": 105, "endOffset": 131}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al.", "startOffset": 132, "endOffset": 177}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al.", "startOffset": 132, "endOffset": 199}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that, rather surprisingly, even for such simple classes, there can be gaps between the ERM sample complexity and the PAC sample complexity.", "startOffset": 132, "endOffset": 229}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that, rather surprisingly, even for such simple classes, there can be gaps between the ERM sample complexity and the PAC sample complexity. This settles in negative an open question raised by Collins (2005). We also derive computationally efficient optimal learners for linear classes, based on the concept of compression schemes.", "startOffset": 132, "endOffset": 445}, {"referenceID": 13, "context": "Taskar et al. (2003)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Taskar et al. (2003)). For example, in speech recognition, the label space might me the collection of all sequences of \u2264 20 English words. To motivate the definition, consider the case that we are to recognize a t-letter word appearing in an image. Let q be the size of the alphabet. The set of possible labels is naturally associated with [q]t. A popular method to tackle this task (see for example Taskar et al. (2003)) is the following: The image is broken into t parts, each of which contains a single letter.", "startOffset": 0, "endOffset": 421}, {"referenceID": 2, "context": "Even though the number of labels is exponential in t, this class (in the realizable case) can be learnt in time polynomial in d, t and q (see Collins (2005)).", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Daniely et al. (2011)) that the sample complexity of every ERM for this class is O (", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Collins (2005)) that the sample complexity of every ERM for this class is O (", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "However, it was not known whether the gap is real: In (Collins, 2005), it was left as an open question to show whether the perceptron\u2019s bound holds for every ERM.", "startOffset": 54, "endOffset": 69}, {"referenceID": 6, "context": "This strengthens the result of (Daniely et al., 2011) who showed that it is bounded by O(d log(d)).", "startOffset": 31, "endOffset": 53}, {"referenceID": 6, "context": "This strengthens the result of (Daniely et al., 2011) who showed that it is bounded by O(d log(d)). It is known (e.g. Daniely et al. (2012)) that for the multivector construction (example 1), in which the dimension of the range of \u03a8 is dk, the 1.", "startOffset": 32, "endOffset": 140}, {"referenceID": 6, "context": "These theorems show that the phenomenon of gaps between different ERMs, as reported in (Daniely et al., 2011), happens also in hypothesis classes that are used in practice.", "startOffset": 87, "endOffset": 109}, {"referenceID": 2, "context": "For the case of margin-based classes, this result is not new \u2014 an efficient algorithm based on the multiclass perceptron has been proposed in Collins (2002). For completeness, we briefly survey this approach in the appendix.", "startOffset": 142, "endOffset": 157}, {"referenceID": 9, "context": "Moreover, a beautiful result of Haussler et al. (1988) shows that \u2022 If |Y| = 2, then VCdim(H) \u2264 \u03bcH(m) \u2264 2VCdim(H) for every m \u2265 VCdim(H).", "startOffset": 32, "endOffset": 55}, {"referenceID": 6, "context": "First, it is known (Daniely et al., 2011) that the graph dimension does not characterize the sample complexity, since it can be substantially larger than the sample complexity in several cases.", "startOffset": 19, "endOffset": 41}, {"referenceID": 12, "context": "By combination of theorems 2 and Rubinstein et al. (2006), a weaker version of conjecture 11 is true.", "startOffset": 33, "endOffset": 58}, {"referenceID": 6, "context": "Theorem 15 (Daniely et al. (2011)) For every hypothesis class H with graph dimension d, \u03a9 ( d+ log(1/\u03b4) \u01eb )", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "Definition 16 (Natarajan dimension) Let H \u2286 (Y \u222a {\u2296}) be a hypothesis class. We say that A \u2286 X is N -shattered if there exist h1, h2 : A \u2192 Y such that \u2200x \u2208 A, h1(x) 6= h2(x) and for every B \u2286 A there is h \u2208 H for which \u2200x \u2208 B, h(x) = h1(x) while \u2200x \u2208 A \\B, h(x) = h2(x) . The Natarajan dimension of H, denoted Ndim(H), is the maximal cardinality of an N shattered set. Theorem 17 (essentially Natarajan (1989)) For every hypothesis class H \u2282 (Y \u222a {\u2296}) with Natarajan dimension d, \u03a9 ( d+ log(1/\u03b4) \u01eb )", "startOffset": 15, "endOffset": 410}, {"referenceID": 6, "context": "(3) We also note that (Daniely et al., 2011) conjectured that the logarithmic factor of |Y| in Theorem 17 can be eliminated (maybe with the expense of poly-logarithmic factors of 1\u01eb , 1 \u03b4 and Ndim(H)).", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "We note that the upper bound in the last theorem follows from theorem 15 and the fact that (see Ben-David et al. (1995)) for every hypothesis class H, Gdim(H) \u2264 5 log(|Y|)Ndim(H) .", "startOffset": 96, "endOffset": 120}, {"referenceID": 12, "context": "Precisely, we have: Theorem 18 (Littlestone and Warmuth (1986)) Suppose that there exists a compression scheme of size d for a class H.", "startOffset": 32, "endOffset": 63}, {"referenceID": 6, "context": "Lemma 19 (Daniely et al. (2011)) \u2022 The graph dimension of HX ,Cantor is |X |.", "startOffset": 10, "endOffset": 32}, {"referenceID": 6, "context": "Lemma 20 (essentially Daniely et al. (2011)) Let H \u2282 YX be a hypothesis class with the following property: There is a label \u2217 \u2208 Y such that, for every f \u2208 H and x \u2208 X , either f(x) = \u2217 or f is the only function in H whose value at x is f(x).", "startOffset": 22, "endOffset": 44}, {"referenceID": 0, "context": "Alon and Spencer (2000)).", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "Matousek (2002), chapter 13).", "startOffset": 0, "endOffset": 16}], "year": 2014, "abstractText": "The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of \u201chow to learn\u201d? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al. (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).", "creator": "LaTeX with hyperref package"}}}