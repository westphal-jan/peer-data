{"id": "1405.2652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning", "abstract": "We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, the agent only has several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting and more importantly generalize to the case where the models given to the learner do not contain a true model giving an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.", "histories": [["v1", "Mon, 12 May 2014 07:45:54 GMT  (77kb)", "https://arxiv.org/abs/1405.2652v1", null], ["v2", "Wed, 14 May 2014 12:43:36 GMT  (43kb)", "http://arxiv.org/abs/1405.2652v2", null], ["v3", "Wed, 9 Jul 2014 14:40:20 GMT  (64kb)", "http://arxiv.org/abs/1405.2652v3", null], ["v4", "Mon, 21 Jul 2014 11:52:37 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v4", null], ["v5", "Tue, 12 Aug 2014 12:19:55 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v5", null], ["v6", "Mon, 15 Sep 2014 08:32:45 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v6", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ronald ortner", "odalric-ambrym maillard", "daniil ryabko"], "accepted": false, "id": "1405.2652"}, "pdf": {"name": "1405.2652.pdf", "metadata": {"source": "CRF", "title": "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning", "authors": ["Ronald Ortner", "Odalric-Ambrym Maillard", "Daniil Ryabko"], "emails": ["rortner@unileoben.ac.at,", "odalric-ambrym.maillard@ens-cachan.org,", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.26 52v6 [cs.LG] 1 5SE p"}, {"heading": "1 Introduction", "text": "Inspired by [3], a learning environment has been introduced in [6] in which the learner does not have explicit information about the state space of the underlying Markov decision-making process (MDP). Instead, he has a series of models that map the story (i.e. observations, actions chosen, and rewards collected) to states. However, only some models give a correct MDP representation, and the first limits of regret in this context are derived in [6], which have recently been improved in [7] and extended to infinite model sets in [8]. At this point, we expand and improve the results of [7] as follows: First, we no longer assume that the model given to the learner contains a true model that leads to an MDP representation; instead, models will only approximate to an MDP. Second, we improve the limits of [7] in terms of dependence on the state space and the associated work on state representations (such as MDP)."}, {"heading": "1.1 Setting", "text": "For each time step t = 1, 2,., let Ht: = O \u00b7 (A \u00b7 R \u00b7 O) reward is any number of stories up to a certain time t, where O is the amount of observations, a finite amount of factors, and R = [0, 1] the amount of possible rewards. We consider the following affirmation problems: The learner receives some initial observations h1 = o1 \u00b2 H1 = O. Then the learner at any time chooses an action based on the current story ht-Ht and receives an immediate reward rt and the next observation ot + 1 from the unknown environment. Thus ht + 1 is the concatenation of ht with ht (at, rt + 1). State representation models. A state representation model is a function of the series of stories H = t. A certain role is played by state representation models that bring about a decision to Markov's decision."}, {"heading": "2 Preliminaries: MDP Approximations", "text": "Before we give the exact concept of approximation, we will first point out that in our determination of the transition probabilities p (h) | h, a) for all two probabilities p (h), h (h) and an action a (a) are well defined. Then, an arbitrary model DP (h) and a state s (p) are shown that we use the aggregated transition probabilities pagg (s), a), h (h) and s (h) p (h) | h, a). Note that the true transition probabilities under DP (s) are then followed by p (s), a), p (s), a), pagg (s), b), b), b), b (h), b) and c), b), b), b), b), b), and c), b)."}, {"heading": "3 Algorithm", "text": "The OAMS algorithm (shown in detail as algorithm 1) is a generalization of the OMS algorithm of [7]. Applying the original OMS algorithm to our setting would not work, since OMS obviously compares the collected rewards of each model with the reward it would receive if the model did not give Markov sufficiently high rewards. In our case, there may not be a Markov model in the series of given models. So the main difference to OMS is that OAMS gets estimates for each model and takes into account the possible approximation error in relation to an approximate Markov model."}, {"heading": "4 Regret Bounds", "text": "Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 4. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. Theorem 3. 3. Theorem 3. 3. 3. Theorem 3. 3. 3. Theorem 3. 3. 3. 3. Theorem 3. 3. 3. 3. 3. 3. Theorem 3. 3. 3. 3."}, {"heading": "5 Proof of Theorem 3", "text": "The proof is divided into three parts and follows the lines of [7], taking into account the necessary changes to deal with the approximation error. First, in Section 5.1 we deal with the error of approximative approximation. Then, in Section 5.2 we show that all models of state representation \u03c6, which represent an approximation to a Markov model, pass the test in (12) on the rewards collected so far with high probability, provided that the estimate is used in Section 5.3 to deduce the limit of regret according to Theorem 3."}, {"heading": "5.1 Error Bounds for \u01eb-Approximate Models", "text": "We begin with some observations about the empirical rewards and transition probabilities that our algorithm calculates and uses for each model. While the estimated rewards and transition probabilities used by the algorithm generally do not correspond to any underlying true values, the expected values of r and p are still well defined as the story h and p is so far. Let us consider some h and h if the action a is after the story h1, h2, h2, h2 and an action a, and assume that the estimates r and p, a and p, a and p, a and p, a and p, and p, and p, are calculated when the action a is after the story h1, h2, h2 and., hn and H are mapped to the same state s. (In the following, we will describe the states of an estimate r and i, the states of an estimate n."}, {"heading": "5.3 Preliminaries for the proof of Theorem 3", "text": "We begin with some auxiliary results for the proof of theorem 3. Lemma 5 limits the bias of optimistic politics, Lemma 6 deals with the estimated precision of \u03c6kj, and Lemma 7 provides a limit on the number of episodes. For evidence see Appendix E, F and G. Lemma 5. Let us assume that the confidence intervals given in Lemma 1 at some step t for all states as well as for all actions a. Then the set of plausible MDPs Mt, \u03c6 contains an MDP M with a diameter D (M) above the true diameter D, provided that the respective bias range (u + t) is also limited by D. Lemma 6. If all selected models pass all tests in course j of episode k, then the respective bias is limited by D."}, {"heading": "5.4 Bounding the regret (Proof of Theorem 3)", "text": "We assume that all selected models will pass all tests in succession small small if they have been selected in succession small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small small"}, {"heading": "1. Bartlett, P.L., Tewari, A.: REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In: UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25\u201342. AUAI Press (2009)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. Hallak, A., Castro, D.D., Mannor, S.: Model selection in Markovian processes. In: 19th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, KDD", "text": "2013, pp. 374-382. ACM (2013) 3. Hutter, M.: Feature Reinforcement Learning: Part I: Unstructured MDPs. J. Artificial General Intelligence 1, 3-24 (2009) 4. Littman, M, Sutton, R., Singh S.: Predictive representations of state. In: Adv. Neural Inf. Process. Syst. 15, pp. 1555-1561 (2002) 5. Jaksch, T., Ortner, R., Auer, P.: Near-optimal regret bounds for reinforcement learning. J. Mach. Learn. Res. 11, 1563-1600 (2010) 6. Maillard, O.A., Munos, R., Ryabko, D.: Selecting the state-representation in reinforcement learning. In: Adv. Neural Inf. Inf. Mc. Process. Syst. 24, pp."}, {"heading": "A Proof of Theorem 1", "text": "We begin with an error that amounts to an approximation, provided we compare two MDPs about the same state scope of action. (Tagm 8. Let us consider a communicating MDP M = (S, A, R, P) and another MDP M = (S, A, R, P) about the same state scope of action that is an approximation to M (for ID). Let us assume that an optimal policy is implemented from M (s) to M (s). (D + 1) + D (s) is the number of times in which the state s is visited under these steps. (Thens) (M) \u2212 s (S) \u00b7 r (s). (s) < (s) + D + 1) + D (s). (1 / D) + D (s) with probability at least 1 (s)."}, {"heading": "B Proof of Lemma 1", "text": "For any number of observations n applies that (cf. Appendix C.1 of [5]) for each \u03b8 > 0P {r (s, a) \u2212 E [r (s, a)] \u2264 \u221a log (2 / \u03b8) n} < \u03b8, P {s, a) \u2212 E [p, a) [s, a)]."}, {"heading": "C Proof of Lemma 2", "text": "We have already seen in (14) and (18) that M [r] is an alignment of the true MDP M, so that according to Theorem 1 the confidence intervals of Lemma 1 apply to all pairs of state shareholders, so that M) is included in the set of plausible MDPs Mt (D (M) + 1). (43) It remains to deal with the difference between B (M) and B (S). Assuming that the confidence intervals of Lemma 1 apply to all pairs of state shareholders, so that M) is included in the series of plausible MDPs Mt, E (defined by the empirical rewards and transition probabilities r (S) and P (S), it follows together with (8) the set of plausible MDPs Mt, D (defined by the empirical rewards and transition probabilities r (S) and P (S)."}, {"heading": "D Proof of Lemma 4", "text": "Assuming that t \"is the last step in this run and is the total number of steps in this run, the test will be passed in all steps of the run with probability of error, which will be achieved by (using this 2t\" \u2265 2j \") 6 t\" 2 \"(t\" + \") = 2\" t \"(t\" + \") = 2\" t \"(t\" + \") = 2\" t \"(t\" + \") = 2\" t \"(t\" + \") = 2\" t \"(t\" + \") = 2\" t \"(t\") = 2 \"t\" (t \") = 2\" t \"2\" (t \"2\") = 2 \"t\" (t \") = 2.\" The summary of all episodes and runs shows that the test will be passed in all time steps with a probability of at least 1 \"2\" (g \") 1\" (g \")."}, {"heading": "E Proof of Lemma 5", "text": "We define an MDP M on the state area S\u03c6 as follows: First, \u03b2: S\u03c6 \u2192 S is an arbitrary reference state, which is assigned to a state in S in S\u03c6, so that \u03b1 (\u03b2 (s) = s, a). Intuitively, \u03b2 (s) is an arbitrary reference state, which is assigned to a state in s (s) = s (s), a). (45) Then we obtain the transition probabilities of M (s), a) \u2212 p (s), a), section 1 = s (s), p (s), p (s), a) through (18) and Lemma 1."}, {"heading": "F Proof of Lemma 6", "text": "According to the definition of the algorithm, the initial value of each model \u03c6 has the value \u03c6 0 and is doubled if \u03c6 does not pass a test. Thus, if it is assumed that the value of \u03c6 (\u03c6) will no longer change, and that consequently the value of \u03c6 (\u03c6) < 2\u044b (\u03c6) always changes as soon as ds (\u03c6). On the other hand, the value of ds (\u03c6) < 2\u0445 (\u03c6) always remains the same. However, if the initial value of ds (\u03c6) < 2\u0445 (\u03c6) then also applies vice versa, then the value of ds (\u03c6) \u2265 (\u03c6) applies to the initial value b (\u03c6) = b 0 and again that the assumption D (\u03c6) = b 0 remains unchanged, so that the value of ds (\u03c6) \u2264 g 0 applies."}, {"heading": "G Proof of Lemma 7", "text": "First, remember that an episode ends when either the number of visits to a state action pair (s, a) has been doubled (line 12 of the algorithm) or when the test on the collected rewards has failed (line 9). By adoption, the test is passed, provided that the number of visits to a state action pair (s, a) has been doubled (line 12 of the algorithm). If the number of visits to a state action pair (s, a) has been doubled (line 12 of the algorithm), then the test is trivial (line 9). Otherwise, the test only fails log2 (line 9) times until the number of visits to a state action pair (s, a) is doubled."}, {"heading": "H Proof of Theorem 4", "text": "Since the proof of regret for OMS given in [7] follows the same lines as the proof for theorem 3 given here, we will only sketch the key step leading to the improvement of the limit. Note that by (36) and since the average rewards from the assumption in [0, 1] apply to the model chosen in any run j of an episode k, the pin (\u03c6kj, tkj) \u2264 pin (\u03c6, tkj) \u2212 1. It follows that the chosen model is always satisfactory according to the definition of penalty expression (9) and since 1 \u2264 (u + t, \u03c6) \u2264 D (2 \u221a 2Skj + 3 \u221a 2). SkjA protocol (48SkjAt3 \u043c) + 2 \u221a 2 log (24t2kj \u043c) \u2264 (2D \u2212 2S \u0445 + 3 \u0445 2)."}], "references": [{"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "UAI 2009, Proc. 25th Conf. on Uncertainty in Artificial Intelligence, pp. 25\u201342. AUAI Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Model selection in Markovian processes", "author": ["A. Hallak", "D.D. Castro", "S. Mannor"], "venue": "19th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, KDD 2013, pp. 374\u2013382. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature Reinforcement Learning: Part I: Unstructured MDPs", "author": ["M. Hutter"], "venue": "J. Artificial General Intelligence 1, 3\u201324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive representations of state", "author": ["M Littman", "R. Sutton", "Singh S."], "venue": "Adv. Neural Inf. Process. Syst. 15, pp. 1555\u20131561", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "J. Mach. Learn. Res. 11, 1563\u20131600", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["O.A. Maillard", "R. Munos", "D. Ryabko"], "venue": "Adv. Neural Inf. Process. Syst. 24, pp. 2627\u20132635", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimal regret bounds for selecting the state representation in reinforcement learning", "author": ["O.A. Maillard", "P. Nguyen", "R. Ortner", "D. Ryabko"], "venue": "Proc. 30th Int\u2019l Conf. on Machine Learning, ICML 2013, JMLR Proc., vol. 28, pp. 543\u2013551", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Competing with an infinite set of models in reinforcement learning", "author": ["P. Nguyen", "O.A. Maillard", "D. Ryabko", "R. Ortner"], "venue": "Proc. 16th Int\u2019l Conf. on Artificial Intelligence and Statistics, AISTATS 2013, JMLR Proc., vol. 31, pp. 463\u2013471", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Pseudometrics for state aggregation in average reward Markov decision processes", "author": ["R. Ortner"], "venue": "ALT 2007. LNCS (LNAI), vol. 4754, pp. 373\u2013387. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning, In: Adv", "author": ["R. Ortner", "D. Ryabko"], "venue": "Neural Inf. Process. Syst. 25, pp. 1772\u20131780", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP).", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "Inspired by [3], in [6] a reinforcement learning setting has been introduced where the learner does not have explicit information about the state space of the underlying Markov decision process (MDP).", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "The first regret bounds in this setting were derived in [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "They recently have been improved in [7] and extended to infinite model sets in [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "They recently have been improved in [7] and extended to infinite model sets in [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "Here we extend and improve the results of [7] as follows.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Second, we improve the bounds of [7] with respect to the dependence on the state space.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 6, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 7, "context": "For discussion of potential applications and related work on learning state representations in POMDPs (like predictive state representations [4]), we refer to [6\u20138].", "startOffset": 159, "endOffset": 164}, {"referenceID": 1, "context": "Here we only would like to mention the recent work [2] that considers a similar setting, however is mainly interested in the question whether the true model will be identified in the long run, a question we think is subordinate to that of minimizing the regret, which means fast learning of optimal behavior.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "actions, and R = [0, 1] the set of possible rewards.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In the following we assume that rewards are bounded in [0, 1], which implies that span(\u03bb) is upper bounded by D, cf.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "[5, 1].", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[5, 1].", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[5, 1, 6], as \u2206(\u03c6\u25e6, T ) := T\u03c1\u2217(\u03c6\u25e6)\u2212Tt=1 rt , where rt are the rewards received when following the proposed strategy and \u03c1\u2217(\u03c6\u25e6) is the average optimal reward in \u03c6\u25e6, i.", "startOffset": 0, "endOffset": 9}, {"referenceID": 8, "context": "It generalizes bounds of [9] from ergodic to communicating MDPs.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "This is an improvement over the results in [9], which only showed that the error approaches 1 when the diameter goes to infinity.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "The OAMS algorithm (shown in detail as Algorithm 1) we propose for the setting introduced in Section 1 is a generalization of the OMS algorithm of [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "This can be done by extended value iteration (EVI) [5].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[5])", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Similar to the REGAL algorithm of [1] this shall prefer simpler models (i.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "When the learner knows that \u03a6 contains a Markov model \u03c6\u25e6, the original OMS algorithm of [7] can be employed.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": ", S > D2|\u03a6|S\u25e6, we can improve on the state space dependence of the regret bound given in [7] as follows.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The proof (found in Appendix H) is a simple modification of the analysis in [7] that exploits that by (11) the selected models cannot have arbitrarily large state space.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "This could also give improvements over current regret bounds for continuous reinforcement learning as given in [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The proof is divided into three parts and follows the lines of [7], now taking into account the necessary modifications to deal with the approximation error.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": ", Lemma 10 of [5]) yields that with probability at least 1\u2212 \u03b4 24t\u20323", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "2 of [7], \u2211 k Jk \u2264 KT log2(2T/KT ), \u2211 k \u2211 j 2 j \u2264 2(T + KT ) and \u2211 k \u2211 j 2 j/2 \u2264 \u221a 2KT log2(2T/KT )(T +KT ), and we may conclude the proof applying Lemma 7 and some minor simplifications.", "startOffset": 5, "endOffset": 8}], "year": 2014, "abstractText": "We consider a reinforcement learning setting introduced in [6] where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, she has access to several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting, and more importantly generalize to the case where the models given to the learner do not contain a true model resulting in an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}