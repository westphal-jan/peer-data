{"id": "1405.3167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2014", "title": "Clustering, Hamming Embedding, Generalized LSH and the Max Norm", "abstract": "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by (Charikar 2002) and to the max-norm ball, and the differences between their symmetric and asymmetric versions.", "histories": [["v1", "Tue, 13 May 2014 14:36:59 GMT  (38kb)", "http://arxiv.org/abs/1405.3167v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur", "yury makarychev", "nathan srebro"], "accepted": false, "id": "1405.3167"}, "pdf": {"name": "1405.3167.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bneyshabur@ttic.edu", "yury@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.31 67v1 [cs.LG] 1 3M ay2 014Keywords: Clustering, Hamming Embedding, LSH, Max Norm"}, {"heading": "1 Introduction", "text": "Convex relaxations play an important role in designing efficient learning and recovery algorithms, as well as in statistical learning and online optimization. It is therefore desirable to understand the convex shell of hypotheses, to achieve tractable relaxation of these convex sheaths, and to understand the density of such relaxations. In this paper, we look at convex relaxations of two important problems, namely clustering and hammering embedding, and examine the convex sheaths of the corresponding hypotheses classes: cluster incidence matrices and similarities with a short hammering embedding. In Section 2, we formally introduce these classes and understand the relationship between them by showing how hammering embedding can be viewed as a generalization of clustering. In Section 3, we discuss their convex shell and their relationship to symmetric embedding (LSsymmetrization)."}, {"heading": "2 Clustering and Hamming Embedding", "text": "In this section, we present the problems of clustering and hamming embedding and offer a unified view of both problems, with hamming embedding seen as a direct generalization of clustering. In any case, our starting point is a predetermined similarity function sim: S \u00b7 S \u2192 [\u2212 1, + 1] over a (possibly infinite) set of objects S. \"Clustering,\" as we think here, is the problem of dividing the elements of S into disjointed clusters so that the elements in the same cluster are similar, while the elements in different clusters are not similar. \"Hamming embedding\" is the problem of embedding S in a certain hammer space, so that the similarity between objects is captured by the hamming distance between their mappings."}, {"heading": "2.1 Clustering", "text": "We represent a cluster identity for each element, where the meaning of the different identities is arbitrary. The alphabet could have a fixed finite cardinality if we want to have a cluster identity with a certain number of clusters. E.g. a binary alphabet corresponds to the standard diagram division into two clusters. If we can assume that the objectivity of the cluster identity could be infinitely countable (e.g. a binary alphabet corresponds to the standard diagram division into two clusters. If we can assume that the objectivity of the cluster identification is infinitely countable (e.g. a cluster = N), then we do not restrict cluster incidence."}, {"heading": "2.2 Hamming Embedding and Binary Matrix Factorization", "text": "In the problem of binary hamming embedding (also known as binary hashing), we want to find an association of each object x-S to binary string b (x), b (y), d (1), d (1), that the similarity between two binary hashes is approximated by the hamming distance between their images, and so such a hash is useful for calculating very quickly similarities between massive collections of objects. In addition, hash tables can be used to speed up the recovery of similar objects. Binary hamming embedding can be considered a generalization of clusters, as follows: For each position i = 1,..., d in the hash group, we can imagine bi (x) as cluster d."}, {"heading": "3 Locality Sensitive Hashing Schemes", "text": "If we move from a finite average of clusters with a fixed number of components, as in embedding hamming, to an endless average, we arrive at the term LSH as investigated by [6]. In a collection of objects, an alphabet and a similarity function sim: S \u00b7 S \u2192 [\u2212 1, 1] in such a way that for each x-S sim (x, x) = 1 we have a location-dependent hash scheme (LSH). (3) [6] We discuss similarity functions sim: S \u00d7 S \u2192 [0, 1] as a probability distribution on the family of cluster functions (hash functions) H = {h: S \u2192 \u0432} such that [6]: Eh \u0445H [x, y) = sim (x, y)."}, {"heading": "3.1 \u03b1-LSH", "text": "If we are willing to tolerate a fixed offset between our embedding and the desired similarity, we can instead demand that the requirement (3) be too high. < < (5) that a distribution over h that obeys (5) is referred to as an alpha LSH. We can now verify that, for h1,., that the drawn i.i.d. of an alpha LSH, and any x, y, y, y, y, S: E [sim (x, y) \u2212 d that the alpha LSH cannot exist. (6) The length of the LSH is required to achieve an exact approximation of a similarity function."}, {"heading": "3.2 Generalized \u03b1-LSH", "text": "In the following section we will see how to break through the barrier imposed by Claim 1 by allowing asymmetry, emphasizing additional performance asymmetry. However, before we do so, we will consider another attempt to loosen the definition of an \u03b1-LSH, motivated by the work of [5] and [1]: In order to decouple the shift from scaling \u03b1, we will allow another arbitrary shift to sim (i.e. to the diagonal of sim) in quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quanti"}, {"heading": "4 Asymmetry", "text": "To enable greater power, we now contact the asymmetric variants of clustering to which we provide a similar function as M, T, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "5 Convex Relaxations, \u03b1-LSH and Max-norm", "text": "We now turn to two questions that are really the same: Can we achieve a narrow convex relaxation of the set M (S, T), k of (asymmetric) cluster incidence functions, and can we characterize the values of \u03b1 for which we can obtain an \u03b1-ALSH for a certain similarity measurement? To simplify the notation, we will now fix S and T and use Mk to denote M (S, T), k."}, {"heading": "5.1 The Ratio Function", "text": "The closest possible convex relaxation of Mk is simply its convex-hull-Mk problem. (We assume that P 6 = NP, convMk is not polynmically tractable.) What we are asking here is whether it has a tractable narrow relaxation of convMk. (To measure the density of some convexer B-Mk, we need to measure its cluster ratio (B) = min {r | Z-r convexMk} = min {r / r convexMk}.That is how much we need to inflate Mk so that it contains Z-B. The supremum-Ratik ratio (B) is then the maximum rate of inflation between convexMk and B, i.e., so that the convexe-Mk-Ratif-Ratif ratio is the same."}, {"heading": "5.2 A Tight Convex Relaxation using the Max-Norm", "text": "Remember that the maximum norm (also known as the maximum norm) of a matrix is defined as follows: [18] However, the maximum norm for this matrix is not the maximum norm of the matrix. [18] The maximum norm is the maximum norm of the matrix. [18] The maximum norm is the maximum norm of the matrix. [17] Even if S and T are not finite, and therefore sim is not a finite matrix, the maximum norm can be defined as above, where U and V can now be thought of as mappings of S and T. [17] Even if S and T are not finite, sim (x, y) = < U (x), V (y) > and U (x).U (x).We define the centralized norm."}, {"heading": "5.3 Proofs", "text": "Let us now assume that we have a family of pairwise independent hash functions of the form in which each element has the same chance of being included in -1 or 1. We have the same chance of being included in -1. Let us now assume that we can be a family of pairwise independent hash functions. (Let us) that each element has the same chance of being included in -1 or 1. Let us be included in -2 different functions. (Let us) that we can be a family of pairwise independent hash functions of the form. (Let us now assume that each element has the same chance of being included in -1 or 1. Let us be included in -2 different functions. (Let us) that we can be a family of pairwise independent hash functions of the form. (Let us) that each element has the same chance of being included in -1 or 1. Let us have that we are included in -1. (Let us) that we can be a family of pairwise independent hash functions of the form."}, {"heading": "A Random Matrices", "text": "In this section, we will examine the location-sensitive hash schemes for these random vectors. We will generate a random n \u00b7 n positive semidefinitive matrix Z of precedence at most d by randomly selecting the n-dimensional vectors x (i) from the unity sphere and Zij = < x (i), x (j), x (j), x (j) >. Since we generate the data randomly and E [Zij] = 0, we do not expect any major changes by using the matrix thresholds. Therefore, our analysis is limited to the LSH, without a threshold, i.e. ltx = 0.Since we are based on Theorem 2, we already know that there is an arbitrary set of unity vectors x (1), x (n) and Zij = < x (i), x (j), x (j), x (j), x (j) > units that we are equal, there is a KR-ALH for the matrix that we are interested in just the random H for the Smmers."}], "references": [{"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["N. Alon", "A. Naor"], "venue": "SIAM Journal on Computing, 35(4):787\u2013803,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "On nonapproximability for quadratic programs", "author": ["Sanjeev Arora", "Eli Berger", "Hazan Elad", "Guy Kindler", "Muli Safra"], "venue": "In Foundations of Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A generalized maximum entropy approach to bregman co-clustering and matrix approximation", "author": ["A. Banerjee", "I. Dhillon", "J. Ghosh", "S. Merugu", "D. S D.S. Modha"], "venue": "SIGKDD, pages 509\u2013514,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Two new approaches to obtaining estimates in the danzergrunbaum problem", "author": ["L.V. Buchok"], "venue": "Mathematical Notes, 87(4):489\u2013496,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximizing quadratic programs: Extending grothendieck\u2019s inequality", "author": ["M. Charikar", "A. Wirth"], "venue": "In FOCS, pages 54\u201360,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsh-preserving functions and their applications", "author": ["F. Chierichetti", "R. Kumar"], "venue": "SODA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "\u00dcber zwei probleme bez\u00fcglich konvexer k\u00f6rper von p", "author": ["L Danzer", "B Gr\u00fcnbaum"], "venue": "erd\u00f6s und von vl klee. Mathematische Zeitschrift, 79(1):95\u201399,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1962}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures & Algorithms, 22(1):60\u201365,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "S.V. Mirrokni"], "venue": "In Proc. 20th SoCG, pages 253\u2013262,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Information-theoretic co-clustering", "author": ["I.S. Dhillon", "M. Subramanyam", "S.M. Dharmendra"], "venue": "SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xuo"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustering using max-norm constrained optimization", "author": ["A. Jalali", "N. Srebro"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Sur la constante de grothendieck", "author": ["J.L. Krivine"], "venue": "C. R. Acad. Sci. Paris Ser. A-B 284, pages 445\u2013446,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}, {"title": "The power of asymmetry in binary hashing", "author": ["B. Neyshabur", "P. Yadollahpour", "Y. Makarychev", "R. Salakhutdinov", "N. Srebro"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Rajeev Motwani Piotr Indyk"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "COLT,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 5, "context": "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "In section 3 we discuss their convex hull and its relationship to notion of Locality Sensitive Hashing (LSH) as studied by [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 9, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 6, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 10, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 0, "endOffset": 6}, {"referenceID": 14, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "Moving on from a finite average of clusterings, with a fixed number of components, as in hamming embedding, to an infinite average, we arrive at the notion of LSH as studied by [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Given a collection S of objects, an alphabet \u0393 and a similarity function sim : S \u00d7 S \u2192 [\u22121, 1] such that for any x \u2208 S we have sim(x, x) = 1,a locality sensitive hashing scheme (LSH) is a probability distribution on the family of clustering functions (hash functions) H = {h : S \u2192 \u0393} such that [6]: Eh\u2208H[\u03bah(x, y)] = sim(x, y).", "startOffset": 294, "endOffset": 297}, {"referenceID": 5, "context": "(3) [6] discuss similarity functions sim : S \u00d7 S \u2192 [0, 1] as so require Ph\u2208H[h(x) = h(y)] = sim(x, y).", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "(3) [6] discuss similarity functions sim : S \u00d7 S \u2192 [0, 1] as so require Ph\u2208H[h(x) = h(y)] = sim(x, y).", "startOffset": 51, "endOffset": 57}, {"referenceID": 5, "context": "The importance of an LSH, as an object in its own right as studied by [6], is that a hamming embedding can be obtained from an LSH by randomly generating a finite number of hash functions from the distribution over the family H.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Unfortunately, even the requirement (5) of an \u03b1-LSH is quite limiting and difficult to obey, as captured by the following theorem, which is based on lemmas 2 and 3 of [6]:", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "Using lemma 3 in [6], we can say that 1\u2212sim(x,y) \u03b1 can be isometrically embedded in the Hamming cube which means 1 \u2212 sim(x, y) can be embedded in Hamming cube with no distortion.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "As noted by [6] (and stated in claim 2), we can therefore unfortunately conclude that there is no \u03b1-LSH for several important similarity measures such as the Euclidian inner product, Overlap coefficient and Dice\u2019s coefficient.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "But before doing so, let us consider a different attempt at relaxing the definition of an \u03b1-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift \u03b8 from the scaling \u03b1, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "But before doing so, let us consider a different attempt at relaxing the definition of an \u03b1-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift \u03b8 from the scaling \u03b1, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "According to [5], if a matrix Z with unit diagonal is positive semidefinite, then there is a probability distribution over a family H of hash functions such that for any x 6= y: Eh\u2208H[h(x)h(y)] = Z(x, y) C logn .", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "Given two collections of objects S, T , which might or might not be identical, and an alphabet \u0393 , an asymmetric clustering (or co-clustering [11]) is specified by pair of mappings f : S \u2192 \u0393 and g : T \u2192 \u0393 and is captured by the asymmetric cluster incidence matrix \u03baf,g(x, y) where \u03baf,g(x, y) = 1 if f(x) = g(y) and \u03baf,g(x, y) = \u22121 otherwise.", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "In a recent work, [15] showed that even when S = T and the similarity function sim is a well-behaved symmetric similarity function, asymmetric binary embedding could be much more powerful in approximating the similarity, using shorter lengths d, both theoretically and empirically on data sets of interest.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "2 A Tight Convex Relaxation using the Max-Norm Recall that the max-norm (also known as the \u03b32 : l1 \u2192 l\u221e norm) of a matrix is defined as [18]: \u2016Z\u2016max = min UV \u22a4 max(\u2016U\u20162,\u221e, \u2016V \u20162,\u221e) where \u2016U\u20162,\u221e is the maximum l2 norm of rows of the matrix U .", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "The maxnorm is SDP representable and thus tractable [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Moreover, it follows from the result of [2] that there is no efficiently computable upper bound \u03b2 for \u03b1\u0302g such that \u03b2 log n \u2264 \u03b1\u0302g \u2264 \u03b2 (under a standard complexity assumption that NP 6\u2286 DTIME(nlog3 )).", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "(Krivine\u2019s lemma [14]) For any two sets of unit vectors {ui} and {vj} in a Hilbert space H, there are two sets of unit vectors {ui} and {v\u2032 j} in a Hilbert space H \u2032 such that for any ui and vj, sin(c\u3008ui, vj\u3009) = \u3008ui, v\u2032 j\u3009 where c = sinh\u22121(1).", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "A part of the proof is similar to [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "g\u2208MT,2 \u03bc(f, g) = \u03c0 2c \u2016sim\u2016max = KR\u2016sim\u2016max The inequality KG \u2264 K is known due to [1].", "startOffset": 82, "endOffset": 85}], "year": 2014, "abstractText": "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.", "creator": "LaTeX with hyperref package"}}}