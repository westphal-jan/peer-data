{"id": "1405.3515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Temporal Analysis of Language through Neural Language Models", "abstract": "We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.", "histories": [["v1", "Wed, 14 May 2014 14:47:22 GMT  (246kb,D)", "http://arxiv.org/abs/1405.3515v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yoon kim", "yi-i chiu", "kentaro hanaki", "darshan hegde", "slav petrov"], "accepted": false, "id": "1405.3515"}, "pdf": {"name": "1405.3515.pdf", "metadata": {"source": "CRF", "title": "Temporal Analysis of Language through Neural Language Models", "authors": ["Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov"], "emails": ["dh1806}@nyu.edu", "slav@google.com"], "sections": [{"heading": "1 Introduction", "text": "Language changes over time; existing words take on additional senses (gay), new words are created (Internet), and some words \"die out\" (many irregular verbs, such as burned, are replaced by their regularized counterparts (Lieberman et al., 2007). Traditionally, however, the scarcity of digitized historical corpora has prevented the use of contemporary machine learning algorithms - which typically require large amounts of data - in such temporal analyses. However, the publication of the Google Book Ngram-Corpus in 2009 has contributed to increased interest in cultural studies, with researchers analyzing changes in human culture through digitized texts (Michel et al., 2011). The development of computer-based methods to detect and quantify language changes is of interest to theoretical linguists as well as NLP researchers working with diachronic corpora."}, {"heading": "2 Related Work", "text": "Mihalcea and Nastase (2012) use a variation of latent semantic analysis to identify semantic changes in certain words from early to modern English. Wijaya and Yeniterzi (2011) use a Topics-overTime model and K-Mean clusters to identify periods in which selected words move from one topic / cluster to another, correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use a coexistence census of words from the 1960s and 1990s to detect semantic changes, and find that the words identified by the model are consistent with ratings of human raters. Popescu and Strapparava (2013) use statistical tests on frequencies of political, social and emotional changes."}, {"heading": "3 Neural Language Models", "text": "Similar to traditional language models, NLMs involve predicting a number of future words based on the history of previous words. In NLMs, however, words from a sparse, 1-by-V encoding (where V is the size of the vocabulary) are projected onto a lower dimensional vector space over a hidden layer. This allows for a better representation of the semantic properties of words compared to traditional language models (where words are represented as indexes in a vocabulary). Words that are semantically close together would have word vectors that are also \"close\" (measured by distance metry) in vector space. In fact, Mikolov et al. (2013a) report that word vectors obtained by NLMs capture a much deeper semantic information than was previously assumed."}, {"heading": "3.1 Training", "text": "The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all published books (Lin et al., 2012). We sampled 10 million 5-grams from the English fiction corpus for each year from 1850-2009. We shrink all words according to the sample and restrict vocabulary to words that occurred at least 10 times in the corpus from 1850-2009. For the model, we use a window size of 4 and a dimensionality of 200 for the word vectors. Within each year, we iterate over epochs to convergence, the measurement of convergence being defined as the average angle change of the word vectors between epochs. That is, if V (y) is the vocabulary for the year y, and xw (y, e) is the word vector for the word w in year y and the epoch number e is, we iterate further over epochs to, 1 (y)."}, {"heading": "4 Results and Discussion", "text": "For the analysis we consider 1850-1899 as the initialization phase and begin our study from 1900."}, {"heading": "4.1 Word Comparisons", "text": "By comparing the cosmic similarity between the same words over different periods of time, we are able to actually identify words whose use has changed. We are also able to identify words that have not changed. Table 1 has a list of 10 most / least changed words between 1900 and 2009. To better understand how these words have changed, we look at the composition of their adjacent words for 1900 and 2009 (Table 2). As a further check, we are searching Google Books for sentences that contain the above words. Below are some sample sentences from 1900 and 2009 with the verified word: \"However, he has verified himself over time by saying,\" She was about to say a little more, but to harm herself. \""}, {"heading": "4.2 Periods of Change", "text": "Since we train the model chronologically year after year, we can trace the temporal sequence of the distance of a word from its adjacent words (from different years) to detect periods of change. Figure 2 (above) has such an illustration for the word cell compared to its early neighbors, cupboard and dungeon and the newer neighbors, telephone and cordless. Figure 2 (below) has a similar illustration for homosexuals. Such illustrations also allow us to identify the period of change of a word in relation to its adjacent words and thus provide context of how it has evolved. This could be useful for researchers who want to understand (say) when gay was used as a synonym for gay. We can also identify periods of change independently of adjacent words by analyzing the cosmic similarity of a word with itself from a reference year (Figure 3). Since some of the changes are due to random samples and random deviations, we can additionally trace the average cosmic similarity of all words against their 3."}, {"heading": "4.3 Limitations", "text": "In this paper, the identification of a modified word is made conditional on its occurrence occurring frequently enough 3http: / / library.thinkquest.org / 04oct / 02001 / origin.htmin during the study period. If the use of a word has decreased drastically (or is no longer used at all), its word vector has remained the same and will therefore not be changed. One way to overcome this might be to combine cosinal distance and frequency to define a new measurement that measures how the use of a word has changed."}, {"heading": "5 Conclusions and Future Work", "text": "An interesting line of research could include the analysis and characterization of the different types of change. With a few exceptions, we have deliberately kept our analysis general by saying that the use of a word has changed. We have avoided inferring the type of change (e.g. semantic versus syntactic, widening versus narrowing, pejoration versus improvement). It may be the case that words subject to widening of the senses (say) exhibit regularities in the type of movement of vector space, which allows researchers to characterize the type of change that has occurred."}], "references": [{"title": "Neural Probabilitistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent."], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Latent Dirichlet Allocation", "author": ["D. Blei", "A. Ng", "M. Jordan", "J. Lafferty."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuglu", "P. Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 2011", "shortCiteRegEx": "Deerwester et al\\.", "year": 2011}, {"title": "A Distributional Similarity Approach to the Detection of Semantic Change in the Google Books Ngram Corpus", "author": ["K. Gulordava", "M. Baroni."], "venue": "Proceedings of the GEMS 2011 Workshop.", "citeRegEx": "Gulordava and Baroni.,? 2011", "shortCiteRegEx": "Gulordava and Baroni.", "year": 2011}, {"title": "Quantifying the evolutionary dynamics of language", "author": ["E. Lieberman", "J.B. Michel", "J. Jackson", "T. Tang", "M.A. Nowak."], "venue": "Nature, 449: 716\u2013716, October.", "citeRegEx": "Lieberman et al\\.,? 2007", "shortCiteRegEx": "Lieberman et al\\.", "year": 2007}, {"title": "Syntactic Annotations for the Google Books Ngram Corpus", "author": ["Y. Lin", "J.B. Michel", "E.L. Aiden", "J. Orwant", "W. Brockman", "S. Petrov."], "venue": "Proceedings of the Association for Computational Linguistics 2012.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Quantitative Analysis of Culture Using Millions of Digitized Books", "author": ["J.B Michel", "Y.K. Shen", "A.P. Aiden", "A. Veres", "M.K. Gray", "J.P. Pickett", "D. Hoiberg", "D. Clancy", "P. Norvig", "J. Orwant", "S.Pinker", "M.A. Nowak", "E.L. Aiden."], "venue": "Science, 331(6014): 176\u2013", "citeRegEx": "Michel et al\\.,? 2011", "shortCiteRegEx": "Michel et al\\.", "year": 2011}, {"title": "Word Epoch Disambiguation: Finding How Words Change Over Time", "author": ["R. Mihalcea", "V. Nastase."], "venue": "Proceedings of the Association for Computational Linguistics 2012.", "citeRegEx": "Mihalcea and Nastase.,? 2012", "shortCiteRegEx": "Mihalcea and Nastase.", "year": 2012}, {"title": "Recurrent Neural Network Based Language Model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur."], "venue": "Proceedings of Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.T Yih", "G. Zweig."], "venue": "Proceedings of NAACL-HLT 2013, 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space arXiv Preprint", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J.Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Behind the Times: Detecting Epoch Changes using Large Corpora", "author": ["O. Popescu", "C. Strapparava."], "venue": "International Joint Conference on Natural Language Processing, 347\u2013355", "citeRegEx": "Popescu and Strapparava.,? 2013", "shortCiteRegEx": "Popescu and Strapparava.", "year": 2013}, {"title": "Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space", "author": ["E. Sagi", "S. Kaufmann", "B. Clark"], "venue": "Proceedings of the EACL 2009 Workshop on GEMS: 104\u2013111. D.T. Wijaya, R. Yeniterzi. 2011. Understanding se-", "citeRegEx": "Sagi et al\\.,? 2009", "shortCiteRegEx": "Sagi et al\\.", "year": 2009}, {"title": "Learning Discriminative Projections for Text Similarity Measures", "author": ["W. Yih", "K. Toutanova", "J. Platt", "C. Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, 247\u2013256.", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "Existing words adopt additional senses (gay), new words are created (internet), and some words \u2018die out\u2019 (many irregular verbs, such as burnt, are being replaced by their regularized counterparts (Lieberman et al., 2007)).", "startOffset": 196, "endOffset": 220}, {"referenceID": 7, "context": "Publication of the Google Books Ngram corpus in 2009, however, has contributed to an increased interest in culturomics, wherein researchers analyze changes in human culture through digitized texts (Michel et al., 2011).", "startOffset": 197, "endOffset": 218}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context.", "startOffset": 0, "endOffset": 28}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English.", "startOffset": 0, "endOffset": 166}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another.", "startOffset": 0, "endOffset": 314}, {"referenceID": 4, "context": "Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change.", "startOffset": 0, "endOffset": 28}, {"referenceID": 4, "context": "Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs.", "startOffset": 0, "endOffset": 241}, {"referenceID": 4, "context": "Whereas previous work has generally involved researchers manually identifying words that have changed (with the exception of Gulordava and Baroni (2011)), we are able to automatically identify them.", "startOffset": 125, "endOffset": 153}, {"referenceID": 9, "context": "In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought.", "startOffset": 9, "endOffset": 32}, {"referenceID": 9, "context": "In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple \u2212 xapples \u2248 xcar \u2212 xcars \u2248 xfamily \u2212 xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples).", "startOffset": 9, "endOffset": 390}, {"referenceID": 1, "context": ", 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al.", "startOffset": 82, "endOffset": 126}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al.", "startOffset": 82, "endOffset": 151}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)).", "startOffset": 82, "endOffset": 170}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora.", "startOffset": 82, "endOffset": 236}, {"referenceID": 9, "context": "Despite its simplicity\u2014 and thus, computational efficiency\u2014compared to other NLMs, Mikolov et al. (2013b) note that the Skip-gram is competitive with other vector space models in the Semantic-Syntactic Word Relationship test set when trained on the same data.", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all books published (Lin et al., 2012).", "startOffset": 111, "endOffset": 129}], "year": 2014, "abstractText": "We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as cell and gay as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.", "creator": "TeX"}}}