{"id": "1405.3518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification", "abstract": "We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents.", "histories": [["v1", "Wed, 14 May 2014 14:50:59 GMT  (12kb)", "https://arxiv.org/abs/1405.3518v1", null], ["v2", "Sat, 28 Jun 2014 17:18:54 GMT  (12kb)", "http://arxiv.org/abs/1405.3518v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yoon kim", "owen zhang"], "accepted": false, "id": "1405.3518"}, "pdf": {"name": "1405.3518.pdf", "metadata": {"source": "CRF", "title": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification", "authors": ["Yoon Kim"], "emails": ["yhk255@nyu.edu", "zhonghua.zhang2006@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.35 18v2 [cs.CL] 2 8Ju n20 14"}, {"heading": "1 Introduction", "text": "Typically, discriminatory methods of text classification include training a linear classifier against the sack-of-words (BoW) representations of documents. In the BoW representations (also known as Vector Space Models), a document is presented as a vector, with each entry being a counting (or binary counting) of tokens that appeared in the document. Since some tokens are more informative than others, a common technique is to apply a weighting scheme to give more weight to discriminatory tokens and less weight to non-discriminatory ones. Term frequency inverse document frequency (tfidf) (Salton and McGill, 1983) is an uncontrolled weighting technique that is commonly used. In tf-idf, each token i in document d is assigned the following weight, wi, d \u00b7 log Ndfi (1), where tfi, d is the number of tokens i in document d, the number of documents N is the number of documents, and the number of documents pidus and the number of documents in the documents."}, {"heading": "2 The Method", "text": "Consider a binary classification task. Let Ci, k be the number of tokens i in the class k, with k, {\u2212 1, 1}. Let us call Ci the number of tokens i over both classes and y (d) the class of the document. For each occurrence of the token i in the training set, we calculate the following: s (j) i = {Ci, 1 Ci if y (d) = 1 Ci, \u2212 1 Ci if y (d) = \u2212 1 (2) Here j is the j-th occurrence of the token i. Since Ci has such occurrences, we assign an index from 1 to Ci to the token i. We assign a score to the token i, s, i = 1CiCi instead of j = 1s (j) i (3) i is intuitively the average probability of making the correct classification given to the token i in the document if i was the only token dot that is this document."}, {"heading": "2.1 Credibility Adjustment", "text": "Suppose s-i = s-j = 0.75 for two different characters i and j, but Ci = 5 and Cj = 100. Intuition suggests that s-i is a more credible value than s-i and that s-i should be shrunk in the direction of the population mean. Let s-i be the (weighted) population mean, that is, s-i-s-i-i-C (5), where C is the number of all characters in the corpus. We define credibility-adjusted values for characters i-i, si = C2i, 1 + C 2 i, \u2212 1 + s-g-g-C2i + \u03b3 (6), where \u03b3 is an additive smoothing parameter. If Ci, k-s are small, then si-s-i (otherwise si-s-i-i). This is a form of Buhlmann credibility adjustment from the actuarial literature (Buhlmann and Gisler 2005)."}, {"heading": "2.2 Sublinear Scaling", "text": "Paltoglou and Thelwall (2010) confirm that sublinear scaling of term frequency leads to significant improvements in various text classification tasks. We use logarithmic scaling, where tf is replaced by log (tf) + 1. In our method, tf is simply replaced by log (tf) + 1. We have found virtually no performance difference between log scaling and other sublinear scaling methods (such as augmented scaling, where tf is replaced by 0.5 + 0.5 + tfmax tf)."}, {"heading": "2.3 Normalization", "text": "The use of normalized features resulted in significant performance improvements compared to the use of non-normalized features. We therefore use x (d) = x (d) / | | x (d) | | 2 in SVM, where x (d) is the feature vector derived from cred-tf-idf weights for document d."}, {"heading": "2.4 Naive-Bayes SVM (NBSVM)", "text": "Wang and Manning (2012) achieve excellent (sometimes state-of-the-art) results on many benchmarks by using binary log ratios of Naive Bayes (NB) as characteristics in an SVM. Within their framework are wi, d = 1 {tfi, d} log (dfi, 1 + \u03b1) / \u2211 i (dfi, 1 + \u03b1) (dfi, \u2212 1 + \u03b1) / \u2211 i (dfi, \u2212 1 + \u03b1) (9), where dfi, k is the number of documents containing tokens i in class k, \u03b1 is a smoothing parameter and 1 {\u00b7} is the indicator function that is otherwise one when tfi, d > 0 and zero. As an additional benchmark, we implement NBSVM with \u03b1 = 1.0 and compare it with our results.1"}, {"heading": "3 Datasets and Experimental Setup", "text": "The Snippet datasets are: \u2022 PL-sh: Short film reviews with one sentence per review. The classification task is to determine whether the sentence is objective or subjective. (Pang and Lee, 2004) And the longer document datasets are: 1Wang and Manning (2012) use the same quotient, but differ from our NBSVM task summaries in two ways. Firstly, they use l2 hinged losses (as opposed to l1 loss in this paper) and secondly, they interpolate NBSVM weights with multivariable naive bayes (MNB)."}, {"heading": "3.1 Support Vector Machine (SVM)", "text": "For each document, we construct the feature vector x (d) using weights obtained from cred-tf-idf with log scaling and l2 normalization; for credtf-idf, \u03b3 is set to 1.0; NBSVM and tf-idf (also with log scaling and l2 normalization) are used to establish baselines; predicting a test document is done by y (d) = characters (wTx (d) + b) (10) In all experiments, we use a support vector machine (SVM) with a linear kernel and penalty parameters of C = 1.0; for the SVM, w, b are determined by minimizing, w T w + CN, w = 1max (0, 1 \u2212 y (d) (wTx (d) + b))))) (11) using the LIBLINEAR library (Fan et al., 2008)."}, {"heading": "3.2 Tokenization", "text": "We make all words smaller, but we do not perform any parentage or lemmatization. We limit the vocabulary to all characters that occurred at least twice in the training set."}, {"heading": "4 Results and Discussion", "text": "The standard tensile test splits are used on IMDB and newsgroup datasets.4.1 cred-tf-idf exceeds tf-idf table 2 when comparing the results for the different datasets. Our method exceeds the traditional tf-idf for all benchmarks for both unigrams and bigrams. While some of the performance differences at the level of 0.05 are significant (e.g. IMDB), some are not comparable (e.g. PL-2k).The Wilcoxon-signed ranking test is a non-parametric test that is commonly used in cases where two classifiers are compared across multiple datasets (Demsar, 2006).The Wilcoxon signed ranking test shows that the overall performance of Bsquared-M is significant."}, {"heading": "5 Conclusions and Future Work", "text": "Our method exceeds the traditional tf-idf weighting scheme for several benchmarks, which include snippets as well as longer documents. Furthermore, we have shown that tf-idf is competitive with the right scaling and normalization parameters compared to other modern methods.From a performance point of view, it would be interesting to see if our method is able to achieve even better results on the above tasks if the \u03b3 parameter is adjusted appropriately. Relatively, our method could potentially be combined with other monitored variants of tf-idf, either directly or by pretending to further improve performance."}], "references": [{"title": "Customizing sentiment classifiers to new domains: A case study", "author": ["A. Aue", "M. Gamon"], "venue": "Proceedings of the International Conference on Recent Advances in NLP,", "citeRegEx": "Aue and Gamon.,? \\Q2005\\E", "shortCiteRegEx": "Aue and Gamon.", "year": 2005}, {"title": "A Course in Credibility Theory and its Applications Springer-Verlag, Berlin", "author": ["H. Buhlmann", "A. Gisler"], "venue": null, "citeRegEx": "Buhlmann and Gisler.,? \\Q2005\\E", "shortCiteRegEx": "Buhlmann and Gisler.", "year": 2005}, {"title": "Supervised Term Weighting for Automated Text Categorization", "author": ["F. Debole", "F. Sebastiani"], "venue": "Proceedings of the 2003 ACM symposium on Applied Computing", "citeRegEx": "Debole and Sebastiani.,? \\Q2003\\E", "shortCiteRegEx": "Debole and Sebastiani.", "year": 2003}, {"title": "Statistical Comparison of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar.,? \\Q2006\\E", "shortCiteRegEx": "Demsar.", "year": 2006}, {"title": "A study of supervised term weighting scheme for sentiment analysis", "author": ["Z. Deng", "K. Luo", "H. Yu"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R. Fan", "K. Chang", "J. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A. Maas", "R. Daly", "P. Pham", "D. Huang", "A. Ng", "C. Potts"], "venue": "In Proceedings of ACL", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Delta TFIDF: An Improved Feature Space for Sentiment Analysis", "author": ["J. Martineau", "T. Finin"], "venue": "Third AAAI International Conference on Weblogs and Social Media", "citeRegEx": "Martineau and Finin.,? \\Q2009\\E", "shortCiteRegEx": "Martineau and Finin.", "year": 2009}, {"title": "A study of Information Retrieval weighting schemes for sentiment analysis", "author": ["G. Paltoglou", "M. Thelwall"], "venue": "In Proceedings of ACL", "citeRegEx": "Paltoglou and Thelwall.,? \\Q2010\\E", "shortCiteRegEx": "Paltoglou and Thelwall.", "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "In Proceedings of ACL", "citeRegEx": "Pang and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "In Proceedings of ACL", "citeRegEx": "Pang and Lee.,? \\Q2005\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Semi-Supervised Recursive Autoen", "author": ["R. Socher", "J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic Compositionality through Recursive MatrixVector Spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Discourse Connectors for Latent Subjectivity in Sentiment Analysis", "author": ["R. Trivedi", "J. Eisenstein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Trivedi and Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Trivedi and Eisenstein.", "year": 2013}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": null, "citeRegEx": "Salton and McGill.,? \\Q1983\\E", "shortCiteRegEx": "Salton and McGill.", "year": 1983}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["S. Wang", "C. Manning"], "venue": "In proceedings of ACL", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Inverse-CategoryFrequency Based Supervised Term Weighting Schemes for Text Categorization", "author": ["D. Wang", "H. Zhang"], "venue": "Journal of Information Science and Engineering", "citeRegEx": "Wang and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2013}, {"title": "Using appraisal taxonomies for sentiment analysis", "author": ["C. Whitelaw", "N. Garg", "S. Argamon"], "venue": "In Proceedings of CIKM", "citeRegEx": "Whitelaw et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Whitelaw et al\\.", "year": 2005}, {"title": "Multilevel Structured Models for Document-level Sentiment Classification", "author": ["A. Yessenalina", "Y. Yue", "C. Cardie"], "venue": "In Proceedings of ACL", "citeRegEx": "Yessenalina et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Term frequency-inverse document frequency (tfidf ) (Salton and McGill, 1983) is an unsupervised weighting technique that is commonly employed.", "startOffset": 51, "endOffset": 76}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)).", "startOffset": 59, "endOffset": 88}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)).", "startOffset": 59, "endOffset": 116}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)).", "startOffset": 59, "endOffset": 139}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al.", "startOffset": 59, "endOffset": 342}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes.", "startOffset": 59, "endOffset": 365}, {"referenceID": 2, "context": "Many supervised and unsupervised variants of tf-idf exist (Debole and Sebastiani (2003); Martineau and Finin (2009); Wang and Zhang (2013)). The purpose of this paper is not to perform an exhaustive comparison of existing weighting schemes, and hence we do not list them here. Interested readers are directed to Paltoglou and Thelwall (2010) and Deng et al. (2014) for comprehensive reviews of the different schemes. In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and longer documents. We also compare our method against Wang and Manning (2012)\u2019s Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and find that it performs competitively against NBSVM.", "startOffset": 59, "endOffset": 813}, {"referenceID": 1, "context": "This is a form of Buhlmann credibility adjustment from the actuarial literature (Buhlmann and Gisler, 2005).", "startOffset": 80, "endOffset": 107}, {"referenceID": 8, "context": "Paltoglou and Thelwall (2010) confirm that sublinear scaling of term frequency results in significant improvements in various text classification tasks.", "startOffset": 0, "endOffset": 30}, {"referenceID": 15, "context": "We test our method on both long and short text classification tasks, all of which were used to establish baselines in Wang and Manning (2012). Table 1 has summary statistics of the datasets.", "startOffset": 118, "endOffset": 142}, {"referenceID": 10, "context": "(Pang and Lee, 2005).", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "(Pang and Lee, 2004).", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "\u2022 PL-2k: 2000 full-length movie reviews that has become the de facto benchmark for sentiment analysis (Pang and Lee, 2004).", "startOffset": 102, "endOffset": 122}, {"referenceID": 6, "context": "\u2022 IMDB: 50k full-length movie reviews (25k training, 25k test), from IMDB (Maas et al., 2011).", "startOffset": 74, "endOffset": 93}, {"referenceID": 5, "context": "using the LIBLINEAR library (Fan et al., 2008).", "startOffset": 28, "endOffset": 46}, {"referenceID": 3, "context": "The Wilcoxon signed ranks test is a non-parametric test that is often used in cases where two classifiers are compared over multiple datasets (Demsar, 2006).", "startOffset": 142, "endOffset": 156}, {"referenceID": 15, "context": "cred-tf-idf did not outperform Wang and Manning (2012)\u2019s NBSVM (Wilcoxon signed ranks test pvalue = 0.", "startOffset": 31, "endOffset": 55}, {"referenceID": 15, "context": "cred-tf-idf did not outperform Wang and Manning (2012)\u2019s NBSVM (Wilcoxon signed ranks test pvalue = 0.1). But it did outperform our own implementation of NBSVM, implying that the extra modifications by Wang and Manning (2012) (i.", "startOffset": 31, "endOffset": 226}, {"referenceID": 5, "context": "Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 5, "context": "Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al.", "startOffset": 124, "endOffset": 165}, {"referenceID": 5, "context": "Indeed, the traditional tf-idf outperformed many of the more sophisticated methods that employ distributed representations (Maas et al. (2011); Socher et al. (2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al.", "startOffset": 124, "endOffset": 221}, {"referenceID": 4, "context": "(2011)) or other weighting schemes (Martineau and Finin (2009); Deng et al. (2014)).", "startOffset": 64, "endOffset": 83}, {"referenceID": 17, "context": "Tax: Uses appraisal taxonomies from WordNet (Whitelaw et al., 2005).", "startOffset": 44, "endOffset": 67}, {"referenceID": 18, "context": "SVM: Uses OpinionFinder to find objective versus subjective parts of the review (Yessenalina et al., 2010).", "startOffset": 80, "endOffset": 106}, {"referenceID": 4, "context": "aug-tf-mi: Uses augmented term-frequency with mutual information gain (Deng et al., 2014).", "startOffset": 70, "endOffset": 89}, {"referenceID": 13, "context": ": Uses discourse connectors to generate additional features (Trivedi and Eisenstein, 2013).", "startOffset": 60, "endOffset": 90}, {"referenceID": 6, "context": ": Learns sentiment-specific word vectors to use as features combined with BoW features (Maas et al., 2011).", "startOffset": 87, "endOffset": 106}, {"referenceID": 0, "context": "LLR: Uses log-likelihood ratio on features to select features (Aue and Gamon, 2005).", "startOffset": 62, "endOffset": 83}, {"referenceID": 11, "context": "RAE: Recursive autoencoders (Socher et al., 2011).", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "MV-RNN: Matrix-Vector Recursive Neural Networks (Socher et al., 2012).", "startOffset": 48, "endOffset": 69}, {"referenceID": 9, "context": "Rows 7-11 are MNB and NBSVM results from Wang and Manning (2012). Our NBSVM results are not directly comparable to theirs (see footnote 1).", "startOffset": 41, "endOffset": 65}], "year": 2014, "abstractText": "We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}