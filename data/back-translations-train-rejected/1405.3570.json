{"id": "1405.3570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Exchanging Conflict Resolution in an Adaptable Implementation of ACT-R", "abstract": "In computational cognitive science, the cognitive architecture ACT-R is very popular. It describes a model of cognition that is amenable to computer implementation, paving the way for computational psychology. Its underlying psychological theory has been investigated in many psychological experiments, but ACT-R lacks a formal definition of its underlying concepts from a mathematical-computational point of view. Although the canonical implementation of ACT-R is now modularized, this production rule system is still hard to adapt and extend in central components like the conflict resolution mechanism (which decides which of the applicable rules to apply next).", "histories": [["v1", "Wed, 14 May 2014 16:57:36 GMT  (52kb)", "http://arxiv.org/abs/1405.3570v1", "To appear in Theory and Practice of Logic Programming (TPLP). Accepted paper for ICLP 2014. 12 pages + appendix"]], "COMMENTS": "To appear in Theory and Practice of Logic Programming (TPLP). Accepted paper for ICLP 2014. 12 pages + appendix", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel gall", "thom fr\\\"uhwirth"], "accepted": false, "id": "1405.3570"}, "pdf": {"name": "1405.3570.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["John R. Anderson"], "emails": ["daniel.gall@uni-ulm.de)", "thom.fruehwirth@uni-ulm.de)"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.35 70v1 [cs.AI] 1In this paper, we present a concise implementation of ACT-R based on Constraint Handling Rules, derived from a formalization in previous work. To demonstrate the adaptability of our approach, we implement several different conflict resolution mechanisms discussed in the ACT-R literature, resulting in the first implementation of such a mechanism. For the other mechanisms, we empirically evaluate whether our implementation is consistent with the results of reference implementations of ACT-R.KEYWORDS: computational cognitive modeling, computational psychology, ACT-R, Constraint Handling Rules, production rule systems, conflict solution"}, {"heading": "1 Introduction", "text": "It is an approach in the cognitive sciences that examines human cognition by implementing detailed computational models, enabling researchers to execute their models and simulate human behavior (Sun 2008).Because of their feasibility, computational models can be precisely defined. To implement cognitive models, it is helpful to introduce cognitive architectures that pool well-studied research results from multiple disciplines of psychology into a single theory. On the basis of such an architecture, researchers are able to implement domain-specific computational models without having to deal with fundamental psychological outcomes."}, {"heading": "2 A CHR implementation of ACT-R", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2.2 The Procedural Module in CHR", "text": "The procedural module is the core of the ACT-R production control system. Our implementation is based on the translation of production control systems to CHR as presented in (Spring 2009, Chapter 6.1). However, we must take into account the concepts of chunks and buffers, because ACT-R differs from other production systems in these particular points. Details of the implementation can be found in (Gall 2013). The amount of chunks can be represented in CHR by a constraint chunk (C, T), where C is the name of the chunk and T. The slots provided by this chunk and their values can be stored in constraints chunk _ has _ slot (C, V), which denotes the value of chunk C in slot S. Special consistency rules can be used to ensure that no chunk has two values in its slots and that it contains only the slots."}, {"heading": "C \\ match <=> add_q(Now + 0.05,0,apply_rule(rule(r,C))).", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C \\ apply_rule(rule(r,C)) <=> A, get_time(Now), add_q(Now,-10,match).", "text": "Actions A plan their effects on the buffers at the current time with different priorities. Requests are sent only to the corresponding module, and its effects on the requested buffer are planned at a later time. Finally, a new match event is scheduled at the current time, but with a low priority of \u2212 10. This ensures that all current actions are executed before the next rule is set to fireworks. Otherwise, if no rule matches and the Procedure module is free (i.e., a match restriction exists), a rule can only match if the contents of the buffer change. Therefore, immediately after the next event, a new match restriction is added to the queue, which models the fact that the Procedure module is permanently looking for suitable rules when it is free to add unnecessary match events."}, {"heading": "3 Conflict Resolution", "text": "Thus, if there are multiple applicable productions, the system must decide who should be shot. This process is called conflict resolution (McDermott and Forgy 1977). In most implementations, the CHR simply chooses the rule by which to shoot, which is a valid conflict resolution mechanism. However, in ACT-R a more advanced approach is required that uses sub-symbolic concepts to faithfully model human cognition."}, {"heading": "3.1 General Conflict Resolution Process", "text": "In (Fru Fr\u00fchwirth 2009, p. 151) a general method for implementing different conflict resolution mechanisms is given in the CHR, which is adapted to our CHR implementation of ACT-R. The first rule of each CHR rule pair in Section 2.3 can be replaced by: match, C = = > G | conflict _ set (rule (r, C)). Therefore, the application of a matching production is delayed by adding the rule to the conflict package instead of selecting the first matching rule to be applied in the program by schedulingconflict _ set / 1 constraints, which can then be reduced to a single constraint containing only the rule to be applied according to an arbitrary strategy. As a final production rule, the Match < = > select. rule appears in the program. This rule is always applied last (since rules are applied in textual order in the CHR). It removes the remaining match constraint and adds a constraint that tributes the selection process."}, {"heading": "3.2 Conflict Resolution Strategies", "text": "Over time, several conflict resolution strategies have been proposed for ACT-R. In order to demonstrate the adaptability of our implementation of the CHR, we are implementing some of these strategies. In the reference implementation of ACT-R, such adaptations may require a great deal of knowledge about its internal structures (Stewart and West 2007).In general, ACT-R conflict resolution strategies typically use the sub-symbolic concept of production services. The production benefit for a production i is the function Ui: N \u2192 R, which expresses the utility value of a particular production in its ninth application, which can be adapted according to a learning strategy. In the conflict resolution process, the current utility values for all matching functions are compared and the production with the highest utility value is selected. Therefore, the production service value can be considered a dynamic rule priority, which is adapted according to a specific strategy. Below, we present some different learning strategies to adapt the benefit of a production. Finally, the concept of rule breaking, which is a general conflict resolution concept and can be applied to all presented learning strategies."}, {"heading": "3.2.1 Reinforcement-Learning-Based Utility Learning", "text": "The current implementation of ACT-R 6.0 uses a conflict resolution mechanism motivated by the Rescorla-Wagner learning equation (Rescorla and Wagner 1972).The basic concept is that there are special production rules that recognize a successful state (by a model-specific definition) and then trigger a certain amount of reward, measured in units of time, to obtain a certain reward (Anderson 2007, p. 161).All productions that lead to a successful state, i.e. all productions that have been applied receive a portion of the triggered reward, which is more time between the application of the production rule and the triggering of the reward. The benefit of a production i is then adjusted."}, {"heading": "3.2.2 Success-/Cost-Based Utility Learning", "text": "In previous implementations of ACT-R, utility learning is based on a success / cost approach (Anderson et al. 2004; Taatgen et al. 2006). A more detailed description can be found in (ACT-R Tutorial 2004, Unit 6). Each production rule i is assigned to the values Pi, which characterize the likelihood of success of production and Ci its costs. In this approach, the benefit of a production rule is measured as: Ui (n) = Pi (n) G \u2212 Ci (n) indicating that the current benefit does not depend on the value of the last benefit, but on the current value of the parameters. Therefore, the order of application does not matter. Usually, Ci is measured in units of time to achieve a goal, whereas G - the target value - is an architectural parameter and is usually set to 20 s."}, {"heading": "3.2.3 Random Estimated Costs", "text": "In (Belavkin and Ritter 2004), a conflict resolution strategy is presented that is motivated by research results in decision-making, but the current implementation differs slightly from this description (Belavkin 2005), and we stick to this latest approach to better comparability of outcomes, based on performance / cost-based benefit learning from Section 3.2.2 and using the same sub-symbolic information (the number of successes and failures and efforts).However, rather than calculating the average cost of Ci, the expected cost of success is estimated by a rule: \u03b8i: = E (Ci) \u2248 effortsi # sucessessessesi (4) From the expected cost of rule i, the random estimated cost is derived by taking a random number ri from a uniform distribution U (0, 1) and setting i = \u2212 prevortsi log \u00b7 (1 \u2212 ri)."}, {"heading": "3.2.4 Production Rule Refraction", "text": "Unlike the previous strategies, which only exchange the learning part of the benefit, the production rule refraction adapts the general conflict resolution mechanism and can be combined with all the other strategies presented. It was first proposed in Young 2003 to avoid overprogramming of models in the sense that the sequence of application of a set of rules is determined in advance by adding artificial signals to ensure the desired order. Rule refraction can avoid such operational concepts by preventing the application of the same rule instantiation more than once. To the best of knowledge, our implementation is the first of its kind. Refraction for ACT-R. Refraction can be implemented by storing the instantiation of any applied production with the rule _ rule (R) = = > Instantiation (R). When building the conflict set, all productions that have already been applied from the rule selection process are eliminated by the following rule: Instantiation (R)\\ Conflict set (R) < > true."}, {"heading": "4 Evaluation", "text": "After implementing some different conflict resolution strategies, we will test their validity using an example model of the game Rock, Paper, Scissors. The idea is that the model simulates a player playing against three opponents with different preferences on the three playing fields. We will then observe how the model adapts its strategy among the different conflict resolution mechanisms and test whether the results of the ACT-R implementation and our CHR implementation match."}, {"heading": "4.1 Setup", "text": "The player is essentially modelled by the production rules Play-Rock, Play-Paper and Play-Scissors for the three decisions a player has in the game. Initially, the production rules have the same utilities, which are then adjusted by the use mechanisms of the three conflict resolution strategies. Since we only want to test our conflict resolution strategies, we try to exclude all other factors that could influence the behavior of our model, but only one test of our implementation. We can better compare the noise where possible."}, {"heading": "4.2 Availability of the Strategies", "text": "Our approach allows the user to share the complete conflict resolution strategy without relying on provided interfaces and hooks, except for the very basic information that a rule is part of the conflict set or is being applied. This information is based on the basic concept of the ACT-R match-select application cycle. In the reference implementations of the strategies, there are deeper dependencies and assumptions about when and how sub-symbolic information is adapted and stored, leading to incompatibilities: the enhanced learning strategy is only available for ACT-R 6.0. Although the performance / cost-based strategy ships with ACT, it leads to further incompatibility problems when using modules that are not available for ACT-R 5.0 (which is generally difficult to expand due to the lack of architectural modules). Since the randomly estimated cost method is based on the success / cost-based strategy, it is also the best suggested for our Young-R implementation in 2003."}, {"heading": "4.3 Reinforcement-Learning-Based Utility Learning", "text": "In the reinforcement and learning-based strategy, we marked the production rules for winning with a reward of 2 and the rules for detecting defeats with 0, resulting in negative rewards for all the rules applied when a defeat is determined. Draws do not result in adjustments to the strategy in our configuration. We executed the model on ACT-R 6.0 version 1.5- r1451 and our CHR implementation. Our implementation matches the results of the reference implementation exactly when rounded to the same decimal accuracy (see online appendix B.2). Differences in floating point accuracy did not affect the results as ACT-R rounded the end result to thousandths. As expected, the model usually rewards the paper rule most when played against players 1 and 2 (average benefit at the end of the round for player 1: (Ur, Up, Us) = (0, 1.87, \u2212 0.02); player 2 (0, 0.81, 0.49)."}, {"heading": "4.4 Success-/Cost-Based Utility Learning", "text": "For the performance / cost-based strategy, the production rules that recognize a winning situation are marked as a success, and analogously, the production rules for defeat situations are marked as a failure. We used ACT-R 5.0 to test our implementation against the reference implementation, since it is not available for ACT-R 6.0. Again, noise is disabled for better comparability. As the selection mechanism for rules with the same benefit differs from ACT-R 6.0, we have adjusted the order in which the rules appear in the source code. Our implementation matches exactly the results of the reference implementation (see online appendix B.3). It is evident that this strategy is unable to detect the optimal moves for player 1. Analyses have shown that due to the order of the rules, the model initially selects fields, which results in a draw and thus no adjustment of the tools. Therefore, rocks are played repeatedly. In real-world models, repeated fields will help players overcome the average of such problems. 2"}, {"heading": "5 Related Work", "text": "There are several implementations of the ACT-R theory in different programming languages. First, there is the official ACT-R implementation in Lisp (ACT-R 2014), which we used as a reference, and there are a lot of enhancements to this implementation, some of which were included in the original package in later versions such as the ACT-R / PM extension in ACT-R 6.0 (Bothell, p. 264).The implementation comes with an experimental environment that provides a graphical user interface for loading, executing and observing models.In (Stewart and West 2006; Stewart and West 2007), a Python implementation is presented, which also aims to simplify and harmonize parts of the ACT-R theory by finding the central components of the theory.The architecture has been reduced to only the procedural and declarative memory used to build other models that combine and adapt them in different ways."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented an implementation of ACT-R with Constraint Handling Rules that is capable of closing the gap between the theory of ACT-R and its technical realization. Our implementation abstracts from technical artifacts and is close to theory, but can reproduce the results of the reference implementation. Moreover, the implementation itself can be extended by implementations against this reference. Implementation of the various conflict resolution strategies provided by ACT-R. has shown the adaptability of ACT-R and our implementation of the refraction of production rules. In the future, implementation can be expanded by other modules such as the perceptive / motor modules provided by ACT-R. Currently, there is an ongoing student project to implement a temporary module that can be used to investigate the perception of time. Formalization and translation of CHR pave the path to the development of test tools for ACT-R (e.g. a Confluence-T analysis tool)."}, {"heading": "Appendix B Evaluation Results", "text": "In this appendix we list the results of our experiments as described in section 4.B.1. SamplesIn table B 1 and B 3 the samples used by player 2 and player 3 are listed. Table B2 and B 4 show the frequencies of the rock, paper and scissors within a sample. The sum 11 is a control value and ensures that 20 movements per sample have been produced. The p-values denote the probabilities of the rock and the scissors. Sample1 r r r. 1 r: 0 r: 0 r: 0 r: 0 r: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: 0 R: R R R: 0 R: R R R: 0: R R R R: 0: R R R"}], "references": [{"title": "The ACT-R Homepage", "author": ["ACT-R"], "venue": "http://act-r.psy.cmu.edu/.", "citeRegEx": "ACT.R,? 2014", "shortCiteRegEx": "ACT.R", "year": 2014}, {"title": "How can the human mind occur in the physical universe", "author": ["J.R. Anderson"], "venue": null, "citeRegEx": "Anderson,? \\Q2007\\E", "shortCiteRegEx": "Anderson", "year": 2007}, {"title": "An integrated theory of the mind", "author": ["J.R. Anderson", "D. Bothell", "M.D. Byrne", "S. Douglass", "C. Lebiere", "Y. Qin"], "venue": "Psychological Review 111, 4, 1036\u20131060.", "citeRegEx": "Anderson et al\\.,? 2004", "shortCiteRegEx": "Anderson et al\\.", "year": 2004}, {"title": "The Atomic Components of Thought", "author": ["J.R. Anderson", "C. Lebiere"], "venue": "Lawrence Erlbaum Associates, Inc.", "citeRegEx": "Anderson and Lebiere,? 1998", "shortCiteRegEx": "Anderson and Lebiere", "year": 1998}, {"title": "Optimist conflict resolution overlay for the ACT\u2013R cognitive architecture", "author": ["R. Belavkin"], "venue": "http://www.eis.mdx.ac.uk/staffpages/rvb/software/optimist/optimist-for-actr.pdf .", "citeRegEx": "Belavkin,? 2005", "shortCiteRegEx": "Belavkin", "year": 2005}, {"title": "Optimist: A new conflict resolution algorithm for act-r", "author": ["R. Belavkin", "F.E. Ritter"], "venue": "ICCM. 40\u201345.", "citeRegEx": "Belavkin and Ritter,? 2004", "shortCiteRegEx": "Belavkin and Ritter", "year": 2004}, {"title": "Constraint Handling Rules", "author": ["T. Fr\u00fchwirth"], "venue": "Cambridge University Press.", "citeRegEx": "Fr\u00fchwirth,? 2009", "shortCiteRegEx": "Fr\u00fchwirth", "year": 2009}, {"title": "A rule-based implementation of ACT-R using constraint handling rules", "author": ["D. Gall"], "venue": "Master Thesis, Ulm University .", "citeRegEx": "Gall,? 2013", "shortCiteRegEx": "Gall", "year": 2013}, {"title": "Production system conflict resolution strategies", "author": ["J. McDermott", "C. Forgy"], "venue": "SIGART Bull. 63 (June), 37\u201337.", "citeRegEx": "McDermott and Forgy,? 1977", "shortCiteRegEx": "McDermott and Forgy", "year": 1977}, {"title": "A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement", "author": ["R.A. Rescorla", "A.W. Wagner"], "venue": "Appleton-Century-Crofts, New York, Chapter 3, 64\u201399.", "citeRegEx": "Rescorla and Wagner,? 1972", "shortCiteRegEx": "Rescorla and Wagner", "year": 1972}, {"title": "Compiling constraint handling rules for efficient tabled evaluation", "author": ["B. Sarna-Starosta", "C.R. Ramakrishnan"], "venue": "In 9th International Symposium on Practical Aspects of Declarative Languages (PADL).", "citeRegEx": "Sarna.Starosta and Ramakrishnan,? 2007", "shortCiteRegEx": "Sarna.Starosta and Ramakrishnan", "year": 2007}, {"title": "Deconstructing ACT-R", "author": ["T.C. Stewart", "R.L. West"], "venue": "Proceedings of the Seventh International Conference on Cognitive Modeling. 298\u2013303.", "citeRegEx": "Stewart and West,? 2006", "shortCiteRegEx": "Stewart and West", "year": 2006}, {"title": "Deconstructing and reconstructing ACT-R: exploring the architectural space", "author": ["T.C. Stewart", "R.L. West"], "venue": "Cognitive Systems Research 8, 3 (Sept.), 227\u2013236.", "citeRegEx": "Stewart and West,? 2007", "shortCiteRegEx": "Stewart and West", "year": 2007}, {"title": "Introduction to computational cognitive modeling", "author": ["R. Sun"], "venue": "The Cambridge Handbook of Computational Psychology, R. Sun, Ed. Cambridge University Press, New York, 3\u201319.", "citeRegEx": "Sun,? 2008", "shortCiteRegEx": "Sun", "year": 2008}, {"title": "Why do children learn to say \u201cbroke\u201d? a model of learning the past tense without feedback", "author": ["N.A. Taatgen", "J.R. Anderson"], "venue": "Cognition 86, 2, 123\u2013155.", "citeRegEx": "Taatgen and Anderson,? 2002", "shortCiteRegEx": "Taatgen and Anderson", "year": 2002}, {"title": "Modeling paradigms in ACT-R", "author": ["N.A. Taatgen", "C. Lebiere", "J. Anderson"], "venue": "Cognition and Multi-Agent Interaction: From Cognitive Modeling to Social Simulation. Cambridge University Press, 29\u201352.", "citeRegEx": "Taatgen et al\\.,? 2006", "shortCiteRegEx": "Taatgen et al\\.", "year": 2006}, {"title": "Should ACT-R include production refraction", "author": ["R.M. Young"], "venue": "In Proceedings of 10th Annual ACT-R Workshop", "citeRegEx": "Young,? \\Q2003\\E", "shortCiteRegEx": "Young", "year": 2003}], "referenceMentions": [{"referenceID": 13, "context": "This enables researchers to execute their models and simulate human behavior (Sun 2008).", "startOffset": 77, "endOffset": 87}, {"referenceID": 15, "context": "Additionally, cognitive architectures ideally constrain modeling to plausible models which facilitates the modeling process (Taatgen et al. 2006).", "startOffset": 124, "endOffset": 145}, {"referenceID": 3, "context": "Anderson (Anderson and Lebiere 1998; Anderson et al. 2004).", "startOffset": 9, "endOffset": 58}, {"referenceID": 2, "context": "Anderson (Anderson and Lebiere 1998; Anderson et al. 2004).", "startOffset": 9, "endOffset": 58}, {"referenceID": 14, "context": "It has been used to model cognitive tasks like learning the past tense (Taatgen and Anderson 2002), but is also used in human-computer interaction or to improve educational software by simulating human students (Anderson et al.", "startOffset": 71, "endOffset": 98}, {"referenceID": 12, "context": "The situation improved with the modularization of the psychological theory, but it is still difficult to exchange more central parts of the implementation like conflict resolution (Stewart and West 2007).", "startOffset": 180, "endOffset": 203}, {"referenceID": 7, "context": "The formalization may support the understanding of the details of our implementation, hence we refer to (Gall 2013) and and the online appendix (Appendix A).", "startOffset": 104, "endOffset": 115}, {"referenceID": 6, "context": "For reasons of space, we refer to the literature for an introduction to CHR (Fr\u00fchwirth 2009).", "startOffset": 76, "endOffset": 92}, {"referenceID": 2, "context": "For a more detailed introduction to ACT-R, see (Anderson et al. 2004) and (Taatgen et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 15, "context": "2004) and (Taatgen et al. 2006).", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "The reference implementation of ACT-R is written in Lisp and can be obtained from the ACT-R website (ACT-R 2014).", "startOffset": 100, "endOffset": 112}, {"referenceID": 7, "context": "Details of our implementation including the formalization it is based on can be found in (Gall 2013).", "startOffset": 89, "endOffset": 100}, {"referenceID": 7, "context": "Details of the implementation can be found in (Gall 2013).", "startOffset": 46, "endOffset": 57}, {"referenceID": 8, "context": "This process is called conflict resolution (McDermott and Forgy 1977).", "startOffset": 43, "endOffset": 69}, {"referenceID": 12, "context": "In the reference implementation of ACT-R, such adaptations might need a lot of knowledge about its internal structures (Stewart and West 2007).", "startOffset": 119, "endOffset": 142}, {"referenceID": 9, "context": "0 uses a conflict resolution mechanism which is motivated by the Rescorla-Wagner learning equation (Rescorla and Wagner 1972).", "startOffset": 99, "endOffset": 125}, {"referenceID": 2, "context": "In prior implementations of ACT-R, the utility learning is based on a success-/cost approach (Anderson et al. 2004; Taatgen et al. 2006).", "startOffset": 93, "endOffset": 136}, {"referenceID": 15, "context": "In prior implementations of ACT-R, the utility learning is based on a success-/cost approach (Anderson et al. 2004; Taatgen et al. 2006).", "startOffset": 93, "endOffset": 136}, {"referenceID": 5, "context": "In (Belavkin and Ritter 2004), a conflict resolution strategy motivated by research results in decision-making is presented.", "startOffset": 3, "endOffset": 29}, {"referenceID": 4, "context": "The current implementation varies slightly from this description (Belavkin 2005) and we stick to this most recent approach for a better comparability of the results.", "startOffset": 65, "endOffset": 80}, {"referenceID": 5, "context": "If G = 0, the production rule with minimal random estimated costs will be fired (as suggested in (Belavkin and Ritter 2004)).", "startOffset": 97, "endOffset": 123}, {"referenceID": 16, "context": "It was first suggested in (Young 2003) to avoid over-programming of models in the sense that the order of application of a set of rules is fixed in advance by adding artificial signals to ensure the desired order.", "startOffset": 26, "endOffset": 38}, {"referenceID": 16, "context": "Our implementation of the refraction-based method is to the best of our knowledge the only existing implementation for ACT-R, although it has been suggested in (Young 2003).", "startOffset": 160, "endOffset": 172}, {"referenceID": 0, "context": "First of all, there is the official ACT-R implementation in Lisp (ACT-R 2014) which we used as a reference.", "startOffset": 65, "endOffset": 77}, {"referenceID": 11, "context": "In (Stewart and West 2006; Stewart and West 2007), a Python implementation is presented which also has the aim to simplify and harmonize parts of the ACT-R theory by finding the central components of the theory.", "startOffset": 3, "endOffset": 49}, {"referenceID": 12, "context": "In (Stewart and West 2006; Stewart and West 2007), a Python implementation is presented which also has the aim to simplify and harmonize parts of the ACT-R theory by finding the central components of the theory.", "startOffset": 3, "endOffset": 49}], "year": 2014, "abstractText": "In computational cognitive science, the cognitive architecture ACT-R is very popular. It describes a model of cognition that is amenable to computer implementation, paving the way for computational psychology. Its underlying psychological theory has been investigated in many psychological experiments, but ACT-R lacks a formal definition of its underlying concepts from a mathematical-computational point of view. Although the canonical implementation of ACTR is now modularized, this production rule system is still hard to adapt and extend in central components like the conflict resolution mechanism (which decides which of the applicable rules to apply next). In this work, we present a concise implementation of ACT-R based on Constraint Handling Rules which has been derived from a formalization in prior work. To show the adaptability of our approach, we implement several different conflict resolution mechanisms discussed in the ACT-R literature. This results in the first implementation of one such mechanism. For the other mechanisms, we empirically evaluate if our implementation matches the results of reference implementations of ACT-R.", "creator": "LaTeX with hyperref package"}}}