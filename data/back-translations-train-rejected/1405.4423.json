{"id": "1405.4423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2014", "title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "abstract": "Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement.", "histories": [["v1", "Sat, 17 May 2014 18:20:13 GMT  (70kb,D)", "http://arxiv.org/abs/1405.4423v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tapio pahikkala", "michiel stock", "antti airola", "tero aittokallio", "bernard de baets", "willem waegeman"], "accepted": false, "id": "1405.4423"}, "pdf": {"name": "1405.4423.pdf", "metadata": {"source": "CRF", "title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "authors": ["Tapio Pahikkala", "Michiel Stock", "Antti Airola", "Tero Aittokallio", "Bernard De Baets", "Willem Waegeman"], "emails": ["firstname.surname@utu.fi", "firstname.surname@UGent.be", "firstname.surname@fimm.fi"], "sections": [{"heading": null, "text": "ar Xiv: 1 Theoretically, the two-step method is very closely related to the tensor product core approach and leads experimentally to a slightly better prediction capability. In addition, the two-step method, unlike existing tensor product core methods, enables closed solutions for training and parameter selection through cross-validation estimates in both full and almost full cold start, making the approach much more efficient and easier to implement."}, {"heading": "1 A subdivision of dyadic prediction methods", "text": "Many of the real-world learning problems can, of course, be presented as pairwise learning or dyadic prediction problems, where the representation of two different types of objects (a.k.a. dyad) is used together to predict a relationship between these objects. Among other things, applications of this kind arise in biology (e.g. the prediction of protein-RNA interactions).For many dyadic prediction problems, it is extremely important to implement appropriate training and evaluation procedures. [28] Make an important distinction between four main settings (e.g. personalized product recommendations) and recommendations."}, {"heading": "1.1 The problem setting considered in this article", "text": "Matrix factorization and hybrid filtering strategies are not applicable to Setting D. Compared to the other three settings, Setting D will receive less attention in the literature (with some exceptions, see e.g. [20, 23, 24, 27]) and will be our main focus in this article. In addition, we will also examine the transition period between Settings C and D if it occurs very rarely in the training dataset, while d of the dyad (d, t) is observed only in the predictive phase. We refer to this setting as the almost complete cold start problem. Full and almost complete cold start problems can only be solved by considering characteristic representations of dyads (also known as ancillary information in the recommendation system literature)."}, {"heading": "1.2 Formulation as a transfer learning problem", "text": "As discussed above, dyadic prediction is closely related to several sub-areas of machine learning. Furthermore, in this article we decide to introduce multi-task learning or transfer terminology, with d and t denoting the characteristics of instances and tasks. In this perspective, Setting C corresponds to a specific instance of a traditional transfer learning scenario in which the goal is to transfer knowledge from previously learned auxiliary tasks to the target task of interest [26]. Extending the concept of transfer learning even further, in the case of so-called zero data learning, leads to Setting D, which is characterized by no available labeled training data for the target task [16]. If the target task is unknown during the training period, the learning method must be able to generalize it \"on the fly.\""}, {"heading": "2 Solving full and almost full cold start prob-", "text": "We assume that each training input can be represented as x = (d, t), where d = D and t = T are the objects and tasks respectively, and D and T are the corresponding spaces for objects and tasks. Furthermore, D = {di} mi = 1 and T = {tj} q = 1 should each denote the sets of different objects and tasks encountered in the training set with m = | D | and q = | T |. We say that the training set is complete if it contains each object-task pair with object in D and task in T. For complete training sets, we introduce another notation for the matrix of labels Y = Rm \u00b7 q, so that its rows are indexed by the objects in D and the columns by the tasks in T."}, {"heading": "2.1 Kernel ridge regression with tensor product ker-", "text": "An alternative approach that makes generalizing new tasks easier, however, is to use the tensor product of paired kernels [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object task pairs. (x, x) = \u03b1 (d, t), (d, t) = k (d, d) g (t, 8, 12, 23, 27], in which kernels are defined on object task pairs. (x, x) = \u03b1 (d, t), (d) = k (t, d) g (1) as a product of the data core k and task core. Given that K, Rm, m and G, Rq are the optimization matrices for the data points and tasks, the kernel matrix, or the kernel matrix for the object task pairs, is, for a complete training, the product or product."}, {"heading": "2.2 Two-step kernel ridge regression", "text": "Next, we present a two-step procedure for performing transfer learning.In the following, we assume that we will receive a training set in which each auxiliary task has the same designated training objects. This assumption is algorithm 1 Two-step nuclear backward regression 1: C \u2190 argminC-Rm \u00b7 q {0-CG \u2212 Y-2F + \u03bbttr (CGCT)} 2: z \u2190 (zTL, (CUg) T) T3: a-argmina-Rm {(Ka \u2212 z) T (Ka \u2212 z) + \u03bbdaTKa} 4: return ft (\u00b7) = 1 aik (di, \u00b7) quite realistically in many practical environments, since, for example, you can perform a preliminary final step using the extensive toolkit of missing value imputation or matrix execution algorithms."}, {"heading": "2.3 Computational considerations and model selection", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 - \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 Theoretical considerations", "text": "This, in turn, allows us to show the consistency of the two-step KRR training through its universal approach and spectral regularization. Let's consider the consistency of the two-step KRR training. Let's consider a full cold-start setting with a full training. Let's build a model of two-step KRR tasks in which the link between the two-step and paired KRR task is characterized by the following result. Let's consider a full cold-start setting with a full training."}, {"heading": "4 Experiments", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is not a country, but a country in which it is a country, a region and a country."}], "references": [{"title": "Incorporating side information into probabilistic matrix factorization using Gaussian processes", "author": ["R.P. Adams", "G.E. Dahl", "I. Murray"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Kernels for vector-valued functions: a review", "author": ["M. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundation and Trends in Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Unifying collaborative and content-based filtering. Proceedings of the twenty-first international conference on Machine learning (ICML\u201904)", "author": ["J. Basilico", "T. Hofmann"], "venue": "ACM International Conference Proceeding Series, vol", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "On regularization algorithms in learning theory", "author": ["F. Bauer", "S. Pereverzev", "L. Rosasco"], "venue": "Journal of Complexity", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Kernel methods for predicting protein-protein interactions", "author": ["A. Ben-Hur", "W. Noble"], "venue": "Bioinformatics 21 Suppl", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Kernel multi-task learning using task-specific features", "author": ["E.V. Bonilla", "F. Agakov", "C. Williams"], "venue": "In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Comprehensive analysis of kinase inhibitor selectivity", "author": ["M.I. Davis", "J.P. Hunt", "S. Herrgard", "P. Ciceri", "L.M. Wodicka", "G. Pallares", "M. Hocker", "D.K. Treiber", "P.P. Zarrinkar"], "venue": "Nature biotechnology 29(11),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Matrix co-factorization for recommendation with rich side information and implicit feedback", "author": ["Y. Fang", "L. Si"], "venue": "Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Concordance probability and discriminatory power in proportional hazards regression", "author": ["M. G\u00f6nen", "G. Heller"], "venue": "Biometrika 92(4),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Self-measuring similarity for multi-task gaussian process", "author": ["K. Hayashi", "T. Takenouchi", "R. Tomioka", "H. Kashima"], "venue": "ICML Workshop on Unsupervised and Transfer Learning, JMLR Proceedings,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Protein-ligand interaction prediction: an improved chemogenomics", "author": ["L. Jacob", "J. Vert"], "venue": "approach\u201d, bioinformatics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Link propagation: A fast semi-supervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "Proceedings of the SIAM International Conference on Data Mining (SDM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "Proceedings of the 23rd national conference on Artificial intelligence (AAAI\u201908),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "W.S.S.: The spectrum kernel: a string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "Noble"], "venue": "Proceedings of the Pacific Symposium on Biocomputing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Spectral algorithms for supervised learning", "author": ["L. Lo Gerfo", "L. Rosasco", "F. Odone", "E. De Vito", "A. Verri"], "venue": "Neural Computation 20(7),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Shifted Kronecker product systems", "author": ["C.D. Martin", "C.F. Van Loan"], "venue": "SIAM Journal on Matrix Analysis and Applications 29(1),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "A log-linear model with latent features for dyadic prediction", "author": ["A. Menon", "C. Elkan"], "venue": "ICDM, pp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Using feature conjunctions across examples for learning pairwise classifiers", "author": ["S. Oyama", "C. Manning"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Toward more realistic drug-target interaction predictions", "author": ["T. Pahikkala", "A. Airola", "S. Pietil\u00e4", "S. Shakyawar", "A. Szwajda", "J. Tang", "T. Aittokallio"], "venue": "Briefings in Bioinformatics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient regularized least-squares algorithms for conditional ranking on relational data", "author": ["T. Pahikkala", "A. Airola", "M. Stock", "B.D. Baets", "W. Waegeman"], "venue": "Machine Learning 93(2-3),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Conditional ranking on relational data", "author": ["T. Pahikkala", "W. Waegeman", "A. Airola", "T. Salakoski", "B. De Baets"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Learning intransitive reciprocal relations with kernel methods", "author": ["T. Pahikkala", "W. Waegeman", "E. Tsivtsivadze", "T. Salakoski", "B. De Baets"], "venue": "European Journal of Operational Research", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.T. Park", "W. Chu"], "venue": "Proceedings of the Third ACM Conference on Recommender Systems, pp", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Flaws in evaluation schemes for pair-input computational predictions", "author": ["Y. Park", "E.M. Marcotte"], "venue": "Nature Methods", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Fast and scalable algorithms for semisupervised link prediction on static and dynamic graphs", "author": ["R. Raymond", "H. Kashima"], "venue": "Proceedings of the European conference on Machine learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Notes on regularized least squares", "author": ["R. Rifkin", "R. Lippert"], "venue": "Tech. Rep. MIT-CSAIL-TR-2007-025, Massachusetts Institute of Technology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Input space versus feature space in kernel-based methods", "author": ["B. Sch\u00f6lkopf", "S. Mika", "C. Burges", "P. Knirsch", "K.R. M\u00fcller", "G. R\u00e4tsch", "A. Smola"], "venue": "IEEE Transactions On Neural Networks", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "On protocols and measures for the validation of supervised methods for the inference of biological networks", "author": ["M. Schrynemackers", "R. K\u00fcffner", "P. Geurts"], "venue": "Front Genet", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Classifying pairs with trees for supervised biological network inference", "author": ["M. Schrynemackers", "L. Wehenkel", "M.M. Babu", "P. Geurts"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Generalized probabilistic matrix factorizations for collaborative filtering", "author": ["H. Shan", "A. Banerjee"], "venue": "ICDM, pp", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "The ubiquitous kronecker product", "author": ["C.F. Van Loan"], "venue": "Journal of Computational and Applied Mathematics 123(1\u20132),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "A kernel-based framework for learning graded relations from data", "author": ["W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets"], "venue": "IEEE Transactions on Fuzzy Systems", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Kernelized probabilistic matrix factorization: Exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "[28] make in a recent Nature-review on dyadic prediction an important distinction between four main settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] for a review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 32, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 36, "context": "[1, 10, 20, 34, 38] for a not at all exhaustive list.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "[32] for a recent review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 21, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 22, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 25, "context": "[20, 23, 24, 27]), and it will be our main focus in this article.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 118, "endOffset": 125}, {"referenceID": 25, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 118, "endOffset": 125}, {"referenceID": 5, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 170, "endOffset": 177}, {"referenceID": 12, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 170, "endOffset": 177}, {"referenceID": 11, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "Such feature representations have been successfully applied in order to solve problems such as product recommendation [5, 27], prediction of protein-protein interactions [7, 14], drug design [13], prediction of game outcomes [25] and document retrieval [23].", "startOffset": 253, "endOffset": 257}, {"referenceID": 12, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 21, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 25, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 60, "endOffset": 72}, {"referenceID": 21, "context": "Efficient optimization approaches based on gradient descent [14, 23, 27] and closed form solutions [23] have been introduced.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "From this viewpoint, Setting C corresponds to a specific instantiation of a traditional transfer learning scenario, in which the aim is to transfer knowledge obtained from already learned auxiliary tasks to the target task of interest [26].", "startOffset": 235, "endOffset": 239}, {"referenceID": 14, "context": "Stretching the concept of transfer learning even further, in the case of so-called zerodata learning, one arrives at Setting D, which is characterized by no available labeled training data for the target task [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 31, "context": "[33] have recently proposed a similar two-step approach based on tree-based ensemble methods for biological network inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4] and several other authors (see [2] and references therein) have extended KRR to involve task correlations via matrix-valued kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[4] and several other authors (see [2] and references therein) have extended KRR to involve task correlations via matrix-valued kernels.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 5, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 6, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 10, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 19, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 21, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 25, "context": "An alternative approach, allowing the generalization to new tasks more straightforwardly, is to use the tensor product pairwise kernel [5, 7, 8, 12, 21, 23, 27], in which kernels are defined on object-task pairs", "startOffset": 135, "endOffset": 160}, {"referenceID": 1, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 12, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 17, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 22, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 27, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 34, "context": "Several authors have pointed out that, while the size of the above system is considerably large, its solution can be found efficiently via tensor algebraic optimization [2, 14, 19, 24, 29, 36].", "startOffset": 169, "endOffset": 192}, {"referenceID": 29, "context": "Nystr\u00f6m method in order to lower both the time and space complexities of kernel methods [31], and hence in the following we assume that d \u2264 m and r \u2264 q.", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "It is well known that, for KRR, the LOOCV performance can be efficiently computed without training the model from scratch during each CV round (we refer to [30] for details).", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "[35] A continuous kernel k on a compact metric space X (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "The universality property indicates that the hypothesis space induced by an universal kernel can approximate any continuous function to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find the approximation [35].", "startOffset": 313, "endOffset": 317}, {"referenceID": 35, "context": "The product of two universal kernels is also universal, as considered in our previous work [37].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The full cold start setting with complete auxiliary training set allows us to consider the two-step approach from the spectral filtering regularization point of view [18], an approach that has recently gained some attention due to its ability to study various types of regularization approaches under the same framework.", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 4, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 16, "context": "To further analyze the above filter functions, we follow [4, 6, 18] and say that a function \u03c6\u03bb : [0, \u03ba ] \u2192 R, 0 < \u03bb \u2264 \u03ba, parameterized by 0 < \u03bb \u2264 \u03ba, is an admissible regularizer if there exists constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that", "startOffset": 57, "endOffset": 67}, {"referenceID": 2, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 4, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 16, "context": "We refer to [4, 6, 18] for a detailed consideration and further results.", "startOffset": 12, "endOffset": 22}, {"referenceID": 9, "context": "The performance is measured using the concordance index [11] (C-index), also known as the pairwise ranking accuracy 1 |{(i,j)|yi>yj}| \u2211 yi>yj H(\u0177i \u2212 \u0177j), where yi denote the true and \u0177i the predicted labels, and H is the Heaviside step function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "The drug-target interaction prediction data [9, 22] consists of 68 drug compounds and 442 protein targets.", "startOffset": 44, "endOffset": 51}, {"referenceID": 20, "context": "The drug-target interaction prediction data [9, 22] consists of 68 drug compounds and 442 protein targets.", "startOffset": 44, "endOffset": 51}, {"referenceID": 15, "context": "For the amino acid sequences we used the normalized spectrum kernel [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "Previously, [32] have in their overview article on dyadic prediction in the biological domain made the observation that in terms of predictive accuracy experimentally there does not seem to be a clear winner between the single-task and multi-task type of learning approaches.", "startOffset": 12, "endOffset": 16}], "year": 2014, "abstractText": "Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show 1 ar X iv :1 40 5. 44 23 v1 [ cs .L G ] 1 7 M ay 2 01 4 theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement. 1 A subdivision of dyadic prediction methods Many real-world machine learning problems can be naturally represented as pairwise learning or dyadic prediction problems, for which feature representations of two different types of objects (aka a dyad) are jointly used to predict a relationship between those objects. Amongst others, applications of that kind emerge in biology (e.g. predicting protein-RNA interactions), medicine (e.g. design of personalized drugs), chemistry (e.g. prediction of binding between two types of molecules), social network analysis (e.g. link prediction) and recommender systems (e.g. personalized product recommendation). For many dyadic prediction problems it is extremely important to implement appropriate training and evaluation procedures. [28] make in a recent Nature-review on dyadic prediction an important distinction between four main settings. Given t and d as the feature representations of the two types of objects, those four settings can be summarized as follows: \u2022 Setting A: Both t and d are observed during training, as parts of separate dyads, but the label of the dyad (t,d) must be predicted. \u2022 Setting B: Only t is known during training, while d is not observed in any dyad, and the label of the dyad (t,d) must be predicted. \u2022 Setting C: Only d is known during training, while t is not observed in any dyad, and the label of the dyad (t,d) must be predicted. \u2022 Setting D: Neither t nor d occur in any training dyad, but the label of the dyad (t,d) must be predicted (referred to as the full cold start problem). Setting A is of all four settings by far the most studied setting in the machine literature. Motivated by applications in collaborative filtering and", "creator": "LaTeX with hyperref package"}}}