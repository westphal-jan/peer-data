{"id": "1405.4589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm", "abstract": "A large number of experimental data shows that Support Vector Machine (SVM) algorithm has obvious advantages in text classification, handwriting recognition, image classification, bioinformatics, and some other fields. To some degree, the optimization of SVM depends on its kernel function and Slack variable, the determinant of which is its parameters $\\delta$ and c in the classification function. That is to say,to optimize the SVM algorithm, the optimization of the two parameters play a huge role. Ant Colony Optimization (ACO) is optimization algorithm which simulate ants to find the optimal path.In the available literature, we mix the ACO algorithm and Parallel algorithm together to find a well parameters.", "histories": [["v1", "Mon, 19 May 2014 03:50:21 GMT  (137kb)", "https://arxiv.org/abs/1405.4589v1", "3 pages, 2 figures, 2 tables"], ["v2", "Tue, 20 May 2014 11:53:39 GMT  (142kb)", "http://arxiv.org/abs/1405.4589v2", "3 pages, 2 figures, 2 tables"]], "COMMENTS": "3 pages, 2 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["chao zhang", "hong-cen mei", "hao yang"], "accepted": false, "id": "1405.4589"}, "pdf": {"name": "1405.4589.pdf", "metadata": {"source": "CRF", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm", "authors": ["Chao Zhang", "Hong-Cen Mei", "Hao Yang"], "emails": ["zhch040200@gmail.com", "hongcenmei@yeah.net", "chongqingyanghao@yeah.net"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.45 89v2 [cs] 2 0M ay2 01I is the order of magnitude of the difference. SUPPORT VECTOR CLASSIFICATION AND PARAMETERSSVM is based on the principle of structural risk minimization, whereby limited training samples are used to obtain the higher generalizability of the decision function. Suppose a sample is specified (xi, yi), where i = 1, 2... N means the number of training samples, x \u00b2 R means the sample characteristic, y \u00b2 1, \u2212 1} means the sample classification. SVM classification function: y = number x + b means weight vector, b means setover) Functional margin: number of training samples (x + b) = yf (x) Function margin is the minimum margin from the hyperplane (xi, yi) to T (xi, yi) Geometric margin = 1 of the margin (yxi)."}], "references": [{"title": "Paramters selection and stimulation of support vector machines based on ant colony optimization algorithm", "author": ["P.F. LIU Chun-bo", "WANG Xian-fang"], "venue": "J.Cent,South Univ, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Libsvm : a library for support vector machines", "author": ["C. chung Chang", "C.-J. Lin"], "venue": "Linux Journal, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Ant Colony Optimization - Techniques and Applications", "author": ["E. by Helio J.C. Barbosa"], "venue": "InTech, Chapters published February", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Automatic parameters selection for SVM based on GA[C", "author": ["J.L.-c. ZHENG Chun-hong"], "venue": "NJ:IEEE Press,2004:1869-1872.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machine parameter tuning using dynamic encoding algorithm for handwritten digit recognition", "author": ["Y. Park", "S.-W. Kim", "H.-S. Ahn"], "venue": "2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A new support vector machine optimized by improved particle swarm optimization and its application", "author": ["X. Li", "S. dong Yang", "J. xun Qi"], "venue": "Journal of Central South University of Technology, vol. 13, pp. 568\u2013572, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel Ant Colony Optimization on Graphics Processing Units", "author": ["A. Delevacq", "P. Delisle", "M. Gravel"], "venue": "Journal of Parallel and Distributed Computing, vol. 73, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving ant colony optimization algorithm for data clustering", "author": ["R. Tiwari", "M. Husain", "S. Gupta", "A. Srivastava"], "venue": "pp. 529\u2013534, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1] The significant digit of C are assumed to be the five, and the highest level of C is hundred\u2019s place.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "be considered as a superset of the latter, they basically differ in the following points[4]:", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In this article, we use openCL to realize the parallel of ant[4].", "startOffset": 61, "endOffset": 64}], "year": 2014, "abstractText": "A large number of experimental data shows that Support Vector Machine (SVM) algorithm has obvious a large advantages in text classification, handwriting recognition, image classification, bioinformatics, and some other fields. To some degree, the optimization of SVM depends on its kernel function and Slack variable, the determinant of which is its parameters \u03b4 and c in the classification function. That is to say, to optimize the SVM algorithm, the optimization of the two parameters play a huge role. Ant Colony Optimization (ACO) is optimization algorithm which simulate ants to find the optimal path. In the available literature, we mix the ACO algorithm and Parallel algorithm together to find a well parameters. Keyword: SVM, Parameters, ACO, OpenCL, Parallel I. SUPPORT VECTOR CLASSIFICATION AND PARAMETERS SVM is based on the principle of structural risk minimization,using limited training samples to obtain the higher generalization ability of decision function. Suppose a sample set (xi, yi) , where i = 1, 2...N means the number of training samples, x\u2208R means the sample characteristics, y \u2208 {+1,\u22121} means the sample classification. SVM Classification function: y = \u03c9x+ b (\u03c9 means weight vector,b means setover)", "creator": "LaTeX with hyperref package"}}}