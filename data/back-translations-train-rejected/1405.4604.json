{"id": "1405.4604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "On the saddle point problem for non-convex optimization", "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance.", "histories": [["v1", "Mon, 19 May 2014 04:56:30 GMT  (379kb,D)", "http://arxiv.org/abs/1405.4604v1", "11 pages, 8 figures"], ["v2", "Wed, 28 May 2014 03:05:00 GMT  (409kb,D)", "http://arxiv.org/abs/1405.4604v2", "11 pages, 8 figures"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["razvan pascanu", "yann n dauphin", "surya ganguli", "yoshua bengio"], "accepted": false, "id": "1405.4604"}, "pdf": {"name": "1405.4604.pdf", "metadata": {"source": "CRF", "title": "On the saddle point problem for non-convex optimization", "authors": ["Razvan Pascanu", "Yann N. Dauphin", "Surya Ganguli"], "emails": ["r.pascanu@gmail.com", "dauphiya@iro.umontreal.ca", "sganguli@standford.edu", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "It is often the case that our geometric intuition, derived from our experience within a low-dimensional physical world, is woefully insufficient to think about the geometry of typical fault surfaces in high-dimensional spaces. For example, if we have a typical, randomly selected error function of a single peculiar eigenfunction, the error function could be a single drawing of a Gaussian process (Rasmussen and Williams, 2005). Such a random error function would, with a high probability of choosing the function, have many local minima (maxima) in which the gradient disappears and the second derivative is negative (positive). However, it is highly unlikely to have a saddle point (see Figure 1) in which the gradient and the second derivative disappear. In fact, such saddle points are a degenerated condition, would occur with a probability of zero."}, {"heading": "1.1 The prevalence of saddle points", "text": "A number of proofs therefore come from statistical physics, where the nature of the critical points of random Gaussian error functions is studied on high-dimensional dimensional autonomous domains (2007); Fyodorov and Williams (2007) counted the typical number of critical points of a random function in a finite cubic volume in N-dimensional space, a theoretical technique for analyzing high-dimensional systems with quenched perturbations such as spinning glasses (see Parisi (2007) for a current review); and Bray and Dean (2007) in particular counted the typical number of critical points of a random function in N-dimensional space within a range of errors and index values is the fraction of negative eigenvalues of Hessian at the critical point. Of course, each such function has a unique global minimum at = min and a global maximum at = 1, where min and max non-detailed function depend on the random function."}, {"heading": "2 Optimization algorithms near saddle points", "text": "The above work suggests that errors are made both in typical, randomly selected high-dimensional fault surfaces (12,000) and in neural network training surfaces (12,000).We will now provide a theoretically justified algorithm to deal with this problem. We will focus on peculiarities, namely those for which the Hessian is not exactly singular, which can be analyzed locally based on a unique repair of the function described in Morse's dilemma (see Chapter 7.3, Theorem 7.16 in Callahan (2010).This repair metrization occurs by extending the function f around the critical point. If we assume that the Hessian is not singular, then there is a neighborhood around this critical point where this approximation is reliable and the first order is derived."}, {"heading": "3 Generalized trust region methods", "text": "The first change is that we allow a Taylor extension of the first-order function in order to minimize, rather than always relying on a second-order Taylor extension typically used in trust region methods; the second change is that we replace the restriction of the norm of the step with a restriction of the distance between \u03b8 and \u03b8 + \u0432. Distance measurement is also not specified and can be specific to the example of the general trust region method used; if we define Tk (f.) to indicate a Taylor extension of the series by the k-th order evaluated in \u0432 + \u0432, then we can summarize a generalized trust region as follows: \u0438 = arg min."}, {"heading": "4 Addressing the saddle point problem", "text": "In order to address the saddle point problem, we are looking for a solution within the family of generalized trust region methods. We know that the use of Hessian as a rescaling factor can lead to a non-descending direction due to the negative curvature. The above analysis also suggests that the correction of negative curvature by an additive term leads to a sub-optimal step, so we want the resulting step from this trust region method not to be a function of Hessian. Therefore, we are using a first order Taylor expansion of loss. This means that the curvature information must come from the constraint by selecting a suitable distance measure d."}, {"heading": "4.1 Limit the influence of second order terms \u2013 Saddle-Free Newton Method", "text": "The analysis we have done for various optimization techniques, for example, states that we can still question the function of optimization while we want to do so, is that we shift the gradient in any direction, thus avoiding the attraction of the dynamics of learning - the idea, the absolute value of this algorithm, or even an adequate detailed exploration of these ideals.The problem is that one cannot simply replace H with H by taking the absolute value of each intrinsic value of H without the proper justification."}, {"heading": "5 Experimental results \u2013 empirical evidence for the saddle point problem", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6 Conclusion", "text": "We have tried to expand this collection of results by intuitively analyzing how different existing optimization techniques behave in the vicinity of such structures. Our analysis clearly shows that while some algorithms are not \"stuck\" in the plateau around the saddle point, they do make a suboptimal step. Moreover, the analysis suggests what would be the optimal step. We extend this observation, which was made prior to this work, to an appropriate optimization algorithm, relying on the framework of general trust region methods. Within this framework, we optimize at each step a Taylor approach of the first order of our function, limited to a region within which this approximation is reliable. The size of this region (in each direction) is determined by how different the approximation of the first order function to the second order is."}, {"heading": "Acknowledgments", "text": "The authors are grateful for the support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR. We also thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). Razvan Pascanu is supported by a DeepMind Fellowship."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks, 2(1), 53\u201358.", "citeRegEx": "Baldi and Hornik,? 1989", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13, 281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["A.J. Bray", "D.S. Dean"], "venue": "Physics Review Letter, 98, 150201.", "citeRegEx": "Bray and Dean,? 2007", "shortCiteRegEx": "Bray and Dean", "year": 2007}, {"title": "Advanced Calculus: A Geometric View", "author": ["J. Callahan"], "venue": "Undergraduate Texts in Mathematics. Springer.", "citeRegEx": "Callahan,? 2010", "shortCiteRegEx": "Callahan", "year": 2010}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Y.V. Fyodorov", "I. Williams"], "venue": "Journal of Statistical Physics, 129(5-6), 1081\u20131116.", "citeRegEx": "Fyodorov and Williams,? 2007", "shortCiteRegEx": "Fyodorov and Williams", "year": 2007}, {"title": "On-line learning theory of soft committee machines with correlated hidden units steepest gradient descent and natural gradient descent", "author": ["M. Inoue", "H. Park", "M. Okada"], "venue": "Journal of the Physical Society of Japan, 72(4), 805\u2013810.", "citeRegEx": "Inoue et al\\.,? 2003", "shortCiteRegEx": "Inoue et al\\.", "year": 2003}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Roux et al\\.,? 2007", "shortCiteRegEx": "Roux et al\\.", "year": 2007}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "International Conference in Machine Learning, pages 735\u2013742.", "citeRegEx": "Martens,? 2010", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning", "author": ["E. Mizutani", "S. Dreyfus"], "venue": "Advances in Neural Information Processing Systems, pages 1669\u20131677.", "citeRegEx": "Mizutani and Dreyfus,? 2010", "shortCiteRegEx": "Mizutani and Dreyfus", "year": 2010}, {"title": "Newton-type methods", "author": ["W. Murray"], "venue": "Technical report, Department of Management Science and Engineering, Stanford University.", "citeRegEx": "Murray,? 2010", "shortCiteRegEx": "Murray", "year": 2010}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer.", "citeRegEx": "Nocedal and Wright,? 2006", "shortCiteRegEx": "Nocedal and Wright", "year": 2006}, {"title": "Mean field theory of spin glasses: statistics and dynamics", "author": ["G. Parisi"], "venue": "Technical Report Arxiv 0706.0094.", "citeRegEx": "Parisi,? 2007", "shortCiteRegEx": "Parisi", "year": 2007}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Pascanu and Bengio,? 2014", "shortCiteRegEx": "Pascanu and Bengio", "year": 2014}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2005", "shortCiteRegEx": "Rasmussen and Williams", "year": 2005}, {"title": "Natural Gradient Descent for On-Line Learning", "author": ["M. Rattray", "D. Saad", "S.I. Amari"], "venue": "Physical Review Letters, 81(24), 5461\u20135464.", "citeRegEx": "Rattray et al\\.,? 1998", "shortCiteRegEx": "Rattray et al\\.", "year": 1998}, {"title": "On-line learning in soft committee machines", "author": ["D. Saad", "S.A. Solla"], "venue": "Physical Review E, 52, 4225\u20134243.", "citeRegEx": "Saad and Solla,? 1995", "shortCiteRegEx": "Saad and Solla", "year": 1995}, {"title": "Learning hierarchical category structure in deep neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "Proceedings of the 35th annual meeting of the Cognitive Science Society, pages 1271\u20131276.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural network", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Saxe et al\\.,? 2014", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "On the distribution of the roots of certain symmetric matrices", "author": ["E.P. Wigner"], "venue": "The Annals of Mathematics, 67(2), 325\u2013327.", "citeRegEx": "Wigner,? 1958", "shortCiteRegEx": "Wigner", "year": 1958}], "referenceMentions": [{"referenceID": 15, "context": "More precisely, the error function could be a single draw of a Gaussian process (Rasmussen and Williams, 2005).", "startOffset": 80, "endOffset": 110}, {"referenceID": 4, "context": "One line of evidence comes from statistical physics, where the nature of critical points of random Gaussian error functions on high dimensional dimensional continuous domains is studied Bray and Dean (2007); Fyodorov and Williams (2007) using replica theory, a theoretical technique for analyzing high dimensional systems with quenched disorder like spin glasses (see Parisi (2007) for a recent review).", "startOffset": 186, "endOffset": 207}, {"referenceID": 4, "context": "One line of evidence comes from statistical physics, where the nature of critical points of random Gaussian error functions on high dimensional dimensional continuous domains is studied Bray and Dean (2007); Fyodorov and Williams (2007) using replica theory, a theoretical technique for analyzing high dimensional systems with quenched disorder like spin glasses (see Parisi (2007) for a recent review).", "startOffset": 186, "endOffset": 237}, {"referenceID": 4, "context": "One line of evidence comes from statistical physics, where the nature of critical points of random Gaussian error functions on high dimensional dimensional continuous domains is studied Bray and Dean (2007); Fyodorov and Williams (2007) using replica theory, a theoretical technique for analyzing high dimensional systems with quenched disorder like spin glasses (see Parisi (2007) for a recent review).", "startOffset": 186, "endOffset": 382}, {"referenceID": 4, "context": "One line of evidence comes from statistical physics, where the nature of critical points of random Gaussian error functions on high dimensional dimensional continuous domains is studied Bray and Dean (2007); Fyodorov and Williams (2007) using replica theory, a theoretical technique for analyzing high dimensional systems with quenched disorder like spin glasses (see Parisi (2007) for a recent review). In particular, Bray and Dean (2007) counted the typical number of critical points of a random function in a finite cubic volume in N dimensional space within a range of error and index \u03b1.", "startOffset": 186, "endOffset": 440}, {"referenceID": 4, "context": "One line of evidence comes from statistical physics, where the nature of critical points of random Gaussian error functions on high dimensional dimensional continuous domains is studied Bray and Dean (2007); Fyodorov and Williams (2007) using replica theory, a theoretical technique for analyzing high dimensional systems with quenched disorder like spin glasses (see Parisi (2007) for a recent review). In particular, Bray and Dean (2007) counted the typical number of critical points of a random function in a finite cubic volume in N dimensional space within a range of error and index \u03b1. By definition the index \u03b1 is the fraction of negative eigenvalues of the Hessian at the critical point. Of course every such function has a unique global minimum at = min and \u03b1 = 0 and a global maximum at = max and \u03b1 = 1, where min and max do not depend strongly on the detailed realization of the random function due to concentration of measure. In Bray and Dean (2007), the authors found that any such function, over a large enough volume, has exponentially many critical points, but in the \u2212 \u03b1 plane all the critical points are overwhelmingly likely to be located on a monotonically increasing curve \u2217(\u03b1) that rises from min to max as \u03b1 ranges from 0 to 1.", "startOffset": 186, "endOffset": 963}, {"referenceID": 20, "context": "Bray and Dean (2007) found that the entire eigenvalue distribution of the Hessian took the form of Wigner\u2019s famous semi-circular law (Wigner, 1958), but shifted by an amount determined by .", "startOffset": 133, "endOffset": 147}, {"referenceID": 4, "context": "Bray and Dean (2007) found that the entire eigenvalue distribution of the Hessian took the form of Wigner\u2019s famous semi-circular law (Wigner, 1958), but shifted by an amount determined by .", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Bray and Dean (2007) found that the entire eigenvalue distribution of the Hessian took the form of Wigner\u2019s famous semi-circular law (Wigner, 1958), but shifted by an amount determined by . Indeed, a completely unconstrained random symmetric matrix has a symmetric eigenvalue density on the real axis shaped like a semicircle with both mode and mean at 0. Thus any eigenvalue is positive or negative with probability 1/2. The eigenvalue distribution of the Hessian of the critical point at error is a shifted semicircle, where the shift ensures that the fraction of negative eigenvalues \u03b1 is given exactly by the solution to = \u2217(\u03b1). When the error = min, the semicircular distribution of the Hessian is shifted so far to the right that all eigenvalues are positive, corresponding to the global minimum. As the error of the critical point increases, the semicircular eigenvalue distribution shifts to the left, and the fraction of negative eigenvalues \u03b1 increases. At intermediate error , half way between min and max, the semicircular distribution of eigenvalues has its mode at 0. This implies that the highest density of eigenvalues occurs near 0, and so a typical critical point at intermediate error not only has many negative curvature directions, but also many approximate plateau directions, in which a finite fraction of eigenvalues of the Hessian lie near 0. The existence of these approximate plateau directions, in addition to the negative directions, would of course have significant implications for high dimensional non-convex optimization, in particular, dramatically slowing down gradient descent dynamics. Moreover, Fyodorov and Williams (2007) give a review of work in which qualitatively similar results are derived for random error functions superimposed on a quadratic error surface.", "startOffset": 0, "endOffset": 1662}, {"referenceID": 4, "context": "The Hessian at a critical point with error very close to the global minimum is not a fully unconstrained random Gaussian matrix; the fact that the error is so small, shifts its eigenvalue distribution to the right, so that more eigenvalues are positive Bray and Dean (2007); Fyodorov and Williams (2007).", "startOffset": 253, "endOffset": 274}, {"referenceID": 4, "context": "The Hessian at a critical point with error very close to the global minimum is not a fully unconstrained random Gaussian matrix; the fact that the error is so small, shifts its eigenvalue distribution to the right, so that more eigenvalues are positive Bray and Dean (2007); Fyodorov and Williams (2007).", "startOffset": 253, "endOffset": 304}, {"referenceID": 18, "context": "These learning dynamics exhibit plateaus of high error followed by abrupt transitions to better performance, and they qualitatively recapitulate aspects of the hierarchical development of semantic concepts in infants (Saxe et al., 2013).", "startOffset": 217, "endOffset": 236}, {"referenceID": 0, "context": "In Baldi and Hornik (1989) the error surface of a single hidden layer MLP with linear units is analysed.", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "In Baldi and Hornik (1989) the error surface of a single hidden layer MLP with linear units is analysed. The number of hidden units is assumed to be less than the number of inputs units. Such an error surface shows only saddle-points and no local minimum or local maximum. This result is qualitatively consistent with the observation made by Bray and Dean (2007). In fact, as long as we do not get stuck in the plateaus surrounding these saddle points, for such a model we are guaranteed to obtain the global minimum of the error.", "startOffset": 3, "endOffset": 363}, {"referenceID": 0, "context": "In Baldi and Hornik (1989) the error surface of a single hidden layer MLP with linear units is analysed. The number of hidden units is assumed to be less than the number of inputs units. Such an error surface shows only saddle-points and no local minimum or local maximum. This result is qualitatively consistent with the observation made by Bray and Dean (2007). In fact, as long as we do not get stuck in the plateaus surrounding these saddle points, for such a model we are guaranteed to obtain the global minimum of the error. Indeed Saxe et al. (2014), analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of deep linear feedforward models.", "startOffset": 3, "endOffset": 557}, {"referenceID": 0, "context": "In Baldi and Hornik (1989) the error surface of a single hidden layer MLP with linear units is analysed. The number of hidden units is assumed to be less than the number of inputs units. Such an error surface shows only saddle-points and no local minimum or local maximum. This result is qualitatively consistent with the observation made by Bray and Dean (2007). In fact, as long as we do not get stuck in the plateaus surrounding these saddle points, for such a model we are guaranteed to obtain the global minimum of the error. Indeed Saxe et al. (2014), analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of deep linear feedforward models. These scaling symmetries enabled Saxe et al. (2014) to find new exact solutions to the nonlinear dynamics of learning in deep linear networks.", "startOffset": 3, "endOffset": 792}, {"referenceID": 15, "context": "In Saad and Solla (1995) the dynamics of stochastic gradient descent are analyzed for soft committee machines.", "startOffset": 3, "endOffset": 25}, {"referenceID": 15, "context": "Rattray et al. (1998); Inoue et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "(1998); Inoue et al. (2003) provides further analysis, stating that the initial phase of learning is plagued with saddle point structures caused by symmetries in the weights.", "startOffset": 8, "endOffset": 28}, {"referenceID": 5, "context": "16 in Callahan (2010)).", "startOffset": 6, "endOffset": 22}, {"referenceID": 12, "context": "This can be done regardless of the approximation of the Newton method used, for example as either a truncated Newton method or a BFGS approximation (see Nocedal and Wright (2006) chapters 4 and 7).", "startOffset": 153, "endOffset": 179}, {"referenceID": 9, "context": "For example, the recently proposed Hessian-Free Optimization (Martens, 2010) was shown to be a variant of natural gradient descent (Pascanu and Bengio, 2014).", "startOffset": 61, "endOffset": 76}, {"referenceID": 14, "context": "For example, the recently proposed Hessian-Free Optimization (Martens, 2010) was shown to be a variant of natural gradient descent (Pascanu and Bengio, 2014).", "startOffset": 131, "endOffset": 157}, {"referenceID": 10, "context": "Numerically, this means that the Gauss Newton direction can be (close to) orthogonal to the gradient at some distant point from \u03b8\u2217 (Mizutani and Dreyfus, 2010).", "startOffset": 131, "endOffset": 159}, {"referenceID": 8, "context": "For example, the recently proposed Hessian-Free Optimization (Martens, 2010) was shown to be a variant of natural gradient descent (Pascanu and Bengio, 2014). The algorithm ends up doing an update similar to the Newton method, just that instead of inverting the Hessian we invert Fisher Information Matrix, F, which is positive definite by construction. It is argued in Rattray et al. (1998); Inoue et al.", "startOffset": 62, "endOffset": 392}, {"referenceID": 7, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively.", "startOffset": 8, "endOffset": 28}, {"referenceID": 7, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively. Specifically, it can resolve those saddle points arising from having units behaving very similar. In Mizutani and Dreyfus (2010), however, it is argued that natural gradient descent does also suffer when negative curvature is present.", "startOffset": 8, "endOffset": 244}, {"referenceID": 8, "context": "The same is true for TONGA Le Roux et al. (2007), an algorithm similar to natural gradient descent.", "startOffset": 30, "endOffset": 49}, {"referenceID": 14, "context": "Similar to Pascanu and Bengio (2014), we can use Lagrange multipliers to get the solution of this constraint optimization.", "startOffset": 11, "endOffset": 37}, {"referenceID": 2, "context": "The hyper-parameters of minibatch SGD \u2013 the learning rate, minibatch size and the momentum constant \u2013 are chosen using random search (Bergstra and Bengio, 2012).", "startOffset": 133, "endOffset": 160}, {"referenceID": 3, "context": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 53, "endOffset": 98}, {"referenceID": 1, "context": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 53, "endOffset": 98}], "year": 2017, "abstractText": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance.", "creator": "LaTeX with hyperref package"}}}