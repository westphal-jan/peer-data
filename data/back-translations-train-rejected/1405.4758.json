{"id": "1405.4758", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms", "abstract": "We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.", "histories": [["v1", "Mon, 19 May 2014 14:56:51 GMT  (240kb,D)", "http://arxiv.org/abs/1405.4758v1", "COLT 2014"]], "COMMENTS": "COLT 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stefan magureanu", "richard combes", "alexandre proutiere"], "accepted": false, "id": "1405.4758"}, "pdf": {"name": "1405.4758.pdf", "metadata": {"source": "CRF", "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms", "authors": ["Stefan Magureanu", "Richard Combes", "Alexandre Proutiere", "MAGUREANU COMBES PROUTIERE"], "emails": ["MAGUR@KTH.SE", "RICHARD.COMBES@SUPELEC.FR", "ALEPRO@KTH.SE"], "sections": [{"heading": null, "text": "For discrete Lipschitz bandits, we derive asymptotic problem-specific lower limits for the regret that is fulfilled by each algorithm and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal because their asymptotic regret coincides with the lower limit. Regret analysis of our algorithms is based on a new concentration imbalance for weighted sums of KL divergences between the empirical distribution of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discredit the scope of action and then apply OSLB or CKL-UCB, algorithms that have been proven to efficiently exploit the structure. This approach is demonstrated by numerical experiments to significantly exceed existing algorithms that have to do directly with the continuous arms."}, {"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. Models", "text": "We are looking at a stochastic, multi-armed bandit problem in which the set of arms is a subset {x1,.., xK} of the interval [0, 1]. Results can easily be extended to the case in which the set of arms is a subset of a metric space, as considered in Kleinberg et al. (2008). The set of arms is finite cardinality, possibly large, and we assume without loss of generality that x1 < x2 <. < xK. Problems with continuous sets of arms are described in Section 7. Time passes in rounds indicated by n = 1, 2,... At each round, the decision maker selects an arm and observes the corresponding random reward. Arm xk is referred to as arm k for simplicity. For each k is the reward of the arm k in round n."}, {"heading": "3. Regret Lower Bound", "text": "In this section we deduce an asymptotic problem (when T grows large)."}, {"heading": "4. Algorithms", "text": "In this section, we present two algorithms for discrete Lipschitz bandit problems: The first of these algorithms, called OSLB (Optimal Sampling for Lipschitz Bandits), regrets that the algorithm 1 OSLB () is selected for all n \u2265 1 in such a way that: If it selects the arm k (n), then: If it is equal? (n) \u2265 maxk 6 = L (n) bk (n), then k (n) = L (n); if not (n) < K tk (n) (n) (n), then k (n) = k (n); Else k (n) = k (n).the lower limit derived in Theorem 1, i.e. asymptotically optimal. OSLB requires that in each round an LP similar to the (3) be solved; and the second algorithm, CKL-UCB (CombKLUCB), is much easier to guarantee, although it is much weaker to implement its structure."}, {"heading": "4.1. The OSLB Algorithm", "text": "(k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (k). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n. (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n).). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n). (n).). (n).). ("}, {"heading": "4.2. The CKL-UCB Algorithm", "text": "Next, we introduce the algorithm CKL-UCB (Combined KL - UCB). The sequential decisions made under CKL-UCB are based on the indices bk (n) and CKL-UCB examines the seemingly suboptimal weapons by first selecting the least played weapons. If the leader has the largest index, it is played, and otherwise we play the arm in {k: bk (n) > bL (n) (n)}, the group of weapons that may be better than the leader, with the least number of current games. Note that the forced log (n) exploration is unnecessary in practice and only helps in the regret analyses.The logic behind CKL-UCB is that if we get a set of suboptimal weapons by examining them, we first eliminate weapons whose expected reward is low (these weapons do not require many games to be eliminated)."}, {"heading": "5. Regret Analysis", "text": "In this section, we present time limits for the regrets achieved under OSLB and CKLUCB."}, {"heading": "5.1. Concentration Inequalities", "text": "In order to analyze the regret of the algorithms for bandit optimization problems, one often has to use the results related to the phenomenon of concentration of metrics. More specifically, in order to define the indices bk (n), we have to determine a concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions, which extends to the multidimensional case of the concentration inequality derived in Garivier (2013) for a single KL divergence. We believe that this inequality can be instrumental for the analysis of general structured bandit problems, as well as for statistical tests involving vectors whose components are distributed in an exponential family with a parameter (such as Bernoulli or Gaussian distributions)."}, {"heading": "5.2. Finite time analysis of OSLB", "text": "Next, we provide a finite time analysis of the regret achieved under OSLB under the following mild assumption, which greatly simplifies the analysis. Assumption 1 The solution of the LP (3) is unique. It should be noted that the set of parameters according to which assumption 1 is fulfilled represents a dense subset of the LP (3) for all > 0 under assumption 1. The regret achieved under assumption 1 fulfils: for all OLPs () for all OLPs, for all OLPs (0) and T (1), R\u03c0 (T) \u2264 (1 +) log (T) + C1 log (T) + K3 \u2212 1\u03b4 \u2212 2 + 3K\u03b4 \u2212 2, (6) where C\u043c (OLP) \u2192 C (OLP), as solution \u2192 0 +, and C1 > 0.Given the above theorem, OSLB () approaches the basic performance limit derived from theory 1. Specifically, we can find (OLLP) > C (LP) > L\u00d6C > (\u00d6T) > LP (LP) > (LP) > LP (LP) > (LP) is small enough."}, {"heading": "5.3. Finite Time analysis of CKL-UCB", "text": "To analyze the regret of CKL-UCB, we define the following optimization problem. Let's define the matrix of Kullback-Leibler divergence numbers A = (1,.., K), k \"K with aik = I.\" Consider the optimal value of the following linear program: min \"1,..., a subset of Arms N.\" (1,., K), k \"k,\" and \u03b10 \"K.\" We define the optimal value of the following linear program: min \"1,..., k\" k., \"k.\" k., \"k.\" (K), k. \"k.\" (K), k. \"k.\" (K), k. \"k.\" (K), k. \"k.\" (K), k. \"k.\" (K), k. \"k.\" (K), \"k.\" k. \"(K),\" k. \"(K),\" k. \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k.\" (K), \"k),\" k. \"k.\" (K \"k.\" (K), \"k.\" k. \"(K\"), \"k.\" (K), \"k.\" k. \"(K),\" k. \"(K (K),\" k), \"k.\" k. \"k.\" (k. \"),\" k. \"(k.\"), \"k.\" (k. \"),\" k. \"k.\" (k. \",\"), \"k.\" k. \"(\" k. \"), k.\" (k. \",\"), \"k.\" k. \",\", \"k.\", \"k.\" (\"k.,\", \"k.,\" k., \"k.,\" K (\"K (\" k), \"k.\", \"K (\" k), \"k.\", \""}, {"heading": "6. Contextual Bandit with Similarities", "text": "The above algorithms and results can be extended to the case of contextual bandit problems with similarities, as explored in Slivkins (2011). In such problems, the decision maker observes a context in each round and then decides which arm he chooses. The expected reward of the various weapons depends on the context and is assumed to be Lipschitz in arm and context. We assume that contexts arrive after an i.i.d. process, the distribution of which is unknown to the decision maker. This is in contrast to most work in contextual bandits, where the context process is counterproductive."}, {"heading": "6.1. Model", "text": "We assume that y1 <.. < yJ. For the sake of simplicity, the context yj is called context j. For each context, the expected reward is j. j. if the context is j.) We consider a general scenario in which the reward is a Lipschitz function both in the arm and in the context. There is L (known to the decision maker) so that for all (i, k), (j, l), (j), J \u00d7 K, | haftk (i) \u2212 l (j), the reward j."}, {"heading": "6.2. Regret Lower Bound", "text": "To specify the lower limit of regret, we introduce for each context the following suboptimal arms for the context: J, K, K, K, the vector (i, l, K, i, J). We also introduce for each context: L, 2: lim inf, T, R, R, K, the vector (i, l, K, i, J). Theorem 5 It is a uniformly good algorithm. Then, for each context: L, 2: lim inf, T, R, R, R, R, K, K, (9), where C, the lowest value of the following optimization problem is: L, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, the lowest value of the following optimization problem is: L, K, K, K, K, K, K, K, K, K, K, K, J, K, J, K, J, K, J, K, K, J, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, lowest value of the following optimization problem, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "6.3. Algorithms", "text": "The algorithms proposed for Lipschitz bandits can be naturally extended to the case of contextual bandits with similarities. To be concise, we only present CCKL-UCB (Contextual Combined KL - UCB), the extension of CKL-UCB. Its repentance analysis can be performed as that of CKL-UCB with minor changes. To describe CCKL-UCB, we introduce the following notations. Let us define the index bck (j, n) of the arm k for the arm k for the context j to round n \u2212 1. tk (j, n) is the number of times context j is represented and arm k is represented until round n \u2212 1. We define the index bck (j, n) of the arm k for round n, when the context j is observed for the context j to round n, as: bck (n, j) the context j for the context j (n), j = the context {q, j, the context (n)."}, {"heading": "7. Numerical Experiments", "text": "In this section, we present numerical experiments that demonstrate the performance of our algorithms compared to other existing algorithms."}, {"heading": "7.1. Discrete Lipschitz Bandits", "text": "In Figure 2, we compare the performance of KL-UCB and CKL-UCB. To improve numerical performance, we ignore the log (n) terms in the indices (i.e. f (n) = log (n)) for both algorithms. On the left, we represent the expected reward as a function of the arm, as well as the (scaled) number of times E [tk (n)] / log (n) suboptimal arm k is played as a function of time under both algorithms. Under KL-UCB, the number of times for arm 1 / I (successk, \u03b8?) approaches, while under CKL-UCB E [tk (n)] the upper limits of KUCB are considered satisfactory."}, {"heading": "7.2. Continuous Lipschitz Bandits", "text": "We consider two reward functions that behave differently around their maximum: (1) \u03b8 (x) = 0.8 \u2212 0.5 \u2212 x | (triangle) and (2) \u03b8 (x) = max. (0.1, 0.9 \u2212 3.2 \u0445 (0.7 \u2212 x) 2) (square function). To adapt KL-UCB and CKL-UCB to this continuous setting, we use uniform arm discrediting, using uniform arm discrediting with \u03b4 \u2212 1 = d \u221a T / log (T) e arms. This discrediting is known to be optimal for functions that are regularly around their maximum Kleinberg (2004). To avoid giving a positive bias to KL-UCB and CKL-UCB, we ensure that the maximum does not reach the maximum of the reward functions of the reward functions."}, {"heading": "8. Conclusion", "text": "We look at stochastic multi-armed bandits (discrete or continuous), where the expected reward is a Lipschitz function of the arm. For discrete Lipschitz bandits, we derive asymptotic lower limits for the remorse achieved under each algorithm. We propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure. OSLB is asymptotically optimal and CKL-UCB is a computationally lightweight algorithm that efficiently utilizes the Lipschitz structure. The remorse analysis is based on a new concentration imbalance for sums of KL divergences that may be relevant to bandit problems with correlated weapons. For continuous Lipschitz bandits, we adapt OSLB and CKL-UCB by a simple discretization. For both discrete and continuous bandits, numerical initial experiments clearly exceed our algorithms."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "In order to establish the asymptotic lower limit, we apply the techniques used in Graves and Lai (1997) to investigate efficient adaptive decision rules in controlled Markov chains. Let us recall their general framework. Let us consider a controlled Markov chain (Xt) t \u2265 0 to a finite state space S with a control clause U. The transition probabilities are parameterized by taking values in a compact metric space K. The probability of moving from state x to state y is p (x, y; u, \u03b8). The parameters are not known. The decision maker is assumed to be assigned to a finite series of stationary control laws G = {g1,., gK} where each control law gj is an assignment from S to U: if control law gj is applied in state x, the control applied is u = gj (x). It is assumed that if the decision maker always chooses the same control law, the markov chain is reductible with the corresponding one."}, {"heading": "Appendix B. Proof of Theorem 2", "text": "In this section we first ascertain the concentration inequality under the assumption that Lemma 6 = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (the case in which for some k, tk (n) = 0 is treated in a similar way. Evidence for Theorem 2: K + 1 Tk (n) I + 1 and 0. We define D = dlog (n) / log (1 + 1) e, and the set D = {1,.,., D} K \u2212 K \u2212 K = 1 tk (n) I + (n) k = K (n), D = Lemk). We have k = K = K = K = 1 on k."}, {"heading": "Appendix C. Proof of Theorem 3", "text": "We first present two important results of our concentration inequality (theorem 2). (k) We leave f (n) = log (n) + (3K + 1) log log (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (n) p (p) p (n) p (p) p (p) p (n) p (p) p (p) p (n) p (p) p (p) p (n) p (p) p (n) p (p) p) p (n) p (p) p (n) p (p) p (p) p) p (p) p (n) p (p) p) p (p) p (n) p (p) p (p) p) p (n) p (p) p (p) p) p (n) p (p) p) p (n) p (p) p (p) p) p (n) p (p) p (n) p (p) p) p (n) p (p) p (p) p (n) p (p) p (n) p (p (p) p) p (n) p (n) p (p) p (p (n) p (p (p) p) p (p) p (p (p) p (n) p (p (p) p (p) p (n) p) p (p (p (p) p (n) p (p (p (p) p) p (n) p (p (p (p) p (n) p (p) p (p (p) p (p) p (p (p) p (n) p (p (p) p (p) p (p) p) p (p (p) p (p) p (p) p (p) p (p) p (p (p) p (p) p (p) p (p) p (p) p (p ("}, {"heading": "Appendix D. Proof of Theorem 4", "text": "D.1. Proof of (i) Let's 0 < (c) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) k (k) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (k) n (n) n (n) n (k) n (n) n (n) n (n) n (n) n (k) n (k) n (k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k (k) k) k (k) k) k (k) k (k) k) k (k) k (k) k (k) k) k (k) k) k (k) k (k) k (k) k) k (k) k (k) k) k (k) k (k) k) k (k) k) k (k) k) k (k) k (k) k) k (k) k) k (k) k) k (k) k (k) k) k (k) k) k (k) k) k (k) k) k (k) k) k) k) k (k) k (k) k) k (k) k (k) k) k) k (k) k) k (k) k) k) k (k) k (k) k) k (k)"}], "references": [{"title": "The continuum-armed bandit problem", "author": ["R. Agrawal"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bubeck et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Kullback-leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2013}, {"title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "A. Proutiere"], "venue": "In Proc. of ICML,", "citeRegEx": "Combes and Proutiere.,? \\Q2014\\E", "shortCiteRegEx": "Combes and Proutiere.", "year": 2014}, {"title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "author": ["R. Combes", "A. Proutiere"], "venue": "Technical Report, people.kth.se/ \u0303alepro/pdf/tr-icml2014.pdf,", "citeRegEx": "Combes and Proutiere.,? \\Q2014\\E", "shortCiteRegEx": "Combes and Proutiere.", "year": 2014}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In Proc. of ACM/SIAM symposium on Discrete Algorithms (SODA),", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Informational confidence bounds for self-normalized averages and applications", "author": ["A. Garivier"], "venue": "In Information Theory Workshop,", "citeRegEx": "Garivier.,? \\Q2013\\E", "shortCiteRegEx": "Garivier.", "year": 2013}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Garivier and Capp\u00e9.,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Capp\u00e9.", "year": 2011}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Graves and Lai.,? \\Q1997\\E", "shortCiteRegEx": "Graves and Lai.", "year": 1997}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["R. Kleinberg"], "venue": "In Proc. of the conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg.", "year": 2004}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "In Proc. of the 40th annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics, 15(3):1091\u20131114,", "citeRegEx": "Lai.,? \\Q1987\\E", "shortCiteRegEx": "Lai.", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Comparison Methods for Stochastic Models and Risks", "author": ["A. M\u00fcller", "D. Stoyan"], "venue": null, "citeRegEx": "M\u00fcller and Stoyan.,? \\Q2002\\E", "shortCiteRegEx": "M\u00fcller and Stoyan.", "year": 2002}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "In Proc. of Conference On Learning Theory (COLT),", "citeRegEx": "Slivkins.,? \\Q2011\\E", "shortCiteRegEx": "Slivkins.", "year": 2011}, {"title": "Continuity of solutions to parametric linear programs We state and prove Lemma 13, a technical result about the continuity of the solutions of a parametric linear program with respect to its parameters. It follows from the general conditions", "author": [], "venue": "Lemma 13 Consider", "citeRegEx": "C.1.,? \\Q1985\\E", "shortCiteRegEx": "C.1.", "year": 1985}, {"title": "D, so that both K and D have non-empty interior. By Wets (1985)[Corollary 7], t \u2192 K and t \u2192 D are continuous on T since they have non-empty interior and all rows of (A, 1) and columns", "author": ["K . Then B \u2282 K"], "venue": null, "citeRegEx": "\u2229,? \\Q1985\\E", "shortCiteRegEx": "\u2229", "year": 1985}], "referenceMentions": [{"referenceID": 9, "context": "Introduction In their seminal paper, Lai and Robbins (1985) solve the classical stochastic Multi-Armed Bandit (MAB) problem.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al.", "startOffset": 45, "endOffset": 122}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.", "startOffset": 45, "endOffset": 143}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.", "startOffset": 45, "endOffset": 226}, {"referenceID": 1, "context": "The most popular of these algorithms are UCB Auer et al. (2002) and its extensions, e.g. KL-UCB Garivier and Capp\u00e9 (2011), Capp\u00e9 et al. (2013) \u2013 note that the KL-UCB algorithm was initially proposed and analysed in Lai (1987), see (2.6). When the expected rewards of the various arms are not related as in Lai and Robbins (1985), the regret of the best algorithm essentially scales as O(K log(T )) where K denotes the number of arms, and T is the time horizon.", "startOffset": 45, "endOffset": 329}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 52}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al.", "startOffset": 12, "endOffset": 74}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al.", "startOffset": 12, "endOffset": 101}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm.", "startOffset": 12, "endOffset": 135}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 711}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 736}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound.", "startOffset": 12, "endOffset": 758}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure.", "startOffset": 12, "endOffset": 861}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al.", "startOffset": 12, "endOffset": 1340}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al.", "startOffset": 12, "endOffset": 1365}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms.", "startOffset": 12, "endOffset": 1387}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure.", "startOffset": 12, "endOffset": 1699}, {"referenceID": 0, "context": ", Lipschitz Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008), linear Dani et al. (2008), and convex Flaxman et al. (2005). In this paper, we revisit bandit problems where the expected reward is a Lipschitz function of the arm. The set of arms is a subset of [0, 1] and we address both discrete Lipschitz bandits where this set is finite, and continuous Lipschitz bandits where this set is [0,1]. For discrete Lipschitz bandits, we derive problem specific regret lower bounds, and propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. Most previous work on Lipschitz bandit problems address the case where the set of arms is [0,1], Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008). For these problems, there is no known problem specific regret lower bound. In Kleinberg et al. (2008), a regret lower bound is derived for the worst Lipschitz structure. The challenge in the design of efficient algorithms for continuous Lipschitz bandits stems from the facts that such algorithms should adaptively select a subset of arms to sample from, and based on the observed samples, establish tight confidence intervals and construct arm selection rules that optimally exploit the Lipschitz structure revealed by past observations. The algorithms proposed in Agrawal (1995), Kleinberg et al. (2008), Bubeck et al. (2008) adaptively define the set of arms to play, but used simplistic UCB indexes to sequentially select arms. In turn, these algorithms fail at exploiting the problem structure revealed by the past observed samples. For continuous bandits, we propose to first discretize the set of arms (as in Kleinberg et al. (2008)), and then apply OSLB, an algorithm that optimally exploits past observations and hence the problem specific structure. As it turns out, this approach outperforms algorithms directly dealing with continuous sets of arms. Our contributions. (a) For discrete Lipschitz bandit problems, we derive an asymptotic regret lower bound satisfied by any algorithm. This bound is problem specific in the sense that it depends in an explicit manner on the expected rewards of the various arms (this contrasts with existing lower bounds for continuous Lipschitz bandits). (b) We propose OSLB (Optimal Sampling for Lipschitz Bandits), an algorithm whose regret matches our lower bound. We further present CKL-UCB (Combined KL-UCB), an algorithm that exhibits lower computational complexity than that of OSLB, and that is yet able to exploit the Lipschitz structure. (c) We provide a finite time analysis of the regret achieved under OSLB and CKL-UCB. The analysis relies on a new concentration inequality for a weighted sum of KL divergences between the empirical distributions of rewards and their true distributions. We believe that this inequality can be instrumental for various bandit problems with structure. (d) We evaluate our algorithms using numerical experiments for both discrete and continuous sets of arms. We compare their performance to that obtained using existing algorithms for continuous bandits. (e) We extend our results and algorithms to the case of contextual bandits with similarities as investigated in Slivkins (2011).", "startOffset": 12, "endOffset": 3230}, {"referenceID": 11, "context": "Results can be easily extended to the case where the set of arms is a subset of a metric space as considered in Kleinberg et al. (2008). The set of arms is of finite cardinality, possibly", "startOffset": 112, "endOffset": 136}, {"referenceID": 13, "context": "Without loss of generality, we restrict our attention to so-called uniformly good algorithms, as defined in Lai and Robbins (1985). \u03c0 \u2208 \u03a0 is uniformly good if for all \u03b8 \u2208 \u0398L, R\u03c0(T ) = o(T a) for all a > 0.", "startOffset": 108, "endOffset": 131}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix.", "startOffset": 92, "endOffset": 114}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (\u03b8k, k \u2208 K) are not related (i.", "startOffset": 92, "endOffset": 342}, {"referenceID": 10, "context": "The regret lower bound is a consequence of results in optimal control of Markov chains, see Graves and Lai (1997). All proofs are presented in appendix. As in classical bandits, the minimal regret scales logarithmically with the time horizon. Observe that the lower bound (2) is smaller than the lower bound derived in Lai and Robbins (1985) when the various average rewards (\u03b8k, k \u2208 K) are not related (i.e., in absence of the Lipschitz structure). Hence (2) quantifies the gain one may expect by designing algorithms optimally exploiting the structure of the problem. Note that for any k \u2208 K\u2212, the variable ck corresponding to a solution of (3) characterizes the number of times arm k should be played under an optimal algorithm: arm k should be roughly played ck log(n) times up to round n. It should be also observed that our lower bound is problem specific (it depends on \u03b8), which contrasts with existing lower bounds for continuous Lipschitz bandits, see e.g. Kleinberg et al. (2008). The latter are typically derived by selecting the problems that yield maximum regret.", "startOffset": 92, "endOffset": 991}, {"referenceID": 8, "context": "The latter extends to the multi-dimensional case the concentration inequality derived in Garivier (2013) for a single KL divergence.", "startOffset": 89, "endOffset": 105}, {"referenceID": 15, "context": "M\u00fcller and Stoyan (2002).", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Contextual Bandit with Similarities The algorithms and results presented above can be extended to the case of contextual bandit problems with similarities as studied in Slivkins (2011). In such problems, in each round, the decision maker observes a context, and then decides which arm to select.", "startOffset": 169, "endOffset": 185}, {"referenceID": 8, "context": "This discretization is known to be order-optimal for functions which are regular around their maximum Kleinberg (2004). In order not to give a positive bias to KL-UCB and CKL-UCB, we make sure that the maximum of the reward functions is not achieved in one of the arms in the discretization: the maximum is placed at a distance of at least \u03b4/4 from any arm in the discretization.", "startOffset": 102, "endOffset": 119}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal).", "startOffset": 92, "endOffset": 176}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al.", "startOffset": 92, "endOffset": 443}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to \u221a log(n)/(2 \u2217 tk(n)) in round n.", "startOffset": 92, "endOffset": 471}, {"referenceID": 2, "context": "We compare the performance of KL-UCB and CKL-UCB to that of the algorithm HOO introduced in Bubeck et al. (2008), and the Zooming algorithm proposed in Kleinberg et al. (2008). The two latter algorithms have performance guarantees (they are order-optimal). We also compare KL-UCB and CKL-UCB to HOO+ and Zooming+, two improved versions of HOO and Zooming, respectively. In these tuned versions, the confidence radius (see Bubeck et al. (2008) and Kleinberg et al. (2008) for details) is set equal to \u221a log(n)/(2 \u2217 tk(n)) in round n. HOO+ and Zooming+ exhibit better performance than their initial versions, but their regrets have not been analytically studied. In the experiments, we limit the time horizon to T = 25000 rounds, and the expected regret is calculated by averaging over 100 independent runs. Figure 3 presents the expected regret of the various algorithms for the triangular reward function (left) and for the quadratic reward function (right). First note that surprisingly, KL-UCB, an algorithm that does not leverage the Lipschitz structure, outperforms some of the algorithms designed to exploit the structure. Observe that CKL-UCB clearly outperforms KL-UCB and all other algorithms in both problem instances. For quadratic reward functions, it is known that the optimal discretization of the set of arms should roughly have (log(T )/T )1/4 arms, Combes and Proutiere (2014a). We also plot the regret achieved under CKL-UCB using this optimized discretization, and we observe that this indeed further reduces the regret.", "startOffset": 92, "endOffset": 1394}], "year": 2014, "abstractText": "We consider stochastic multi-armed bandit problems where the expected reward is a Lipschitz function of the arm, and where the set of arms is either discrete or continuous. For discrete Lipschitz bandits, we derive asymptotic problem specific lower bounds for the regret satisfied by any algorithm, and propose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz structure of the problem. In fact, we prove that OSLB is asymptotically optimal, as its asymptotic regret matches the lower bound. The regret analysis of our algorithms relies on a new concentration inequality for weighted sums of KL divergences between the empirical distributions of rewards and their true distributions. For continuous Lipschitz bandits, we propose to first discretize the action space, and then apply OSLB or CKL-UCB, algorithms that provably exploit the structure efficiently. This approach is shown, through numerical experiments, to significantly outperform existing algorithms that directly deal with the continuous set of arms. Finally the results and algorithms are extended to contextual bandits with similarities.", "creator": "LaTeX with hyperref package"}}}