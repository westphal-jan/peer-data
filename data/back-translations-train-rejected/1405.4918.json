{"id": "1405.4918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "Fighting Authorship Linkability with Crowdsourcing", "abstract": "Massive amounts of contributed content -- including traditional literature, blogs, music, videos, reviews and tweets -- are available on the Internet today, with authors numbering in many millions. Textual information, such as product or service reviews, is an important and increasingly popular type of content that is being used as a foundation of many trendy community-based reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown that, due partly to their specialized/topical nature, sets of reviews authored by the same person are readily linkable based on simple stylometric features. In practice, this means that individuals who author more than a few reviews under different accounts (whether within one site or across multiple sites) can be linked, which represents a significant loss of privacy.", "histories": [["v1", "Mon, 19 May 2014 23:26:44 GMT  (2354kb,D)", "http://arxiv.org/abs/1405.4918v1", null]], "reviews": [], "SUBJECTS": "cs.DL cs.CL", "authors": ["mishari almishari", "ekin oguz", "gene tsudik"], "accepted": false, "id": "1405.4918"}, "pdf": {"name": "1405.4918.pdf", "metadata": {"source": "CRF", "title": "Fighting Authorship Linkability with Crowdsourcing", "authors": ["Mishari Almishari", "Ekin Oguz", "Gene Tsudik"], "emails": [], "sections": [{"heading": null, "text": "First, we try to harness the global power of crowdsourcing by involving random strangers in the process of rewriting reviews. As our empirical results (obtained from Amazon Mechanical Turk) clearly show, crowdsourcing delivers impressively reasonable ratings that reflect sufficiently different stylistic characteristics to render earlier stylometric linkage techniques largely ineffective. We also consider using machine translation to automatically rewrite reviews. Contrary to what has been assumed so far, our results show that the linkability of authorship decreases with increasing numbers of interlocutors. Finally, we examine the combination of crowdsourcing and machine translation and report on outcomes. Keywords Copyright Assignment, Authorship Attribution, Authorship Anonymization, Crowdsourcing, Stylemetrics"}, {"heading": "1. INTRODUCTION", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2. RELATED WORK", "text": "There are many studies in the literature. For example, [20] shows that many Yelp reviewers are linkable with only very simple feature set. While the setting is similar to ours, there are some notable differences. First, we get high linkability with very few reviews per author. Second, we rely only on features that are extracted from review text. A study of blog posts achieves 80% linkability accuracy [22]. Author identification is also in the context of academic reviews, the accuracy of the work reaching 90% [21]. A big difference between these studies and our work is that we use reviews that are shorter, less formal and less restrictive in word choice than blogs and scientific papers. Abbasi and Chen suggest a well-known author assignment technique that is based on Karhunen's stameves to extract a large list of writers."}, {"heading": "3. BACKGROUND", "text": "Merriam-Webster dictionary defines styometry as: the study of an author's chronology and development, based in particular on the repetition of certain expressions or currents of thought. We use styometry in conjunction with the following two tools: Writeprints feature set: known stylometric features used to analyze the author's writing style. Chi-square test: a technique that calculates the discrepancy between reviews of each author to assess linkability."}, {"heading": "3.1 Writeprints", "text": "Writing impressions are essentially a combination of static and dynamic stylometric features that capture lexical, syntactic, structural, textual, and idiosyncratic properties of a given body of text [10]. Some features include: \u2022 Average letter per word: Total number of characters divided by total number of words. \u2022 Top letter trigrams: Frequency of consecutive sequences of 3 characters, e.g. aaa, aab, aac,..., zzy, zzz. There are 17576 (263) possible permutations of letter trigrams in English. \u2022 Part of the language (POS) Tag Bigrams: POS tags are the mapping of words to their syntactic behavior within a sentence, e.g. noun or verb. POS tag bigrams denote two consecutive parts of language tags. We used Stanford POS Maxent Taggers [27] to reuse each word with a set of 45 possible phrases of POS 5chtags."}, {"heading": "3.2 Chi-Squared Test", "text": "The chi square test (CS) is used to measure the distance between two distributions [25]. For any two distributions P and Q, it is defined as follows: CSd (P, Q) = \u2211 i (P (i) \u2212 Q (i)) 2 P (i) + Q (i) CSd is a symmetrical measure, i.e. CSd (P, Q) = CSd (Q, P). Moreover, it is always not negative; a value of zero means that P and Q are identical distributions."}, {"heading": "4. LINKABILITY STUDY PARAMETERS", "text": "This section describes the data set and the problem definition for our link analysis."}, {"heading": "4.1 Dataset", "text": "We use a large dataset of reviews from Yelp1 with 1, 076, 850 reviews written by 1, 997 different contributors. We select this particular dataset for two reasons: 1. A large number of authors with vastly different number of reviews: The average number of reviews per author is 539, with a standard deviation of 354.2."}, {"heading": "4.2 Problem Setting", "text": "The problem becomes difficult when the number of anonymous and identified reviews is relatively small, and the exact problem is as follows: We first randomly select 40 authors, and we select this relatively small number to make subsequent crowdsourcing experiments feasible, as in Section 6. Then we randomly mix their reviews for each author and select the first N. Next, we divide the selected reviews into two groups: \u2022 The first X reviews form the Anonymous Record (AR) group. We experiment with AR reviews of varying sizes. \u2022 Subsequent (N \u2212 X) reviews form the Identified Record (IR) group. Our problem is reduced to linking ARs to the corresponding IRs. We set N = 100 and vary X from 1 to 5. This makes our IRs and ARs relatively small compared to an average of 539 reviews per author in the data set."}, {"heading": "5. LINKABILITY ANALYSIS", "text": "First, we use a subset of the popular Writeprints function set2 to convert each AR and IR into a token set. Then, we use Chi-Square3 to calculate the distances between these token sets. We now describe our methodology in more detail. Notation and abbreviations can be found in Table 1."}, {"heading": "5.1 Methodology", "text": "First, we tokenize each AR and IR set using each attribute - F - in our set of selected attributes - SF - to obtain a set of tokens FT = {FT1, FT2,..., FTn}, where FTi denotes the i-th token in FT. Then, we calculate distributions for all tokens. Next, we calculate the distance between AR and IR based on the respective token distributions. Specifically, to link AR to any feature F, we calculate CSd between the distribution of tokens in FT for AR and the distribution of tokens in FT for each IR. Then, we sort the distances in ascending order of the CSd values (IR, AR) and return the resulting list. The first entry corresponds to the IR with the closest distance to AR, i.e., the most likely match of tokens in FT for each IR. For more generality in our analysis, we repeat this experiment by selecting different IR and IR values three times."}, {"heading": "5.2 Feature Selection", "text": "The idea is to identify the most influential features and gradually combine them in SF until we come across a high LR.5.2.1 WPall. As a benchmark, we start by setting the features SFto WPall, which combine all 22 write features. We calculate LR using WPall in the CS model with | AR | = 5. Unfortunately, WPall results are in low LRs - only 52.5% in the top 1 and 82.5% in the top 4. We believe that combining many features increases noise, which in turn increases linkability.2 Improving WPall features in low LRs - only 52.5% in the top 1 and 82.5% in the top 4."}, {"heading": "5.4 Summary", "text": "In summary, our main results can be summarized as follows: 1. We started with a well-known Writeprints feature set and achieved modest LRs of up to 52.5% in Top-1 and 82.5% in Top-4 using the CS model. (See Section 5.2.1) 2. We then tried each Writeprint feature individually with the intuition that combining several features would have more noise, which would reduce linkability. Surprisingly, we only achieved significantly better LR than all Writeprints features with Top-Letter trigrams or POS bigrams. (See Section 5.2.2) 3. Next, we chose Top Letter trigrams, which yielded 91% and 96% LR in Top-1 and Top-4 as our aspirational main goal. Then, we increased linkability to 96% in Top-1 and 100% in Top-4, using BigLetter trigrams, which yielded 91% and 96% LR in Top-1 and Top-4. Then we increased linkability to 96% in Top-1 and 100% in Top-4, using BigLetter trigrams, which yielded the number of possible LR in Top-7 resolution (see 2.7)."}, {"heading": "6. FIGHTING AUTHORSHIP LINKABILITY", "text": "We now come to the main objective of this paper: to explore techniques that reduce the linkability of authorship. We will consider two general approaches: 1. Crowdsourcing: described in Section 6.1.2. Machine translation: described in Section 6.2."}, {"heading": "6.1 Crowdsourcing to the Rescue", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able to move, and in which they will be able to move to another world, in which they are able to move, in which they are in."}, {"heading": "7A sample readability study task and its submission are pre-", "text": "We note that most MTurkers do not change the skeleton of the original review. Instead, they change the structure of individual sentences by changing the order of subject, noun, and verb, and converting an active sentence into a passive sentence, or vice versa. We also observe that MTurker's words are interchanged with synonyms. We believe that these results can be combined into an automated tool that can help authors rewrite their own reviews, which is one of the elements for future work discussed in more detail in Section 7.6.1.6. We will now summarize the most important results from the crowdsourcing experiment. 1. MTurk-based crowdsourcing yields rewritten reviews that are low-cost - we paid only $0.12 including a 10% service fee for rewriting each 250-word article."}, {"heading": "6.2 Translation Experiments", "text": "The goal, however, is to assess the effectiveness of the translation for stylometric obfuscation and see if it can be integrated into a single socio-technological linkage technology in combination with crowdsourcing. However, it is both natural and intuitive to consider machine (automated, on-line) translation for obfuscating stylometric features of reviews. A well-known technique is to gradually translate the text into a sequence of languages and then translate back to the original language. For example, translating a review from (and into) English using three levels of translation could be done as follows: English \u2192 German \u2192 Japanese \u2192 English. The main intuition is that we use the online translator as an external re-author, so that stylometric features would change as a translator."}, {"heading": "7. DISCUSSION", "text": "Recent years have shown that the number of unemployed has been many times higher in the last ten years than the number of unemployed in the last ten years."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "This work examined the linkability of authorship in the review of the community and explored some ways to mitigate it. First, we demonstrated that the linkability of authorship is higher than previously reported through a linkability study using a proper subset of the Writeprint feature set. Then, we published reviews and asked random strangers to rewrite them for a nominal fee. Then, we conducted a readability study that showed that newly written reviews are meaningful and remain similar to the originals. Then, we reviewed the linkability of newly written reviews and found that it decreases significantly. Next, we considered rewriting the readability of reviews and showed that linkability decreases as the number of intermediate languages increases. Af-10https: / / www.mturk.com / mturk.com / conditionsofuseter that, we evaluated the readability of translated reviews and recognized the online translation results as unreadable."}, {"heading": "9. REFERENCES", "text": "[1] Amazon Mechanical Turk. http: / / www.mturk.com / mturk /. [2] Bing Translator. http: / / www.bing.com / translator. [3] Bing Translator Language Codes. http: / / msdn. microsoft.com / en-us / library / hhh456380.aspx. [4] Fighting Authorship Linkability with Crowdsourcing, Extended Version. https: / / www.dropbox.com / s / trx7skoja6j7xo6 / userhide-extended.pdf. [5] Google Translate.Kite.google.com /."}, {"heading": "Appendix A: Crowdsourcing Examples", "text": "Note that the full collection of original and rewritten reviews can be found in [7].A.1: Rewriting Example Review for the task given in Figure 8 (a): \"When we arrived, the line was all the way around the block, so we were more than willing to sit with strangers. It didn't bother me the most. What bothered me the most was that we were far back in the establishment. As the shopping cart crowded to help us, they had nothing to eat except chicken feet. It took half an hour for us to get the attention of the staff to tell them we needed to be fed. The food was delicious, or it was my hunger that whetted my appetite. I chose eggplants, which was better in China Garden. But Chinese-American women came in with their mothers."}, {"heading": "Appendix B: Translation Examples", "text": "We present the machine translation of the original translation in Figure 8 (a) with three, six and nine random choices between the languages, and also the translation-fixed version of MTurk below: Random machine translation of the outbound and return journey with three languages English \u2212 \u2192 Welsh \u2212 \u2192 Irish \u2212 Ukrainian \u2212 \u2192 English: \"The line was all the way down. We're ready to sit down with other people. It wasn't a problem. It was that we were sitting at the table, and the way in the back longer. So, if the car even bothers to come back to us, they run out of food (surprisingly still chicken feet). Finally, when we passed half an hour, the food was good. Nothing stimulates the appetite like hunger. Best stuffed eggplants in the garden of China. And many Chinese-American girls and their mothers come back to us, run out of food (surprisingly still chicken feet)."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Massive amounts of contributed content \u2013 including tradi-<lb>tional literature, blogs, music, videos, reviews and tweets<lb>\u2013 are available on the Internet today, with authors number-<lb>ing in many millions. Textual information, such as product<lb>or service reviews, is an important and increasingly popular<lb>type of content that is being used as a foundation of many<lb>trendy community-based reviewing sites, such as TripAd-<lb>visor and Yelp. Some recent results have shown that, due<lb>partly to their specialized/topical nature, sets of reviews au-<lb>thored by the same person are readily linkable based on sim-<lb>ple stylometric features. In practice, this means that indi-<lb>viduals who author more than a few reviews under different<lb>accounts (whether within one site or across multiple sites)<lb>can be linked, which represents a significant loss of privacy.<lb>In this paper, we start by showing that the problem is<lb>actually worse than previously believed. We then explore<lb>ways to mitigate authorship linkability in community-based<lb>reviewing. We first attempt to harness the global power of<lb>crowdsourcing by engaging random strangers into the pro-<lb>cess of re-writing reviews. As our empirical results (ob-<lb>tained from Amazon Mechanical Turk) clearly demonstrate,<lb>crowdsourcing yields impressively sensible reviews that re-<lb>flect sufficiently different stylometric characteristics such that<lb>prior stylometric linkability techniques become largely inef-<lb>fective. We also consider using machine translation to auto-<lb>matically re-write reviews. Contrary to what was previously<lb>believed, our results show that translation decreases author-<lb>ship linkability as the number of intermediate languages grows.<lb>Finally, we explore the combination of crowdsourcing and<lb>machine translation and report on the results.", "creator": "LaTeX with hyperref package"}}}