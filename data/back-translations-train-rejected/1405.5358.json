{"id": "1405.5358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2014", "title": "Off-Policy Shaping Ensembles in Reinforcement Learning", "abstract": "Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel with- out sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble.", "histories": [["v1", "Wed, 21 May 2014 10:20:15 GMT  (828kb,D)", "http://arxiv.org/abs/1405.5358v1", "Full version of the paper to appear in Proc. ECAI 2014"]], "COMMENTS": "Full version of the paper to appear in Proc. ECAI 2014", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["anna harutyunyan", "tim brys", "peter vrancx", "ann nowe"], "accepted": false, "id": "1405.5358"}, "pdf": {"name": "1405.5358.pdf", "metadata": {"source": "CRF", "title": "Off-Policy Shaping Ensembles in Reinforcement Learning", "authors": ["Anna Harutyunyan", "Peter Vrancx", "Ann Now\u00e9"], "emails": ["anowe}@vub.ac.be"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2 Background", "text": "The environment of an RL agent is usually modeled as a Markov decision process (1 value), Q functions are calculated, Q functions are calculated (23] given by a quadruple < S, A, T, R >, where S is the set of states, A is the set of actions available to the agent, T: S \u00b7 A \u00b7 S \u2192 R is the transition function with T (s, a, s \u2032) indicating the probability of ending in a state s, and R: S \u00b7 A \u00b7 S \u2192 R is the reward function with R (s, s \u2032) indicating the expected reward at the transition from s to s \u2032 after taking action a. Markovian's assumption is that st + 1 and the reward rt + 1 depends only on st and at, where t denotes the discrete time step. A stochastic policy: S \u00b7 A \u2192 R defines a probability distribution for actions in each state sp:"}, {"heading": "2.1 Horde", "text": "Unfortunately, FA may cause non-political boat-strapping methods such as Q-Learning to differ from each other even in the case of simple problems [1, 27]. The family of gradient effect time difference algorithms (GTD) solves this problem for the first time, while maintaining the constant complexity per step, provided that it provides for a firm (or slowly changing) behaviour [25, 17]. They achieve this by introducing a gradient effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect in the TD update. Mechanically, they require the maintenance and the learning of a second series of weights w, together with the following rules for updating the effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect TD fix point by introducing a gradient effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect TD update. The following rules are applicable: 3\u03b8t + 1 Horse parentage-effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect effect-effect-effect-effect-effect-effect-effect-effect-prediction-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-prediction-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-prediction-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-update-effect-effect-effect-effect-effect-effect-TD update-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-effect-"}, {"heading": "2.2 Reward shaping", "text": "The Reward Form expands the true reward signal with an additional heuristic reward provided by the designer. It was originally conceived as a way to expand the methods of the Reward Form to address difficult problems [7], as RL generally suffers from unfeasibly long learning periods. However, if applied lightly, it can slow down or even prevent the finding of the optimal policy [28]. Ng et al. [21] show that the grounding of the reward function in state potentials is both necessary and sufficient to ensure the preservation of the (optimal) strategies of the original MDP. The potential-based reward formula retains a potential function \u0445: S \u2192 R and defines the auxiliary reward function F as: F (s, a, s \u2032) = quasi (s) policy (9), where the most important discounting factor is."}, {"heading": "3 Ensembles of Shapings", "text": "In this section, we further explain why we find the Horde to be a well-suited framework for ensemble learning; certainly only if general learning methods differ from Q functions, and why measures achieved through potentially based reward are good candidates for such an ensemble learning. Ensemble techniques such as cranking [9] and packing [3] are often used in practice as effective methods to reduce prejudice and deviation from solutions. Ensembles in RL have been extremely sparse to date. Most previous uses of ensemble policies that run independently of each other, combined with the combination that happens post-factum [8], are limited in practical application as it requires a large computational and exemplary overhead translation, adopts a repeatable setup and does not improve learning speed. Other, general, lack of convergence guarantees, 5 either with mixed and political learners [31] or under LearningLearnners [4-Q]."}, {"heading": "4 Architecture", "text": "The reward function is a vector: R = < R + F0, R + F1,..., R + F | D | \u2212 1 >, where F0 = 0 (d0 always learns only on the basis of the reward), andFi, i = 1,.., | D | \u2212 1 are potentially based rewards provided by (9) on potentials provided by the designer. We adopt the terminology of Sutton et al. [26], and designate individual players within the horde as demons. Each demon learns a greedy policy \u03c0i w.r.t. its reward Ri. We refer to the demons learning on shaped rewards as shaped rewards. At each point of learning we can design a combination policy by choosing the adjustments based on all formed demons (d1, d2, etc.)."}, {"heading": "5 Experiments", "text": "We remind the reader that while all measures ultimately lead to the same (optimal) solution, our focus is the time it takes to get there. We focus our attention on a classic benchmark domain of the mountain car [24]. The task is to drive an underpowered car up a hill (fig. 2). The (continuous) state of the system is composed of the current position (in [\u2212 1,2, 0.6]) and the current speed (in [\u2212 0.07, 0.07]) of the car. Measures are discreet, a throttling of {\u2212 1, 0}. The agent starts at position \u2212 0.5 and a speed of 0, and the target is at position 0.6. The rewards are \u2212 1 for each time step. An episode ends when the goal is reached, or when 2000 steps are taken."}, {"heading": "6 Conclusions and future work", "text": "We have the first political system that is both solid and able to learn in real time, harnessing the power of Horde architecture to learn a single policy. Value works in our ensemble11 An artifact of valuable methods learned outside the political framework will move ever closer to the attainable optimum, but we will not address the context of this paper, as our main focus is on improving learning time within the extra-parliamentary framework. We have used a voting method to combine them."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Anna Harutyunyan is supported by the IWT-SBO project MIRAD (grant no. 120057). Tim Brys is supported by a doctoral scholarship of the Research Foundation Flanders (FWO)."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning, pp. 30\u201337. Morgan Kaufmann, ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto", "P. Kaelbling"], "venue": "Machine Learning, pp. 22\u201333, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn., 24(2), 123\u2013140, ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Multi-objectivization in reinforcement learning", "author": ["T. Brys", "A. Harutyunyan", "P. Vrancx", "M.E. Taylor", "D. Kudenko", "A. Now\u00e9"], "venue": "Technical Report AI-TR-13-354, AI Lab, Vrije Universiteit Brussel, ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Off-policy actor-critic", "author": ["T. Degris", "M. White", "R.S. Sutton"], "venue": "Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML), ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "An empirical study of potentialbased reward shaping and advice in complex", "author": ["S. Devlin", "D. Kudenko", "M. Grzes"], "venue": "multi-agent systems\u2019, Advances in Complex Systems (ACS), 14(02), 251\u2013278, ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robot shaping: Experiment in behavior engineering", "author": ["M. Dorigo", "M. Colombetti"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Ensemble methods for reinforcement learning with function approximation.", "author": ["S. Fauer", "F. Schwenker"], "venue": "in MCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Experiments with a New Boosting Algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "International Conference on Machine Learning, pp. 148\u2013156, ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis", "author": ["M. Grzes"], "venue": "Ph.D. dissertation, University of York", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Theoretical and empirical analysis of reward shaping in reinforcement learning", "author": ["M. Grzes", "D. Kudenko"], "venue": "Machine Learning and Applications, Fourth International Conference on, 0, 337\u2013344, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning of shaping rewards in reinforcement learning", "author": ["M. Grzes", "D. Kudenko"], "venue": "Neural Networks, 23(4), 541 \u2013 550, ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M.I. Jordan", "S.P. Singh"], "venue": "Neural Computation, 6, 1185\u20131201, ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural network ensembles", "author": ["A. Krogh", "J. Vedelsby"], "venue": "cross validation, and active learning\u2019, in Advances in Neural Information Processing Systems, pp. 231\u2013238. MIT Press, ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "The influence of reward on the speed of reinforcement learning: An analysis of shaping", "author": ["A. Laud", "G. DeJong"], "venue": "In Proc. 20th International Conference on Machine Learning. AAAI Press, ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "Ph.D. dissertation, University of Alberta", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "gq(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proceedings of the Third Conf. on Artificial General Intelligence., ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward offpolicy learning control with function approximation", "author": ["H.R. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML 2010), eds., Johannes F\u00fcrnkranz and Thorsten Joachims, pp. 719\u2013726. Omnipress, ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "An ensemble of linearly combined reinforcement-learning agents", "author": ["V. Marivate", "M. Littman"], "venue": "AAAI Workshops", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic shaping and decomposition of reward functions", "author": ["B. Marthi"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pp. 601\u2013608, New York, NY, USA, ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning, pp. 278\u2013287. Morgan Kaufmann, ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive artificial limbs: a real-time approach to prediction and anticipation", "author": ["P.M. Pilarski", "M.R. Dawson", "T. Degris", "J.P. Carey", "K.M. Chan", "J.S. Hebert", "R.S. Sutton"], "venue": "Robotics Automation Magazine, IEEE, 20(1), 53\u201364, ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, Inc., New York, NY, USA, 1st edn.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 116, Cambridge Univ Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning, ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, AAMAS \u201911, pp. 761\u2013768, Richland, SC, ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "Technical report, IEEE Transactions on Automatic Control, ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms, Ph.D", "author": ["H. van Hasselt"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3), 272\u2013292, ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Ensemble algorithms in reinforcement learning\u2019, Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["M.A. Wiering", "H. van Hasselt"], "venue": "IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 23, "context": "Reinforcement learning (RL) is a framework [24], where an agent learns from interacting with its (typically Markovian) environment.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "the popular Q-learning potentially diverges [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 17, "context": "This issue was recently resolved by the advancement of the family of gradient temporal-difference methods, such as Greedy-GQ [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "An interesting implication of this is the possibility to learn multiple tasks in parallel from a shared experience stream in a sound framework, an architecture dubbed Horde by Sutton et al [26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 30, "context": "In the spirit of ensemble methods [31], we use this idea in the context of learning a single task faster.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "observe that it improves performance in the multi-agent context [6], and Brys et al.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "using a multi-objectivization formalism demonstrate its usefullness while treating different shapings as correlated objectives [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 17, "context": "[18] refer to as latent learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "convergence in realistic setups \u2013 guarantees provided by Horde [26].", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "The environment of a RL agent is usually modeled as a Markov Decision Process (MDP) [23] given by a 4-tuple \u3008S,A, T,R\u3009, where S is the set of states, A is the set of actions available to the agent, T : S \u00d7 A \u00d7 S \u2192 R is the transition function with T (s, a, s\u2032) denoting the probability of ending up in state s\u2032 upon taking action a in state s, and R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) denoting the expected reward on the transition from s to s\u2032 upon taking action a.", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "environment dynamics (given by T and R) are unknown, one can solve the MDP by applying the family of temporal difference (TD) algorithms [24] to iteratively estimate the value functions.", "startOffset": 137, "endOffset": 141}, {"referenceID": 29, "context": "The following is the update rule of the popular Q-learning method in its simplest form [30]:", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "Eligibility traces controlled by a trace decay parameter \u03bb can be used as a way to speed up knowledge propagation [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "[13] show that in the tabular case this process converges to the optimal solution, under standard stochastic approximation assumptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "as Q-learning, to diverge even on simple problems [1, 27].", "startOffset": 50, "endOffset": 57}, {"referenceID": 26, "context": "as Q-learning, to diverge even on simple problems [1, 27].", "startOffset": 50, "endOffset": 57}, {"referenceID": 24, "context": "The family of gradient temporal-difference (GTD) algorithms resolve this issue for the first time, while keeping the constant per-step complexity, provided a fixed (or slowly changing) behavior [25, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 16, "context": "The family of gradient temporal-difference (GTD) algorithms resolve this issue for the first time, while keeping the constant per-step complexity, provided a fixed (or slowly changing) behavior [25, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 1, "context": "LSTD [2]), run in constant time and memory per-step, and", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "2 Please refer to Maei\u2019s dissertation for the full details [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "3 This is the simplest form of the update rules for gradient temporaldifference algorithms, namely that of TDC [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "[26] formalize a framework of parallel real-time off-policy learning, naming it Horde.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "There have been further successful applications of Horde in realistic robotic setups [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "It was originally thought of as a way of scaling up RL methods to handle difficult problems [7], as RL generally suffers from infeasibly long learning times.", "startOffset": 92, "endOffset": 95}, {"referenceID": 27, "context": "If applied carelessly, however, shaping can slow down or even prevent finding the optimal policy [28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "[21] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Potential-based shaping has been repeatedly validated as a way to speed up learning in problems with uninformative rewards [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Ensemble techniques such as boosting [9] and bagging [3] are widely used in supervised learning as effective methods to reduce bias and variance of solutions.", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "Ensemble techniques such as boosting [9] and bagging [3] are widely used in supervised learning as effective methods to reduce bias and variance of solutions.", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Most previous uses of ensembles of policies involved independent runs for each policy, with the combination happening post-factum [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 30, "context": "Others, in general, lack convergence guarantees, either using mixed on- and offpolicy learners [31], or Q-learners under function approximation [4].", "startOffset": 95, "endOffset": 99}, {"referenceID": 3, "context": "Others, in general, lack convergence guarantees, either using mixed on- and offpolicy learners [31], or Q-learners under function approximation [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 25, "context": "[26] give Horde in terms of general value functions, each with 4 auxilary inputs: \u03c0, \u03b3, r, z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "2 of van Hasselt\u2019s dissertation [29].", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "ensemble learning lies in the diversity of information its components contribute [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "-greedy) policy, Q-learning used with FA does not diverge, even despite the famous counterexamples [1, 27], ensembles of diverse Q-learners are bound to have larger disagreement amongst themselves and with the behavior policy, and have a much larger potential of becoming unstable.", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "-greedy) policy, Q-learning used with FA does not diverge, even despite the famous counterexamples [1, 27], ensembles of diverse Q-learners are bound to have larger disagreement amongst themselves and with the behavior policy, and have a much larger potential of becoming unstable.", "startOffset": 99, "endOffset": 106}, {"referenceID": 13, "context": "Krogh and Vedelsby [14] show in the context of neural networks that effective ensembles have accurate and diverse components, namely that they make their errors at different parts of the space.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "Marivate and Littman [19] consider diversity of MDPs, by improving performance in a generalized MDP through an ensemble trained on sample MDPs, which also requires a two-stage learning process.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].", "startOffset": 131, "endOffset": 143}, {"referenceID": 19, "context": "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].", "startOffset": 131, "endOffset": 143}, {"referenceID": 20, "context": "The effects of reward shaping on the learning process are usually considered to lie in the guidance of exploration during learning [10, 20, 21].", "startOffset": 131, "endOffset": 143}, {"referenceID": 14, "context": "Laud and DeJong [15] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "5 of Sutton and Barto [24] relating potential to diverge to the proximity of behavior and target policies.", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "[26], and refer to individual agents within Horde as demons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] discuss several intuitive ways to do so, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities, i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "We focus our attention to a classical benchmark domain of mountain car [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "The state space is approximated with the standard tile-coding technique [24], using ten tilings of 10 \u00d7 10, with a parameter vector learnt for each action.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Each is normalized into the range [0, 1].", "startOffset": 34, "endOffset": 40}, {"referenceID": 4, "context": "[5]; since the shaped rewards are more informative, they can get by with very rarely reaching the goal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "This has a roughly similar flavor to the approach of Marivate and Littman [19], who learn to solve many variants of a problem for the best parameter settings in a generalized MDP.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "One could go further and attempt to learn the best potential functions [20, 12].", "startOffset": 71, "endOffset": 79}, {"referenceID": 11, "context": "One could go further and attempt to learn the best potential functions [20, 12].", "startOffset": 71, "endOffset": 79}, {"referenceID": 20, "context": "[21], the best potential function correlates with the optimal value function V \u2217, learning which would solve the base problem itself and render the potentials pointless.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel without sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble.", "creator": "LaTeX with hyperref package"}}}