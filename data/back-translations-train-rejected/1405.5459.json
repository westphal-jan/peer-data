{"id": "1405.5459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2014", "title": "Projective simulation applied to the grid-world and the mountain-car problem", "abstract": "We study the model of projective simulation (PS) which is a novel approach to artificial intelligence (AI). Recently it was shown that the PS agent performs well in a number of simple task environments, also when compared to standard models of reinforcement learning (RL). In this paper we study the performance of the PS agent further in more complicated scenarios. To that end we chose two well-studied benchmarking problems, namely the \"grid-world\" and the \"mountain-car\" problem, which challenge the model with large and continuous input space. We compare the performance of the PS agent model with those of existing models and show that the PS agent exhibits competitive performance also in such scenarios.", "histories": [["v1", "Wed, 21 May 2014 15:51:18 GMT  (1063kb,D)", "http://arxiv.org/abs/1405.5459v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexey a melnikov", "adi makmal", "hans j briegel"], "accepted": false, "id": "1405.5459"}, "pdf": {"name": "1405.5459.pdf", "metadata": {"source": "CRF", "title": "Projective simulation applied to the grid-world and the mountain-car problem", "authors": ["Alexey A. Melnikov", "Adi Makmal", "Hans J. Briegel"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people who are able are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move and to move."}, {"heading": "II. THE PS MODEL - BRIEF SUMMARY", "text": "For the benefit of the reader, we will first give a brief description of PS, for a more detailed description see [1, 5]. PS is an AI model in which the information received from the agent is processed in a so-called episodic and compositional memory (ECM). ECM is described by a weighted network of \"clips,\" which are the units of episodic memory: input leads to the stimulation of corresponding perceptual clips, whereas the stimulation of action clips triggers real actions as output, as shown in Fig. 1. Once a perceptive clip is stimulated, the stimulation between clips becomes more likely until it reaches an action clip. In other words, in PS agent, perceptive input and action output is triggered by a random passage in the memory.For illustration, we consider a hypothetical PS network, as shown in Fig. 1. Each edge connects some clips to a clip with a CJ (depending on time and weight)."}, {"heading": "III. GRID WORLD", "text": "The network world environment [7, 9] is a labyrinth in which an agent should learn an optimal path to a fixed goal. The world is divided into discrete cells (or spaces) in which the agent can reside. At each time step, the agent can switch to one of the adjacent cells by selecting between four actions: left, right, up or down. Here, we look at the labyrinth of [7] as shown in Figure 2, which consists of 6 by 9 cells, some of which are marked as black cells, and a target (marked by a star) that is always located at the upper right cell. At the beginning of each experiment, the agent is placed in the first cell of the third row of the third row. If the agent decides to go to a square labeled \"wall\" or to go beyond the grid, then no movement is performed, but the time step is counted. The agent receives a reward for the goal 1 = only after reaching the goal, whatever the end of the goal is marked."}, {"heading": "IV. MOUNTAIN CAR", "text": "In the mountain-car task, defined in [11, 12], an agent will drive a car on a surface between two hills = always new effect x01, where a target at the top of the right hill is expected, as shown in Figure 7. At the beginning of each study, the agent has a random position x0 and a random speed v0. Then, in each of the following time steps, the agent receives his new position xnew and new velocity vnew as input and must choose between three possible actions: forward thrust (to the right), no thrust and a random thrust (to the left). Once the agent has found the target, he is rewarded with a new and new speed vnew as input and the study ends. Until then, as in the web world, the agent receives no rewards. To measure the agent's performance, we count the number of steps he needs to find the target at each study. Clearly, a well-executed agent would require fewer steps than the number of attempts."}, {"heading": "V. CONCLUSION", "text": "We investigated the performance of the projected simulation (PS) in the navigation tasks of the network world and the mountain bike, where an agent is to learn to find a target in a minimum number of steps. In the network world, the agent has to reckon with a delayed reward, which, however, is given only at the end of the experiment and with a large number of possible input perceptions. In both tasks, we saw that the PS agent finds the target faster. In the Berwelt, the input state is even infinite, with a limited number of possible input perceptions."}, {"heading": "Appendix A: Analyzing the physics background of the mountain-car problem", "text": "A car of mass m drives up the hill (xt + 1, vt + 1) coordinates the next level of current (vvt): it has motor acceleration ~ a = 1 and experiences gravitational acceleration ~ g as in Fig. 11. Its equation of motion is therefore unnecessary by: d ~ v / dt = a + 7.0. (A1) By projecting the vectors onto the direction of motion, we can project the vectors onto the direction of motion that we use as a whole. (A2), where the angle between the vectors ~ a and \u2212 g is. Integration results in vt = 1 + 2 of the direction of motion we getdv / dt = a - 1 of the direction of motion that we \u2212 t \u2212 1 of the direction of motion \u2212 x -1 of the direction of motion, we \u2212 vx = \u2212 1 of the direction of motion, we \u2212 t = \u2212 1 of the direction of motion \u2212 1, \u2212 1 of the direction of \u2212 1, \u2212 1 of the \u2212 x = \u2212 1."}], "references": [{"title": "Reinforcement learning: An introduction", "author": ["R.S.A.G. Sutton", "Barto"], "venue": "(MIT press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Advances in neural information processing systems", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "(University of Cambridge,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "The model, which can be naturally applied to reinforcement learning (RL) tasks [2\u20134], was tested in a recent paper [5] on a number of discrete toy-problems and was shown to perform well, also in comparison with the standard models of Q-learning [2] and extended learning classifier systems [6].", "startOffset": 79, "endOffset": 84}, {"referenceID": 1, "context": "The mountain-car task presents an additional challenge, by imposing a continuous input space [11\u201318].", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "In the mountain-car task, defined in [11, 12], an agent drives a car on a surface between two hills, where a goal awaits at the top of the right hill, as shown in Fig.", "startOffset": 37, "endOffset": 45}, {"referenceID": 2, "context": "For comparison, we next look at the performance of the SARSA algorithm [19] in the mountain-car problem, as reported in [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "[3] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[12] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[19] G.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We study the model of projective simulation (PS) which is a novel approach to artificial intelligence (AI). Recently it was shown that the PS agent performs well in a number of simple task environments, also when compared to standard models of reinforcement learning (RL). In this paper we study the performance of the PS agent further in more complicated scenarios. To that end we chose two well-studied benchmarking problems, namely the \u201cgrid-world\u201d and the \u201cmountain-car\u201d problem, which challenge the model with large and continuous input space. We compare the performance of the PS agent model with those of existing models and show that the PS agent exhibits competitive performance also in such scenarios.", "creator": "LaTeX with hyperref package"}}}