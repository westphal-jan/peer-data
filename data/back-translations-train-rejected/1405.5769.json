{"id": "1405.5769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2014", "title": "Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT", "abstract": "Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.", "histories": [["v1", "Thu, 22 May 2014 14:35:52 GMT  (2824kb,D)", "http://arxiv.org/abs/1405.5769v1", null], ["v2", "Wed, 24 Jun 2015 09:16:28 GMT  (0kb,I)", "http://arxiv.org/abs/1405.5769v2", "This paper has been merged witharXiv:1406.6909"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["philipp fischer", "alexey dosovitskiy", "thomas brox"], "accepted": false, "id": "1405.5769"}, "pdf": {"name": "1405.5769.pdf", "metadata": {"source": "META", "title": "Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT", "authors": ["Philipp Fischer", "Alexey Dosovitskiy"], "emails": ["fischer@cs.uni-freiburg.de", "dosovits@cs.uni-freiburg.de", "brox@cs.uni-freiburg.de"], "sections": [{"heading": "1 Introduction", "text": "This year, it has never been as good as it has been this year."}, {"heading": "2 Feature Learning with Convolutional Neural Nets", "text": "In the past, Convolutionary Neural Networks (CNNs) were mainly used for digit and document recognition [10,11] and for small-scale object recognition [8]. Typical training datasets were MNIST and CIFAR-10, each with 60,000 images in about 30 x 30 pixels. Thanks to an optimized GPU implementation and new regulation techniques, Krizhevsky et al. [9] successfully applied CNNs to the classification in the ImageNet dataset with over 1 million high-resolution images. The feature representation learned from this network demonstrates excellent performance not only in the ImageNet classification task for which it was trained, but also in a variety of other recognition tasks [4, 6, 16, 17].A typical criticism of the use of huge labeled datasets for training CNNs is that extracting such data requires expensive manual notation."}, {"heading": "2.1 Supervised Training", "text": "Instead of learning a network from scratch on ImageNet, we used a pre-trained model available at [7]. The architecture of the network follows Krizhevsky et al. [9]: The network consists of 5 revolutionary layers followed by 2 fully connected layers and a Softmax classifier. Each fully connected layer is 11 x 11, 5 x 5, 3 x 3, 3 x 3, 3 x 3, 3 x 3 pixels. The number of filters is 96, 256, 384, 384, 256 and / or. Each fully connected layer contains 4096 neurons. Maxpooling and normalization of local reactions occur after a few layers. The activation function is a Reflected linear unit (ReLU) nonlinearity, dropouts are applied in fully connected layers. For further details on the architecture and training algorithm, refer the reader to [9]."}, {"heading": "2.2 Unsupervised Training", "text": "The main idea of the approach is to create replacement labels for a blank set of training packages and thus replace the labeled data sets. We could, of course, argue that the described data is already available, there is no additional cost in using it. However, the reader can find more details in the original paper [5]. To generate the replacement data, the algorithm requires an blank data set as input. Instead of STL-10 blank data sets as in [5], we used random images from Flickr, because we expect that these will be better representatives of the distribution of natural images."}, {"heading": "3 Experimental Study", "text": "While detection tasks such as image classification and object detection are based on the underlying semantic structure of the scene, we expect the matching of interest points to be independent of this information, which makes assessing automatically learned features on matching tasks interesting, and raises the question of whether a feature representation trained for classification can also work well as a local interest description point. We compare the features learned from monitored and unmonitored Convolutionary Networks, as well as two baselines: SIFT and raw RGB values. SIFT is the preferred descriptor for matching tasks (see [14] for a comparison), while raw RGB patches serve as a weak \"naive\" baseline. Our focus is on matching performance, so we initially calculated areas of interest in all images (see details below) and used them as input for all description methods, allowing us to specify the performance of descriptors and not analyze detectors."}, {"heading": "3.1 Datasets", "text": "The common matching dataset by Mikolajczyk et al. [15] contains only 48 images. This dataset size limits the reliability of the conclusions drawn from the results, especially when comparing different design decisions, such as the depth of the network layer from which we derive the characteristics. We have created an additional dataset of 416 images, which was generated by applying 6 different types of transformations of varying thickness to 16 base images obtained from Flickr. These images were not included in the amount we used to train the unattended CNN. Figure 1 shows some of them. On each base image, we applied the geometric transformations rotation, zoom, perspective and nonlinear deformations, covering both rigid and affine transformations as well as more complex ones. In addition, we applied changes to lighting and focus by adding blur. Each transformation was applied to different sizes so that its effect on performance could be analyzed in the Figure 2."}, {"heading": "3.2 Performance Measure", "text": "In order to evaluate the match performance for a pair of images, we strictly followed the procedure described in [14]. First, we extracted elliptical regions of interest and corresponding image fields from both images using the Maximum Stability of Extreme Regions (MSER) detector [13]. We chose this detector because it proved to be consistently good and widely used in [15]. Then, we calculated the descriptors of all extracted regions and greedily adjusted them based on Euclidean distance, resulting in a ranking of the descriptor pairs. A pair is considered to be truly positive if the ellipse of the descriptor in the target image and the basic truth ellipse in the target image have an intersection of unification (IOU) of at least 0.6. All other pairs are considered false positive."}, {"heading": "3.3 Parameter Choices for Descriptor Computation", "text": "The MSER detector provides ellipses of different sizes depending on the scale of the detected region. Descriptors are derived from these elliptical regions by normalizing the image patch to a fixed size. It is not immediately clear which patch size is the best: larger patches offer higher resolution, but excessive magnification can highlight interpolation artifacts and the effect of radio frequency noise. Therefore, we optimized the patch size on our larger dataset for each method. Figure 3 shows the average performance of each method when varying the patch size. We did not test the patch size 47 for the unattended network because it was trained on 64 \u00d7 64 images and therefore cannot be applied to smaller images. While SIFT prefers a patch size of 69 \u00d7 69, neural networks work better with larger patch sizes. As usual, SIFT divides the image patch into 4 x 4 cells."}, {"heading": "3.4 Results", "text": "Figure 4 shows the mean mean precision on the different transformations of the new dataset with the optimized parameters. Surprisingly, both neural networks perform much better than SIFT on all transformations except blur. This shows the importance of these results: CNNs outperform one of the best artisanal descriptors as much as this descriptor outperforms the na\u00efve basics. An exception is the strong blur, which seems to be problematic especially for the monitored CNN values."}, {"heading": "4 Conclusions", "text": "The comparison allows for several conclusions: 1. Both CNNs outperform SIFT in the task of descriptor matching, which is about as high as improving SIFT over simple RGB patches. 2. While class names provided during training for neural networks are advantageous when the properties are used for classification, unattended CNN training is superior to descriptor matching. 3. The experiment to blur transformations indicates a limitation of the CNN that was trained on ImageNet. With the unattended network, we showed that this can be handled to some extent by incorporating blurred images during training. However, blurring is still the most difficult transformation for neural networks, which may indicate a general weakness in the architecture. 4. Calculation costs are in favor of SIFT. While SIFT is still interesting for tasks, speed and simplicity are of great importance for all computer tasks that rely on the use of contextual networks."}, {"heading": "Acknowledgments", "text": "The work was partly funded by the ERC Starting Grant VideoLearn."}], "references": [{"title": "Speeded-up robust features (surf)", "author": ["H. Bay", "A. Ess", "T. Tuytelaars", "L. Van Gool"], "venue": "Comput. Vis. Image Underst", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In: CVPR. pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "CAF: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "Darrell", "T.: De"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Unsupervised feature learning by augmenting single images (2014), pre-print, arXiv:1312.5242v3 [cs.CV], ICLR\u201914 workshop track", "author": ["A. Dosovitskiy", "J.T. Springenberg", "T. Brox"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding. http: //caffe.berkeleyvision.org", "author": ["Y. Jia"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis, Department of Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In: NIPS. pp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "(Nov", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Robust wide baseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "Proc. BMVC. pp", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "A performance evaluation of local descriptors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "L.J.V.: A comparison of affine region detectors", "author": ["K. Mikolajczyk", "T. Tuytelaars", "C. Schmid", "A. Zisserman", "J. Matas", "F. Schaffalitzky", "T. Kadir", "Gool"], "venue": "IJCV 65(1-2),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Visualizing and understanding convolutional networks (2013), preprint, arXiv:1311.2901v3 [cs.CV", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction For years, local descriptors based on orientation histograms, such as SIFT, HOG, SURF [1, 3, 12], etc.", "startOffset": 101, "endOffset": 111}, {"referenceID": 2, "context": "1 Introduction For years, local descriptors based on orientation histograms, such as SIFT, HOG, SURF [1, 3, 12], etc.", "startOffset": 101, "endOffset": 111}, {"referenceID": 11, "context": "1 Introduction For years, local descriptors based on orientation histograms, such as SIFT, HOG, SURF [1, 3, 12], etc.", "startOffset": 101, "endOffset": 111}, {"referenceID": 1, "context": "The considerable progress we have seen in recognition is largely due to them: image retrieval is based on aggregated SIFT descriptors, structure from motion started to work reliably due to SIFT correspondences, and even in low level problems, such as optical flow estimation, orientation histograms help deal with large motion [2].", "startOffset": 327, "endOffset": 330}, {"referenceID": 8, "context": "[9] has finally been successful in outperforming handcrafted features in basically all important recognition tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "There are several papers demonstrating that a network trained for classification on ImageNet also performs very well on other datasets and recognition tasks [4, 6, 16, 17].", "startOffset": 157, "endOffset": 171}, {"referenceID": 5, "context": "There are several papers demonstrating that a network trained for classification on ImageNet also performs very well on other datasets and recognition tasks [4, 6, 16, 17].", "startOffset": 157, "endOffset": 171}, {"referenceID": 15, "context": "There are several papers demonstrating that a network trained for classification on ImageNet also performs very well on other datasets and recognition tasks [4, 6, 16, 17].", "startOffset": 157, "endOffset": 171}, {"referenceID": 16, "context": "There are several papers demonstrating that a network trained for classification on ImageNet also performs very well on other datasets and recognition tasks [4, 6, 16, 17].", "startOffset": 157, "endOffset": 171}, {"referenceID": 4, "context": "[5] and approaches two limitations of a supervised ConvNet: (1) no labeled data is needed for training the network and (2) the training objective may fit better to a descriptor matching task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15], which is the most popular benchmark dataset for this task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "2 Feature Learning with Convolutional Neural Nets In the past, convolutional neural networks (CNNs) were used mainly for digit and document recognition [10,11] as well as small-scale object recognition [8].", "startOffset": 152, "endOffset": 159}, {"referenceID": 10, "context": "2 Feature Learning with Convolutional Neural Nets In the past, convolutional neural networks (CNNs) were used mainly for digit and document recognition [10,11] as well as small-scale object recognition [8].", "startOffset": 152, "endOffset": 159}, {"referenceID": 7, "context": "2 Feature Learning with Convolutional Neural Nets In the past, convolutional neural networks (CNNs) were used mainly for digit and document recognition [10,11] as well as small-scale object recognition [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 8, "context": "[9] successfully applied CNNs to classification on the ImageNet dataset with over 1 million labeled high resolution images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The feature representation learned by this network shows excellent performance not only on the ImageNet classification task it was trained for, but also on a variety of other recognition tasks [4, 6, 16, 17].", "startOffset": 193, "endOffset": 207}, {"referenceID": 5, "context": "The feature representation learned by this network shows excellent performance not only on the ImageNet classification task it was trained for, but also on a variety of other recognition tasks [4, 6, 16, 17].", "startOffset": 193, "endOffset": 207}, {"referenceID": 15, "context": "The feature representation learned by this network shows excellent performance not only on the ImageNet classification task it was trained for, but also on a variety of other recognition tasks [4, 6, 16, 17].", "startOffset": 193, "endOffset": 207}, {"referenceID": 16, "context": "The feature representation learned by this network shows excellent performance not only on the ImageNet classification task it was trained for, but also on a variety of other recognition tasks [4, 6, 16, 17].", "startOffset": 193, "endOffset": 207}, {"referenceID": 4, "context": "[5] proposed an unsupervised approach to train CNNs by making use of data augmentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For training CNNs and subsequent feature extraction we use the caffe code [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "1 Supervised Training Instead of training a network on ImageNet from scratch, we used a pre-trained model available at [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "[9]: the network contains 5 convolutional layers followed by 2 fully connected layers and a softmax classifier on top.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We refer the reader to [9] for more details on the architecture and training algorithm.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "2 Unsupervised Training We performed unsupervised training of a CNN as described in [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "We briefly review this method and describe our specific design choices, the reader can find more details in the original paper [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "Instead of STL-10 unlabeled dataset as in [5], we used random images from Flickr because we expect those to be better representatives of the distribution of natural images.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "SIFT is the preferred descriptor in matching tasks (see [14] for a comparison), while raw RGB patches serve as a weak \u2019naive\u2019 baseline.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "[15] contains only 48 images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] was not generated synthetically but contains photos taken from different viewpoints or with different camera settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "2 Performance Measure To evaluate the matching performance for a pair of images, we strictly followed the procedure described in [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "We first extracted elliptic regions of interest and corresponding image patches from both images using the maximally stable extremal regions (MSER) detector [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "We chose this detector because it was shown to perform consistently well in [15] and it is widely used.", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "in [5] seems particularly promising.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.", "creator": "LaTeX with hyperref package"}}}