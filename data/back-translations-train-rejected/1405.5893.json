{"id": "1405.5893", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2014", "title": "Computerization of African languages-French dictionaries", "abstract": "This paper relates work done during the DiLAF project. It consists in converting 5 bilingual African language-French dictionaries originally in Word format into XML following the LMF model. The languages processed are Bambara, Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced languages concerning Natural Language Processing tools. Once converted, the dictionaries are available online on the Jibiki platform for lookup and modification. The DiLAF project is first presented. A description of each dictionary follows. Then, the conversion methodology from .doc format to XML files is presented. A specific point on the usage of Unicode follows. Then, each step of the conversion into XML and LMF is detailed. The last part presents the Jibiki lexical resources management platform used for the project.", "histories": [["v1", "Thu, 22 May 2014 20:15:57 GMT  (867kb)", "http://arxiv.org/abs/1405.5893v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chantal enguehard", "mathieu mangeot"], "accepted": false, "id": "1405.5893"}, "pdf": {"name": "1405.5893.pdf", "metadata": {"source": "CRF", "title": "Computerization of African languages-French dictionaries", "authors": ["Chantal Enguehard", "Mathieu Mangeot"], "emails": ["Chantal.Enguehard@univ-nantes.fr,", "Mathieu.Mangeot@imag.fr"], "sections": [{"heading": null, "text": "Keywords: DiLAF, Dictionary, Jibiki"}, {"heading": "1. Introduction", "text": "The work behind this work was done as part of the DiLAF project to computerize African-French dictionaries (Bambara, Hausa, Kanuri, Tamajaq, Zarma) in order to spread them widely and extend their reach. We present a methodology for converting dictionaries from the Word.doc format to a structured XML format according to the standards Unicode Character Encodings and Lexical Markup Framework (LMF). Natural language processing of African languages is still in its infancy, and it is our duty to help our colleagues from the South in this way, which requires, among other things, the publication of articles that are a valuable resource for underfunded languages. In the past, many studies have been conducted in this area. Nevertheless, it seemed interesting to redefine a new methodology taking into account current developments such as the Open Document Format (ODF) or LMF standards, which can be used exclusively on the basis and free source texts, so that any other variable (ML) text can be used."}, {"heading": "2. Presentation of the DiLAF project", "text": "If access to computers is considered to be the most important indicator of the digital divide in Africa, we must recognise that the availability of resources in African languages is a handicap with incalculable consequences for the development of Informat ion Technology and Communication Technologies (ICT). \u2022 Most languages in French-speaking West Africa are under-resourced (Berment, 2004): electronic resources are scarce, poorly distributed or lacking, making the use of these languages difficult when introduced into the education system and, above all, developing their use in administration and daily life. Dictionaries are the cornerstone of natural language processing, whether in the mother tongue or in a foreign language."}, {"heading": "3. Presentation of the dictionaries", "text": "Four of the five dictionaries have been created as part of the Sout\u00e9ba project (Programme for the Promotion of Basic Education) with the support of German cooperation and the European Union. These dictionaries for basic education have a simple structure, as they are designed for primary school children in a bilingual school (lessons are held there in a national language and in French).Most of the terms used in lexicology, such as lexicology, language parts, synonyms, antagonyms, genres, dialect variations, etc., are recorded in the dictionary in the language concerned and help to falsify and spread a metalanguage in the local language, a technical terminology. The entries are listed in alphabetical order, even for Tamajaq (although it is common for this language to sort entries by lexical roots) because the vowels are explicitly written (this type of classification has been preferred because it is well known by children)."}, {"heading": "3.1. Hausa-French dictionary", "text": "The Hausa-French dictionary contains 7,823 entries sorted according to the following lexicographic order: a b'c d'e f fy g gw gy h i j k kw ky w y l'm n o p r s sh t ts u w y z (R\u00e9publique du Niger, 1999a), structured according to the part of the speech with different patterns, all entries are typographical, followed by the pronunciation (sounds are marked with diacritics placed on vowels) and the part of the speech. At the semantic level, there is a definition in Hausa, a usage example (identified by the use of italics), and the equivalent in French. In a noun, the gender, feminine, plural, plural and sometimes dialectal variants are noted: verbs, it is sometimes necessary to indicate the degree to calculate morphological derivatives."}, {"heading": "3.2. Kanuri-French dictionary", "text": "The Kanuri-French dictionary contains 5,994 centering entries in the following lexicographic order: a b c d e f g h i j k l m ny o p r's sh t u w y z. The orthographic form of the entry is followed by a reference to pronunciation aimed at evaluation tones; the speech section is presented in italics, followed by a definition, a usage example, a French translation and meaning in French. Additional information may appear as variants. Example: abba'wa [aba] w\u00e0] cu."}, {"heading": "3.3. So\u014bay Zarma-French dictionary", "text": "The Zarma-French dictionary contains 6916 entries sorted according to the following lexicographic order: a \u00e3 b c d e f g h i-j k l m n-o \u00f5 p r s t u-w y z (R\u00e9publique du Niger, 1999d) Each entry has an orthographic form followed by a phonetic transcription in which the tones are evaluated according to the conventions already established for the kanuri. Speech passages explicitly specify the volatility or intransience of the verbs. Some entries contain antonyms, synonyms and references. A glossary in French, a definition and an example conclude the entry. \u25cf Za zankey di hansu-aro mteeb. \u25cf brusquement (d\u00e9tal) \u25cf sanniize no ka\u00e7ga cabe ka\u00e7boro na zuray sambu ngaabi sahdin \u25cf Za zankey di anshu-kaagaro."}, {"heading": "3.4. Tamajaq-French dictionary", "text": "The Tamajaq-French dictionary contains 5,205 entries, sorted according to the following lexicographic order: a \"\u0103 b c d'e \u00ea f g-g-h i-j-k-l-m\" n-o-q r s-t-w x y z \"(R\u00e9publique du Niger, 1999c) The orthographic form of the entry is followed by the speech section and a glossy in French, which is presented in italics. Nouns often contain morphological information about the state of annexation, plural and gender are also explicitly stated, followed by a definition and an example of use. Other information may appear as variants, synonyms, etc., as Tamajaq is not a tonal language, phonetics does not appear."}, {"heading": "3.5. Bambara-French dictionary", "text": "The Bambara-French dictionary by Father Charles Bailleul (1996 edition) contains more than 10,000 entries, sorted according to the following lexicographic order: a b c d e-f g g h i j k l'n, b c p r s t u w y z. This dictionary is intended primarily for French speakers who want to improve Bambara, but it is also a resource for Bambara speakers. According to the author, the dictionary plays \"the role of a working tool for literacy, education and Bambara culture.\" To date, it can be considered the most comprehensive dictionary of the language. It is also used by specialists of other varieties of the language such as Dyula (Burkina Faso, C\u00f4te d'Ivoire) and Malink\u00e9 (Guinea, Gambia, Sierra Leone, Liberia, etc.)."}, {"heading": "4. General conversion methodology and tools", "text": "The main goal is to convert dictionaries from a word-processing format suitable for human use to XML and explicitly highlight all information so that it can be automatically used for natural language processing tasks. On the one hand, the limitations are to work with free, open, and cross-platform tools, and on the other, to define a simple process that can be understood by linguists who have no computer knowledge other than regular expressions, and then run on their own."}, {"heading": "4.1. Conversion methodology", "text": "The conversion method follows these steps: 1. Conversion of problematic characters to Unicode; 2. Conversion of the OpenOffice format to XML; 3. Identification and explicit marking of each part of the information (keyword, part of the language, etc.); 4. XML validation and manual correction of errors in the data (closed value lists, references); 5. Structuring of entries according to the LMF standard."}, {"heading": "4.2. Tools used", "text": "OpenOffice (or LibreOffice) is ideal for this step. It is free and open source. In addition, in addition to the ISO standard Open Document Format (odt), it can open many Microsoft formats (rtf,.doc, and.docx). Finally, the XML produced is simple, especially compared to Office Open XML Microsoft. OpenOffice has a regular expression engine for search / replace functions. This tool can be used for many steps before converting to XML. However, finding / replacing the styles can be problematic, because the boundaries of text styles can be changed during substitutions (part of a word is suddenly italic). As we rely on styles for converting to XML, we limit the conversion to Unicode characters in order to keep the styles intact. Then we need an editor to modify the files. For these operations, the XML editors are not very useful, because they cannot directly interpret the text editing with the firecode editors for most of the regular ML text editors."}, {"heading": "4.3. Incremental backups needed", "text": "The methodology aims to create backups at each stage and track all search / replace operations performed in order to go back when errors are detected as a result of improper action. Sometimes, an error is detected long after the error. If an error cannot simply be corrected by a new search / replacement, it is possible to go back from a previous version. Despite all the precautions, errors are sometimes detected very late and it is very difficult to go back. If the error cannot be detected automatically, this requires manual correction. It should be remembered that no one is perfect and others, who are even better trained, had to forget the possibility of automatically correcting all errors in the conversion process."}, {"heading": "5. Use of Unicode", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Characters conversion to Unicode", "text": "Although the alphabets of the languages we have worked on (Enguehard 2009) are predominantly of Latin origin, linguists have, in a series of sessions, adopted new characters necessary to write certain sounds in some languages with a single character. So, each of the alphabets we have presented so far includes at least one of these special characters:. Characters consisting of a Latin character and a diacritical mark have also been created: \u00e2\u00ea\u00ee\u00ee\u00f4\u00fb\u0103\u00e3o. Although most of these characters have been present in the Unicode standard for several years (based on the work of ISO 10646 (Haralambo 2004)), dictionaries with old hacked fonts have been written. A methodology has been defined to identify the insufficient characters and replace them with those defined in the Unicode standard. It implies that all the identified characters are stored in a file, so that this process can be easily repeated."}, {"heading": "5.2. Digraphs lexicographical order", "text": "Numbers can easily be typed with two characters, but their use changes the sort order that determines the lexicographic representation of dictionary entries. Thus, in the Hausa dictionary, the word \"sha\" (drink) precedes the word \"suya\" (fried), and in Kanuri, the word \"suwuttu\" (undo) precedes the name \"shadda\" (cymbal). These subtle differences can hardly be edited by software and require graphics to appear as a proper character in the Unicode repertoire. Some graphics used in other languages already exist, sometimes under their different letters: \"DZ\" (U + 01F1), \"Dz\" (U + 01F2), \"dz\" (U + 01F3) are used in Slovak, \"NJ\" (U + 01CA), \"Nj\" (U + 0ribic), \"Croatian,\" dj \"(CC + 1) and\" various alphabets \"etc."}, {"heading": "5.3. Characters with diacritics", "text": "Some characters with diacritics are contained in Unicode as a unique character, others can only be obtained by composition. Thus, vowels with tilde \"a,\" \"i,\" \"o\" and \"u\" can be found in lowercase and uppercase form in Unicode, while the \"e\" is missing with tilde and must be composed with the sign \"e\" or \"E\" followed by the tilde accent (U + 303), which can lead to other letters with inclination (e.g. inclination at different heights) when viewed or printed. The letter j with Caron exists in Unicode as a character (U + 1F0), but its uppercase form J must consist of the letter J and the checkered character (U + 30C). The letters e, E and J should be added to the Unicode standard."}, {"heading": "5.4. Letter case change", "text": "Word processors usually provide the capitalization function, but they do not always recognize it correctly. For example, during our work, we found that the OpenOffice Writer software (version 3.2.1) fails to convert \"\" to \"from lower case to upper case or vice versa (the character remains unchanged), while Notepad + + (version 5.8.6) fails to convert\" to \"J.\""}, {"heading": "6. Conversion of the format towards XML", "text": "Figure 1 shows an extract from the Zarma-French dictionary in the original.odt format. All the following examples are based on this dictionary. The Open Document Format has the great advantage that it is based on XML. Instead of a conversion, we actually fetch the contents of the XML document and transform it to get what we want. An ODF document is actually a zip archive that contains multiple files including the text content in XML. This content is actually stored in the file \"content.xml\" in the archive. To retrieve this file, some clever manipulations have to be followed. In MacOs, you have to create an empty folder and then copy the.odt file into it. Then, with a terminal, you have to \"unzip\" the command to extract the file. On Windows, the.odt file extension is converted to a.zip file and then converted to.zip archive."}, {"heading": "7. Explicit tagging of the information", "text": "This step is to explicitly mark the words of their language. < < > Information is usually different from others in the original file with a different style. Figure 2 shows a portion of the \"abiyanso\" (airport) entry in the Zarma-French dictionary. The style in which the pronunciation is specified is \"Phonetic _ form.\" Once the information is found, a series of tags must be selected to mark it. This raises the question of the choice of language used for the tags. Choosing English as an international research language may be privileged, but in our case, English is not a language that exists in our dictionaries, and furthermore, it is not controlled by all linguistic colleagues working on the project. The use of French solves this problem as all partners speak the language. However, in the case of underfunded languages, we believe it is important to encourage partners to use the words of their language to define the name of the tags."}, {"heading": "8. Correction of the data", "text": "At this stage, several corrections are made to the data."}, {"heading": "8.1. XML Validation", "text": "In order to use XML tools, our file must be well-shaped, and manipulation of the previous step almost always results in XML syntax errors. FireFox has an XML parser and is also able to specify exactly where the errors in the file are. Once the error is found, it must be checked to see if it is not repeated elsewhere in the file, in which case a regular expression must be written to systematically correct the error rather than executing it manually. In our case, the following regular expression can solve the problem: s / < sanniize ([^ < +) <\\ / sanniize > / < sanniize > $1 <\\ / sanniize > / g The XML file is now well-shaped and it is then possible to manipulate it with XML tools."}, {"heading": "8.2. Verification of closed lists of values", "text": "The level of verification of information that captures its value in a closed list is important. Some errors result from poor handling in the previous steps, while others were present in the original file before conversion. Thus, a dictionary uses language parts, a terminology database uses a list of domains, etc. A copy of the file and only the values to be checked are a systematic approach to verification. In the example of Figure 4, the language part marked \"Kanandi\" can be extracted with the following expression: s / ^ * < kanandi > ([^ <] +) <\\ / kanandi > * $/ $1 / The resulting list must then be sorted alphabetically. TextWrangler and Notepad + plugin with its TextFX have the necessary commands. If the editor does not offer this option, OpenOffice Calc spreadsheet can be used to quickly detect irregularities."}, {"heading": "8.3. Simple corrections", "text": "A CSS stylesheet can be set to display the data directly in a browser. A compact display with a different style for each type of information helps to detect structuring errors in an entry. In the example of Figure 3, we see immediately that there is a definition (bold) and an example (italic) missing for the entry \"abunaadam.\" With an XSL stylesheet, you can modify the data before viewing it, like adding a unique identifier for each entry, then define a hypertext link to the corresponding entry for each reference. When the linguist browses the file, he can click on the hyperlinks to ensure that the references are also entries in the dictionary, such as the entry \"abunaadam\" in Figure 3 with a reference to the entry \"adamayse.\" It is essential to check the data to detect some errors, even if they can be corrected automatically with regular expressions."}, {"heading": "9. Structure of the entries", "text": "The entries can now be restructured. In files originating from word processors, the data structure is usually implied. We need to add new structure elements to achieve a more standardized structure that allows later reuse. In terms of standards, LMF (Romary et al., 2004) became the ISO standard (Francopoulo et al., 2009) in November 2008. It fits ideally with our goals. As it is a metamodel and not a format, we can apply the principle of the LMF model to our input structure and keep our tags without using the LMF syntax. \"The core metamodel LMF is shown in Figure 5. The\" Lexical Entry \"object contains a\" form \"and one or more\" Sense \"objects.\" Our lexical entries must now follow this meta model and keep our tags without using the LMF syntax. \"The Figures 6 and 7 show entries corresponding to the structure before the object."}, {"heading": "10. Web access via the Jibiki platform", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1. Presentation of the platform", "text": "Jibiki (Mangeot et al., 2003; Mangeot et al., 2006; Mangeot, 2006) is a generic platform for dealing with online lexical resources with users and group management, originally developed for the Papillon project, which is entirely programmed in Java based on the \"Enhydra\" environment. All data is stored in XML format in a Postgres database. This website mainly offers two services: a unified interface for simultaneous access to many heterogeneous resources (http: monolingual or bilingual dictionaries, multilingual databases, etc.) and a specific interface for editing dictionaries available on the platform. Several lexical resource construction projects have used this platform successfully or are still using it, such is the case for the GDEF project (Chalvin et al., 2006), which is building an Estonian bilingual dictionary."}, {"heading": "10.2. Lookup interfaces", "text": "Three different user interfaces are available to the user: - The generic search allows the user to search for a word or a prefix of a word in all dictionaries on the platform. - The language of the word must be specified. - The volume search allows the user to search for a word or a prefix in a specific medium. - On the left side of the results window, the volume headings are displayed, sorted in alphabetical order. An infinite search allows the user to search the entire disk. On the right side of the window, the previously selected entries are displayed on the left. - Advanced search is available for complex multi-criterion queries. For example, it is possible to look up an entry with a specific part of the language created by a particular author. On the left side of the results window, the headings of the corresponding entries are displayed, sorted in alphabetical order."}, {"heading": "10.3. Editing process", "text": "The editor (Mangeot et al., 2004) is based on an HTML interface model that is instantiated with the lexical entry to be published. It is automatically generated using an XML schema that describes the input structure. It can then be modified to improve the display on the screen. Therefore, it is possible to edit any type of dictionary entry, provided it is encoded in XML. The editing process can be customized by level and status to suit specific needs. A quality level (e.g. from 1 to 5 stars, an entry with 1 star is a draft and one with 5 stars is certified by a linguist) can be assigned to each entry. Likewise, a competence level can be assigned to each entry (1 star is a beginner and 5 stars is a certified linguist). Then, when a user edits an entry with 3 stars, the entry level increases to 3 stars. A quality status can also be assigned to each entry (1 star is a beginner and 5 stars is a certified linguist)."}, {"heading": "10.4. Remote access via a REST API", "text": "After uploading the dictionaries to the Jibiki server, they can be accessed via a REST API. Search commands are available to query indexed information: keyword, pronunciation, language part, domain, example, idiom, translation, etc. The API can also be used to edit entries; the user must first be registered on the website."}, {"heading": "11. Conclusion", "text": "The DiLAF project does not end well. Before dictionaries are distributed, there are manual correction steps and possibly data additions. For example, examples of the Zarma-French dictionary are translated into French. Once the dictionaries are converted, we can extend their coverage with a system of contributions / editing / validation that can be done online on the Jibiki platform. Africa's limited Internet access requires the development of alternative methods. We can then use the data as raw material to increase the computerization of these languages: morphological analyzers, spell checkers, machine translation systems, etc."}, {"heading": "12. Acknowledgements", "text": "The DiLAF project is financed by the Francophone des Inforoutes Fund of the International Organisation of Francophone Languages and we also thank all the linguists in the team, without whom this project would not have been possible: Sumana Kane, Issouf Modi, Michel, Radji, Rakia, Mamadou Lamine Sanogo."}, {"heading": "13. References", "text": "Berment V. (2004). M\u00e9thodes pour informatiser deslangues et des groupes de langues (2004? EURALEX 2006). \"Peu dot\u00e9es.\" Thesis de nouveau doctorat, sp\u00e9cialit\u00e9 informatique, Universit\u00e9 Joseph Fourier Grenoble I, Grenoble, France. Buseman A., Buseman K., Jordan D. & Coward D. (2000). \"The linguist's shoebox: tutorial and user's guide: integrated data management and analysis for the field linguist, volume viii. Waxhaw, North Carolina: SIL International. Chalvin, A. & Mangeot, M. (2006). M\u00e9thodes et outils pour la lexicographie bilingue en ligne: le cas du Grand Dictionnaire Estonia-Fran\u00e7ais. Actes d'EURALEX 2006, Turin, Italy, 6-9 septembre 2006."}], "references": [{"title": "M\u00e9thodes pour informatiser des langues et des groupes de langues \u00ab peu dot\u00e9es ", "author": ["V. Berment"], "venue": "These de nouveau doctorat, spe\u0301cialite\u0301 informatique,", "citeRegEx": "Berment,? \\Q2004\\E", "shortCiteRegEx": "Berment", "year": 2004}, {"title": "The linguist\u2019s shoebox: tutorial and user\u2019s guide: integrated data management and analysis for the field linguist, volume viii", "author": ["A. Buseman", "K. Buseman", "D. Jordan", "D. Coward"], "venue": "SIL International", "citeRegEx": "Buseman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Buseman et al\\.", "year": 2000}, {"title": "M\u00e9thodes et outils pour la lexicographie bilingue en ligne : le cas du Grand Dictionnaire Estonien-Fran\u00e7ais", "author": ["A. Chalvin", "M. Mangeot"], "venue": "Actes d'EURALEX", "citeRegEx": "Chalvin and Mangeot,? \\Q2006\\E", "shortCiteRegEx": "Chalvin and Mangeot", "year": 2006}, {"title": "La Lexicologie. Paris : PUF, Que saisje", "author": ["R. Eluerd"], "venue": null, "citeRegEx": "Eluerd,? \\Q2000\\E", "shortCiteRegEx": "Eluerd", "year": 2000}, {"title": "Les langues d\u2019Afrique de l\u2019ouest : de l\u2019imprimante au traitement automatique des langues", "author": ["C. Enguehard"], "venue": "Sciences et Techniques du Langage,", "citeRegEx": "Enguehard,? \\Q2009\\E", "shortCiteRegEx": "Enguehard", "year": 2009}, {"title": "LMF for a selection of African Languages. Chapter 7, book \"LMF: Lexical Markup Framework, theory and practice", "author": ["C. Enguehard", "M. Mangeot"], "venue": null, "citeRegEx": "Enguehard and Mangeot,? \\Q2013\\E", "shortCiteRegEx": "Enguehard and Mangeot", "year": 2013}, {"title": "Environnements centralis\u00e9s et", "author": ["M. France. Mangeot"], "venue": null, "citeRegEx": "Mangeot,? \\Q2001\\E", "shortCiteRegEx": "Mangeot", "year": 2001}, {"title": "Mortureux, Marie-F", "author": ["France : Didier"], "venue": null, "citeRegEx": "Didier.,? \\Q1997\\E", "shortCiteRegEx": "Didier.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Most languages in francophone West Africa area are under-resourced (\u03c0language) (Berment, 2004): electronic resources are scarce, poorly distributed or absent, making use of these languages difficult when it comes to introducing them into the education system and especially develop their use in writing in the administration and daily life.", "startOffset": 79, "endOffset": 94}, {"referenceID": 6, "context": "Five dictionaries were converted and integrated into the Jibiki lexical resources management platform (Mangeot, 2001).", "startOffset": 102, "endOffset": 117}, {"referenceID": 0, "context": "Most languages in francophone West Africa area are under-resourced (\u03c0language) (Berment, 2004): electronic resources are scarce, poorly distributed or absent, making use of these languages difficult when it comes to introducing them into the education system and especially develop their use in writing in the administration and daily life. Dictionaries are the cornerstone of processing natural language, be it in the mother tongue or in a foreign language. The primary function of communication is conveying meaning, yet meaning is primarily conveyed through vocabulary. As David Wilkins, a british linguist (1972) wrote \u201cso aptly \u201cWhile without grammar little can be conveyed, without vocabulary nothing can be conveyed\".", "startOffset": 80, "endOffset": 617}, {"referenceID": 3, "context": "The aim of these usage dictionaries is to popularize the written form of the daily use of African languages in the pure lexicographical tradition (Mator\u00e9, 1973) (Eluerd, 2000).", "startOffset": 161, "endOffset": 175}, {"referenceID": 4, "context": "Although the alphabets of languages on which we have worked (Enguehard 2009) are mainly of Latin origin, new characters needed to note specific sounds in some languages with a single character has been adopted by linguists in a series of meetings.", "startOffset": 60, "endOffset": 76}], "year": 2014, "abstractText": "This paper relates work done during the DiLAF project. It consists in converting 5 bilingual African language-French dictionaries originally in Word format into XML following the LMF model. The languages processed are Bambara, Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced languages concerning Natural Language Processing tools. Once converted, the dictionaries are available online on the Jibiki platform for lookup and modification. The DiLAF project is first presented. A description of each dictionary follows. Then, the conversion methodology from .doc format to XML files is presented. A specific point on the usage of Unicode follows. Then, each step of the conversion into XML and LMF is detailed. The last part presents the Jibiki lexical resources management platform used for the project.", "creator": "Writer"}}}