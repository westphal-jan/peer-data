{"id": "1405.6076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2014", "title": "Online Linear Optimization via Smoothing", "abstract": "We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between \"Follow the Regularized Leader\" and \"Follow the Perturbed Leader\" up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.", "histories": [["v1", "Fri, 23 May 2014 14:33:48 GMT  (42kb)", "http://arxiv.org/abs/1405.6076v1", "COLT 2014"]], "COMMENTS": "COLT 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob abernethy", "chansoo lee", "abhinav sinha", "ambuj tewari"], "accepted": false, "id": "1405.6076"}, "pdf": {"name": "1405.6076.pdf", "metadata": {"source": "CRF", "title": "Online Linear Optimization via Smoothing", "authors": ["Jacob Abernethy", "Abhinav Sinha", "Ambuj Tewari", "ABERNETHY LEE", "SINHA TEWARI"], "emails": ["JABERNET@UMICH.EDU", "CHANSOOL@UMICH.EDU", "ABSI@UMICH.EDU", "TEWARIA@UMICH.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.60 76v1 [cs.LG] 2 3"}, {"heading": "1. Introduction", "text": "In fact, most of us perceive ourselves as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \"incapable,\" as \"incapable\" and \",\" as \"and\" and \"and\" and, \"and\" and \"and\" and, and \"and\" and \"and, and\" and \"and\" and \"and, and\" and \"and\" and, and \"and\" and, and \"and\" and, and \"and\" and, and \"and, and\" and \"and, and\" and, and \"and, and\" and \"and\" and, and \"and\" and, and \"and\" and, and \"and\" and \"and\" and, and \"and\" and \"and\" and \"and\" and, and \"and\" and \"and\" and \"and\" and \"and, and\" and \"and\" and \"and\" and \"and, and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and, and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and, and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and, and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \"and\" and \""}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Convex Analysis", "text": "Let f be a differentiated, closed, and orderly convex function whose domain is domf \u03b2 RN. We say that f is feminine in relation to a norm, if f (x) \u2212 f (y) \u2212 f (y) \u2212 y for all x, y \u2212 dom (f).The Bregman divergence, referred to as Df (y, x), is the gap between f (y) \u2212 f (y) and the linear approximation of f (y) by x. Formally, Df (y, x) \u2212 f (x) \u2212 f (x) \u2212 f (f (f), y \u2212 x >. We say that f is feminine in relation to a norm if we are Df (y, x) feminine."}, {"heading": "2.2. Online Linear Optimization", "text": "Let X and Y be convex and closed subsets of RN. Linear online optimization is defined as the following iterative process: In turn t = 1,..., T, \u2022 the learner plays wt-X. \u2022 the opponent reveals that the learner is rewarded1 < wt, \u03b8t >. We say that X is the set decision and Y is the set reward. Let's say that the learner is the cumulative reward. The goal of the learner is to minimize the (external) regret, defined as: Regret = max w-X < w, basic potential \u2212 T-t = 1 < wt-t >. (1) The basic line function is: = maxw X < w > is the comparison term before which we define regret, and it matches the support function of X."}, {"heading": "3. Online Linear Optimization Algorithms via Smoothing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Gradient-Based Prediction Algorithm", "text": "Follow the leader algorithms solve an optimization target each round and play an action of the form wt = > argmaxw - X f (w, p \u2212 1) in view of a fixed target. < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.2. Smoothability of the Baseline Potential", "text": "Equation 2 shows that the regret of GBPA can be divided into two parts. A source of regret is the Bregman divergence of regret (\u03b2 \u00b7 \u03b2). Since the regret is only known when playing wt, the GBPA always rises along the gradient that is one step behind. An opponent can take advantage of this and cause a large gap between \u03a6t (xt) and the linear approximation of regret by selecting a soft gradient whose gradient changes slowly. However, the learner cannot achieve a small regret by choosing an arbitrarily smooth expression, because the other source of regret is the difference between \u03a6t and \u0438. In short, the GBPA achieves little regret when the potential function provides a favorable compromise between the two sources of regret. This goal conflict is represented by the following definition of flatness (Beck and Teboulle, 2012, definition 2.1)."}, {"heading": "3.3. Algorithms", "text": "(F)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4. Online Linear Optimization via Gaussian Smoothing", "text": "Gaussian smoothing is a standard technique for smoothing a function. In computer vision applications, for example, image pixels are considered a function of (x, y) coordinates, and Gaussian smoothing is used to blur sounds in the image. First, we present basic results on Gaussian smoothing using optimization literature. Definition 6 (Gaussian smoothing) Let Gaussian smoothing be defined as a function. Then, we define its Gaussian smoothing with a scaling parameter. > 0 and a covariance matrix."}, {"heading": "4.1. Experts Setting (\u21131-\u2113\u221e case)", "text": "The expert setting is where X = \"N def =\" (PRW) algorithm requires the knowledge of T \u2212 t and does not adapt to the data. The expert setting is where X = \"N\" (PRW) algorithm applies the scaling parameters that require the knowledge of T \u2212 t and not the adaptation to the data. (2013) The expert setting is where X = \"N.\" (2012) The algorithms use the scaling parameters T \u2212 t that require the knowledge of T \u2212 t and not the adaptation to the data. (2013) The expert setting is where the prediction of Random Walk (PRW) is proposed. (2012) The algorithms use the scaling parameters T \u2212 t that require the knowledge of T \u2212 t and not the adaptation to the data. (2013) The expert setting is where the prediction of Random Walk (PRW) is proposed. (2012) The algorithms use the scaling parameters that require the knowledge of T \u2212 t and do not require the adaptation of T \u2212 the knowledge."}, {"heading": "4.2. Online Linear Optimization over Euclidean Balls (\u21132-\u21132 case)", "text": "The Euclidean balls setting is where X = 1 = 2 + + 2 = 2 + + 2 = 2 = 2 = 2 = 2 = 2 = + + 2 + 2 + + 2 = 2 = 2 = + + 2 = 2 = + + 2 = 2 = 2 = + + 2 = 2 = 2 = + + 2 = 2 = 2 = 2 = + + 2 = 2 = + + + 2 = 2 = 2 = + + + 2 = 2 = 2 = + + + 2 = 2 = + 2 = 2 = + + 2 = 2 = 2 = + + 2 = 2 = 2 = + + + 2 = 2 = 2 = 2 = 2 = + + 2 = 2 = 2 = + + 2 = 2 = 2 = + + + 2 = 2 = 2 = + + 2 = 2 = + + + 2 = 2 = 2 = 2 = + + + + 2 = 2 = 2 = + + + 2 = 2 = + + 2 = 2 = + + 2 = 2 = + + 2 = 2 = 2 = + + + 2 = 2 = 2 = 2 = + + + 2 = 2 = + + 2 = 2 = 2 = + + 2 = + + + + 2 = + 2 = 2 = + + 2 = 2 = + 2 = + + + + 2 + 2 + 2 + 2 + 2 = + + + 2 + 2 + 2 + 2 + + + 2 + 2 + 2 + + 2 + 2 + 2 + + + + 2 + 2 + + + 2 + 2 + + + + + + + 2 + 2 + + 2 + + + 2 + + + 2 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "4.3. General Bound", "text": "In this section, we will use a generic property of Gaussian smoothing to derive a regret limit that applies to any linear online optimization problems. Lemma 13 (Duchi et al., 2012, Lemma E.2) Let's be a real convex function on a closed domain that is a subset of RN. Suppose that L-Lipschitz is what L-Lipschitz is, and let it be the Gaussian smoothing of the goods with the scaling parameter and identity covariance. Then, the basic function \"X-Lipschitz\" is a smoothing of L-Lipschitz with parameters (L-Lipschitz with respect to L-Lipschitz). Let's consider a case of linear online optimization with decision set X and reward set Y."}, {"heading": "4.4. Online Convex Optimization", "text": "In online linear optimization, the learner receives a sequence of convex functions ft whose domain is X and whose gradations are in the set Y (Zinkewitsch, 2003). After the learner plays wt-X, the reward function ft is revealed. The learner gains ft (wt) and observes that ft (wt), a gradation of ft at w.A simple linearization argument shows that our limits for linear online optimization to online convex optimization are generalized. Let's be the optimal fix point in retrospect. Regret is limited by linearized regret upward, as ft (w) \u2212 ft (wt) \u2264 < w \u2012 wt (wt) > for each gradation point ft ft (wt) >, and our analysis limits linearized regret. Unlike in online linearized optimization settings, the limit of regret applies, however, to the potentials with the FTL."}, {"heading": "5. Online Linear Optimization via Inf-conv Smoothing", "text": "Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal folding with a strongly smooth function. In this section we will show that FTRL is equivalent to the GBPA executed with the inf-conv smoothing of the potential baseline. Let (X) be a standardized vector space and (X) be its dual. Let (X) def = be a closed correct convex function, and let S be a strongly smooth function with respect to X."}, {"heading": "Acknowledgments", "text": "CL and AT gratefully acknowledge NSF's support for the IIS-1319810 grant. We would like to thank the anonymous reviewers for their helpful suggestions. We would also like to thank Andre Wibisono for some very useful discussions and his help in improving the manuscript. Finally, we would like to thank Elad Hazan for his early support in developing the ideas contained herein."}, {"heading": "Appendix A. Gradient-Based Prediction Algorithm", "text": "A.1. Proof of Lemma 2Proof We find that the first difference since \u03a60 (0) = 0 (0) = T (T) = T (T) = 1 (T) \u2212 1 (T) = 1 (T) \u2212 3 (T) = < T (T) = 1 (T) + (T) \u2212 1 (T) \u2212 1 (T)) by combining the above two factors: Regret = T (T) \u2212 T (T) \u2212 1 < T (T) = < T (T) = 1 (T), T (T) + T (T) + T (T) = 1Dject (T) + T (T) \u2212 1 < T (T) = 1 < T (T) > (T) \u2212 1 (T)."}, {"heading": "Appendix B. FTPL-FTRL Duality", "text": "B. 1. Proof Theory 5Proof Consider a one-dimensional linear optimization problem where the player chooses an action wt of X = [0, 1] and the opponent chooses a reward of Y = [0, 1]. This can in fact be interpreted as a double expert setting; the action of the player wt \"X\" is the probability of following the first expert, and the opponent chooses a net surplus of reward from the first expert over the second. The base potential for this setting is \"(0, 1) maxw.\" Consider an instance of FTPL with a continuous distribution D whose cumulative density function (cdf) is FD. Consider the smoothed potential function (equation 4) with the distribution D. Its derived isolation is \"(E) = E [argmax w.\" K w. \""}, {"heading": "Appendix C. Gaussian smoothing", "text": "C.1. Proof of the equation 10Let X1,., XN be independent standard Gaussian variables, and let Z = maxi = 1,..., N Xi. For any real number a, we haveexp (aE [Z]) \u2264 E exp (aZ) = E max i = 1,..., Nexp (aXi) \u2264 N = 1E [exp (aXi)] = N exp (a 2 / 2).The first inequality results from the convexity of the exponential function, and the last equality results from the definition of the moment in which the function of the Gaussian random variables is generated. Take the natural logarithm of both sides and divide it by a givesE [Z] \u2264 logN a + a 2.Specifically by the choice of a LogN that we choose."}], "references": [{"title": "Optimal stragies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Efficient market making via convex optimization, and a connection to online learning", "author": ["Jacob Abernethy", "Yiling Chen", "Jennifer Wortman Vaughan"], "venue": "ACM Transactions on Economics and Computation,", "citeRegEx": "Abernethy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2013}, {"title": "Smoothing and first order methods: A unified framework", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Beck and Teboulle.,? \\Q2012\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2012}, {"title": "Stochastic optimization problems with nondifferentiable cost functionals", "author": ["Dimitri P. Bertsekas"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bertsekas.,? \\Q1973\\E", "shortCiteRegEx": "Bertsekas.", "year": 1973}, {"title": "Adaptive newton-based multivariate smoothed functional algorithms for simulation optimization", "author": ["Shalabh Bhatnagar"], "venue": "ACM Transactions on Modeling and Computer Simulation,", "citeRegEx": "Bhatnagar.,? \\Q2007\\E", "shortCiteRegEx": "Bhatnagar.", "year": 2007}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Chris M. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Prediction by random-walk perturbation", "author": ["Luc Devroye", "G\u00e1bor Lugosi", "Gergely Neu"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Randomized smoothing for stochastic optimization", "author": ["John Duchi", "Peter L. Bartlett", "Martin J. Wainwright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Gradient Estimation Via Perturbation Analysis. Kluwer international series in engineering and computer science: Discrete event dynamic systems", "author": ["Paul Glasserman"], "venue": null, "citeRegEx": "Glasserman.,? \\Q1991\\E", "shortCiteRegEx": "Glasserman.", "year": 1991}, {"title": "Approximation to bayes risk in repeated play", "author": ["James Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Oddness of the number of equilibrium points: a new proof", "author": ["John C. Harsanyi"], "venue": "International Journal of Game Theory,", "citeRegEx": "Harsanyi.,? \\Q1973\\E", "shortCiteRegEx": "Harsanyi.", "year": 1973}, {"title": "Fractional moments of a quadratic form in noncentral normal random variables", "author": ["James R. Harvey"], "venue": null, "citeRegEx": "Harvey.,? \\Q1965\\E", "shortCiteRegEx": "Harvey.", "year": 1965}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "ArXiv preprint,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "On the global convergence of stochastic fictitious play", "author": ["Josef Hofbauer", "William H. Sandholm"], "venue": null, "citeRegEx": "Hofbauer and Sandholm.,? \\Q2002\\E", "shortCiteRegEx": "Hofbauer and Sandholm.", "year": 2002}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam T. Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization", "author": ["H. Brendan McMahan"], "venue": "In AISTATS,", "citeRegEx": "McMahan.,? \\Q2011\\E", "shortCiteRegEx": "McMahan.", "year": 2011}, {"title": "Theory of random sets. Probability and its applications", "author": ["Ilya S. Molchanov"], "venue": null, "citeRegEx": "Molchanov.,? \\Q2005\\E", "shortCiteRegEx": "Molchanov.", "year": 2005}, {"title": "Random gradient-free minimization of convex functions", "author": ["Yurii Nesterov"], "venue": "ECORE Discussion Paper,", "citeRegEx": "Nesterov.,? \\Q2011\\E", "shortCiteRegEx": "Nesterov.", "year": 2011}, {"title": "Relax and randomize : From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Convex Analysis. Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "On the universality of online mirror descent", "author": ["Nati Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim van Erven", "Wojciech Kotlowski", "Manfred K. Warmuth"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "Dropout training as adaptive regularization", "author": ["Stefan Wager", "Sida Wang", "Percy Liang"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Convex nondifferentiable stochastic optimization: A local randomized smoothing technique", "author": ["Farzad Yousean", "Angelia Nedi\u0107", "Uday V. Shanbhag"], "venue": "In Proceedings of American Control Conference (ACC),", "citeRegEx": "Yousean et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yousean et al\\.", "year": 2010}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 23, "context": "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools.", "startOffset": 105, "endOffset": 127}, {"referenceID": 22, "context": "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools. In fact, regularization via penalty methods for online learning in general are very well understood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.", "startOffset": 106, "endOffset": 316}, {"referenceID": 18, "context": "(2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.", "startOffset": 168, "endOffset": 183}, {"referenceID": 17, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).", "startOffset": 145, "endOffset": 216}, {"referenceID": 7, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).", "startOffset": 145, "endOffset": 216}, {"referenceID": 15, "context": "(2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": ", 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information.", "startOffset": 31, "endOffset": 51}, {"referenceID": 6, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary.", "startOffset": 171, "endOffset": 1018}, {"referenceID": 6, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary. In short, adding a random perturbation and adding a regularization penalty function are both optimal ways to simulate the worst-case future input sequence. We establish a stronger connection between FTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent up to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact that Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.", "startOffset": 171, "endOffset": 1575}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al.", "startOffset": 0, "endOffset": 165}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al.", "startOffset": 0, "endOffset": 805}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary online linear optimization problem.", "startOffset": 0, "endOffset": 829}, {"referenceID": 0, "context": "In fact, we can bound the Bregman divergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy et al. (2013, Lemma 4.6) shows. Lemma 1 Let f be a twice-differentiable convex function with domf \u2286 R . Let x \u2208 domf , such that vT\u22072f(x+\u03b1v)v \u2208 [a, b] (a \u2264 b) for all \u03b1 \u2208 [0, 1]. Then, a\u2016v\u20162/2 \u2264 Df (x+v, x) \u2264 b\u2016v\u20162/2. The Fenchel conjugate of f is f(\u03b8) = supw\u2208dom(f){\u3008w, \u03b8\u3009 \u2212 f(w)}, and it is a dual mapping that satisfies f = (f) and \u2207f\u22c6 \u2208 dom(f). By the strong convexity-strong smoothness duality, f is \u03b2-strongly smooth with respect to a norm \u2016 \u00b7 \u2016 if and only if f is 1 \u03b2 -strongly smooth with respect to the dual norm \u2016 \u00b7 \u2016\u22c6. For more details and proofs, readers are referred to an excellent survey by Shalev-Shwartz (2012).", "startOffset": 120, "endOffset": 759}, {"referenceID": 7, "context": "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al.", "startOffset": 144, "endOffset": 166}, {"referenceID": 7, "context": "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.", "startOffset": 144, "endOffset": 193}, {"referenceID": 11, "context": "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.", "startOffset": 133, "endOffset": 173}, {"referenceID": 27, "context": "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.", "startOffset": 133, "endOffset": 173}, {"referenceID": 9, "context": ", 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al., 2012).", "startOffset": 70, "endOffset": 90}, {"referenceID": 12, "context": "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.", "startOffset": 66, "endOffset": 105}, {"referenceID": 17, "context": "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.", "startOffset": 66, "endOffset": 105}, {"referenceID": 10, "context": "Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire, 1997), which can be cast as FTRL with an entropic regularization R(w) = \u2211i wi logwi, is equivalent to FTPL with Gumbel-distributed noise.", "startOffset": 58, "endOffset": 85}, {"referenceID": 16, "context": "However, the result appears to be folklore in the area of probabilistic choice models, and it is mentioned briefly in Hofbauer and Sandholm (2002).", "startOffset": 118, "endOffset": 147}, {"referenceID": 20, "context": "In the game theoretic analysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter \u03b7t = \u221a T \u2212 t, which requires the knowledge of T and does not adapt to data.", "startOffset": 60, "endOffset": 82}, {"referenceID": 7, "context": "Devroye et al. (2013) proposed the Prediction by Random Walk (PRW) algorithm, which flips a fair coin every round and decides whether to add 1 to each coordinate.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "This is in contrast to the Hedge Algorithm (Freund and Schapire, 1997), which is an \u03b7-smoothing with parameters (logN, 1, \u2016 \u00b7 \u2016) (See Section 5 for details).", "startOffset": 43, "endOffset": 70}, {"referenceID": 0, "context": "We show that the GBPA with Gaussian smoothing achieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.", "startOffset": 80, "endOffset": 104}, {"referenceID": 28, "context": "Online Convex Optimization In online convex optimization, the learner receives a sequence of convex functions ft whose domain is X and its subgradients are in the set Y (Zinkevich, 2003).", "startOffset": 169, "endOffset": 186}, {"referenceID": 2, "context": "Online Linear Optimization via Inf-conv Smoothing Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal convolution with a strongly smooth function.", "startOffset": 50, "endOffset": 75}, {"referenceID": 23, "context": "21 of Shalev-Shwartz (2012):", "startOffset": 6, "endOffset": 28}], "year": 2014, "abstractText": "We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between \u201cFollow the Regularized Leader\u201d and \u201cFollow the Perturbed Leader\u201d up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.", "creator": "LaTeX with hyperref package"}}}