{"id": "1405.6524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning", "abstract": "Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and \"unsupervised\", meaning it requires no manual data labelling, yet it can improve performance on \"supervised\" tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, with a random forest classifier. We demonstrate that MFCCs are of limited power in this context, leading to worse performance than the raw Mel spectral data. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain.", "histories": [["v1", "Mon, 26 May 2014 09:58:20 GMT  (646kb,D)", "http://arxiv.org/abs/1405.6524v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["dan stowell", "mark d plumbley"], "accepted": false, "id": "1405.6524"}, "pdf": {"name": "1405.6524.pdf", "metadata": {"source": "CRF", "title": "Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning", "authors": ["Dan Stowell", "Mark D. Plumbley"], "emails": [], "sections": [{"heading": null, "text": "To make classification useful in practice, it is critical to improve its accuracy while ensuring that it can operate on large data scales. Many approaches use acoustic measures based on spectrogram-like data, such as the Mel Frequency Receiver Coefficient (MFCC), which is a manually designed summary of spectral information. However, recent machine learning work has shown that features learned automatically from data often transform manually designed features. Feature learning can be performed on a large scale and \"unattended,\" meaning that it does not require manual data labeling, but can improve performance in \"supervised\" tasks such as classification. In this work, we are introducing a technique for learning large amounts of bird noise recording, inspired by techniques that have proven useful in other areas."}, {"heading": "1 INTRODUCTION", "text": "The classification of bird species by their sounds has many potential applications in conservation, ecology and archiving, meaning that they are audible only to a minority. [Laiolo, 2010, Digby et al., 2013, Ranft, 2004]. Nevertheless, it must work with high accuracy across a large number of possible species, with the ability to scale to large amounts of data crucial: remote sensing of bird sounds in digital formats can generate enormous amounts of audio recordings [Aide et al., 2013], and audio archives contain large amounts of audio recordings, much of which are without detailed labelling. For example, the British Library Sound Archive holds over 100,000 records of bird sounds in digital formats, from various sources [Ranft, 2004]. Large data scales must also function without manual intervention, particularly without manual segmentation of recordings in vocal syllables, or in vocal / silent sections. The lack of permanent segmentation is a minority problem for many bird species, and many archival collections are only a minority."}, {"heading": "1.1 Spectral features and feature learning", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 MATERIALS AND METHODS", "text": "Our primary experiment evaluated the automatic classification of species separately over four different sets of bird song data sets. For each set, we trained and tested a random forest classifier [Breiman, 2001], systematically varying the following configuration parameters to determine their impact on performance: 3 / 24 \u2022 Selection of characteristics (MFCCs, Mel spectra or learned characteristics) and their summary over time (mean and standard deviation, maximum or modulation coefficients); \u2022 Whether or not to apply noise reduction to audio spectra as a pre-processing step; \u2022 Decision window: whether to treat the full audio as a single unit for training / testing purposes or to divide it into shorter time frames (1, 5 or 60 seconds); \u2022 How to make an overall decision when using decision diapers (on the mean or maximum of probabilities); \u2022 The classifiers based on their settings and configurations."}, {"heading": "2.1 Datasets", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2.2 Feature learning method", "text": "As discussed in Section 1.1, the goal of uncontrolled learning is to find some transformation patterns of a dataset driven only by the features contained in that dataset, using a method that has been promising in previous studies and can be effectively executed on big data scales: spherical k-means described by Coates and Ng [2012] and initially applied to audio data used by Dieleman and Schrauwen [2013]. Spherical k-means refer to the well-known k-means clustering algorithms [Lloyd 1982], except that instead of looking for cluster centroids that minimize the euclidean at the data points, we look for unit vectors (instructions) to minimize their angular distance from the data points. This is achieved by modifying the iterative actualization process for the k-means algorithms: for an input data point, rather than testing the closest one is 24th."}, {"heading": "2.3 Classification and evaluation", "text": "It's not that you get involved in a situation like that, it's not that you get involved in a situation like that. It's not that you get involved in a situation like that. It's not that you get involved in a situation like that. It's not that you get involved in a situation like that. It's not that you get involved in a situation like that. It's not that you get involved in a situation like that. It's not that you get involved in a situation like that."}, {"heading": "2.4 Additional tests", "text": "The bldawn dataset has relatively few annotations, as it consists of only 60 elements, so we wanted to explore the use of auxiliary information from other sources to improve detection quality, especially using the xccoverbl dataset, which has strong overlaps in the list of species under consideration. In further tests, we tested three ways to use this additional data: 9 / 241. Cross-condition training, i.e. training on one dataset and testing on the other. The two datasets exhibit systematic differences - for example, xccoverbl items are commented with only one type each and are generally shorter - so we did not expect this to produce very strong results."}, {"heading": "3 RESULTS", "text": "It is not just a question of expression, but also a question of expression that affects the way we have found in our tests, the question of the impact on the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved, the way we have behaved."}, {"heading": "4 DISCUSSION", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "5 CONCLUSIONS", "text": "The current interest in automatic bird song classification is motivated by the practical scientific need to identify large volumes of data coming from sources such as remote monitoring stations and sound archives. Unsupervised feature learning is a simple and effective way to increase classification performance by learning spectral regularities in the data. However, it does not require training labels or other side information, it can be used within a classification workflow, and once trained, it requires negligible additional computational effort on the classifier. In experiments, it learns regularities in bird song with similar qualities reported by others. The main practical problem with uncontrolled feature learning is that it requires large volumes of data to be effective, as confirmed in our tests, a synergy with the large volumes of data that are increasingly becoming the standard."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank the people and projects that have provided the data used for this research: the Xeno Canto website and its many volunteer contributors; the SABIOD research project and the Biotope Society; the British Library Sound Archive and its contributors; and the curator Cheryl Tipp.FUNDING SOURCES This work was supported by the EPSRC Leadership Fellowship EP / G007144 / 1 and the EPSRC Early Career Fellowship EP / L020505 / 1.DATA AVAILABILITY \u2022 The xccoverbl data is archived at https: / archive.org / details / xccoverbl _ 2014 - it consists of sound files sourced from http: / / www.xeno-canto.org /. The authors \"sound file and other metadata are included in a CSV file included in the dataset.21 / 24 \u2022 The Dataset.level is available on request from the British Library (BLAB)."}], "references": [{"title": "BTO Bird atlas 2007\u201311: The breeding and wintering birds of Britain and Ireland", "author": ["D. Ballmer"], "venue": null, "citeRegEx": "Ballmer,? \\Q1996\\E", "shortCiteRegEx": "Ballmer", "year": 1996}, {"title": "Audio classification of bird species: A statistical manifold approach", "author": ["L. Breiman"], "venue": "Random forests. Machine Learning,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2001}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "Caruana and Niculescu.Mizil.,? \\Q2012\\E", "shortCiteRegEx": "Caruana and Niculescu.Mizil.", "year": 2012}, {"title": "Bayesian classification of flight calls", "author": ["T. Damoulas", "S. Henry", "A. Farnsworth", "M. Lanzone", "C. Gomes"], "venue": null, "citeRegEx": "Damoulas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Damoulas et al\\.", "year": 2012}, {"title": "Why does unsupervised", "author": ["1111/2041-210X.12060. D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2060}, {"title": "Call-independent identification in birds", "author": [], "venue": "PhD thesis, University of Western Australia,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "LifeCLEF bird identification task 2014", "author": ["H. Glotin", "W.-P. Vellinga", "A. Rauber"], "venue": null, "citeRegEx": "Go\u00ebau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Go\u00ebau et al\\.", "year": 2014}, {"title": "A syllable-level probabilistic framework for bird species", "author": ["B. Lakshminarayanan", "R. Raich", "X. Fern"], "venue": null, "citeRegEx": "Lakshminarayanan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lakshminarayanan et al\\.", "year": 2010}, {"title": "More like this: Machine learning approaches to music similarity", "author": [], "venue": "PhD thesis,", "citeRegEx": "1982", "shortCiteRegEx": "1982", "year": 1982}, {"title": "Sparse coding of sensory inputs", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Natural sound archives: Past, present and future", "author": ["R. Ranft"], "venue": "Computing, Vienna, Austria,", "citeRegEx": "Ranft.,? \\Q2012\\E", "shortCiteRegEx": "Ranft.", "year": 2012}, {"title": "Feature design for multilabel bird song classification in noise (nips4b", "author": ["M.D. Plumbley"], "venue": null, "citeRegEx": "Stowell and Plumbley.,? \\Q2010\\E", "shortCiteRegEx": "Stowell and Plumbley.", "year": 2010}, {"title": "Auditory processing of vocal sounds in birds", "author": ["F.E. Theunissen", "S.S. Shaevitz"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Theunissen and Shaevitz.,? \\Q2006\\E", "shortCiteRegEx": "Theunissen and Shaevitz.", "year": 2006}, {"title": "Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667\u2013685", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2010}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In Proceedings of the International Conference on Research and development in information retrieval (SIGIR", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Keywords: bioacoustics, machine learning, birds, classification, vocalisation, birdsong 1 INTRODUCTION Automatic species classification of birds from their sounds has many potential applications in conservation, ecology and archival [Laiolo, 2010, Digby et al., 2013, Ranft, 2004]. However, to be useful it must work with high accuracy across large numbers of possible species, on noisy outdoor recordings and at big data scales. The ability to scale to big data is crucial: remote monitoring stations can generate huge volumes of audio recordings [Aide et al., 2013], and audio archives contain large volumes of audio, much of it without detailed labelling. For example the British Library Sound Archive holds over 100,000 recordings of bird sound in digital format, from various sources [Ranft, 2004]. Big data scales also imply that methods must work without manual intervention, in particular without manual segmentation of recordings into song syllables, or into vocal/silent sections. The lack of segmentation is a pertinent issue for both remote monitoring and archive collections, since many species of bird may be audible for only a minority of the recorded time, and therefore much of the audio will contain irrelevant information. The task of classifying bird sounds by species has been studied by various authors, at least as far back as McIlraith and Card [1997]. (See Stowell and Plumbley [2010] for a survey.", "startOffset": 244, "endOffset": 1376}, {"referenceID": 3, "context": "Keywords: bioacoustics, machine learning, birds, classification, vocalisation, birdsong 1 INTRODUCTION Automatic species classification of birds from their sounds has many potential applications in conservation, ecology and archival [Laiolo, 2010, Digby et al., 2013, Ranft, 2004]. However, to be useful it must work with high accuracy across large numbers of possible species, on noisy outdoor recordings and at big data scales. The ability to scale to big data is crucial: remote monitoring stations can generate huge volumes of audio recordings [Aide et al., 2013], and audio archives contain large volumes of audio, much of it without detailed labelling. For example the British Library Sound Archive holds over 100,000 recordings of bird sound in digital format, from various sources [Ranft, 2004]. Big data scales also imply that methods must work without manual intervention, in particular without manual segmentation of recordings into song syllables, or into vocal/silent sections. The lack of segmentation is a pertinent issue for both remote monitoring and archive collections, since many species of bird may be audible for only a minority of the recorded time, and therefore much of the audio will contain irrelevant information. The task of classifying bird sounds by species has been studied by various authors, at least as far back as McIlraith and Card [1997]. (See Stowell and Plumbley [2010] for a survey.", "startOffset": 244, "endOffset": 1410}, {"referenceID": 6, "context": ", 2013][Fodor, 2013][Go\u00ebau et al., 2014].", "startOffset": 20, "endOffset": 40}, {"referenceID": 11, "context": "These coefficients, the Mel frequency cepstral coefficients (MFCCs), became widespread in applications of machine learning to audio, including bird vocalisations [Stowell and Plumbley, 2010].", "startOffset": 162, "endOffset": 190}, {"referenceID": 9, "context": "However, such a transformation can help to reveal the manifold structure that may be present in the data [Olshausen and Field, 2004]. Neural networks, both in machine implemetations and in animals, perform such a dimension expansion in cases where one layer of neurons is connected as input to a larger layer of neurons [Olshausen and Field, 2004]. In our study, however, we will not use a feature learning procedure intended to parallel a biological process. Instead, we use spherical k-means, a simple and highly scalable modification of the classic k-means algorithm [Coates and Ng, 2012, Dieleman and Schrauwen, 2013]. We perform a further adaptation of the algorithm to ensure that it can run in streaming fashion across large audio datasets, to be described in Section 2. Birdsong often contains rapid temporal modulations, and this information should be useful for identifying species-specific characteristics [Stowell and Plumbley, 2014]. From this perspective, a useful aspect of feature learning is that it can be applied not only to single spectral frames, but to short sequences (or \u201cpatches\u201d) of a few frames. The representation can then reflect not only characteristics of instantaneous frequency patterns in the input data, but characteristics of frequencies and their short-term modulations, such as chirps sweeping upwards or downwards. This bears some analogy with the \u201cdelta-MFCC\u201d features sometimes used by taking the first difference in the time series of MFCCs, but is more flexible since it can represent amplitude modulations, frequency modulations, and correlated modulations of both sorts (cf. Stowell and Plumbley [2014]).", "startOffset": 106, "endOffset": 1648}, {"referenceID": 4, "context": "We generated synthetic 2D data points by sampling from three clusters which were each Gaussian-distributed in terms of their angle and log-magnitude (coloured dots), and then applied our online spherical k-means algorithm to find 10 unit vectors (crosses). These unit vectors form an overcomplete basis with which one could represent this toy data, projecting two-dimensional space to ten-dimensional space. bldawn dataset. For each species included, we queried Xeno Canto to retrieve three different recordings, preferring to retrieve recordings from the UK, but allowing the system to return recordings from further afield if too few UK recordings were available. Our search query also requested high-quality recordings (quality label \u2018A\u2019), and song rather than calls, where possible. Since we retrieved three examples for each species, this enabled us to partition the dataset for three-fold crossvalidation: not stratified into individual recordists (as was bldawn), but sampled from a wide range of recordists. These datasets have widely varying characteristics, for example in the typical duration of the sound files, the recording location, and the number of classes to distinguish (Table 1). Note that most of the datasets have different and irreconcilable lists of class labels: in particular, for bldawn and xccoverbl the class label is the species, whereas nips4b and lifeclef2014 use separate labels for song and calls. Of our datasets only bldawn and xccoverbl have strong overlap in their species lists. Therefore only these datasets could be combined to create larger pools of training data. In this work we performed automatic classification for each audio file, without any segmentation procedure to select region(s) of bird vocalisation in the file. The only segmentation that is done is implicit in the collection processes for the dataset: for the two datasets originating from Xeno Canto, each audio clip might or might not contain a large amount of silence or other noise, depending on the contributor; for nips4b the audio is collected from remote monitoring stations with no manual selection; for bldawn the audio is selected by the contributor, but not trimmed to a specific vocalisation, instead selected to present a long dawn chorus audio recording. 2.2 Feature learning method As discussed in Section 1.1, the aim of unsupervised feature learning is to find some transformation of a dataset, driven only by the characteristics inherent in that dataset. For this we use a method that has has shown promise in previous studies, and can be run effectively at big data scales: spherical k-means, described by Coates and Ng [2012] and first applied to audio by Dieleman and Schrauwen [2013].", "startOffset": 230, "endOffset": 2655}, {"referenceID": 4, "context": "We generated synthetic 2D data points by sampling from three clusters which were each Gaussian-distributed in terms of their angle and log-magnitude (coloured dots), and then applied our online spherical k-means algorithm to find 10 unit vectors (crosses). These unit vectors form an overcomplete basis with which one could represent this toy data, projecting two-dimensional space to ten-dimensional space. bldawn dataset. For each species included, we queried Xeno Canto to retrieve three different recordings, preferring to retrieve recordings from the UK, but allowing the system to return recordings from further afield if too few UK recordings were available. Our search query also requested high-quality recordings (quality label \u2018A\u2019), and song rather than calls, where possible. Since we retrieved three examples for each species, this enabled us to partition the dataset for three-fold crossvalidation: not stratified into individual recordists (as was bldawn), but sampled from a wide range of recordists. These datasets have widely varying characteristics, for example in the typical duration of the sound files, the recording location, and the number of classes to distinguish (Table 1). Note that most of the datasets have different and irreconcilable lists of class labels: in particular, for bldawn and xccoverbl the class label is the species, whereas nips4b and lifeclef2014 use separate labels for song and calls. Of our datasets only bldawn and xccoverbl have strong overlap in their species lists. Therefore only these datasets could be combined to create larger pools of training data. In this work we performed automatic classification for each audio file, without any segmentation procedure to select region(s) of bird vocalisation in the file. The only segmentation that is done is implicit in the collection processes for the dataset: for the two datasets originating from Xeno Canto, each audio clip might or might not contain a large amount of silence or other noise, depending on the contributor; for nips4b the audio is collected from remote monitoring stations with no manual selection; for bldawn the audio is selected by the contributor, but not trimmed to a specific vocalisation, instead selected to present a long dawn chorus audio recording. 2.2 Feature learning method As discussed in Section 1.1, the aim of unsupervised feature learning is to find some transformation of a dataset, driven only by the characteristics inherent in that dataset. For this we use a method that has has shown promise in previous studies, and can be run effectively at big data scales: spherical k-means, described by Coates and Ng [2012] and first applied to audio by Dieleman and Schrauwen [2013]. Spherical k-means is related to the well-known k-means clustering algorithm [Lloyd, 1982], except that instead of searching for cluster centroids which minimise the Euclidean distance to the data points, we search for unit vectors (directions) to minimise their angular distance from the data points.", "startOffset": 230, "endOffset": 2715}, {"referenceID": 13, "context": "Binary relevance: this divides the multilabel classification task into many single-label tasks, training one separate classifier for each of the potential output labels [Tsoumakas et al., 2010].", "startOffset": 169, "endOffset": 193}, {"referenceID": 13, "context": "Predicting presence/absence of every label simultaneously can be computationally difficult compared against a single-label task, and may require larger training data volumes, but represents the full situation in one model [Tsoumakas et al., 2010].", "startOffset": 222, "endOffset": 246}, {"referenceID": 1, "context": "We filtered out spectral energy below 500 Hz, a common choice to reduce the amount of environmental noise present, and then normalised the root-mean-square (RMS) energy in each spectrogram. For each spectrogram we then optionally applied the noise-reduction procedure that we had found to be useful in our NIPS4B contest submission [Stowell and Plumbley, 2013], a simple and common medianbased thresholding. This consists of finding the median value for each spectral band in a spectrogram, then subtracting this median spectrum from every frame, and setting any resulting negative values to zero. This therefore preserves only the spectral energy that rises above the median bandwise energy. In principle it is a good way to reduce the stationary noise background, but is not designed to cope well with fluctuating noise. However its simplicity makes it easy to apply across large datasets efficiently. The Mel spectrograms, either noise-reduced or otherwise, could be used directly as features. We also tested their reduction to MFCCs (including delta features, making 26-dimensional data), and their projection onto learned features, using the spherical k-means method described above. For the latter option, we tested projections based on single frame as well as on sequences of 2, 3, 4 and 8 frames, to explore the benefit of modelling short-term temporal variation. We also tested the two-layer version based on the repeated application to 4-frame sequences across two timescales. The feature representations thus derived were all time series. In order to reduce them to summary features for use in the classifier, we tested two common and simple techniques: summarising each feature dimension independently by its mean and standard deviation, or alternatively by its maximum. These are widespread but are not designed to reflect any temporal structure in the features (beyond the fine-scale temporal information that is captured by some of our features). Therefore, for the Mel and MFCC features we also tested summarising by modulation coefficients: we took the short-time Fourier transform (STFT) along the time axis of our features, and then downsampled the spectrum to a size of 10 to give a compact representation of the temporal evolution of the features (cf. Lee et al. [2008]).", "startOffset": 38, "endOffset": 2291}, {"referenceID": 1, "context": "We filtered out spectral energy below 500 Hz, a common choice to reduce the amount of environmental noise present, and then normalised the root-mean-square (RMS) energy in each spectrogram. For each spectrogram we then optionally applied the noise-reduction procedure that we had found to be useful in our NIPS4B contest submission [Stowell and Plumbley, 2013], a simple and common medianbased thresholding. This consists of finding the median value for each spectral band in a spectrogram, then subtracting this median spectrum from every frame, and setting any resulting negative values to zero. This therefore preserves only the spectral energy that rises above the median bandwise energy. In principle it is a good way to reduce the stationary noise background, but is not designed to cope well with fluctuating noise. However its simplicity makes it easy to apply across large datasets efficiently. The Mel spectrograms, either noise-reduced or otherwise, could be used directly as features. We also tested their reduction to MFCCs (including delta features, making 26-dimensional data), and their projection onto learned features, using the spherical k-means method described above. For the latter option, we tested projections based on single frame as well as on sequences of 2, 3, 4 and 8 frames, to explore the benefit of modelling short-term temporal variation. We also tested the two-layer version based on the repeated application to 4-frame sequences across two timescales. The feature representations thus derived were all time series. In order to reduce them to summary features for use in the classifier, we tested two common and simple techniques: summarising each feature dimension independently by its mean and standard deviation, or alternatively by its maximum. These are widespread but are not designed to reflect any temporal structure in the features (beyond the fine-scale temporal information that is captured by some of our features). Therefore, for the Mel and MFCC features we also tested summarising by modulation coefficients: we took the short-time Fourier transform (STFT) along the time axis of our features, and then downsampled the spectrum to a size of 10 to give a compact representation of the temporal evolution of the features (cf. Lee et al. [2008]). The multi-frame feature representations already intrinstically included short-term summarisation of temporal variation, so to limit the overall size of the experiment, for the learned feature representations we only applied the mean-and-standard-deviation summarisation. Overall we tested six types of non-learned representation against six types of learned representation (Table 2). To perform classification on our temporally-pooled feature data, then, we used a random forest classifier [Breiman, 2001]. Random forests and other tree-ensemble classifiers perform very strongly in a wide range of empirical evaluations [Caruana and Niculescu-Mizil, 2006], and were used by many of the strongest-performing entries to the SABIOD evaluation contests [Glotin et al., 2013, Fodor, 2013, Potamitis, 2014]. For this experiment we used the implementation from the Python scikit-learn project [Pedregosa et al., 2011]. Note that scikit-learn v0.14 was found to have a specific issue preventing training on large data, so we used a pre-release v0.15 after verifying that it led to the same results with our smaller datasets. We did not manually tune any parameters of the classifier. However, since our experiment covered both single-label and multilabel classification, we did test three different ways of using the classifier to make decisions: 1. Single-label classification: this assumes that there is only one species present in a recording. It therefore cannot be applied to multilabel datasets, but for single-label datasets it may benefit from being well-matched to the task. 2. Binary relevance: this divides the multilabel classification task into many single-label tasks, training one separate classifier for each of the potential output labels [Tsoumakas et al., 2010]. This strategy ignores potential correlations between label occurrence, but potentially allows a difficult task to be approximated as the combination of more manageable tasks. Binary relevance is used e.g. by Fodor [2013]. 3.", "startOffset": 38, "endOffset": 4289}, {"referenceID": 14, "context": "The key difference in effect is that the AUC statistic applies an equal penalty for a mis-ordering at any position on the ranked list, whereas the MAP statistic assigns greater penalties higher up the ranking [Yue et al., 2007].", "startOffset": 209, "endOffset": 227}, {"referenceID": 6, "context": "Summary of MAP scores attained by our system in the public LifeCLEF 2014 Bird Identification Task [Go\u00ebau et al., 2014].", "startOffset": 98, "endOffset": 118}, {"referenceID": 6, "context": "A formal comparison was conducted in Spring 2014 when we submitted decisions from our system to the LifeCLEF 2014 bird identification challenge [Go\u00ebau et al., 2014].", "startOffset": 144, "endOffset": 164}, {"referenceID": 6, "context": "Summary of MAP scores attained by our system in the public LifeCLEF 2014 Bird Identification Task [Go\u00ebau et al., 2014]. The first column lists scores attained locally in our two-fold lifeclef2014 split. The second column lists scores evaluated officially, using a classifier(s) trained across the entire training set. the random-projection modification was a small but significant impairment (\u22120.07). We can compare our results against those obtained recently by others. A formal comparison was conducted in Spring 2014 when we submitted decisions from our system to the LifeCLEF 2014 bird identification challenge [Go\u00ebau et al., 2014]. In that evaluation, our system attained by far the strongest audio-only classification results, with a MAP peaking at 42.9% (Table 4). (Only one system outperformed ours, peaking at 51.1% in a variant of the challenge which provided additional metadata as well as audio.) We submitted the outputs from individual models, as well as model-averaging runs using the simple mean of outputs from multiple models. Notably, the strongest classification both in our own tests and the official evaluation was attained not by model averaging, but by a single model based on two-layer feature learning. Also notable is that our official scores, which were trained and tested on larger data subsets, were substantially higher than our crossvalidated scores, corroborating our observation that the method works particularly well at high data volumes. Considering the nips4b dataset, the peak result from our main tests reached a crossvalidated AUC of 89.8%. In the actual NIPS4B contest (conducted before our current approach was developed), the winning result attained 91.8%; Potamitis [2014], developing further a model submitted to the contest, reports a peak AUC of 91.", "startOffset": 99, "endOffset": 1718}, {"referenceID": 4, "context": "[2010]). The procedure requires large data volumes in order for benefits to be apparent, as indicated by the lack of improvement on our bldawn dataset, and by the failure of two-layer feature learning on the nips4b dataset. However, the use of single-layer feature learning creates a classifier that is equivalent to or better than manually-designed features in all our tests. There were very few differences in performance between our different versions of feature learning. One difference is that two-layer feature learning, while unsuccessful for nips4b, led to the strongest performance for lifeclef2014 which is the largest dataset considered\u2014largest by an order of magnitude in data volume, and by almost an order of magnitude in the number of possible species labels. This confirms the recommendations of Coates and Ng [2012] about the synergy between feature learning and big data scales, here for the case of ecological audio data.", "startOffset": 3, "endOffset": 833}, {"referenceID": 3, "context": "Figure 10. Spectro-temporal receptive fields (STRFs) measured from individual neurons in auditory field L of starling. Adapted from Hausberger et al. [2000] for comparison with Fig.", "startOffset": 7, "endOffset": 157}, {"referenceID": 3, "context": "Other recent methods use templates and DTW but deployed within a kernel-based distance measure, again going beyond a simple one-to-one match [Damoulas et al., 2010]. In light of these other perspectives, we note an analogy with our learned features. In the workflow considered here, the new representation is calculated by taking the dot-product between the input data and each base (such as those in Fig. 9), as given in (3). The form of (3) is the same mathematically as spectro-temporal cross-correlation, where the b j would be thought of more traditionally as \u201ctemplates\u201d. Our features are thus equivalent to the output of an unusual kind of template matching by cross-correlation, where the \u201ctemplates\u201d are not indivudal audio excerpts but generalisations of features found broadly across audio excerpts, and are also of a fixed short duration (shorter than many song syllables, though long enough to encompass many calls). A question that arises from this perspective is whether our approach should use longer series of frames, long enough to encompass many types of song syllable entirely. In our tests we found no notable tendency for improved recognition as we increased the number of frames from one to eight, and we also saw many temporally-compact bases learnt (Fig. 9), so we do not believe lengthening the bases is the route to best performance. Further, the advantage of using relatively short durations is that the feature learning method learns components of bird vocalisations rather than over-specific whole units. These components may co-occur in a wide variety of bird sounds, in temporally-flexible orders, conferring a combinatorial benefit of broad expressivity. Our two-layer feature learning provides a further level of abstraction over temporal combinations of energy patterns, which is perhaps part of its advantage when applied to our largest dataset. We have not explicitly tested our method in comparison to template-based approaches; the relative merits of such approaches will become clear in further research. The bases shown in Fig. 9 also bear some likeness with the spectro-temporal receptive fields (STRFs) measured from neurons found in the early auditory system of songbirds (e.g. Fig. 10, adapted from Hausberger et al. [2000]).", "startOffset": 142, "endOffset": 2269}], "year": 2014, "abstractText": "Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and \u201cunsupervised\u201d, meaning it requires no manual data labelling, yet it can improve performance on \u201csupervised\u201d tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, with a random forest classifier. We demonstrate that MFCCs are of limited power in this context, leading to worse performance than the raw Mel spectral data. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain. However, for one of our datasets, which contains substantial audio data but few annotations, increased performance is not discernible. We study the interaction between dataset characteristics and choice of feature representation through further empirical analysis.", "creator": "LaTeX with hyperref package"}}}