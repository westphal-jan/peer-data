{"id": "1405.6682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Optimality Theory as a Framework for Lexical Acquisition", "abstract": "This paper re-investigates a lexical acquisition system initially developed for French.We show that, interestingly, the architecture of the system reproduces and implements the main components of Optimality Theory. However, we formulate the hypothesis that some of its limitations are mainly due to a poor representation of the constraints used. Finally, we show how a better representation of the constraints used would yield better results.", "histories": [["v1", "Mon, 26 May 2014 18:51:06 GMT  (26kb)", "http://arxiv.org/abs/1405.6682v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thierry poibeau"], "accepted": false, "id": "1405.6682"}, "pdf": {"name": "1405.6682.pdf", "metadata": {"source": "CRF", "title": "Optimality Theory as a Framework for Lexical Acquisition", "authors": ["Thierry Poibeau"], "emails": ["thierry.poibeau@ens.fr"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.66 82v1 [cs.CL] 2 6"}, {"heading": "1 Introduction", "text": "This year, it will be able to establish itself in the region."}, {"heading": "2 From Corpus to Resources", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 OT and Syntax", "text": "The central idea of P & P is that a person's syntactic knowledge can be modelled using two formal mechanisms: for example, a finite set of basic principles that determines whether the subject of a sentence must be pronounced openly, even if it is not pronounced openly. - A finite set of parameters that determine syntactic variability between languages. - A binary parameter that determines whether the subject of a sentence must be pronounced openly or not. - Within this framework, linguistics aims to identify all the principles and parameters that determine syntactic variability between languages."}, {"heading": "2.2 Learning Syntactic Frames from Raw Data", "text": "As I said, comprehensive and accurate lexical resources are key components of natural language processing (NLP). Developing lexical resources is difficult and extremely labor-intensive - especially since NLP systems require statistical information about the behavior of lexical elements in context, and this statistical information changes from one domain to another. Therefore, automatic acquisition of lexical resources is becoming increasingly popular. One of the most useful lexical information for NLP is that it relates to the predictor structure. Subcategorization frames (SCFs) of a predicate capture the different combinations of arguments that a given predicate can take."}, {"heading": "2.3 Introducing Gradience in Lexical Acquisition", "text": "Most authors agree that the additions must be divided between the arguments and appendices, but the distinction between these two categories is far from obvious. Some linguistic tests exist without changing the meaning of the sentence. Is it easy to shift them? Is it easy to pronounce them?"}, {"heading": "3 ASSCI, A State-of-the Art Subcategorization Acquisition System for French", "text": "This system, called ASSCI, is capable of capturing large-format lexicographies from uncommented corpora [20,21]. This system is similar to other systems developed for example for English [16,22], in that it extracts SCFs from data analyzed using a shallow dependency parser [23] and is able to identify a large number of SCFs. However, unlike most other systems that accept raw body data as input, it does not start from a list of predefined SCFs. The system is based on the assumption that the most relevant SCF corresponding to a given surface shape results directly from the application of the constraints on the various candidates, as postulated by OT.ASSCI, taking raw body data as input. Input text is first tagged and syntactically analyzed."}, {"heading": "3.1 Preprocessing : Morphosyntactic Tagging and Syntactic Analysis", "text": "The system first marks and lemmatizes corpus data using TreeTagger and then analyzes it using Syntex [23]. Syntex is a flat parser for French, using a combination of heuristics and statistics to find dependency relationships between the marks in a sentence. It is a relatively accurate parser, for example, it obtained the best precision and F measurement for written French text in the first EASY evaluation campaign (2006).The following example illustrates the dependence relationships established by Syntex (2) for the input set in (1): (1) La se \u0301 cheresse s'abattit sur le Sahel 1972-1973. (2) The drought returned to Sahel in 1972-1973.) (2) DetFS | NOCHS | 1 | DET; 2 | NomFS; 2 | NUMFS \u2012 NOCHERESS | 2 | 2 | NOCHERESS | 2 | NOCHRES, 2 | 2 | RECHRES | 2 | 2, RECHRES | 2, PRES | 2, PRES, 4, PRES, PRES, 4, PRES, PRES, 4 |, PRES, 4, PRES, PRES, PRES, 1, PRES, PRES, 1, PRES, PRES, 1, PRES, PRES, 1, PRES, PRES, 1, PRES, PRES, 2 |, PRES, 1, PRES, 1, PRES, PRES, 2 |, PRES, PRES, 2, PRES, 2, PRES, PRES, 4 |, PRES, PRES, 1, PRES, 1, PRES, 1, PRES, PRES, 1, PRES, PRES, 1, 1, 1, PRES, 2 | PRES, 1, PRES, 1, 1, PRES, 1, 2 | PRES, 1, PRES, 1, 2, PRES, 2, PRES, 2, 1, PRES, 1, 2, PRES, 1, 2, PRES, 2, 2, PRES, 2, PRES, 2, 2, PRES, 1, 2, PRES, 1, 2, PRES, 2, PRES, PRES, 2"}, {"heading": "3.2 Producing the Input (the Pattern Extractor)", "text": "The Pattern Extractor collects the dependencies found by the parser for each occurrence of a target verb. Some cases get special treatment in this module. For example, if the pronoun \"se\" is one of the dependencies of a verb, the system looks at this verb like a new one. In (1) the pattern corresponds to \"s'abattre\" and not \"abattre.\" If a preposition is the header of one of the dependencies, the module examines the syntactical analysis to find out whether it is followed by a noun phrase (+ SN] or an infinitive verb (+ SINF]. (3) displays the output of the pattern extractor for input in (1). (3) VCONJS | s'abattre: Prep + SN | sur | PREP Prep + SN | en | PREP"}, {"heading": "3.3 GEN (the SCF Builder)", "text": "The SCF generator extracts SCF candidates for each verb from the output of the Pattern Extractor and calculates the number of corpus occurrences for each SCF and verb combination. The syntactic components used to build the SCF are the following: 1. SN for nominal sentences; 2. SINF for infinitive sentences; 3. SP [prep + SN] for prepositive sentences, followed by an un-word. Prep is the header preposition; 4. SP [prep + SINF] for prepositive sentences, followed by an aninfinitive verb. Prep is the header preposition; 5. SA for adjective sentences; 6. COMPL for subordinate sentences. If a verb has no dependency, its SCF is considered INTRANS. (4) The output of the SCF generator for (1). (4) S'ABATTRE + s abattre; Sur + SP [SP]."}, {"heading": "3.4 CON and EVAL (SCF Filter)", "text": "Each step of the process is fully automated, so the output of the SCF generator is noisy due to tagging, parsing, or other processing errors. It is also noisy due to the difficulty of distinguishing between argument and addition, the latter being difficult even for humans. Many criteria defined here are not usable in our case because they are based either on lexical information that the parser cannot use (since the task is to obtain this information) or on semantic information that even the best parsers cannot reliably learn yet. The approach here is based on the assumption that true arguments tend to occur more frequently in argument positions than additives, thus correcting many frequent SCFs in the system output. Then, the strategy is to filter low frequency entries from the SCF generator output, using the maximum probability estimates [24]. This simple method involves calculating the relative frequency of each empemptive SCF comparison with a specific (irical) and (irical) one."}, {"heading": "4 Some Limitations of this Approach", "text": "Since the approach is based on automatic tools (especially parsers) that are far from perfect, the resources obtained always contain errors and must be validated manually. In addition, the system must be given sufficient examples to derive relevant information. Therefore, there is usually a lack of information for many products with low productivity (the famous \"austerity problem\"). More fundamentally, some constructions are automatically difficult to capture and characterize. On the one hand, idioms are not recognized as such by most collection systems. On the other hand, some additions often appear with certain verbs (e.g. some verbs such as dormir - sleep - often with location complements. The system then assumes that these are arguments, whereas linguistic theory would undoubtedly say that they are additions. Finally, surface cubes are sometimes insufficient to detect ambiguous constructions (see.... manger cafune caf\u00e9ce a la vanille vs... manilla-un-glace-ice system, to eat in the open air)."}, {"heading": "5 A Solution: Provide an Explicit Modeling of the Set of Constraints (CON)", "text": "We have shown in the previous section that some of the errors generated are due to an excessive simplification of the original model, so it is necessary to consider other parameters in order to achieve better results."}, {"heading": "5.1 Refining CON", "text": "The problems we have reported in the previous section do not mean that automatic methods are flawed, but they have a number of disadvantages that should be addressed; the acquisition process, based on an analysis of the parallelism of the verb with its immediate additions (along with filtering techniques), makes the approach highly functional; it is a good approach to the problem. However, this model often takes into account external limitations; analyzing the parallel occurrences of the verb with its addition is useful, but not sufficient to fully grasp the problem; the fact that some phraselike additions (with a specific head nomenclature) are often associated with a particular verb is useful most of the time, especially to identify idioms [25], collisions [26] and slight verb constructions [27]; on the other hand, the fact that a certain prepositional phrase appears with a large number of verbs indicating that the addition introduces an argument rather than an addendum."}, {"heading": "5.2 Modifying EVAL", "text": "All constraints can be evaluated separately to obtain an ideal evaluation of the parameter for each of them. There are two ways to do this: i) by automatically deriving the different weights from a set of annotated data, or ii) by estimating the results of different manually defined weights. We are currently using this last method because annotating the data is very expensive, but the first approach would certainly lead to more accurate results, and the weight and order of the different constraints must then be studied. A linear model can provide a first approximation, but there are certainly better ways to integrate the various constraints. Some studies provide some clues, but they must be evaluated appropriately in order to be integrated into this framework [5]."}, {"heading": "5.3 Manual Validation", "text": "Finally, the approach requires manual validation. Instead of leaving aside the validation process for further examination by a linguist, we propose to integrate it into the acquisition process itself. Taking into account the number of examples and the complexity of sentences used for training, it is possible to link trust values with the different constructions of a particular verb: the linguist is then able to quickly focus on the most problematic cases. It is also possible to propose provisional constructions to the linguist if there are not enough occurrences available for training. In the end, if there are too few examples available, the linguist can provide the machine with relevant information. However, with a well thought-out and dynamic validation process, it is possible to obtain precise and comprehensive encyclopaedias, taking only a small fraction of the time to develop a lexicon manually from scratch."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a new approach to the automatic acquisition of lexical knowledge from corpora by applying the theory of optimality. Using this model, it is possible to represent a large part of language activity through constraints. We have shown that the individual evaluation of each constraint delivers very accurate and precise results. Implementation of this model is currently being carried out for Japanese [28]. The model offers a better integration of linguistic contracts within the automatic processing system. Initial results have been competitive with other approaches while providing a more precise linguistic description."}], "references": [{"title": "Syntactic Gradience: The Nature of Grammatical Indeterminacy", "author": ["B. Aarts"], "venue": "Oxford University Press, Oxford", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimality Theory", "author": ["R. Kager"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Doing Optimality Theory", "author": ["J. McCarthy"], "venue": "Blackwell, Oxford", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimality Theory: Constraint Interaction in Generative Grammar", "author": ["A. Prince", "P. Smolensky"], "venue": "Blackwell, Oxford", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A Quantification Model of Grammaticality", "author": ["P. Blache", "J.P. Prost"], "venue": "Constraints Solving and Language Processing", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "The Minimalist Program", "author": ["N. Chomsky"], "venue": "The MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Can subcategorisation probabilities help a statistical parser", "author": ["G.M. John Carroll", "T. Briscoe"], "venue": "Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora, Montreal (Canada)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Lexicalization in crosslinguistic probabilistic parsing: The case of French", "author": ["A. Arun", "F. Keller"], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), Ann Arbor, Michigan, Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information", "author": ["S. Schulte im Walde", "C. Brew"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Using Predicate-Argument Structures for Information Extraction", "author": ["M. Surdeanu", "S.M. Harabagiu", "J. Williams", "P. Aarseth"], "venue": "Proceedings of the Association of Computational Linguistics (ACL).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Investigating the Cross-linguistic Potential of VerbNet-style Classification", "author": ["L. Sun", "A. Korhonen", "T. Poibeau", "C. Messiant"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. COLING \u201910, Stroudsburg, PA, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "M\u00e9thodes en syntaxe", "author": ["M. Gross"], "venue": "Hermann, Paris", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1975}, {"title": "The Lefff 2 Syntactic Lexicon for French: Architecture, Acquisition, Use", "author": ["B. Sagot", "L. Cl\u00e9ment", "E. de La Clergerie", "P. Boullier"], "venue": "Language Resource and Evaluation Conference (LREC), Genoa", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax", "author": ["M.R. Brent"], "venue": "Computational Linguistics 19", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Automatic Acquisition of a Large Subcategorization Dictionary from Corpora", "author": ["C.D. Manning"], "venue": "Proceedings of the Meeting of the Association for Computational Linguistics.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Automatic Extraction of Subcategorization from Corpora", "author": ["T. Briscoe", "J. Carroll"], "venue": "Proceedings of the 5th ACL Conference on Applied Natural Language Processing, Washington, DC.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A Large Subcategorization Lexicon for Natural Language Processing Applications", "author": ["A. Korhonen", "Y. Krymolowski", "T. Briscoe"], "venue": "Proceedings of the 5th international conference on Language Resources and Evaluation, Genova, Italy", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "A Subcategorisation Lexicon for German Verbs induced from a Lexicalised PCFG", "author": ["S. Schulte im Walde"], "venue": "Proceedings of the 3rd Conference on Language Resources and Evaluation. Volume IV., Las Palmas de Gran Canaria, Spain", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic syntax", "author": ["C.D. Manning"], "venue": "In Press, M., ed.: Probabilistic Linguistics, R. Bod, J. Hay, S. Jannedy", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "ASSCI : A Subcategorization Frames Acquisition System For French", "author": ["C. Messiant"], "venue": "Proceedings of the Association for Computational Linguistics (ACL) Student Research Workshop, Colombus, Ohio, Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "LexSchem: A Large Subcategorization Lexicon for French Verbs", "author": ["C. Messiant", "A. Korhonen", "T. Poibeau"], "venue": "Proceedings of the Language Resource and Evaluation Conference, Maroc", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival Subcategorization Frames from Corpora", "author": ["J. Preiss", "T. Briscoe", "A. Korhonen"], "venue": "Proceedings of the Meeting of the Association for Computational Linguistics, Prague", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Syntex, analyseur syntaxique de corpus", "author": ["D. Bourigault", "M.P. Jacques", "C. Fabre", "C. Fr\u00e9rot", "S. Ozdowska"], "venue": "Actes des 12\u00e8mes journ\u00e9es sur le Traitement Automatique des Langues Naturelles, Dourdan", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Statistical Filtering and Subcategorization Frame Acquisition", "author": ["A. Korhonen", "G. Gorrell", "D. McCarthy"], "venue": "Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Exploiter des corpus annots syntaxiquement pour observer le continuum entre arguments et circonstants", "author": ["C. Fabre", "D. Bourigault"], "venue": "Journal of French Language Studies 18(1)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Descriptive Linguistics and the Study of English", "author": ["J.R. Firth"], "venue": "Selected Papers of John R. Firth.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1968}, {"title": "The Light Verb Jungle", "author": ["M. Butt"], "venue": "Harvard Working Papers in Linguistics 9", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Representing the Continuum between Arguments and Adjuncts within Predicate-Frames", "author": ["P. Marchal", "T. Poibeau", "Y. Lepage"], "venue": "NINJAL International Symposium on \u201cValency Classes and Alternations in Japanese\u201d, Tokyo", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "However, surprisingly, in the acquisition community, relatively few investigations have been done on the structure of the linguistic constraints themselves, beyond the engineering point of view (but note that this work has been extensively done for parsing, see [1]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 1, "context": "OT is based on a number of assumptions which are absolutely relevant for the lexical acquisition context [2,3,4]:", "startOffset": 105, "endOffset": 112}, {"referenceID": 2, "context": "OT is based on a number of assumptions which are absolutely relevant for the lexical acquisition context [2,3,4]:", "startOffset": 105, "endOffset": 112}, {"referenceID": 3, "context": "OT is based on a number of assumptions which are absolutely relevant for the lexical acquisition context [2,3,4]:", "startOffset": 105, "endOffset": 112}, {"referenceID": 4, "context": "However, despite these observations, OT has been mainly applied to phonology, more rarely to morphology or syntax [5,1].", "startOffset": 114, "endOffset": 119}, {"referenceID": 0, "context": "However, despite these observations, OT has been mainly applied to phonology, more rarely to morphology or syntax [5,1].", "startOffset": 114, "endOffset": 119}, {"referenceID": 5, "context": "OT has been mainly applied to syntax in the framework of the Principles and Parameters (P&P) theory developed by Chomsky [6] as part of his Minimalist Program.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "For example, they can be used to enhance tasks such as parsing [7,8] and semantic classification [9] as well as applications such as information extraction [10] and machine translation.", "startOffset": 63, "endOffset": 68}, {"referenceID": 7, "context": "For example, they can be used to enhance tasks such as parsing [7,8] and semantic classification [9] as well as applications such as information extraction [10] and machine translation.", "startOffset": 63, "endOffset": 68}, {"referenceID": 8, "context": "For example, they can be used to enhance tasks such as parsing [7,8] and semantic classification [9] as well as applications such as information extraction [10] and machine translation.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "For example, they can be used to enhance tasks such as parsing [7,8] and semantic classification [9] as well as applications such as information extraction [10] and machine translation.", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "They also make it possible to infer large multilingual semantic classifications [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "For French these include the large French dictionary \u201cLe Lexique Grammaire\u201d [12] and the more recent Lefff [13] and Dicovalence (http://bach.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "For French these include the large French dictionary \u201cLe Lexique Grammaire\u201d [12] and the more recent Lefff [13] and Dicovalence (http://bach.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Some work has been conducted on automatic sub-categorization acquisition, mostly on English [14,15,16,17] but also on other languages, from which German is just one example [18].", "startOffset": 92, "endOffset": 105}, {"referenceID": 14, "context": "Some work has been conducted on automatic sub-categorization acquisition, mostly on English [14,15,16,17] but also on other languages, from which German is just one example [18].", "startOffset": 92, "endOffset": 105}, {"referenceID": 15, "context": "Some work has been conducted on automatic sub-categorization acquisition, mostly on English [14,15,16,17] but also on other languages, from which German is just one example [18].", "startOffset": 92, "endOffset": 105}, {"referenceID": 16, "context": "Some work has been conducted on automatic sub-categorization acquisition, mostly on English [14,15,16,17] but also on other languages, from which German is just one example [18].", "startOffset": 92, "endOffset": 105}, {"referenceID": 17, "context": "Some work has been conducted on automatic sub-categorization acquisition, mostly on English [14,15,16,17] but also on other languages, from which German is just one example [18].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "As outlined by Manning [19] \u201crather than maintaining a categorical argument / adjunct distinction and having to make in/out decisions about such cases, we might instead try to represent SCF information as a probability distribution over argument frames, with different verbal dependents expected to occur with a verb with a certain probability\u201d.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "This system called ASSCI is capable of acquiring large scale lexicons from un-annotated corpora [20,21].", "startOffset": 96, "endOffset": 103}, {"referenceID": 20, "context": "This system called ASSCI is capable of acquiring large scale lexicons from un-annotated corpora [20,21].", "startOffset": 96, "endOffset": 103}, {"referenceID": 15, "context": "This system is close to other systems developed for example for English [16,22] in that it extracts SCFs from data parsed using a shallow dependency parser [23] and is capable of identifying a large number of SCFs.", "startOffset": 72, "endOffset": 79}, {"referenceID": 21, "context": "This system is close to other systems developed for example for English [16,22] in that it extracts SCFs from data parsed using a shallow dependency parser [23] and is capable of identifying a large number of SCFs.", "startOffset": 72, "endOffset": 79}, {"referenceID": 22, "context": "This system is close to other systems developed for example for English [16,22] in that it extracts SCFs from data parsed using a shallow dependency parser [23] and is capable of identifying a large number of SCFs.", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "For a more detailed description of ASSCI, see [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "The system first tags and lemmatizes corpus data using TreeTagger and then parses it thanks to Syntex [23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "This is done using the maximum likelihood estimates [24].", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "The fact that some phrasal complements (with a specific head noun) frequently co-occur with a given verb is most of the time useful, especially to identify idioms [25], colligations [26] and light verb constructions [27].", "startOffset": 163, "endOffset": 167}, {"referenceID": 25, "context": "The fact that some phrasal complements (with a specific head noun) frequently co-occur with a given verb is most of the time useful, especially to identify idioms [25], colligations [26] and light verb constructions [27].", "startOffset": 182, "endOffset": 186}, {"referenceID": 26, "context": "The fact that some phrasal complements (with a specific head noun) frequently co-occur with a given verb is most of the time useful, especially to identify idioms [25], colligations [26] and light verb constructions [27].", "startOffset": 216, "endOffset": 220}, {"referenceID": 4, "context": "Some studies provide some cues but they need to be proper evaluated in order to be integrated in this framework [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 27, "context": "An implementation of this model is currently being done for Japanese [28].", "startOffset": 69, "endOffset": 73}], "year": 2014, "abstractText": "This paper re-investigates a lexical acquisition system initially developed for French. We show that, interestingly, the architecture of the system reproduces and implements the main components of Optimality Theory. However, we formulate the hypothesis that some of its limitations are mainly due to a poor representation of the constraints used. Finally, we show how a better representation of the constraints used would yield better results.", "creator": "LaTeX with hyperref package"}}}