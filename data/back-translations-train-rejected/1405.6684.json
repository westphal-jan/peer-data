{"id": "1405.6684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Visualizing Random Forest with Self-Organising Map", "abstract": "Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.", "histories": [["v1", "Mon, 26 May 2014 19:00:15 GMT  (386kb,D)", "http://arxiv.org/abs/1405.6684v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["piotr p{\\l}o\\'nski", "krzysztof zaremba"], "accepted": false, "id": "1405.6684"}, "pdf": {"name": "1405.6684.pdf", "metadata": {"source": "CRF", "title": "Visualizing Random Forest with Self-Organising Map", "authors": ["Piotr P lo\u0144ski", "Krzysztof Zaremba"], "emails": ["pplonski@ire.pw.edu.pl", "zaremba@ire.pw.edu.pl"], "sections": [{"heading": null, "text": "Keywords: Random Forest, Self-Organizing Maps, Visualization, Classification, Proximity Matrix"}, {"heading": "1 Introduction", "text": "The human readability of the model is an important factor for a good data mining algorithm. Among various data mining methods very popular are decision trees [20], [3]. Although they have an easy to interpret model, a single tree does not always achieve the highest accuracy. To overcome this problem, various ensemble methods have been proposed, among them the popular method Random Forest (RF), which was proposed by Leo Breiman [2]. The RF builds a series of trees using dredging and random subspace methods. The final output is a mode of reactions from all individual trees. The RF can be used for classification and regression tasks. Despite the high accuracy of the RF, the human readability of the model is lost. There are some methods to search within the RF black box, such as: Investigation of variable meaning [4], parallel co-ordinate plots by variable. [2] or visualization of the RF Proximity 07 / 10p. The final release is at http: / proximity 07 / 10p."}, {"heading": "2 Methods", "text": "Let us call the dataset D = {(xi, ci)}, where xi is an attribute vector, x-RM, M an attribute vector length and ci a discrete class number of the i-th sample, i = [1, 2,..., N] and c = [1, 2,..., C]."}, {"heading": "2.1 Self Organising Maps", "text": "In this paper, we used the SOM as a two-dimensional grid of neurons. Each neuron is represented by a weighting vector Wpq, where (p, q) indices of the neuron are in the grid. It is important to note that the weighting vector of the neuron has the same length as the attribute vector of the sample in the dataset. The weights of the neuron correspond directly to the attributes in the dataset. In the training phase, we look for a neuron for each sample that comes closest to the i-th sample. In the original SOM algorithm, the distance with the square Euclidean distance is calculated by the following equation: Disttrain (Di, Wpq) \u2212 p-class T (xi \u2212 Wpq) T (1). The neuron (p, q) with the smallest distance to the i-th sample is called the distance so-called the Best Matching Unit (BMU), and we note its indicators as (the weights) that is found when the BMU is updated."}, {"heading": "2.2 Random Forest", "text": "In the RF algorithm, a set of individual trees is built. The process of constructing a tree can be described in the following steps: 1. Draw a bootstrap dataset D \u2032 by selecting n times with substitutes from all N training samples. 2. Determine a decision at the node with only m attributes where m is smaller than M. The division is based on a maximum gain of information. 3. Move the data through the node in relation to the decision from step 2. 4. Repeat steps 2, 3 until an entire tree has grown. At the end of the tree construction, each leaf is assigned the class label, based on a class of samples in it. In the test phase, a new sample is pressed through all trees. A class name will be remembered from each tree based on the class of the leaf attained. The final answer is the mode of voices from all trees. The proximity matrix Prox, with size NxN, can be easily obtained by placing all the samples close to the tree (if all the samples are equal in the vicinity of the i and the two trees in the vicinity of the samples)."}, {"heading": "2.3 Self Organising Maps learned with Random Forest (RF-SOM)", "text": "In the proposed procedure, we assume that the RF is already learned. Learning the network in an epoch can be summarized in the following steps: 1. Build a dataset H as a union of all network weights W and attribute vector xj of the j-th sample, H = W-xj. The matrix W size is LxM, where the L is a total number of neurons in the network. In this matrix, each row contains weights from a neuron, the mapping of neurons 2D grid to the matrix W is assumed. 2. For the set H calculation of the dissimilarity DisH using the RF size is the DisH size (L + 1) x (L + 1) x. 3. Finding the smallest distance to the neurons in the dissimilarity matrix W is assumed."}, {"heading": "3 Results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4 Conclusions", "text": "The proposed method of visualization provides a better understanding of the relationship between the data in the RF structure than the MDS. Unlike the MDS, the RF-SOM provides a mapping of the data on the 2D neuron grid. In the case of incoming samples, there is no need to recalculate the entire RF proximity matrix as with the MDS method. Furthermore, the proposed method exhibits a lower memory complexity than the MDS, which is not applicable to large data sets. Furthermore, the experimental results show that the RF-SOM has learned with RF dissimilarity better or with the same accuracy as the SOM learned with Euclidean removal. As shown in [16], the RF dissimilarity has attractive properties: It can handle mixed variable types well, is invariant to monotonous results of the SOM obtained with the help of transformation variable and other RF shaping possibilities."}, {"heading": "5 Acknowledgements", "text": "PP was supported by the European Union within the framework of the European Social Fund through the Programme for Development of the Warsaw University of Technology."}], "references": [{"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "University of California, Irvine, School of Information and Computer Sciences", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Multi-Test Decision Trees for Gene Expression Data Analysis, Lecture", "author": ["M. Czajkowski", "M. Grze\u015b", "M. Kretowski"], "venue": "Notes in Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Random Forest-Based Manifold Learning for Classification of Imaging Data in Dementia", "author": ["K.R. Gray", "P. Aljabar", "R.A. Heckemann", "A. Hammers", "D. Rueckert"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Classification and Regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Fuzzy Supervised Self-Organizing Map for Semisupervised Vector Quantization", "author": ["M. K\u00e4stner", "T. Villmann"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Self-Organizing Map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE, vol.78, pp 14641480", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning Associations by Self-Organization: The LASSO model, Neurocomputing, vol.6, pp", "author": ["S. Midenet", "A. Grumbach"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Randomized Clustering Forests for Image Classification", "author": ["F. Moosmann", "E. Nowak", "F. Jurie"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Employing Self-Organizing Map for Fraud Detection", "author": ["D. Olszewski", "J. Kacprzyk", "S. Zadrozny"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "An Experimental Study on Asymmetric Self-Organizing Map", "author": ["D. Olszewski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Fuzzy Clustering Neural Network for Classification of ECG Beats", "author": ["S. Osowski", "T.H. Linh"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Self-Organising Maps for Classification with MetropolisHastings Algorithm for Supervision", "author": ["P. P lo\u0144ski", "K. Zaremba"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Improving Performance of Self-Organising Maps with Distance Metric Learning Method", "author": ["P. P lo\u0144ski", "K. Zaremba"], "venue": "Lecture Notes in Computer Science, vol.7267, pp 169-177", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Learning with Random Forest Predictors", "author": ["T. Shi", "S. Horvath"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma", "author": ["T. Shi", "D. Seligson", "A.S. Belldegrun", "A. Palotie", "S. Horvath"], "venue": "Modern Pathology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Self Organizing Maps for Visualization of Categories", "author": ["J. Szyma\u0144ski", "W. Duch"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Self\u2013Organizing Map Representation for Clustering Wikipedia Search Results", "author": ["J. Szyma\u0144ski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Among various data mining methods very popular are decision trees [20], [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Among them, the popular is Random Forest (RF) proposed by Leo Breiman [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "There exist some methods to look inside RF black-box, like: examining variable importance [4], parallel cooridinate plots by variable [2] or visualizing the RF proximity distance 0 The final publication is available at http://link.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Zaremba matrix with Multidimensional Scaling [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "Herein, we propose a novel method for visualizing the RF proximity matrix based on Self-Organising Maps (SOM) [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 12, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "It was shown that using more sophisticated distance metric than Euclidean can improve the accuracy of the SOM [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "[16] presented method for building clusters from the RF learned with unlabeled data and successfully used it for tumor detection [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] presented method for building clusters from the RF learned with unlabeled data and successfully used it for tumor detection [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "[10] used the RF for efficient segmentation of images, where leaves were assigned to distinct image regions rather than to specific class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] used the RF proximity matrix and the MDS for classification of medical images of different types of dementia.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "There were used data sets: \u2019Glass\u2019, \u2019Wine\u2019, \u2019Iris\u2019, \u2019Sonar\u2019, \u2019Ionosphere\u2019, \u2019Pima\u2019 from the \u2019UCI Machine Learning Repository\u2019 [1].", "startOffset": 125, "endOffset": 128}], "year": 2014, "abstractText": "Abstract. Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.", "creator": "TeX"}}}