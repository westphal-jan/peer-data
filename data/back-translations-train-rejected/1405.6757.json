{"id": "1405.6757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces", "abstract": "In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified \"safety\" guarantees, and remains in a stable region of the parameter space (iii) how to design \"off-policy\" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators.", "histories": [["v1", "Mon, 26 May 2014 23:11:40 GMT  (3211kb,D)", "http://arxiv.org/abs/1405.6757v1", "121 pages"]], "COMMENTS": "121 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sridhar mahadevan", "bo liu", "philip thomas", "will dabney", "steve giguere", "nicholas jacek", "ian gemp", "ji liu"], "accepted": false, "id": "1405.6757"}, "pdf": {"name": "1405.6757.pdf", "metadata": {"source": "CRF", "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces", "authors": ["Sridhar Mahadevan", "Bo Liu", "Philip Thomas", "Will Dabney", "Steve Giguere", "Ji Liu"], "emails": ["mahadeva@cs.umass.edu,", "boliu@cs.umass.edu,", "imgemp@cs.umass.edu,", "PThomasCS@gmail.com", "sgiguere9@gmail.com", "amarack@gmail.com", "jliu@cs.rochester.edu"], "sections": [{"heading": null, "text": "Reinforcement learning is a simple, and yet, comprehensive theory of learning that simultaneously models the adaptive behavior of artificial agents = Andrew (such as robotic and autonomous software programs, as such to explain the emergent behavior of biological systems \u2032. It also leads to computer-based ideas that provide a powerful tool for solving problems involving sequential prediction and decision-making. Temporal difference learning is the most widely used method of solving amplification problems, with a rich history stretching back more than three decades. For these and many other reasons, this article is not currently reviewed for the journal Foundations and Trends in ML, but will be submitted for formal peer review sometime in the future once the draft reaches a stable \"equilibrium\" state.ar Xiv: 1oping a complete theory of reinforcement learning, one that is both rigorous and useful has been an ongoing research investigation for several decades."}, {"heading": "1 Introduction 1", "text": "1.1 Elements of the Overall Framework 2 1.2 Illustrating the Solution 8 1.3 Safe Reinforcement Learning 9 1.4 Real Stochastic Gradient Reinforcement Learning 11 1.5 Thrifty Reinforcement Learning Using Mirror Descent 12 1.6 Summary 13"}, {"heading": "2 Background 15", "text": "2.1 Learn amplification 15 2.2 Stochastic composite optimization 18 2.3 Subdifferential and monotonous operators 25 2.4 First-order convex concave saddle-point algorithms 26 2.5 Abstraction by proximal operators 27 2.6 Decomposition by operator splitting 28 2.7 Natural gradient methods 31 2.8 Summary 32iii Contents"}, {"heading": "3 Sparse Temporal Difference Learning in Primal", "text": "Doublerooms 333.1 Problem Formulation 34 3.2 Mirror Descent RL 35 3.3 Convergence Analysis 39 3.4 Experimental Results: Discrete MDPs 41 3.5 Experimental Results: Continuous MDPs 44 3.6 Comparison of Link Functions 46 3.7 Summary 47"}, {"heading": "4 Regularized Off-Policy Temporal Difference", "text": "Learning 484.1 Introduction 49 4.2 Problem formulation 50 4.3 Algorithm design 54 4.4 Theoretical analysis 57 4.5 Empirical results 57 4.6 Summary 61"}, {"heading": "5 Safe Reinforcement Learning using Projected", "text": "Natural Actor Critic 625.1 Introduction 62 5.2 Related Work 64 5.3 Equivalence of Natural Gradient Descent and MirrorDescent 655.4 Projected Natural Gradients 66 5.5 Compatibility of Projection 67 5.6 Natural Actor-Critic Algorithms 69 5.7 Projected Natural Actor-Critics 70 5.8 Case Study: Functional Electrical Stimulation 72 5.9 Case Study: uBot Balancing 73 5.10 Summary 756 True Stochastic Gradient Temporal DifferenceContents iiiLearning Algorithms 776.1 Introduction 78 6.2 Background 79 6.3 Problem Formulation 80 6.4 Algorithm Design 82 6.5 Accelerated Gradient Temporal Difference LearningAlgorithms 846.6 Theoretical Analysis 84 6.7 Experimental Study 87 6.8 Summary 88"}, {"heading": "7 Variational Inequalities: The Emerging Frontier of", "text": "Machine Learning 907.1 Variational Inequalities 91 7.2 Variational Inequalities Algorithms 97"}, {"heading": "8 Appendix: Technical Proofs 102", "text": "8.1 Convergence Analysis of the Saddle Point TemporalDifference Learning 1028.2 Convergence Analysis of the True Gradient TemporalDifference Learning 1031 Introduction In this chapter, we present the elements of our novel framework for attachment learning [1], based on learning temporal differences not in the original space, but in a dual space defined by a so-called mirror map. We show how this technical device holds the fundamental key to solving a whole range of unresolved problems in attachment learning, from the development of stable and reliable non-political algorithms to safety guarantees and finally to their scalability in high dimensions. This new vision of attachment learning, which we have developed in recent years, leads to mathematically rigorous solutions to long-standing important issues in the field that remain unresolved for almost three decades."}, {"heading": "1.1 Elements of the Overall Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1.1 Primal Dual Mirror Maps", "text": "In this section, we offer a concise explanation of the overall frame, leaving many technical details to future chapters. Central to the proposed frame is the idea of mirror maps that make it possible to perform temporal learning updates not only in the usual primordial space, but also in a dual space. More specifically, given gradient updates in primordial space D are a mirror map if they are strongly convex, differentiable, and the gradient of the primordial value has the range Rn (i.e., it takes all possible vector values). Instead of performing gradient updates in primordial space, we perform gradient updates in dual space, which correspond as follows: Given gradient updates in the dual space (y) x x = gradient value (x) \u2212 mmatic differences in the cult value area is a practicable parameter. To return to the primordial space, we use the gradient updates in the space we use."}, {"heading": "1.1.2 Mirror Descent, Extragradient, and Mirror Prox Methods", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "1.1.3 Proximal Operators", "text": "The proximal figure associated with a convex function is defined as: proxh (x) = argminu (x), the indicator function for a convex set C, then proxIC (x) = x (x), the projector to a set C (x), the projector to a set C (x), then proxh (x) = x, the identity function. For economical representations, the case is when h (w) = is considered to be a case in which h (the L1 standard of w) is considered to be particularly important. In this case: proxh (w) i = wi \u2212 \u03bb, if wi > procedure 0, if | wi | \u2264 wi + \u03bb, otherwiseAn interesting observation follows from the finding that the projected subgradient method can be used in an equivalent manner."}, {"heading": "1.1.4 Operator Splitting Strategies", "text": "In our framework, a key finding used to derive a true stochastic gradient method for amplification learning is based on the powerful concept of operator splitting [17, 18]. Figure 1.4 illustrates this concept for the convex feasibility problem, where we must obtain a collection of convex sets and find a point at their intersection, which originally motivated the development of Bregman divergences [4]. The convex feasibility problem in Figure 1.4 is an example of many real problems, such as 3D voxel reconstruction in brain imaging [15], a high-dimensional problem for which the descent of mirrors was developed. To find an element in the common intersection of two groups A and B in Figure 1.4, a standard method known as alternating projections works as follows. Given a starting point x0, the first step projects it onto one of the two convex groups, we give the operators a point A and a point X0."}, {"heading": "1.2 Illustrating the Solution", "text": "Now that we have described the general elements of our framework, we give some selected examples of the tangible solutions that arise to the problem of developing safe, reliable and stable reinforcement learning algorithms. We choose three cases: how to develop a \"safe\" reinforcement1.3 Safe reinforcement learning method; how to develop a \"real\" stochastic reinforcement learning method; and finally, how to develop a \"robust\" reinforcement learning method that does not fit your training experience."}, {"heading": "1.3 Safe Reinforcement Learning", "text": "Figure 1.5 shows a complex humanoid robot with a high degree of freedom. Teaching robots complex skills is a difficult problem, especially since learning amplification can not only take a long time, but can also cause these robots to operate in hazardous areas of parameter space. Our proposed framework solves this problem by establishing a political gradient method that exploits this equivalence to provide a safe method of training complex robots through amplification learning between the mirror descent and the known but not yet related class of algorithms, the so-called natural gradient [22]. We explain the importance of the result below, which combines mirror descent and natural gradient methods later in this essay, when describing a novel class of methods called projected Natural Actor Critics (PNAC).10 Introduction Theorem 1.3.1. Updating the natural gradient descent in step with metric tensor, corresponds to Gensk (xk = xk = 1 \u2212 k1) (G + kk = xk \u2212 1)."}, {"heading": "1.4 True Stochastic Gradient Reinforcement Learning", "text": "Although many more complex methods have not really been developed in the last three decades, such as the \"least-squares based temporal difference\" approaches, including LSTD [23], LSPE [24] and LSPI [25], the development of the gradient TD (GTD) family of methods may be more graceful than other methods. A crucial step in the development of our framework has been the development of a novel saddle point frame for sparsely regulated GTD methods. However, there are several unresolved questions regarding the current off-policy TD algorithms. (1) The first is the convergence rate of these algorithms. Although these algorithms are motivated by the gradient of an objective function, they are related to the current off-policy TD algorithms. (1) The first is the convergence rate of these algorithms."}, {"heading": "1.5 Sparse Reinforcement Learning using Mirror Descent", "text": "How can we design reinforcement learning algorithms that are robust to overmatched? In this paper, we examine a new framework for (politically convergent) TD learning algorithms based on mirror descent and related algorithms. Mirror descent can be considered an advanced gradient method that is particularly suitable for minimizing convex functions in high-dimensional spaces. In contrast to traditional methods for learning temporal difference, learning temporal differences in mirror descent results in updates of weights in both dual space and primordial space, which are linked to each other by a Legendre transformation. Mirror descent can cause 1.6. Summary 13 Considered as an approximate algorithm where the distance-generating function used is a Bregman divergence. We will introduce a new class of time difference methods based on proximal gradient (TD) that are based on different Bregman divergences that are significantly more variable than the coefficient-generating examples of D."}, {"heading": "1.6 Summary", "text": "The fundamentally new idea underlying the 14 Introduction approach is the systematic use of mirror maps to perform updates of temporal differences, not in the original primordial space, but in a dual space. This technical tool, as we will show in the following chapters, offers a number of significant advantages. By carefully selecting the mirror map, we can generalize popular methods such as natural, gradient-based actor-critic methods and provide security guarantees. We can design more robust learning methods for temporal differences that are less prone to overtake an agent's experience. Finally, we can use proximal mappings to design a rich variety of truly stochastic gradient methods. These advantages, when combined, provide a compelling case for the fundamental veracity of our approach. However, much remains to be done to validate the proposed framework in a broader way to complex real applications as well as to advance a deeper theoretical analysis of our proposed approach."}, {"heading": "2.1 Reinforcement Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 MDP", "text": "The learning environment for decision-making is generally modelled by the known Markov decision-making process [29] M = (S, A, P, R, \u03b3), which is derived from a Markov chain. Definition 2.1.1. (Markov chain): A Markov decision-making process is a stochastic process defined as M = (S, P). At any time, the actor is in a state of st \u2212 1 at a time step t \u2212 1 at a time state st at a time st.A Markov decision-making process (MDPs) consists of a set of states S, a set of (possibly state-dependent) actions A < a dynamic system consisting of a state st \u2212 1 at a time st.A Markov decision-making process (MDPs) consists of a set of states S, a set of (possibly state-dependent) actions A < a dynamic model consisting of a state st \u2212 1 at a time. A Markov decision-making process (MDPs) consists of a set of states A < a dynamic model consisting of a state-dependent decision-making system (DPs)."}, {"heading": "2.1.2 Basics of Reinforcement Learning", "text": "A Definition 2.1.3. (Politics): A deterministic stationary policy \u03c0: S \u2192 A assigns an action to each state of the Markov decision-making process. A stochastic policy \u03c0: S \u00b7 A \u2192 [0, 1]. Value functions are used to compare and evaluate the performance of politics. Definition 2.1.4. (Value function): A value function w.r.t assigns a policy \u03c0: S \u2192 R assigns to each state the expected sum of discounted rewards. V \u00b2 R assigns to each state the expected sum of discounted rewards. V \u00b2 is a fixed point of the Bellman equation V \u00b2 (st) = E [r (st) = E [r (st).The goal of reinforcement learning is to find a (near optimal) policy that maximizes the value function. V \u00b2 is a fixed point of the Bellman equation V \u00b2 s equation."}, {"heading": "2.1.3 Value Function Approximation", "text": "The most popular and widely used RL method is the method of time difference (TD) learning [30]. TD learning is a stochastic approach to solving the equation (2,1,2). The value of the state action Q * (s, a) represents a convenient reformulation of the value function, defined as the long-term value of performing a first and then optimal operation according to V *: Q * (s, a) = E (rt + 1 + \u03b3a \u00b2 Q * (st + 1, a) | st = s, where rt + 1 is the actual reward obtained at the next step, and st + 1 is the state resulting from performing an action in a state. The (optimal) effect value formulation is convenient because it is approximately achieved by a time difference (TD) learning technique called Q-Learning [31]. The simplest TD method, called TD (0), estimates the value sequence associated with fixed politics."}, {"heading": "2.2 Stochastic Composite Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Stochastic Composite Optimization Formulation", "text": "Stochastic optimization examines the use of first-order gradient methods to solve convex optimization problems. (Definition 2.2.1. (Lipschitz continuous gradient): The gradient of a closed convex function f (x) is L-Lipschitz continuous if (x) is a convex function. (Definition 2.2.2. (Strong convexity): A convex function is \u00b5 -strongly convex if (x) \u2212 f (y). (Lipschitz continuous gradient). (Definition 2.2.2. (Strong convexity): A convex function is \u00b5 -strongly convex if (x). (Lipschitz-related function)."}, {"heading": "2.2.2 Proximal Gradient Method and Mirror Descent", "text": "Before we proceed to the introduction of the mirror reflection, let us first introduce some definitions and notations.20 BackgroundDefinition = q = q = q. (Distance-generating function) [35]: A distance-generating function is defined as a continuously differentiable, strongly convex function. (Bregman divergence) [35]: In contrast to the Legendre transformation, which is defined as a constant divergence function, the Bregman divergence function caused by the divergence divergence divergence is defined as: Daco x (x, y) = x-divergence method caused by the divergence method, defined as: Daco (x) = x-divergence method."}, {"heading": "2.2.3 Dual Averaging", "text": "It is a variant of the dual averaging (DA) with \"simple\" regularizations, such as the l1 regularization.The DA method is strongly linked to the cutting-plane methods.The DA method formulates a polyhedrical subordinate model of objective function, in which each gradient from past iterations contributes a supporting hyperplane. The DA method approaches this subordinate model with an approximate (possibly non-supporting) subordinate hyperplane averaging all past gradients. [40] We explain the RDA from the proximal gradient perspective, which is often expensive to calculate.The DA method approaches a safe (possibly non-supporting) subordinate hyperplane averaging all past gradients. [40] We explain the RDA from the proximal gradient perspective."}, {"heading": "2.2.4 Extragradient", "text": "The extragradient gradient method was first proposed by Korpelevich [8] as a loosening of the usual gradient slope in order to solve the problems of variable inequality (VI).The extragradient method can only be used to solve the VI problems if some strict limitations such as strong monotonicity of the user or compactness of the practicable theory.The extragradient methods were proposed in order to loosen the aforementioned strict limitations.The essence of the extragradient methods is that instead of moving along the steepest gradient descend direction, two steps, i.e. an extrapolation step and a gradient descend.In the extrapolation step, 24 steps are taken along the steepest gradient descend towards the starting point."}, {"heading": "2.2.5 Accelerated Gradient", "text": "The AC method consists of three main steps: an interpolation step, a proximal gradient step, and a weighted mean step. During each iteration, yt = \u03b1txt \u2212 1 + (1 \u2212 \u03b1t) zt \u2212 1 xt = arg minx {< x, f (yt) > + h (x) + 1\u03b2t D\u0443 (x, xt \u2212 1)} zt = \u03b1txt + (1 \u2212 \u03b1t) zt \u2212 1Remarkably, the stabilizer starts in the proximal gradient step xt \u2212 1 and goes along the gradient downward direction of \u0432f (yt), which is quite similar to the extraministerial gradient method. The essence of Nesterov's accelerated gradient method is the careful selection of the proximal gradient \u2212 1 and the selection of two interference step optimization steps, with Nesterov's method for the convection effect 25,1 and the subsequent method for the convection."}, {"heading": "2.3 Subdifferentials and Monotone Operators", "text": "We present the important concept of a subdifference.Definition 2.1. The subdifferential of a convex function f is defined as the set value mapping \u2202 f (x) = {v-Rn: f (z) \u2265 f (x) + vT (z-x), \u0432-z-dom (f) A simple example of a subdifferential is the normal cone, which is the subdifferential of the indicator function IK of a convex set K (defined as 0 within the set and + \u221e outside). Formally, the normal cone NK (x) at the vector x \u043c of a convex set K is defined as NK (x-Rn) = {y-Rn | yT (x-x-x) \u2264 K. Each vector v-red f (x) is defined as a subgradient f at x.An important property of closed correct convex functions is that their differences produce a monetary relationship."}, {"heading": "2.4 Convex-concave Saddle-Point First Order Algorithms", "text": "A central recent contribution to our work is a convex-concave saddle point formula for mounting processes. A convex-concave saddle point problem is formulated as follows: Let x-X, y-Y, where X, Y are both non-empty closed convex sets, and f (x): X-R is a convex function. If there is a function that f (x) can be represented as f (x): = supy-Y sets (x, y), then the pair (x, Y) is called the saddle point representation of f. The optimization problem of minimizing f over X is converted into an equivalent convex-Xsupy-Y saddle point representation (x, y) is called the saddle point representation of f-Y."}, {"heading": "2.5 Abstraction through Proximal Operators", "text": "A general method for solving the monotonous inclusion problem, the proximal dot algorithm [50], uses the following identities: 0 \u0445 \u2202 f (x) \u2194 0 \u0445 \u03b1 \u0445 f (x) \u2194 x (I + \u03b1 \u2202 (x)) \u2194 x = (I + \u03b1 \u0445 f) \u2212 1 (x) Here \u03b1 > 0 is any real number. The proximal dot algorithm is based on the so-called resolution of a relationship, which is defined as follows: Definition 2.4. The resolution of a relationship F is specified as relationship RF = (I + \u03bbF) \u2212 1, where \u03bb > 0.In the case where the relationship R = proximal f of a convex function f is, the resolution of a relationship F can be specified as ratio RF = (I + \u03bbF) \u2212 1, in which the proximal-proximal projection projection function x is x."}, {"heading": "2.6 Decomposition through Operator Splitting", "text": "Operator splitting [17, 18] is a generic approach to break down complex optimizations and variable inequality problems into simpler ones that involve calculating the resolutions of individual relationships rather than sums or other compositions of relationships. For example, given a monotonous inclusion problem of the form: 0-A (x) + B (x) for two relationships A and B, how can we find the solution x * without calculating the solvent (I + \u03bb (A + B) -1 specified in Figure 2.1, which may be complicated but only calculates the resolutions of A and B individually? There are several classes of operator splitting systems. We will focus primarily on the Douglas-Rachford algorithm [18] specified in Figure 2.1 because it leads to a widely used optimization method called Alternating Direction Method Multipliers (ADMM)."}, {"heading": "2.6.1 Forward Backwards Splitting", "text": "In this section, we give a brief overview of proximal splitting algorithms [28], which are widely used in machine learning, signal processing, and stochastic optimization and provide a general framework for large-scale optimization. Proximal mapping, which is associated with a convex function h, is defined as: proxh (x) = arg minu (h) + 1 2 (u) 2 (u) 2 (u) 2) Operator splitting is commonly used to reduce the computational complexity of many optimization problems, leading to algorithms such as sequential non-iterative approach (SNIA), strand splitting, and sequential iterative approach (SIA). Proximal splitting is a technique that combines proximal operators and operator splitting, and addresses problems where proximal operator splitting is difficult."}, {"heading": "2.6.2 Nonlinear Primal Problem Formulation", "text": "In this paper, we will examine a scenario of proximal splitting that differs from the formulation of the problem in section (2.6.1), namely the non-linear primary formmin function, K being a linear operator, which is the induced standard | | K | |. In the following, we will refer to F (K) as F-semicontinuous (l.s.c) nonlinear convex function. The proximal operator of this problem is \u03b8t + 1 = arg min.p = arg min.p = 1 2\u03b1t | | | 22} = prox\u03b1t (F-K + h) 2.7. Natural gradient methods 31.eIn many cases, although prox\u03b1tF and prox\u03b1tK are easy to calculate, prox\u03b1tF-proproprostitute this problem."}, {"heading": "2.6.3 Primal-Dual Splitting", "text": "The corresponding primordial formula [57, 28, 58] of section (2,6,2) is the legendary transformation of the convex nonlinear function F (\u00b7), which is defined as F * (y) = supx x x * (y) + h (\u03b8), where F * (\u00b7) is the legendary transformation of the convex, non-linear function F (\u00b7), which is defined as F * (y) = supx x x x * X (< x > \u2212 F (x). Proximal splitting update per iteration is asyt + 1 = arg min y * Y < \u2212 Kt (\u03b8t), y > + F \u0445 (y) + 12\u03b1t | y \u2212 yt | yt \u00b2 \u00b2 \u00b2 \u00b2, on the other hand, arg min \u2022 X < Kt (xi), yt > + h (IS) + 12\u043b\u0435\u0441\u0442t (KIS), so we have the general update rule + 1 asyt = arg."}, {"heading": "2.7 Natural Gradient Methods", "text": "Consider the problem of minimizing a differentiable function f: Rn \u2192 R. The standard approach to the downward slope is to first select an x0-32 background Rn, calculate the direction of the steepest descent, \u2212 f (x0), and then shift a certain amount in this direction (scaled by a step size parameter, \u03b10); this process is then repeated indefinitely: xk + 1 = xk \u2212 \u03b1k-f (xk), where {\u03b1k} is a step size plan and k-downward direction (1,...). The downward slope of the gradient has been criticized for its low asymptotic convergence rate. However, natural gradients are a quasi-Newton approach to improve the convergence rate of the downward slope. When calculating the direction of the steepest descent, the downward slope assumes that the vector xk resides in Euclidean area."}, {"heading": "2.8 Summary", "text": "The following chapters provide further elaboration of this material as required, and the overall goal of our work is to bring the learning of amplification into the main structure of modern stochastic optimization theory. As we show in the following chapters, achieving this goal allows us access to many advanced algorithms and analytical tools. It is worth noting that we hardly use the classical stochastic approximation theory, which is traditionally used to analyze amplification methods (as in books such as [32]). Classical stochastic approximation theory provides only asymptotic convergence limits, but we are interested in obtaining narrower sample complexity limits that provide stochastic optimization."}, {"heading": "3.1 Problem Formulation", "text": "The formulation of the problem in this chapter is based on the lasso-TD problem, which is defined as follows: \"The problem is fixed.\" (Lasso-TD = fixed problem) used in LARS-TD and LCP-TD. (L1-regulated projection) We first define the l1 regulated projection and then give the definition of lasso-TD objective functionality. (Definition 3.1.1) [60] (l1-regulated projection is the l1 regulated projection, defined as: [60]: \"L1y\" is a non-explicit mapping method, which is a non-expansive mapping method, which is a non-explicit mapping method, which is a non-expansive mapping method, which is a non-expansive mapping method, which is non-expansive mapping method, which is non-explicit mapping method, which is non-explicit mapping, which is non-explicit mapping in, 2, which is explicit mapping."}, {"heading": "3.2 Mirror Descent RL", "text": "Algorithm 1 describes the proposed mirror descent method.2 Unlike regular TDs, the weights are updated based on the TD error in the2. All the described algorithms extend to the case of the action value in which \u03c6 (s) is replaced by \u03c6 (s, a).36 Sparse Temporal Difference Learning in Primal Dual SpacesAlgorithm 5 Adaptive Mirror Descent TD (\u03bb) Let \u03c0 be a fixed procedure for an MDP M and s0 be the initial state. Let it be some fixed or automatically generated basics.1: Repeat 2: Do the procedure \u03c0 (st) and observe the next state st + 1 and reward rt rt. 3: Update the permission paths et + perc."}, {"heading": "3.2.1 Choice of Bregman Divergence", "text": "We will now discuss various options for the distance-generating function in algorithm 1. Although in the simplest case we assume that the gradient lengths of p and p correspond to the regular TD (\u03bb), since the gradient lengths of p and p correspond to the identity function. A much more interesting choice of the gradient lengths of p and p is that the mirror descend TD (\u03bb) corresponds to the regular TD (\u03bb), and its conjugated legendary transformations 3.2. Mirror descend RL 372 (w) = 12, w and p correspond to the identity function."}, {"heading": "3.2.2 Sparse Learning with Mirror Descent TD", "text": "Algorithm 2 describes a modification to obtain sparse value functions leading to a sparsely mirrored TD (\u03bb) algorithm. The main difference is that the double weights \u03b8 are truncated according to Equation 1.1.3 to satisfy the l1 penalty on the weights. In this case, \u03b2 is a sparsity parameter. An analogous approach was proposed in [37] for l1 punished classification and regression."}, {"heading": "3.2.3 Composite Mirror Descent TD", "text": "Another possible mirror descent algorithm uses as distance generation function a Mahalanobis distance, which is derived from the partial gradients generated during the actual experiments. We base our derivation on the composite mirror descent approach to classification and regression proposed in [67]. The composite mirror descent solves the following partial gradients in Primal Dual Difference Learning in Primal Dual SpacesAlgorithm 6 Sparse Mirror Descent TD (\u03bb) 1: Repeat 2: Do action (st) and observe the next state + 1 and reward rt. 3: Update the permission strace et (st)."}, {"heading": "3.3 Convergence Analysis", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.4 Experimental Results: Discrete MDPs", "text": "Figure 3.2 shows that Mirror Descendant TD converges faster with much smaller Bellman errors than LARS-TD [68] on a discrete \"Tworoom\" MDP [69]. The base matrix \u03a6 was automatically generated as 50 proto-value functions by diagonalizing the Laplacian diagram of discrete state-space connectivity [69]. The figure also shows that the algorithm 2 (sparsely mirrored Mirror Descendant TD) scales more gracefully than LARS-TD. Note: LARS-TD is unstable for \u03b3 = 0.9. It is worth noting that the compilation costs of LARS-TD O (sparsely mirrored Mirror Descendant TD), while for algorithm 2 O (Nd) is the number of samples, d is the number of base functions, and m is the number of interference functions."}, {"heading": "3.5 Experimental Results: Continuous MDPs", "text": "Figure 3.6 compares the performance of Q-learning vs. mirror-descent Q-learning for the mountain car task, which leads more quickly to a better solution with much less variance. Figure 3.7 shows that the mirror-descent Q-learning with learned diffusion wave bases converges quickly on the 4-dimensional Acrobot task. Finally, we tested the mirror-descent approach on a morecomplex 8 dimensional continuous MDP. The 3-link inverted pendulum [71] is a highly nonlinear time system that is flat in 1000 steps. We tested the mirror-descent approach on a morecomplex 8 dimensional continuous MDP. The 3-link inverted pendulum [71] is a highly non-linear time-variable under-actuated system, 3.5. Experimental results: Continuous MDPs 45which is a standard benchmark bed in the control community."}, {"heading": "3.6 Comparison of Link Functions", "text": "The two most commonly used linkage functions in the mirror descent are the linkage function [6] and the relative entropy function for the exposed gradient (EG) [64]. Both linkage functions offer a multiplicative update rule compared to regular additive gradient methods. The differences between these two are discussed here. First, the loss function for EG is relative entropy, while that of the p-normative linkage function is the square l2-norm function. Second, and more importantly, EG does not produce sparse solutions, since it must keep the weights away from zero, otherwise its potential (relative entropy) on rebound will be unlimited. Another advantage of p-normative linkage functions over EG is that the p-norm linkage function provides a flexible interpolation between additive and multiplicative gradient updates, which are considered more efficient if the linkages are shown to be coherent."}, {"heading": "3.7 Summary", "text": "We have proposed a new framework for reinforcement learning by mirroring the convex optimizations. Spiegel Descent Q-Learning has the following advantage over regular Q-Learning: faster convergence rate and lower variance due to larger increments with theoretical convergence guarantees [72]. Compared to existing sparse reinforcement learning algorithms such as LARS-TD, Algorithm 2 has lower sample complexity and lower computational costs, advantages resulting from the primary mirror descent framework combined with a proximal imaging [37]. There are many promising future research topics along this line. We are currently investigating a mirrored Fast-Descent RL method that is both convergent off-policy and faster than fast gradient TD methods such as GTD and TDC [2]."}, {"heading": "4.1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Off-Policy Reinforcement Learning", "text": "As emphasized in [75], target policy is often a deterministic policy that approaches the optimal policy, and behavioral policy is often stochastical, examining all possible actions in each state as part of determining the optimal policy. Learning target policy from the examples generated by behavioral policy allows for a greater variety of exploration strategies. It also allows learning from training data generated by independent controllers, including manual human control, and from previously collected data. Another reason for the interest in non-policy learning is that it allows learning about multiple target strategies (e.g. optimal strategies for multiple sub-targets) from a single exploration strategy generated by a single behavioral policy, which has triggered an interesting area of research called \"parallel reinforcement learning.\" In addition, non-political methods are of broader application as they are able to learn while executing an exploration strategy, from a single behavioral policy that is generated by a consistent behavioral policy, whereby the DC numbers are proposed."}, {"heading": "4.1.2 Convex-concave Saddle-point First-order Algorithms", "text": "The key novel in this chapter is a convex-concave saddle point formula for regularized off-policy TD learning. A convex-concave saddle point problem is formulated as follows: Let x-X, y-Y, where X, Y are both non-empty delimited, closed convex sentences, and f (x): X-R is a convex function. If there is a function that f (x) can be represented as f (x): = supy, Y (x, y), then the pair (x, Y) is called the saddle-point representation of f. The optimization problem of minimizing f over X is converted into an equivalent 50 regularized off-policy Temporal Difference Learning Convex Saddle Point Formation, y-Y variation (x, y) of f-Y."}, {"heading": "4.2 Problem Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Objective Function Formulation", "text": "Now let's examine the concept of MSPBE. MSPBE is defined as MSPBE = Projected = Projected = Projected = Projected as MSPBE = Projected as Projected (Projected) = Projected (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected) (Projected (Projected) (Projected) (Projected) (Projected) (Projected) (Projected (Projected) (Projected) (Projected) (Projected) (Projected) (Projected (Projected) (Projected) (Projected) (Projected) (Projected (Projected) (Projected) (Projected = (Projected) (Projected) (Projected = (Projected) (Projected) (Projected = (Projected) (Projected) (Projected (Projected = (Projected) (Projected) (Projected = (Projected) (Projected) (Projected = (Projected) (Projected = (Projected) (Projected) (Projected = (Projected = (Projected) (Projected) (Projected (Projected = (Projected =) (Projected) (Projected) (Projected = (Projected) (Projected = (Projected) (Projected) (Projected = (Projected) (Projected = (Projected) (Projected) (Projected = (Projected) (Projected = (Projected) (Projected (Projected"}, {"heading": "4.2.2 Squared Loss Formulation", "text": "It is also worth mentioning that there is another formulation of the loss function than Equation (4,2,1) with the following convex-concave formulation as in [77, 47], min x1 2 x-1 x-2 x-2 x-1 = max x-2 x-1 x-1 (bT y-2 yT y) = min x-max x-u-2 x-1, y (xTu + yT (Ax \u2212 b) \u2212 2 yT y) Here we give the detailed deduction of the formulation into Equation (4,1). Firstly, the standard LASSO problem formulation cannot be formulated as a convex representation by reformulating the standard LASSO problem formulation in such a way that the A matrix in the linear equation representation of GTD2 is symmetrical but not PSD, so that it cannot be formulated as a convex problem formulation."}, {"heading": "4.3 Algorithm Design", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 RO-TD Algorithm Design", "text": "In this section, the problem of (4.2.1) as a convex-concave saddle point problem is formulated = = 12 points, and the RO-TD algorithm is proposed. Similar to (4.1.2), the regularized loss function can be solved by an iteration procedure as follows: xt = [wt].xt + 1 = 2 yT (Ax \u2212 b) + h (x) Similar to (2.4), the equation (4.3.1) can be solved as follows: xt = [wt; 2].xt + 2 = xt \u2212 xt yt, yt + 1 = 2 (Atxt \u2212).bt xt \u2212 bt xt + 1 = proxanth (xt + 12), yt + 1 = (yt + 1) 3 Let w = \u2212 y, then we will have the same formulation as in COLT2012.2."}, {"heading": "4.3.2 RO-GQ(\u03bb) Design", "text": "GQ (\u03bb) [79] is a generalisation of the TDC algorithm with claim tracking and non-political learning of temporally abstract predictions, in which the gradient changes from Equation (4,2,1) to claim tracking + 1 = claim tracking. The central element is to extend the MSPBE function to include claim tracking - claim tracking - claim tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - application tracking - claim tracking - claim tracking - claim tracking - claim tracking - claim tracking - claim tracking - application tracking - application tracking - application tracking."}, {"heading": "4.4 Theoretical Analysis", "text": "The theoretical analysis of the RO-TD algorithm can be found in the appendix."}, {"heading": "4.5 Empirical Results", "text": "We will now demonstrate the effectiveness of the RO-TD algorithm against other algorithms in a number of benchmark domains. LARS-TD [62], a popular economical second-order gain learning algorithm, is used as the base algorithm for feature selection and TDC as a non-political convergent RL base algorithm."}, {"heading": "4.5.1 MSPBE Minimization and Off-Policy Convergence", "text": "This experiment aims to show the minimization of the MSPBE and the off-policy convergence of the RO-TD algorithm. Therefore, the star MDP is unsuitable for LSTD-based algorithms, including LARS-TD, since TR = 0 is always valid. The random walk problem is a standard Markov chain with 5 states and two absorbing states at two ends. Three sets of different bases \u03a658 Regularized Off-Policy Temporal Difference Learning4.5. Empirical results 59 are used in [26], which are tabular features, inverted features and dependent features. An identical experiment set to [26] is used for these two areas. The regularization term h (x) is set to 0 to provide a fair comparison with TD and TDC. Provert features or dependent features."}, {"heading": "4.5.2 Feature Selection", "text": "To make a fair comparison, we use the same basic function setting as in [62], where two-dimensional rasters of 2, 4, 8, 16, and 32 RBFs are used, giving a total of 1365 basic functions; for LARS-TD, 500 samples are used; for RO-TD and TDC, 3000 samples are used, running 15 episodes with 200 steps for each episode, with increments ranging from t = 0.001, and between 1 and 0.2. We use the result of LARS-TD and l2 LSTD reported in [62]; as the result in Table 4.1 shows, RO-TD is able to perform the function selection successfully, whereas TDC and TD fail, so that the results of LARS-TD and LSTD are not comparable."}, {"heading": "4.5.3 High-dimensional Under-actuated Systems", "text": "The tripartite inverted pendulum [71] is a highly nonlinear underactivated system with 8-dimensional state space and discrete action space. The state space consists of the angles and angular velocities of each arm as well as the position and speed of the car. However, the discrete action space is {0, 5Newton, \u2212 5Newton}. The goal is to learn a policy that can balance the arms for Nx steps within a minimum number of learning phases. The maximum number of episodes allowed is 300. The pendulum initiates from zero equilibrium state and the first action is randomly selected to push the pendulum away from the initial state. We test the performance of RO-GQ (\u03bb), GQ (\u03bb) and LARS-TD. Two experiments are performed with Nx = 10, 000 and 100, 000 S, whereby the first action is randomly chosen to push the pendulum away from the initial state."}, {"heading": "4.6 Summary", "text": "In this chapter, we present a novel unified framework for the development of regulated, convergent RL algorithms that combines a convex concave saddle point problem for RL with stochastic firstorder methods.5 Detailed experimental analysis shows that the proposed RO-TD algorithm is both convergent and robust against loud traits. 5 Safe Reinforcement Learning using Projected Natural Actor CriticNatural Actor critics form a popular class of political search algorithms to find locally optimal strategies for Markov decision-making processes. In this paper, we address a disadvantage of natural actor-critics that limits their applicability in the real world - their lack of safety guarantees. We present a principle-driven algorithm for implementing natural gradient-critic algorithms over a limited range 1. In the context of amplification learning, this allows natural actor-critic algorithms to remain within a safe region of a known political spatial region."}, {"heading": "5.1 Introduction", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5.2 Related Work", "text": "Bendrahim and Franklin [88] demonstrated how a walking bipedal robot can switch to a stabilizing controller when the robot leaves a stable area of state space. Similar state-avoiding approaches to security have been suggested by several others [89, 90, 91]. However, these approaches do not take into account situations where actions over an unavoidable area of state space itself are dangerous. Kuindersma et al. [92] developed a method for conducting risk-sensitive policy searches that models the variance of objective function for each policy and allows maturity adjustments of risk sensitivity, but their approach does not guarantee that an uncertain region of state space or political space is avoided. 5.3. Equivalence of natural gradient and mirror descent 65Bhatnagar et al. [93] presented projected natural actor-critic algorithms for average reward adjustment. As suggested in our projected natural projection algorithms, the actuator-critic algorithms."}, {"heading": "5.3 Equivalence of Natural Gradient Descent and Mirror Descent", "text": "We begin by showing an important relationship between natural gradient methods and mirror descent. Theorem 5,3.1. Updating the natural gradient descent at step k with metric tensor Gk, G (xk): xk + 1 = xk \u2212 \u03b1kG \u2212 1k \u00b2 f (xk), (5,1) corresponds to updating the mirror descent at step k, first noting that the function on the right is strictly concave, the x it maximizes is its critical point."}, {"heading": "5.4 Projected Natural Gradients", "text": "If x is limited to a certain set, X-k is expanded in mirror parentage by the indicator function IX (where IX (x) = 0 if x-X, and otherwise. \u2212 k Therefore, all references to this extended version.For this proximal function, the subdifference is x-x (x) = (1 / 2) xxx- \u2212 Gkx (x) \u2212 X-X (x), where all references to this extended version.For this proximal function, the subdifference is x (x) x-x (x) = Gk (x) + N-X) (x), where N-X), where N-X (x), where N-X (x), where N-X (x), IX-X (x) and, in the mean, Gk and N-X, are relationships."}, {"heading": "5.5 Compatibility of Projection", "text": "The default projected subgradient method (PSG) follows the negative gradient (as opposed to the negative natural gradient) and projects back to X with the Euclidean standard. (PNG) The standard projected subgradient method (PNG) follows the negative gradient (as opposed to the negative natural gradient method) and projects back to X (PNG).Any such x method is a fixed point. (PNG) The results of this x method are a fixed point. (PNG) This means that a small step in the negative direction of a subdifferential from f at x is projected back to x level. (Our choice of projection, GkX, results in PNG, results in PNG with the same fixed points (see Lemma 5.5.1). This means that if the algorithm is taken at x level and a small step down the natural gradient from f at x level. (GkX) Our choice of projections, Lemma, PNG level, have the same results (see results in PNG 5.5.1)."}, {"heading": "5.6 Natural Actor-Critic Algorithms", "text": "An MDP is a tuple M = (S, A, P, R, d0, Q), where S is a set of states, A is a set of measures, P (s, s, a) indicates the probability density of the system that occurs in the state s when trading in the state s, R (s, a) is the expected reward, r when trading in the state s, d0 is the original distribution of the state, and Q [0, 1) is a reward parameter. A parameterized policy is a conditional probability density function - (a, s) is the probability density of acting a in the state s in the face of a vector of political parameters, and Q is a reward parameter. 70 Safe Reinforcement Learning using Projected Natural Actor CriticLet J (T) = E [E [E, xi] is the optimal reward parameter of policy."}, {"heading": "5.7 Projected Natural Actor-Critics", "text": "When we are given a closed convex gradient, we must use an unbiased algorithm, since the predicted policy parameters (e.g. the stable region of gains for a PID controller) are not fixed (e.g. the stable region of gains for a PID controller), we can ensure that the policy parameters remain within what was described in the previous section. The natural actor-critic algorithms described in the previous section offer no such guarantee. However, their policy parameters require updated equations, which are natural gradient-rise updates, which can be easily changed to the projected natural gradient-rise update in (5.5) by projecting the parameters back to the previous level by following on to the previous level of \"G\" (\u03b8k + 1 = \"G (successk + \u03b1kwk).5.7 The projected natural actor-critiques 71Many of the existing political gradient algorithms, including NAC-Sargent-103, Sargent-NAC and Sargent-LD-103."}, {"heading": "5.8 Case Study: Functional Electrical Stimulation", "text": "In this case study, we looked for proportional-derivative (PD) gains to control a simulated human arm going through FES. We used the Dynamic Arm Simulator 1 (DAS1) [104], a detailed biomechanical simulation of a human arm undergoing functional electrical stimulation. In a previous study, a controller using DAS1 reacted well to an actual human subject going through FES, although it required some additional tuning measures to cope with biceps pasticity, suggesting that it is a relatively accurate model of an ideal arm. The DAS1 model presented in Figure 2a has a state that st = (\u03c62, zepte 2) has a goal of achieving biceps pasticity (85), which means that it is a reasonably accurate model of an ideal arm represented in Figure 2a."}, {"heading": "5.9 Case Study: uBot Balancing", "text": "In the previous case study, the optimal policy was outside the designated safe region of the political space (this is often the case when a single failure is so costly that adding a penalty to the reward function for a failure is impractical since a single failure is unacceptable).We present a second case study where the optimal policy lies within the designated safe region of the political space, but where an unrestricted search algorithm can enter the unsafe region during its search for the political space (the point at which large negative rewards return it to the safe region. uBot-5, shown in Figure 5.2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21]. During the experiments, he often uses his arms to interact with the world."}, {"heading": "5.10 Summary", "text": "We introduced a class of algorithms that we call projected natural actor critics. PNACs are the simple modification of existing natural actor criteria that involve a projection of recalculated political parameters. We argued that a projection in principle results in an unrestricted parentage level being achieved."}, {"heading": "6.1 Introduction", "text": "The first order temporal difference (TD) learning methods is a widely used class of techniques in reinforcement theory. Although the least square temporal difference approaches, such as LSTD [23], LSPE [24], and LSPI [25], are well equipped with moderate size problems, the first group of TD learning algorithms will scale more gracefully to high-dimensional problems. The first class of TD methods was known to converge only when samples are taken \"on-policy,\" which motivates the development of the TD (GTD) gradient family of methods [26]. A novel noble-point framework for sparsely regulated GTD methods was recently proposed [14]. However, there are several questions regarding current off-policy TD algorithms. (1) The first is the convergence rate of these algorithms. Although these algorithms are motivated by the gradients of an objective function such as SPEU and NBE, they are not true in objective terms."}, {"heading": "6.2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Markov Decision Process and Reinforcement Learning", "text": "In the linear value function approximation, it is assumed that a value function lies in the linear range of a basic function matrix \u03a6 of dimension, where d is the number of linear independent features. Consequently, V \u2248 V\u03b8 = \u03a6\u03b8. For the t-th sample, \u03c6t (the t-th line), \u03c6 \u2032 t (the t-th line) is the characteristic vectors corresponding to st, s \u2032 t, or the weight vector for the t-th sample in the first TD learning method, and \u03b4t = (rt + \u03b3\u03c6 \u2032 t) \u2212 \u03c6Tt is the time difference error. TD Learning uses the following update rule successt + 1 = \u0445t + \u0442t t\u0441t\u0441t\u0441t\u0441t\u0442t, where 80 True Stochastic Gradient Temporal Difference Learning Algorithm is the step-by-step correspondence. However, TD Learning is only the convergence in approach-related politics, although in many off-policy situations it is a GTD convergence."}, {"heading": "6.3 Problem Formulation", "text": "Biased sampling is a well-known problem in amplification theory. Biased sampling is caused by E [\u03c6 \"Tt\" t] or E [\u03c6 \"T], where\" t \"is the characteristic vector for the state s\" t in the sample (st, at, s \"t). Due to the stochastic nature of politics, there may be many s\" t \"that can be solved via stochastic gradient descent (SGD), as shown in [27] although many algorithms are motivated by well-defined convex objective functions, such as MSPBE and NEW objective functions that cannot be solved due to the biascific sampling problem."}, {"heading": "6.4 Algorithm Design", "text": "In the following, we will build on the operator splitting methods introduced in Section 2.6.3, which should be checked before reading the following section."}, {"heading": "6.4.1 NEU Objective Function", "text": "The primary dual formulation of the NEW defined in section (6.2.1) is as follows: Min. + X (12 NEW (\u03b8) + h (\u03b8)) = Min. + X max y (<; T) (R + g)."}, {"heading": "6.4.2 MSPBE Objective Function", "text": "Based on the definition of MSPBE in section (6.2.1), we can reformulate MSPBE where MSPBE is used as double model. (Thus, we can reformulate MSPBE as double model (MSPBE) = = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model = double model (MBE) = double model (MBE) = double model (MBE) = double model (MBE) = double model (MBE) = double model = (MBE) (MBE) = double model (MBE) (MBE) = double model (MBE) (MBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model (MSPBE = double model = double model = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE) = double model = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model (MSPBE) = double model = double model (MSPBE = double model (MSPBE"}, {"heading": "6.5 Accelerated Gradient Temporal Difference Learning Algorithms", "text": "In this section we will discuss the acceleration of GTD2 and TDC. Acceleration of GTD is not discussed for space reasons, which is similar to GTD2. A comprehensive overview of the convergence rate of different approaches to stochastic saddle point problems is given in [108]. In this section we will introduce accelerated algorithms based on the stochastic mirror prox (SMP) algorithm [47, 109]. Algorithm 11, referred to as GTD2-MP, is accelerated with extragradient GTD2. Algorithm 12, referred to as TDC-MP, is accelerated with extragradient TDC. Algorithm 10 Algorithm Template Get a fixed policy of an MDP M that has some solid foundations. 1: Repeat 2: Calculation-related delays, delays and TD errors: Delays: T = rt + extragradients."}, {"heading": "6.6 Theoretical Analysis", "text": "In this section we discuss the convergence rate and error limit of GTD, GTD2 and GTD2-MP."}, {"heading": "6.6.1 Convergence Rate", "text": "Proposition 1 The convergence rates of the GTD / GTD2 algorithms with the primary average rate are O (LF + LK + \u03c3 \u221a N), with the convergence rates of the GTD / GTD2 algorithms with the primary average rate in [75].6.6. Theoretical analysis 85 algorithm 11 GTD2-MP (1) wt + 1 \u00b2 2 = wt + \u03b2t (\u03b4t \u2212 \u03c6Tt wt) \u03c6t, \u03b8t + 12 = prox\u03b1th (\u03b8t + \u03b1t (\u03c6t \u2212 \u03b3t) (\u03c6Tt wt)))) (2) \u0441t + 12 = proximation-related (GTD2) algorithms with the primary average rate. The convergence rates of the GTD / GTD2 algorithms with the primary average rate are O (LF + \u03b2T).6.6 Theoretical analysis 85 algorithm 11 GTD2-MP (1) wt + Twt \u00b2 -STD (STDT)."}, {"heading": "6.6.2 Value Approximation Error Bound", "text": "Thesis 3: For GTD / GTD2 the prediction error is of | | V \u2212 VTB | | limited by | V \u2212 VTB | | \u221e \u2264 LTB \u00b7 O (LF \u0445 + LK + \u03c3 \u221a N); for GTD2-MP it86 is True Stochastic Gradient Temporal Difference Learning Algorithmsis limited by the learning algorithms of | V \u2212 VTB | \u221e \u2264 LTB \u03c6 1 \u2212 \u03b3 \u00b7 O (LF \u0445 + LKN + \u03c3 \u221a N), where LTB = maxs | | (\u03a6T) \u2212 1\u03c6 (s) | | 1. Proof: see Appendix."}, {"heading": "6.6.3 Related Work", "text": "To the best of our knowledge, the closest related work is the RO-TD algorithm, which first introduced the convex-concave saddle-point framework to regulate the TDC family of algorithms. However, the main difference is that RO-TD is motivated by the linear inverse problem formulation of the TDC algorithm and uses its dual standard representation as an objective function that does not explore the auxiliary variable. In contrast, by introducing the operator-splitting framework, we show that the GTD family of algorithms can be explained nicely as a \"true\" SGD approach, with the auxiliary variable wt \u2212 having a nice explanation. Another interesting question is whether ADMM is suitable for the operator-splitting algorithm here. Let's take, for example, the NEW family of algorithms (ADMM formulation is as follows: K (2001) = GS1, where we can derive other scenarios from K and K, and other similar ones)."}, {"heading": "6.7 Experimental Study", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.7.1 Off-Policy Convergence: Baird Example", "text": "The Baird example is a well-known example of where TD deviates and TDC converges. Steps are constants, with \u03b2t = \u00b5\u03b1t as shown in Figure 6.1. From Figure 6.1 we can see that GTD2-MP and TDC-MP have a significant advantage over GTD2 and TDC algorithms, significantly reducing both MSPBE and variance."}, {"heading": "6.7.2 Regularization Solution Path: Two-State Example", "text": "Now we look at the two-state MDP in [111]. The transition matrix and the reward vector are [0, 1; 0, 1] and R = [0, \u2212 1] T, \u03b3 = 0.9 and a one-feature base \u03a6 = [1, 2] T. The objective functions are referred to as l1-NEW and l1-MSPBE short. In Figure 6.2, both l1-NEW and l1-MSPBE show well-defined solution88 True Stochastic Gradient Temporal Difference Learning algorithm spaths w.r.t, whereas Lasso-TD may have several solutions if the P-matrix condition is not met [62].6.7.3 On-Policy Performance: 400-State Random Meas.In this experiment, we will compare the political GTD spaths w.r.t, whereas Lasso-TD may have several solutions if the P-matrix condition is not met."}, {"heading": "6.8 Summary", "text": "This chapter shows that the GTD / GTD2 algorithms are true stochastic gradients."}, {"heading": "7.1 Variational Inequalities", "text": "In this case, it is only a matter of time before that happens."}, {"heading": "7.1.1 Definition", "text": "The formal definition of one VI x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "7.1.2 Equilibrium Problems in Game Theory", "text": "The VI framework provides a mathematically elegant approach to balance problems in the game theory [119, 120]. A Nash game consists of m players, whereby player i chooses a strategy xi that belongs to a closed convex group Xi-Rn. After executing the joint action, each player is penalized (or rewarded) by the amount Fi (x1,.., xm), where Fi: Rni \u2192 R is a continuously differentiating feature. A set of strategies x-Rn = (x-x-x-x-m) is penalized by the amount Fi (x1,., xm), if no player can reduce the penalty incurred (or increase the reward incurred) by deviating unilaterally from the chosen strategy conve7.2. if each Fi problem conveys x-x-x x-x x-x x x-x x-x x-x x-x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "7.2 Algorithms for Variational Inequalities", "text": "In the following, we briefly describe two algorithms for solving varying inequalities: the projection method and the extragradient method. We conclude with a brief discussion of how these are related to amplification learning."}, {"heading": "7.2.1 Projection-Based Algorithms for VIs", "text": "The basic projection method (algorithm 1) for solving VIs is based on theorem 7.4, that is, the basic approach is based on theorem 7.4, that is, the basic approach is defined on the basis of theorem 7.4. < The basic approach is based on VIs to be researched. < The basic approach is based on VIs (F, K), and a symmetric positive definitive matrix D.1: sentence k = 0 and xk K. 2: sentence xk + 1: sentence xk + 1: sentence xk + 1: sentence xkHere is the projector based on the natural standard generated by D., where it is the basic approach. < x, Dx > It can be shown that the basic approach is based on V I (F, K) for the 98 variations.The basic approach is based on the VIs; the basic approach is based on VIs < the VIs."}, {"heading": "7.2.2 Variational Inequaities and Reinforcement Learning", "text": "In this case, it can be shown that Figure F is affine to the VI defined by the gain and represents a (linear) complementarity problem. In this case, a number of special properties can be used in designing a faster, more scalable class of algorithms. Recall from theory 7.4 that each VI (F, K) corresponds to the solution of a particular fixed point problem x (x) that led to the projection of algorithms (Algorithm 1). Generalization of this problem considers the solution to the fixed point of a projected equation x)."}, {"heading": "Acknowledgements", "text": "We would like to acknowledge the valuable feedback provided by past and present members of the Autonomous Learning Laboratory at the University of Massachusetts, Amherst. Main funding for this research was provided by the National Science Foundation under the NSF IIS-1216467 grant program. Previous work by the first author was funded by NSF IIS0534999 and IIS-0803288.8 grants."}, {"heading": "Appendix: Technical Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Convergence Analysis of Saddle Point Temporal Difference Learning", "text": "Proof of Proposition 1We provide a descriptive proof here. We first present the monotone operator corresponding to the two-dimensional saddle point problem, and then extend it to stochastic approximations with certain restrictive assumptions, and use the result in [47]. The monotone operator \u03a6 (x, y) with saddle point problem SadV al = infx, y) is a point-to-sentence operator \u03a6 (x, y) = [x, y) = {x, y). For the two-dimensional problem in Equation (4.1.2), the corresponding equation (x, y) is the subgradient of \u03c6 (x, \u00b7) over x and x, y) is the subgradient of the equation (\u00b7, y) over y. For the two-dimensional problem in Equation (4.1.2), the corresponding equation (x, y) is the equation in x."}, {"heading": "8.2 Convergence Analysis of True Gradient Temporal Difference Learning", "text": "Assumption 1 (MDP): The underlying Markov Reward Process (MRP) M = (S, P, R, \u03b3) is finite and mixed, with stationary distribution \u03c0. The training sequence (st, at, s) is an i.i.d sequence. Assumption 2 (basic function): The reversals E [\u03c6t\u03c6Tt] \u2212 1 and [\u03c6t (\u03c6t \u2212 \u03b3t \u2032 T) \u2212 1 exist. Hence, it is a complete column sequence. Assumption 2 (base function): The characteristics of the K \u2212 itz have uniformly limited second moments, and the assumption of the K < + procedure t, neither is given."}, {"heading": "8.2.1 Convergence Rate", "text": "Here we discuss the convergence rate of the proposed algorithms. First, let's discuss the nonlinear primary formmin definition-convergence convergence-convergence-convergence-convergence-convergence-convergence-convergence-convergence-convergence-convergence-convergence-definition-definition-convergence-definition-definition-definition-convergence-definition-definition-convergence-definition-definition-definition-convergence-definition-definition-definition-definition-definition-definition-definition-definition-convergence-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition-definition"}, {"heading": "8.2.2 Value Approximation Error Bound", "text": "A key question is how to determine the error limit of \"V \u2212 V \u2212 VTB\" after the \"V \u2212 VTB\" equation (V \u2212 VTB = VTB = VTB = VTB = VTB = VTB = VTB = VTB = VTB = VTB), and the second is that VTB \u2212 V \u2212 VTB = VTB = V \u2212 V (VTB), VTB = V = VTB (VTB = VTB), and the third is that VTB \u2212 V = VTB = VTB (VTB = VTB), we have TV + (VTB = VTB), VTB (VTB = VTB), VTB (VTB = VTB = VTB), VTB = VTB VTB = VTB (VTB = VTB), VTB = VTB (VTB = VTB), VTB = VTB (VTB = VTB), VTB = VTB (VTB), VTB = VTB = VTB), VTB = VTB (VTB)."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A.G. Barto"], "venue": "MIT Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Richard S. Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesvri", "Eric Wiewiora"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "author": ["L. Bregman"], "venue": "USSR Computational Mathematics and Mathematical Physics, 7:200\u2013217,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1967}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovksi", "D. Yudin"], "venue": "John Wiley Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters, Jan", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization, 15(1):229\u2013251,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Korpelevich. The extragradient method for finding saddle points and other problems", "author": ["M. G"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "On some nonlinear elliptic differential functional equations", "author": ["P. Hartman", "G. Stampacchia"], "venue": "Acta Mathematica, 115:271\u2013310,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1966}, {"title": "Traffic equilibria and variational inequalities", "author": ["S. Dafermos"], "venue": "Transportation Science, 14:42\u201354,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1980}, {"title": "Network Economics: A Variational Inequality Approach", "author": ["A. Nagurney"], "venue": "Kluwer Academic Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Finite-Dimensional Variational Inequalities and Complimentarity Problems", "author": ["F. Facchinei", "Pang J"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Regularized off-policy TDlearning", "author": ["B. Liu", "S. Mahadevan", "J. Liu"], "venue": "Advances in Neural Information Processing Systems 25, pages 845\u2013853,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The ordered subsets mirror descent optimization method with applications to tomography", "author": ["A. Ben-Tal", "T. Margalit", "A. Nemirovski"], "venue": "SIAM Journal of Optimization, Jan", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The robustness of the p-norm algorithms", "author": ["Claudio Gentile"], "venue": "ISSN 0885-6125. doi: 10. 1023/A:1026319107706. URL http://dl.acm.org/citation. cfm?id=948445.948447", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Proximal splitting methods in signal processing", "author": ["P. Combetes", "J.C. Pesquel"], "venue": "Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "On the numerical solution of heat conduction problems in two and three space variables", "author": ["J. Douglas", "H. Rachford"], "venue": "Transactions of the American Mathematical Society, 82:421\u2013439,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1956}, {"title": "Convergence Analysis of True Gradient Temporal Difference Learning 109 tributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein. Dis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Whole-Body Strategies for Mobility and Manipulation", "author": ["P. Deegan"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Dexterous mobility with the uBot-5 mobile manipulator", "author": ["S.R. Kuindersma", "E. Hannigan", "D. Ruiken", "R.A. Grupen"], "venue": "Proceedings of the 14th International Conference on Advanced Robotics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Why natural gradient", "author": ["S. Amari", "S. Douglas"], "venue": "In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S. Bradtke", "A. Barto"], "venue": "Machine Learning, 22:33\u201357,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Least-squares policy evaluation algorithms with linear function approximation", "author": ["A. Nedic", "D. Bertsekas"], "venue": "Discrete Event Systems Journal, 13,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Least-squares policy iteration", "author": ["M. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research, 4:1107\u20131149,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "International Conference on Machine Learning, pages 993\u20131000,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1): 1\u2013103,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Proximal splitting methods in signal processing", "author": ["P. L Combettes", "J. C Pesquet"], "venue": "Fixed-point algorithms for inverse problems in science and engineering, pages 185\u2013212.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "Wiley Interscience, New York, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning, 3:9\u201344,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1988}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "PhD thesis, King\u2019s  110 Appendix: Technical Proofs College, Cambridge, England,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1989}, {"title": "Neuro-Dynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific, Belmont, Massachusetts,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning representation and control in Markov Decision Processes: new frontiers", "author": ["S. Mahadevan"], "venue": "Foundations and Trends in Machine Learning, 1(4):403\u2013565,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "An optimal method for stochastic composite optimization", "author": ["G. Lan"], "venue": "Mathematical Programming, 133(1-2):365\u2013397,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters, 31:167\u2013175,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "The robustness of the p-norm algorithms", "author": ["C. Gentile"], "venue": "Machine Learning, 53(3):265\u2013299,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Stochastic methods for l1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research, pages 1865\u20131892, June", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "The Journal of Machine Learning Research, 11:2543\u20132596,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "COLT, pages 14\u201326,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "A voted regularized dual averaging method for large-scale discriminative training in natural language processing", "author": ["J. Gao", "T. Xu", "L. Xiao", "X. He"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization", "author": ["H.B. McMahan"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 525\u2013533,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "The Journal of Machine Learning Research, 10:777\u2013801,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic methods for l1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 929\u2013936,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Manifold identification in dual averaging 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 111 for regularized stochastic online learning", "author": ["S. Lee", "S.J. Wright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "On accelerated proximal gradient methods for convexconcave optimization", "author": ["P. Tseng"], "venue": "submitted to SIAM Journal on Optimization,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex Analysis", "author": ["R Rockafellar"], "venue": "Princeton University Press,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1970}, {"title": "Optimization for Machine Learning, chapter First-Order Methods for Nonsmooth Convex Large- Scale Optimization", "author": ["A. Juditsky", "A. Nemirovski"], "venue": "MIT Press,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-Euclidean restricted memory level method for large-scale convex optimization", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": "Mathematical Programming, 102(3):407\u2013456,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19:1574\u20131609,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R. Rockafellar"], "venue": "SIAM Journal of Optimization, 14(5):877\u2013898,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1976}, {"title": "Functions convexes duales et points proximaux dans un espace hilbertien", "author": ["J. Moreau"], "venue": "Reports of the Paris Academy of Sciences, Series A, 255:2897\u20132899,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1962}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R. T Rockafellar"], "venue": "SIAM Journal on Control and Optimization, 14(5): 877\u2013898,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1976}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in optimization, 1(3):123\u2013231,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient learning using forward-backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse Reinforcement Learning via Convex Optimization", "author": ["Z. Qin", "W. Li"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex analysis and mono-  112 Appendix: Technical Proofs tone operator theory in Hilbert spaces", "author": ["H. H Bauschke", "P. L Combettes"], "venue": "Springer,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "Journal of Mathematical Imaging and Vision, 40(1):120\u2013145,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2011}, {"title": "Natural gradient works efficiently in learning", "author": ["S. Amari"], "venue": "Neural Computation, 10:251\u2013276,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "Finite- Sample Analysis of Lasso-TD", "author": ["M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation policy iteration", "author": ["S. Mahadevan"], "venue": "Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 372\u201337. AUAI Press,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 521\u2013528,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation, 132,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning, pages 285\u2013318,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1988}, {"title": "Exponentiated gradient methods for reinforcement learning", "author": ["D. Precup", "R.S. Sutton"], "venue": "ICML, pages 272\u2013277,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1997}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J. Zico Kolter", "Andrew Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2009}, {"title": "Proto-Value Functions: A Laplacian framework for learning representation and control in Markov Decision Processes", "author": ["S. Mahadevan", "M. Maggioni"], "venue": "Journal of Machine Learning Research, 8: 2169\u20132231,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "Value function approximation in reinforcement learning using the fourier basis", "author": ["G. Konidaris", "S. Osentoski", "PS Thomas"], "venue": "Computer Science Department Faculty Publication Series, page 101,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning control by association and reinforcement", "author": ["J. Si", "Y. Wang"], "venue": "IEEE Transactions on Neural Networks, 12:264\u2013 276,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A Nemirovski", "A Juditsky", "G Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 14(4):1574\u20131609,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2009}, {"title": "On the worst-case analysis of temporal-difference learning algorithms", "author": ["Robert Schapire", "Manfred K. Warmuth"], "venue": "In Machine Learning,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1994}, {"title": "Stochastic Approximation: A Dynamical Systems Viewpoint", "author": ["V. Borkar"], "venue": "Cambridge University Press,", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear off-policy actorcritic", "author": ["T. Degris", "M. White", "R.S. Sutton"], "venue": "International Conference on Machine Learning,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "www.optimization-online.org,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2007}, {"title": "Subgradient methods for saddle-point problems", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "Journal of optimization theory and applications, 142 (1):205\u2013228,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2009}, {"title": "GQ (\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence, pages 91\u201396,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2010}, {"title": "Value function approximation in reinforcement learning using the fourier basis", "author": ["G. Konidaris", "S. Osentoski", "P.S. Thomas"], "venue": "Proceedings of the Twenty-Fifth Conference on Artificial In-  114 Appendix: Technical Proofs telligence,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2011}, {"title": "PID Controllers: Theory, Design, and Tuning", "author": ["K.J. \u00c5str\u00f6m", "T. H\u00e4gglund"], "venue": "ISA: The Instrumentation, Systems, and Automation Society,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast calculation of stabilizing PID controllers", "author": ["M.T. S\u00f6ylemez", "N. Munro", "H. Baki"], "venue": "Automatica, 39(1):121\u2013126,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2003}, {"title": "A real-time 3-D musculoskeletal model for dynamic simulation of arm movements", "author": ["E.K. Chadwick", "D. Blana", "A.J. van den Bogert", "R.F. Kirsch"], "venue": "In IEEE Transactions on Biomedical Engineering,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2009}, {"title": "A proportional derivative FES controller for planar arm movement", "author": ["K. Jagodnik", "A. van den Bogert"], "venue": "In 12th Annual Conference International FES Society,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2007}, {"title": "Application of the actor-critic architecture to functional electrical stimulation control of a human arm", "author": ["P.S. Thomas", "M.S. Branicky", "A.J. van den Bogert", "K.M. Jagodnik"], "venue": "In Proceedings of the Twenty-First Innovative Applications of Artificial Intelligence,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2009}, {"title": "Lyapunov design for safe reinforcement learning", "author": ["T.J. Perkins", "A.G. Barto"], "venue": "Journal of Machine Learning Research, 3: 803\u2013832,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2003}, {"title": "Biped dynamic walking using reinforcement learning", "author": ["H. Bendrahim", "J.A. Franklin"], "venue": "Robotics and Autonomous Systems, 22: 283\u2013302,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1997}, {"title": "Control of markov chains with safety bounds", "author": ["A. Arapostathis", "R. Kumar", "S.P. Hsu"], "venue": "IEEE Transactions on Automation Science and Engineering, volume 2, pages 333\u2013343, October", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2005}, {"title": "Control design for Markov chains under safety constraints: A convex approach", "author": ["E. Arvelo", "N.C. Martins"], "venue": "CoRR, abs/1209.2883,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2012}, {"title": "Risk-sensitive reinforcement learning applied to control under constraints", "author": ["P. Geibel", "F. Wysotzki"], "venue": "Journal of Artificial Intelligence Research 24, pages 81\u2013108,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2005}, {"title": "Variational bayesian 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 115 optimization for runtime risk-sensitive control", "author": ["S. Kuindersma", "R. Grupen", "A.G. Barto"], "venue": "In Robotics: Science and Systems VIII,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2012}, {"title": "Natural actor-critic algorithms", "author": ["S. Bhatnagar", "R.S. Sutton", "M. Ghavamzadeh", "M. Lee"], "venue": "Automatica, 45(11):2471\u20132482,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Technical Report UCB/EECS-2010-24, Electrical Engineering and Computer Sciences, University of California at Berkeley, March", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex Analysis", "author": ["R. Tyrell Rockafellar"], "venue": "Princeton University Press, Princeton, New Jersey,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1970}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer, second edition,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2006}, {"title": "A natural policy gradient", "author": ["S. Kakade"], "venue": "Advances in Neural Information Processing Systems, volume 14, pages 1531\u20131538,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2002}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Advances in Neural Information Processing Systems 12, pages 1057\u20131063,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2000}, {"title": "Utilizing the natural gradient in temporal difference reinforcement learning with eligibility traces", "author": ["T. Morimura", "E. Uchibe", "K. Doya"], "venue": "International Symposium on Information Geometry and its Application,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2005}, {"title": "Natural actor-critic", "author": ["J. Peters", "S. Schaal"], "venue": "Neurocomputing, 71:1180\u20131190,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2008}, {"title": "Motor primitive discovery", "author": ["P.S. Thomas", "A.G. Barto"], "venue": "Procedings of the IEEE Conference on Development and Learning and EPigenetic Robotics,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["T. Degris", "P.M. Pilarski", "R.S. Sutton"], "venue": "Proceedings of the 2012 American Control Conference,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2012}, {"title": "Bias in natural actor-critic algorithms", "author": ["P.S. Thomas"], "venue": "Technical Report UM-CS-2012-018, Department of Computer Science, University of Massachusetts at Amherst,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2012}, {"title": "Combined feedforward and feedback control of a redundant, nonlinear, dynamic musculoskeletal system", "author": ["D. Blana", "R.F. Kirsch", "E.K. Chadwick"], "venue": "Medical and Biological Engineering and 116 Appendix: Technical Proofs Computing, 47:533\u2013542,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2009}, {"title": "The Fixed Points of Off-Policy TD", "author": ["J. Zico Kolter"], "venue": "Advances in Neural Information Processing Systems 24, pages 2169\u20132177,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2011}, {"title": "A convergent o(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesvari", "H.R. Maei"], "venue": "Neural Information Processing Systems, pages 1609\u20131616,", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2008}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "International Conference on Machine Learning, pages 30\u201337,", "citeRegEx": "107", "shortCiteRegEx": null, "year": 1995}, {"title": "Optimal primal-dual methods for a class of saddle point problems", "author": ["Y. Chen", "G. Lan", "Y. Ouyang"], "venue": "arXiv preprint arXiv:1309.5548,", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2013}, {"title": "Solving variational inequalities with stochastic mirror-prox algorithm", "author": ["A. Juditsky", "A.S. Nemirovskii", "C. Tauvel"], "venue": "Arxiv preprint arXiv:0809.0815,", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2008}, {"title": "A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science", "author": ["E. Esser", "X. Zhang", "T. F Chan"], "venue": "SIAM Journal on Imaging Sciences, 3(4): 1015\u20131046,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "A Dantzig Selector Approach to Temporal Difference Learning", "author": ["M. Geist", "B. Scherrer", "A. Lazaric", "M. Ghavamzadeh"], "venue": "International Conference on Machine Learning,", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2012}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research, 15:809\u2013883,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2014}, {"title": "LSTD with Random Projections", "author": ["M. Ghavamzadeh", "A. Lazaric", "O.A. Maillard", "R. Munos"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical kernel-based reinforcement learning", "author": ["A. MS Barreto", "D. Precup", "J. Pineau"], "venue": null, "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2013}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 1017\u20131024. 8.2. Convergence Analysis of True Gradient Temporal Difference Learning 117 ACM,", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2009}, {"title": "Migration equilibrium and variational inequalities", "author": ["A. Nagurney"], "venue": "Economics Letters, 31:109\u2013112,", "citeRegEx": "116", "shortCiteRegEx": null, "year": 1989}, {"title": "Evolutionary Dynamics: Exploring the Equations of Life", "author": ["M. Novak"], "venue": "Harvard Belknap Press,", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2006}, {"title": "Networks, Crowds, and Markets: Reasoning About a Highly Connected World", "author": ["D. Easley", "J. Kleinberg"], "venue": "Cambridge University Press,", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2010}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D. Levine"], "venue": "MIT Press,", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithmic Game Theory", "author": ["N. Nisan", "T. Roughgarden", "E. Tardos", "V. Vazirani"], "venue": "Cambridge University Press,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2007}, {"title": "Economics", "author": ["P. Samuelson", "W. Nordhaus"], "venue": "McGraw Hill Press,", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2009}, {"title": "Projected Dynamical Systems and Variational Inequalities with Applications", "author": ["A. Nagurney", "D. Zhang"], "venue": "Kluwer Academic Press,", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1996}, {"title": "Multiagent learning with a variable learning rate", "author": ["M. Bowling", "M. Veloso"], "venue": "Artificial Intelligence, 136:215\u2013250,", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2002}, {"title": "Nash convergence of gradient dynamics in general-sum games", "author": ["S. Singh", "M. Kearns", "Y. Mansour"], "venue": "Proceedings of the Uncertainty in AI conference,", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2000}, {"title": "The extragradient method for finding saddle points and other problems", "author": ["G. Korpelevich"], "venue": "Matekon, 13:35\u201349,", "citeRegEx": "125", "shortCiteRegEx": null, "year": 1977}, {"title": "Structured prediction, dual extragradient and bregman projections", "author": ["B. Taskar", "S. Lacoste-Julien", "Michael Jordan"], "venue": "Matekon, 7:627\u20131653,", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2008}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Bruckner", "C. Kanzow", "T. Scheffer"], "venue": "Journal of Machine Learning Research, 13:2617\u20132654,", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2012}, {"title": "Numerical Recipes in C", "author": ["W. Press", "S. Tuekolsky", "W. Vettering", "B. Flannery"], "venue": "Cambridge University Press,", "citeRegEx": "128", "shortCiteRegEx": null, "year": 1992}, {"title": "First order methods for nonsmooth convex large-scale optimization, i: General purpose methods", "author": ["A. Juditsky", "A. Nemirovski"], "venue": "Optimization in Machine Learning. MIT Press,", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2011}, {"title": "First order methods for non- 118 Appendix: Technical Proofs smooth convex large-scale optimization, ii: Utilizing problem structure", "author": ["A. Juditsky", "A. Nemirovski"], "venue": "Optimization in Machine Learning. MIT Press,", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear Complementarity, Linear and Nonlinear Programming", "author": ["K. Murty"], "venue": "Heldermann Verlag,", "citeRegEx": "131", "shortCiteRegEx": null, "year": 1988}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B, Jan", "citeRegEx": "132", "shortCiteRegEx": null, "year": 1996}, {"title": "Fast active-set-type algorithms for L1regularized linear regression", "author": ["J. Kim", "H. Park"], "venue": "Proceedings of the Conference on AI and Statistics, pages 397\u2013404,", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2010}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "Proceedings of Advances in Neural Information Processing Systems, 23,", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularized off-policy TDlearning", "author": ["B. Liu", "S. Mahadevan", "J. Liu"], "venue": "Proceedings of the International Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1997}, {"title": "A variant of Korpelevich\u2019s method for variational inequalities with a new search strategy", "author": ["A. Iusem", "B. Svaiter"], "venue": "Optimization, 42:309\u2013321,", "citeRegEx": "137", "shortCiteRegEx": null, "year": 1997}, {"title": "Modification of the extragradient method for solving variational inequalities of certain optimization problems", "author": ["E. Khobotov"], "venue": "USSR Computational Mathematics and Mathematical Physics, 27:120\u2013127,", "citeRegEx": "138", "shortCiteRegEx": null, "year": 1987}, {"title": "Application of Khobotov\u2019s algorithm to variational inequalities and network equilibrium problems", "author": ["P. Marcotte"], "venue": "INFORM, 29,", "citeRegEx": "139", "shortCiteRegEx": null, "year": 1991}, {"title": "A new hybrid extragradient method for generalized mixed equilibrium problems, fixed point problems, and variational inequality problems", "author": ["J. Peng", "J. Yao"], "venue": "Taiwanese Journal of Mathematics, 12:1401\u20131432,", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2008}, {"title": "Dual extrapolation and its application to solving variational inequalities and related problems", "author": ["Y. Nesterov"], "venue": "Mathematical Programming Series B., 109:319\u2013344,", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2007}, {"title": "A new projection method for variational inequality problems", "author": ["M. Solodov", "B. Svaiter"], "venue": "SIAM Journal of Control and Optimization, 37(3):756\u2013776,", "citeRegEx": "142", "shortCiteRegEx": null, "year": 1999}, {"title": "Projected equations, variational inequalities, and temporal difference methods", "author": ["D. Bertsekas"], "venue": "Technical Report LIDS-P-2808, MIT, March", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2009}, {"title": "Galerkin methods for complementarity problems and variational inequalities", "author": ["G. Gordon"], "venue": "Arxiv, June", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2013}, {"title": "Introductory lectures on convex optimization: A basic course, volume", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2004}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B. T Polyak", "A. B Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855,", "citeRegEx": "146", "shortCiteRegEx": null, "year": 1992}, {"title": "New error bounds for approximations from projected linear equations", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Technical Report C-2008-43, Dept. Computer Science, Univ. of Helsinki,", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "In this chapter, we lay out the elements of our novel framework for reinforcement learning [1], based on doing temporal difference learning not in the primal space, but in a dual space defined by a so-called mirror map.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "examples of the benefits of our framework, showing each of the four key pieces of our solution: the improved performance of our new off-policy temporal difference methods over previous gradient TD methods, like TDC and GTD2 [2]; how we are able to generalize natural gradient actor critic methods using mirror maps, and achieve safety guarantees to control learning in complex robots; and finally, elements of our saddle point reformulation of temporal difference learning.", "startOffset": 224, "endOffset": 227}, {"referenceID": 2, "context": "endre\u201d transform [3].", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "We use Bregman divergences [4] to ensure that safety constraints are adhered to, where the projection is defined as:", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].", "startOffset": 32, "endOffset": 38}, {"referenceID": 5, "context": "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].", "startOffset": 32, "endOffset": 38}, {"referenceID": 6, "context": "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "The framework of mirror descent [5, 6] plays a central role in our framework, which includes not just the original mirror descent method, but also the mirror-prox method [7], which generalizes the extragradient method to non-Euclidean geometries [8].", "startOffset": 246, "endOffset": 249}, {"referenceID": 8, "context": "Variational inequalities, in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid-1960s in the", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Finitedimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11], who showed that the traffic network equilibrium problem could be formulated as a finite-dimensional VI.", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "could also be formulated as finite-dimensional VIs \u2013 the books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "could also be formulated as finite-dimensional VIs \u2013 the books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "We will see later how this property of extragradient makes its appearance in accelerating gradient temporal difference learning algorithms, such as TDC [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "The mirror-prox algorithm (MP) [7] is a first-order approach that is able to solve saddle-point problems at a convergence rate of O(1/t).", "startOffset": 31, "endOffset": 34}, {"referenceID": 12, "context": "The MP method plays a key role in our framework as our approach extensively uses the saddle point reformulation of reinforcement learning developed by us [14].", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "With this introduction, we can now introduce the main concept of mirror descent, which was originally proposed by Nemirovksi and Yudin [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "We follow the treatment in [6] in presenting the mirror descent algorithm as a nonlinear proximal method based on a distance generator function that is a Bregman divergence [4].", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "We follow the treatment in [6] in presenting the mirror descent algorithm as a nonlinear proximal method based on a distance generator function that is a Bregman divergence [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "It is shown in [15] that the mirror descent procedure specified in Equation 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "3 with the Bregman divergence defined by the p-norm function [16] can outperform regular projected subgradient method by a factor n logn where n is the dimensionality of the space.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "In our framework, a key insight used to derive a true stochastic gradient method for reinforcement learning is based on the powerful concept of operator splitting [17, 18].", "startOffset": 163, "endOffset": 171}, {"referenceID": 16, "context": "In our framework, a key insight used to derive a true stochastic gradient method for reinforcement learning is based on the powerful concept of operator splitting [17, 18].", "startOffset": 163, "endOffset": 171}, {"referenceID": 3, "context": "This problem originally motivated the development of Bregman divergences [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "The convex feasibility problem is an example of many real-world problems, such as 3D voxel reconstruction in brain imaging [15], a high-dimensional problem that mirror descent was originally developed for.", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "Many different operator splitting strategies have been developed, such as Douglas Rachford splitting [18], which is a generalization of widely used distributed optimization methods like Alternating Direction Method of Multipliers [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "Many different operator splitting strategies have been developed, such as Douglas Rachford splitting [18], which is a generalization of widely used distributed optimization methods like Alternating Direction Method of Multipliers [19].", "startOffset": 230, "endOffset": 234}, {"referenceID": 20, "context": "Our proposed framework solves this problem by establishing a key technical result, stated below, between mirror descent and the well-known, but previously unrelated, class of algorithms called natural gradient [22].", "startOffset": 210, "endOffset": 214}, {"referenceID": 18, "context": "5: The uBot-5 is a 11 degree of freedom mobile manipulator developed at the Laboratory of Perceptual Robotics (LPR) at the University of Massachusetts, Amherst [20, 21].", "startOffset": 160, "endOffset": 168}, {"referenceID": 19, "context": "5: The uBot-5 is a 11 degree of freedom mobile manipulator developed at the Laboratory of Perceptual Robotics (LPR) at the University of Massachusetts, Amherst [20, 21].", "startOffset": 160, "endOffset": 168}, {"referenceID": 20, "context": "How can we design a \u201csafe\u201d reinforcement learning algorithm which is guaranteed to ensure that policy learning will not violate pre-defined constraints such that such robots will operate in dangerous regions of the control parameter space? Our framework provides a key solution, based on showing an equivalence between mirror descent and a previously well-studied but unrelated algorithm called natural gradient [22].", "startOffset": 412, "endOffset": 416}, {"referenceID": 21, "context": "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.", "startOffset": 165, "endOffset": 169}, {"referenceID": 22, "context": "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "Although many more sophisticated methods have been developed over the past three decades, such as least-squares based temporal difference approaches, including LSTD [23], LSPE [24] and LSPI [25], first-order temporal difference learning algorithms may scale more gracefully to high dimensional problems.", "startOffset": 190, "endOffset": 194}, {"referenceID": 24, "context": "This motivated the development of the gradient TD (GTD) family of methods [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "A crucial step in the development of our framework was the development of a novel saddle-point framework for sparse regularized GTD [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "Although these algorithms are motivated from the gradient of an objective function such as mean-squared projected Bellman error (MSPBE) and NEU [26], they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "Although these algorithms are motivated from the gradient of an objective function such as mean-squared projected Bellman error (MSPBE) and NEU [26], they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.", "startOffset": 257, "endOffset": 261}, {"referenceID": 12, "context": "(4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit.", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "In this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28].", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "7 illustrates a sample result, showing how the mirror descent variant of temporal difference learning results in faster convergence, and much lower variance (not shown) on the classic mountain car task [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 27, "context": "The learning environment for decision-making is generally modeled by the well-known Markov Decision Process[29] M = (S,A, P,R, \u03b3), which is derived from a Markov chain.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "(Markov Decision Process)[29]: A Markov Decision Process is a tuple (S,A, P,R, \u03b3) where S is a finite set of states, A is a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition kernel, where P (s, a, s\u2032) is the probability of transmission from state s to state s\u2032 given action a, and reward r : S \u00d7 A \u2192 R+ is a reward function, 0 \u2264 \u03b3 < 1 is a discount factor.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "(Markov Decision Process)[29]: A Markov Decision Process is a tuple (S,A, P,R, \u03b3) where S is a finite set of states, A is a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition kernel, where P (s, a, s\u2032) is the probability of transmission from state s to state s\u2032 given action a, and reward r : S \u00d7 A \u2192 R+ is a reward function, 0 \u2264 \u03b3 < 1 is a discount factor.", "startOffset": 163, "endOffset": 169}, {"referenceID": 0, "context": "A stochastic policy \u03c0 : S \u00d7A\u2192 [0, 1].", "startOffset": 30, "endOffset": 36}, {"referenceID": 28, "context": "The most popular and widely used RL method is temporal difference (TD) learning [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 29, "context": "The (optimal) action value formulation is convenient because it can be approximately solved by a temporal-difference (TD) learning technique called Q-learning [31].", "startOffset": 159, "endOffset": 163}, {"referenceID": 30, "context": "TD(0) converges to the optimal value function V \u03c0 for policy \u03c0 as long as the samples are \u201con-policy\u201d, namely following the stochastic Markov chain associated with the policy; and the learning rate \u03b1t is decayed according to the Robbins-Monro conditions in stochastic approximation theory: \u2211 t \u03b1t = \u221e, \u2211 t \u03b1 2 t < \u221e [32].", "startOffset": 316, "endOffset": 320}, {"referenceID": 31, "context": ") or automatically generated basis functions [33].", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Here we define the problem of Stochastic Composite Optimization (SCO)[34]:", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "(Distance-generating Function)[35]: A distancegenerating function \u03c8(x) is defined as a continuously differentiable \u03bcstrongly convex function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "(Bregman Divergence)[35]: Given distancegenerating function \u03c8, the Bregman divergence induced by \u03c8 is defined as:", "startOffset": 20, "endOffset": 24}, {"referenceID": 34, "context": "1 p + 1 q = 1 [36].", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "The mirror descent [35] algorithm is a generalization of classic gradient descent, which has led to developments of new more powerful machine learning methods for classification and regression.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].", "startOffset": 97, "endOffset": 101}, {"referenceID": 36, "context": "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "Mirror descent has become the cornerstone of many online l1 regularization approaches such as in [37], [38] and [39].", "startOffset": 112, "endOffset": 116}, {"referenceID": 36, "context": "Regularized dual averaging (RDA) [38] is a variant of Dual averaging (DA) with \u201csimple\u201d regularizers, such as l1 regularization.", "startOffset": 33, "endOffset": 37}, {"referenceID": 38, "context": "The DA method approximates this lower bound model with an approximate (possibly not supporting) lower bound hyperplane with the averaging of all the past gradients [40].", "startOffset": 164, "endOffset": 168}, {"referenceID": 39, "context": "RDA with local stabilizer can be seen in [41].", "startOffset": 41, "endOffset": 45}, {"referenceID": 40, "context": "Compared with other firstorder l1 regularization algorithms of the mirror-descent type, including truncated gradient method [42] and SMIDAS [43], RDA tends to produce sparser solutions in that the RDA method is more aggressive on sparsity than many other competing approaches.", "startOffset": 124, "endOffset": 128}, {"referenceID": 41, "context": "Compared with other firstorder l1 regularization algorithms of the mirror-descent type, including truncated gradient method [42] and SMIDAS [43], RDA tends to produce sparser solutions in that the RDA method is more aggressive on sparsity than many other competing approaches.", "startOffset": 140, "endOffset": 144}, {"referenceID": 42, "context": "It is worth noting that problems with non-smooth regularization functions often lead to solutions that lie on a low-dimensional supporting data manifold, and regularized dual averaging is capable of identifying this manifold, and thus bringing the potential benefit of accelerating convergence rate by searching on the low-dimensional manifold after it is identified, as suggested in [44].", "startOffset": 384, "endOffset": 388}, {"referenceID": 7, "context": "The extragradient method was first proposed by Korpelevich[8] as a relaxation of ordinary gradient descent to solve variational inequality (VI) problems.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "4) is guaranteed under the constraints 0 < \u03b1t < 1 \u221a 2L [7], where L is the Lipschitz constant for \u2207f(x).", "startOffset": 55, "endOffset": 58}, {"referenceID": 43, "context": "Later work and variants of Nesterov\u2019s method utilizing the strong convexity of the loss function with Bregman divergence are summarized in [45].", "startOffset": 139, "endOffset": 143}, {"referenceID": 32, "context": "from deterministic smooth convex optimization to stochastic composite optimization, termed as AC-SA, is studied in [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "An important property of closed proper convex functions is that their subdifferentials induce a relation on Rn called a maximal monotone operator [17, 46].", "startOffset": 146, "endOffset": 154}, {"referenceID": 44, "context": "An important property of closed proper convex functions is that their subdifferentials induce a relation on Rn called a maximal monotone operator [17, 46].", "startOffset": 146, "endOffset": 154}, {"referenceID": 45, "context": "The convexconcave saddle-point problems are, therefore, usually better suited for first-order methods [47].", "startOffset": 102, "endOffset": 106}, {"referenceID": 46, "context": "A comprehensive overview on extending convex minimization to convex-concave saddle-point problems with unified variational inequalities is presented in [48].", "startOffset": 152, "endOffset": 156}, {"referenceID": 47, "context": "Using the approach in [49], Equation (2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 48, "context": "A general procedure for solving the monotone inclusion problem, the proximal point algorithm [50], uses the following identities:", "startOffset": 93, "endOffset": 97}, {"referenceID": 49, "context": "In the case where the relation R = \u2202f of some convex function f , the resolvent can be shown to be the proximal mapping [51], a crucially important abstraction of the concept of projection, a cornerstone of constrained optimization.", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "Operator splitting [17, 18] is a generic approach to decomposing complex optimization and variational inequality problems into simpler ones that involve computing the resolvents of individual relations, rather than sums or other compositions of relations.", "startOffset": 19, "endOffset": 27}, {"referenceID": 16, "context": "Operator splitting [17, 18] is a generic approach to decomposing complex optimization and variational inequality problems into simpler ones that involve computing the resolvents of individual relations, rather than sums or other compositions of relations.", "startOffset": 19, "endOffset": 27}, {"referenceID": 16, "context": "We will primarily focus on the Douglas Rachford algorithm [18] specified in Figure 2.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "1, because it leads to a widely used distributed optimization method called Alternating Direction Method of Multipliers (ADMM) [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "The ADMM algorithm has been extensively studied in optimization; a detailed review is available in the tutorial paper by Boyd and colleagues [19], covering both its theoretical properties, operator splitting origins, and applications to high-dimensional data mining.", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "In this section we will give a brief overview of proximal splitting algorithms [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 50, "context": "Proximal methods [53, 54], which are widely used in machine learning, signal processing, and stochastic optimization, provide a general framework for large-scale optimization.", "startOffset": 17, "endOffset": 25}, {"referenceID": 51, "context": "Proximal methods [53, 54], which are widely used in machine learning, signal processing, and stochastic optimization, provide a general framework for large-scale optimization.", "startOffset": 17, "endOffset": 25}, {"referenceID": 52, "context": "is Forward-Backward Splitting (FOBOS) [55]", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "To solve this problem, Douglas-Rachford splitting [28] and Alternating Direction of Multiple Multipliers (ADMM) can be used.", "startOffset": 50, "endOffset": 54}, {"referenceID": 53, "context": "Recently, ADMM has been used proposed for sparse RL [56].", "startOffset": 52, "endOffset": 56}, {"referenceID": 54, "context": "The corresponding primal-dual formulation [57, 28, 58] of Section (2.", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "The corresponding primal-dual formulation [57, 28, 58] of Section (2.", "startOffset": 42, "endOffset": 54}, {"referenceID": 55, "context": "The corresponding primal-dual formulation [57, 28, 58] of Section (2.", "startOffset": 42, "endOffset": 54}, {"referenceID": 20, "context": "However, in several settings it is more appropriate to assume that xk resides in a Riemannian space with metric tensor G(xk), which is an n \u00d7 n positive definite matrix that may vary with xk [22].", "startOffset": 191, "endOffset": 195}, {"referenceID": 56, "context": "In this case, the direction of steepest descent is called the natural gradient and is given by \u2212G(xk)\u2207f(xk) [59].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "In certain cases, (which include our policy search application), following the natural gradient is asymptotically Fisher-efficient [22].", "startOffset": 131, "endOffset": 135}, {"referenceID": 30, "context": "It is worth noting that we make little use of classical stochastic approximation theory, which has traditionally been used to analyze reinforcement learning methods (as discussed in detail in books such as [32]).", "startOffset": 206, "endOffset": 210}, {"referenceID": 57, "context": "[60] (l1-regularized Projection): \u03a0l1 is the l1regularized projection defined as:", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "t weighted l2 norm, as proven in [60].", "startOffset": 33, "endOffset": 37}, {"referenceID": 57, "context": "[60]: \u03a0\u03c1 is a non-expansive mapping such that \u2200x, y \u2208 R, ||\u03a0\u03c1x\u2212\u03a0\u03c1y|| \u2264 ||x\u2212 y|| \u2212 ||x\u2212 y \u2212 (\u03a0\u03c1x\u2212\u03a0\u03c1y)||", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[60] (Lasso-TD) Lasso-TD is a fixed-point equation w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "The properties of Lasso-TD is discussed in detail in [60].", "startOffset": 53, "endOffset": 57}, {"referenceID": 58, "context": "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].", "startOffset": 93, "endOffset": 97}, {"referenceID": 59, "context": "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].", "startOffset": 106, "endOffset": 110}, {"referenceID": 60, "context": "Several prevailing sparse RL methods use Lasso-TD as the objective function, such as SparseTD[61], LARS-TD[62] and LCP-TD[63].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "LCPTD [12] formulates LASSO-TD as a linear complementarity problem (LCP), which can be solved by a variety of available LCP solvers.", "startOffset": 6, "endOffset": 10}, {"referenceID": 52, "context": "We then derive the major step by formulating the problem as a forward-backward splitting problem (FOBOS) as in [55],", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "This \u03c8(w) leads to the p-norm link function \u03b8 = f(w) where f : Rd \u2192 Rd [16]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "The p-norm function has been extensively studied in the literature on online learning [16], and it is well-known that for large p, the corresponding classification or regression method behaves like a multiplicative method (e.", "startOffset": 86, "endOffset": 90}, {"referenceID": 61, "context": ", the p-norm regression method for large p behaves like an exponentiated gradient method (EG) [64, 65]).", "startOffset": 94, "endOffset": 102}, {"referenceID": 62, "context": ", the p-norm regression method for large p behaves like an exponentiated gradient method (EG) [64, 65]).", "startOffset": 94, "endOffset": 102}, {"referenceID": 5, "context": "Another distance generating function is the negative entropy function \u03c8(w) = \u2211 iwi logwi, which leads to the entropic mirror descent algorithm [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 63, "context": "Interestingly, this special case has been previously explored [66] as the exponentiated-gradient TD method, although the connection to mirror descent and Bregman divergences were not made in this previous study, and EG does not generate sparse solutions [37].", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "Interestingly, this special case has been previously explored [66] as the exponentiated-gradient TD method, although the connection to mirror descent and Bregman divergences were not made in this previous study, and EG does not generate sparse solutions [37].", "startOffset": 254, "endOffset": 258}, {"referenceID": 35, "context": "An analogous approach was suggested in [37] for l1 penalized classification and regression.", "startOffset": 39, "endOffset": 43}, {"referenceID": 64, "context": "We base our derivation on the composite mirror-descent approach proposed in [67] for classification and regression.", "startOffset": 76, "endOffset": 80}, {"referenceID": 57, "context": "Definition 2 [60]: \u03a0l1 is the l1-regularized projection defined as: \u03a0l1y = \u03a6\u03b1 such that \u03b1 = arg minw\u2016y \u2212 \u03a6w\u2016 +\u03b2\u2016w\u20161, which is a nonexpansive mapping w.", "startOffset": 13, "endOffset": 17}, {"referenceID": 57, "context": "t weighted l2 norm induced by the on-policy sample distribution setting, as proven in [60].", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "In Algorithm 2, the l1 solver is related to the SMIDAS l1 regularized mirror-descent method for regression and classification [37].", "startOffset": 126, "endOffset": 130}, {"referenceID": 35, "context": "Employing the result of Theorem 3 in [37], after the N -th iteration, the l1 approximation error is bounded by", "startOffset": 37, "endOffset": 41}, {"referenceID": 65, "context": "2 shows that mirror-descent TD converges more quickly with far smaller Bellman errors than LARS-TD [68] on a discrete \u201ctworoom\u201d MDP [69].", "startOffset": 99, "endOffset": 103}, {"referenceID": 66, "context": "2 shows that mirror-descent TD converges more quickly with far smaller Bellman errors than LARS-TD [68] on a discrete \u201ctworoom\u201d MDP [69].", "startOffset": 132, "endOffset": 136}, {"referenceID": 66, "context": "The basis matrix \u03a6 was automatically generated as 50 proto-value functions by diagonalizing the graph Laplacian of the discrete state space connectivity graph[69].", "startOffset": 158, "endOffset": 162}, {"referenceID": 40, "context": "ferent values of p yield an interpolation between the truncated gradient method [42] and SMIDAS [43].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "ferent values of p yield an interpolation between the truncated gradient method [42] and SMIDAS [43].", "startOffset": 96, "endOffset": 100}, {"referenceID": 67, "context": "6: Top: Q-learning; Bottom: mirror-descent Q-learning with pnorm link function, both with 25 fixed Fourier bases [70] for the mountain car task.", "startOffset": 113, "endOffset": 117}, {"referenceID": 68, "context": "The triple-link inverted pendulum [71] is a highly nonlinear time-variant under-actuated system,", "startOffset": 34, "endOffset": 38}, {"referenceID": 68, "context": "We base our simulation using the system parameters described in [71], except that the action space is discretized because the algorithms described here are restricted to policies with discrete actions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "The two most widely used link functions in mirror descent are the p-norm link function [6] and the relative entropy function for exponentiated gradient (EG) [64].", "startOffset": 87, "endOffset": 90}, {"referenceID": 61, "context": "The two most widely used link functions in mirror descent are the p-norm link function [6] and the relative entropy function for exponentiated gradient (EG) [64].", "startOffset": 157, "endOffset": 161}, {"referenceID": 61, "context": "It has been shown that when the features are dense and the optimal coefficients \u03b8\u2217 are sparse, EG converges faster than the regular additive gradient methods [64].", "startOffset": 158, "endOffset": 162}, {"referenceID": 63, "context": "It has been pointed out [66] that in the EG-Sarsa algorithm, rescaling can fail, and replacing eligible traces instead of regular additive eligible traces is used to prevent overflow.", "startOffset": 24, "endOffset": 28}, {"referenceID": 67, "context": "Thanks to the flexible interpolation capability between multiplicative and additive gradient updates, the p-norm link function is more robust and applicable to various basis functions, such as polynomial, radial basis function (RBF), Fourier basis [70], proto-value functions (PVFs), etc.", "startOffset": 248, "endOffset": 252}, {"referenceID": 69, "context": "Mirror Descent Q-learning demonstrates the following advantage over regular Q learning: faster convergence rate and reduced variance due to larger stepsizes with theoretical convergence guarantees [72].", "startOffset": 197, "endOffset": 201}, {"referenceID": 35, "context": "Compared with existing sparse reinforcement learning algorithms such as LARS-TD, Algorithm 2 has lower sample complexity and lower computation cost, advantages accrued from the first-order mirror descent framework combined with proximal mapping [37].", "startOffset": 245, "endOffset": 249}, {"referenceID": 1, "context": "We are currently exploring a mirror-descent fast-gradient RL method, which is both convergent off-policy and quicker than fast gradient TD methods such as GTD and TDC [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 64, "context": "We are also undertaking a more detailed theoretical analysis of the mirror-descent RL framework, building on existing analysis of mirror-descent methods [67, 37].", "startOffset": 153, "endOffset": 161}, {"referenceID": 35, "context": "We are also undertaking a more detailed theoretical analysis of the mirror-descent RL framework, building on existing analysis of mirror-descent methods [67, 37].", "startOffset": 153, "endOffset": 161}, {"referenceID": 70, "context": "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].", "startOffset": 131, "endOffset": 135}, {"referenceID": 69, "context": "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].", "startOffset": 203, "endOffset": 207}, {"referenceID": 30, "context": "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].", "startOffset": 222, "endOffset": 230}, {"referenceID": 71, "context": "Two types of theoretical investigations are being explored: regret bounds of mirror-descent TD methods, extending previous results [73] and convergence analysis combining robust stochastic approximation [72] and RL theory [32, 74].", "startOffset": 222, "endOffset": 230}, {"referenceID": 72, "context": "As pointed out in [75], the target policy is often a deterministic policy that approximates the optimal policy, and the behavior policy is often stochastic, exploring all possible actions in each state as part of finding the optimal policy.", "startOffset": 18, "endOffset": 22}, {"referenceID": 73, "context": "Besides, offpolicy methods are of wider applications since they are able to learn while executing an exploratory policy, learn from demonstrations, and learn multiple tasks in parallel [76].", "startOffset": 185, "endOffset": 189}, {"referenceID": 24, "context": "[26] introduced convergent off-policy temporal difference learning algorithms, such as TDC, whose computation time scales linearly with the number of samples and the number of features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Recently, a linear off-policy actor-critic algorithm based on the same framework was proposed in [76].", "startOffset": 97, "endOffset": 101}, {"referenceID": 45, "context": "Thus, convex-concave saddle-point problems are, therefore, usually better suited for first-order methods [47].", "startOffset": 105, "endOffset": 109}, {"referenceID": 46, "context": "A comprehensive overview on extending convex minimization to convex-concave saddlepoint problems with unified variational inequalities is presented in [48].", "startOffset": 151, "endOffset": 155}, {"referenceID": 47, "context": "Using the approach in [49], Equation (4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "MSPBE(\u03b8) = \u2016\u03a6\u03b8 \u2212\u03a0T (\u03a6\u03b8)\u20162\u039e = (\u03a6T\u039e(T\u03a6\u03b8 \u2212 \u03a6\u03b8))T (\u03a6T\u039e\u03a6)\u22121\u03a6T\u039e(T\u03a6\u03b8 \u2212 \u03a6\u03b8) = E[\u03b4t(\u03b8)\u03c6t]E[\u03c6t\u03c6t ]E[\u03b4t(\u03b8)\u03c6t] To avoid computing the inverse matrix (\u03a6T\u039e\u03a6)\u22121 and to avoid the double sampling problem [1] in (4.", "startOffset": 187, "endOffset": 190}, {"referenceID": 24, "context": "Following [26], the TDC algorithm solution follows from the linear equation Ax = b, where a single iteration gradient update would be", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "The two time-scale gradient descent learning method TDC [26] is", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "As pointed out in [1], double-sampling is a necessary condition to obtain an unbiased estimator if the objective function is the Bellman residual or its derivatives (such as projected Bellman residual), wherein the product of Bellman error or projected Bellman error metrics are involved.", "startOffset": 18, "endOffset": 21}, {"referenceID": 74, "context": "1) with the following convexconcave formulation as in [77, 47],", "startOffset": 54, "endOffset": 62}, {"referenceID": 45, "context": "1) with the following convexconcave formulation as in [77, 47],", "startOffset": 54, "endOffset": 62}, {"referenceID": 45, "context": "The averaging step, which plays a crucial role in stochastic optimization convergence, generates the approximate saddle-points [47, 78]", "startOffset": 127, "endOffset": 135}, {"referenceID": 75, "context": "The averaging step, which plays a crucial role in stochastic optimization convergence, generates the approximate saddle-points [47, 78]", "startOffset": 127, "endOffset": 135}, {"referenceID": 76, "context": "GQ(\u03bb)[79] is a generalization of the TDC algorithm with eligibility traces and off-policy learning of temporally abstract predictions, where the gradient update changes from Equation (4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 76, "context": "where eligibility traces et, and \u03c6\u0304t, T \u03c0\u03bb are defined in [79].", "startOffset": 58, "endOffset": 62}, {"referenceID": 59, "context": "LARS-TD [62], which is a popular second-order sparse reinforcement learning algorithm, is used as the baseline algorithm for feature selection and TDC is used as the off-policy convergent RL baseline algorithm, respectively.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "are used in [26], which are tabular features, inverted features and dependent features respectively.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "An identical experiment setting to [26] is used for these two domains.", "startOffset": 35, "endOffset": 39}, {"referenceID": 59, "context": "To make a fair comparison, we use the same basis function setting as in [62], where two dimensional grids of 2, 4, 8, 16, 32 RBFs are used so that there are totally 1365 basis functions.", "startOffset": 72, "endOffset": 76}, {"referenceID": 59, "context": "We use the result of LARS-TD and l2 LSTD reported in [62].", "startOffset": 53, "endOffset": 57}, {"referenceID": 45, "context": "It is worth noting that comparing the performance of RO-TD and LARS-TD is not the major focus here, since LARS-TD is not convergent off-policy and RO-TD\u2019s performance can be further optimized using the mirror-descent approach with the Mirror-Prox algorithm [47] which incorporates mirror descent with an extragradient [8], as discussed below.", "startOffset": 257, "endOffset": 261}, {"referenceID": 7, "context": "It is worth noting that comparing the performance of RO-TD and LARS-TD is not the major focus here, since LARS-TD is not convergent off-policy and RO-TD\u2019s performance can be further optimized using the mirror-descent approach with the Mirror-Prox algorithm [47] which incorporates mirror descent with an extragradient [8], as discussed below.", "startOffset": 318, "endOffset": 321}, {"referenceID": 68, "context": "The triple-link inverted pendulum [71] is a highly nonlinear underactuated system with 8-dimensional state space and discrete action space.", "startOffset": 34, "endOffset": 38}, {"referenceID": 77, "context": "Fourier basis [80] with order 2 is used, resulting in 6561 basis functions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 56, "context": "Natural actor-critics form a class of policy search algorithms for finding locally optimal policies for Markov decision processes (MDPs) by approximating and ascending the natural gradient [59] of an objective", "startOffset": 189, "endOffset": 193}, {"referenceID": 78, "context": "For example, proportional-integral-derivative controllers (PID controllers) are the most widely used control algorithms in industry, and have been studied in depth [81].", "startOffset": 164, "endOffset": 168}, {"referenceID": 79, "context": "Techniques exist for determining the set of stable gains (policy parameters) when a model of the system is available [82].", "startOffset": 117, "endOffset": 121}, {"referenceID": 80, "context": "There has been a recent push to develop controllers that specify how much and when to stimulate each muscle in a human arm to move it from its current position to a desired position [84].", "startOffset": 182, "endOffset": 186}, {"referenceID": 81, "context": "Hence, a proportional-derivative (PD) controller, tuned to a simulation of an ideal human arm, required manual tuning to obtain desirable performance on a human subject with biceps spasticity [85].", "startOffset": 192, "endOffset": 196}, {"referenceID": 82, "context": "Researchers have shown that policy search algorithms are a viable approach to creating controllers that can automatically adapt to an individual\u2019s arm by training on a few hundred two-second reaching movements [86].", "startOffset": 210, "endOffset": 214}, {"referenceID": 83, "context": "Researchers have addressed safety concerns like these before [87].", "startOffset": 61, "endOffset": 65}, {"referenceID": 84, "context": "Bendrahim and Franklin [88] showed how a walking biped robot can switch to a stabilizing controller whenever the robot leaves a stable region of state space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 85, "context": "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].", "startOffset": 81, "endOffset": 93}, {"referenceID": 86, "context": "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].", "startOffset": 81, "endOffset": 93}, {"referenceID": 87, "context": "Similar state-avoidant approaches to safety have been proposed by several others [89, 90, 91].", "startOffset": 81, "endOffset": 93}, {"referenceID": 88, "context": "[92] developed a method for performing risk-sensitive policy search, which models the variance of the objective function for each policy and permits runtime adjustments of risk sensitivity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 89, "context": "[93] presented projected natural actor-critic algorithms for the average reward setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 90, "context": "[94] presented mirror descent using the Mahalanobis norm for the proximal function, which is very similar to the proximal function that we show to cause mirror descent to be equivalent to natural gradient descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 90, "context": "Also, since Gk depends on k, \u03c8k is an adaptive proximal function [94].", "startOffset": 65, "endOffset": 69}, {"referenceID": 91, "context": "2 N\u0302X(x) is the normal cone of X at x if x \u2208 X and \u2205 otherwise [95].", "startOffset": 63, "endOffset": 67}, {"referenceID": 92, "context": "A necessary and sufficient condition for x to be a fixed point of PSG is that \u2212\u2207f(x) \u2208 N\u0302X(x) [96].", "startOffset": 94, "endOffset": 98}, {"referenceID": 93, "context": "Natural actor-critics, first proposed by Kakade [97], are algorithms that estimate and ascend the natural gradient of J(\u03b8), using the average Fisher information matrix as the metric tensor:", "startOffset": 48, "endOffset": 52}, {"referenceID": 94, "context": "where d\u03c0 is a policy and objective function-dependent distribution over the state set [98].", "startOffset": 86, "endOffset": 90}, {"referenceID": 95, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 123, "endOffset": 127}, {"referenceID": 96, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 177, "endOffset": 182}, {"referenceID": 96, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 221, "endOffset": 226}, {"referenceID": 97, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 276, "endOffset": 281}, {"referenceID": 98, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 323, "endOffset": 328}, {"referenceID": 89, "context": "There are many natural actor-critics, including Natural policy gradient utilizing the Temporal Differences (NTD) algorithm [99], Natural Actor-Critic using LSTD-Q(\u03bb) (NAC-LSTD) [100], Episodic Natural Actor-Critic (eNAC) [100], Natural Actor-Critic using Sarsa(\u03bb) (NAC-Sarsa) [101], Incremental Natural Actor-Critic (INAC) [102], and Natural-Gradient Actor-Critic with Advantage Parameters (NGAC) [93].", "startOffset": 397, "endOffset": 401}, {"referenceID": 99, "context": "Many of the existing natural policy gradient algorithms, including NAC-LSTD, eNAC, NAC-Sarsa, and INAC, follow biased estimates of the natural policy gradient [103].", "startOffset": 159, "endOffset": 164}, {"referenceID": 99, "context": "The unbiased discounted reward form of NAC-Sarsa was recently derived [103].", "startOffset": 70, "endOffset": 75}, {"referenceID": 89, "context": "where {\u03bct} is a stepsize schedule [93].", "startOffset": 34, "endOffset": 38}, {"referenceID": 100, "context": "We used the Dynamic Arm Simulator 1 (DAS1) [104], a detailed biomechanical simulation of a human arm undergoing functional electrical stimulation.", "startOffset": 43, "endOffset": 48}, {"referenceID": 81, "context": "In a previous study, a controller created using DAS1 performed well on an actual human subject undergoing FES, although it required some additional tuning in order to cope with biceps spasticity [85].", "startOffset": 195, "endOffset": 199}, {"referenceID": 0, "context": "The arm is controlled by providing a stimulation in the interval [0, 1] to each of six muscles.", "startOffset": 65, "endOffset": 71}, {"referenceID": 81, "context": "The reward function used was similar to that of Jagodnik and van den Bogert [85], which punishes joint angle error and high muscle stimulation.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21].", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "2, is an 11-DoF mobile manipulator developed at the University of Massachusetts Amherst [20, 21].", "startOffset": 88, "endOffset": 96}, {"referenceID": 21, "context": "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "Although least-squares based temporal difference approaches, such as LSTD [23], LSPE [24] and LSPI [25] perform well with moderate size problems, first-order temporal difference learning algorithms scale more gracefully to high dimensional problems.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "This motivated the development of the gradient TD (GTD) family of methods [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "A novel saddle-point framework for sparse regularized GTD was proposed recently [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "Although these algorithms are motivated from the gradient of an objective function such as MSPBE and NEU, they are not true stochastic gradient methods with respect to these objective functions, as pointed out in [27], which make the convergence rate and error bound analysis difficult, although asymptotic analysis has been carried out using the ODE approach.", "startOffset": 213, "endOffset": 217}, {"referenceID": 12, "context": "(4) The fourth question is on regularization: although the saddle point framework proposed in [14] provides an online regularization framework for the GTD family of algorithms, termed as RO-TD, it is based on the inverse problem formulation and is thus not quite explicit.", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "In this paper, we propose a novel approach to TD algorithm design in reinforcement learning, based on introducing the proximal splitting framework [28].", "startOffset": 147, "endOffset": 151}, {"referenceID": 101, "context": "However, TD is only guaranteed to converge in the on-policy setting, although in many off-policy situations, it still has satisfactory performance [105].", "startOffset": 147, "endOffset": 152}, {"referenceID": 24, "context": "TDC [26] aims to minimize the mean-square projected Bellman error (MSPBE) with a similar two-time-scale technique, which is defined as MSPBE(\u03b8) =", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "As pointed out in [27], although many algorithms are motivated by well-defined convex objective functions such as MSPBE and NEU, due to the biased sampling problem, the unbiased stochastic gradient is impossible to obtain, and thus the algorithms are not true SGD methods w.", "startOffset": 18, "endOffset": 22}, {"referenceID": 53, "context": "This auxiliary variable technique is also used in [56].", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "As pointed out in [27], although the GTD family of algorithms are derived from the gradient w.", "startOffset": 18, "endOffset": 22}, {"referenceID": 102, "context": "Note that if h(\u03b8) = 0 andX = Rd, then we will have the GTD algorithm proposed in [106].", "startOffset": 81, "endOffset": 86}, {"referenceID": 24, "context": "Note that if h(\u03b8) = 0 and X = Rd, then we will have the GTD2 algorithm proposed in [26].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Note that if h(\u03b8) = 0 and X = Rd, then we will have TDC algorithm proposed in [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 103, "context": "which is the update rule of residual gradient [107], and is proven not to converge to NEU any more.", "startOffset": 46, "endOffset": 51}, {"referenceID": 104, "context": "A comprehensive overview of the convergence rate of different approaches to stochastic saddle-point problems is given in [108].", "startOffset": 121, "endOffset": 126}, {"referenceID": 45, "context": "In this section we present accelerated algorithms based on the Stochastic Mirror-Prox (SMP) Algorithm [47, 109].", "startOffset": 102, "endOffset": 111}, {"referenceID": 105, "context": "In this section we present accelerated algorithms based on the Stochastic Mirror-Prox (SMP) Algorithm [47, 109].", "startOffset": 102, "endOffset": 111}, {"referenceID": 72, "context": "1 It converges to mean-square TD error (MSTDE), as proven in [75].", "startOffset": 61, "endOffset": 65}, {"referenceID": 106, "context": "However, using the pre-conditioning technique introduced in [110], ADMM", "startOffset": 60, "endOffset": 65}, {"referenceID": 12, "context": "2 Although only regularized TDC was proposed in [14], the algorithm can be easily extended to regularized GTD and GTD2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 55, "context": "can be reduced to the primal-dual splitting method as pointed out in [58].", "startOffset": 69, "endOffset": 73}, {"referenceID": 107, "context": "Now we consider the two-state MDP in [111].", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "9, and a one-feature basis \u03a6 = [1, 2]T .", "startOffset": 31, "endOffset": 37}, {"referenceID": 1, "context": "9, and a one-feature basis \u03a6 = [1, 2]T .", "startOffset": 31, "endOffset": 37}, {"referenceID": 59, "context": "t \u03c1, whereas Lasso-TD may have multiple solutions if the P -matrix condition is not satisfied [62].", "startOffset": 94, "endOffset": 98}, {"referenceID": 108, "context": "We use the random generated MDP with 400 states and 10 actions in [112].", "startOffset": 66, "endOffset": 71}, {"referenceID": 108, "context": "The parameters of each algorithm are chosen via comparative studies similar to [112].", "startOffset": 79, "endOffset": 84}, {"referenceID": 109, "context": "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].", "startOffset": 147, "endOffset": 152}, {"referenceID": 110, "context": "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].", "startOffset": 194, "endOffset": 204}, {"referenceID": 111, "context": "Future research is ongoing to explore other operator splitting techniques beyond primal-dual splitting as well as incorporating random projections [113], and investigating kernelized algorithms [114, 115].", "startOffset": 194, "endOffset": 204}, {"referenceID": 7, "context": "Methods like extragradient [8] and the mirror-prox algorithm were originally proposed to solve variational inequalities and related saddle point problems.", "startOffset": 27, "endOffset": 30}, {"referenceID": 112, "context": "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].", "startOffset": 142, "endOffset": 147}, {"referenceID": 113, "context": "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].", "startOffset": 209, "endOffset": 214}, {"referenceID": 114, "context": "The concept of equilibrium plays a key role in understanding not only the Internet, but also other networked systems, such as human migration [116], evolutionary dynamics and the spread of infectious diseases [117], and social networks [118].", "startOffset": 236, "endOffset": 241}, {"referenceID": 115, "context": "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.", "startOffset": 50, "endOffset": 60}, {"referenceID": 116, "context": "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.", "startOffset": 50, "endOffset": 60}, {"referenceID": 117, "context": "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.", "startOffset": 72, "endOffset": 77}, {"referenceID": 27, "context": "Equilibria are also a central idea in game theory [119, 120], economics [121], operations research [29], and many related areas.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "We are currently exploring two powerful mathematical tools for the study of equilibria \u2013 variational inequalities (VIs) and projected dynamical systems (PDS) [12, 122] \u2013 in developing a new machine learning framework for solving", "startOffset": 158, "endOffset": 167}, {"referenceID": 118, "context": "We are currently exploring two powerful mathematical tools for the study of equilibria \u2013 variational inequalities (VIs) and projected dynamical systems (PDS) [12, 122] \u2013 in developing a new machine learning framework for solving", "startOffset": 158, "endOffset": 167}, {"referenceID": 8, "context": "Variational inequalities (VIs), in the infinite-dimensional setting, were originally proposed by Hartman and Stampacchia [10] in the mid1960s in the context of solving partial differential equations in mechanics.", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Finite-dimensional VIs rose in popularity in the 1980s partly as a result of work by Dafermos [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "books by Nagurney [12] and Facchinei and Pang [13] provide a detailed introduction to the theory and applications of finite-dimensional VIs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 118, "context": "Projected dynamical systems (PDS) [122] are a class of ordinary differential equations (ODEs) with a discontinuous right-hand side.", "startOffset": 34, "endOffset": 39}, {"referenceID": 119, "context": "[123, 119, 124].", "startOffset": 0, "endOffset": 15}, {"referenceID": 115, "context": "[123, 119, 124].", "startOffset": 0, "endOffset": 15}, {"referenceID": 120, "context": "[123, 119, 124].", "startOffset": 0, "endOffset": 15}, {"referenceID": 121, "context": "One of the original algorithms for solving finite-dimensional VIs is the extragradient method proposed by Korpelevich [125].", "startOffset": 118, "endOffset": 123}, {"referenceID": 122, "context": "[126].", "startOffset": 0, "endOffset": 5}, {"referenceID": 123, "context": "[127] use a modified extragradient method for solving the spam filtering problem modeled as a prediction game.", "startOffset": 0, "endOffset": 5}, {"referenceID": 124, "context": "We are developing a new family of extragradient-like methods based on well-known numerical methods for solving ordinary differential equations, specifically the Runge Kutta method [128].", "startOffset": 180, "endOffset": 185}, {"referenceID": 4, "context": "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called \u201cmirrror-prox\u201d algorithm [129, 130].", "startOffset": 134, "endOffset": 137}, {"referenceID": 125, "context": "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called \u201cmirrror-prox\u201d algorithm [129, 130].", "startOffset": 191, "endOffset": 201}, {"referenceID": 126, "context": "In optimization, the extragradient algorithm was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called \u201cmirrror-prox\u201d algorithm [129, 130].", "startOffset": 191, "endOffset": 201}, {"referenceID": 127, "context": "In an NCP, whenever the mapping function F is affine, that is F (x) = Mx + b, where M is an n \u00d7 n matrix, then the corresponding NCP is called a linear complementarity problem (LCP) [131].", "startOffset": 182, "endOffset": 187}, {"referenceID": 128, "context": "Recent work on learning sparse models using L1 regularization has exploited the fact that the standard LASSO objective [132] of L1 penalized regression can be reduced to solving an LCP [133].", "startOffset": 119, "endOffset": 124}, {"referenceID": 129, "context": "Recent work on learning sparse models using L1 regularization has exploited the fact that the standard LASSO objective [132] of L1 penalized regression can be reduced to solving an LCP [133].", "startOffset": 185, "endOffset": 190}, {"referenceID": 130, "context": "This reduction to LCP has been used in recent work on sparse value function approximation as well in a method called LCP-TD [134].", "startOffset": 124, "endOffset": 129}, {"referenceID": 115, "context": "The VI framework provides a mathematically elegant approach to model equilibrium problems in game theory [119, 120].", "startOffset": 105, "endOffset": 115}, {"referenceID": 116, "context": "The VI framework provides a mathematically elegant approach to model equilibrium problems in game theory [119, 120].", "startOffset": 105, "endOffset": 115}, {"referenceID": 125, "context": "Nash games are closely related to saddle point problems [129, 130, 135].", "startOffset": 56, "endOffset": 71}, {"referenceID": 126, "context": "Nash games are closely related to saddle point problems [129, 130, 135].", "startOffset": 56, "endOffset": 71}, {"referenceID": 131, "context": "Nash games are closely related to saddle point problems [129, 130, 135].", "startOffset": 56, "endOffset": 71}, {"referenceID": 10, "context": "Many equilibria problems in economics can be modeled using VIs [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 121, "context": "The extragradient method of Korpolevich [125] addresses some of these concerns, and is defined as Algorithm 2 below.", "startOffset": 40, "endOffset": 45}, {"referenceID": 132, "context": "3: Left: This figure illustrates a VI where the basic projection algorithm (Algorithm 1) fails, but the extragradient algorithm (Algorithm 2) succeeds [136].", "startOffset": 151, "endOffset": 156}, {"referenceID": 133, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 134, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 135, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 136, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 137, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 138, "context": ", see [137, 138, 139, 140, 141, 142].", "startOffset": 6, "endOffset": 36}, {"referenceID": 134, "context": "Khobotov [138] proved that the extragradient method converges under the weaker requirement of pseudo-monotone mappings, 6 when the learning rate is automatically adjusted based on a local measure of the Lipschitz constant.", "startOffset": 9, "endOffset": 14}, {"referenceID": 133, "context": "Iusem [137] proposed a variant whereby the current iterate was projected onto a hyperplane separating the current iterate from the final solution, and subsequently projected from the hyperplane onto the feasible set.", "startOffset": 6, "endOffset": 11}, {"referenceID": 138, "context": "Solodov and Svaiter [142] proposed another hyperplane method, whereby the current iterate is projected onto the intersection of the hyperplane and the feasible set.", "startOffset": 20, "endOffset": 25}, {"referenceID": 4, "context": "Finally, the extragradient method was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called \u201cmirrror-prox\u201d algorithm [129].", "startOffset": 123, "endOffset": 126}, {"referenceID": 125, "context": "Finally, the extragradient method was generalized to the non-Euclidean case by combining it with the mirror-descent method [5], resulting in the so-called \u201cmirrror-prox\u201d algorithm [129].", "startOffset": 180, "endOffset": 185}, {"referenceID": 30, "context": "Variational inequalities also provide a useful framework for reinforcement learning [32, 1].", "startOffset": 84, "endOffset": 91}, {"referenceID": 0, "context": "Variational inequalities also provide a useful framework for reinforcement learning [32, 1].", "startOffset": 84, "endOffset": 91}, {"referenceID": 139, "context": "Generalizing this, consider solving for the fixed point of a projected equation x\u2217 = \u03a0\u015cT (x \u2217) [143, 144] for a functional mapping T : Rn \u2192 Rn, where \u03a0\u015c is the projector onto a low-dimensional convex subspace \u015c w.", "startOffset": 95, "endOffset": 105}, {"referenceID": 140, "context": "Generalizing this, consider solving for the fixed point of a projected equation x\u2217 = \u03a0\u015cT (x \u2217) [143, 144] for a functional mapping T : Rn \u2192 Rn, where \u03a0\u015c is the projector onto a low-dimensional convex subspace \u015c w.", "startOffset": 95, "endOffset": 105}, {"referenceID": 139, "context": "Following [143], note that this is a variational inequality of the form \u3008F (x\u2217), (x\u2212x\u2217)\u3009 \u2265 0 if we identify F (x) = \u039e(x\u2212T (x)), and in the lowerdimensional space, \u3008F (\u03a6r\u2217),\u03a6(r\u2212 r\u2217)\u3009, \u2200r \u2208 R\u0302.", "startOffset": 10, "endOffset": 15}, {"referenceID": 139, "context": "It is shown in [143] that if T is a contraction mapping, then F (x) = \u039e(x \u2212 T (x)) is strongly monotone.", "startOffset": 15, "endOffset": 20}, {"referenceID": 140, "context": "Gordon [144] proposes an alternative approach separating the projection of the current iterate on the low-dimensional subspace spanned by \u03a6 from its projection onto the feasible set.", "startOffset": 7, "endOffset": 12}, {"referenceID": 139, "context": "Both of these approaches [143, 144] have been only studied with the simple projection method (Algorithm 1), and can be generalized to a more powerful class of VI methods that we are currently developing.", "startOffset": 25, "endOffset": 35}, {"referenceID": 140, "context": "Both of these approaches [143, 144] have been only studied with the simple projection method (Algorithm 1), and can be generalized to a more powerful class of VI methods that we are currently developing.", "startOffset": 25, "endOffset": 35}, {"referenceID": 45, "context": "We first present the monotone operator corresponding to the bilinear saddle-point problem and then extend it to stochastic approximation case with certain restrictive assumptions, and use the result in [47].", "startOffset": 202, "endOffset": 206}, {"referenceID": 105, "context": "d noise, then with the result in [109], we can prove that the RO-TD algorithm converges to the global minimizer of", "startOffset": 33, "endOffset": 38}, {"referenceID": 75, "context": "With the subgradient boundedness assumption and using the result in Proposition 1 in [78], this can be proved.", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "We first present the assumptions for the MDP and basis functions, which are similar to [26, 14].", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "We first present the assumptions for the MDP and basis functions, which are similar to [26, 14].", "startOffset": 87, "endOffset": 95}, {"referenceID": 104, "context": "Next we present the assumptions for the stochastic saddle point problem formulation, which are similar to [108, 109].", "startOffset": 106, "endOffset": 116}, {"referenceID": 105, "context": "Next we present the assumptions for the stochastic saddle point problem formulation, which are similar to [108, 109].", "startOffset": 106, "endOffset": 116}, {"referenceID": 0, "context": ", \u2200\u03b8, \u03b8\u2032 \u2208 X,\u03bb \u2208 [0, 1], K(\u03bb\u03b8 + (1\u2212 \u03bb)\u03b8)\u2264CK\u03bbK(\u03b8) + (1\u2212 \u03bb)K(\u03b8\u2032),", "startOffset": 17, "endOffset": 23}, {"referenceID": 54, "context": "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.", "startOffset": 42, "endOffset": 54}, {"referenceID": 55, "context": "The corresponding primal-dual formulation [57, 28, 58] of Equation (8.", "startOffset": 42, "endOffset": 54}, {"referenceID": 141, "context": "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + \u03c3 \u221a N ).", "startOffset": 27, "endOffset": 42}, {"referenceID": 105, "context": "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + \u03c3 \u221a N ).", "startOffset": 27, "endOffset": 42}, {"referenceID": 104, "context": "Using the bounds proved in [145, 109, 108], the optimal convergence rate of stochastic saddle-point problem is O(F N2 + LK N + \u03c3 \u221a N ).", "startOffset": 27, "endOffset": 42}, {"referenceID": 142, "context": "Hence, by adding the primal average step, GTD/GTD2 algorithms will become standard Polyak\u2019s algorithms [146], and thus the convergence rates are O(FK \u221a N ) ac-", "startOffset": 103, "endOffset": 108}, {"referenceID": 47, "context": "cording to [49].", "startOffset": 11, "endOffset": 15}, {"referenceID": 105, "context": "According to [109] which extends the SMP algorithm to solving saddle-point problems and variational inequality problems, the convergence rate is accelerated to O(FK N + \u03c3 \u221a N ).", "startOffset": 13, "endOffset": 18}, {"referenceID": 107, "context": "Here we use the result in [111], which is similar to the one in [147].", "startOffset": 26, "endOffset": 31}, {"referenceID": 143, "context": "Here we use the result in [111], which is similar to the one in [147].", "startOffset": 64, "endOffset": 69}, {"referenceID": 107, "context": "Lemma 2 [111]: For any V\u03b8 = \u03a6\u03b8, the following component-wise equality holds V \u2212 V\u03b8 = (I \u2212 \u03b3\u03a0P )\u22121 (( V \u2212\u03a0V ) + \u03a6(\u03a6T\u039e\u03a6)\u22121K(\u03b8) )", "startOffset": 8, "endOffset": 13}], "year": 2014, "abstractText": "Reinforcement learning is a simple, and yet, comprehensive theory of learning that simultaneously models the adaptive behavior of artificial agents, such as robots and autonomous software programs, as well as attempts to explain the emergent behavior of biological systems. It also gives rise to computational ideas that provide a powerful tool to solve problems involving sequential prediction and decision making. Temporal difference learning is the most widely used method to solve reinforcement learning problems, with a rich history dating back more than three decades. For these and many other reasons, devel1 This article is currently not under review for the journal Foundations and Trends in ML, but will be submitted for formal peer review at some point in the future, once the draft reaches a stable \u201cequilibrium\u201d state. ar X iv :1 40 5. 67 57 v1 [ cs .L G ] 2 6 M ay 2 01 4 oping a complete theory of reinforcement learning, one that is both rigorous and useful has been an ongoing research investigation for several decades. In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified \u201csafely\u201d guarantees, and remains in a stable region of the parameter space (iii) how to design \u201coff-policy\u201d temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators. The most important idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform, as we show, elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design \u201ctrue\u201d stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators in Hilbert spaces, both in optimization and for variational inequalities. The latter framework, the subject of another ongoing investigation by our group, holds the promise of an even more elegant framework for reinforcement learning. Its explication is currently the topic of a further monograph that will appear in due course. Dedicated to Andrew Barto and Richard Sutton for inspiring a generation of researchers to the study of reinforcement learning. Algorithm 1 TD (1984) (1) \u03b4t = rt + \u03b3\u03c6 \u2032 t T \u03b8t \u2212 \u03c6t \u03b8t (2) \u03b8t+1 = \u03b8t + \u03b2t\u03b4t Algorithm 2 GTD2-MP (2014) (1) wt+ 1 2 = wt + \u03b2t(\u03b4t \u2212 \u03c6t wt)\u03c6t, \u03b8t+ 1 2 = prox\u03b1th ( \u03b8t + \u03b1t(\u03c6t \u2212 \u03b3\u03c6t)(\u03c6t wt) ) (2) \u03b4t+ 1 2 = rt + \u03b3\u03c6 \u2032 t T \u03b8t+ 1 2 \u2212 \u03c6t \u03b8t+ 1 2 (3) wt+1 = wt + \u03b2t(\u03b4t+ 1 2 \u2212 \u03c6t wt+ 1 2 )\u03c6t , \u03b8t+1 = prox\u03b1th ( \u03b8t + \u03b1t(\u03c6t \u2212 \u03b3\u03c6t)(\u03c6t wt+ 1 2 ) )", "creator": "LaTeX with hyperref package"}}}