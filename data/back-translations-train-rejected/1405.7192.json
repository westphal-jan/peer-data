{"id": "1405.7192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2014", "title": "The PeerRank Method for Peer Assessment", "abstract": "We propose the PeerRank method for peer assessment. This constructs a grade for an agent based on the grades proposed by the agents evaluating the agent. Since the grade of an agent is a measure of their ability to grade correctly, the PeerRank method weights grades by the grades of the grading agent. The PeerRank method also provides an incentive for agents to grade correctly. As the grades of an agent depend on the grades of the grading agents, and as these grades themselves depend on the grades of other agents, we define the PeerRank method by a fixed point equation similar to the PageRank method for ranking web-pages. We identify some formal properties of the PeerRank method (for example, it satisfies axioms of unanimity, no dummy, no discrimination and symmetry), discuss some examples, compare with related work and evaluate the performance on some synthetic data. Our results show considerable promise, reducing the error in grade predictions by a factor of 2 or more in many cases over the natural baseline of averaging peer grades.", "histories": [["v1", "Wed, 28 May 2014 10:51:57 GMT  (91kb,D)", "http://arxiv.org/abs/1405.7192v1", "To appear in Proc. of ECAI 2014"]], "COMMENTS": "To appear in Proc. of ECAI 2014", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["toby walsh"], "accepted": false, "id": "1405.7192"}, "pdf": {"name": "1405.7192.pdf", "metadata": {"source": "CRF", "title": "The PeerRank Method for Peer Assessment", "authors": ["Toby Walsh"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "We are considering how to combine peer assessments of some papers to achieve an overall evaluation of this work. An important application of our proposed framework is the evaluation in massive open online courses (MOOCs). In such an environment, it may be impossible to offer anything but automatic assessments (where possible), and the application of the proposed framework is also the evaluation of funding applications. Often, there is only a small pool of experts who are able to review funding applications in a particular subsection. In many cases, these people have also submitted funding applications."}, {"heading": "2 PEER RANK RULE", "text": "We assume that agents evaluate their own work, but this can be relaxed. Moreover, as we show in the experimental section, the proposed PeerRank rule is relatively insensitive to any bias an agent may have in relation to his or her own work or that of other agents. Thus, the grade of each agent is constructed from the grades of the agents who rate the agent. Since the grade is a measure of their ability to rate the agent correctly, we rate an agent's grade according to their own grade. Thus, the grade of an agent is the weighted average of the grades of the agents who rate the agent. Now, the grades of the agents who rate an agent are themselves very different from the weighted averages of the agents who rate the agent. An agent's grade gives another agent according to their own grade. Thus, the grade of an agent is the weighted average of the agents who rate the agent who evaluate the agent."}, {"heading": "Proof:", "text": "In the matrix notation we have at the fixed point: X = (1 \u2212 \u03b1) X + \u03b1 | X | A.XThat is, X = X \u2212 \u03b1X + \u03b1 | X | A.XOrdering and canceling results in: \u03b1 | X | A.X = \u03b1XDividing by \u03b1 and leaving \u03bb = | X | results in:"}, {"heading": "A.X = \u03bbX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 SOME EXAMPLES", "text": "To illustrate how the PeerRank rule works in some simple cases, we will consider a few examples."}, {"heading": "Unanimous grade matrix", "text": "Suppose that every entry in the note matrix A is the note k with 0 \u2264 k \u2264 1. Now an eigenvector of A and the PeerRank solution assigns this note k to each agent. The weighted average of identical note values is always the same, regardless of the weights. That's what we might expect. That's all the note matrix tells us."}, {"heading": "Identity grade matrix", "text": "Suppose the grade matrix A is the identity matrix. That is, each agent gives itself the maximum grade 1 and each other agent the minimum grade 0. Now, an eigenvector of A and the PeerRank solution assign each agent the average grade 1. Again, this is what we might expect. The grade matrix tells us no more than all agents are symmetrical, and therefore it might seem reasonable to divide the marker between them."}, {"heading": "Bivalent grade matrices", "text": "Suppose the agents are divided into two types: good and bad agents. Good agents give other good agents a grade of 1 and bad agents a grade of 0. Bad agents give each agent a grade of 1. In each iteration of the PeerRank method, the grades of good agents remain unchanged at 1. On the other hand, the grades of bad agents drop monotonously towards their fixed point at 0. We also get the same fixed point if the bad agents give each agent but themselves a grade of 0 (regardless of the grade they give themselves). Again, this is what we might expect: the PeerRank method identifies the good and bad agents and rewards them accordingly."}, {"heading": "4 PROPERTIES", "text": "The PeerRank rule has a number of desirable (axiomatic) properties. Several of these properties (e.g. no dummy and no discrimination) are properties that have been studied in peer selection of a price [4]. First, we argue that the PeerRank rule returns a normalized grade."}, {"heading": "Proof:", "text": "We prove that Xni \u2264 1 by induction to n in the base case is X0i \u2264 1, since it is the average of terms that are themselves smaller or equal to 1. In increments, we assume that 0 \u2264 Xni \u2264 1 for all i. Let us leave Xni = 1 \u2212 where 0 \u2264 1. ThenXn + 1i = (1 \u2212 \u03b1) (1 \u2212) + \u03b1 \u2211 j n j j.Ai, j \u2264 1 \u2212 \u03b1 \u2212 (1 \u2212 \u03b1) + \u03b1 = 1 \u2212 (1 \u2212 \u03b1) \u2264 1Note that these limits are achievable. In particular, if all PeerRank values are 0 (1), then the PeerRank rule gives each agent this grade. Next, we argue that if all agents give the same grade to an agent, this is their final grade.Proposition 3 (discrepancy) If all PeerRank agents give this agent the grade, then the PeerRank rule gives the agent the grade."}, {"heading": "Proof:", "text": "Suppose all agents give the agent i the note k. Consider the ith component of the fixed-point equation: Xi = (1 \u2212 \u03b1).Xi + \u03b1 \u2211 j Xj \u2211 j.Ai, jRearrange results in: \u03b1Xi = \u03b1 \u2211 j \u2211 j Xj.Ai, divide by \u03b1 and multiply the fraction results in: (\u2211 j Xj).Xi = \u2211 j Xj.Ai, j = (\u2211 j Xj).kDivide by the common term \u2211 j Xj, we get: Xi = k. The PeerRank rule also fulfils a non-discrimination axiom. Any vector of notes is possible.Sentence 4 (No discrimination) Given any vector of notes, there is a note matrix with which the PeerRank rule returns this vector."}, {"heading": "Proof:", "text": "Then we construct the note matrix with Ai, j = ki and call for unanimity. The PeerRank rules also fulfill a no-dummy axiom, since each agent has some influence on the final note. Suggestion 5 (No Dummy) There are two note matrices that differ only in the notes assigned by an agent and for which PeerRank delivers different endnotes."}, {"heading": "Proof:", "text": "Consider the note matrix, in which each agent gives each other the highest grade 1, and the note matrix, which is identical except for Agent i, gives each agent the minimum grade 0. Then, in the first case, PeerRank Agent i gives a grade of 1 and in the second one of 0. Therefore, I am not a dummy. The PeerRank rule also fulfills a simple symmetry axiom. Proposition 6 (symmetry) If we swap the notes of two agents and the notes that the two agents receive, then the PeerRank rule reverses the notes assigned to the two agents. It is also interesting to identify characteristics that the PeerRank rule does not have. For example, it is not impartial. Your grades of others affect your own final grade. As a second example, it is not anonymous. It is important who gives you a grade. It is better to get good grades from an agent who himself than from an agent who himself receives bad grades."}, {"heading": "5 GENERALIZED PEERRANK", "text": "The PeerRank rule proposed so far does not give agents an incentive to accurately evaluate other agents or even themselves. We therefore add an additional term to provide such an incentive. Suppose that \u03b1 and \u03b2 parameters are \u03b1 + \u03b2 \u2264 1. Then, we recursively define the generalized PeerRank rule by the following equation: Xn + 1i = (1 \u2212 \u03b1 \u2212 \u03b2).X n i + \u03b1\u03b2. The new term measures the normalized absolute error in the grades given by an agent. This is similar to the reward given in the most recent mechanism for reviewing NSF proposals in the SSS program [6]. The agent \"receives\" a credit with respect to their grade \u03b2 times this normalized grade."}, {"heading": "Proof:", "text": "Let us prove that Xni \u2264 1 is to n by induction. Let us suppose that 0 \u2264 Xni \u2264 1 is to all i. Let us suppose Xni = 1 \u2212 where 0 \u2264 \u2264 \u2264 \u2264 1.ThenXn + 1i = (1 \u2212 \u03b1 \u2212 \u03b2) (1 \u2212) + \u03b1 \u2211 j n j \u2211 j Xnj.Ai, j + \u03b2 m \u2211 j 1 \u2212 | Aj, i \u2212 Xnj | \u2264 1 \u2212 \u03b1 \u2212 \u03b2 \u2212 (1 \u2212 \u03b2) + \u03b1 \u2211 j n j \u2211 j Xnj + \u03b2m \u0445 j 1 \u2264 1 \u03b1 \u2212 \u03b2) + \u03b2 \u2264 1 \u2212 (1 \u2212 (\u03b1 + \u03b2))) Let us remember that \u03b1 + \u03b2 \u2264 1 and \u2265 0. So let us consider (1 \u2212 (\u03b1 + \u03b2)) \u2265 0. Therefore, Xn + 1i \u2264 1.To demonstrate the effects of the new term that favor accurate grading, let us revisit the simple matrices that were previously considered matrices."}, {"heading": "Unanimous grade matrix", "text": "Suppose each entry in the note matrix A is the note k. Now, the generalized PeerRank solution assigns each agent a note greater or equal to k (if m = 1, k = 1, or \u03b2 = 0)."}, {"heading": "Identity grade matrix", "text": "Suppose the grade matrix A is the identity matrix. That is, each agent gives himself the maximum grade 1 and each other agent the minimum grade 0. Now, the generalized PeerRank solution assigns each agent a grade greater than or equal to 1 m (if equal to m = 1 or \u03b2 = 0)."}, {"heading": "Bivalent grade matrices", "text": "Suppose the agents are divided into two types: good and bad. Good agents give the good agents a grade of 1 and bad agents a grade of 0. Bad agents give each agent a grade of 1. The general PeerRank method now gives the good agents a grade of less than or equal to 1 and the bad agents a grade of more or equal to 0. Bad agents get some recognition for the (semi) accurate grading of the good agents. This means that the grading of the bad agents by the good agents was a little too harsh and their own grade suffers."}, {"heading": "6 EXPERIMENTAL EVALUATION", "text": "We tested the performance of the generalised PeerRank rule on the basis of some synthetic data. In all experiments, we set \u03b1 = \u03b2 = 0.1. However, the results are relatively insensitive to the actual choice of \u03b1 or \u03b2. Based on the promise shown in these experiments, we are currently preparing a Peer World test with students. Our typical experimental setup consists of 10 agents giving each other an integer grade between 0 and 10 and an actual grade between 0 and 100. Therefore, a simple baseline against which we compare is the sum of this peer grade (or equivalent to the average of the normalized peer grade). We call this the AVERAGE rule. We investigated a number of different distributions of marks among the agents (e.g. binomial, normal, uniform, uniform). In the next section, these are discussed in more detail by comparing the rating of the rating of this rating with the rating of the rating of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the benchmark of the bending of the benchmark of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending of the bending."}, {"heading": "6.1 Mark distributions", "text": "We start with a simple binomial distribution of the notes. We let the actual markers of the agents be a binomial distribution of 100 studies with a given probability p. In Figure 1, we record the RMSE (root mean square error) of the predicted markers as a percentage of the 100 markers for different p. Thus, an RMSE of 5% means that the PeerRank grade is out with a root mean square error of 5 markers (out of the 100 possible markers). If we use the notes of 10, this means that we are less than half a note worse. For p > 0.6 (in other words, where the notes are typically above 60 out of 100, the generalized PeerRank method simply performs better than the average marks of 10. For p > 0.65, the error is 4% or less, which compares well with the error returned by simply deviating the comparison marks (which are mostly above 10%)."}, {"heading": "6.2 Group size", "text": "Next, we look at the influence that the size of this group has on the accuracy of the PeerRank method. Therefore, we conducted an experiment in which we varied the number of PeerRank agents. Again, we used a binomial grade distribution with a mean of 70. For 5 or more agents, the error of the generalized PeerRank method was less than 5% and was half or half as high as the average of the PeerRank method. For 10 to 20 agents, the error of the generalized PeerRank method was less than one-third of the average value of the PeerRank grades. These results suggest that the PeerRank method does not require many PeerRank grades to achieve an accurate result. Ideally, we need about 10 grades for each agent, but even with only 5 grades we are often able to achieve acceptable results."}, {"heading": "6.3 Biased marks", "text": "Although there is no explicit collusion, there are studies that suggest that students give each other generous grades (e.g. [5]). To investigate this, we blend the mean of peer grades up or down by a factor of r. For example, if r = 1.1 then increases the mean peer grade by 10%. On the other hand, if r = 0.9 then decreases the mean peer grade by 10%, we again use a binomial distribution of actual grades with a mean of 70. In Figure 4, we present the RMSE of the predicted grade again as a percentage of 100 grades, while we vary the bias of peer grades. At 0.75 \u2264 r \u2264 1.25, the error of the generalized PeerRank method is 5% or less of the overall grade. That is, we are able to tolerate a bias in the grading of peer grades without significantly increasing the error."}, {"heading": "7 RELATED WORK", "text": "In fact, it is in such a way that one sees oneself in a position to go to another world, in which one can go to another world, in which one feels oneself to be transported back to another world, in which one is able to go to another world, in which one feels oneself to be transported back to another world, in which one feels oneself to be transported back to another world, in which one feels oneself to be transported, in which one is able to go to a world, in which one feels oneself to be transported back to a world, in which one feels oneself to be transported, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives, in which one lives."}, {"heading": "8 Conclusions", "text": "The PeerRank method weights the grades according to the grades of the grading agents. In addition, it rewards agents for good grading and punishes those who are poorly graded. Since an agent's grades depend on the grades of the grading agents, and since they themselves depend on the grades of other agents, the PeerRank method is defined by a fixed-point equation similar to the PageRank method. We have identified some formal properties of the PeerRank method, discussed some examples, and evaluated performance using some synthetic data. In many cases, the method reduces the error in grading by a factor of 2 or more above the natural baseline of average grading. As the method prefers consensus, it is best suited for domains where there are objective answers, but the number of agents is too large for grading to be anything but graded. There are many possible extensions."}], "references": [{"title": "Sum of us: Strategyproof selection from the selectors", "author": ["N. Alon", "F. Fischer", "A. Procaccia", "M. Tennenholtz"], "venue": "Proceedings of the 13th Conferene on Theoretical Aspects of Rationality and Knowledge,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Shavlovsky, \u2018Crowdgrader: Crowdsourcing the evaluation of homework assignments", "author": ["M.L. de Alfaro"], "venue": "Technical Report 1308.5273,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Impartial division of a dollar", "author": ["G. DeClippel", "H. Moulin", "N. Tideman"], "venue": "Journal of Economic Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Impartial nominations for a prize", "author": ["R. Holzman", "H. Moulin"], "venue": "Econometrica, 81(1),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Domazlicky, \u2018Peer grading of essays in a principles of microeconomics course", "author": ["P.M. Kerr", "K.H. Park", "B.R"], "venue": "Journal of Education for Business,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Telescope time without tears: a distributed approach to peer review", "author": ["M.R. Merrifield", "D.G. Saari"], "venue": "Astronomy & Geophysics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Incentives, quality, and risks: A look into the NSF proposal review pilot", "author": ["P. Naghizadeh", "M. Liu"], "venue": "Technical Report 1307.6528,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "The PageRank citation ranking: Bringing order to the web.", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical Report 1999-66,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Tuned models of peer assessment in MOOCs", "author": ["Chris Piech", "Jonathan Huang", "Zhenghao Chen", "Chuong Do", "Andrew Ng", "Daphne Koller"], "venue": "Proceedings of The 6th International Conference on Educational Data Mining (EDM 2013),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "PEAS: Peer expect autograde self: Peer evaluation system for MOOC", "author": ["J. Singh", "K. Jain", "N. Vedula", "P. Mathur", "S. Agrawal", "P. Agraewal"], "venue": "Poster,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Grades are normalised into the interval [0, 1].", "startOffset": 40, "endOffset": 46}, {"referenceID": 7, "context": "This is reminiscent of the problem faced by the PageRank algorithm [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "no dummy and no discrimination) are properties that have been studied by in peer selection of a prize [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "Proposition 2 (Domain) The PeerRank rule returns grades in [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 5, "context": "This is similar to the reward given in the recent mechanism for reviewing NSF proposals in the SSS program [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "For the domain property, we need to prove afresh that the additional term cannot take us outside the interval [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "Proposition 7 (Domain) The generalized PeerRank rule returns grades in [0, 1].", "startOffset": 71, "endOffset": 77}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "have estimated and corrected for grader biases and reliabilities [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "PEAS is a new peer evaluation extension for the EdX open source MOOC platform [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 5, "context": "The NSF has therefore decided to pilot a mechanism for peer review that is adapted from one first proposed by Merrifield and Saari [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "Another work which is close to ours is the CrowdGrader mechanism for peer evaluation from UC Santa Cruz [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "For example, Holzman and Moulin have studied a related problem in which a set of agents wish to select one amongst them to receive a prize [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "Another related problem is \u201cselection from the selectors\u201d [1].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "A closely related problem is the division of cash between a group of partners [3].", "startOffset": 78, "endOffset": 81}], "year": 2014, "abstractText": "We propose the PeerRank method for peer assessment. This constructs a grade for an agent based on the grades proposed by the agents evaluating the agent. Since the grade of an agent is a measure of their ability to grade correctly, the PeerRank method weights grades by the grades of the grading agent. The PeerRank method also provides an incentive for agents to grade correctly. As the grades of an agent depend on the grades of the grading agents, and as these grades themselves depend on the grades of other agents, we define the PeerRank method by a fixed point equation similar to the PageRank method for ranking web-pages. We identify some formal properties of the PeerRank method (for example, it satisfies axioms of unanimity, no dummy, no discrimination and symmetry), discuss some examples, compare with related work and evaluate the performance on some synthetic data. Our results show considerable promise, reducing the error in grade predictions by a factor of 2 or more in many cases over the natural baseline of averaging peer grades.", "creator": "LaTeX with hyperref package"}}}