{"id": "1405.7430", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits", "abstract": "BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.", "histories": [["v1", "Thu, 29 May 2014 00:37:28 GMT  (11kb)", "http://arxiv.org/abs/1405.7430v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ruben martinez-cantin"], "accepted": false, "id": "1405.7430"}, "pdf": {"name": "1405.7430.pdf", "metadata": {"source": "CRF", "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits", "authors": ["Ruben Martinez-Cantin"], "emails": ["rmcantin@unizar.es"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.74 30v1 [cs.LG] 2 9M ay"}, {"heading": "1 Introduction", "text": "Bayesian optimization (Mockus, 1989; Brochu et al., 2010) is a special case of nonlinear optimization, where the algorithm decides on functions P (f) based on the analysis of a distribution, such as a Gaussian process or other surrogate model, based on a specific criterion known as the capture function. Bayesian optimization has the advantage of having a memory of all observations encoded in the rear region of the surrogate model P (f | D) (see Figure 1). Normally, this posterior distribution is updated sequentially using a nonparametric model. In this setup, each observation improves knowledge of the function throughout the input space thanks to the spatial correlation (core) of the model. Consequently, it requires a lower number of iterations compared to other nonlinear optimization algorithms. However, updating the posterior distribution and maximizing the acquisition functions to the cost optimization function of the sample bayesian function (rule) can significantly reduce the cost optimization function."}, {"heading": "2 BayesOpt library", "text": "BayesOpt uses a surrogate model of the form: f (x) = \u03c6 (x) Tw + \u0442 (x), which is a non-parametric process, e.g. a Gaussian, Student-t or a mixture of Gaussian processes. This model can be regarded as a linear regression model \u03c6 (x) Tw with heteroscedastic disorder \u0432 (x), as a non-parametric process with unequal mean function or as a semi-parametric model. The library allows the definition of hyperpriors based on w, \u03c32s and \u03b8. The marginal posterior P (f | D) can be compiled in closed form, with the exception of the core parameters \u03b8. Thus, BayesOpt allows the use of various posterior hyperpriors based on empirical Bayes (Santner et al., 2003) or Mnoet al. (Snoet al., 2012)."}, {"heading": "2.1 Implementation", "text": "For empirical Bayes (ML or MAP), we believe that a combination of global and local derivative methods such as DIRECT (Jones et al., 1993) and BOBYQA (Powell, 2009) may be unnecessary or even counterproductive in terms of CPU time and the graduation used in calculating total costs. One of the most critical components in terms of computational costs is calculating the reversal of the kernel probability used for each component. Furthermore, we have found that the Cholesky decomponents have a different method in terms of performance and numerical stability. Furthermore, we can take advantage of the structure of Bayesian optimization in terms of each component."}, {"heading": "2.2 Compatibility", "text": "BayesOpt is designed to be highly compatible with many platforms and setups. It has been tested and compiled under different operating systems (Linux, Mac OS, Windows), with different compilers (Visual Studio, GCC, Clang, MinGW). The core of the library is written in C + +, but offers interfaces for C, Python and Matlab / Octave."}, {"heading": "2.3 Using the library", "text": "There is a common API for multiple languages and programming paradigms. Before performing optimization, we need to follow two simple steps:"}, {"heading": "2.3.1 Target function definition", "text": "The definition of the function we want to optimize can be achieved in two ways: We can send the function (or pointer) directly to the optimizer, based on a function template. E.g. in C / C + +: double my function (unsigned int n query, const double \u0445 query, double \u0445 gradient, void \u0445 f unc data); The gradient has been added for future compatibility; the interfaces Python, Matlab, and Octave define a similar template function; for a more object-oriented approach, we can inherit the abstract module and define the virtual methods; with this approach, we can also include nonlinear constraints in the checkReachability method, which is available for C + + and Python. E.g. in C + +: class MyOptimization: public bayesopt::: ContinuousModel {:: MyOptimization (s i z e t dim, bopt params param param): Continuousdic {:}"}, {"heading": "2.3.2 BayesOpt parameters", "text": "The parameters are defined in the bopt parameters struct - or a dictionary in Python. Details of each parameter can be found in the accompanying documentation. The user can define expressions to combine different functions (cores, criteria, etc.) All parameters have a default value, so it is not necessary to define them all. For example, in Matlab: par. surr name ='sStudentTProcessNIG';% Surrogate model and hype rp r i o r s% We combine Expected Improvement, Lower Conf idence Bound Thompson sampling par. c r i t name = 'cHedge (cEI, cLCB, cThompsonSampling)'; par. kernel name = 'kSum (kMaternISO3, kRQISO) and Thompson sampling par."}], "references": [{"title": "Algorithms for hyper-parameter optimization", "author": ["James Bergstra", "Remi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Eric Brochu", "Vlad M. Cora", "Nando de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Convergence rates of efficient global optimization algorithms", "author": ["Adam D. Bull"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bull.,? \\Q2011\\E", "shortCiteRegEx": "Bull.", "year": 2011}, {"title": "Towards an empirical foundation for assessing bayesian optimization of hyperparameters", "author": ["Katharina Eggensperger", "Matthias Feurer", "Frank Hutter", "James Bergstra", "Jasper Snoek", "Holger Hoos", "Kevin Leyton-Brown"], "venue": "In BayesOpt workshop (NIPS),", "citeRegEx": "Eggensperger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eggensperger et al\\.", "year": 2013}, {"title": "Portfolio allocation for Bayesian optimization", "author": ["Matthew Hoffman", "Eric Brochu", "Nando de Freitas"], "venue": "In UAI,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In LION-5, page 507523,", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Lipschitzian optimization without the Lipschitz constant", "author": ["Donald R. Jones", "Cary D. Perttunen", "Bruce E. Stuckman"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Jones et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1993}, {"title": "Bayesian optimisation for intelligent environmental monitoring", "author": ["Roman Marchant", "Fabio Ramos"], "venue": "In IEEE/RSJ IROS,", "citeRegEx": "Marchant and Ramos.,? \\Q2012\\E", "shortCiteRegEx": "Marchant and Ramos.", "year": 2012}, {"title": "Bayesian Approach to Global Optimization, volume 37 of Mathematics and Its Applications", "author": ["Jonas Mockus"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Mockus.,? \\Q1989\\E", "shortCiteRegEx": "Mockus.", "year": 1989}, {"title": "The BOBYQA algorithm for bound constrained optimization without derivatives", "author": ["Michael J.D. Powell"], "venue": "Technical Report NA2009/06,", "citeRegEx": "Powell.,? \\Q2009\\E", "shortCiteRegEx": "Powell.", "year": 2009}, {"title": "Gaussian processes for machine learning (GPML) toolbox", "author": ["Carl E. Rasmussen", "Hannes Nickisch"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rasmussen and Nickisch.,? \\Q2010\\E", "shortCiteRegEx": "Rasmussen and Nickisch.", "year": 2010}, {"title": "DiceKriging, DiceOptim: two R packages for the analysis of computer experiments by kriging-based metamodelling and optimization", "author": ["Olivier Roustant", "David Ginsbourger", "Yves Deville"], "venue": "Journal of Statistical Software,", "citeRegEx": "Roustant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roustant et al\\.", "year": 2012}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan Adams"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Bayesian optimization in a billion dimensions via random embeddings", "author": ["Ziyu Wang", "Masrour Zoghi", "David Matheson", "Frank Hutter", "Nando de Freitas"], "venue": "In IJCAI,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Bayesian optimization (Mockus, 1989; Brochu et al., 2010) is a special case of nonlinear optimization where the algorithm decides which point to explore next based on the analysis of a distribution over functions P (f), for example a Gaussian process or other surrogate model.", "startOffset": 22, "endOffset": 57}, {"referenceID": 1, "context": "Bayesian optimization (Mockus, 1989; Brochu et al., 2010) is a special case of nonlinear optimization where the algorithm decides which point to explore next based on the analysis of a distribution over functions P (f), for example a Gaussian process or other surrogate model.", "startOffset": 22, "endOffset": 57}, {"referenceID": 12, "context": ", 2003) or MCMC (Snoek et al., 2012).", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "For empirical Bayes (ML or MAP of \u03b8), we found that a combination of global and local derivative free methods such as DIRECT (Jones et al., 1993) and BOBYQA (Powell, 2009) marginally outperforms in CPU time to gradient based method for optimizing \u03b8 by avoiding the overhead of computing the marginal likelihood derivative.", "startOffset": 125, "endOffset": 145}, {"referenceID": 9, "context": ", 1993) and BOBYQA (Powell, 2009) marginally outperforms in CPU time to gradient based method for optimizing \u03b8 by avoiding the overhead of computing the marginal likelihood derivative.", "startOffset": 19, "endOffset": 33}, {"referenceID": 2, "context": "Also, updating \u03b8 every iteration might be unnecessary or even counterproductive (Bull, 2011).", "startOffset": 80, "endOffset": 92}, {"referenceID": 5, "context": "SMAC (Hutter et al., 2011), HyperOpt (Bergstra et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 0, "context": ", 2011), HyperOpt (Bergstra et al., 2011) and Spearmint (Snoek et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 12, "context": ", 2011) and Spearmint (Snoek et al., 2012) used the HPOlib (Eggensperger et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 3, "context": ", 2012) used the HPOlib (Eggensperger et al., 2013) timing system (based on runsolver).", "startOffset": 24, "endOffset": 51}, {"referenceID": 11, "context": "DiceOptim (Roustant et al., 2012) used R timing system (proc.", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "We also provide a method for optimization in high-dimensional spaces (Wang et al., 2013).", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "This can be used to optimize a function considering an additional cost for each sample, for example a moving sensor (Marchant and Ramos, 2012).", "startOffset": 116, "endOffset": 142}, {"referenceID": 8, "context": "Second, inspired by the GPML toolbox by Rasmussen and Nickisch (2010), we can easily combine different components, like a linear combination of kernels or multiple criteria.", "startOffset": 40, "endOffset": 70}, {"referenceID": 4, "context": "BayesOpt also implements metacriteria algorithms, like the bandit algorithm GP-Hedge by Hoffman et al. (2011) that can be used to automatically select the most suitable criteria during the optimization.", "startOffset": 88, "endOffset": 110}, {"referenceID": 4, "context": "BayesOpt also implements metacriteria algorithms, like the bandit algorithm GP-Hedge by Hoffman et al. (2011) that can be used to automatically select the most suitable criteria during the optimization. Examples of these combinations can be found in Section 2.3.2. The third objective is correctness. For example, the library is thread and exception safe, allowing parallelized calls. Numerically delicate parts, such as the GP-Hedge algorithm, had been implemented with variation of the actual algorithm to avoid over- or underflow issues. The library internally uses NLOPT by Johnson (2014) for the inner optimization loops (optimize criteria, learn kernel parameters, etc.", "startOffset": 88, "endOffset": 593}], "year": 2014, "abstractText": "BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.", "creator": "LaTeX with hyperref package"}}}