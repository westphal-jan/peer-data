{"id": "1405.7471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab", "abstract": "K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown.", "histories": [["v1", "Thu, 29 May 2014 05:59:26 GMT  (283kb)", "http://arxiv.org/abs/1405.7471v1", "6 pages, 11 figures, Clustering, K Means"]], "COMMENTS": "6 pages, 11 figures, Clustering, K Means", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mr dibya jyoti bora", "dr anil kumar gupta"], "accepted": false, "id": "1405.7471"}, "pdf": {"name": "1405.7471.pdf", "metadata": {"source": "CRF", "title": "Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab", "authors": ["Dibya Jyoti Bora", "Anil Kumar Gupta"], "emails": [], "sections": [{"heading": "1. INTRODUCTION:", "text": "Clustering is an uncontrolled study. The main goal of clustering is to divide a data set into different subsets (so-called \"clusters\"), so that data in a particular subset has similar properties, while data in different subsets has different properties than data in another subset. Means that cluster techniques should gradually fulfill the two properties mentioned [1] [2]: 1. High Cohesive Property and 2. Low Coupling Property. Cluster algorithms are mainly divided into two types based on developed cluster properties: hierarchical and partitional techniques. Hierarchical methods generally attempt to divide the data set of n objects into a hierarchy of groups [1]. This hierarchical decomposition can be represented by a tree structure diagram called a dendrogram [3], whose root nodes represent the entire data set, and each leaf node is a single object of the data set."}, {"heading": "2. K \u2013MEANS ALGORITHM:", "text": "The K mean [5] is one of the famous partition clustering algorithms [6] [7] [8]. It takes the input parameter k, the number of clusters, and divides a series of n objects into k clusters, so that the resulting cluster similarity is high, but the similarity between clusters is low. The main idea is to define k centroids, one for each cluster. These centroids should be placed in a clever way, since different results are obtained due to different locations. Therefore, the better choice is to place them as far apart as possible. The next step is to take each point belonging to a given dataset and associate it with the nearest centroid. If no point is pending, the first step is completed and an early grouping occurs. At this point, we must recalculate k new centroids. After we have these k new centroids, a new binding between the same datasets and the new one must be made."}, {"heading": "1. Select K points as initial centroids.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. Repeat.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3. Form k clusters by assigning all points to the closest centroid.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4. Recompute the centroid of each cluster.Until the centroids do not change", "text": "www.ijcsit.com 2501A Diagram [9] of the K-Means algorithm is: Figure (1): K-Means algorithm"}, {"heading": "3. DISTANCE MEASUREMENTS IN K-MEANS ALGORITHMS:", "text": "In the K-Means algorithm, we calculate the distance between each point of the dataset and each initialized centrist. On the basis of the values found, the points are assigned to the centrist with minimum distance. Therefore, this distance calculation plays the decisive role in the cluster algorithm. As we know, the distance between two points can be calculated using different available techniques, so our main goal is to choose a suitable technique from the available ones. However, when selecting such techniques, some important points have to be taken into account, such as: the property of the data and the dimension of the dataset. In this experiment, we use \"Cityblock,\" \"Euclidean,\" Cosine \"and\" Correlation \"- these distance measurement techniques for distance calculations in the K-Means algorithm. The description of each technique is mentioned below:"}, {"heading": "3.1 City Block (Manhattan):", "text": "The distance of the city block [10] [11] between two points a and b with k dimensions is defined as follows: The name City Block Distance (also referred to as Manhattan Distance) [11] is explained by taking into account two points in the xy plane. The shortest distance between the two points is along the hypotenuse, i.e. the Euclidean distance. Instead, the distance of the city block is calculated as the distance in x plus distance in y, which corresponds to the way we move in a city (such as Manhattan) in which we must move around the buildings instead of going straight ahead."}, {"heading": "3.2 Euclidean distance:", "text": "The Euclidean distance between two points, a and b, with k dimensions, is calculated as follows [12] [13]: Euclidean (and square) distances are usually calculated from raw data and not from standardized data. An advantage of this method is that the distance between any two objects is not affected by the addition of new objects to the analysis, which can be outliers [13]. However, distances can be strongly influenced by scale differences between the dimensions from which the distances are calculated. For example, if one of the dimensions indicates a measured length in centimeters and is then converted into millimeters (by multiplying the values by 10), the resulting Euclidean dimension can be greatly affected (i.e. distorted by dimensions with larger scale), and consequently, the results of cluster analyses can be very different. In general, it is good practice to transform the dimensions so that they have similar scales [13]."}, {"heading": "3.3 Cosine Distance:", "text": "The cosinal distance between two points is one minus the cosine of the included angle between points (treated as vectors). In the case of a m-times-n data matrix X, which is treated as m (1-times-n) row vectors x1, x2,..., xm, the cosinal distances between the vectors xs and xt are defined as follows [14]: 3.4 Correlation distance: The distance correlation is a measure of the dependence between random vectors [15]. In the case of a m-times-n data matrix X, which is treated as m (1-times-n) row vectors x1, x2,..., xm, the correlation distances between the vectors xs and xt are defined as follows [14]:"}, {"heading": "4. EXPERIMENTS:", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "4.2 Experiment with Wine dataset:", "text": "This year, it has come to the point where you see yourself as being able to live in a country where you are able, where you are able to move, to move, and where you are able to move, to move, to show that you are able, that you are able to unite."}], "references": [{"title": "Data Clustering: A Review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Survey of Clustering Data Mining Techniques", "author": ["P. Berkhin"], "venue": "http://www.accure.com/products/rp_cluster_review.pdf", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Categorization of Several Clustering Algorithms from Different Perspective: A Review", "author": ["Neha Soni", "Amit Ganatra"], "venue": "International Journal of Advanced Research in Computer Science and Software Engineering,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Some Methods for classification and Analysis of Multivariate Observations\",Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1", "author": ["J.B. MacQueen"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1967}, {"title": "Sur la division des corps mat\u00e9riels en parties", "author": ["H. Steinhaus"], "venue": "Bull. Acad. Polon. Sci.(in French)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "Data Mining Model for Higher Education System \u201c,European", "author": ["Shaeela Ayesha", "Tasleem Mustafa", "Ahsan Raza Sattar", "M.Inayat Khan"], "venue": "Journal of Scientific Research, ISSN 1450-216X", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G\u00e1bor J. Sz\u00e9kely", "Maria L. Rizzo", "Nail K. Bakirov"], "venue": "Ann. Statist. Volume 35, Number", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Means to say that clustering should satisfy the two properties [1][2]: 1.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Means to say that clustering should satisfy the two properties [1][2]: 1.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "The hierarchical methods, in general try to decompose the dataset of n objects into a hierarchy of groups [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "decomposition can be represented by a tree structure diagram called as a dendrogram [3]; whose root node represents the whole dataset and each leaf node is a single object of the dataset.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "There are two general approaches for the hierarchical method: agglomerative and divisive [2][3][4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "There are two general approaches for the hierarchical method: agglomerative and divisive [2][3][4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "K \u2013MEANS ALGORITHM: The K-Means [5] is one of the famous partition clustering algorithm [ 6][7][8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "The Formal Algorithm [8] is :", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "4 Correlation Distance: Distance correlation is a measure of dependence between random vectors [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "[1] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Neha Soni, Amit Ganatra, \u201cCategorization of Several Clustering Algorithms from Different Perspective: A Review\u201d, International Journal of Advanced Research in Computer Science and Software Engineering, Volume 2, Issue 8, August 2012, pp 63-68 [4] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] MacQueen, J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Steinhaus, H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "801\u2013804 [7] Lloyd.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "[8] Shaeela Ayesha, Tasleem Mustafa, Ahsan Raza Sattar & M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[15] G\u00e1bor J.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown. KeywordsClustering, K Means, Iris, Wine, Matlab", "creator": "PScript5.dll Version 5.2.2"}}}