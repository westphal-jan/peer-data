{"id": "1405.7624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "Simultaneous Feature and Expert Selection within Mixture of Experts", "abstract": "A useful strategy to deal with complex classification scenarios is the \"divide and conquer\" approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using $L1$ regularization, with a simultaneous expert selection. The experiments are still pending.", "histories": [["v1", "Thu, 29 May 2014 17:32:29 GMT  (87kb,D)", "http://arxiv.org/abs/1405.7624v1", "17 pages, 2 figures"]], "COMMENTS": "17 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["billy peralta"], "accepted": false, "id": "1405.7624"}, "pdf": {"name": "1405.7624.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Feature and Expert Selection within Mixture of Experts", "authors": ["Billy Peraltaa"], "emails": ["bperalta@uct.cl"], "sections": [{"heading": null, "text": "A useful strategy for dealing with complex classification scenarios is the \"divide et conquer\" approach. The mixture of experts (CEE) utilizes this strategy by collectively training a number of classifiers or experts who are specialized in different regions of the entrance space. A global model or gate function complements the experts by learning a function that weights their relevance in different parts of the entrance space. Local feature selection appears to be an attractive alternative to improving the specialization of experts and gate function, especially in the case of high-dimensional data. Our main intuition is that certain subsets of dimensions or subspaces are usually better suited to classifying instances in different regions of the entrance space. Accordingly, this work contributes to a regulated variant of MoE that involves an embedded process of local feature selection using L1 regulation with simultaneous expert selection. Experiments are pending."}, {"heading": "1. Mixture of Experts with embedded variable selection", "text": "Our main idea is to integrate a local feature selection scheme within each expert and gate function of an MoE formulation. Our main intuition is that, in the context of classification, different partitions of input data are best represented by the respective author, Phone: (56 45) 255 3948 E-mail address: bperalta @ uct.cl (Billy Peralta) Form submitted on May 30, 2014ar Xiv: 140 5.76 24v1 [cs.LG] 2 9M ay2 01 Subsets of characteristics. This is particularly relevant in the case of high-dimensional spaces where the frequent presence of noisy or irrelevant characteristics could obscure the recognition of certain class patterns. Specifically, our approach uses the linear nature of each local expert and the gate function in the classical MoE formulation [17], meaning that the L1 regulation can be applied directly. In the following, we briefly discuss the MoE classification for the MoE formulation below."}, {"heading": "1.1. Mixture of Experts", "text": "In the context of supervised classification, there are a number of N training examples, or instance-label pairs (xn, yn), representative of the domain data (x, y) where xn, < D and yn, C. Here, C is a discrete set of responsibilities of the Q class instance {c1,..., cQ}. The goal is to use training data to find a function f that minimizes the quality of f in order to predict the true underlying relationship between x and y. From a probable perspective [4] is a useful approach to find f that uses a conditional formulation: f (x) = arg, ci, Cp (y = ci | x).In the general case of complex relationships between x and y, a useful strategy consists of approximating f through a mixture of local functions. This is similar to the case of modelling a mixture distribution [34] and it leads to conditional probability."}, {"heading": "1.2. Regularized Mixture of Experts (RMoE)", "text": "The idea is to maximize the probability of the data while simultaneously minimizing the number of parameter components that we distinguish from zero. Similarly, this consideration is applied in the work of [29] but in the context of unsupervised learning. The idea is to maximize the probability of the data while simultaneously minimizing the number of parameter components that we distinguish from zero."}, {"heading": "2. Expert Selection", "text": "The MoE o RMoE assumes that all gate functions are applicable to all data. But, for example, in object recognition = = Q = = = = we can assume that there is a group of objects, i.e. groups of vehicles, animals, kitchen utensils, where each group is associated with a gate function. We think that taking into account all groups of objects, the classifiers can confuse. Therefore, we propose to select a subset of gate functions according to the respective data. We call this idea an \"expert selection.\" Considering that the probability is in the regular mix of experts: L = N = 1 K = 1 p (yn) p (yn, mi | xn) p (14). To select a gate, we change the multinomial representation of the gate function (Equation 2) in this way: p (mi xn) = expul in (T i x)."}, {"heading": "A. Optimization of \u00b5 considering norm 0", "text": "Since the parameter \u00b5 depends on data xn, we have to solve the optimization problem: min \u00b5in \u2212 log \u0445 Kk = 1 p (yn | xn, mk) p (mk | xn) Subject: to: | | \u00b5in | | 0 \u2264 \u03bb\u00b5 (24) However, minimizing equation 24 requires the exploration of CK\u03bb\u00b5 combinations, assuming a small number of gates K < 50 and a smaller number of active experts \u03bb\u00b5 < 10, this numerical optimization is feasible in practice."}, {"heading": "B. Optimization of \u00b5 considering norm 1", "text": "A more applicable approach is to loosen the restriction of the 0 standard by using a 1 standard, also known as LASSO regularization. Given that \u00b5 is in the same component of the 0 standard, its solution has many identical steps. In particular, we find almost the same equations. Using the same notations of Equation 19, we have the same log probability for each log probability: None E standard is in = [K norm k = 1 E norm hk-norm hk-norm-norm-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digit-digged-digged-digged-digged-digged-digged-digged-digged-digged-digged-digged-digged-digged-digged-dig"}], "references": [{"title": "Dataset repository in arff", "author": ["J. Aguilar"], "venue": "http://www.upo.es/eps/aguilar/datasets.html", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "http://www.ics.uci.edu/\u223cmlearn/MLRepository.html", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks 5 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C. Bishop"], "venue": "Springer, New York, USA, 2nd edition", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, Cambridge, United Kingdom", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 39 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1977}, {"title": "Pattern Classification", "author": ["R. Duda", "P. Hart", "D. Stork"], "venue": "Wiley-Interscience, USA, second edition", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "View-independent face recognition with hierarchical mixture of experts using global eigenspaces", "author": ["R. Ebrahimpour", "F.M. Jafarlou"], "venue": "Journal of Communication and Computer 7 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "in: Proceedings of the European Conference on Computational Learning Theory, Springer-Verlag, London, UK", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research 3 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Journal of Machine Learning 46 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M. Hall"], "venue": "Ph.D. thesis, University of Waikato", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["J. Hampshire"], "venue": "Waibel, The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition., IEEE Transactions Pattern Analysis and Machine Intelligence 14 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive mixtures of local experts", "author": ["R. Jacobs", "M. Jordan", "S. Nowlan", "G. Hinton"], "venue": "Neural Computation 3 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M. Jordan", "R. Jacobs"], "venue": "Neural Computation 6 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G. John"], "venue": "Artificial Intelligence 97 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Arizona state university: Feature selection datasets", "author": ["H. Liu"], "venue": "http://featureselection.asu.edu/datasets.php", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Chi2: Feature selection and discretization of numeric attributes", "author": ["H. Liu", "R. Setiono"], "venue": "in: J. Vassilopoulos (Ed.), Proceedings of the International Conference on Tools with Artificial Intelligence, IEEE Computer Society, Herndon, Virginia", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Probable networks and plausible predictions \u2013 a review of practical Bayesian methods for supervised neural networks", "author": ["D. MacKay"], "venue": "Network: Computation in Neural Systems 6 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Some Methods for Training Mixtures of Experts", "author": ["P. Moerland"], "venue": "Technical Report, IDIAP Research Institute", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "A system for induction of oblique decision trees", "author": ["S.K. Murthy", "S. Kasif", "S. Salzberg"], "venue": "Journal of Artificial Intelligence Research 2 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "A novel mixture of experts model based on cooperative coevolution", "author": ["M. Nguyen", "H. Abbass", "R. McKay"], "venue": "Neurocomputing 70 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Penalized model-based clustering with application to variable selection", "author": ["W. Pan", "X. Shen"], "venue": "Journal of Machine Learning Research 8 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J. DiCarlo"], "venue": "PLoS Computational Biology 4 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "C4.5: programs for machine learning", "author": ["J. Quinlan"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "Deformable model fitting with a mixture of local experts", "author": ["J. Saragih", "S. Lucey", "J. Cohn"], "venue": "International Conference on Computer Vision ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-dimensional density estimation", "author": ["D. Scott", "S. Sain"], "venue": "Multi-Dimensional Density Estimation, Elsevier, Amsterdam", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B) 58 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1996}, {"title": "A", "author": ["M. Titsias"], "venue": "Likas, Mixture of experts classification using a hierarchical mixture model., Neural Computation 14 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Convergence of block coordinate descent method for nondifferentiable maximization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications 109 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "Information Retrieval", "author": ["C. Van-Rijsbergen"], "venue": "Butterworth-Heinemann, London, UK, 2nd edition", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1979}, {"title": "Variable selection for model-based high dimensional clustering and its application to microarray data", "author": ["S. Wang", "J. Zhu"], "venue": "Biometrics 64 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Winner-take-all mechanisms", "author": ["A. Yuille", "D. Geiger"], "venue": "in: M.A. Arbib (Ed.), The handbook of brain theory and neural networks, MIT Press, Cambridge, MA, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 15, "context": "Specifically, our approach takes advantage of the linear nature of each local expert and gate function in the classical MoE formulation [17], meaning that L1 regularization can be directly applied.", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "From a probabilistic point of view [4], a useful approach to find f is using a conditional formulation:", "startOffset": 35, "endOffset": 38}, {"referenceID": 28, "context": "This is similar to the case of modeling a mixture distribution [34] and it leads to the MoE model.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The traditional MoE technique uses multinomial logit models, also known as softmax functions [4], to represent the gate and expert functions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 34, "context": "The competition in soft-max functions enforces the especialization of experts in different areas of the input space [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "There are several methods to find the value of the hidden parameters \u03bdij and \u03c9lij [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Specifically, the posterior probability of the responsibility Rin assigned by the gate function to expert mi for an instance xn is given by [26]:", "startOffset": 140, "endOffset": 144}, {"referenceID": 21, "context": "Rin = p(mi|xn, yn) (4) = p(yn|xn,mi) p(mi|xn) \u2211K j=1 p(yn|xn,mj) p(mj|xn) Considering these responsibilities and Equation (1), the expected complete loglikelihood \u3008Lc\u3009 used in the EM iterations is [26]:", "startOffset": 197, "endOffset": 201}, {"referenceID": 24, "context": "Similar consideration is used in the work of [29] but in the context of unsupervised learning.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "To maximize Equation (6) with respect to model parameters, we use first the standard fact that the likelihood function in Equation (5) can be decomposed in terms of independent expressions for gate and expert models [26].", "startOffset": 216, "endOffset": 220}, {"referenceID": 21, "context": "In this way, the maximization step of the EM based solution can be performed independently with respect to gate and expert parameters [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 31, "context": "As shown in [20], this problem can be solved by using a coordinate ascent optimization strategy [37] given by a sequential two-step approach that first models the problem as an unregularized logistic regression and afterwards incorporates the regularization constraints.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "Fortunately, as described in [18] and later in [26], Equation (8) can be approximated by using a transformation that implies inverting the soft-max function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Fortunately, as described in [18] and later in [26], Equation (8) can be approximated by using a transformation that implies inverting the soft-max function.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "Using this transformation, Equation (8) is equivalent to an optimization problem that can be solved using a weighted least squares technique [4]:", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "Again deriving Equation (5), in this case with respect to parameters \u03bdij and applying the transformation suggested in [18], we obtain:", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "min \u03bdi \u2211N n=1 ( \u03bd i xn \u2212 logRin )2 (10) (11) Optimization of the regularized likelihood Following the procedure of [20], we add the regularization term to the optimization problem given by Equation (9), obtaining an expression that can be solved using quadratic programming [35]:", "startOffset": 274, "endOffset": 278}, {"referenceID": 4, "context": "subject: to ||\u03bdi||1 \u2264 \u03bb\u03bd (13) A practical advantage of using quadratic programming is that most available optimization packages can be utilized to solve it [6].", "startOffset": 156, "endOffset": 159}], "year": 2014, "abstractText": "A useful strategy to deal with complex classification scenarios is the \u201cdivide and conquer\u201d approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using L1 regularization, with a simultaneous expert selection. The experiments are still pending.", "creator": "LaTeX with hyperref package"}}}