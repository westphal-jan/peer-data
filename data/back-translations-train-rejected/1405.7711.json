{"id": "1405.7711", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language", "abstract": "We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.", "histories": [["v1", "Thu, 16 Jan 2014 04:29:26 GMT  (569kb)", "http://arxiv.org/abs/1405.7711v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david l chen", "joohyun kim", "raymond j mooney"], "accepted": false, "id": "1405.7711"}, "pdf": {"name": "1405.7711.pdf", "metadata": {"source": "CRF", "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language", "authors": ["David L. Chen", "Joohyun Kim", "Raymond J. Mooney"], "emails": ["DLCC@CS.UTEXAS.EDU", "SCIMITAR@CS.UTEXAS.EDU", "MOONEY@CS.UTEXAS.EDU"], "sections": [{"heading": null, "text": "We present a novel framework for learning to interpret and generate language by using perceptual contexts exclusively as supervision. We demonstrate their skills by developing a system that learns to transmit simulated robotic football games in English and Korean without language-specific prior knowledge. In training, only ambiguous supervision is used, consisting of a stream of descriptive text comments and a sequence of events extracted from the simulation track. At the same time, the system establishes similarities between individual comments and the events they describe, and builds a translation model that supports both parsing and generation. We also present a novel algorithm for learning what events are worth describing."}, {"heading": "1. Introduction", "text": "Most current natural language processing (NLP) systems are built with statistical learning algorithms trained on large annotated corpora, but annotating sentences with the required parse trees (Marcus, Santorini, & Marcinkiewicz, 1993), meaning (Ide & Je \u0301 ronis, 1998), and semantic roles (Kingsbury, Palmer, & Marcus, 2002) is a difficult and expensive endeavor. In contrast, children acquire language through exposure to linguistic input in the context of a rich, relevant, perceptual environment. Also, by linking words and phrases to objects and events in the world, the semantics of language are grounded in perceptual experience (Harnad, 1990). Ideally, a machine learning system would be capable of resembling a human learning system in the language."}, {"heading": "2. Background", "text": "Systems for learning semantic parsers induce a function that maps sentences of natural language (NL) to representations of meaning (MRs) in a formal logical language (Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic, Keizer, Mairesse, Thomson, & Young, 2009). Such humanly annotated corpora are expensive and difficult to produce, which limits the usefulness of this approach. Kate and Mooney (2007) have introduced an extension to such a system, KRISP (Kate & Mooney, 2006), so that it can learn from ambiguous training data that require little or no human annotation."}, {"heading": "2.1 KRISP and KRISPER", "text": "In fact, it is as if most of them are able to hide, and that they are able to abide by the rules that they have imposed on themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...)"}, {"heading": "2.2 WASP and WASP\u22121", "text": "The answer to this question is: \"I don't think there is a solution.\" The answer to this question is: \"No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,"}, {"heading": "3. Sportscasting Data", "text": "In fact, it is the case that most people who have lived in the USA in the last ten years are not able to assert themselves in the world, but in the world in which they live, to live and to live. In the world of today, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world of tomorrow, in the world, in the world of tomorrow, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the, in, in the world, in the, in the world, in"}, {"heading": "4. Learning Tactical Generation from Ambiguous Supervision", "text": "We need a system that can handle both ambiguous supervision like KRISPER and language like WASP. Below, we present three systems that can do both. An overview of the differences between the existing systems and the new systems we present can be found in Table 3. All three systems presented here are based on extensions to WASP, our underlying language learning program. The main problem we need to solve is the ambiguity of the training data so that we can train WASP as before to create a speech generator. Each of the new systems uses different disambiguation criteria to determine the best match between the NL sentences and the MRs."}, {"heading": "4.1 WASPER", "text": "The first system is an extension of WASP in the same way that KRISP was extended to generate KRISPER records. It uses an EM-like retraining to process ambiguously commented data, resulting in a system we call WASPER. Generally, any system that learns semantic parsers can be expanded to process ambiguous data, as long as it can generate confidence levels for given NL-MR pairs. Given a set of sets of s and the set of MRs associated with each set of MR (s), we can decode the data by finding pairs (s, m), s, and m (s), so that m = arg maxm Pr (m | s) is used. Although probability is used here, a ranking of relative potential parses would suffice. Pseudocode for WASPER is shown in Algorithm 2: The only difference from the KRISPER parameter is that we are now using a main parameter, a WASP code."}, {"heading": "4.2 KRISPER-WASP", "text": "KRISP has proven to be quite robust in handling noisy training data (Kate & Mooney, 2006), which is important when training with the very noisy training data used to initialize the parser in the first iteration of KRISPER. However, KRISPER cannot learn the speech generator needed for our sports broadcast task. As a result, we are creating a new system called KRISPER-WASP that is both good at disambiguating training data and capable of generating it. First, we use KRISPER to train on the ambiguous data and generate a unique training set by using its prediction for the most likely MR for each set. This unique training set is then used to train WASP to generate both a parser and a generator."}, {"heading": "4.3 WASPER-GEN", "text": "In both KRISPER and WASPER, the criterion for selecting the best NL-MR pairs during retraining is to maximize the likelihood of parsing a sentence into a particular MR. However, since WASPER is capable of both analyzing and generating, we could alternatively select the best NL-MR pairs by evaluating how likely it is to generate the sentence from a particular MR. Therefore, we have built another version of WASPER called WASPER-GEN that makes the training data clear to maximize the performance of the generation rather than analyzing it. Pseudo-code is shown in Algorithm 3. The algorithm is the same as WASPER except for the evaluation function. It uses a generation-based score rather than a parsing-based score to select the best NL-MR sets rather than selecting a BLS pair."}, {"heading": "4.4 Experimental Evaluation", "text": "In fact, it is that most of them are able to follow the rules that they have imposed on themselves, and that they are able to follow the rules that they have imposed on themselves. (...) It is not as if they are following the rules. (...) It is as if they are following the rules. (...) It is as if they are following the rules. (...) It is not as if they are following the rules. (...) It is as if they are following the rules. (...) It is as if they are following the rules. (...) It is not as if they are following the rules. (...) It is not as if they are following the rules. (...)"}, {"heading": "4.4.1 MATCHING NL AND MR", "text": "Since dealing with ambiguous training data is an important aspect of learning grounded languages, we first evaluate how well the different systems select the correct NL-MR pairs. Figure 3 shows the Fmeasure for identifying the right pairings for the different systems. All learning systems are significantly better than random, with an F-measure below 0.5. In both English and Korean data, WASPER-GEN is the best system. WASPER also matches or even surpasses the previous system KRISPER."}, {"heading": "4.4.2 SEMANTIC PARSING", "text": "Next, we present results on the accuracy of the learned semantic parsers. Each trained system is used to analyze and generate an MR for each set in the test set that has a correct MR in the gold standard matching. A parser is only considered correct if it exactly matches the gold standard. Parsing is quite a difficult task because there is usually more than one way to describe the same event. Thus, \"Player1 goes to Player2\" can refer to the same event as \"Player1 kicks the ball to Player2.\" Since we do not provide the systems with prior knowledge, they must learn all the different ways people describe events. Synonyms are not limited to verbs. In our data, \"Pink1,\" \"PinkG\" and \"Pink Goalie\" all refer to Player1 on the pink team. Since we do not provide the systems with prior knowledge, they must learn all these different ways of referring to the same event."}, {"heading": "4.4.3 GENERATION", "text": "The third evaluation task is the generation. All WASP-based systems each receive an MR in the test set, which has a gold standard that matches an NL set and is asked for an NL description, and the quality of the generated set is measured by comparing it to the gold standard by applying BLEU Scoring. This task is more noise tolerant in the training data than the analysis, as the system only has to learn one way to accurately describe an event. This characteristic is reflected in the results shown in Figure 5, where even the basic WASP system performs quite well with random matching, outperforming KRISPER-WASP in both sets and WASPER in the Korean data. Since the number of event types is relatively low, only a relatively small number of correct comparisons is required to perform this task well, as long as each event type is associated with a correct set pattern that more frequently than any other set pattern, which overlaps with another set pattern."}, {"heading": "5. Learning for Strategic Generation", "text": "D eeisrcnlhsrrf\u00fc rrrf\u00fc ide rf\u00fc ide eeisrsrteeVnlrgte\u00fc\u00fceegnln rf\u00fc ide eeisrrrrrrrrrrteeegnln rf\u00fc ide eeisrrrrrteeegnr rf\u00fc ide eeisrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrteeetnlrrf\u00fc rf\u00fc rso \"lso\" lrf\u00fc \"D eros\" rf\u00fc D, nlrf\u00fc D, nlrf\u00fc \"rf\u00fc\" rf\u00fc, rrf\u00fc D, rf\u00fc rrf\u00fc, rrf\u00fc D, rrrf\u00fc D, rf\u00fc rf\u00fc D, rrf\u00fc D, rf\u00fc rrf\u00fc D, rf\u00fc D, rf\u00fc rf\u00fc D, rf\u00fc rf\u00fc, rrrrf\u00fc D, rf\u00fc D, rf\u00fc D, rf\u00fc D, rf\u00fc D, rf\u00fc D, rf\u00fc, rf\u00fc."}, {"heading": "5.1 Experimental Evaluation", "text": "The different Strategic Generation Learning Methods have been evaluated based on how often the events they describe in the test data correspond to the events that humans choose to describe. For the first approach, the results are presented using the inferred matchings generated by KRISPER, WASPER and WASPER-GEN in Figure 7. It is clear from the figure that IGSL exceeds the learning from the inferred matchings and actually occurs at a level similar to the use of gold matching. However, it is important to note that we limit the potential of learning from gold matching by using only the predicates to decide whether we are talking about an event. For the English data, the probabilities learned by IGSL and the inferred matchings of WASPER-GEN are for the five most common events so that they are shown in Table 4."}, {"heading": "6. Using Strategic Generation to Improve Matching", "text": "In this section, we examine how knowledge of strategic generation can be used to improve the accuracy of matching sentences with MRs. In the previous section, we described several ways to learn strategic generation, including IGSL, which learns directly from the ambiguous training data. Knowing what events people tend to talk about should also help resolve ambiguities in the training data. Events that are more likely to be discussed should also be more likely to be matched with an NL set when the training data is unclear. Therefore, this section describes methods that integrate strategic generation results (such as those in Table 4) into the scoring of NL-MR pairs used in the matching process."}, {"heading": "6.1 WASPER-GEN-IGSL", "text": "WASPER-GEN-IGSL is an extension of WASPER-GEN that also uses IGSL strategic generation results. WASPER-GEN uses NIST score to select the best MR value for a set by finding the MR that generates the set cabinet with the actual NL set. WASPER-GEN-IGSL combines tactical (NIST) and strategic (IGSL) generation results to select the best NL-MR pairs. It simply multiplies the NIST score and the IGSL score to a composite score. This new score distorts the selection of matching pairs to include events IGSL a priori more likely to be discussed. This is very helpful, especially at the outset when WASP does not produce a particularly good voice generator. In many cases, the generated sets for all possible MRs are equally bad and do not overlap with BERSER WICT's target set if the WICT can generate a good self-WIGT."}, {"heading": "6.2 Variant of WASPER-GEN Systems", "text": "Although WASPER-GEN uses the NIST score to estimate the quality of NL-MR pairs, it could easily use any MT evaluation metric. We have already discussed the unsuitability of BLEU for comparing short individual sentences since it assigns zero to many pairs. However, the NIST score also has limitations. For example, it is not normalized, which can affect the performance of WASPER-GENIGSL when combined with the IGSL score. Another limitation arises from the use of higher N-grams. Comments in our area are often short, so there are often no superior N-gram matches between generated sentences and targets NL sentences. The METEOR metric (Banerjee & Lavie, 2005) is designed to address various weaknesses in the BLEU- and NISTmetrics of the language: 1 sentence: and NISTLines of the M1 algorithm: focus on the word-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-"}, {"heading": "6.3 Experimental Evaluation", "text": "We evaluated the new systems, WASPER-GEN-IGSL, with both NIST and METEOR evaluated using the methodology of Section 4.4. Matching results are shown in Figure 8, including results for WASPER-GEN, the best system from the previous section. Both WASPER-GEN-IGSL with NIST and METEOR performed significantly better than WASPER-GEN, suggesting that strategic generation information can help decipher data. Using different MT metrics produces a less noticeable effect. There is no clear winner in the English data; however, METEOR appears to improve performance in the Korean data. As mentioned above, the analysis results generally reflect the matching results. Both new systems again exceed WASPER-GEN, the previously best system. Again, the English data do not show a clear advantage in using NIST or MEOTER."}, {"heading": "7. Using a Generative Alignment Model", "text": "Recently, Liang et al. (2009) developed a generative model that can be used to match natural anguage sentences with facts in a corresponding database to which they can refer. As one of their assessment areas, they used our English RoboCup sports casting data. Their method solves the matching problem for our data, but does not address the tasks of semantic analysis or speech generation. However, their generative model elegantly integrates simple strategic and tactical speech generation models to find the overall most likely alignment of sentences and events. They demonstrated improved matching performance on our English data by generating more precise NL-MR pairs than our best system. Thus, we were curious if their results could be used to improve our own systems, which also perform semantic parsing and generation. We also ran their code on our new Korean data, but this resulted in much worse matching results compared to our best systems in comparison."}, {"heading": "8. Removing Superfluous Comments", "text": "During training, all of our methods assume that each NL set corresponds exactly to one of the potential MRs. However, some comments are superfluous in the sense that they do not refer to a currently extracted event that is represented in the list of potential MRs. Of course, they occur in the language, because people do not always talk about the current environment. In our section, athletes often mention past events or more general information about specific teams or players. Furthermore, depending on the application, the selected MRL may not represent all the things that people are talking about. For example, our RoboMRL systems cannot represent information about players who are not actively involved with the ball."}, {"heading": "8.1 Estimating the Superfluous Rate Using Internal Cross Validation", "text": "We propose to use a form of internal (i.e. within the training set) cross-validation to estimate the rate of superfluous comments. Although this algorithm can be used in conjunction with any of our systems, we decided to implement it for KRISPER, which trains much faster than our other systems, making it easier to train many different semantic parsers and select the best one. The basic idea is to use some of the ambiguous training data to estimate the accuracy of a semantic parser, even if we do not know the correct associations. If we assume a reasonable superfluous set rate, we know that most of the time the correct MR is contained in the set of MRs associated with an NL set. Therefore, we assume that a semantic parser that parses an NL set into one of the associated MRs is better than one that parses it into an MR set that is not included in the set."}, {"heading": "8.2 Experiments", "text": "For comparison, we only show results for KRISPER-WASP, as KRISPER is not generationally.The voting results shown in Figure 11 show that removing superfluous sentences usually improves the accuracy of both KRISPER and KRISPER-WASP slightly, although the difference is small in absolute terms. The voting results shown in Figure 12 suggest that removing superfluous sentences usually slightly improves the accuracy of both KRISPER and KRISPER-WASP. As we have often observed, the poll results are consistent with the voting outcomes shown in Figure 13. Finally, the tactical results shown in Figure 13 suggest that removing superfluous sentences slightly improves the accuracy of both KRISPER and KRISPER-WASP. As we have frequently observed, the poll results are consistent with the voting results shown in Figure 13 that removing superfluous comments actually decreases performance somewhat."}, {"heading": "9. Human Subjective Evaluation", "text": "In fact, most people who are able to surpass themselves, to surpass themselves, and to surpass themselves, try to surpass themselves. However, most of them are not able to surpass themselves. Most of them are able to surpass themselves. Most of them are not able to surpass themselves. Most of them are not able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are not able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves. Most of them are able to surpass themselves."}, {"heading": "10. Related Work", "text": "In this section we will discuss some of the related work in the fields of semantic parsing, natural language generation, and learning grounded languages."}, {"heading": "10.1 Semantic Parsing", "text": "Some semantics parser learners additionally require either syntactic annotations (Ge & Mooney, 2005) or previous syntactic knowledge of the target language (Ge & Mooney, 2009; Zettlemoyer & Collins, 2005, 2007).Since the world does not provide direct feedback on the syntactic structure, language learning methods that require syntactic annotations are not directly applicable to learning grounded language. Therefore, methods that learn only from semantic annotations are crucial for learning language from the perceptual context.While we use logical formulas our MRs, the special MRL contains only atomic formulas and can be presented as frames and slots accordingly."}, {"heading": "10.2 Natural Language Generation", "text": "There are several existing systems that host RoboCup games (Andre \u0301 et al., 2000). Given the game states provided by the RoboCup simulator, they extract game events and generate real-time commentary. They consider many practical issues such as topicality, coherence, variability and emotion required to produce good sports broadcasts. However, these systems are handmade and generate language using predefined templates and rules. By contrast, we focus on the learning problem and induce the generation components from ambiguous training data. However, augmenting our system with some of the other components in these systems could improve final sports broadcasts."}, {"heading": "10.3 Grounded Language Learning", "text": "There are a number of rules that, however, cannot easily be adapted to new spatial relationships and interesting movement events from these representations. Guesses, plans, and interactions between the agents are also extracted based on domain-specific knowledge. However, since their system is hand-coded, it cannot easily be adapted to new domains and interesting motion events from these representations. Srihari and Burhan's (1994) used terms that help to identify people and objects; they introduced the idea of visual semantics, a theory of extracting visual information and constraints from accompanying text. For example, the system can determine the spatial relationship between the aforementioned entities, the likely size and shape of the object, and whether the being is natural or artificial."}, {"heading": "11. Future Work", "text": "This year, it has come to the point where it only takes one year to get there, to get to the point where it is able to unfold."}, {"heading": "12. Conclusion", "text": "In addition, we have demonstrated that a system can learn language by simply observing linguistic descriptions of ongoing events. We have also demonstrated the linguistic independence of the system by successfully teaching it to produce sports broadcasts in English and Korean. Dealing with the ambiguous supervision inherent in the training environment is a critical problem when learning language from the perceptual context. We have evaluated various methods of decrypting training data to learn semantic parsers and speech generators. Using a generation evaluation metric as a criterion for selecting the best NL-MR pairs has yielded better results than using semantic parsing values when the initial training data is very loud. Our system also learns a model for strategic generation from the ambiguous training data generated by using competition results to achieve results that are demonstrated with the respective system to improve competition capability."}, {"heading": "Acknowledgments", "text": "We would like to thank Adam Bossy for his work on simulation of perception for the RoboCup games. We would also like to thank Percy Liang for sharing his software and experimental results with us. Finally, we would like to thank the anonymous critics of JAIR and the editor Lillian Lee for their insightful comments, which have helped to improve the final presentation of this work, which was funded by the NSF grant IS- 0712907X. Most of the experiments were carried out on the Mastodon Cluster, which was provided by the NSF grant EIA-0303609."}, {"heading": "Appendix A. Details of the meaning representation language", "text": "Table 10 shows brief explanations of the various events that we have developed with our simulated perception.Below, we find the context-free grammar that we have developed for our meaning representation.All derivatives start with the root symbol * S. * S - > Play Mode (* PLAYMODE) * S - > Ball Stop * S - > Turnover (* PLAYER, * PLAYER) * S - > Kick (* PLAYER) * S - > Kick (* PLAYER, * PLAYER) * S - > Defense (* PLAYER, * PLAYER) * S - > PLAYPLAYPLAYER > PLAYPLAYER > PLAYPLAYER > PLAYPLAYER > PLAYPLAYER - PLAYLAER - PLAYLAYER PLAYER - PLAYER PLAYER * PLAYYYER * PLAYYYER * PLAYYYYYER - PLAYLAYLAYER > PLAYYLAYLAYER > PLAYLAYLAYER - PLAYLAYER - PLAYER - PLAYLAYLAYER - PLAYLAYER * PLAYLAYLAYER * PLAYLAYLAYER * PLAYLAYYYLAYER * PLAYLAYYYYER * PLAYYYYYYYER * PLAYYYYLAYER > PLAYLAYYYYYYLAER * PLAYLAYLAYLAYER * PLAYYYLAYER * PLAYYER * PLAYLAYYLAYYER > PLAYYYYLAYYYER * * PLAYYYLAYER * PLAYLAYER - PLAYER * PLAYLAYLAYER - PLAYER * PLAYLAYYLAYER * PLAYLAYER *"}], "references": [{"title": "The Theory of Parsing, Translation, and Compiling", "author": ["A.V. Aho", "J.D. Ullman"], "venue": null, "citeRegEx": "Aho and Ullman,? \\Q1972\\E", "shortCiteRegEx": "Aho and Ullman", "year": 1972}, {"title": "Three RoboCup simulation league commentator systems", "author": ["E. Andr\u00e9", "K. Binsted", "K. Tanaka-Ishii", "S. Luke", "G. Herzog", "T. Rist"], "venue": "AI Magazine,", "citeRegEx": "Andr\u00e9 et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Andr\u00e9 et al\\.", "year": 2000}, {"title": "Modeling embodied lexical development", "author": ["D. Bailey", "J. Feldman", "S. Narayanan", "G. Lakoff"], "venue": "In Proceedings of the Nineteenth Annual Conference of the Cognitive Science Society", "citeRegEx": "Bailey et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bailey et al\\.", "year": 1997}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization", "author": ["S. Banerjee", "A. Lavie"], "venue": null, "citeRegEx": "Banerjee and Lavie,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Barnard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2003}, {"title": "Collective content selection for concept-to-text generation", "author": ["R. Barzilay", "M. Lapata"], "venue": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-05)", "citeRegEx": "Barzilay and Lapata,? \\Q2005\\E", "shortCiteRegEx": "Barzilay and Lapata", "year": 2005}, {"title": "Bootstrapping lexical choice via multiple-sequence alignment", "author": ["R. Barzilay", "L. Lee"], "venue": "In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-02)", "citeRegEx": "Barzilay and Lee,? \\Q2002\\E", "shortCiteRegEx": "Barzilay and Lee", "year": 2002}, {"title": "Who\u2019s in the picture", "author": ["T.L. Berg", "A.C. Berg", "J. Edwards", "D.A. Forsyth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Berg et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2004}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S. Branavan", "H. Chen", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processin (ACL-IJCNLP", "citeRegEx": "Branavan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "A statistical approach to machine translation", "author": ["P.F. Brown", "J. Cocke", "S.A. Della Pietra", "V.J. Della Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J. Della Pietra", "S.A. Della Pietra", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Subsequence kernels for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bunescu and Mooney,? \\Q2005\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2005}, {"title": "Learning to sportscast: A test of grounded language acquisition", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proceedings of 25th International Conference on Machine Learning (ICML-2008) Helsinki,", "citeRegEx": "Chen and Mooney,? \\Q2008\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2008}, {"title": "Users manual: RoboCup soccer server manual for soccer server version 7.07 and later", "author": ["M. Chen", "E. Foroughi", "F. Heintz", "S. Kapetanakis", "K. Kostiadis", "J. Kummeneje", "I. Noda", "O. Obst", "P. Riley", "T. Steffens", "Y. Wang", "X. Yin"], "venue": "Available at http://sourceforge. net/projects/sserver/", "citeRegEx": "Chen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2003}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chiang,? \\Q2005\\E", "shortCiteRegEx": "Chiang", "year": 2005}, {"title": "New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron", "author": ["M. Collins"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics", "author": ["G. Doddington"], "venue": "In Proceedings of ARPA Workshop on Human Language Technology,", "citeRegEx": "Doddington,? \\Q2002\\E", "shortCiteRegEx": "Doddington", "year": 2002}, {"title": "Statistical acquisition of content selection rules for natural language generation", "author": ["P.A. Duboue", "K.R. McKeown"], "venue": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Duboue and McKeown,? \\Q2003\\E", "shortCiteRegEx": "Duboue and McKeown", "year": 2003}, {"title": "Situated models of meaning for sports video retrieval. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics", "author": ["M. Fleischman", "D. Roy"], "venue": null, "citeRegEx": "Fleischman and Roy,? \\Q2007\\E", "shortCiteRegEx": "Fleischman and Roy", "year": 2007}, {"title": "A statistical semantic parser that integrates syntax and semantics", "author": ["R. Ge", "R.J. Mooney"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning", "citeRegEx": "Ge and Mooney,? \\Q2005\\E", "shortCiteRegEx": "Ge and Mooney", "year": 2005}, {"title": "Learning a compositional semantic parser using an existing syntactic parser", "author": ["R. Ge", "R.J. Mooney"], "venue": "In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processin (ACL-IJCNLP", "citeRegEx": "Ge and Mooney,? \\Q2009\\E", "shortCiteRegEx": "Ge and Mooney", "year": 2009}, {"title": "A robot that uses existing vocabulary to infer non-visual word meanings from observation", "author": ["K. Gold", "B. Scassellati"], "venue": "In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07)", "citeRegEx": "Gold and Scassellati,? \\Q2007\\E", "shortCiteRegEx": "Gold and Scassellati", "year": 2007}, {"title": "Speaking with your sidekick: Understanding situated speech in computer role playing games", "author": ["P. Gorniak", "D. Roy"], "venue": "In Proceedings of the 4th Conference on Artificial Intelligence and Interactive Digital Entertainment", "citeRegEx": "Gorniak and Roy,? \\Q2005\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2005}, {"title": "Using closed captions to train activity recognizers that improve video retrieval", "author": ["S. Gupta", "R. Mooney"], "venue": "In Proceedings of the CVPR-09 Workshop on Visual and Contextual Learning from Annotated Images and Videos (VCL) Miami,", "citeRegEx": "Gupta and Mooney,? \\Q2009\\E", "shortCiteRegEx": "Gupta and Mooney", "year": 2009}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": "Physica D,", "citeRegEx": "Harnad,? \\Q1990\\E", "shortCiteRegEx": "Harnad", "year": 1990}, {"title": "VIsual TRAnslator: Linking perceptions and natural language descriptions", "author": ["G. Herzog", "P. Wazinski"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Herzog and Wazinski,? \\Q1994\\E", "shortCiteRegEx": "Herzog and Wazinski", "year": 1994}, {"title": "Introduction to the special issue on word sense disambiguation: The state of the art", "author": ["N.A. Ide", "J. J\u00e9ronis"], "venue": "Computational Linguistics,", "citeRegEx": "Ide and J\u00e9ronis,? \\Q1998\\E", "shortCiteRegEx": "Ide and J\u00e9ronis", "year": 1998}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "In Proceedings of the Tenth European Conference on Machine Learning", "citeRegEx": "Joachims,? \\Q1998\\E", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Transformationbased learning for semantic parsing", "author": ["J. Jurcicek", "M. Gasic", "S. Keizer", "F. Mairesse", "B. Thomson", "S. Young"], "venue": null, "citeRegEx": "Jurcicek et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jurcicek et al\\.", "year": 2009}, {"title": "Using string-kernels for learning semantic parsers", "author": ["R.J. Kate", "R.J. Mooney"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kate and Mooney,? \\Q2006\\E", "shortCiteRegEx": "Kate and Mooney", "year": 2006}, {"title": "Learning language semantics from ambiguous supervision", "author": ["R.J. Kate", "R.J. Mooney"], "venue": "In Proceedings of the Twenty-Second Conference on Artificial Intelligence", "citeRegEx": "Kate and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Kate and Mooney", "year": 2007}, {"title": "Learning and playing in wubble world", "author": ["W. Kerr", "P.R. Cohen", "Chang", "Y.-H"], "venue": "In Proceedings of the Fourth Artificial Intelligence for Interactive Digital Entertainment Conference", "citeRegEx": "Kerr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kerr et al\\.", "year": 2008}, {"title": "Adding semantic annotation to the Penn treebank", "author": ["P. Kingsbury", "M. Palmer", "M. Marcus"], "venue": "In Proceedings of the Human Language Technology Conference", "citeRegEx": "Kingsbury et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kingsbury et al\\.", "year": 2002}, {"title": "Two-level, many-paths generation", "author": ["K. Knight", "V. Hatzivassiloglou"], "venue": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Knight and Hatzivassiloglou,? \\Q1995\\E", "shortCiteRegEx": "Knight and Hatzivassiloglou", "year": 1995}, {"title": "Interpreting written how-to instructions", "author": ["T. Lau", "C. Drews", "J. Nichols"], "venue": "In Proceedings of the Twenty-first International Joint Conference on Artificial Intelligence (IJCAI-2009)", "citeRegEx": "Lau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2009}, {"title": "Inductive Logic Programming: Techniques and Applications", "author": ["N. Lavrac", "S. Dz\u0306eroski"], "venue": null, "citeRegEx": "Lavrac\u0306 and Dz\u0306eroski,? \\Q1994\\E", "shortCiteRegEx": "Lavrac\u0306 and Dz\u0306eroski", "year": 1994}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processin (ACL-IJCNLP", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["W. Lu", "H.T. Ng", "W.S. Lee", "L.S. Zettlemoyer"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Lu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2008}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["M. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Discourse strategies for generating natural-language text", "author": ["K.R. McKeown"], "venue": "Artificial Intelligence,", "citeRegEx": "McKeown,? \\Q1985\\E", "shortCiteRegEx": "McKeown", "year": 1985}, {"title": "Spoken language understanding in dialogue systems, using a 2-layer Markov logic network: improving semantic accuracy", "author": ["I.V. Meza-Ruiz", "S. Riedel", "O. Lemon"], "venue": "Proceedings of Londial", "citeRegEx": "Meza.Ruiz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meza.Ruiz et al\\.", "year": 2008}, {"title": "Learning for semantic parsing", "author": ["R.J. Mooney"], "venue": "Computational Linguistics and Intelligent Text Processing: Proceedings of the 8th International Conference,", "citeRegEx": "Mooney,? \\Q2007\\E", "shortCiteRegEx": "Mooney", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training", "author": ["S. Riezler", "D. Prescher", "J. Kuhn", "M. Johnson"], "venue": "In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Riezler et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2000}, {"title": "Learning visually grounded words and syntax for a scene description task", "author": ["D. Roy"], "venue": "Computer Speech and Language,", "citeRegEx": "Roy,? \\Q2002\\E", "shortCiteRegEx": "Roy", "year": 2002}, {"title": "Analyzing multiagent activity logs using process mining techniques", "author": ["A. Rozinat", "S. Zickler", "M. Veloso", "W. van der Aalst", "C. McMillen"], "venue": "In Proceedings of the 9th International Symposium on Distributed Autonomous Robotic Systems", "citeRegEx": "Rozinat et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rozinat et al\\.", "year": 2008}, {"title": "Name-it: Naming and detecting faces in video by the integration of image and natural language processing", "author": ["S. Satoh", "Y. Nakamura", "T. Kanade"], "venue": "In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97)", "citeRegEx": "Satoh et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Satoh et al\\.", "year": 1997}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "A uniform architecture for parsing and generation", "author": ["S.M. Shieber"], "venue": "In Proceedings of the 12th International Conference on Computational Linguistics", "citeRegEx": "Shieber,? \\Q1988\\E", "shortCiteRegEx": "Shieber", "year": 1988}, {"title": "A computational study of cross-situational techniques for learning word-tomeaning", "author": ["J.M. Siskind"], "venue": "mappings. Cognition,", "citeRegEx": "Siskind,? \\Q1996\\E", "shortCiteRegEx": "Siskind", "year": 1996}, {"title": "Database-text alignment via structured multilabel classification", "author": ["B. Snyder", "R. Barzilay"], "venue": "In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-2007)", "citeRegEx": "Snyder and Barzilay,? \\Q2007\\E", "shortCiteRegEx": "Snyder and Barzilay", "year": 2007}, {"title": "Visual semantics: Extracting visual information from text accompanying pictures", "author": ["R.K. Srihari", "D.T. Burhans"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94)", "citeRegEx": "Srihari and Burhans,? \\Q1994\\E", "shortCiteRegEx": "Srihari and Burhans", "year": 1994}, {"title": "An efficient probabilistic context-free parsing algorithm that computes prefix probabilities", "author": ["A. Stolcke"], "venue": "Computational Linguistics,", "citeRegEx": "Stolcke,? \\Q1995\\E", "shortCiteRegEx": "Stolcke", "year": 1995}, {"title": "Language, Thought, and Reality: Selected Writings", "author": ["B.L. Whorf"], "venue": null, "citeRegEx": "Whorf,? \\Q1964\\E", "shortCiteRegEx": "Whorf", "year": 1964}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Y. Wong", "R.J. Mooney"], "venue": "In Proceedings of Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics Annual Meeting", "citeRegEx": "Wong and Mooney,? \\Q2006\\E", "shortCiteRegEx": "Wong and Mooney", "year": 2006}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y. Wong", "R.J. Mooney"], "venue": "In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Wong and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney", "year": 2007}, {"title": "A syntax-based statistical translation model", "author": ["K. Yamada", "K. Knight"], "venue": "In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yamada and Knight,? \\Q2001\\E", "shortCiteRegEx": "Yamada and Knight", "year": 2001}, {"title": "On the integration of grounding language and learning objects", "author": ["C. Yu", "D.H. Ballard"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence", "citeRegEx": "Yu and Ballard,? \\Q2004\\E", "shortCiteRegEx": "Yu and Ballard", "year": 2004}, {"title": "Learning what to talk about in descriptive games", "author": ["H. Zaragoza", "Li", "C.-H"], "venue": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zaragoza et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zaragoza et al\\.", "year": 2005}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Proceedings of 21st Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Zettlemoyer and Collins,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2005}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Zettlemoyer and Collins,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2007}], "referenceMentions": [{"referenceID": 25, "context": "Also, by connecting words and phrases to objects and events in the world, the semantics of language is grounded in perceptual experience (Harnad, 1990).", "startOffset": 137, "endOffset": 151}, {"referenceID": 47, "context": "While there has been a fair amount of research on \u201cgrounded language learning\u201d (Roy, 2002; Bailey, Feldman, Narayanan, & Lakoff, 1997; Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Yu & Ballard, 2004; Gold & Scassellati, 2007), most of the focus has been on dealing with raw perceptual data rather than language issues.", "startOffset": 79, "endOffset": 241}, {"referenceID": 1, "context": "By exploiting existing techniques for abstracting a symbolic description of the activity on the field from the detailed states of the physical simulator (Andr\u00e9 et al., 2000), we obtain a pairing of natural language with a symbolic description of the perceptual context in which it was uttered.", "startOffset": 153, "endOffset": 173}, {"referenceID": 43, "context": "Existing work has focused on learning from a supervised corpus in which each sentence is manually annotated with its correct MR (Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic, Keizer, Mairesse, Thomson, & Young, 2009).", "startOffset": 128, "endOffset": 264}, {"referenceID": 15, "context": "Existing work has focused on learning from a supervised corpus in which each sentence is manually annotated with its correct MR (Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic, Keizer, Mairesse, Thomson, & Young, 2009). Such human annotated corpora are expensive and difficult to produce, limiting the utility of this approach. Kate and Mooney (2007) introduced an extension to one such system, KRISP (Kate & Mooney, 2006), so that it can learn from ambiguous training data that requires little or no human annotation effort.", "startOffset": 157, "endOffset": 397}, {"referenceID": 28, "context": "SVMs are state-ofthe-art machine learning methods that learn maximum-margin separators to prevent over-fitting in very high-dimensional data such as natural language text (Joachims, 1998).", "startOffset": 171, "endOffset": 187}, {"referenceID": 15, "context": "Recently, kernels over strings and trees have been effectively applied to a variety of problems in text learning and NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko, Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005).", "startOffset": 121, "endOffset": 257}, {"referenceID": 15, "context": "Recently, kernels over strings and trees have been effectively applied to a variety of problems in text learning and NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko, Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005). In particular, KRISP uses the string kernel introduced by Lodhi et al. (2002) to classify substrings in an NL sentence.", "startOffset": 219, "endOffset": 337}, {"referenceID": 14, "context": "WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-art statistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) to learn semantic parsers.", "startOffset": 138, "endOffset": 261}, {"referenceID": 14, "context": "WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-art statistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) to learn semantic parsers. SMTmethods learn effective machine translators by training on parallel corpora consisting of human translations of documents into one or more alternative natural languages. The resulting translators are typically significantly more effective than manually developed systems and SMT has become the dominant approach to machine translation. Wong and Mooney (2006) adapted such methods to learn to translate from NL to MRL rather than from one NL to another.", "startOffset": 248, "endOffset": 651}, {"referenceID": 14, "context": "Complete MRs are then formed by combining these NL substrings and their translations using a grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), which forms the basis of most existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005).", "startOffset": 223, "endOffset": 260}, {"referenceID": 55, "context": "To translate a novel NL sentence into its MR, a probabilistic chart parser (Stolcke, 1995) is used to find the most probable synchronous derivation that generates the given NL, and the corresponding MR generated by this derivation is returned.", "startOffset": 75, "endOffset": 90}, {"referenceID": 51, "context": "This allows the same learned grammar to be used for both parsing and generation, an elegant property that has important advantages (Shieber, 1988).", "startOffset": 131, "endOffset": 146}, {"referenceID": 9, "context": "The generation system, WASP\u22121, uses a noisy-channel model (Brown et al., 1990): arg max", "startOffset": 58, "endOffset": 78}, {"referenceID": 12, "context": "Complete MRs are then formed by combining these NL substrings and their translations using a grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), which forms the basis of most existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). In an SCFG, the right hand side of each production rule contains two strings, in our case one in NL and the other in MRL. Derivations of the SCFG simultaneously produce NL sentences and their correspondingMRs. The bilingual lexicon acquired from word alignments over the training data is used to construct a set of SCFG production rules. A probabilistic parser is then produced by training a maximum-entropy model using EM to learn parameters for each of these SCFG productions, similar to the methods used by Riezler, Prescher, Kuhn, and Johnson (2000), and Zettlemoyer and Collins (2005).", "startOffset": 247, "endOffset": 816}, {"referenceID": 12, "context": "Complete MRs are then formed by combining these NL substrings and their translations using a grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), which forms the basis of most existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). In an SCFG, the right hand side of each production rule contains two strings, in our case one in NL and the other in MRL. Derivations of the SCFG simultaneously produce NL sentences and their correspondingMRs. The bilingual lexicon acquired from word alignments over the training data is used to construct a set of SCFG production rules. A probabilistic parser is then produced by training a maximum-entropy model using EM to learn parameters for each of these SCFG productions, similar to the methods used by Riezler, Prescher, Kuhn, and Johnson (2000), and Zettlemoyer and Collins (2005). To translate a novel NL sentence into its MR, a probabilistic chart parser (Stolcke, 1995) is used to find the most probable synchronous derivation that generates the given NL, and the corresponding MR generated by this derivation is returned.", "startOffset": 247, "endOffset": 852}, {"referenceID": 1, "context": "Based on the ROCCO RoboCup commentator\u2019s incremental event recognition module (Andr\u00e9 et al., 2000) we manually developed symbolic representations of game events and a rule-based system to automatically extract them from the simulator traces.", "startOffset": 78, "endOffset": 98}, {"referenceID": 17, "context": "NIST measures the precision of a translation in terms of the proportion of n-grams it shares with a human translation (Doddington, 2002).", "startOffset": 118, "endOffset": 136}, {"referenceID": 41, "context": "In addition to tactical generation which is deciding how to to say something, a sportscaster must also preform strategic generation which is choosing what to say (McKeown, 1985).", "startOffset": 162, "endOffset": 177}, {"referenceID": 37, "context": "Recently, Liang et al. (2009) developed a generative model that can be used to match naturallanguage sentences to facts in a corresponding database to which they may refer.", "startOffset": 10, "endOffset": 30}, {"referenceID": 37, "context": "English dataset Korean dataset Algorithm No initialization Initialized No initialization Initialized Liang et al. (2009) 75.", "startOffset": 101, "endOffset": 121}, {"referenceID": 37, "context": "Systems run with initialization are initialized with the matchings produced by Liang et al.\u2019s (2009) system.", "startOffset": 79, "endOffset": 101}, {"referenceID": 37, "context": "Systems run with initialization are initialized with the matchings produced by Liang et al.\u2019s (2009) system.", "startOffset": 79, "endOffset": 101}, {"referenceID": 37, "context": "Systems run with initialization are initialized with the matchings produced by Liang et al.\u2019s (2009) system.", "startOffset": 79, "endOffset": 101}, {"referenceID": 12, "context": "Picking from the top 5 generated sentences also added variability to the machine-generated sportscasts that improved the results compared with earlier experiments presented by Chen and Mooney (2008). However, the machine still sometimes misses significant plays such as scoring or corner kicks.", "startOffset": 176, "endOffset": 199}, {"referenceID": 29, "context": "There are systems that use transformation-based learning (Jurcicek et al., 2009), or Markov logic (Meza-Ruiz, Riedel, & Lemon, 2008) to learn semantic parsers using frames and slots.", "startOffset": 57, "endOffset": 80}, {"referenceID": 1, "context": "There are several existing systems that sportscast RoboCup games (Andr\u00e9 et al., 2000).", "startOffset": 65, "endOffset": 85}, {"referenceID": 1, "context": "There are several existing systems that sportscast RoboCup games (Andr\u00e9 et al., 2000). Given game states provided by the RoboCup simulator, they extract game events and generate real-time commentaries. They consider many practical issues such as timeliness, coherence, variability, and emotion that are needed to produce good sportscasts. However, these systems are hand-built and generate language using pre-determined templates and rules. In contrast, we concentrate on the learning problem and induce the generation components from ambiguous training data. Nevertheless, augmenting our system with some of the other components in these systems could improve the final sportscasts produced. There is also prior work on learning a lexicon of elementary semantic expressions and their corresponding natural language realizations (Barzilay & Lee, 2002). This work uses multiple-sequence alignment on datasets that supply several verbalizations of the corresponding semantics to extract a dictionary. Duboue andMcKeown (2003) were the first to propose an algorithm for learning strategic generation automatically from data.", "startOffset": 66, "endOffset": 1024}, {"referenceID": 2, "context": "Several robotics and computer vision researchers have worked on inferring grounded meanings of individual words or short referring expressions from visual perceptual context (e.g., Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004).", "startOffset": 174, "endOffset": 254}, {"referenceID": 4, "context": "Several robotics and computer vision researchers have worked on inferring grounded meanings of individual words or short referring expressions from visual perceptual context (e.g., Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004).", "startOffset": 174, "endOffset": 254}, {"referenceID": 46, "context": "Srihari and Burhans (1994) used captions accompanying photos to help identify people and objects.", "startOffset": 0, "endOffset": 27}, {"referenceID": 45, "context": "Siskind (1996) performed some of the earliest work on learning grounded word meanings.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": ", Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, the complexity of the natural language used in this existing work is very restrictive, many of the systems use pre-coded knowledge of the language, and almost all use static images to learn language describing objects and their relations, and cannot learn language describing actions. The most sophisticated grammatical formalism used to learn syntax in this work is a finite-state hidden-Markov model. By contrast, our work exploits the latest techniques in statistical context-free grammars and syntax-based statistical machine translation that handle more of the complexities of natural language. More recently, Gold and Scassellati (2007) built a system called TWIG that uses existing language knowledge to help it learn the meaning of new words.", "startOffset": 13, "endOffset": 729}, {"referenceID": 2, "context": ", Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, the complexity of the natural language used in this existing work is very restrictive, many of the systems use pre-coded knowledge of the language, and almost all use static images to learn language describing objects and their relations, and cannot learn language describing actions. The most sophisticated grammatical formalism used to learn syntax in this work is a finite-state hidden-Markov model. By contrast, our work exploits the latest techniques in statistical context-free grammars and syntax-based statistical machine translation that handle more of the complexities of natural language. More recently, Gold and Scassellati (2007) built a system called TWIG that uses existing language knowledge to help it learn the meaning of new words. The robot uses partial parses to focus its attention on possible meanings of new words. By playing a game of catch, the robot was able to learn the meaning of \u201cyou\u201d and \u201cme\u201d as well as \u201cam\u201d and \u201care\u201d as identity relations. There has also been a variety of work on learning from captions that accompany pictures or videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). This area is of particular interest given the large amount of captioned images and video available on the web and television. Satoh et al. (1997) built a system to detect faces in newscasts.", "startOffset": 13, "endOffset": 1377}, {"referenceID": 2, "context": ", Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, the complexity of the natural language used in this existing work is very restrictive, many of the systems use pre-coded knowledge of the language, and almost all use static images to learn language describing objects and their relations, and cannot learn language describing actions. The most sophisticated grammatical formalism used to learn syntax in this work is a finite-state hidden-Markov model. By contrast, our work exploits the latest techniques in statistical context-free grammars and syntax-based statistical machine translation that handle more of the complexities of natural language. More recently, Gold and Scassellati (2007) built a system called TWIG that uses existing language knowledge to help it learn the meaning of new words. The robot uses partial parses to focus its attention on possible meanings of new words. By playing a game of catch, the robot was able to learn the meaning of \u201cyou\u201d and \u201cme\u201d as well as \u201cam\u201d and \u201care\u201d as identity relations. There has also been a variety of work on learning from captions that accompany pictures or videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). This area is of particular interest given the large amount of captioned images and video available on the web and television. Satoh et al. (1997) built a system to detect faces in newscasts. However, they use fairly simple manually-written rules to determine the entity in the picture to which the language refers. Berg et al. (2004) used a more elaborate learning method to cluster faces with names.", "startOffset": 13, "endOffset": 1565}, {"referenceID": 2, "context": ", Roy, 2002; Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, the complexity of the natural language used in this existing work is very restrictive, many of the systems use pre-coded knowledge of the language, and almost all use static images to learn language describing objects and their relations, and cannot learn language describing actions. The most sophisticated grammatical formalism used to learn syntax in this work is a finite-state hidden-Markov model. By contrast, our work exploits the latest techniques in statistical context-free grammars and syntax-based statistical machine translation that handle more of the complexities of natural language. More recently, Gold and Scassellati (2007) built a system called TWIG that uses existing language knowledge to help it learn the meaning of new words. The robot uses partial parses to focus its attention on possible meanings of new words. By playing a game of catch, the robot was able to learn the meaning of \u201cyou\u201d and \u201cme\u201d as well as \u201cam\u201d and \u201care\u201d as identity relations. There has also been a variety of work on learning from captions that accompany pictures or videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). This area is of particular interest given the large amount of captioned images and video available on the web and television. Satoh et al. (1997) built a system to detect faces in newscasts. However, they use fairly simple manually-written rules to determine the entity in the picture to which the language refers. Berg et al. (2004) used a more elaborate learning method to cluster faces with names. Using the data, they estimate the likelihood of an entity appearing in a picture given its context. Some recent work on video retrieval has focused on learning to recognize events in sports videos and connecting them to English words appearing in accompanying closed captions (Fleischman & Roy, 2007; Gupta & Mooney, 2009). However, this work only learns the connection between individual words and video events and does not learn to describe events using full grammatical sentences. To avoid difficult problems in computer vision, our work uses a simulated world where perception of complex events and their participants is much simpler. In addition to observing events passively, there has also been work on grounded language learning in more interactive environments such as in computer video games (Gorniak & Roy, 2005). In this work, players cooperate and communicate with each in order to accomplish a certain task. The system learns to map spoken instructions to specific actions; however, it relies on existing statistical parsers and does not learn the syntax and semantics of the language from the perceptual environment alone. Kerr, Cohen, and Chang (2008) developed a system that learns grounded word-meanings for nouns, adjectives, and spatial prepositions while a human is instructing it to perform tasks in a vir-", "startOffset": 13, "endOffset": 2800}, {"referenceID": 8, "context": "Recently, there has been some interest in learning how to interpret English instructions describing how to use a particular website or perform other computer tasks (Branavan et al., 2009; Lau, Drews, & Nichols, 2009).", "startOffset": 164, "endOffset": 216}, {"referenceID": 37, "context": "Some recent projects attempt to align text from English summaries of American football games with database records that contain statistics and events about the game (Snyder & Barzilay, 2007; Liang et al., 2009).", "startOffset": 165, "endOffset": 210}, {"referenceID": 8, "context": "Recently, there has been some interest in learning how to interpret English instructions describing how to use a particular website or perform other computer tasks (Branavan et al., 2009; Lau, Drews, & Nichols, 2009). These systems learn to predict the correct computer action (pressing a button, choosing a menu item, typing into a text field, etc.) corresponding to each step in the instructions. Instead of using parallel training data from the perceptual context, these systems utilize direct matches between words in the natural language instructions and English words explicitly occurring in the menu items and computer instructions in order to establish a connection between the language and the environment. One of the core subproblems our work addresses is matching sentences to facts in the world to which they refer. Some recent projects attempt to align text from English summaries of American football games with database records that contain statistics and events about the game (Snyder & Barzilay, 2007; Liang et al., 2009). However, Snyder and Barzilay (2007) use a supervised approach that requires annotating the correct correspondences between the text and the semantic representations.", "startOffset": 165, "endOffset": 1076}, {"referenceID": 8, "context": "Recently, there has been some interest in learning how to interpret English instructions describing how to use a particular website or perform other computer tasks (Branavan et al., 2009; Lau, Drews, & Nichols, 2009). These systems learn to predict the correct computer action (pressing a button, choosing a menu item, typing into a text field, etc.) corresponding to each step in the instructions. Instead of using parallel training data from the perceptual context, these systems utilize direct matches between words in the natural language instructions and English words explicitly occurring in the menu items and computer instructions in order to establish a connection between the language and the environment. One of the core subproblems our work addresses is matching sentences to facts in the world to which they refer. Some recent projects attempt to align text from English summaries of American football games with database records that contain statistics and events about the game (Snyder & Barzilay, 2007; Liang et al., 2009). However, Snyder and Barzilay (2007) use a supervised approach that requires annotating the correct correspondences between the text and the semantic representations. On the other hand, Liang et al. (2009) have developed an unsupervised approach using a generative model to solve the alignment problem.", "startOffset": 165, "endOffset": 1245}, {"referenceID": 56, "context": "However, it would be interesting to include a more \u201cWhorfian\u201d style of language learning (Whorf, 1964) in which an unknown word such as \u201csloppiness\u201d could actually cause the creation of a new concept.", "startOffset": 89, "endOffset": 102}, {"referenceID": 37, "context": "Initializing our systemwith the output from Liang et al. (2009), which uses a generative model that includes both strategic and tactical components, produced somewhat better results.", "startOffset": 44, "endOffset": 64}, {"referenceID": 37, "context": "Initializing our systemwith the output from Liang et al. (2009), which uses a generative model that includes both strategic and tactical components, produced somewhat better results. However, the interaction between all these components is very loose and a tighter integration of the different pieces could yield stronger results in all the tasks. An obvious extension to the current work is to apply it to real RoboCup games rather than simulated ones. Recent work by Rozinat, Zickler, Veloso, van der Aalst, and McMillen (2008) analyzes games in the RoboCup Small Size League using video from the overhead camera.", "startOffset": 44, "endOffset": 530}], "year": 2010, "abstractText": "We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.", "creator": "gnuplot 4.2 patchlevel 3 "}}}