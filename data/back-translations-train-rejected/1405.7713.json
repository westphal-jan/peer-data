{"id": "1405.7713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Using Local Alignments for Relation Recognition", "abstract": "This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results.", "histories": [["v1", "Thu, 16 Jan 2014 04:51:47 GMT  (727kb)", "http://arxiv.org/abs/1405.7713v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["sophia katrenko", "pieter adriaans", "maarten van someren"], "accepted": false, "id": "1405.7713"}, "pdf": {"name": "1405.7713.pdf", "metadata": {"source": "CRF", "title": "Using Local Alignments for Relation Recognition", "authors": ["Sophia Katrenko", "Pieter Adriaans", "Maarten van Someren"], "emails": ["S.Katrenko@uva.nl", "P.W.Adriaans@uva.nl", "M.W.vanSomeren@uva.nl"], "sections": [{"heading": "1. Introduction", "text": "Once concepts and semantic relationships are identified, they can be used for a variety of applications, such as answering questions (QA), ontology construction, generating hypotheses, and others. In ontology construction, the relationship that is most studied is used in two ways: as a relationship (or hypernymy) that organizes concepts into a taxonomy (Snow, Jurafsky, & Ng, 2006). In the information tribune, semantic relationships are used in relation to refracted questions before the results are reproduced by a search engine (e.g. identifying whether a fragment of text contains a particular relationship or not). The most common relationships to query expansion are hypernymy (or broader terms from a thesaurus) and synonyms."}, {"heading": "2. Kernel Methods", "text": "Recent years have seen an increase in interest in the core methods, their theoretical analysis and practical application in various areas (Burges, 1998; Shawe-Taylor & Christianini, 2000).The idea of having a method that works with different structures and representations, starting from the simplest representation, which uses a limited number of attributes on complex structures such as trees, actually seems very attractive.Before defining a core function, we recall the default settings for the superior classification."}, {"heading": "2.1 The Spectrum Kernel", "text": "Leslie, Eskin, and Noble (2002) proposed a discriminatory approach to protein classification. For each sequence x-X, the authors define the m-spectrum as a set S of all related sub-sequences of x, whose length is equal to m. All possible m-long sub-sequences q-S are indexed by the frequency of their occurrence (\u03c6q (x)). Consequently, a characteristic map for a sequence x and the alphabet A is equal to \u03a6m (x) = (\u03c6q (x)) q-Am. The spectral nucleus for two sequences x and y is defined as an inner product between the corresponding characteristic maps: kS (x, y) = < \u03a6m (x), \u03a6m (y) >. Even assuming contiguous sub-sequences for small m, the characteristic core under consideration is very large for two sequences x and y. The authors suggest that all sub-sequences of m are guaranteed by the use of a suffix tree-fast sub-sequence (the y)."}, {"heading": "2.2 Mismatch Kernels", "text": "The incongruence nucleus, later introduced by Leslie et al. (2004), is essentially an extension of the latter. An obvious limitation of the kernel spectrum is that all the subsequences considered are coherent and should exactly match. In the incongruence nucleus, the contiguity is maintained while the match criterion is changed. In other words, instead of looking for all possible subsequences of length m for a given sequence, one looks for all possible subsequences of length m that allow up to r incongruences. such a comparison leads to a larger subset of sequences, but the nuclei defined in this way can still be calculated fairly quickly. The kernel is formulated similarly to the spectrum nucleus and the only big difference is the calculation of the characteristic map for all sequences. Specifically, a characteristic map for a sequence x is defined as \u03a6m, r (x) = q (q), where the sequence is a missing sequence."}, {"heading": "2.3 Kernel Methods and NLP", "text": "One of the advantages of the core methods is the ability to design cores for different structures such as strings or trees. In the NLP field (and especially in terms of extraction), most of the work falls roughly into two categories. In the first, cores are defined by pure text using word strings; in the second, linguistic structures such as dependency paths or trees or the output of shallow parsing are used. In this short review, we do not take a chronological perspective, but start with methods based on sequences and proceed with approaches that use syntactical information. In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence cores that provide flexible means of working with text data. In particular, sub-sequences are not necessarily coherent and are weighted according to their length (using a decay factor)."}, {"heading": "2.3.1 Subsequence Kernels", "text": "This approach was followed by Bunescu and MooneyUsing Local Alignments for Relation Recognition (2005b), whose choice of sequences was motivated by textual patterns in corpora. For example, they observed that some relationships are expressed by \"subject-verb-object\" constructions, while others are part of the noun and prepositional phrases. As a result, three sequence types were considered: before (words before and between two named entities), between (words only between two entities) and after (words between and after two entities). The length of sequences is limited. To deal with data economy, the authors generalize existing sequences using PoS tags, entity types, and WordNet synsets. A generalized subsequence core is recursively defined as the number of weighted sparse subsequences that two sequences are made to share this information."}, {"heading": "2.3.2 Distributional Kernels", "text": "Recently, O'Se \u0301 aghdha and Copestake (2008) introduced distribution cores based on probability distributions of co-occurrence, and the co-occurrence statistics they use are either in the form of syntactic relationships or n-grams, showing that it is possible to derive nuclei from distances such as Jensen-Shannon divergence (JSD) or Euclidean distance (L2) (Lee, 1999). JSD is a smoothed version of the Kullback-Leibler divergence, an information-theoretical measure of divergence between two probability distributions. The main motivation behind this approach is the fact that distribution similarities have proven useful for NLP tasks. To extract information about the co-occurrence, the authors use two corpora, the British National Corpus (BNC) and the Web 1T 5-Gram Corpus (Corpus 5-Gram)."}, {"heading": "2.3.3 Kernels for Syntactic Structures", "text": "To achieve this goal, the authors rely on the sub-trees that a pair of trees have in common. Later, Moschitti (2006) explored folding cores on dependency and constituency structures to perform semantic role labels and question classifications. This work introduces a novel kernel, called sub-tree core (PT), which essentially builds on two previously suggested cores, the sub-tree core (ST), which contains all subsequent nodes from a target root (including leaves), and the sub-tree core (SST), which is more flexible and allows internal sub-trees that do not necessarily contain leaves. A sub-tree structure is a generalization of a tree structure that has better results than sub-tree structures, with sub-tree structures (SVP) performing better."}, {"heading": "2.3.4 Kernel on Shallow Parsing Output", "text": "Zelenko et al. (2003) use shallow parsing and designed cores to extract relationships from text. In contrast to full parsing, shallow parsing partially generates interpretations of sentences. Each node in such a tree is enriched with information about roles (corresponding to the arguments of a relationship). The similarity of two trees is determined by the similarity of their nodes. Depending on how the similarity is calculated, Zelenko et al. define two types of cores, contiguous subtree cores, and sparse cores. Both types have been tested on two types of relationships, \"personal affiliation\" and \"organizational location,\" which perform well. In particular, sparse cores perform better than contiguous subtree cores, leading to the conclusion that partial matching is important when it comes to typical sparse natural language data. However, calculating the sparse kernel O (mn3) time (where Berner and the number of children are taking into account) takes into account the number of kernel while taking two children into account."}, {"heading": "2.3.5 Shortest Path Kernel", "text": "Bunescu and Mooney's (2005a) shortest path kernel represents yet another approach for relationship extraction that is kernel-based and relies on information found in dependency trees. A main assumption here is that not the entire dependency structure is relevant, and one can focus on the path that connects two relation arguments instead. The more similar these paths are, the more likely are two examples of relations belonging to the same category. In the spirit of their previous work, Bunescu and Mooney seek generalizations about existing paths by adding information sources such as part of the speech (PoS) categories or designated entity types. The shortest path between relation arguments is extracted, and a kernel between two sequences (paths) x = {x1,."}, {"heading": "3. A Local Alignment Kernel", "text": "From our brief overview of the nuclei designed for NLP, it can be inferred that many researchers use sub-structures and suggest variants such as subsequence nuclei (Bunescu & Mooney, 2005b), a sub-tree nucleus (Moschitti, 2006) or a kernel for superficial analysis results (Zelenko et al., 2003) for relation extraction. In this paper, we focus on dependency paths as input and formulate the following requirements for a core function: \u2022 It should allow partial agreement so that similarity can be measured for paths of varying lengths \u2022 It should be possible to include prior knowledge Recall that we mean prior knowledge coming either from larger companies or from existing resources such as ontologies."}, {"heading": "3.1 Smith-Waterman Measure and Local Alignments", "text": "It has been shown that it is possible to design valid cores based on a similarity measurement for strings (Saigo, Vert, & Akutsu, 2006) (2004), the Smith-Waterman (SW) similarity measurement (Smith & Waterman, 1981) (see below) to measure the similarity between two sequences of amino acids. String distance measurements can be divided into measurements based on terms, Edit Distance and Hidden Markov Models (HMM) (Cohen, Ravikumar, & Fienberg, 2003). Term-based measurements such as measures based on the TF-IDF score, consider a pair of word sequences as two sets of words that ignore their sequence."}, {"heading": "3.1.1 Computational complexity", "text": "The LA kernel, like many other cores discussed in Section 2, can be efficiently calculated using dynamic programming, and for any two sequences of length n and m, its complexity is proportional to n \u00b7 m. Additional costs may arise from the substitution matrix, which, unlike the biomedical domain, can be very large. However, looking up the substitution values can also be done efficiently, resulting in a fast core computation. For example, calculating a kernel matrix for the largest AImed (3,763 instances) dataset used in this paper takes 805 seconds on a 2.93 GHz Intel (R) Core (TM) 2 calculator."}, {"heading": "3.2 Designing a Local Alignment Kernel for Relation Extraction", "text": "The Smith-Waterman measurement is based on transformations, in particular the deletion of elements that differ between strings, but elements that differ can be similar to some degree. These similarities can be used as part of the similarity measurement. For example, if two elements are words that are different but are synonyms, then we count them as less different than if they are completely independent of each other. We will call them Katrenko, Adriaans, & van Somerensibilities \"substitution results\" (Equation 12) and define them in two different ways: on the basis of distributional similarity and on the basis of semantic kinship in an ontology. For example, 1 we would like to conclude that \"Bricko\" is more similar to \"Beijing\" even if these two words do not exactly coincide. Moreover, if we have phrases \"his arrival in Beijing\" and \"his arrival in January,\" then we would like to say that \"bridge\" is more similar to \"Beijing\" than the two words exactly match in January. \""}, {"heading": "3.2.1 Distributional Similarity Measures", "text": "There are a number of distributional equality yardsticks that have been proposed over the years, including cosines, cubes and Jaccard coefficients. Distributional equality yardsticks have already been extensively studied (Lee, 1999; Weeds, Weir, & McCarthy, 2004). The main hypothesis behind distributional yardsticks is that words that occur in the same context should have a similar meaning (Firth, 1957). Context can be defined either by proximity in the text or using grammatical relationships. In this paper, we use the first option where the context is a sequence of words in the text and its length is predetermined. We have chosen the following units of measurement: cubes, cosines and L2 (Euclidean), the definitions of which are given in Table 2. In the definition of Cosinus and L2 it is possible to use either frequency counts or probability estimates in the prex."}, {"heading": "3.2.2 WordNet Relatedness Measures", "text": "Most of them are based on the concepts of the shortest paths between two concepts and two hierarchies."}, {"heading": "3.2.3 Substitution Matrix for Relation Extraction", "text": "So far, we have discussed two possible ways to calculate the substitution value d (\u00b7, \u00b7), using either distributional equality yardsticks or WordNet-defined yardsticks. To take this into account, we are revising the definition of d (\u00b7). We assume that the dependency paths generated by parsers contain not only words (or lemmas), but also syntactic functions such as subjects, objects, modifiers, and others. We assume that the sequences x = x1x2.... xn and x. \"x\" m \"contain words (xi-W) (xi-W refers to a set of words) and syntactic functions accompanied by direction (xi-W. The elements of W are unique words (or lemmats) found in the dependency paths, for example, the paths\" his, \"his \u2192 actions\" x \"and\" his \"arrival in Beijing.\""}, {"heading": "4. Experimental Set-up", "text": "In this section, we describe the data sets we used in the experiments and provide information about the data sets used to estimate distribution similarity."}, {"heading": "4.1 Data", "text": "To evaluate the performance of the LA nucleus, we look at two types of text data, domain-specific data coming from the biomedical domain, and generic or domain-independent data representing a variety of well-known and widespread relationships such as PartWhole and Cause Effect. Like other work, we extract a dependency path between two nodes that corresponds to the arguments of a binary relationship. We also assume that each analysis leads to a tree, and since it is an acyclic graph, there is a unique path between each node pair. However, we do not consider other structures that can be derived from full syntactic analysis, such as partial trees (Moschitti, 2006)."}, {"heading": "4.1.1 Biomedical Relations", "text": "In fact, the two cases involved two different types, which differ in the most different expressions: those of the proteins and those of the genes LLL (77 sentences in the training period and 87 in the test phase), which differ in the different expressions of the individual expressions. LLL corpus types were compiled from the different expressions of the individual expressions and distinguished from the different expressions of the individual expressions of the individual expressions. LLL corpus types were formed from the different expressions of the individual expressions of the individual expressions. The differences between the individual expressions of the individual expressions were determined by the different expressions of the individual expressions of the individual expressions of the individual expressions of the individual expressions were divided by the different expressions of the individual expressions of the individual expressions in the different expressions. The differences between the individual expressions of the individual expressions were determined by the different expressions of the expressions of the different expressions of the different expressions of the different expressions of the different expressions of the different expressions of the expressions of the individual expressions of the expressions."}, {"heading": "4.1.2 Generic Relations", "text": "The second type of relationship we are looking at is generic relationships. Their arguments are sometimes commented on with external resources such as WordNet, which allow to use semantic relations between nominals. An example of such an approach is Using Local Alignments for Relation Recognitiondata used for the SemEval-2007 challenge, \"Task 04: Classification of Semantic Relations between Nominals\" (Girju et al., 2009).The aim of Task 4 was to classify seven semantic relationships (Cause - Effect, Instrument - Agency, Product - Producer, Origin - Entity, Theme - Tool, Part - Whole and Content - Container), the examples of which were collected from the web using some predefined queries. In other words, given a number of examples and a relationship, the expected output would be a binary classification of whether an example belongs to the given relationship or not."}, {"heading": "4.2 Substitution Matrix", "text": "To form a substitution matrix for the LA kernel, we use either distributional similarity or semantic kinship measures for WordNet. For a dataset of dependency pathways that contains t unique elements (words and syntactic functions), the size of the matrix is t \u00b7 t. If k elements are words from t, the number of substitution values calculated by distributional similarity (or semantic kinship) is equal to k (k + 1) / 2. This is due to the fact that the words used are symmetrical; the substitution matrix is formed for each corpus used in the experiments that results in three substitution matrices for the biomedical domain (for BC-PPI, LLL, and AImed) and seven substitution matrices for generic relationships. In what follows, we discuss the settings used to calculate the substitution matrix that can be estimated in 1999 with greater textual specificity theory (using either Lee theory)."}, {"heading": "4.2.1 Biomedical domain", "text": "To estimate the distribution similarity in the biomedical field, we use the collection TREC 2006 Genomics (Hersch, Cohen, Roberts, & Rakapalli, 2006), which contains 162,259 documents from 49 journals, all of which have been pre-edited by removing HTML tags, quotations in the text and reference sections, and originated from the Porter stammer (van Rijsbergen, Robertson, & Porter, 1980), and the query probability approach with dirichlet smoothing (Chen & Goodman, 1996) to retrieve fonts that occur in a query, all of which are ranked according to their probability of generating the query, dirichlet smoothing is used to avoid zero probabilities and poor probability estimates (which can happen when words do not appear in the documents), and all of the k unique words that appear in the series of dependence paths are used as queries to determine the similarity between the local contexts and the correlations."}, {"heading": "4.2.2 Generic relations", "text": "For general relationships, we use all of the WordNet kinship measures described in Section 3.2.2. We have already shown that the WordNet kinship measures only work on synsets that assume that all words must be commented manually with information from WordNet. Since this only applies to the arguments of the relationships (see example in Figure 3), and for no other words in sentences (and correspondingly in the dependency paths), we create a substitution matrix as follows. 5 For example, if we look at the words in the dependency path in (25) and in the Wu Palmer package (wup), their substitution value corresponds to a value returned by a kinship measure used, the substitution values we get are 0 if the words are identical, and otherwise 0."}, {"heading": "4.3 Baselines and Kernel Settings", "text": "In this section we will discuss two basic lines and kernel settings."}, {"heading": "4.3.1 Baselines", "text": "To test how well local alignment cores perform in comparison to previously proposed cores, we implemented the shortest path kernel described in the work of Bunescu and Mooney5, even in cases where the relation arguments could not have been commented on with WordNet information.Katrenko, Adriaans, & van Someren (2005a) (Section 2.3.5) as one of the fundamentals (Baseline I. This method seems to be the most natural choice, since it works on the same data structures (dependency paths).Similar to Bunescu and Mooney (2005a), we work with lemma, a part of language tag and direction, but we do not take into account entity type or negative polarity of elements. The choice of LA kernel in this paper was motivated not only by its ability to flexibly compare sequences, but also by the ability to obtain additional information (which is not present in the training set) via a substitution matrix."}, {"heading": "4.3.2 Kernel settings", "text": "The cores we calculate are used together with the support vector engine LibSVM (Chang & Lin, 2001) to detect hyperplanes separating positive from negative examples. Before all kernel matrices are inserted into LibSVM for 10-fold cross-validation, they are weighted as in Equation 26.k (x \u2032, y \u2032) = k (x, y) 270 k (x, x) k (y, y) (26) to handle unbalanced datasets (especially AImed and BC-PPI). All significance tests were weighted using a two-stage paired t test with confidence level (i.e. all class A training examples are weighted with 1 / prob (A), with prob (A) being the fraction of the class A training examples).All significance tests were weighted with a confidence level of 95% (\u03b1 = 0.05) weighted in all experiments we have in the penalty parameters (4)."}, {"heading": "5. Experiment I: Domain-Specific Relations", "text": "In this section, we report on the experiments carried out on three biomedical corpora, with the LA kernel based on distribution-specific similarity measurements, two baseline results and results previously published (e.g., with the graph kernel from Airola et al., 2008 or the tree kernel from S\u00e6tre et al., 2008). To test how Lodhi et al. (2002) kernel results have not yet been applied to dependency paths, a gap-weighted string (described in Section 2) can also allow the gapping results and thus be compared with the LA kernel. To test how Lodhi et al. (2002) apply kernel results to dependency paths, we use itUsing itLocal Alignments for Relation Recognitionon all three corpora."}, {"heading": "5.3 LA Kernel Parameters", "text": "Saigo et al. (2004) have already shown that the scaling parameter \u03b2 (equation \u03b2 11) has significant effects on accuracy. We have also conducted additional experiments with different gap values and the value of \u03b2. The results are visualized in Figure 5. The values of the opening and expansion gap are separated by the slash symbol and the values on the X-axis in the form of \"a / b\" should be considered as \"the opening gap is set to a and the expansion gap is equal to b.\" The kernel matrices have been normalized and all examples have been weighted. According to our previous experiments, the results of the dice measurement do not differ significantly from those obtained by the cosine measurement, and we chose the dice measurement to perform all experiments. The performance of the BC-PPI data is shown in Figure 5.Katrenko, Adriaans, & van SomerenUsing Local Alignments for Relation Recotion.The results are shown to differ from the decoding in an image."}, {"heading": "6. Experiment II: Generic Relations", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Discussion", "text": "In this section, we will review the objectives set out at the end of Section 2 and discuss our results in detail."}, {"heading": "7.1 The LA Kernel for Relation Extraction", "text": "We have introduced the LA kernel, which has proven to be effective in biomedical problems, to the NLP area and demonstrated that it is well suited for relation extraction. Specifically, the experiments in two different areas either exceed existing methods or provide state-of-the-art performance. One of the reasons for using the LA kernel in the relation extraction task is the use of prior knowledge. Here, we examine two possibilities, namely the distribution similarity and the information provided by WordNet."}, {"heading": "7.1.1 Distributional Similarity Measures", "text": "The results indicate that the Jaccard coefficient (which is related to the cube measure) is one of the most powerful measures, followed by some others, including Cosine. Euclidean distance fell into the group with the highest error rates. In light of earlier work by Lee (1999), one would expect the Euclidean distance to perform worse than Katrenko, Adriaans, & van Somerenthe other two measures. In fact, the LA kernel using L2 shows a significant decrease in performance. As for the other measures, the method used by Dice, which is based on the L2 corpus, outperforms only the LLL corpus, while there is no significant improvement in the BC-PPI dataset."}, {"heading": "7.1.2 WordNet Similarity Measures", "text": "The difference in the F score between models that use semantic relations and the core in which the relations are randomly generated (baseline II) is nearly 20%. All metrics perform differently in terms of the seven generic relationships we considered. For example, we can observe that the results in terms of totality, ratio, and relation almost always deliver the best results, regardless of which relationship is taken into account. We found that the Resnik score and Jiang and Conrad's metrics provide lower results than other metrics. Although the F scores per relation are quite substantial (by counting Cause Effect, Theme Tool, and Origin Entity among the most difficult relationships), two metrics, wup and lch, are the most powerful metrics for all seven relationships. These two metrics explore the WordNet taxonomy using the length of paths between two concepts or the wordline in their hierarchy."}, {"heading": "7.2 Factors and Parameters that Influence the LA Kernel Performance", "text": "Our experiments in two areas have shown that the LA kernel either outperforms existing methods on the same corpora or delivers a performance comparable to existing modern cores."}, {"heading": "7.2.1 Baselines", "text": "One advantage of the LA kernel over the Bunescu method with the shortest path (baseline I) is that it is able to handle paths of different lengths. By allowing and penalizing gaps, the final kernel matrix becomes less sparse. Also, the shortest path tries to generalize across paths of dependency, but it usually overgeneralizes, resulting in high recall values (Table 5 and Table 6), but poor overall performance. One explanation for overgeneralization might be that this method is good for structural similarity (assuming the sequences are of the same length), but does not provide more precise distinctions between paths of dependencies. For example, consider two sequences: \"trip\" makes \u2192 tram \"and\" coffee makes \u2192 guy, \"where the first path is a negative instance of the product-producer relationship and the second path is a positive one. Although they do not exactly match, the elements that do not match all nouns."}, {"heading": "7.2.2 Comparison with Other Methods", "text": "For this reason, we have also applied the gap-weighted string kernel (Lodhi et al., 2002) to all data sets. In this case, dependency paths can be flexibly compared because gapping is allowed but no additional information is used. This kernel exceeds Baseline I by increasing the precision of relation extraction while maintaining a relatively high callback rate. LLL test data is the only data set in which it does not yield good results, and we believe this is due to differences in LLL formation and test data. For all data sets, the LA kernel performs better than the gap-weighted string kernel kernel, but the margin is for different data sets. In the biomedical domain, the differences between the two methods can be seen more clearly."}, {"heading": "7.2.3 The LA Parameters", "text": "We have shown that the choice of LA parameters is crucial for good performance. In our experiments, the scaling parameter \u03b2 at most contributes to the overall performance, but other parameters such as gap values must also be taken into account. As \u03b2 approaches infinity, the LA kernel approaches the Smith-Waterman distance, but increasing \u03b2 does not necessarily have a positive effect on final performance. This finding is consistent with the results that Saigo et al. (2004) reported on the task of homology detection. The best performance is achieved by setting the scaling parameter to 1 or slightly higher and penalizing the gap expansion less than the gap opening."}, {"heading": "8. Conclusions and Future Work", "text": "We have presented a novel approach to relation extraction based on the local arrangement of sequences, and the use of an LA nucleus gives us the opportunity to explore various sources of information and investigate their role in relation detection, so possible future directions include an investigation of other distribution problems, the effects of which on the extraction of generic relationships, and the search for other sources of information that could be helpful for relation detection. It might be interesting to take into account the Relational similarity (Turney, 2006), which looks for the correspondence between relation problems. In this case, one should be able to conclude that \"Doctor\" corresponds with \"Scalpel\" in a similar way to \"Fischer\" with \"Net\" (where both (scalpel, doctor) and (net, Fischer) are examples of instruments - agency."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Simon Carter and Gerben de Vries for their comments and proofreading, as well as three anonymous reviewers for their valuable feedback, and acknowledge the contribution of the Adaptive Information Management (AIM) group at the University of Amsterdam. Preliminary version of this work was discussed at the 22nd International Conference on Computational Linguistics (CoLing 2008) and the Seventh International Tbilisi Symposium on Language, Logic and Informatics (2007) and carried out within the Virtual Laboratory for e-Science Projects (www.vl-e.nl), a project supported by a BSIK scholarship from the Dutch Ministry of Education, Culture and Science (OC & W) and part of the ICT Innovation Programme of the Ministry of Economy (EEZ)."}], "references": [{"title": "Allpaths graph kernel for protein-protein interaction extraction with evaluation of crosscorpus learning", "author": ["A. Airola", "S. Pyysalo", "J. Bj\u00f6rne", "T. Pahikkala", "F. Ginter", "T. Salakoski"], "venue": "BMC Bioinformatics,", "citeRegEx": "Airola et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Airola et al\\.", "year": 2008}, {"title": "UIUC: A Knowledge-rich Approach to Identifying Semantic Relations between Nominals", "author": ["B. Beamer", "S. Bhat", "B. Chee", "A. Fister", "A. Rozovskaya", "R. Girju"], "venue": "In Proceedings of the Workshop on Semantic Evaluations (SemEval),", "citeRegEx": "Beamer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Beamer et al\\.", "year": 2007}, {"title": "UC3M: Classification of semantic relations between nominals using sequential minimal optimization", "author": ["I.S. Bedmar", "D. Samy", "J.L. Martinez"], "venue": null, "citeRegEx": "Bedmar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bedmar et al\\.", "year": 2007}, {"title": "Evaluating WordNet-based measures of lexical semantic relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky and Hirst,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky and Hirst", "year": 2006}, {"title": "Learning for Information Extraction", "author": ["R.C. Bunescu"], "venue": "Ph.D. thesis, Department of Computer Sciences, University of Texas at Austin.", "citeRegEx": "Bunescu,? 2007", "shortCiteRegEx": "Bunescu", "year": 2007}, {"title": "Comparative experiments on learning information extractors for proteins and their interactions", "author": ["R.C. Bunescu", "R. Ge", "R.J. Kate", "E.M. Marcotte", "R.J. Mooney", "A.K. Ramani", "Y.W. Wong"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Joint Conference on Human Language Technology / Empirical Methods in Natural Language Processing (HLT/EMNLP),", "citeRegEx": "Bunescu and Mooney,? \\Q2005\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2005}, {"title": "Subsequence kernels for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceedings of the 19th Conference on Neural Information Processing Systems,", "citeRegEx": "Bunescu and Mooney,? \\Q2005\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2005}, {"title": "Text Mining and Natural Language Processing, chap. Extracting Relations from Text", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "From Word Sequences to Dependency Paths. Springer", "citeRegEx": "Bunescu and Mooney,? \\Q2006\\E", "shortCiteRegEx": "Bunescu and Mooney", "year": 2006}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J.C. Burges"], "venue": "Data Mining and Knowledge Discovery, 2 (2), 121\u2013167.", "citeRegEx": "Burges,? 1998", "shortCiteRegEx": "Burges", "year": 1998}, {"title": "The use of background knowledge in inductive logic programming", "author": ["R. Camacho"], "venue": "Report.", "citeRegEx": "Camacho,? 1994", "shortCiteRegEx": "Camacho", "year": 1994}, {"title": "LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": null, "citeRegEx": "Chen and Goodman,? \\Q1996\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1996}, {"title": "Computational-Linguistic Approaches to Biological Text Mining", "author": ["A.B. Clegg"], "venue": "Ph.D. thesis, University of London.", "citeRegEx": "Clegg,? 2008", "shortCiteRegEx": "Clegg", "year": 2008}, {"title": "A comparison of string distance metrics for name-matching tasks", "author": ["W.W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": "IIWeb", "citeRegEx": "Cohen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2003}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["M. Collins"], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Collins,? 1999", "shortCiteRegEx": "Collins", "year": 1999}, {"title": "Convolution kernels for natural language", "author": ["M. Collins", "N. Duffy"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Collins and Duffy,? \\Q2001\\E", "shortCiteRegEx": "Collins and Duffy", "year": 2001}, {"title": "Support vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "Cortes and Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Cortes and Vapnik", "year": 1995}, {"title": "Classification of semantic relationships between nominals using pattern clusters", "author": ["D. Davidov", "A. Rappoport"], "venue": "In Proceedings of ACL-08:HLT,", "citeRegEx": "Davidov and Rappoport,? \\Q2008\\E", "shortCiteRegEx": "Davidov and Rappoport", "year": 2008}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["W.B. Dolan", "C. Quirk", "C. Brockett"], "venue": "COLING", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Semi-supervised classification for extracting protein interaction sentences using dependency parsing", "author": ["G. Erkan", "A. \u00d6zg\u00fcr", "D.R. Radev"], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Erkan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2007}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": "MIT Press.", "citeRegEx": "Fellbaum,? 1998", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["J.R. Firth"], "venue": "Studies in Linguistic Analysis. Philological Society, Oxford. Reprinted in Palmer, F. (ed.), 1968.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "RelEx - relation extraction using dependency parse", "author": ["K. Fundel", "R. Kueffner", "R. Zimmer"], "venue": "trees. Bioinformatics,", "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Automatic discovery of part-whole relations", "author": ["R. Girju", "A. Badulescu", "D. Moldovan"], "venue": "Computational Linguistics,", "citeRegEx": "Girju et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2006}, {"title": "SemEval2007 Task 04: Classification of semantic relations between nominals", "author": ["R. Girju", "P. Nakov", "V. Nastase", "S. Szpakowicz", "P. Turney", "D. Yuret"], "venue": null, "citeRegEx": "Girju et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Classification of semantic relations between nominals", "author": ["R. Girju", "P. Nakov", "V. Nastase", "S. Szpakowicz", "P. Turney", "D. Yuret"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Girju et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2009}, {"title": "FBK-IRST: Kernel methods for semantic relation extraction", "author": ["C. Giuliano", "A. Lavelli", "D. Pighin", "L. Romano"], "venue": null, "citeRegEx": "Giuliano et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Giuliano et al\\.", "year": 2007}, {"title": "Exploiting shallow linguistic information for relation extraction from biomedical literature", "author": ["C. Giuliano", "A. Lavelli", "L. Romano"], "venue": null, "citeRegEx": "Giuliano et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Giuliano et al\\.", "year": 2006}, {"title": "Message Understanding Conference - 6: A brief history", "author": ["R. Grishman", "B. Sundheim"], "venue": "In Proceedings of the 16th International Conference on Computational Linguistics", "citeRegEx": "Grishman and Sundheim,? \\Q1996\\E", "shortCiteRegEx": "Grishman and Sundheim", "year": 1996}, {"title": "Convolution kernels on discrete structures", "author": ["D. Haussler"], "venue": "Tech. rep. UCS-CRL-99-10, UC Santa Cruz.", "citeRegEx": "Haussler,? 1999", "shortCiteRegEx": "Haussler", "year": 1999}, {"title": "Automatic acquisition of hyponyms from large text data", "author": ["M. Hearst"], "venue": "Proceedings of COLING-92, pp. 539\u2013545.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "ILK: Machine learning of semantic relations with shallow features and almost no data", "author": ["I. Hendrickx", "R. Morante", "C. Sporleder", "A. van den Bosch"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2007}, {"title": "TREC 2006 genomics track overview", "author": ["W. Hersch", "A.M. Cohen", "P. Roberts", "H.K. Rakapalli"], "venue": "In Proceedings of the 15th Text Retrieval Conference", "citeRegEx": "Hersch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hersch et al\\.", "year": 2006}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "In Proceedings of International Conference on Research in Computational Linguistics (ROCLING X),", "citeRegEx": "Jiang and Conrath,? \\Q1997\\E", "shortCiteRegEx": "Jiang and Conrath", "year": 1997}, {"title": "Transductive inference for text classification using Support Vector Machines", "author": ["T. Joachims"], "venue": "Proceedings of ICML.", "citeRegEx": "Joachims,? 1999", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Semantic types of some generic relation arguments: Detection and evaluation", "author": ["S. Katrenko", "P. Adriaans"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT),", "citeRegEx": "Katrenko and Adriaans,? \\Q2008\\E", "shortCiteRegEx": "Katrenko and Adriaans", "year": 2008}, {"title": "Extracting causal knowledge from a medical database using graphical patterns", "author": ["C.S.G. Khoo", "S. Chan", "Y. Niu"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Khoo et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Khoo et al\\.", "year": 2000}, {"title": "MELB-KB: Nominal classifications as noun compound interpretation", "author": ["S.N. Kim", "T. Baldwin"], "venue": null, "citeRegEx": "Kim and Baldwin,? \\Q2007\\E", "shortCiteRegEx": "Kim and Baldwin", "year": 2007}, {"title": "Incorporating Prior Knowledge in Support Vector Machines for Classification: a Review", "author": ["F. Lauer", "G. Bloch"], "venue": null, "citeRegEx": "Lauer and Bloch,? \\Q2008\\E", "shortCiteRegEx": "Lauer and Bloch", "year": 2008}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["C. Leacock", "M. Chodorow"], "venue": null, "citeRegEx": "Leacock and Chodorow,? \\Q1998\\E", "shortCiteRegEx": "Leacock and Chodorow", "year": 1998}, {"title": "Parsing biomedical literature", "author": ["M. Lease", "E. Charniak"], "venue": "In Proceedings of IJCNLP", "citeRegEx": "Lease and Charniak,? \\Q2005\\E", "shortCiteRegEx": "Lease and Charniak", "year": 2005}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pp. 25\u201332.", "citeRegEx": "Lee,? 1999", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Mismatch string kernels for discriminative protein", "author": ["C. Leslie", "E. Eskin", "A. Cohen", "J. Weston", "W.S. Noble"], "venue": "classification. Bioinformatics,", "citeRegEx": "Leslie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2004}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "W.S. Noble"], "venue": "In Pacific Symposium on Biocomputing", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "A novel string-to-string distance measure with applications to machine translation evaluation", "author": ["G. Leusch", "N. Ueffing", "H. Ney"], "venue": "In Machine Translation Summit IX,", "citeRegEx": "Leusch et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Leusch et al\\.", "year": 2003}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "Proceedings of the 15th International Conference on Machine Learning, pp. 296\u2013304.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Christianini", "C. Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Extracting relations from unstructured text", "author": ["R. McDonald"], "venue": "Tech. rep. MS-CIS-0506, UPenn.", "citeRegEx": "McDonald,? 2005", "shortCiteRegEx": "McDonald", "year": 2005}, {"title": "Dpendency syntax: theory and practice", "author": ["I. Mel\u2019\u010duk"], "venue": null, "citeRegEx": "Mel.\u010duk,? \\Q1988\\E", "shortCiteRegEx": "Mel.\u010duk", "year": 1988}, {"title": "Machine Learning", "author": ["T. Mitchell"], "venue": "McGraw Hill.", "citeRegEx": "Mitchell,? 1997", "shortCiteRegEx": "Mitchell", "year": 1997}, {"title": "Task-oriented evaluation of syntactic parsers and their representations", "author": ["Y. Miyao", "R. S\u00e6tre", "K. Sagae", "T. Matsuzaki", "J. Tsuji"], "venue": "In Proceedings of ACL-08:HLT,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "Measuring Semantic Distance using distributional profiles of concepts", "author": ["S. Mohammad"], "venue": "Ph.D. thesis, Graduate Department of Computer Science of University of Toronto.", "citeRegEx": "Mohammad,? 2008", "shortCiteRegEx": "Mohammad", "year": 2008}, {"title": "The field matching problem: Algorithms and applications", "author": ["A.E. Monge", "C. Elkan"], "venue": "KDD", "citeRegEx": "Monge and Elkan,? \\Q1996\\E", "shortCiteRegEx": "Monge and Elkan", "year": 1996}, {"title": "Efficient convolution kernels for dependency and constituent syntactic trees", "author": ["A. Moschitti"], "venue": "ECML 2006, pp. 318\u2013329.", "citeRegEx": "Moschitti,? 2006", "shortCiteRegEx": "Moschitti", "year": 2006}, {"title": "UCB: System description for SemEval task #4", "author": ["P. Nakov"], "venue": "SemEval-2007.", "citeRegEx": "Nakov,? 2007", "shortCiteRegEx": "Nakov", "year": 2007}, {"title": "Paraphrasing verbs for noun compound interpretation", "author": ["P. Nakov"], "venue": "Proceedings of the Workshop on Multiword Expressions (MWE\u201908), in conjunction with the Language Resources and Evaluation conference, Marrakech, Morocco, 2008.", "citeRegEx": "Nakov,? 2008", "shortCiteRegEx": "Nakov", "year": 2008}, {"title": "Solving relational similarity problems using the web as a corpus", "author": ["P. Nakov", "M.A. Hearst"], "venue": "In Proceedings of ACL-08:HLT", "citeRegEx": "Nakov and Hearst,? \\Q2008\\E", "shortCiteRegEx": "Nakov and Hearst", "year": 2008}, {"title": "Learning Language in Logic - Genic Interaction Extraction Challenge", "author": ["C. N\u00e9dellec"], "venue": "Proceedings of the Learning Language in Logic workshop.", "citeRegEx": "N\u00e9dellec,? 2005", "shortCiteRegEx": "N\u00e9dellec", "year": 2005}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Needleman and Wunsch,? \\Q1970\\E", "shortCiteRegEx": "Needleman and Wunsch", "year": 1970}, {"title": "UCD-PN: Classification of semantic relations between nominals using WordNet and web counts", "author": ["P. Nulty"], "venue": "SemEval-2007.", "citeRegEx": "Nulty,? 2007", "shortCiteRegEx": "Nulty", "year": 2007}, {"title": "Semantic classification with WordNet kernels", "author": ["D. \u00d3 S\u00e9aghdha"], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference (NAACL-HLT), Boulder, CO.", "citeRegEx": "S\u00e9aghdha,? 2009", "shortCiteRegEx": "S\u00e9aghdha", "year": 2009}, {"title": "Semantic classification with distributional kernels", "author": ["D. \u00d3 S\u00e9aghdha", "A. Copestake"], "venue": "In Proceedings of CoLing", "citeRegEx": "S\u00e9aghdha and Copestake,? \\Q2008\\E", "shortCiteRegEx": "S\u00e9aghdha and Copestake", "year": 2008}, {"title": "Verb semantics for English-Chinese translation", "author": ["M. Palmer", "Z. Wu"], "venue": "Tech. rep., Technical Report No. MS-CIS-95-39,", "citeRegEx": "Palmer and Wu,? \\Q1995\\E", "shortCiteRegEx": "Palmer and Wu", "year": 1995}, {"title": "WordNet::Similarity - Measuring the Relatedness of Concepts", "author": ["T. Pedersen", "S. Patwardhan", "J. Michelizzi"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence", "citeRegEx": "Pedersen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2004}, {"title": "Ontologizing semantic relations", "author": ["M. Pennacchiotti", "P. Pantel"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Pennacchiotti and Pantel,? \\Q2006\\E", "shortCiteRegEx": "Pennacchiotti and Pantel", "year": 2006}, {"title": "Knowledge Derived from Wikipedia for Computing Semantic Relatedness", "author": ["S.P. Ponzetto", "M. Strube"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ponzetto and Strube,? \\Q2007\\E", "shortCiteRegEx": "Ponzetto and Strube", "year": 2007}, {"title": "Using information content to evaluate semantic similarity", "author": ["P. Resnik"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence, pp. 448\u2013453.", "citeRegEx": "Resnik,? 1995", "shortCiteRegEx": "Resnik", "year": 1995}, {"title": "Syntactic features for protein-protein interaction extraction", "author": ["R. S\u00e6tre", "K. Sagae", "J. Tsuji"], "venue": "In 2nd International Symposium on Languages in Biology and Medicine,", "citeRegEx": "S\u00e6tre et al\\.,? \\Q2008\\E", "shortCiteRegEx": "S\u00e6tre et al\\.", "year": 2008}, {"title": "Dependency parsing and domain adaptation with LR models and parser ensembles", "author": ["K. Sagae", "J. Tsujii"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Sagae and Tsujii,? \\Q2007\\E", "shortCiteRegEx": "Sagae and Tsujii", "year": 2007}, {"title": "Optimizing amino acid substitution matrices with a local alignment kernel", "author": ["H. Saigo", "Vert", "J.-P", "T. Akutsu"], "venue": "BMC Bioinformatics,", "citeRegEx": "Saigo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Saigo et al\\.", "year": 2006}, {"title": "Protein homology detection using string alignment", "author": ["H. Saigo", "Vert", "J.-P", "N. Ueda", "T. Akutsu"], "venue": "kernels. Bioinformatics,", "citeRegEx": "Saigo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Saigo et al\\.", "year": 2004}, {"title": "Applying spelling error correction techniques for improving semantic role labeling", "author": ["Sang", "E.F.T. K", "S. Canisius", "A. van den Bosch", "T. Bogers"], "venue": "In Proceedings of the Ninth Conference on Natural Language Learning,", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Syllables and other string kernel extensions", "author": ["C. Saunders", "H. Tschach", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Saunders et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 2002}, {"title": "Support vector learning", "author": ["B. Sch\u00f6lkopf"], "venue": "Ph.D. thesis, Berlin Technical University.", "citeRegEx": "Sch\u00f6lkopf,? 1997", "shortCiteRegEx": "Sch\u00f6lkopf", "year": 1997}, {"title": "Identifying the interaction between genes and gene products based on frequently seen verbs in Medline abstracts", "author": ["T. Sekimizu", "H.S. Park", "J. Tsujii"], "venue": "Genome Informatics,", "citeRegEx": "Sekimizu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sekimizu et al\\.", "year": 1998}, {"title": "Support Vector Machines and Other KernelBased Learning Methods", "author": ["J. Shawe-Taylor", "N. Christianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Christianini,? \\Q2000\\E", "shortCiteRegEx": "Shawe.Taylor and Christianini", "year": 2000}, {"title": "Hidden Markov models and optimized sequence alignment", "author": ["L.H. Smith", "L. Yeganova", "W.J. Wilbur"], "venue": "Computational Biology and Chemistry,", "citeRegEx": "Smith et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2003}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S. Waterman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Smith and Waterman,? \\Q1981\\E", "shortCiteRegEx": "Smith and Waterman", "year": 1981}, {"title": "Learning named entity hyponyms for question answering", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of COLING/ACL", "citeRegEx": "Snow et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Implicit text linkages between Medline records: Using Arrowsmith as an aid to scientific discovery", "author": ["D.R. Swanson", "N.R. Smalheiser"], "venue": "Library Trends,", "citeRegEx": "Swanson and Smalheiser,? \\Q1999\\E", "shortCiteRegEx": "Swanson and Smalheiser", "year": 1999}, {"title": "Automatic extraction of protein interactions from scientific abstracts", "author": ["J. Thomas", "D. Milward", "C. Ouzounis", "S. Pulman"], "venue": "In Proceedings of Pacific Symposium on Biocomputing", "citeRegEx": "Thomas et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2000}, {"title": "CMU-AT: Semantic distance and background knowledge for identifying semantic relations", "author": ["A. Tribble", "S.E. Fahlman"], "venue": null, "citeRegEx": "Tribble and Fahlman,? \\Q2007\\E", "shortCiteRegEx": "Tribble and Fahlman", "year": 2007}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32 (3), 379\u2013416.", "citeRegEx": "Turney,? 2006", "shortCiteRegEx": "Turney", "year": 2006}, {"title": "Automatic Lexico-Semantic Acquisition for Question Answering", "author": ["L. van der Plas"], "venue": "Ph.D. thesis,", "citeRegEx": "Plas,? \\Q2008\\E", "shortCiteRegEx": "Plas", "year": 2008}, {"title": "New models in probabilistic information retrieval", "author": ["C.J. van Rijsbergen", "S.E. Robertson", "M.F. Porter"], "venue": "Tech. rep. 5587,", "citeRegEx": "Rijsbergen et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Rijsbergen et al\\.", "year": 1980}, {"title": "Estimation of Dependences Based on Empirical Data", "author": ["V. Vapnik"], "venue": "New York: SPringer Verlag.", "citeRegEx": "Vapnik,? 1982", "shortCiteRegEx": "Vapnik", "year": 1982}, {"title": "Characterising measures of lexical distributional similarity", "author": ["J. Weeds", "D. Weir", "D. McCarthy"], "venue": "In Proceedings of CoLing", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Learning the semantic correlation: An alternative way to gain from unlabeled text", "author": ["Y. Zhang", "J. Schneider", "A. Dubrawski"], "venue": "In Proceedings of the 22nd Conference on Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 13, "context": "For instance, relation extraction in the biomedical domain would require an accurate recognition of named entities such as gene names (Clegg, 2008), and in the area of food it needs information on relevant named entities such as toxic substances.", "startOffset": 134, "endOffset": 147}, {"referenceID": 56, "context": "Generic relations very often occur in nominal complexes such as \u2018flu virus\u2019 in (2) and lack of sentential context boosts such approaches as paraphrasing (Nakov, 2008).", "startOffset": 153, "endOffset": 166}, {"referenceID": 5, "context": "Previous work on relation extraction suggests that in this case the accuracy of relation recognition is much higher than in the case when they have to be discovered automatically (Bunescu et al., 2005).", "startOffset": 179, "endOffset": 201}, {"referenceID": 48, "context": "Furthermore, most existing solutions to relation extraction (including work presented in this paper) focus on relation examples that occur within a single sentence and do not consider discourse (McDonald, 2005).", "startOffset": 194, "endOffset": 210}, {"referenceID": 81, "context": "Although patterns are often precise, they usually produce poor recall (Thomas et al., 2000).", "startOffset": 70, "endOffset": 91}, {"referenceID": 31, "context": "Hand-written sequential patterns were initially used for extraction of Hypernymy (Hearst, 1992), with several attempts to extend them to other relations.", "startOffset": 81, "endOffset": 95}, {"referenceID": 88, "context": "In recent years kernel-based methods have become popular because they can handle high-dimensional problems (Zelenko et al., 2003; Bunescu & Mooney, 2006; Airola et al., 2008).", "startOffset": 107, "endOffset": 174}, {"referenceID": 0, "context": "In recent years kernel-based methods have become popular because they can handle high-dimensional problems (Zelenko et al., 2003; Bunescu & Mooney, 2006; Airola et al., 2008).", "startOffset": 107, "endOffset": 174}, {"referenceID": 26, "context": "Hand-written sequential patterns were initially used for extraction of Hypernymy (Hearst, 1992), with several attempts to extend them to other relations. The second type of patterns (Khoo, Chan, & Niu, 2000) take the syntactic structure of a sentence into account. The dependency structure of a sentence can usually be represented as a tree and the patterns then become subtrees. Such patterns are sometimes referred to as graphical patterns. To identify examples of the Cause-Effect relation, Khoo et al. (2000) applied this type of patterns to texts in the medical domain.", "startOffset": 82, "endOffset": 513}, {"referenceID": 0, "context": ", 2003; Bunescu & Mooney, 2006; Airola et al., 2008). These methods transform text fragments, complete sentences or segments around named entitites or verbs, to vectors, and apply Support Vector Machines to classify new fragments. Some Machine Learning methods use prior knowledge that is given to the system in addition to labeled examples (Sch\u00f6lkopf, 1997, p. 17). The use of prior knowledge is often motivated by, for example, poor quality of data and data sparseness. Prior knowledge can be used in many ways, from changing the representation of existing training examples to adding more examples from unlabelled data. For NLP tasks, prior knowledge exists in the form of manually (or automatically) constructed ontologies or large collections of unannotated data. These enrich the textual data and thereby improve the recognition of relations (Sekimizu, Park, & Tsujii, 1998; Tribble & Fahlman, 2007). Recently, Zhang et al. (2008) showed that semantic correlation of words can be learned from unlabelled text collections, transferred among documents and used further to improve document classification.", "startOffset": 32, "endOffset": 937}, {"referenceID": 0, "context": ", 2003; Bunescu & Mooney, 2006; Airola et al., 2008). These methods transform text fragments, complete sentences or segments around named entitites or verbs, to vectors, and apply Support Vector Machines to classify new fragments. Some Machine Learning methods use prior knowledge that is given to the system in addition to labeled examples (Sch\u00f6lkopf, 1997, p. 17). The use of prior knowledge is often motivated by, for example, poor quality of data and data sparseness. Prior knowledge can be used in many ways, from changing the representation of existing training examples to adding more examples from unlabelled data. For NLP tasks, prior knowledge exists in the form of manually (or automatically) constructed ontologies or large collections of unannotated data. These enrich the textual data and thereby improve the recognition of relations (Sekimizu, Park, & Tsujii, 1998; Tribble & Fahlman, 2007). Recently, Zhang et al. (2008) showed that semantic correlation of words can be learned from unlabelled text collections, transferred among documents and used further to improve document classification. In general, while use of large collections of text allows us to derive almost any information needed, it is done with varying accuracy. In contrast, existing resources created by humans can provide very precise information, but it is less likely that they will cover all possible areas of interest. In this paper, as in the work of Bunescu and Mooney (2006), we use the syntactic structure of sentences, in particular, dependency paths.", "startOffset": 32, "endOffset": 1467}, {"referenceID": 9, "context": "The past years have witnessed a boost of interest in kernel methods, their theoretical analysis and practical applications in various fields (Burges, 1998; Shawe-Taylor & Christianini, 2000).", "startOffset": 141, "endOffset": 190}, {"referenceID": 86, "context": "A hyperplane that separates mapped examples with the largest possible margin would be the best option (Vapnik, 1982).", "startOffset": 102, "endOffset": 116}, {"referenceID": 9, "context": "The past years have witnessed a boost of interest in kernel methods, their theoretical analysis and practical applications in various fields (Burges, 1998; Shawe-Taylor & Christianini, 2000). The idea of having a method that works with different structures and representations, starting from the simplest representation using a limited number of attributes to complex structures such as trees, seems indeed very attractive. Before we define a kernel function, recall the standard setting for supervised classification. For a training set S of n objects (instances) (x1, y1), . . . , (xn, yn) where x1, . . . ,xn \u2208 X are input examples in the input space X with their corresponding labels y1, . . . , yn \u2208 {0,1}, the goal is to infer a function h : X \u2192 {0, 1} such that it approximates a target function t. However, h can still err on the data which has to be reflected in a loss function, l(h(xi), yi). Several loss functions have been proposed in the literature so far, the best known of which is the zero-one loss. This loss is a function that outputs 1 each time a method errs on a data point (h(xi) 6= yi), and 0 otherwise. The key idea of kernel methods lies in the implicit mapping of objects to a highdimensional space (by using some mapping function \u03c6) and considering their inner product (similarity) k(xi,xj) =< \u03c6(xi), \u03c6(xj) >, rather than representing them explicitly. Functions that can be used in kernel methods have to be symmetric and positive semi-definite, whereby positive semi-definiteness is defined by \u2211n i=1 \u2211n j=1 cicjk(xi,xj) \u2265 0 for any n > 0, any objects x1, . . . ,xn \u2208 X , and any choice of real numbers c1, . . . , cn \u2208 R. If a function is not positive semi-definite, the algorithm may not find the global optimal solution. If the requirements w.r.t. symmetry and positive semi-definiteness are met, a kernel is called valid. Using the idea of a kernel mapping, Cortes and Vapnik (1995) introduced support vector machines (SVM) as a method which seeks the linear separation between two classes of the input points by a function f(x) such that f(x) = wT\u03c6(x) + b, wT \u2208 Rp, b \u2208 R and h(x) = sign(f(x)).", "startOffset": 142, "endOffset": 1916}, {"referenceID": 10, "context": "Inductive logic programming offers one possible solution to use it explicitly, in the form of additional Horn clauses (Camacho, 1994).", "startOffset": 118, "endOffset": 133}, {"referenceID": 50, "context": "In the Bayesian learning paradigm information on the hypothesis without seeing any data is encoded in a Bayesian prior (Mitchell, 1997) or in a higher level distribution in a hierarchical Bayesian setting.", "startOffset": 119, "endOffset": 135}, {"referenceID": 29, "context": "It has been shown by Haussler (1999) that a complex kernel (referred to as a convolution kernel) can be defined using simpler kernels.", "startOffset": 21, "endOffset": 37}, {"referenceID": 43, "context": "The novelty of Leslie et al.\u2019s (2002) method lies in its generality and its low computational complexity.", "startOffset": 15, "endOffset": 38}, {"referenceID": 43, "context": "The mismatch kernel that was introduced later by Leslie et al. (2004) is essentially an extension of the latter.", "startOffset": 49, "endOffset": 70}, {"referenceID": 46, "context": "The second uses linguistic structures such as dependency paths or trees or the output of shallow parsing. In this short review we do not take a chronological perspective but rather start with the methods that are based on sequences and proceed with the approaches that make use of syntactic information. In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence kernels that provide flexible means to work with text data.", "startOffset": 16, "endOffset": 384}, {"referenceID": 46, "context": "The second uses linguistic structures such as dependency paths or trees or the output of shallow parsing. In this short review we do not take a chronological perspective but rather start with the methods that are based on sequences and proceed with the approaches that make use of syntactic information. In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence kernels that provide flexible means to work with text data. In particular, subsequences are not necessarily contiguous and are weighted according to their length (using a decay factor \u03bb). The length of the subsequences is fixed in advance. The authors claim that even without the use of any linguistic information their kernels are able to capture semantic information. This is reflected in the better performance on the text classification task compared to the bag-of-words approach. While Lodhi et al.\u2019s (2002) kernel works on sequences of characters, a kernel proposed by Cancedda et al.", "startOffset": 16, "endOffset": 927}, {"referenceID": 46, "context": "The second uses linguistic structures such as dependency paths or trees or the output of shallow parsing. In this short review we do not take a chronological perspective but rather start with the methods that are based on sequences and proceed with the approaches that make use of syntactic information. In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence kernels that provide flexible means to work with text data. In particular, subsequences are not necessarily contiguous and are weighted according to their length (using a decay factor \u03bb). The length of the subsequences is fixed in advance. The authors claim that even without the use of any linguistic information their kernels are able to capture semantic information. This is reflected in the better performance on the text classification task compared to the bag-of-words approach. While Lodhi et al.\u2019s (2002) kernel works on sequences of characters, a kernel proposed by Cancedda et al. (2003) is applied to word sequences.", "startOffset": 16, "endOffset": 1012}, {"referenceID": 46, "context": "The second uses linguistic structures such as dependency paths or trees or the output of shallow parsing. In this short review we do not take a chronological perspective but rather start with the methods that are based on sequences and proceed with the approaches that make use of syntactic information. In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence kernels that provide flexible means to work with text data. In particular, subsequences are not necessarily contiguous and are weighted according to their length (using a decay factor \u03bb). The length of the subsequences is fixed in advance. The authors claim that even without the use of any linguistic information their kernels are able to capture semantic information. This is reflected in the better performance on the text classification task compared to the bag-of-words approach. While Lodhi et al.\u2019s (2002) kernel works on sequences of characters, a kernel proposed by Cancedda et al. (2003) is applied to word sequences. String kernels can be also extended to syllable kernels which proved to do well on text categorization (Saunders, Tschach, & Shawe-Taylor, 2002). Because all these kernels can be defined recursively, their computation is efficient. For instance, the time complexity of Lodhi et al.\u2019s (2002) kernel is O(n|s||t|), where n is the length of the subsequence, and t and s are documents.", "startOffset": 16, "endOffset": 1333}, {"referenceID": 23, "context": "A method proposed by Giuliano et al. (2006) was largely inspired by the work of Bunescu and Mooney (2005b).", "startOffset": 21, "endOffset": 44}, {"referenceID": 4, "context": "(2006) was largely inspired by the work of Bunescu and Mooney (2005b). However, instead of looking for subsequences in three types of sequences, the authors treat them as a bag-of-words and define what is called a global kernel as follows.", "startOffset": 43, "endOffset": 70}, {"referenceID": 42, "context": "They show that it is possible to derive kernels from such distances as Jensen-Shannon divergence (JSD) or Euclidean distance (L2) (Lee, 1999).", "startOffset": 130, "endOffset": 141}, {"referenceID": 59, "context": "Recently, \u00d3 S\u00e9aghdha and Copestake (2008) introduced distributional kernels on co- occurrence probability distributions.", "startOffset": 12, "endOffset": 42}, {"referenceID": 15, "context": "Re-ranking parsing trees (Collins & Duffy, 2001) was one of the first applications of kernel methods to NLP problems. To accomplish this goal, the authors rely on the subtrees that a pair of trees have in common. Later on, Moschitti (2006) explored convolution kernels on dependency and constituency structures to do semantic role labeling and question classification.", "startOffset": 26, "endOffset": 240}, {"referenceID": 4, "context": "Bunescu and Mooney (2005a) use several features such as word (e.", "startOffset": 0, "endOffset": 27}, {"referenceID": 35, "context": "Further, various machine learning methods are used to do classification, including SVM and transuctive SVM (TSVM), which is an extension of SVM (Joachims, 1999).", "startOffset": 144, "endOffset": 160}, {"referenceID": 19, "context": "Here, Erkan et al. (2007) use dependency paths as input and compare them by means of cosine similarity or edit distance.", "startOffset": 6, "endOffset": 26}, {"referenceID": 0, "context": "Airola et al. (2008) propose a graph kernel which makes use of the entire dependency structure.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Airola et al. (2008) propose a graph kernel which makes use of the entire dependency structure. In their work, each sentence is represented by two subgraphs, one of which is built from the dependency analysis, and the other corresponds to the linear structure of the sentence. Further, a kernel is defined on all paths between any two vertices in the graph. The method by Airola et al. (2008) achieves state-of-the-art performance on biomedical data sets, and is further discussed, together with the shortest path kernel and the work", "startOffset": 0, "endOffset": 393}, {"referenceID": 73, "context": "In our view, these types of methods can complement each other (Saunders et al., 2002).", "startOffset": 62, "endOffset": 85}, {"referenceID": 19, "context": "by Erkan et al. (2007), in Section 5 on relation extraction in the biomedical domain in this paper.", "startOffset": 3, "endOffset": 23}, {"referenceID": 19, "context": "by Erkan et al. (2007), in Section 5 on relation extraction in the biomedical domain in this paper. Finally, kernels can be defined not only on graphs of syntactic structures, but also on graphs of a semantic network. This is illustrated by \u00d3 S\u00e9aghdha (2009), who uses graph kernels on the graph built from the hyponymy relations in WordNet.", "startOffset": 3, "endOffset": 259}, {"referenceID": 54, "context": "One can note from our short overview of the kernels designed for NLP above that many researchers use partial structures and propose variants such as subsequence kernels (Bunescu & Mooney, 2005b), a partial tree kernel (Moschitti, 2006), or a kernel on shallow parsing output (Zelenko et al.", "startOffset": 218, "endOffset": 235}, {"referenceID": 88, "context": "One can note from our short overview of the kernels designed for NLP above that many researchers use partial structures and propose variants such as subsequence kernels (Bunescu & Mooney, 2005b), a partial tree kernel (Moschitti, 2006), or a kernel on shallow parsing output (Zelenko et al., 2003) for relation extraction.", "startOffset": 275, "endOffset": 297}, {"referenceID": 72, "context": "The Levenshtein distance has been used in the natural language processing field as a component in a variety of tasks, including semantic role labeling (Sang et al., 2005), construction of paraphrase corpora (Dolan, Quirk, & Brockett, 2004), evaluation of machine translation output (Leusch, Ueffing, & Ney, 2003), and others.", "startOffset": 151, "endOffset": 170}, {"referenceID": 14, "context": "The Smith-Waterman measure is mostly used in the biological domain, there are, however, some applications of a modified Smith-Waterman measure to text data as well (Monge & Elkan, 1996; Cohen et al., 2003).", "startOffset": 164, "endOffset": 205}, {"referenceID": 78, "context": "For gapping, Smith and Waterman (1981) suggested to use a gap value which is at least equal to the difference between a match (d(xi, xj), xi = xj) and a mismatch (d(xi, x \u2032 j), xi 6= xj).", "startOffset": 13, "endOffset": 39}, {"referenceID": 70, "context": "However, Saigo et al. (2004) observed that the Smith-Waterman measure may not result in a valid kernel because it may not be positive semi-definite.", "startOffset": 9, "endOffset": 29}, {"referenceID": 30, "context": "Finally, the LA kernel is a composition of several kernels (k0, ka, and kg), which is in the spirit of convolution kernels (Haussler, 1999).", "startOffset": 123, "endOffset": 139}, {"referenceID": 30, "context": "Finally, the LA kernel is a composition of several kernels (k0, ka, and kg), which is in the spirit of convolution kernels (Haussler, 1999). According to Saigo et al. (2004), similarity of the aligned sequences\u2019 elements (ka kernel) is defined as follows:", "startOffset": 124, "endOffset": 174}, {"referenceID": 70, "context": "The results in the biological domain suggest that kernels based on the Smith-Waterman distance are more relevant for the comparison of amino acids than string kernels (Saigo et al., 2006).", "startOffset": 167, "endOffset": 187}, {"referenceID": 42, "context": "Distributional similarity measures have been extensively studied before (Lee, 1999; Weeds, Weir, & McCarthy, 2004).", "startOffset": 72, "endOffset": 114}, {"referenceID": 22, "context": "The main hypothesis behind distributional measures is that words occurring in the same context should have similar meaning (Firth, 1957).", "startOffset": 123, "endOffset": 136}, {"referenceID": 42, "context": "Here, we adopt the definitions given by Lee (1999), which are based on probability estimates P .", "startOffset": 40, "endOffset": 51}, {"referenceID": 21, "context": "For generic relations, the most commonly used resource is WordNet (Fellbaum, 1998), which is a lexical database for English.", "startOffset": 66, "endOffset": 82}, {"referenceID": 21, "context": ", \u2018fountain pen\u2019), and pointers that describe the relations between this synset and other synsets\u201d (Fellbaum, 1998).", "startOffset": 99, "endOffset": 115}, {"referenceID": 27, "context": "WordNet can be employed for different purposes such as studying semantic constraints for certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008), or enriching the training set (Giuliano et al., 2007; Nulty, 2007).", "startOffset": 207, "endOffset": 243}, {"referenceID": 60, "context": "WordNet can be employed for different purposes such as studying semantic constraints for certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008), or enriching the training set (Giuliano et al., 2007; Nulty, 2007).", "startOffset": 207, "endOffset": 243}, {"referenceID": 21, "context": "For generic relations, the most commonly used resource is WordNet (Fellbaum, 1998), which is a lexical database for English. In WordNet, words are grouped together in synsets where a synset \u201cconsists of a list of synonymous words or collocations (e.g., \u2018fountain pen\u2019), and pointers that describe the relations between this synset and other synsets\u201d (Fellbaum, 1998). WordNet can be employed for different purposes such as studying semantic constraints for certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008), or enriching the training set (Giuliano et al., 2007; Nulty, 2007). To compare two concepts given their synsets c1 and c2 we use five different measures that have been proposed in the past years. Most of them rely on the notions of the length of the shortest path between two concepts c1 and c2, len(c1, c2), the depth of a node in the WordNet hierarchy (which is equal to the length of the path from the root to the given synset ci), dep(ci), and a least common subsumer (or lowest super-ordinate) between c1 and c2, lcs(c1, c2), which in turn is a synset. To the measures that are exclusively based on these notions belong conceptual similarity proposed by Palmer and Wu (1995) (simwup in Equation 16) and the formula of scaled semantic similarity introduced by Leacock and Chodorow (1998) (simlch in Equation 17).", "startOffset": 67, "endOffset": 1225}, {"referenceID": 21, "context": "For generic relations, the most commonly used resource is WordNet (Fellbaum, 1998), which is a lexical database for English. In WordNet, words are grouped together in synsets where a synset \u201cconsists of a list of synonymous words or collocations (e.g., \u2018fountain pen\u2019), and pointers that describe the relations between this synset and other synsets\u201d (Fellbaum, 1998). WordNet can be employed for different purposes such as studying semantic constraints for certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008), or enriching the training set (Giuliano et al., 2007; Nulty, 2007). To compare two concepts given their synsets c1 and c2 we use five different measures that have been proposed in the past years. Most of them rely on the notions of the length of the shortest path between two concepts c1 and c2, len(c1, c2), the depth of a node in the WordNet hierarchy (which is equal to the length of the path from the root to the given synset ci), dep(ci), and a least common subsumer (or lowest super-ordinate) between c1 and c2, lcs(c1, c2), which in turn is a synset. To the measures that are exclusively based on these notions belong conceptual similarity proposed by Palmer and Wu (1995) (simwup in Equation 16) and the formula of scaled semantic similarity introduced by Leacock and Chodorow (1998) (simlch in Equation 17).", "startOffset": 67, "endOffset": 1337}, {"referenceID": 67, "context": "Aiming at combining information from several sources, Resnik (1995) introduced yet another measure that is grounded in information content (simres in Equation 18).", "startOffset": 54, "endOffset": 68}, {"referenceID": 34, "context": "To overcome this, Jiang and Conrath (1997) proposed a solution that takes into account information about the synsets being compared (simjcn in Equation 19).", "startOffset": 18, "endOffset": 43}, {"referenceID": 54, "context": "We do not consider, however, other structures that might be derived from the full syntactic analysis as in, for example, subtrees (Moschitti, 2006).", "startOffset": 130, "endOffset": 147}, {"referenceID": 20, "context": "To be able to compare our results on AImed with the performance reported in the work of Erkan et al. (2007) and S\u00e6tre et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 20, "context": "To be able to compare our results on AImed with the performance reported in the work of Erkan et al. (2007) and S\u00e6tre et al. (2008), we use exactly the same dependency paths with argument labels.", "startOffset": 88, "endOffset": 132}, {"referenceID": 26, "context": "data used for the SemEval-2007 challenge, \u201cTask 04: Classification of Semantic Relations between Nominals\u201d (Girju et al., 2009).", "startOffset": 107, "endOffset": 127}, {"referenceID": 42, "context": "Distributional similarity can be estimated either by using contextual information (\u00d3 S\u00e9aghdha & Copestake, 2008), or by exploring grammatical relations between words (Lee, 1999).", "startOffset": 166, "endOffset": 177}, {"referenceID": 49, "context": "For instance, even though, according to dependency grammar theory (Mel\u2019\u010duk, 1988), adjectives do not govern other words, they may still occur in the dependency paths.", "startOffset": 66, "endOffset": 81}, {"referenceID": 4, "context": "Similarly to Bunescu and Mooney\u2019s (2005a) work, in our experiments we use lemma, part of speech tag and direction, but we do not consider entity type or negative polarity of items.", "startOffset": 13, "endOffset": 42}, {"referenceID": 71, "context": "The choice of the scaling value was motivated by the experiments on amino acids in the biological domain (Saigo et al., 2004).", "startOffset": 105, "endOffset": 125}, {"referenceID": 0, "context": ", using the graph kernel by Airola et al., 2008 or the tree kernel by S\u00e6tre et al., 2008). To the best of our knowledge, string kernels have not been applied to dependency paths yet. However, a gap-weighted string kernel (described in Section 2) also allows gapping and can be thus compared to the LA kernel. To test how Lodhi et al.\u2019s (2002) kernel performs on dependency paths, we use it", "startOffset": 28, "endOffset": 343}, {"referenceID": 47, "context": "80 Gap-weighted string kernel (Lodhi et al., 2002) 72.", "startOffset": 30, "endOffset": 50}, {"referenceID": 46, "context": "On both data sets, the LA method using distributional similarity measures significantly outperforms the baselines. Interestingly, the gap-weighted string kernel by Lodhi et al. (2002) yields good performance too and seems to be a better choice than the subsequence", "startOffset": 108, "endOffset": 184}, {"referenceID": 47, "context": "Lodhi et al. (2002) have mentioned in their paper that \u201cthe F1 numbers (with respect to SSK) seem to peak at a subsequence length between 4 and 7\u201d.", "startOffset": 0, "endOffset": 20}, {"referenceID": 28, "context": "kernel based on shallow linguistic information (Giuliano et al., 2006).", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "Airola et al. (2008) apply a graph kernel-based approach to extract interactions and use, among others, the LLL and AImed data sets.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "76 Graph kernel (Airola et al., 2008) 72.", "startOffset": 16, "endOffset": 37}, {"referenceID": 47, "context": "8 Gap-weighted string kernel (Lodhi et al., 2002) 83.", "startOffset": 29, "endOffset": 49}, {"referenceID": 28, "context": "88 Shallow linguistic kernel (Giuliano et al., 2006) 62.", "startOffset": 29, "endOffset": 52}, {"referenceID": 23, "context": "70 Rule-based method (Fundel et al., 2007) 68 83 75", "startOffset": 21, "endOffset": 42}, {"referenceID": 47, "context": "In addition, the gap-weighted string kernel (Lodhi et al., 2002) seems to perform much worse on the test set.", "startOffset": 44, "endOffset": 64}, {"referenceID": 27, "context": "In contrast, the approach reported by Giuliano et al. (2006) does not make use of syntactic information, and on the data subset without coreferences achieves higher recall.", "startOffset": 38, "endOffset": 61}, {"referenceID": 27, "context": "In contrast, the approach reported by Giuliano et al. (2006) does not make use of syntactic information, and on the data subset without coreferences achieves higher recall. On the other hand, lower recall can also be caused by using actual names of proteins and genes as arguments. In the work reported before, the relation arguments and other named entities are often replaced by their types (e.g., PROTEIN) and these are used as input for the learning algorithm. We conducted additional experiments using named entity types in the dependency paths, which led to a great improvement in terms of recall and F-score (Table 7, LLL-coref-LABEL, LLL-nocoref-LABEL, LLL-coref-LABEL). Our method clearly outperforms the shallow linguistic kernel and also achieves better results than the best-performing system in the LLL competition (Sbest), which, according to N\u00e9dellec (2005), applied Markov logic to the syntactic paths.", "startOffset": 38, "endOffset": 873}, {"referenceID": 0, "context": "Airola et al. (2008) do not report on the performance on the LLL data set and, for this reason, information on the graph all-paths kernel is not included in Table 7.", "startOffset": 0, "endOffset": 21}, {"referenceID": 28, "context": "LLL-coref Shallow linguistic kernel (Giuliano et al., 2006) 29.", "startOffset": 36, "endOffset": 59}, {"referenceID": 28, "context": "0 LLL-nocoref Shallow linguistic kernel (Giuliano et al., 2006) 54.", "startOffset": 40, "endOffset": 63}, {"referenceID": 28, "context": "6 LLL-all Shallow linguistic kernel (Giuliano et al., 2006) 56.", "startOffset": 36, "endOffset": 59}, {"referenceID": 47, "context": "6 LLL-all Gap-weighted string kernel (Lodhi et al., 2002) 56.", "startOffset": 37, "endOffset": 57}, {"referenceID": 58, "context": "LLL-all Sbest (N\u00e9dellec, 2005) 60.", "startOffset": 14, "endOffset": 30}, {"referenceID": 63, "context": "The experiments we report here are done using the first setting and can be directly compared against the methods described in the work of S\u00e6tre et al. (2008), Erkan et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 18, "context": "(2008), Erkan et al. (2007) and Giuliano et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": "(2008), Erkan et al. (2007) and Giuliano et al. (2006). In addition, we use the same dependency paths for the LA kernel as the ones employed by S\u00e6tre et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "The results by Airola et al. (2008) and by Bunescu (2007) are obtained by cross-validating on the level of documents.", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": "The results by Airola et al. (2008) and by Bunescu (2007) are obtained by cross-validating on the level of documents.", "startOffset": 15, "endOffset": 58}, {"referenceID": 0, "context": "The results by Airola et al. (2008) and by Bunescu (2007) are obtained by cross-validating on the level of documents. We conducted experiments by setting the distributional measure to Dice, referred to as LA-Dice in Table 8. In the upper part of the table we used dependency paths generated by the Stanford parser and in the lower part those obtained by Enju. As we discussed in Section 2, Erkan et al. (2007) use similarity measures to compare dependency paths, but they do not consider any additional sources whose information can be incorporated into the learning procedure.", "startOffset": 15, "endOffset": 410}, {"referenceID": 0, "context": "The results by Airola et al. (2008) and by Bunescu (2007) are obtained by cross-validating on the level of documents. We conducted experiments by setting the distributional measure to Dice, referred to as LA-Dice in Table 8. In the upper part of the table we used dependency paths generated by the Stanford parser and in the lower part those obtained by Enju. As we discussed in Section 2, Erkan et al. (2007) use similarity measures to compare dependency paths, but they do not consider any additional sources whose information can be incorporated into the learning procedure. They, however, experiment with supervised (SVM) and semi-supervised learning (TSVM), where the number of training instances is varied. Table 8 shows the best performance that was achieved by Erkan et al.\u2019s (2007) method.", "startOffset": 15, "endOffset": 791}, {"referenceID": 0, "context": "The results by Airola et al. (2008) and by Bunescu (2007) are obtained by cross-validating on the level of documents. We conducted experiments by setting the distributional measure to Dice, referred to as LA-Dice in Table 8. In the upper part of the table we used dependency paths generated by the Stanford parser and in the lower part those obtained by Enju. As we discussed in Section 2, Erkan et al. (2007) use similarity measures to compare dependency paths, but they do not consider any additional sources whose information can be incorporated into the learning procedure. They, however, experiment with supervised (SVM) and semi-supervised learning (TSVM), where the number of training instances is varied. Table 8 shows the best performance that was achieved by Erkan et al.\u2019s (2007) method. Among models based on SVM, the one with Cosine distance, SVM-Cos, yields the best results. In the TSVM setting, the one with the Edit measure performs the best. We observe that LA-Dice slightly outperforms both and has, in particular, high precision. In their work, S\u00e6tre et al. (2008) explore several parsers and combinations of features.", "startOffset": 15, "endOffset": 1085}, {"referenceID": 68, "context": "In general, the method by S\u00e6tre et al. also uses SVM, but in this case it focuses on tree kernels (discussed in Section 2.3.3). To make a fair comparison, we conducted experiments on the paths obtained by deep syntactic analysis (Enju parser) and compared our scores against S\u00e6tre et al.\u2019s (2008) results.", "startOffset": 26, "endOffset": 297}, {"referenceID": 4, "context": "02 Baseline I (Bunescu, 2007) Collins 69.", "startOffset": 14, "endOffset": 29}, {"referenceID": 20, "context": "07 SVM-Cos (Erkan et al., 2007) Stanford 61.", "startOffset": 11, "endOffset": 31}, {"referenceID": 20, "context": "09 TSVM-Edit (Erkan et al., 2007) Stanford 59.", "startOffset": 13, "endOffset": 33}, {"referenceID": 47, "context": "96 Gap-weighted string kernel (Lodhi et al., 2002) Stanford 67.", "startOffset": 30, "endOffset": 50}, {"referenceID": 68, "context": "40 Tree kernel (S\u00e6tre et al., 2008) Enju 76.", "startOffset": 15, "endOffset": 35}, {"referenceID": 68, "context": "0 Tree kernel (S\u00e6tre et al., 2008) Enju+KSDEP+W 78.", "startOffset": 14, "endOffset": 34}, {"referenceID": 0, "context": "5 Graph kernel (Airola et al., 2008) Charniak-Lease 52.", "startOffset": 15, "endOffset": 36}, {"referenceID": 28, "context": "4 Shallow linguistic kernel (Giuliano et al., 2006) none 60.", "startOffset": 28, "endOffset": 51}, {"referenceID": 15, "context": "Likewise, the Collins parser is a statistical parser (Collins, 1999).", "startOffset": 53, "endOffset": 68}, {"referenceID": 27, "context": "Many participants of this challenge considered WordNet either explicitly (Tribble & Fahlman, 2007; Kim & Baldwin, 2007), or as a part of a complex system (Giuliano et al., 2007).", "startOffset": 154, "endOffset": 177}, {"referenceID": 32, "context": "Since it is not always obvious how to use WordNet so that it yields the best performance, many researchers have made additional decisions such as use of supersenses (Hendrickx et al., 2007), selection of a predefined number of high-level concepts (Nulty, 2007), or cutting the WordNet hierarchy at a certain level (Bedmar et al.", "startOffset": 165, "endOffset": 189}, {"referenceID": 60, "context": ", 2007), selection of a predefined number of high-level concepts (Nulty, 2007), or cutting the WordNet hierarchy at a certain level (Bedmar et al.", "startOffset": 65, "endOffset": 78}, {"referenceID": 2, "context": ", 2007), selection of a predefined number of high-level concepts (Nulty, 2007), or cutting the WordNet hierarchy at a certain level (Bedmar et al., 2007).", "startOffset": 132, "endOffset": 153}, {"referenceID": 2, "context": ", 2007), selection of a predefined number of high-level concepts (Nulty, 2007), or cutting the WordNet hierarchy at a certain level (Bedmar et al., 2007). Some other systems such as the one by Nakov (2007) were based solely on information collected from the Web.", "startOffset": 133, "endOffset": 206}, {"referenceID": 24, "context": "Note that in the Task 4 overview paper, Girju et al. (2007) reported on three baselines, which, in their case, were (i) guessing \u2018true\u2019 or \u2018false\u2019 for all examples, depending on which class is the majority class in the test set (Baseline III), (ii) always guessing \u2018true\u2019 (Baseline IV), and (iii) guessing \u2018true\u2019 or \u2018false\u2019 with the probability that corresponds to the class distribution in the test set (Baseline V).", "startOffset": 40, "endOffset": 60}, {"referenceID": 1, "context": "Moreover, when compared to the best results of the SemEval-2007 competition (Beamer et al., 2007), our method approaches performance yielded by the best system (bestSV ).", "startOffset": 76, "endOffset": 97}, {"referenceID": 1, "context": "Moreover, when compared to the best results of the SemEval-2007 competition (Beamer et al., 2007), our method approaches performance yielded by the best system (bestSV ). This system used not only various lexical, syntactic, and semantic feature sets, but also expanded the training set by adding examples from many different sources. We have already mentioned in Section 2 that the recent work by \u00d3 S\u00e9aghdha (2009) explores WordNet structure and graph kernels to classify semantic relations.", "startOffset": 77, "endOffset": 416}, {"referenceID": 47, "context": "4 Gap-weighted string kernel (Lodhi et al., 2002) 61.", "startOffset": 29, "endOffset": 49}, {"referenceID": 27, "context": "This finding is in line with the results of Giuliano et al. (2007) whose system was a combination of kernels on the same data.", "startOffset": 44, "endOffset": 67}, {"referenceID": 30, "context": "Some other recent work on the SemEval Task 4 data set includes investigation of distributional kernels (\u00d3 S\u00e9aghdha & Copestake, 2008), pattern clusters (Davidov & Rappoport, 2008), relational similarity (Nakov & Hearst, 2008), and WordNet kernels. Unlike WordNet kernels, the first three approaches do not use WordNet. \u00d3 S\u00e9aghdha and Copestake (2008) report an accuracy of 70.", "startOffset": 212, "endOffset": 351}, {"referenceID": 18, "context": "5 as the best results yielded by distributional kernels and the best performance of Davidov and Rappoport\u2019s (2008) method is an accuracy of 70.", "startOffset": 84, "endOffset": 115}, {"referenceID": 60, "context": "F-score is comparable to the performance reported by \u00d3 S\u00e9aghdha and Copestake (2008) and by Davidov and Rappoport (2008).", "startOffset": 55, "endOffset": 85}, {"referenceID": 18, "context": "F-score is comparable to the performance reported by \u00d3 S\u00e9aghdha and Copestake (2008) and by Davidov and Rappoport (2008).", "startOffset": 92, "endOffset": 121}, {"referenceID": 42, "context": "For instance, Lee (1999) uses them to detect similar nouns based on verb-object co-occurrence pairs.", "startOffset": 14, "endOffset": 25}, {"referenceID": 42, "context": "For instance, Lee (1999) uses them to detect similar nouns based on verb-object co-occurrence pairs. The results suggest the Jaccard coefficient (which is related to the Dice measure) to be one of the best performing measures followed by some others including Cosine. Euclidean distance fell into the group with the largest error rates. Given previous work by Lee (1999), one would expect Euclidean distance to achieve worse results than", "startOffset": 14, "endOffset": 371}, {"referenceID": 3, "context": "For example, Budanitsky and Hirst (2006) use semantic relatedness measures to detect malapropism and show that Jiang and Conrath\u2019s measure (jcn) yields the best results, followed by Lin\u2019s measure (lin), and the one by Leacock and Chodorow (lch), and then by Resnik\u2019s measure (res).", "startOffset": 13, "endOffset": 41}, {"referenceID": 47, "context": "For this reason, we have also applied the gap-weighted string kernel (Lodhi et al., 2002) to all data sets.", "startOffset": 69, "endOffset": 89}, {"referenceID": 0, "context": "The LA kernel also achieves the best performance on the LLL training set, outperforming the graph kernel (Airola et al., 2008), the shallow linguistic kernel (Giuliano et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 28, "context": ", 2008), the shallow linguistic kernel (Giuliano et al., 2006) and the rule-based system by Fundel et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 0, "context": "The LA kernel also achieves the best performance on the LLL training set, outperforming the graph kernel (Airola et al., 2008), the shallow linguistic kernel (Giuliano et al., 2006) and the rule-based system by Fundel et al. (2007). All three have used different input for their methods, varying from plain text to dependency structures.", "startOffset": 106, "endOffset": 232}, {"referenceID": 68, "context": "Two other approaches whose performance has been reported on the AImed data set include the tree kernel (S\u00e6tre et al., 2008) and TSVM (Erkan et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 20, "context": ", 2008) and TSVM (Erkan et al., 2007).", "startOffset": 17, "endOffset": 37}, {"referenceID": 20, "context": ", 2008) and TSVM (Erkan et al., 2007). Both of them explore syntactic information in different ways. While S\u00e6tre et al. consider subtrees, the method of Erkan et al. has more similarities with our approach because it relies on the dependency path comparison. To do this comparison, they only use information already available in the dependency paths (SVM setting), or more dependency paths (TSVM setting). According to Lauer and Bloch (2008), TSVMs fall into the category using prior knowledge by \u2018sampling methods\u2019, because it explores prior knowledge by generating new examples.", "startOffset": 18, "endOffset": 442}, {"referenceID": 46, "context": "In our experiments, the scaling parameter \u03b2 contributes to the overall performance at most, but the other parameters such as gap values have to be taken into account as well. When \u03b2 approaches infinity, the LA kernel approximates the Smith-Waterman distance, but increasing \u03b2 does not necessarily have a positive impact on the final performance. This finding is in line with the results reported by Saigo et al. (2004) on the homology detection task.", "startOffset": 27, "endOffset": 419}, {"referenceID": 83, "context": "It may be interesting to consider relational similarity (Turney, 2006), which looks for the correspondence between relation instances.", "startOffset": 56, "endOffset": 70}, {"referenceID": 52, "context": "Recently, Mohammad (2008) in his thesis investigated the compatibility of distributional measures with ontological ones.", "startOffset": 10, "endOffset": 26}, {"referenceID": 46, "context": "The preliminary version of this work has been dicussed at the 22nd International Conference on Computational Linguistics (CoLing 2008) and at the Seventh International Tbilisi Symposium on Language, Logic and Computation (2007). This work was carried out in the context of the Virtual Laboratory for e-Science project (www.", "startOffset": 109, "endOffset": 228}], "year": 2010, "abstractText": "This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results.", "creator": "TeX"}}}