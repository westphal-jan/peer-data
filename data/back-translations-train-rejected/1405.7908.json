{"id": "1405.7908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2014", "title": "Semantic Composition and Decomposition: From Recognition to Generation", "abstract": "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (\"red salmon\"), generate a noun unigram that is synonymous with the given bigram (\"sockeye\"). A test for semantic decomposition is, given a context vector for a noun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous with the given unigram (\"brandy glass\"). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.", "histories": [["v1", "Fri, 30 May 2014 16:36:07 GMT  (48kb)", "http://arxiv.org/abs/1405.7908v1", "National Research Council Canada - Technical Report"]], "COMMENTS": "National Research Council Canada - Technical Report", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["peter d turney"], "accepted": false, "id": "1405.7908"}, "pdf": {"name": "1405.7908.pdf", "metadata": {"source": "CRF", "title": "Semantic Composition and Decomposition: From Recognition to Generation", "authors": ["Peter D. Turney"], "emails": ["peter.turney@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 140 5.79 08v1 [cs.CL] 3 0M ay2 01"}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of us do not know what to do, and that they do not know what to do, and that they do not know what to do, and that they do not know why they should do what they should do. \"\" I do not believe that they should do what they should do, \"he says.\" I do not believe that they should do what they should do. \"\" \"No.\" \"\" No. \"\" \"No.\" \"No.\" No. \"\" No. \"\" No. \"\" No. \"\" No. \"No.\" \"No.\" No. \"\" No. \"No.\" \"No.\" No. \"\" No. \"No.\" \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \".\" No. \".\" No. \".\""}, {"heading": "2. Related Work", "text": "We first examine related work on paraphrase in general, and then work on composition and decomposition."}, {"heading": "2.1 Paraphrasing", "text": "Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present in-depth surveys of data-rich approaches to paraphrasing. Paraphrases can also be generated using knowledge-based techniques, but our emphasis here is on corpus-based methods, as they generally require much less human effort than knowledge-based techniques. Generally, corpus-based approaches to paraphrasing have broadened the distribution hypothesis of words to phrases. The extended distribution hypothesis is that phrases occurring in similar contexts tend to have similar meanings (Lin & Pantel, 2001). Consider, for example, the following text fragments (Pasca & Dienes, 2005): \u2022 1989, when Soviet troops withdrew from Afghanistan, when Soviet troops withdrew from Afghanistan."}, {"heading": "2.2 Composition", "text": "Let's have a choice of words and the remaining 20% of the meaning of a bigramtschi (2001 and 2009). Let's assume that we have context vectors a and b that represent the terms a and b. One of the earliest suggestions for a semantic composition is to use the cosine of the angle between a + b and the context vector c for c.This simple proposal actually works relatively well (Mitchell & Lapata, 2008, 2010), although it lacks the sensitivity of the order. Since one + b + a, animal farmer and agricultural animal has the same representation, one type of farm and the other is a type of animal. Landauer (2002) estimates that 80% of the meaning of the English text comes from choice of words and the remaining 20% of the word choice, so measures at least 20% of the meaning of a bigramtsch.Kinelli (2001 and 2009)."}, {"heading": "2.3 Decomposition", "text": "Collins-Thompson and Callan (2007) describe the definition task in which human subjects are asked to generate a short definition of a specific target word in order to evaluate their understanding of the word. They propose an algorithm for automatically evaluating the responses of human subjects. Their algorithm measures the semantic similarity between a gold standard reference definition given by a human expert and the definition of the human subject. The semantic similarity is based on a Markov chain. Kartsaklis et al. (2012) created a data set of 72 target terms and 40 target verbs unigrams.4 Each target unigram has three gold standard definitions, with the definitions containing an average of 2.4 words (the majority being bigrams). Table 3 shows the first four target terms and their corresponding definitions. Kartsaklis et al. (2012) treat each target term (unigram) as a class term and evaluate each one based on its classification, which is generated by its accuracy."}, {"heading": "3. Datasets", "text": "In this section we present the four datasets used in the experiments, the two standard datasets are based on WordNet synonym sets, the two standard datasets are based on the holistic approach of Bigrams, the two standard datasets are based on the two standard datasets selected from the 2180 different datasets, the two standard datasets are derived from the 2180 selected datasets used in previous work (Turney, 2012) to test the composition of the characters (see Table 1 in Section 2.2), the original datasets include both compositional bigrams and idiomatic (non-compositional) bigrams. Regarding the difficulty of the generation problem, relative to the recognition problem, we decided to make the datasets easier by avoiding idiomatic bigrams. We used WordNet glosses as heuristic clues for searching for compositional bigrams."}, {"heading": "4. Features", "text": "Comp, Decomp and Super use five types of attributes to rank candidates. These attributes are functions that use uni or pseudo-unigrams as arguments and return real values. Tables 6 and 7 list the five functions and their arguments. DS and FS were introduced in Turney (2012) and supplemented by LUF and PPMI in Turney (2013)."}, {"heading": "4.1 LUF: Log Unigram Frequency", "text": "We define LUF (a) as a log (UF (a) + 1. We add one to the frequency because log (0) is not defined. If a is not in the Waterloo corpus, UF (a) is zero and hence LUF (a) is also zero. For a pseudo-unigram, a b is UF (a) the frequency of the bigram in the Waterloo corpus. We calculate UF (a) for the 73,000 unigrams and 36,000 bigrams in WordNet, so LUF (a) covers 73,000 unigrams and 36,000 pseudo-unigrams. In our experiments, we use a hash table to quickly obtain LUF (a).6"}, {"heading": "4.2 LBF: Log Bigram Frequency", "text": "For LBF (a, b) we use the Google Web 1T 5-gram (Web1T5) dataset (Brants & Franz, 2006).7 Let BF (a, b) be the frequency of the bigram in the Web1T5 dataset, let us define LBF (a, b) as the log (BF (a, b) + 1. For a pseudo-unigram, a b, BF (a b, c) is the frequency of the abc trigram in the Web1T5 dataset, and BF (c, a b) is the frequency of the trigram booth. In our experiments, we never have to calculate BF if both arguments are pseudo-unigrams, BF (a b, c d), so that we do not have to work with 4-grams."}, {"heading": "4.3 PPMI: Positive Pointwise Mutual Information", "text": "In fact, it is as if most people are able to survive themselves, and that they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "4.4 DS: Domain Similarity", "text": "The following experiments use the Turney domain matrix (2012).11 To capture the domain matrix, Turney (2012) first constructed a frequency matrix in which the lines correspond to the n-grammars in WordNet and the columns observed near the n-grammars in the Waterloo corpus.The hypothesis was that the nouns near a term characterize the topics associated with the term. In the face of an n-grammar, a-grammar, and Waterloo corpus, phrases containing one and the phrases were processed."}, {"heading": "4.5 FS: Function Similarity", "text": "The following experiments use the function matrix of Turney (2012).13 It is similar to the domain matrix except that the context is based on verbal patterns rather than nearby nouns. The hypothesis was that the functional role of a word is characterized by the patterns that correlate the word with nearby verbs. The word context frequency matrix for functional space has about 114,000 lines (WordNet terms) and 50,000 columns (verb pattern contexts, functional roles) with a density of 1.2%. The frequency matrix was converted into a PPMI matrix and smoothed out with SVD.14. The similarity of two words in functional space, FS (a, b, k, p), is calculated by extracting the row vectors in ocposition p k corresponding to n-grammes a and b, and then the equation."}, {"heading": "5. Algorithms", "text": "In this section, the Comp, Decomp and Super algorithms are introduced. Comp and Decomp use fast, unattended algorithms to create initial candidate lists (Turney, 2012). Super uses a slower, monitored algorithm to refine the first lists of Comp or Decomp (Turney, 2013). Super processes both the composition and the decomposition using the same algorithm, but requires different training records and creates different models for the two different tasks. For the composition, Comp uses a Bigram-ab as input and generates a ranking list of Maxu unigrams as output. Super then takes the Bigram and the list of Maxu unigrams as input and generates a new ranking list of the list as output. For the decomposition, Decomp takes a Unigram a as input and generates a ranking list of Maxb bigrams as output. Super then takes the Unigram and the list of Maxb bigrams as output. For the decomposition, Decomp uses a Unigram and a Unigram as output."}, {"heading": "5.1 Comp", "text": "The results are based on the geometric mean of DS (domain similarity) and FS (functional similarity). The geometric mean is only suitable for positive numbers, but DS and FS can be negative. We suppress these negative values for Comp and Decomp, but we allow them for the geometric mean of DS (domain similarity) and FS (functional similarity). The geometric mean is only suitable for positive numbers, but DS and FS can be negative. We suppress these negative values for Comp and Decomp, but we allow them for the Supervised learning algorithm should be able to make use of them. The geometric mean is only suitable for positive numbers, but DS and FS can be negative."}, {"heading": "5.2 Decomp", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "5.3 Super", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to change the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they"}, {"heading": "6. Experiments with Composition", "text": "This section describes the experiments with Comp and Super on the standard and integral composition datasets."}, {"heading": "6.1 Evaluation with the Standard Composition Dataset", "text": "Table 12 uses the target bigram red salmon to illustrate the process of comp and super. (This is the first target in the standard composition test record; see Table 4.) The solution unigram, sockeye, is marked with an asterisk. Among the first 2000 candidates in the issue of comp, sockeye is ranked 445, with a score of 0.167, according to Equation 10. If the 2000 candidates from comp are rescored by super, sockeye rises to third place, with a score of 0.5, according to the SVM model. Table 13 summarizes the performance of comp and super on the standard composition test record, with 351 target bigrams. The line with the mean of the first correct answer is, if any, in the first row of 2000 guesses. The line with the mean of 3 percent is the correct solution 15. Weka is available at http: / / www.cs.waikato.ac.nz / we.ka /.an rank is the median percentage when we do not give the correct percentage of the first row."}, {"heading": "6.2 Comparison with Baselines", "text": "We compare comp and super matrix with three baselines, the best performance in Turney (2012), which we have used for optimization. We compare comp and super matrix with three baselines, vector addition, elemental multiplication and holistic approach. We compare comp and super matrix with three baselines, vector addition, element coupling and optimization optimization. We compare the results of optimization with the holistic approach, element coupling and the holistic approach of optimization, and the holistic approach of optimization optimization, and holistic. with vector addition, we will triple multiplication by cos (a, b, c) (Mitchell & Lapata, 2008, 2010), the cosinistic approach of the angle between a + b and c (Landauer & Dumais, 1997). We will treat the context vector matector for the pseudo-unigram matrix, which is used for this triple vector (see section 4 of Adcos)."}, {"heading": "6.3 Evaluation with the Holistic Composition Dataset", "text": "Considering the standard dataset, we learn from 5,580 triples (see Section 5.3), derived from 154 WordNet synonym sets (see Table 5). Super learns from the expertise embedded in WordNet. However, we would like to be able to train Super without using this kind of expertise.Previous work with adjective-noun bigrams has shown that we can use holistic Bigram vectors to learn a holistic regression model (Guevara, 2010; Baroni & Zamparelli, 2010).Turney (2013) applied this approach to higher-level datasets for composition identification, and the purpose of the two holistic datasets (see Section 3) is to investigate whether we can apply holistic training to the composition and decomposition of the generation.The holistic datasets were used without the use of WordNet synonym sets-sets-to generate the WordNet as the source."}, {"heading": "6.4 Evaluation with the Adjective-Noun Dataset", "text": "Here we experiment with the 620 adjective-noun phrases of Dinu et al. (2013) (see Table 2 in Section 2.2).16 Table 17 shows the performance of Comp and Super on the adjective-noun16. Georgiana Dinu, Nghia The Pham, and Marco Baroni gave us a copy of their 620 adjective-noun phrases, which are the corresponding noun phrases for each adjective-nouns.dataset. For the training, we merged the standard trainings and tests datasets (154 + 351 = 505 targets) and removed all target-noun phrases that appeared in the adjective-noun phrase, and their vocabularies. For testing, we used all 620 adjective-noun phrases. There is a slight drop in performance with the adjective-noun phrases."}, {"heading": "7. Experiments with Decomposition", "text": "This section describes the experiments with Decomp and Super on the standard and integral decomposition datasets."}, {"heading": "7.1 Evaluation with the Standard Decomposition Dataset", "text": "Decomp first builds a list of maxm modifiers using scorem and a list of maxh modifiers using scoreh. Table 20 illustrates this first step using the target unigram sockeye as an example. Table 4 shows that sockeye has several possible solution bigrams: blue salmon, red salmon, sockeye salmon and Oncorhynchus nerka. In perspective, the solution that will eventually be found is sockeye salmon. In Table 20, Sockeye ranks first in the list of candidate modifiers and salmon sixth in the list of candidate heads. (They are marked with asterisks in the table: Onchisks) Next, Decomp builds a list of maxb bigrams with scoreb, while the combination of individual maxm modifiers and salmon are sixth in the list of candidate modifiers."}, {"heading": "7.2 Comparison with Baselines", "text": "As with the composition (Section 6.2), we compare Decomp and Super with three baselines, vector addition, elementary multiplication, and the holistic approach. Addition and multiplication take the output of Decomp and rescore it. The holistic approach evaluates all the bigrams in WordNet and treats the bigrams as pseudo-unigrams. Table 23 shows the results. Addition and multiplication perform significantly worse than Decomp and Super by percent in the top 1, top 10, and top 100 (Fishers Exact Test, 95% confidence level). The holistic approach performs significantly better than Decomp and Super by percent in the top 1, top 10, and top 100 (Fishers Exact Test, 95% confidence level). Note that the holistic approach can only decompose a unigram to a bigram if the bigram is one of the 36,000 bigrams in WordNet, while Super Decomp and Igrip can decompile."}, {"heading": "7.3 Evaluation with the Holistic Decomposition Dataset", "text": "As with the composition task (see Table 15), the results show that the holistic dataset is not nearly as difficult as the WordNet-based dataset. Table 26 provides results for various combinations of test and training datasets. Here, we see some benefit in merging the two training datasets using the domain fitting algorithm of thumb \u0301 III (2007) (see both columns). However, if we focus on percentages in the top 100, the differences between the training with the standard dataset and the merged dataset (80.3% versus 82.3% for holistic tests and 50.7% versus 52.7% for standard tests) are not significant."}, {"heading": "8. Discussion and Analysis", "text": "In Section 6.1, we evaluated Comp and Super by comparing their results with WordNet synonym sets. For the Bigrams in the standard Compositon database, the top 100 ranked Unikates are a WordNet synonym 77.8% of the time (Table 13). In Section 7.1, we evaluated Decomp and Super with WordNet, where the top 100 Bigrams include a WordNet synonym 50.7% of the time (Table 22). Together, we explore 73,000 candidate unigram compositions for a Bigramm. Decomp and Super Exploration Exploration Dataset Dataset are the top 100 candidate Bigrams decomposition data for a Word synonym 50.7% of the time (Table 22). Comp and Super Exploration Exploration Exploration Exploration Exploration differences for a Bigramm. Decomp and Super Exploration Dataset Dataset are the top 100 candidate decomposition data for a Word synonym Exploration Exploration 5.7% of the time (Table 22). Comp and Super Exploration Exploration Exploration Exploration Exploration Exploration Exploration Exploration Dataset are the top candidates for a Bigram Exploration Dataset."}, {"heading": "9. Limitations and Future Work", "text": "The main limitation of the work described here is that we have focused our attention on noun-modifier-bigrams and noun-unique specimens. A natural next step would be to consider subject-verb-bigrams, verb-object-bigrams, or subject-verb-object-trigrams. We plan to extend this approach to detection when a sentence is a paraphrase paraphrase of another sentence. To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Finally, we hope to be able to go beyond sentence-paraphrase recognition to generate sentence-paraphrase generation. Turney (2013) has shown that the features used here (Section 4) work well to detect semantic relationships; for example, we can see that the semantic relationship between mason and stone is an analogy that is comparable to the semantic relationship between table and wood."}, {"heading": "10. Conclusions", "text": "The main weakness of the recognition task is that the degree of challenge in the task depends on the given list of candidates. A critic can always claim that the given list was too simple; the distraction factors were too different from the solution. The generational task avoids this criticism. Furthermore, the ability to generate solutions opens up opportunities for new applications, beyond the applications that are possible with the ability to detect solutions. Following the example of Dinu et al. (2013) and Li et al. (2014), we have advanced distribution models of composition from the task of recognition to the task of generation. Our results on their adjective noun dataset suggest that the similarity composition is a better approach to generating compositions as context composition (Section 6.4). Experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al. (2014) are limited by their dependence on holistic education."}, {"heading": "Acknowledgments", "text": "Thanks to Charles Clarke and Egidio Terra for sharing the Waterloo corpus. Thanks to my colleagues at the NRC for providing many helpful comments during a presentation of this research. Thanks to Georgiana Dinu, Nghia The Pham and Marco Baroni for sharing their noun adjective dataset and vocabulary of 21,000 nouns."}], "references": [{"title": "A survey of paraphrasing and textual entailment methods", "author": ["I. Androutsopoulos", "P. Malakasiotis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Androutsopoulos and Malakasiotis,? \\Q2010\\E", "shortCiteRegEx": "Androutsopoulos and Malakasiotis", "year": 2010}, {"title": "Distributional memory: A general framework for corpusbased semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni and Lenci,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Lenci", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Baroni and Zamparelli,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "author": ["G. Bouma"], "venue": "Proceedings of the Biennial Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2009), pp. 31\u201340.", "citeRegEx": "Bouma,? 2009", "shortCiteRegEx": "Bouma", "year": 2009}, {"title": "Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia", "author": ["T. Brants", "A. Franz"], "venue": null, "citeRegEx": "Brants and Franz,? \\Q2006\\E", "shortCiteRegEx": "Brants and Franz", "year": 2006}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: Stop-lists, stemming, and SVD", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2012}, {"title": "Experiments with LSA scoring: Optimal rank and basis", "author": ["J. Caron"], "venue": "In Proceedings of the SIAM Computational Information Retrieval Workshop,", "citeRegEx": "Caron,? \\Q2001\\E", "shortCiteRegEx": "Caron", "year": 2001}, {"title": "The Logical Structure of Linguistic Theory", "author": ["N. Chomsky"], "venue": "Plenum Press.", "citeRegEx": "Chomsky,? 1975", "shortCiteRegEx": "Chomsky", "year": 1975}, {"title": "Word association norms, mutual information, and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,", "citeRegEx": "Church and Hanks,? \\Q1989\\E", "shortCiteRegEx": "Church and Hanks", "year": 1989}, {"title": "A compositional distributional model of meaning", "author": ["S. Clark", "B. Coecke", "M. Sadrzadeh"], "venue": "In Proceedings of the 2nd Symposium on Quantum Interaction,", "citeRegEx": "Clark et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Combining symbolic and distributional models of meaning", "author": ["S. Clark", "S. Pulman"], "venue": "In Proceedings of the AAAI Spring Symposium on Quantum Interaction,", "citeRegEx": "Clark and Pulman,? \\Q2007\\E", "shortCiteRegEx": "Clark and Pulman", "year": 2007}, {"title": "Automatic and human scoring of word definition responses", "author": ["K. Collins-Thompson", "J. Callan"], "venue": "In Proceedings of the HLT-NAACL", "citeRegEx": "Collins.Thompson and Callan,? \\Q2007\\E", "shortCiteRegEx": "Collins.Thompson and Callan", "year": 2007}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 256\u2013263, Prague, Czech Republic.", "citeRegEx": "Daum\u00e9,? 2007", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "How to make words with vectors: Phrase generation in distributional semantics", "author": ["G. Dinu", "M. Baroni"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Dinu and Baroni,? \\Q2014\\E", "shortCiteRegEx": "Dinu and Baroni", "year": 2014}, {"title": "General estimation and evaluation of compositional distributional semantic models", "author": ["G. Dinu", "N.T. Pham", "M. Baroni"], "venue": "In Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality (CVSC", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Towards a semantics for distributional representations", "author": ["K. Erk"], "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), Potsdam, Germany.", "citeRegEx": "Erk,? 2013", "shortCiteRegEx": "Erk", "year": 2013}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["J.R. Firth"], "venue": "Studies in Linguistic Analysis, pp. 1\u201332. Blackwell, Oxford.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "The Compositionality Papers", "author": ["J. Fodor", "E. Lepore"], "venue": null, "citeRegEx": "Fodor and Lepore,? \\Q2002\\E", "shortCiteRegEx": "Fodor and Lepore", "year": 2002}, {"title": "Experimenting with transitive verbs in a DisCoCat", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics", "citeRegEx": "Grefenstette and Sadrzadeh,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh", "year": 2011}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["E. Guevara"], "venue": "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS 2010), pp. 33\u201337.", "citeRegEx": "Guevara,? 2010", "shortCiteRegEx": "Guevara", "year": 2010}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, 10 (23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "The class imbalance problem: A systematic study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Japkowicz and Stephen,? \\Q2002\\E", "shortCiteRegEx": "Japkowicz and Stephen", "year": 2002}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["D. Kartsaklis", "M. Sadrzadeh", "S. Pulman"], "venue": "In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters,", "citeRegEx": "Kartsaklis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Predication", "author": ["W. Kintsch"], "venue": "Cognitive Science, 25 (2), 173\u2013202.", "citeRegEx": "Kintsch,? 2001", "shortCiteRegEx": "Kintsch", "year": 2001}, {"title": "On the computational basis of learning and cognition: Arguments from LSA", "author": ["T.K. Landauer"], "venue": "Ross, B. H. (Ed.), The Psychology of Learning and Motivation: Advances in Research and Theory, Vol. 41, pp. 43\u201384. Academic Press.", "citeRegEx": "Landauer,? 2002", "shortCiteRegEx": "Landauer", "year": 2002}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Improving the lexical function composition model with pathwise optimized elastic-net regression", "author": ["J. Li", "M. Baroni", "G. Dinu"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "DIRT \u2013 discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": "In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "Generating phrasal and sentential paraphrases: A survey of data-driven methods", "author": ["N. Madnani", "B. Dorr"], "venue": "Computational Linguistics,", "citeRegEx": "Madnani and Dorr,? \\Q2010\\E", "shortCiteRegEx": "Madnani and Dorr", "year": 2010}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Mitchell and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "Aligning needles in a haystack: Paraphrase acquisition across the web", "author": ["M. Pasca", "P. Dienes"], "venue": "In Proceedings of the 2005 International Joint Conference on Natural Language Processing (IJCNLP", "citeRegEx": "Pasca and Dienes,? \\Q2005\\E", "shortCiteRegEx": "Pasca and Dienes", "year": 2005}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods: Support Vector Learning, pp. 185\u2013208, Cambridge, MA. MIT Press.", "citeRegEx": "Platt,? 1998", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research, 44, 533\u2013585.", "citeRegEx": "Turney,? 2012", "shortCiteRegEx": "Turney", "year": 2012}, {"title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "author": ["P.D. Turney"], "venue": "Transactions of the Association for Computational Linguistics, 1, 353\u2013366.", "citeRegEx": "Turney,? 2013", "shortCiteRegEx": "Turney", "year": 2013}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Turney et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Computational semantics of noun compounds in a semantic space model", "author": ["A. Utsumi"], "venue": "Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09), pp. 1568\u20131573.", "citeRegEx": "Utsumi,? 2009", "shortCiteRegEx": "Utsumi", "year": 2009}, {"title": "Semantic vector products: Some initial investigations", "author": ["D. Widdows"], "venue": "Proceedings of the 2nd Symposium on Quantum Interaction, Oxford, UK.", "citeRegEx": "Widdows,? 2008", "shortCiteRegEx": "Widdows", "year": 2008}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Third Edition", "author": ["I.H. Witten", "E. Frank", "M.A. Hall"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Distributional semantics is based on the hypothesis that words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957).", "startOffset": 124, "endOffset": 151}, {"referenceID": 17, "context": "Distributional semantics is based on the hypothesis that words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957).", "startOffset": 124, "endOffset": 151}, {"referenceID": 16, "context": "Much recent work has been concerned with the problem of extending vector space models beyond words, to phrases and sentences (Erk, 2013).", "startOffset": 125, "endOffset": 136}, {"referenceID": 36, "context": "The approach that has worked with words does not scale up to phrases and sentences, due to data sparsity and growth in model size (Turney, 2012).", "startOffset": 130, "endOffset": 144}, {"referenceID": 36, "context": "bigram and a list of candidate unigrams, can the model recognize the synonymous unigram among the candidates (Turney, 2012)? Given a target unigram and a list of candidate bigrams, can the model recognize the synonymous bigram among the candidates (Kartsaklis, Sadrzadeh, & Pulman, 2012)? Many noun unigrams have synonymous noun-modifier bigrams.", "startOffset": 109, "endOffset": 123}, {"referenceID": 35, "context": "bigram and a list of candidate unigrams, can the model recognize the synonymous unigram among the candidates (Turney, 2012)? Given a target unigram and a list of candidate bigrams, can the model recognize the synonymous bigram among the candidates (Kartsaklis, Sadrzadeh, & Pulman, 2012)? Many noun unigrams have synonymous noun-modifier bigrams. For example, the WordNet synonym set for contrabass is bass fiddle, bass viol, bull fiddle, double bass, contrabass, and string bass.1 Each of the bigrams may be viewed as a decomposition of the unigram contrabass into parts (factors, aspects, constituents). For example, the bigram bass fiddle expresses that a contrabass is a type of fiddle that covers the bass range; the head noun fiddle expresses the type aspect of contrabass and the modifier bass expresses the range aspect of contrabass. Decomposition (unigram \u2192 bigram) maps a word (contrabass) to a (miniature) definition (bass fiddle), whereas composition (bigram \u2192 unigram) maps a definition to a word (by composing the components of the definition into a whole). Past work has achieved promising results on recognizing noun-modifier compositions and decompositions, given relatively short lists of candidates. For example, Turney (2013) achieves 75.", "startOffset": 110, "endOffset": 1247}, {"referenceID": 23, "context": "9% accuracy on noun-modifier composition recognition, given seven candidate unigrams for each target bigram, and Kartsaklis et al. (2012) achieve 24% accuracy on noun decomposition recognition, given seventy-two choices.", "startOffset": 113, "endOffset": 138}, {"referenceID": 36, "context": "Comp and Decomp are variations on the unsupervised learning algorithm of Turney (2012). Super is based on the supervised algorithm of Turney (2013).", "startOffset": 73, "endOffset": 87}, {"referenceID": 36, "context": "Comp and Decomp are variations on the unsupervised learning algorithm of Turney (2012). Super is based on the supervised algorithm of Turney (2013). These algorithms were originally designed for the recognition task.", "startOffset": 73, "endOffset": 148}, {"referenceID": 28, "context": "Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present thorough surveys of datadriven approaches to paraphrasing.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present thorough surveys of datadriven approaches to paraphrasing.", "startOffset": 28, "endOffset": 68}, {"referenceID": 36, "context": "We call this the holistic (non-compositional) approach to paraphrase (Turney, 2012), because the phrases are treated as opaque wholes.", "startOffset": 69, "endOffset": 83}, {"referenceID": 8, "context": "Holistic approaches to paraphrase do not address the creative power of language (Chomsky, 1975; Fodor & Lepore, 2002).", "startOffset": 80, "endOffset": 117}, {"referenceID": 24, "context": "One of the earliest proposals for semantic composition is to represent the bigram ab by the vector sum a+ b (Landauer & Dumais, 1997). To measure the similarity of a noun-modifier phrase, ab, and a noun, c, we calculate the cosine of the angle between a+ b and the context vector c for c. This simple proposal actually works relatively well (Mitchell & Lapata, 2008, 2010), although it lacks order sensitivity. Since a+b = b+ a, animal farm and farm animal have the same representation, although one is a type of farm and the other is a type of animal. Landauer (2002) estimates that 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order, thus vector addition misses at least 20% of the meaning of a bigram.", "startOffset": 109, "endOffset": 569}, {"referenceID": 24, "context": "Kintsch (2001) and Utsumi (2009) propose variations of additive composition in which ab is represented by a+b+ \u2211", "startOffset": 0, "endOffset": 15}, {"referenceID": 24, "context": "Kintsch (2001) and Utsumi (2009) propose variations of additive composition in which ab is represented by a+b+ \u2211", "startOffset": 0, "endOffset": 33}, {"referenceID": 36, "context": "This approach does not scale up, but it does work well for a predetermined small set of high frequency n-grams (Turney, 2012).", "startOffset": 111, "endOffset": 125}, {"referenceID": 20, "context": "For example, a regression model can be trained to map the context vectors a and b to the holistic context vector for ab (Guevara, 2010).", "startOffset": 120, "endOffset": 135}, {"referenceID": 36, "context": "Recently there have been several overviews of this topic (Mitchell & Lapata, 2010; Turney, 2012; Erk, 2013).", "startOffset": 57, "endOffset": 107}, {"referenceID": 16, "context": "Recently there have been several overviews of this topic (Mitchell & Lapata, 2010; Turney, 2012; Erk, 2013).", "startOffset": 57, "endOffset": 107}, {"referenceID": 41, "context": "Most of the proposed extensions to distributional semantics involve operations from linear algebra, such as tensor products (Clark & Pulman, 2007; Widdows, 2008; Clark, Coecke, & Sadrzadeh, 2008; Grefenstette & Sadrzadeh, 2011).", "startOffset": 124, "endOffset": 227}, {"referenceID": 27, "context": "Mitchell and Lapata (2010) found that a simple additive model peformed better than an additive model that included neighbours.", "startOffset": 0, "endOffset": 27}, {"referenceID": 18, "context": "Guevara (2010) and Baroni and Zamparelli (2010) point out that a small set of bigrams with holistic context vectors can be used to train a regression model.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Guevara (2010) and Baroni and Zamparelli (2010) point out that a small set of bigrams with holistic context vectors can be used to train a regression model.", "startOffset": 19, "endOffset": 48}, {"referenceID": 41, "context": "Much work focuses on finding the right f for various types of semantic composition (Clark & Pulman, 2007; Widdows, 2008; Mitchell & Lapata, 2008, 2010; Guevara, 2010; Grefenstette & Sadrzadeh, 2011).", "startOffset": 83, "endOffset": 198}, {"referenceID": 20, "context": "Much work focuses on finding the right f for various types of semantic composition (Clark & Pulman, 2007; Widdows, 2008; Mitchell & Lapata, 2008, 2010; Guevara, 2010; Grefenstette & Sadrzadeh, 2011).", "startOffset": 83, "endOffset": 198}, {"referenceID": 34, "context": "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition.", "startOffset": 0, "endOffset": 21}, {"referenceID": 34, "context": "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.", "startOffset": 0, "endOffset": 283}, {"referenceID": 34, "context": "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.3 Table 1 shows one of the questions. The stem is the target noun-modifier bigram and there are seven candidate unigrams. These questions were generated automatically from WordNet. The stem and the solution always belong to the same WordNet synonym set. The intention is to evaluate a proposed similarity measure, sims(ab, c), by its accuracy on the 1500 testing questions. Using an unsupervised learning algorithm and a hand-built function for f , Turney (2012) achieved an accuracy of 58.", "startOffset": 0, "endOffset": 866}, {"referenceID": 34, "context": "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.3 Table 1 shows one of the questions. The stem is the target noun-modifier bigram and there are seven candidate unigrams. These questions were generated automatically from WordNet. The stem and the solution always belong to the same WordNet synonym set. The intention is to evaluate a proposed similarity measure, sims(ab, c), by its accuracy on the 1500 testing questions. Using an unsupervised learning algorithm and a hand-built function for f , Turney (2012) achieved an accuracy of 58.3% on the 1500 testing questions. Vector addition reached 50.1% and element-wise multiplication attained 57.5%. Turney (2013) used supervised learning to", "startOffset": 0, "endOffset": 1019}, {"referenceID": 36, "context": "Table 1: A seven-choice noun-modifier question based on WordNet (Turney, 2012).", "startOffset": 64, "endOffset": 78}, {"referenceID": 15, "context": "Table 2: Adjective-noun composition problems based on WordNet (Dinu et al., 2013).", "startOffset": 62, "endOffset": 81}, {"referenceID": 15, "context": "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset. The performance of the models was measured by the medians of the ranks of the solution nouns in the ranked lists of 21,000 candidates. Our experiments (Sections 6 and 7) use the same general approach to evaluation of models. In a noun-modifier phrase, the modifier may be either a noun or an adjective; therefore adjective-noun phrases are a subset of noun-modifier phrases. Dinu et al. (2013) hypothesize that adjectives are functions that map nouns onto modified nouns (Baroni & Zamparelli, 2010), thus they believe that noun-noun phrases and adjective-noun phrases should have different kinds of models.", "startOffset": 0, "endOffset": 479}, {"referenceID": 15, "context": "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset. The performance of the models was measured by the medians of the ranks of the solution nouns in the ranked lists of 21,000 candidates. Our experiments (Sections 6 and 7) use the same general approach to evaluation of models. In a noun-modifier phrase, the modifier may be either a noun or an adjective; therefore adjective-noun phrases are a subset of noun-modifier phrases. Dinu et al. (2013) hypothesize that adjectives are functions that map nouns onto modified nouns (Baroni & Zamparelli, 2010), thus they believe that noun-noun phrases and adjective-noun phrases should have different kinds of models. The models we present here (Section 5) treat all noun-modifiers the same way, hence our datasets contain both noun-noun phrases and adjective-noun phrases. For comparison, we will also evaluate our models on Dinu et al.\u2019s (2013) adjective-noun dataset (Section 6.", "startOffset": 0, "endOffset": 921}, {"referenceID": 23, "context": "Table 3: A sample of the dataset for the term-definition task (Kartsaklis et al., 2012).", "startOffset": 62, "endOffset": 87}, {"referenceID": 22, "context": "Kartsaklis et al. (2012) created a dataset with 72 target noun unigrams and 40 target verb unigrams.", "startOffset": 0, "endOffset": 25}, {"referenceID": 22, "context": "Kartsaklis et al. (2012) created a dataset with 72 target noun unigrams and 40 target verb unigrams.4 Each target unigram has three gold standard definitions, where the definitions contain 2.4 words on average (the majority are bigrams). Table 3 shows the first four target terms and their corresponding definitions. Kartsaklis et al. (2012) treat each target term (unigram) as a class label and evaluate each model by its accuracy on classifying the definitions.", "startOffset": 0, "endOffset": 342}, {"referenceID": 14, "context": "Recently Dinu and Baroni (2014) partially addressed the task of decomposing a noun unigram into an adjective-noun bigram.", "startOffset": 9, "endOffset": 32}, {"referenceID": 36, "context": "5 The two standard datasets are derived from the 2180 seven-choice noun-modifier questions used in previous work (Turney, 2012) to test composition recognition (see Table 1 in Section 2.", "startOffset": 113, "endOffset": 127}, {"referenceID": 19, "context": "This idea was inspired by Guevara (2010) and Baroni and Zamparelli (2010), who used holistic vectors to train regression models.", "startOffset": 26, "endOffset": 41}, {"referenceID": 2, "context": "This idea was inspired by Guevara (2010) and Baroni and Zamparelli (2010), who used holistic vectors to train regression models.", "startOffset": 45, "endOffset": 74}, {"referenceID": 36, "context": "The features DS and FS were introduced in Turney (2012). These two features were supplemented with LUF and PPMI in Turney (2013).", "startOffset": 42, "endOffset": 56}, {"referenceID": 36, "context": "The features DS and FS were introduced in Turney (2012). These two features were supplemented with LUF and PPMI in Turney (2013). The feature LBF is a new addition to the group.", "startOffset": 42, "endOffset": 129}, {"referenceID": 3, "context": "(Another approach to normalizing PPMI is given in Bouma (2009).)", "startOffset": 50, "endOffset": 63}, {"referenceID": 36, "context": "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).", "startOffset": 75, "endOffset": 100}, {"referenceID": 36, "context": "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).9 It is a word-context matrix in which the rows correspond to n-grams in WordNet and the columns correspond to unigrams from WordNet, marked left or right.", "startOffset": 75, "endOffset": 199}, {"referenceID": 36, "context": "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).9 It is a word-context matrix in which the rows correspond to n-grams in WordNet and the columns correspond to unigrams from WordNet, marked left or right. There are approximately 114,000 rows in the matrix and 140,000 columns. The matrix has a density of about 1.2%. Let a be an n-gram in WordNet, b be a unigram in WordNet, and h be either left or right. Suppose that a corresponds to the i-th row in the matrix and b, marked with the handedness h, corresponds to the j-th column in the matrix. We define PPMI(a, b, h) as the value in the i-th row and j-th column of the matrix. This value is the normalized positive pointwise mutual information of observing b on the h side of a in the Waterloo corpus, where b is either immediately adjacent to a or separated from a by one or more stop words.10 Any word that is not in WordNet is treated as a stop word. If a does not correspond to a row in the matrix or b (marked h) does not correspond to column, then PPMI(a, b, h) is assigned the value zero. Turney et al. (2011) estimated PPMI(a, b, h) by sampling the Waterloo corpus for phrases containing a and then looking for b on the h side of a in the sampled phrases.", "startOffset": 75, "endOffset": 1220}, {"referenceID": 36, "context": "The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus.", "startOffset": 53, "endOffset": 67}, {"referenceID": 36, "context": "The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus.", "startOffset": 53, "endOffset": 111}, {"referenceID": 36, "context": "Turney et al. (2011) did not normalize PPMI in their experiments.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "The parameter p raises the singular values in \u03a3k to the power p (Caron, 2001).", "startOffset": 64, "endOffset": 77}, {"referenceID": 36, "context": "Decreasing p has the effect of making the similarity measure more discriminating (Turney, 2012).", "startOffset": 81, "endOffset": 95}, {"referenceID": 36, "context": "Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012).", "startOffset": 72, "endOffset": 111}, {"referenceID": 36, "context": "Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012). For Comp and Decomp, we use the parameter settings given by Turney (2012). For Super, we generate features with a wide range of parameter settings and let the supervised learning algorithm decide how to use the features.", "startOffset": 98, "endOffset": 187}, {"referenceID": 36, "context": "The following experiments use the function matrix from Turney (2012).13 It is similar to the domain matrix, except the context is based on verbal patterns, instead of nearby nouns.", "startOffset": 55, "endOffset": 69}, {"referenceID": 36, "context": "Comp and Decomp use fast unsupervised algorithms to generate initial lists of candidates (Turney, 2012).", "startOffset": 89, "endOffset": 103}, {"referenceID": 37, "context": "Super uses a slower supervised algorithm to refine the initial lists from Comp or Decomp (Turney, 2013).", "startOffset": 89, "endOffset": 103}, {"referenceID": 36, "context": "The values of the first four parameters were copied from Turney (2012). The last parameter, maxu, was set to 2000, based on a small number of trials using the standard composition training dataset.", "startOffset": 57, "endOffset": 71}, {"referenceID": 36, "context": "These are the same features as in Turney (2013), except for the six new LBF features.", "startOffset": 34, "endOffset": 48}, {"referenceID": 33, "context": "Super uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten, Frank, & Hall, 2011).", "startOffset": 105, "endOffset": 147}, {"referenceID": 33, "context": "Super uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten, Frank, & Hall, 2011).15 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. After training, we apply Super to the testing dataset. For each testing target, we rank the 2000 candidates in descending order of the probability that they belong in class 1, as estimated by the SVM. Table 11 shows the parameter settings that we use for Super in the following experiments. The Super parameter values are copied from Turney (2013), except for ratio01, which was set based on a small number of trials using the standard composition training dataset.", "startOffset": 106, "endOffset": 669}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k.", "startOffset": 114, "endOffset": 538}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements.", "startOffset": 114, "endOffset": 839}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors).", "startOffset": 114, "endOffset": 1059}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012).", "startOffset": 114, "endOffset": 1437}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices.", "startOffset": 114, "endOffset": 1525}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details.", "startOffset": 114, "endOffset": 1616}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details. We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1300 and p set to \u22120.5. Since Turney (2012) showed that the geometric mean of domain similarity and function similarity performed better than vector addition and element-wise multiplication on the noun-modifier composition recognition task, we decided to apply addition and multiplication to the output of Comp, instead of applying them to the full set of 73,000 unigrams.", "startOffset": 114, "endOffset": 1815}, {"referenceID": 25, "context": "With vector addition, we score the triple \u3008a, b, c\u3009 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a\u2299 b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1000 and p set to \u22120.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets \u00d7 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details. We used the training dataset to optimize the parameters k and p of the smoothed matrix, Uk\u03a3 p k. The best results were obtained with k set to 1300 and p set to \u22120.5. Since Turney (2012) showed that the geometric mean of domain similarity and function similarity performed better than vector addition and element-wise multiplication on the noun-modifier composition recognition task, we decided to apply addition and multiplication to the output of Comp, instead of applying them to the full set of 73,000 unigrams. The intention was to give vector addition and element-wise multiplication the benefit of preprocessing by Comp. On the other hand, Turney (2012) found that the holistic approach was more accurate than all other approaches; therefore we applied the holistic approach to the whole set of 73,000 unigrams, with no preproessing by Comp.", "startOffset": 114, "endOffset": 2289}, {"referenceID": 20, "context": "Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni & Zamparelli, 2010).", "startOffset": 127, "endOffset": 169}, {"referenceID": 20, "context": "Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni & Zamparelli, 2010). Turney (2013) adapted this approach for supervised classification applied to composition recognition.", "startOffset": 128, "endOffset": 185}, {"referenceID": 13, "context": "use Daum\u00e9 III\u2019s (2007) domain adaptation algorithm for training.", "startOffset": 4, "endOffset": 23}, {"referenceID": 13, "context": "use Daum\u00e9 III\u2019s (2007) domain adaptation algorithm for training. This algorithm allows us to train Super with both the standard training dataset and the holistic training dataset. Daum\u00e9 III\u2019s (2007) algorithm is a general strategy for merging datasets that have somewhat different statistical distributions of class labels, such as our standard and holistic datasets.", "startOffset": 4, "endOffset": 199}, {"referenceID": 13, "context": "use Daum\u00e9 III\u2019s (2007) domain adaptation algorithm for training. This algorithm allows us to train Super with both the standard training dataset and the holistic training dataset. Daum\u00e9 III\u2019s (2007) algorithm is a general strategy for merging datasets that have somewhat different statistical distributions of class labels, such as our standard and holistic datasets. Table 16 shows that the model learned from standard training (with WordNet synonym sets) carries over well to holistic testing. Focusing on percent in top 100, standard training achieves 92.3% on holistic testing, which is not significantly different from the 93.7% obtained with holistic training (Fisher\u2019s Exact Test at the 95% confidence level). On the other hand, the model learned from holistic training (without WordNet synonym sets) does not carry over well to standard training. Holistic training only achieves 64.4% on standard testing, whereas standard training achieves 77.8%, which is significantly higher. Table 16 suggests that there is not much benefit to merging the holistic and standard training datasets with Daum\u00e9 III\u2019s (2007) domain adaptation algorithm.", "startOffset": 4, "endOffset": 1115}, {"referenceID": 15, "context": "Here we experiment with the 620 adjective-noun phrases of Dinu et al. (2013) (see Table 2 in Section 2.", "startOffset": 58, "endOffset": 77}, {"referenceID": 27, "context": "Table 18: Performance of models on the adjective-noun dataset, from Li et al. (2014).", "startOffset": 68, "endOffset": 85}, {"referenceID": 15, "context": "Mult is element-wise multiplication (Mitchell & Lapata, 2008) with powers as weights (Dinu et al., 2013).", "startOffset": 85, "endOffset": 104}, {"referenceID": 20, "context": "Fulladd multiplies each vector by a weight matrix and then adds the resulting weighted vectors (Guevara, 2010).", "startOffset": 95, "endOffset": 110}, {"referenceID": 15, "context": "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014).", "startOffset": 50, "endOffset": 69}, {"referenceID": 15, "context": "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014). Table 18 is a copy of their results (this is Table 3 in their paper).", "startOffset": 50, "endOffset": 160}, {"referenceID": 15, "context": "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014). Table 18 is a copy of their results (this is Table 3 in their paper). They evaluated seven different models on the adjective-noun dataset. The reduction column indicates the algorithm used to reduce the dimensionality of the matrix, nonnegative matrix factorization (NMF) or singular value decomposition (SVD). The dim column indicates the number of factors in the dimensionality reductions. The rank column gives the median rank of the solution in the ranked list of candidates. In Table 18, Add is vector addition with weights (Mitchell & Lapata, 2008). Dil is the dilation model introduced by Mitchell and Lapata (2008). Mult is element-wise multiplication (Mitchell & Lapata, 2008) with powers as weights (Dinu et al.", "startOffset": 50, "endOffset": 784}, {"referenceID": 27, "context": "Enetlex is a lexical function model, like Lexfunc, where the model is trained using elastic-net regression (Li et al., 2014).", "startOffset": 107, "endOffset": 124}, {"referenceID": 27, "context": "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup.", "startOffset": 68, "endOffset": 85}, {"referenceID": 27, "context": "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns.", "startOffset": 68, "endOffset": 218}, {"referenceID": 27, "context": "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns. For a fair comparison, we restrict Comp to the same 21,000 nouns. FilterComp is a modified version of Comp that filters the output of Comp to remove any unigrams that are not among the 21,000 nouns of Li et al. (2014). Second, we only calculate the mean and median rank for the targets that have solutions among the top 2000 candidates, because Super does not rank the other candidates.", "startOffset": 68, "endOffset": 459}, {"referenceID": 27, "context": "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns. For a fair comparison, we restrict Comp to the same 21,000 nouns. FilterComp is a modified version of Comp that filters the output of Comp to remove any unigrams that are not among the 21,000 nouns of Li et al. (2014). Second, we only calculate the mean and median rank for the targets that have solutions among the top 2000 candidates, because Super does not rank the other candidates. Since Li et al. (2014) use a single pass approach, they rank all of the candidates.", "startOffset": 68, "endOffset": 651}, {"referenceID": 13, "context": "Here we see some benefit to merging the two training datasets, using Daum\u00e9 III\u2019s (2007) domain adaptation algorithm (see the columns labeled both).", "startOffset": 69, "endOffset": 88}, {"referenceID": 20, "context": "Given the good performance of the holistic baseline, it is natural to consider using holistic datasets as a relatively inexpensive way to train a supervised system (Guevara, 2010; Baroni & Zamparelli, 2010; Turney, 2013).", "startOffset": 164, "endOffset": 220}, {"referenceID": 37, "context": "Given the good performance of the holistic baseline, it is natural to consider using holistic datasets as a relatively inexpensive way to train a supervised system (Guevara, 2010; Baroni & Zamparelli, 2010; Turney, 2013).", "startOffset": 164, "endOffset": 220}, {"referenceID": 33, "context": "To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Eventually, we hope to be able to move beyond sentence paraphrase recognition to sentence paraphrase generation.", "startOffset": 85, "endOffset": 106}, {"referenceID": 33, "context": "To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Eventually, we hope to be able to move beyond sentence paraphrase recognition to sentence paraphrase generation. Turney (2013) has demonstrated that the features we have used here (Section 4) work well for recognizing semantic relations; for example, we can recognize that the semantic relation between mason and stone is analogous to the semantic relation between carpenter and wood.", "startOffset": 85, "endOffset": 234}, {"referenceID": 15, "context": "Following the example of Dinu et al. (2013) and Li et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 15, "context": "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation.", "startOffset": 25, "endOffset": 65}, {"referenceID": 15, "context": "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation. Our results on their adjective-noun dataset suggest that similarity composition is a better approach to generating compositions than context composition (Section 6.4). The experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al.", "startOffset": 25, "endOffset": 444}, {"referenceID": 15, "context": "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation. Our results on their adjective-noun dataset suggest that similarity composition is a better approach to generating compositions than context composition (Section 6.4). The experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al. (2014) are limited by their reliance on holistic pseudo-unigram training (Section 6.", "startOffset": 25, "endOffset": 465}, {"referenceID": 13, "context": "The results with Daum\u00e9 III\u2019s (2007) domain adaptation algorithm (Section 7.", "startOffset": 17, "endOffset": 36}], "year": 2014, "abstractText": "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (red salmon), generate a noun unigram that is synonymous with the given bigram (sockeye). A test for semantic decomposition is, given a context vector for a noun unigram (snifter), generate a noun-modifier bigram that is synonymous with the given unigram (brandy glass). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}