{"id": "1405.7975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2014", "title": "Multi-layered graph-based multi-document summarization model", "abstract": "Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity).", "histories": [["v1", "Sat, 17 May 2014 22:21:00 GMT  (45kb)", "http://arxiv.org/abs/1405.7975v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ercan canhasi"], "accepted": false, "id": "1405.7975"}, "pdf": {"name": "1405.7975.pdf", "metadata": {"source": "CRF", "title": "Multi-layered graph-based multi-document summarization model", "authors": ["Ercan Canhasi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 5.79 75v1 [cs.IR] 1 7M ay2 01Keywords: text mining, multiple document summary, graph-based summary, graph-based ranking algorithm, PageRank"}, {"heading": "1 Introduction", "text": "Most of them are able to play by the rules they have imposed on themselves."}, {"heading": "2 Related work", "text": "The graph-based models have been developed by the extractive document summary community in recent years [3,6]. Conventionally, they model a document or a series of documents as a text graph, using a text unit as a node and similarities between text units as edges. The meaning of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4]. Sentences in document (s) are classified based on the calculated node meaning and the most prominent ones are selected to form an extractive summary. To calculate the sentence meaning, an algorithm called LexRank [3] was used, adapted from PageRank, which was then used as a criterion for ranking and selecting summary sentences. Meanwhile, Mihalcea and Tarau [6] presented their PageRank variant called TextRank in the same year."}, {"heading": "3 Multilayered graph model", "text": "In this section, we introduce our novel graph, which is used in frame ranking algorithms that will be introduced in the next section. Let's be a set of documents D atext similarity graph G = (Vf, Vd, E Vf, EVs, E Vs, E Vd, Vd, Vd, Vd), in which Vs and Vd represent the box, sentence and document edge, respectively. EVf, Vs, Vs, Vs, Vd, Vd are boxes, set and document edge. EVd, set and document edge are boxes, set, sentence and document edge."}, {"heading": "4 Multilayered graph-based ranking algorithm", "text": "In the previous session, the idea of multi-layered text similarity graphics is presented, based on this section, we present a modified iterative graph-based sentence ranking algorithm. Our algorithm is extended by existing PageRank-like algorithms that calculate the graph only at the sentence level [3,6], or sentence and document level [7,10]. In summary, PageRank method (in matrix notation) as described in the original font [1] is\u03c0 (k) TH + (k) T a + 1 \u2212 \u03b1) vTwhere H is a very frugal, crude sub-stochastic hyperlink matrix method that is called a scaling parameter between 0 and 1, analog T is the stationary vector vector of H."}, {"heading": "5 Evaluation", "text": "Task 2 of DUC 2004 is to generate a short summary (665 bytes) of an input set of topic-related news articles; the total number of document groups is 50, with an average of 10 articles in each group; four NIST assessors were asked to produce a short summary for each group; machine-generated summaries (also known as ROUGE-1) are evaluated using an automatic n-gram comparison that measures performance based on the number of simultaneous occurrences between machine-generated and ideal summaries in different word units; and the 1-gram ROUGE score (also known as ROUGE-1) correlates very well with human judgments at a 95% confidence level based on various statistical metrics; although in this version of the method we did not consider sentence positions or other summary quality improvement techniques such as sentence reduction, its overall performance is promising."}, {"heading": "6 Conclusion and future work", "text": "We have presented a multi-layer graph model and a ranking algorithm for generic MDS. The most important contributions of our work are the introduction of the concept of the three-layer graph model. Results of applying this model to extractive summary are quite promising, but much remains to be done. We are now working on further improvements to the model and its adaptation to other summary tasks, such as retrieving and updating the summary. Thanks to the anonymous reviewers for their constructive comments."}], "references": [{"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["Sergey Brin", "Lawrence Page"], "venue": "Computer Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Introduction to the conll-2004 shared task: Semantic role labeling", "author": ["Xavier Carreras", "Lluis Marque"], "venue": "In CoNLL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["Jon M. Kleinberg"], "venue": "J. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Chin-Yew Lin", "Eduard H. Hovy"], "venue": "In HLT-NAACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Textrank: Bringing order into text", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "In EMNLP, pages 404\u2013411,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Biased lexrank: Passage retrieval using random walks with question-based priors", "author": ["Jahna Otterbacher", "G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "Inf. Process. Manage.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Centroidbased summarization of multiple documents", "author": ["Dragomir R. Radev", "Hongyan Jing", "Magorzata Sty", "Daniel Tam"], "venue": "Inf. Process. Manage.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Document-based hits model for multi-document summarization", "author": ["Xiaojun Wan"], "venue": "PRICAI, volume 5351 of Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A document-sensitive graph model for multi-document summarization", "author": ["Furu Wei", "Wenjie Li", "Qin Lu", "Yanxiang He"], "venue": "Knowl. Inf. Syst.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "The construction of situation models in narrative comprehension: an event-indexing model", "author": ["R.A. Zwaan", "M.C. Langston", "A.C. Graesser"], "venue": "Psychological Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}], "referenceMentions": [{"referenceID": 10, "context": "For this challenge we consider using the psychology cognitive situation model, namely the Event-Indexing model [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "parser\u2019s [2] output can be mapped to the above proposed cognitive model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "2 Related work The graph-based models have been developed by the extractive document summarization community in the past years [3,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "2 Related work The graph-based models have been developed by the extractive document summarization community in the past years [3,6].", "startOffset": 127, "endOffset": 132}, {"referenceID": 0, "context": "The significance of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "The significance of a node in a graph is estimated by graph-based ranking algorithms, such as PageRank [1] or HITS [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "An algorithm called LexRank [3], adapted from PageRank, was applied to calculate sentence significance, which was then used as the criterion to rank and select summary sentences.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Meanwhile, Mihalcea and Tarau [6] presented their PageRank variation, called TextRank, in the same year.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "respectively where cw(u) denotes the centroid [8] weight of the word u.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 150, "endOffset": 155}, {"referenceID": 5, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 150, "endOffset": 155}, {"referenceID": 6, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 9, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 8, "context": "Our algorithm is extended from those existing PageRank-like algorithms reported in the literature that calculate the graph only in the sentence level [3,6], or sentence and document level [7,10,9].", "startOffset": 188, "endOffset": 196}, {"referenceID": 0, "context": "In the summary, PageRank method (in matrix notation) as described in the original paper [1] is", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Machine-generated summaries are evaluated using ROUGE [5] automatic n-gram matching which measures performance based on the number of co-occurrences between machine-generated and ideal summaries in different word units.", "startOffset": 54, "endOffset": 57}], "year": 2014, "abstractText": "Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity).", "creator": "LaTeX with hyperref package"}}}