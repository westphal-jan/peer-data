{"id": "1406.0223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2014", "title": "Holistic Measures for Evaluating Prediction Models in Smart Grids", "abstract": "The performance of prediction models is often based on \"abstract metrics\" that estimate the model's ability to limit residual errors between the observed and predicted values. However, meaningful evaluation and selection of prediction models for end-user domains requires holistic and application-sensitive performance measures. Inspired by energy consumption prediction models used in the emerging \"big data\" domain of Smart Power Grids, we propose a suite of performance measures to rationally compare models along the dimensions of scale independence, reliability, volatility and cost. We include both application independent and dependent measures, the latter parameterized to allow customization by domain experts to fit their scenario. While our measures are generalizable to other domains, we offer an empirical analysis using real energy use data for three Smart Grid applications: planning, customer education and demand response, which are relevant for energy sustainability. Our results underscore the value of the proposed measures to offer a deeper insight into models' behavior and their impact on real applications, which benefit both data mining researchers and practitioners.", "histories": [["v1", "Mon, 2 Jun 2014 00:34:24 GMT  (816kb,D)", "http://arxiv.org/abs/1406.0223v1", "14 Pages, 8 figures, Accepted and to appear in IEEE Transactions on Knowledge and Data Engineering, 2014. Authors' final version. Copyright transferred to IEEE"]], "COMMENTS": "14 Pages, 8 figures, Accepted and to appear in IEEE Transactions on Knowledge and Data Engineering, 2014. Authors' final version. Copyright transferred to IEEE", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["saima aman", "yogesh simmhan", "viktor k prasanna"], "accepted": false, "id": "1406.0223"}, "pdf": {"name": "1406.0223.pdf", "metadata": {"source": "CRF", "title": "Holistic Measures for Evaluating Prediction Models in Smart Grids", "authors": ["Saima Aman", "Yogesh Simmhan", "Viktor K. Prasanna"], "emails": ["saman@usc.edu"], "sections": [{"heading": null, "text": "Index Terms - Consumption Forecast, Key Figures, Time Series Forecasts, Regression Tree Learning, Smart Grids, Sustainability of Energies.F"}, {"heading": "1 INTRODUCTION", "text": "It is indeed the case that we will be able to set out in search of new paths that we must tread in order to save the world."}, {"heading": "2 RELATED WORK", "text": "Performance evaluation of predictive models often involves a single dimension, such as error measurement, which is easy to interpret and compare, but does not necessarily examine all aspects of the performance of a model or its quality for a particular application. [12] A new measurement is proposed in [12], based on aggregated performance ratios across time series for a fair treatment of over- and underforecasts. [27] It emphasizes the importance of treating predictive values as a multidimensional problem for a more reliable evaluation of trade-offs between different aspects. [15] introduces a measure to reduce the double tightening effect in predictions whose characteristics are shifted in space or time, compared to point-by-value measurements. [35] identifies the need for cost-benefit measurements to capture the performance of a predictive model through a single profit-maximization measurement metric that can be easily interpreted by practitioners. [5] Underlines the importance of sensitivity to decision-making, such as cost."}, {"heading": "3 PERFORMANCE MEASURE DIMENSIONS", "text": "Performance metrics that supplement the standard statistical error metrics for evaluating predictive models fall along several dimensions that we discuss here. Application In / Dependent: Application-independent metrics are set without knowing how predictions from the prediction model are used. These have no specific dependencies on the application scenario and can be used as a uniform measure for comparison between different candidate models. Application-dependent metrics include parameters that are determined by specific application scenarios of the prediction model. The measurement formulation itself is generic, but requires that users set parameters (e.g. acceptable error thresholds) for the application. These allow a nuanced evaluation of prediction models that are tailored to the application in the prediction model. Scale-independent Errors: When defining error metrics, the residual error is predicted as a predicted error between the Wi and the observed and the predicted value."}, {"heading": "4 APPLICATION INDEPENDENT MEASURES", "text": "Several standard statistical metrics with well-understood theoretical properties fall into the category of application-independent metrics. For the sake of completeness, we recognize two relevant, existing non-scale metrics, MAPE [20], [23] and CVRMSE [3], [13]. More importantly, we are introducing new application-independent metrics along the reliability and cost dimensions and their properties."}, {"heading": "4.1 Mean Absolute Percentage Error (MAPE)", "text": "This is a variant of MAE, which is normalized by the observed value 5 in each interval (1) and thus offers scale independence. It is easy to interpret and is often used for the evaluation of predictions in the energy range and related ranges [8], [9], [18], [20], [23]."}, {"heading": "4.2 Coefficient of Variation of RMSE (CVRMSE)", "text": "This is the normalized version of the standard RMSE measure, which divides it by an average of 6 of the observed values (2) to provide scale independence. It is an unbiased estimator that takes into account both the predictive model bias and its variance, and provides a uniform percentage margin of error. CVRMSE is sensitive to rare large errors due to the squared term."}, {"heading": "4.3 Relative Improvement (RIM)", "text": "We propose RIM as a relative measurement of reliability, estimated as the frequency of predictions by a candidate model that is better than a base model. RIM is a simple, uniform measurement that supplements 5. MAPE is not defined if there are zero values in the input, which is rare because energy consumption values (kwh) are generally positive (unless there is a net measurement) and can be ensured by data pre-processing. 6. CVRMSE is not defined if this average is zero, which is rare, since energy consumption values (kwh) are generally positive (unless there is a net measurement) and can be ensured by data pre-processing. Error measurements are made in cases where more often accuracy than a baseline is meaningful and occasional large errors in relation to the baseline are acceptable. RIM = 1n \u00b2 i = 1 C (pi, oi, bi, bi, bi, (pi), 3, pi = \u2212 and \u2212 are the observed values."}, {"heading": "4.4 Volatility Adjusted Benefit (VAB)", "text": "Inspired by the Sharpe Ratio, this relative measure provides a \"risk-adjusted\" scale-dependent error value, which measures the relative improvement in the candidate model's MAPE over the baseline (benefit). If these error improvements are consistent with 7 in i, then their standard deviation would be low (volatility) and the VAB high. But, if volatility is high, the benefit would reduce the lack of consistent improvements. V AB = 1 n-ig i = 1 (| bi \u2212 oi \u2212 oi \u2212 | pi \u2212 oi | oi) \u03c3 (| bi \u2212 oi \u2212 oi \u2212 | pi \u2212 oi | oi) (5), where oi, pi and bi are the observed, forecast, and predicted baseline values for intervals."}, {"heading": "4.5 Computation Cost (CC)", "text": "The cost of training and forecasting a model can prove important when used either on a large scale and / or in real-time applications that respond to prediction latency. CC is defined in seconds as the sum of the time required to build a model, CCt, and the time required to predict with the model, CCp, for a specific forecast duration with a specific horizon. Therefore, CC = CCt + CCp."}, {"heading": "4.6 Data collection Cost (CD)", "text": "Instead of examining the raw size of the data used for training or prediction using a model, a more useful measurement is the effort required to collect and assemble the data. Size can be managed by cheap storage, but collecting the necessary data often requires human and organizational effort. We propose a more scalable measurement of the data cost defined in terms of number.7 The error improvements that a particular model provides over the base model should have a normal distribution, so that VAB is meaningful, of unique values of features that are included in a forecast model.CD is defined for a specific training and prediction duration as the sum of ns, the number of static (time-invariant) features that require a one-time collection effort, and nd the number of dynamic features that require periodic collection. CD = ns = 1 [si] nd + = 1 [di] (6) where [si] and [si] are unique characteristics."}, {"heading": "5 APPLICATION DEPENDENT MEASURES", "text": "Contrary to the previous measures, application-specific performance measures are adapted to specific application scenarios and can be adapted by domain experts to their needs. The novel measures we propose here are not narrowly defined for a single application (although they are motivated by the needs observed in the smart grid area), but are generalized by the use of coefficients that are themselves application-specific. We group them along the dimensions that we introduced earlier."}, {"heading": "5.1 Domain Bias Percentage Error (DBPE)", "text": "We propose DBPE as a significant percentage error value that provides scale independence, indicating whether the predictions are positively or negatively distorted compared to the observed values, which is important if over or under-prediction errors have an uneven effect on the application in relation to observed ones. We define DBPE as an asymmetric loss function based on the sign distortion. DBPE = 1n \u2211 i = 1 L (pi, oi) oi (7), where L (pi, oi) is linear on both sides of the origin but has different gradients on each side. The asymmetric gradients allow different penalties for positive / negative errors. DBPE = 1n \u2211 i = 1 L (pi, oi) oi (7), where L (pi, o\u03b2, oi) the linen loss function is defined as: L (pi, oi) = \u03b1 \u00b7 \u00b7 pi \u2212 oi | when pi \u2212 oi and oi = oi \u2212 oi are measured | when pi > oint, o\u03b2 = \u03b2, \u03b1 = \u03b1 when application is specific, o\u03b2 = \u03b1 \u2212 oi = oi \u2212 oi = oi \u2212 oi and \u2212 oi \u2212 oi when application is measured."}, {"heading": "5.2 Reliability Threshold Estimate (REL)", "text": "Often, applications are less interested in the absolute errors in the predictions of a model and prefer an estimate of how often the errors are within a specified threshold that the application can withstand. We define REL as the frequency of prediction errors less than an application-related error threshold, et.REL = 1n n n \u2211 i = 1 C (pi, oi) (9), where oi and pi are the observed values and the model predicted values for the interval i, and C (pi, oi) is a counting function defined as follows: C (pi, oi) = 1, if | pi \u2212 oi | oi < et 0, if | pi \u2212 oi | oi = et \u2212 1, if | pi \u2212 oi | oi > et (10)."}, {"heading": "5.3 Total Compute Cost (TCC)", "text": "In the context of an application, it makes sense to supplement the data and calculation costs (CD and CC) with an estimate of the total running costs of using a model for an application-specific duration. We define the parameters: \u2022 \u03c4, the number of training sessions of a model within the duration, \u2022 \u03c0, the number of predictions of a model within a particular horizon during that duration. These parameters are not only application-specific, but also vary depending on the candidate model, depending on how often it needs to be trained and its effective forecast horizon. We define the total training costs in seconds for a forecast duration based on \u03c4 and \u03c0 and the unit costs for training and prediction based on the model CCt and CCp introduced in \u00a7 4.5: TCC = CCt \u00b7 \u03c4 + CCp \u00b7 \u03c0 (11)"}, {"heading": "5.4 Cost-Benefit Measure (CBM)", "text": "Instead of treating costs in a vacuum, it is worth looking at the cost of a model in relation to the profits it offers. CBM compares candidate models that have different error measurements and costs, which provides a high reward for a calculation cost per unit of measurement. CBM = (1 \u2212 DBPE) TCC (12) The meter is an estimate of accuracy (minus error measurements), while the denominator is the calculation cost. We use DBPE as error measurement and TCC as cost, but these can be replaced by other application-dependent error measurements (e.g. CVRMSE, MAPE) and costs (e.g. CD, CCp). A model with high accuracy but prohibitive costs may be unsuitable."}, {"heading": "6 CANDIDATE PREDICTION MODELS", "text": "The ARIMA (Autoregressive Integrated Moving Average) model is a commonly used TS prediction model. It is defined by the number (d) of times a time series must be differentiated to make it stationary; the autoregressive order (p), which captures the number of past values; and the moving mean order (q), which captures the number of past white noise error terms. These parameters are determined using autocorrelation and partial autocorrelation functions using the Box-Jenkins test [6]. ARIMA is easy to use as it does not require knowledge of the underlying domain [4]. However, estimating the model parameters d, p, and q requires a human examination of the partial correclogram of the time series. Although some minor functions are required to determine partial consumption values in the area of the model, rapid resorsion can be performed based on these parameters."}, {"heading": "7 EXPERIMENTAL SETUP", "text": "The USC Campus Microgrid [31] is a testing ground for the DOEsponsored Los Angeles Smart Grid Project. ARIMA and Regression Tree prediction models are used to predict energy consumption at 24-hour and 15-minute granularity, for the entire campus and for 35 individual buildings. Here we look at the campus and four representative buildings: DPT, a small department with teaching and office space; RES, a series of residences with decentralized control of refrigeration and appliance services; OFF, which houses administrative offices and telepresence laboratory; and ACD, a large academic teaching building. These buildings were considered after several pilot studies to provide diversity in terms of soil size, age, land use, type of residents and net electricity consumption."}, {"heading": "7.1 Datasets", "text": "Electricity Consumption Data 8: We used 15-minute granularity data collected by USC Facility Management Services between 2008 and 2010 (Table 1), which yielded 3 x 365 x 96 or x 100K samples per building. We interpolated missing values (< 3% of samples) linearly and aggregated 15-minute data each day to obtain the 24-hour granularity values. Observations from 2008 and 2009 were used to train the models, while the predictions were compared with values observed outside the sample for 2010. 9. Weather Data 10: We collected historical average and maximum temperature data curated by NOAA for Los Angeles / USC Campus for 2008-2010. These values were interpolated linearly to obtain 15-minute values. We also recorded daily maximum temperatures used for the 24-hour granularity models. Modular Data 11: We recorded USC fiber information related to the camping calendar and work days."}, {"heading": "7.2 Model Configurations", "text": "Regression Tree (RT) models: For 24 hours (granularity) predictions we used for the RT model: Day of the Week (Sun-Sat), Semester (Fall, Spring, Summer), Maximum and Average Temperatures, and One Week (Granularity) predictions we selected for a period of 11 months. The 15-minute models used in this article are available on request for academic use. 24-hour data were only available until November 2010, and therefore 24-hour models are tested for a period of 11 months. 15-minute models cover the entire 12 months."}, {"heading": "7.3 Smart Grid Applications", "text": "We present three applications used within the USC Microgrid to evaluate our proposed metrics. Planning: Planning capital infrastructure such as building refurbishment and energy system upgrades to increase energy efficiency offsets investments against electricity savings. Medium and long-term electricity consumption forecasts in rough (24-hour) granularity for the campus and individual buildings support this decision-making. Such models are executed six times a year.Customer education: educating electricity customers about their energy consumption can improve their participation in sustainability by curbing demand and meeting monthly budgets [28]. One form of training is to provide consumption forecasts for customers in a building via web and mobile apps. 12 Building forecasts on 24-hour and 15-minute granularity are made during the day (6-10am)."}, {"heading": "8 ANALYSIS OF INDEPENDENT MEASURES", "text": "We first examine the usefulness and value of the six application-independent measures (\u00a7 4) in order to evaluate the candidate models for predicting campus and building consumption with coarse and fine time granularities."}, {"heading": "8.1 24-hour Campus Predictions", "text": "Figure 1a shows the CVRMSE and MAPE metrics for the DoW base, RT and TS models, the latter for four different horizons, for 24-hour on-campus predictions. These metrics provide TS models at different horizons with greater accuracy than the RT and DoW models. This is understandable given the noticeable difference in mean and standard deviations (Table 1) between training and test periods. TS uses incrementally newer data as a moving window, while RT and DoW models are trained only on the test data of the two years. In addition, errors for TS worsen with increasing forecast horizons. This is a consequence of their dependence on the most recent delay values, making them suitable only for short-term predictions. RT models are independent of prediction horizons (assuming future characteristic values are known) and therefore predictions with long horizons are preferable, making them suitable for short-term predictions only. RT models are independent of prediction horizons (assuming that future characteristic values are preferred) and therefore predictions with long horizons are preferable. The fast DoW error is higher than our RID of 11.5% (this is measured by our RIK)."}, {"heading": "8.2 24-hour Building Predictions", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "8.3 15-min Campus Predictions", "text": "The 15-minute predictions for the campus show that the TS (2 hours) comes closest to the observed values, based on CVRMSE (6.88%) and MAPE (4.18%) (Fig. 3a). This accuracy is validated compared to the baseline with high RIM and VAB values (Fig. 4a), reflecting the dual benefits of the campus's large spatial granularity, which changes its consumption more slowly, and the short horizon of the TS (2 hours), which helps it capture temporal similarities."}, {"heading": "8.4 15-min Building Predictions", "text": "In 15-minute predictions for buildings, we see that TS (2 hours) is the only candidate model that consistently performs better than the ToW baseline on all four measures (Fig. 3b-3e & 4b-4e); TS (6 hours) and RT are occasionally better than ToW on CVRMSE and MAPE, and TS (24 hours) are rare; their CVRMSE errors are also consistently greater than MAPE, showing that the models suffer from occasional major errors more frequently; the academic environment with weekly class schedules encourages uniform energy consumption behavior based on ToW, which is hard to beat; RES is the exception where all candidate models are better than the baseline (Fig. 3c), given the deviating summer months when it is unusable; however, when we look at the RIM- and VAB measurements, it is interesting to note that the candidate models are not significantly worse than the baseline (Fig. 3c), but the 4W is better than the 4T for all buildings (in fact 4W)."}, {"heading": "8.5 Cost Measures", "text": "The data and calculation cost measures discussed here are orthogonal to the other application-independent measures, and their values are summarized in Table 2. Cost assessment helps network operators ensure rational use of resources, including skilled manpower and resources, in the prediction process. Data costs (CD): The baseline and TS are universal models that require only the power consumption values for the training and testing periods. Therefore, their data costs are smaller and correspond to the number of intervals that have been trained and tested. The RT model has higher costs due to the addition of multiple features (\u00a7 7.2). However, the costs do not rise linearly with the number of features and instead depend on the number of unique feature values. As a result, the data costs are only \u2212 25% and \u2212 300% higher than the TS prediction time for 24-hour and 15-minute prediction values over the average CCU values. Cost of calculation CC (64 weeks): We train for 24 hours and 24 minutes (2 years)."}, {"heading": "9 ANALYSIS OF DEPENDENT MEASURES", "text": "The Application Dependent Measures (\u00a7 5) allow the selection of models for specific application scenarios. For each application (\u00a7 7.3), the parameter values of the measures listed in Tables 3 & 4 are determined in consultation with the domain experts."}, {"heading": "9.1 Planning", "text": "Planning requires medium- and long-term consumption forecasts with 24-hour granularities for the campus and buildings, six times a year. TS's short horizon (4 weeks) precludes its use. Thus, we only look at DoW and RT models, but report TS results. Campus: For decisions at campus level, both over and under predictions can be punishing. The former will lead to oversupply of capacity with high costs, while the latter can cause a lower usability of capital investment. Therefore, we are equivalent \u03b1 = 1 and \u03b2 = 1 for DBPE, with DBPE reduced to MAPE. We set et = 10%, a relatively lower tolerance, since even a small swing in the error rate% for a large consumer like USC shows large shifts in kWh.Fig.5a RT (and TS), better than the DoW training on DBPE (6.87% vs. 7.56%, in line with MAPE)."}, {"heading": "9.2 Customer Education", "text": "This application uses 24-hour and 15-minute building-level predictions made during the day (6AM10PM) and provides them to residents for monthly budgeting and daily energy savings. 24-hour predictions: 24-hour predictions affect monthly power budgets, and overtime predictions are better, however, to avoid slips. We choose \u03b1 = 0.75 and \u03b2 = 1.25 for DBPE and a margin of error et = 15% for REL. We use a 4-week forecast duration for a 24-hour forecast performed every day by RT and TS. RT is trained once during this period. We report TCC (Table 4), DBPE & CBM (Figures. 5b-5e), and REL (Figures. 6b-6e).As for planning, which is preferred over-predicting, the DBPE is smaller than MAPE here for all models, and it is mostly smaller for RT models."}, {"heading": "9.3 Demand Response", "text": "The campus is a large customer with stricter requirements for the error threshold of et = 5% for REL, while individual buildings with lower impacts are allowed a larger error margin of et = 10%. The forecast period is 4 weeks for cost parameters, with the models used three times a week - before, at the beginning and during the period of 1-5 PM, and RT trained weekly. DBPE is consistently smaller than MAPE for the campus and the buildings (Fig. 7a-7e), with the models used three times a week - before, at the beginning and during the period of 1-5 PM, and RT trained weekly. DBPE is consistently smaller for the campus and the buildings than MAPE (Fig. 7a-7e) and sometimes even halves the errors. Thus, the 4-hour DR periods in the weekdays are more (over) predictable than the TS periods in the weekdays."}, {"heading": "10 CONCLUSION", "text": "In this article, we examine the value of holistic performance measures along the dimensions of scale independence, reliability, and cost. In evaluating these benchmarks for consumption forecasting in smart grids, we see that scale independence ensures that performance can be compared between models and applications and for different customers; reliability evaluates the consistency of a model's performance in terms of baseline models; while cost is a key consideration in providing models on a large scale for real-world applications. We use existing non-scale benchmarks, CVRMSE, and MAPE, while we expand and propose four additional measures, RIM and VAB, to measure reliability; and CD and CC, to calculate data and cost. In addition, our novel application-dependent benchmarks show that they can be customized by domain experts for meaningful model evaluations. These benchmarks include correlation for scale independence, REL for measurement reliability, and CCPM for measurement specific results."}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based on work supported by the United States Department of Energy under premium number DEOE0000192 and the Los Angeles Department of Water and Power (LA DWP). The views and opinions expressed herein do not necessarily reflect the views of the United States Government or any of its agencies, the LA DWP, or its employees."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The performance of prediction models is often based on \u201cabstract metrics\u201d that estimate the model\u2019s ability to limit residual errors between the observed and predicted values. However, meaningful evaluation and selection of prediction models for end-user domains requires holistic and application-sensitive performance measures. Inspired by energy consumption prediction models used in the emerging \u201cbig data\u201d domain of Smart Power Grids, we propose a suite of performance measures to rationally compare models along the dimensions of scale independence, reliability, volatility and cost. We include both application independent and dependent measures, the latter parameterized to allow customization by domain experts to fit their scenario. While our measures are generalizable to other domains, we offer an empirical analysis using real energy use data for three Smart Grid applications: planning, customer education and demand response, which are relevant for energy sustainability. Our results underscore the value of the proposed measures to offer a deeper insight into models\u2019 behavior and their impact on real applications, which benefit both data mining researchers and practitioners.", "creator": "LaTeX with hyperref package"}}}