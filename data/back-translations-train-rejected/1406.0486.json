{"id": "1406.0486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2014", "title": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups", "abstract": "Monte Carlo Tree Search (MCTS) has improved the performance of game playing engines in domains such as Go, Hex, and general-game playing. MCTS has been shown to outperform outperform classic alpha-beta search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during its MCTS simulations. These learned evaluation values are then used to guide future simulations. Compared to current techniques, we show that using implicit minimax backups leads to stronger play performance in Breakthrough, Lines of Action, and Kalah.", "histories": [["v1", "Mon, 2 Jun 2014 19:32:12 GMT  (215kb,D)", "https://arxiv.org/abs/1406.0486v1", "23 pages, 7 figures, 5 tables, expanded version of paper presented at IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference"], ["v2", "Wed, 4 Jun 2014 19:00:30 GMT  (215kb,D)", "http://arxiv.org/abs/1406.0486v2", "24 pages, 7 figures, 9 tables, expanded version of paper presented at IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference"], ["v3", "Sun, 8 Jun 2014 14:06:23 GMT  (215kb,D)", "http://arxiv.org/abs/1406.0486v3", "24 pages, 7 figures, 9 tables, expanded version of paper presented at IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference"], ["v4", "Thu, 19 Jun 2014 22:07:30 GMT  (215kb,D)", "http://arxiv.org/abs/1406.0486v4", "24 pages, 7 figures, 9 tables, expanded version of paper presented at IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference"]], "COMMENTS": "23 pages, 7 figures, 5 tables, expanded version of paper presented at IEEE Conference on Computational Intelligence and Games (CIG) 2014 conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marc lanctot", "mark h m winands", "tom pepels", "nathan r sturtevant"], "accepted": false, "id": "1406.0486"}, "pdf": {"name": "1406.0486.pdf", "metadata": {"source": "CRF", "title": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups", "authors": ["Marc Lanctot", "Mark H.M. Winands", "Tom Pepels", "Nathan R. Sturtevant"], "emails": ["marc.lanctot@maastrichtuniversity.nl,", "m.winands@maastrichtuniversity.nl,", "tom.pepels@maastrichtuniversity.nl,", "sturtevant@cs.du.edu"], "sections": [{"heading": "1 Introduction", "text": "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first-search technique that has been proven to improve performance in areas such as round games, general-game games, real-time strategy games, single-agent planning, and more. While the first applications took place in games where heuristic evaluations are difficult to obtain, advances in MCTS research have shown that heuristics can be effectively combined in MCTS, even in games where classic Minimax search has traditionally been preferred. UCT [19], the most popular MCTS algorithm, performs a single simulation from the root of the search tree to a final state at each iteration. During the iterative process, a game tree is gradually built by adding a new leaf node to the tree at each iteration, the nodes of which maintain statistical estimates such as average payouts. Each new simulation improves these estimates and contributes to future simulations."}, {"heading": "1.1 Related Work", "text": "The first was Coulom's original maximum back propagation [9]. This method of back propagation suggests that the search algorithm converges to a node after a number of simulations and returns an estimate of the best value. Maximum back propagation has also recently been used in other Monte Carlo search algorithms, demonstrating success in probability planning, as an alternative type of forecaster in BRUE [11], and as Bellman backups for dynamic online programming in Trial-based Heuristic Tree Search [17]. The first use of improving MCTS with prior knowledge was in Computer Go [13]. Knowledge acquired offline initialized values of the advanced nodes increased performance against a significantly strong benchmark player. This technique was also proposed to increase strength."}, {"heading": "2 Adversarial Search in Turn-Taking Games", "text": "A finite deterministic Markov decision-making process (MDP) is quadruple (S, A, T, R). Here, S is a finite, non-empty group of states. A is a finite, non-empty group of actions in which we denote A (s) A. T: S \u00b7 A 7 \u2192 N is a transitional function that maps each state and each action to a distribution across successor states. Finally, R: S \u00b7 A \u00b7 A \u00b7 S 7 \u2192 R is a reward function that applies mapping (state, action, successor state) three times to numerical rewards. A two-player perfect information game is an MDP with a specific form. Denote Z = {s \u00b2 S is the set of terminal states. Furthermore, the reward for all non-terminal states is s \"S \u2212 Z, R (s, a, s \u00b2 s \u00b2 s \u00b2 s\") and the identity function of the player: S \u2212 Z. \""}, {"heading": "3 Implicit Minimax Backups in MCTS", "text": "Our proposed technique is based on the following principle: If an evaluation function is available, then it should be possible to expand MCTS to achieve a potential performance gain."}, {"heading": "4 Empirical Evaluation", "text": "In this section, we thoroughly evaluate the practical performance of the implicit Minimax backup technique. Before reporting results from the head-to-head game, we first describe our experimental setup and summarize the techniques used to improve playouts. We then present results from three areas of play: Kalah, Breakthrough and Lines of Action.Unless otherwise stated, our implementations expand one new node in each simulation, with the first node not encountered in the tree. MCTS-Solver is enabled in all experiments because its overhead is negligible and performance is never diminished. After the simulations, the move with the highest number of visits is selected in line 28, the rewards are in {\u2212 1, 0} representing a loss, draw and gain. Rating values are scaled to [\u2212 1, 1] by replacing a domain-dependent score with a cache-optimized sigmoid function."}, {"heading": "4.1 Kalah", "text": "Kalah is a turn-taking game in the Mancala family of games. Each player has six houses, each of which initially contains four checkers, and one shop at the end of the board, which is initially empty. If the last checker lands in the player's shop, that player gets another turn, and there is no limit on the number of checkers that the same player can draw in a row. If the checker ends up on a house that belongs to the player who does not contain any checkers, that player catches all the checkers in the adjacent opponent's house and brings them into the player's shop. The game plays until a player's houses are empty; the opponent then moves his remaining checkers into their shop. The winner is the player who has collected the most checkers in his shop. Kalah is weakly resolved for several different variants of Kalah [16] and has been used as a domain to compare the variants."}, {"heading": "4.2 Breakthrough", "text": "A player has 16 identical pieces on his first two rows. A piece is allowed to move to an empty square, either straight or diagonal, but can only grasp diagonally like chess pieces. A player wins by moving a single piece to the farthest opponent. Breakthrough was first introduced in general game competitions and has been identified as a domain that is particularly difficult for MCTS due to traps and uninformed plays. [14] Our play policy always selects a \"decisive\" winner and prevents immediate \"decisive\" losses. Otherwise, a move is selected that is not uniformed, where undefended pieces are four times more likely than other moves. MCTS with this improved play policy (abbreviated as \"ipp\") beats the one with a uniform 94.3% of the time."}, {"heading": "MCTS Using Lorentz & Horey Evaluation Function", "text": "Instead of repeating all of the above experiments, we simply decided to compare baseline and repeat the initial experiment, using search time every 1 sec. The best output with this evaluation function is fet20 with node priors, which we call alternative baselines, abbreviated bl '. That is, we shorten MCTS (ipp, fet20, np) to MCTS (bl'). We perform the initial \u03b1 experiment using the alternative baseline, which uses the Lorentz & Horey evaluation function, to find the best implicit Minimax player using this more complex evaluation function. Results are shown in Figure 3.: The best evaluation line is \u03b1 [0,5, 0.6] for one second and \u03b1 [0,5, 0.6] for five seconds."}, {"heading": "Comparison to \u03b1\u03b2 Search", "text": "A natural question is how to compare MCTS with implicit Minimax backups with the MCTS search. In this case, we compare MCTS with implicit Minimax backups compared to the MCTS search. Our \u03b1\u03b2 searcher uses iterative depressions and a static pull sequence. The static pull sequence is based on the same information used in the improved play guidelines: first decisive and anti-decisive moves, then capture of defenseless figures, then all other capture sequences, and finally regular moves. The results are in Table 4.The first observation, however, is that the performance of MCTS (vs. \u03b1\u03b2) increases with increasing search time. This is true in all cases by using either evaluation functions with or without implicit Minimax backups. This is similar to observations in plot lines [32] and multiplayer MCTS [29, 23]. The second observation is that MCTS (implicit M\u03b1\u03b2) is significantly better than MCTS (at the base time)."}, {"heading": "4.3 Lines of Action", "text": "In subsection 4.2, we compared the performance of MCTS (im\u03b1) with a basic \u03b1\u03b2 \u03b2-seeker. Our main question at this point is how MCTS (im\u03b1) could perform in a game with a stronger game due to the use of implicit progressive improvements in both \u03b1\u03b2 and MCTS. For this analysis, we now look at the well-researched plot lines (LOA). LOA is a powerful alternation game played on an 8-by-8 board that uses a chessboard and pieces. The goal is to join all the pieces into a single connected group (of whatever size), in which the pieces are connected via adjacent and diagonal squares. A piece can move in any direction, but the number of squares it moves depends on the total number of pieces in the line, including opposing pieces. A piece can jump over its own pieces, but not opposing pieces. Captures occur by landing on opposing pieces. The MTS player is LOMC, which improvements are described in the LOMC series and its implementation."}, {"heading": "4.4 Discussion: Traps and Limitations", "text": "The initial motivation for this work was driven by the pitfalls that present problems in MCTS [26, 2, 14]. However, in LOA we observed that implicit minimax backups did not accelerate MCTS in solving a test set of endgame positions. We tried to construct an example board in Breakthrough to show how implicit minimax backups handle problems with traps. We were unable to do this. In our experience, traps are effectively handled by the improved playout policy. Even without early terminations, it seems sufficient to have decisive and anti-decisive moves and to prefer good capture moves for breakthrough. Even with random playouts, efficient implementation with MCTS solvers is manageable. Therefore, we believe that the explanation for the advantage that implicit minimax backups provide is more subtle than simple 1https: / dke.handletyty.mw.nl / all."}, {"heading": "5 Conclusion", "text": "This technique stores the information from both sources separately and combines only the two sources to guide the selection. Implicit minimax backups can significantly increase the power of MCTS compared to similar techniques to improve MCTS using domain knowledge, even with simple evaluation features that are often readily available. In Breakthrough, our review shows that implicit minimax backups significantly increase the strength of MCTS compared to similar techniques to improve MCTS using domain knowledge. In addition, the technique improves performance in LOA, a more complex domain with sophisticated knowledge and strong MCTS and \u03b1\u03b2 players. The \u03b1 range [0.15, 0.4] seems to be a safe choice. In Breakthrough, this range is higher [0.5, 0.6] when considering node priors at lower time settings and when using alternative baselines. For future work, we would like to use the technique in other games, such as Amazons, and plan to investigate the improvement of initial v0 (s) ratings using source search."}, {"heading": "A Game Images", "text": "This appendix shows pictures of each game to visualize the goals."}, {"heading": "B Tournaments and Playout Comparisons", "text": "This appendix contains details of the results of the games played in order to identify the best players."}, {"heading": "B.1 Parameter Values for Breakthrough and Kalah", "text": "Table 6 shows the parameter values used in parameter tuning experiments."}, {"heading": "B.2 Kalah Playout Optimization", "text": "In Kalah each match has 1000 games included, but as mentioned in the main part of the paper, only the wins and losses are shown and unbalanced boards are removed. B.2.1 Fixed Early Termination Tournament round 1 winner mcts _ h _ fet0 (368) vs. loser mcts _ h _ fet1000 (61) Winner mcts _ h _ fet1 (408) vs. loser mcts _ h _ fet100 (61) Winner mcts _ h _ fet2 (458) vs. loser mcts _ h _ fet2 (61) Winner mcts _ h _ fet3 (460) vs. loser mcts _ h _ fet3 (37) Winner mcts _ h _ fet4 (429) vs. loser mcts _ h _ fet2 (44) vs. loser mcts _ h _ fet3 _ fet3 (460) vs. loser mcts _ fet3 (460) vs. loser mcts _ h _ fet4 (37) Winner mcts _ h _ fet4 (429) vs. loser mcts _ h _ fet2 _ fet2 (44) vs. (44) vs. fetted vs. (61) vs. (61) (61) Winner mcts _ fet3 _ fet4 (429) vs. loser mcts _ h _ fet4 (429) vs. loser mcts _ h _ fet2 (44) vs. fetted _ 44 (61) vs. fetted _ v (61)"}, {"heading": "B.2.3 Kalah Tournament Winner Comparisons", "text": "Looking at the results of the first round, the results of the second round (84) and the second round (84) are considered winners (84). (84) Winner (84) Winner (84) Winner (84). (84) Winner (84) Winner (84). (84) Winner (84) Winner (84). (84) Winner (84) Winner (84). (84) Winner (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84). (84) Winner (84)."}, {"heading": "B.3.4 Breakthrough Tournament Winner Comparisons (using efMS)", "text": "Results of the first round of the first round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the third round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the third round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round of the second round"}, {"heading": "B.5 Breakthrough Tournament Winner Comparisons (using efLH)", "text": "Table 9 shows the results of the best Breakthrough Playout winners using efLH."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Monte-Carlo tree search and minimax hybrids", "author": ["H. Baier", "M.H.M. Winands"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG), pages 129\u2013136", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Old-fashioned computer Go vs Monte-Carlo Go", "author": ["B. Bouzy"], "venue": "IEEE Symposium on Computational Intelligence in Games (CIG)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of Monte Carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Trans. on Comput. Intel. and AI in Games, 4(1):1\u201343", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Score bounded Monte-Carlo tree search", "author": ["T. Cazenave", "A. Saffidine"], "venue": "International Conference on Computers and Games ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "An adaptive sampling algorithm for solving Markov Decision Processes", "author": ["H.S. Chang", "M.C. Fu", "J. Hu", "S.I. Marcus"], "venue": "Operations Research, 53(1):126\u2013139", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Adding expert knowledge and exploration in Monte-Carlo tree search", "author": ["G. Chaslot", "C. Fiter", "J-B. Hoock", "A. Rimmel", "O. Teytaud"], "venue": "Advances in Computer Games, volume 6048 of LNCS, pages 1\u201313", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "H", "author": ["G.M. J-B. Chaslot", "M.H.M. Winands", "J.W.H.M. Uiterwijk"], "venue": "J. van den Herik, and B. Bouzy. Progressive strategies for Monte-Carlo tree search. New Mathematics and Natural Computation, 4(3):343\u2013357", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient selectivity and backup operators in Monte-Carlo tree search", "author": ["R. Coulom"], "venue": "5th International Conference on Computers and Games, volume 4630 of LNCS, pages 72\u201383", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "High-quality policies for the Canadian travelers problem", "author": ["P. Eyerich", "T. Keller", "M. Helmert"], "venue": "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI), pages 51\u2013 58", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Monte-Carlo planning: Theoretically fast convergence meets practical efficiency", "author": ["Z. Feldman", "C. Domshlak"], "venue": "International Conference on Uncertainty in Artificial Intelligence (UAI), pages 212\u2013221", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning simulation control in general game playing agents", "author": ["H. Finnsson", "Y. Bj\u00f6rnsson"], "venue": "Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 954\u2013959", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining online and offline knowledge in UCT", "author": ["S. Gelly", "D. Silver"], "venue": "Proceedings of the 24th Annual International Conference on Machine Learning ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Sufficiency-based selection strategy for MCTS", "author": ["S.F. Gudmundsson", "Y. Bj\u00f6rnsson"], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 559\u2013 565", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting fortresses in chess", "author": ["M. Guid", "I. Bratko"], "venue": "Elektrotehni\u0161ki Vestnik, 79(1\u20132):35\u201340", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Solving Kalah", "author": ["G. Irving", "H.H.L.M. Donkers", "J.W.H.M. Uiterwijk"], "venue": "ICGA Journal, 23(3):139\u2013148", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Trial-based heuristic tree search for finite horizon MDPs", "author": ["T. Keller", "M. Helmert"], "venue": "International Conference on Automated Planning and Scheduling (ICAPS)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Monte-Carlo Techniques: Applications to the Game of Amazons", "author": ["J. Kloetzer"], "venue": "PhD thesis, School of Information Science, JAIST, Ishikawa, Japan", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Bandit-based Monte Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "15th European Conference on Machine Learning, volume 4212 of LNCS, pages 282\u2013293", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Programming Breakthrough", "author": ["R. Lorentz", "T. Horey"], "venue": "8th International Conference on Computers and Games (CG)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Amazons discover Monte-Carlo", "author": ["Richard Lorentz"], "venue": "Proceedings of the 6th International Conference on Computers and Games (CG),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Playout Search for Monte-Carlo Tree Search in Multi-Player Games", "author": ["J.A.M. Nijssen", "M.H.M. Winands"], "venue": "ACG 2011, volume 7168 of LNCS, pages 72\u201383", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Search policies in multi-player games", "author": ["J.A.M. Nijssen", "M.H.M. Winands"], "venue": "ICGA Journal, 36(1):3\u201321", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On adversarial search spaces and samplingbased planning", "author": ["R. Ramanujan", "A. Sabharwal", "B. Selman"], "venue": "20th International Conference on Automated Planning and Scheduling (ICAPS), pages 242\u2013245", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Understanding sampling style adversarial search methods", "author": ["R. Ramanujan", "A. Sabharwal", "B. Selman"], "venue": "26th Conference on Uncertainty in Artificial Intelligence (UAI), pages 474\u2013483", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Trade-offs in sampling-based adversarial planning", "author": ["R. Ramanujan", "B. Selman"], "venue": "21st International Conference on Automated Planning and Scheduling (ICAPS), pages 202\u2013209", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Solving Games and All That", "author": ["Abdallah Saffidine"], "venue": "PhD thesis,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Selective Search in Games of Different Complexity", "author": ["M.P.D. Schadd"], "venue": "PhD thesis, Maastricht University, Maastricht, The Netherlands", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of UCT in multi-player games", "author": ["N.R. Sturtevant"], "venue": "ICGA Journal, 31(4):195\u2013208", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "On the huge benefit of decisive moves in Monte-Carlo tree search algorithms", "author": ["F. Teytaud", "O. Teytaud"], "venue": "IEEE Conference on Computational Intelligence in Games (CIG), pages 359\u2013 364", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Bootstrapping from game tree search", "author": ["J. Veness", "D. Silver", "A. Blair", "W.W. Cohen"], "venue": "Advances in Neural Information Processing Systems 22, pages 1937\u20131945", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "\u03b1\u03b2-based play-outs in Monte-Carlo tree search", "author": ["M.H.M. Winands", "Y. Bj\u00f6rnsson"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG), pages 110\u2013117", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Monte-Carlo tree search solver", "author": ["M.H.M. Winands", "Y. Bj\u00f6rnsson", "J-T. Saito"], "venue": "Computers and Games ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Monte-Carlo tree search solver", "author": ["M.H.M. Winands", "Y. Bj\u00f6rnsson", "J-T. Saito"], "venue": "6th International Conference on Computers and Games ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Monte Carlo tree search in Lines of Action", "author": ["M.H.M. Winands", "Y. Bj\u00f6rnsson", "J-T. Saito"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, 2(4):239\u2013250", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].", "startOffset": 31, "endOffset": 38}, {"referenceID": 18, "context": "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].", "startOffset": 31, "endOffset": 38}, {"referenceID": 3, "context": "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 18, "context": "The most popular MCTS algorithm is UCT [19], which performs a single simulation from the root of the search tree to a terminal state at each iteration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "The first was Coulom\u2019s original maximum backpropagation [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "Maximum backpropagation has also recently been used in other Monte Carlo search algorithms and demonstrated success in probabilistic planning, as an alternative type of forecaster in BRUE [11] and as Bellman backups for online dynamic programming in Trial-based Heuristic Tree Search [17].", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "Maximum backpropagation has also recently been used in other Monte Carlo search algorithms and demonstrated success in probabilistic planning, as an alternative type of forecaster in BRUE [11] and as Bellman backups for online dynamic programming in Trial-based Heuristic Tree Search [17].", "startOffset": 284, "endOffset": 288}, {"referenceID": 12, "context": "The first use of enhancing MCTS using prior knowledge was in Computer Go [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "This technique was also confirmed to be advantageous in Breakthrough [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Another way to introduce prior knowledge is via a progressive bias during selection [8], which has significantly increased performance in Go play strength [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Another way to introduce prior knowledge is via a progressive bias during selection [8], which has significantly increased performance in Go play strength [7].", "startOffset": 155, "endOffset": 158}, {"referenceID": 25, "context": "In games where minimax search performs well, such as Kalah, modifying MCTS to use minimaxstyle backups and heuristic values instead to replace playouts offers a worthwhile trade-off under different search time settings [26].", "startOffset": 219, "endOffset": 223}, {"referenceID": 34, "context": "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].", "startOffset": 195, "endOffset": 199}, {"referenceID": 17, "context": "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].", "startOffset": 209, "endOffset": 217}, {"referenceID": 20, "context": "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].", "startOffset": 209, "endOffset": 217}, {"referenceID": 19, "context": "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].", "startOffset": 236, "endOffset": 240}, {"referenceID": 32, "context": "The prime example is MCTS-Solver [33], which backpropagates proven wins and losses as extra information in MCTS.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "Scorebounded MCTS extends this idea to games with multiple outcomes, leading to \u03b1\u03b2-style pruning in the tree [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "One can use shallow-depth minimax searches in the tree to initialize nodes during expansion, enhance the playout, or to help MCTS-Solver in backpropagation [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 24, "context": "Finally, recent work has attempted to explain and identify some of the shortcomings that arise from estimates in MCTS, specifically compared to situations where classic minimax search has historically performed well [25, 24].", "startOffset": 216, "endOffset": 224}, {"referenceID": 23, "context": "Finally, recent work has attempted to explain and identify some of the shortcomings that arise from estimates in MCTS, specifically compared to situations where classic minimax search has historically performed well [25, 24].", "startOffset": 216, "endOffset": 224}, {"referenceID": 13, "context": ", moves that initially seem promising but then later prove to be bad, such as sufficiency thresholds [14] and shallow minimax searches [2].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": ", moves that initially seem promising but then later prove to be bad, such as sufficiency thresholds [14] and shallow minimax searches [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a\u2032 using", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a\u2032 using", "startOffset": 148, "endOffset": 151}, {"referenceID": 18, "context": "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a\u2032 using", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "Then, similarly to RAVE [13], rather than using Q\u0302 = Q for selection in Equation 1, we use Q\u0302 (s, a) = (1\u2212 \u03b1) r s,a ns,a + \u03b1v s,a, (2)", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "When MCTS-Solver is enabled, proven values take precedence in the selection policy and the resulting scheme is informative and consistent [27], so Algorithm 1 converges to the optimal choice eventually.", "startOffset": 138, "endOffset": 142}, {"referenceID": 19, "context": "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].", "startOffset": 103, "endOffset": 115}, {"referenceID": 25, "context": "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].", "startOffset": 103, "endOffset": 115}, {"referenceID": 20, "context": "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].", "startOffset": 103, "endOffset": 115}, {"referenceID": 2, "context": "This approach has been used as a \u201cmercy rule\u201d in Go [3] and quite successfully in Lines of Action [34].", "startOffset": 52, "endOffset": 55}, {"referenceID": 33, "context": "This approach has been used as a \u201cmercy rule\u201d in Go [3] and quite successfully in Lines of Action [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Another option is to use an -greedy playout policy that chooses a successor randomly with probability and successor state with the largest evaluation with probability 1 \u2212 , with improved performance in Chinese Checkers [29, 22], abbreviated \u201cege \u201d.", "startOffset": 219, "endOffset": 227}, {"referenceID": 21, "context": "Another option is to use an -greedy playout policy that chooses a successor randomly with probability and successor state with the largest evaluation with probability 1 \u2212 , with improved performance in Chinese Checkers [29, 22], abbreviated \u201cege \u201d.", "startOffset": 219, "endOffset": 227}, {"referenceID": 15, "context": "Kalah has been weakly solved for several different variants of Kalah [16], and was used as a domain to compare MCTS variants to classic minimax search [26].", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "Kalah has been weakly solved for several different variants of Kalah [16], and was used as a domain to compare MCTS variants to classic minimax search [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "Therefore, as was done in [26], our experiments produce random starting board positions without any stones placed in the stores.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "The evaluation function was the same one used in [26], the difference between stones in each player\u2019s stores.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "Breakthrough was first introduced in general game-playing competitions and has been identified as a domain that is particularly difficult for MCTS due to traps and uninformed playouts [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 29, "context": "Our playout policy always chooses one-ply \u201cdecisive\u201d wins and prevents immediate \u201canti-decisive\u201d losses [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "The first one is a simple one found in Maarten Schadd\u2019s thesis [28] that assigns each piece a score of 10 and the further row achieved as 2.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "The second one is the more sophisticated one giving specific point values for each individual square per player described in a recent paper by Lorentz & Horey [20], which we abbreviate \u201cefLH\u201d.", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "4) against maximum backpropagation proposed as an alternative backpropagation in the original MCTS work [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "Another question is whether to prefer implicit minimax backups over node priors (abbreviated np) [13], which initializes each new leaf node with wins and losses based on prior knowledge.", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "Node priors were first used in Go, and have also used in path planning problems [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "We use the scheme that worked well in [20] which takes into account the safety of surrounding pieces, and scales the counts by the time setting (10 for one second, 50 for five seconds).", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "MCTS Using Lorentz & Horey Evaluation Function We now run experiments using the more sophisticated evaluation function from [20], efLH, that assigns specific piece count values depending on their position on the board.", "startOffset": 124, "endOffset": 128}, {"referenceID": 31, "context": "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].", "startOffset": 77, "endOffset": 85}, {"referenceID": 22, "context": "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].", "startOffset": 77, "endOffset": 85}, {"referenceID": 34, "context": "The MCTS player is MC-LOA, whose implementation and enhancements are described in [35].", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "Then, we enable shallow \u03b1\u03b2 searches in the playouts described in [32].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].", "startOffset": 95, "endOffset": 106}, {"referenceID": 1, "context": "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].", "startOffset": 95, "endOffset": 106}, {"referenceID": 13, "context": "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].", "startOffset": 95, "endOffset": 106}, {"referenceID": 14, "context": "In watching several Breakthrough games, it seems that MCTS with implicit minimax backups builds \u201cfortress\u201d structures [15] that are then handled better than standard MCTS.", "startOffset": 118, "endOffset": 122}, {"referenceID": 1, "context": "We hope to compare or combine implicit minimax backups to/with other minimax hybrids from [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 30, "context": "For example, \u03b1 could be changed based on the outcomes of each choice made during the game, and Q(s, a) could be used for online search bootstrapping of evaluation function weights [31].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "Finally, the technique could also work in general game-playing using learned evaluation functions [12].", "startOffset": 98, "endOffset": 102}], "year": 2014, "abstractText": "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic \u03b1\u03b2 search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.", "creator": "LaTeX with hyperref package"}}}