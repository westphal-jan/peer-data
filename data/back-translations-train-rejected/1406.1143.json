{"id": "1406.1143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2014", "title": "Identifying Duplicate and Contradictory Information in Wikipedia", "abstract": "Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.", "histories": [["v1", "Wed, 4 Jun 2014 18:59:00 GMT  (112kb,D)", "http://arxiv.org/abs/1406.1143v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.DL cs.SI", "authors": ["sarah weissman", "samet ayhan", "joshua bradley", "jimmy lin"], "accepted": false, "id": "1406.1143"}, "pdf": {"name": "1406.1143.pdf", "metadata": {"source": "CRF", "title": "Identifying Duplicate and Contradictory Information in Wikipedia", "authors": ["Sarah Weissman", "Samet Ayhan", "Joshua Bradley", "Jimmy Lin"], "emails": [], "sections": [{"heading": "Introduction", "text": "Readers of Wikipedia often notice that several articles contain very similar or even identical passages. In some cases, these are duplicate articles marked for merging, but in other cases, there is also an overlap in content. For example, the article about a hurricane and the article about the place where it went ashore might share the same content about the effects of the natural disaster. Identical content is most likely the result of copy and paste between articles, but interestingly, readers occasionally come across highly similar content stating contradictory facts. In a distributed environment where anyone can edit content, these observations may not be surprising, and in this paper we try to rigorously characterize these phenomena by treating the problem as that of near-duplicate sentence recognition. We adapt location-based hash techniques (LSH) to provide clues for nearly duplicate sentences in Wikipedia. We believe that this problem is interesting in several ways: For duplicate sentences, we quantify the scope before Wikipedia replicates the scope."}, {"heading": "Related Work", "text": "The problem we address in this paper relates to a few other problems that have been examined before. Virtually duplicated web page detection (Henzinger 2006) is important in search because web pages are often copied or mirrored with only minor differences (e.g. ads or navigation bars); however, it would be desirable to return only the \"canonical\" versions in search results. In fact, the algorithm we use in this paper, minhash (Broder 1997), was originally developed for just that purpose. Another closely related problem is the detection of plagiarism (Si, Leong, and Lau 1997), or more generally, the \"reuse of texts\" (Bendersky and Croft 2009), which usually focuses on smaller text segments that target entire documents. However, approaches similar to Shingling are applicable to both problems."}, {"heading": "Near-Duplicate Sentence Detection", "text": "We start with a parameterized family of N hash functions Fi, 1 \u2264 i \u2264 n. Each sentence in a Wikipedia article is broken down into n-gram \"shingles\" (at character level), and for shingles a set of M signatures is generated for each sentence (i.e., M drawings of K from N). The signature of a document d is represented as a vector of K minhashes randomly selected from the sentence N. To remember this, we generate M signatures for each sentence (i.e., M drawings of K from N). Broder demonstrates a direct relationship between Minhash collisions (i.e., documents that share the same signature) and their Jaccard similarities, which form the theoretical basis for why and how Minhash \"works."}, {"heading": "Parameter Tuning", "text": "One of the complexities of applying Minhash to real data sets is the myriad of parameters that need to be selected - each setting manifests a compromise between precision, callback, and computational effort. Our approach to parameter setting was based on a combination of analytical calculations and hand settings based on studying the results. We started by defining the hash scheme and the size of the hash family (N = 20), and then generated 10 signatures (M = 10) per input set. Based on Broder's probability of a match for sets A and B, this can be expressed as follows: P [Match (A, B)] = 1 \u2212 (1 \u2212 sK) Mwhere s = Jaccard (A, B). With the settings above, the effects of different K's are shown in Figure 1. Based on this analysis, we set K = 10. This means that if we choose 0.9 Jaccard similarity as our target."}, {"heading": "Final Cluster Generation", "text": "The output of Minhash is a group of clusters where each cluster represents a signature collision. As we generate multiple signatures per set, it is possible for one set to appear in multiple clusters. We adopt the standard practice of merging all clusters that share at least one common set. Cluster merging is performed in one run (outsideMapReduce) with a Union Find data structure (Cormen et al. 2001). In Union Find, each node maintains a pointer to a header node for the set. We maintain a lookup table of each record id on its node in the Union Finde structure. Iterating over the input clusters, we look up the node for each set, merge clusters or create new ones as needed. When merging two clusters, the header node of one cluster is changed to point to the head of the other. Once all clusters are processed, we run a pass over the lookup table to get the assignment of clusters through nodes."}, {"heading": "Experimental Results", "text": "This year, the time has come for us to be able to get to grips with the problems that have been mentioned."}, {"heading": "Future Work and Conclusions", "text": "In this work, we applied Minhash to the problem of near-duplicate detection on Wikipedia. Our implementation of MapReduce is highly scalable and processes English Wikipedia in a short time on a modest cluster. We found that there is a considerable amount of duplicate content."}, {"heading": "Templates", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Identical", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Copyediting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Factual Drift", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Other", "text": "How can we respond to this analysis? We could imagine a robot that monitors Wikipedia to detect inconsistencies and asks editors to intervene and dissolve it. Such a service would be valuable to improve the internal consistency and quality of Wikipedia. There are several directions in which our work can be expanded. Currently, our technique only identifies clusters of almost duplicate sentences - our analysis lacks the notion of the flow of information: what was the source article and what was the goal of copying and pasting? Are there copy chains in which the content has been progressively copied from one article to the next, with possible \"twigs\" (and accumulations of errors along the way)? Our analysis of the large adjacent clusters suggests that there are complex editing histories that form tree-like structures. In addition, there are editor-specific effects?"}], "references": [{"title": "R", "author": ["Bayardo"], "venue": "J.; Ma, Y.; and Srikant, R.", "citeRegEx": "Bayardo. Ma. and Srikant 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "W", "author": ["M. Bendersky", "Croft"], "venue": "B.", "citeRegEx": "Bendersky and Croft 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["Broder"], "venue": "Z.", "citeRegEx": "Broder 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "R", "author": ["T.H. Cormen", "C.E. Leiserson", "Rivest"], "venue": "L.; and Stein, C.", "citeRegEx": "Cormen et al. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "and Ghemawat", "author": ["J. Dean"], "venue": "S.", "citeRegEx": "Dean and Ghemawat 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce", "author": ["J. Lin"], "venue": null, "citeRegEx": "Lin,? \\Q2009\\E", "shortCiteRegEx": "Lin", "year": 2009}, {"title": "R", "author": ["A. Si", "H.V. Leong", "Lau"], "venue": "W. H.", "citeRegEx": "Si. Leong. and Lau 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "L", "author": ["B. Stvilia", "M.B. Twidale", "Smith"], "venue": "C.; and Gasser, L.", "citeRegEx": "Stvilia et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "M", "author": ["Vernica, R.", "Carey"], "venue": "J.; and Li, C.", "citeRegEx": "Vernica. Carey. and Li 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "B", "author": ["D.M. Wilkinson", "Huberman"], "venue": "A.", "citeRegEx": "Wilkinson and Huberman 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [], "year": 2014, "abstractText": "Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.", "creator": "LaTeX with hyperref package"}}}