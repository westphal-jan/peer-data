{"id": "1406.1167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2014", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "abstract": "Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers' outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.", "histories": [["v1", "Wed, 4 Jun 2014 09:16:42 GMT  (80kb)", "http://arxiv.org/abs/1406.1167v1", "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)"]], "COMMENTS": "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xu-cheng yin", "chun yang", "hong-wei hao"], "accepted": false, "id": "1406.1167"}, "pdf": {"name": "1406.1167.pdf", "metadata": {"source": "CRF", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "authors": ["Xu-Cheng Yin", "Hong-Wei Hao"], "emails": ["xuchengyin@ustb.edu.cn"], "sections": [{"heading": null, "text": "However, it is difficult to establish a definitive link between the measure of diversity and ensemble accuracy. Given a list of available component classifiers, how to be adaptable and diverse ensemble classifiers will be a major challenge in the literature. In this paper, we argue that diversity, not direct diversity with data, is highly correlated with ensemble accuracy, and we propose a novel technology for ensemble classifiers who learn to diversify by taking into account accuracy and versatility."}, {"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 RELATED WORK", "text": "The first aims to combine multiple classifiers at the output level, where the results of several available classifiers are combined to solve the problem, e.g. multiple classification systems (classification combination). For example, in most learning application systems there are a lot of decision makers derived from different homogeneous or heterogeneous resources. Given a list of these learners, an applicable solution is to merge all of these decisions, e.g. the merger framework in Waston DeepQA [24]. In this essay, we focus on the second. Namely, when putting together multiple classifiers (available or subsequently learned, homogeneous or heterogeneous), the classification ensemble is divided into the whole ensemble by combining smart of these component optimizers."}, {"heading": "2.1 Diversity-Based Classifier Ensembles", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2.2 Kernel Methods", "text": "In this space, a variety of methods can be used to find relationships in the data. Since mapping can be fairly general (e.g. not necessarily linear), the relationships found in this way are correspondingly very general. Depending on the situation and application, many core functions are proposed, e.g. Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common. Their definitions are as follows: The Linear Kernel is the simplest core function. It is characterized by the inner product < x, y > of the variable pairs plus an optional constant c (x, y)."}, {"heading": "3 NOTATIONS", "text": "Let the training record be Tr = {x1, x2,..., xN}, where yi is the output of the sample xi, and all outputs are in C classes {\u03c91, \u03c92,..., \u03c9C}. The base classifier H = {h1, h2,..., hL} of diversity are trained on the training, and the output of a base classifier hj (xi) is hj (xi). Given each base classifier hj, along with its weight wj, we define the vector of classifier weights as w = [w1, w2, wL], where the number of B classes hj is hj = 1, wj [wj].D, we focus on the linear combination of classifiers. By taking a weighted voice among the base classifiers and selecting the class consent receives the largest weighted voice, the group H classifies the sample xi as H (xi)."}, {"heading": "4 LEARNING TO DIVERSIFY", "text": "In diversity-based ensembles, diversity is measured by the results of the classifiers. We argue that measuring diversity by kernel can be more adaptable and useful for different data in a variety of situations. Furthermore, we are also sure that measuring diversity by kernel by introducing weights, i.e. the results of the classifiers are weighted, can be more representative of different data with noise and redundancy in diversity-based ensembles. Consequently, we expand ensemble diversity by introducing weighted cores and propose a novel ensemble method that learns diversity by means of weighted cores. We hope that this new method can learn to appropriately select and combine a subset of classifiers by taking into account both accuracy and (adaptive) diversity."}, {"heading": "4.1 Learning to Diversify via Weighted Kernels", "text": "We first summarize our learning and diversification problem with weighted kernels (L2DWK) by giving a direct but simple optimization criterion. Given the correctness of the results of the classifiers Y = {y1, y2,..., xN}, classifier weights w, a kernel k for a classifier's output and the core weights w, we optimize the following criterion, minw \u2212 \u2212 loss (Y, k\u03b1, w) \u2212 Div (Y, k\u03b1, w), s.t. w 0, 1Tw = 1. (10), where loss () is the loss function of the classification error, div () is the diversity of the ensemble, and \u03bb is the diversity control parameter. Obviously, this criterion maximizes the accuracy and diversification of the ensemble at the same time."}, {"heading": "4.2 Self-Training Algorithm", "text": "In general, it is difficult to find the solution to the optimization in Equation (14) without knowing both w (classification weights) and (\u03b1 / k) (core weights). However, for known k and \u03b1, the optimization is simplified to a convex square programming problem for learning classification weights. Consequently, we propose a self-training algorithm (shown in Algorithm I) for L2DWK. In this learning algorithm, there are three important steps: initialization, classification weight calculation and grain weight updating, which are presented in detail in the following sections. Finally, we also empirically analyze the convergence performance of this self-training algorithm."}, {"heading": "4.2.1 Initialization", "text": "In the self-training algorithm, first the training set Tr and the basic classifiers H are assigned for optimization, then the pairwise diversity D. Then the maximum epoch T is set as a stop constraint and \u03b1i = 1 / N for each sample xi, where N is the number of samples in Tr. In our method we can use general nuclei (k), e.g. linear, polynomic, and Gaussian nuclei."}, {"heading": "4.2.2 Classifier Weight Calculation", "text": "In each iteration we first use Equation (14) and Equation (11) to calculate the basic classifier weight vector w. As described above, this optimization can be solved as a typical convex quadratic programming problem. Subsequently, samples of the training set Tr are classified with w and the error rate of the ensemble classification t is calculated."}, {"heading": "4.2.3 Kernel Weight Updating", "text": "Updating the kernel weights \u03b1t is a key process in L2DWK = DWge classification. In the algorithm of self-classification, we assume that the kernel weights \u03b1t + 1 have a relationship to \u03b1t, and we use a dynamically damped trick, i.e., the damped weights \u03b2t (0, 1] and \u03b2t \u2264 \u03b2t + 1. We set \u03b2t for the t-th iteration as\u03b2t = 1t. (20) Then we use the following equation to update the kernel weights for the (t + 1) th iteration, \u03b1t + 1 = \u03b2t \u2264 \u03b2t + (1 \u2212 \u03b2t) \u03b1 t t t t, (21) for the t-th iteration to update the original and new kernel weights respectively. Here, we want to update the new weight vector \u03b1t for the kernel weight update to increase the weights of the slightly misclassified samples."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we first compare our L2DWK methods extensively with various state-of-the-art methods on a variety of 32 UCI classification data sets. Then, in order to select parameters in L2DWK, we perform some experiments with different values of the diversity control parameter \u03bb and different nuclei. Next, we present experiments on ensembles with different regularization components (e.g. accuracy and / or diversity with or without weighted nuclei). Finally, we empirically analyze the relationship between accuracy and diversity in ensembles."}, {"heading": "5.1 Experimental Setting and Datasets", "text": "Our L2DWK method is performed with three common cores, i.e., linear (L2DWK-Linear), Gaussian (L2DWK-Gauss) and polynomial cores (L2DWKPoly). We use base classifiers generated from two base ensembles, dredging and random forests. Please note that without specification, in the following X.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED CORE FOR CLASSIFIER ENSEMBLE 7Algorithm I: Self-training algorithm for Learning TO Diversify via Weighted Kernels (L2DWK) Input: the training set. | Tr | = N H = {h1, h2, hL}: the base classifier set, | H}: the base classifier set."}, {"heading": "6 CONCLUSIONS", "text": "Classifier ensemble is widely regarded as an effective method to improve the accuracy of basic classifiers, which has a variety of applications in patterns. https: / / code.google.com / p / randomforest-matlab /. 3. We conduct experiments with L2DWK methods using both L2DWK hinge (Equation (22) and L2DWK-Exp (Equation (23)). Since the experimental results are very similar, we present the results only with L2DWK hinge in this paper. 4. The number of basic classifiers for dredging is always 101 by default, while the number for random forests is sometimes 501 by default. In our experiments we choose 301, a mean value for both dredging and random forests.Recognition, information retrieval and data mining. Generally speaking, the interaction of different classifiers should allow us to achieve higher accuracy."}, {"heading": "ACKNOWLEDGMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, pp. 122\u2013140, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Experiments with a new Boosting algorithm", "author": ["Y. Freund", "R. Schapire"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201996), 1996, pp. 148\u2013156.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to Boosting", "author": ["\u2014\u2014"], "venue": "Journal of Computer and System Sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural Networks, vol. 5, no. 2, pp. 241\u2013260, 1992.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, pp. 5\u201332, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Trans. Pattern Analysis Machine Intelligence, vol. 12, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Ensembling neural networks: Many could be better than all", "author": ["Z.H. Zhou", "J. Wu", "andW. Tang"], "venue": "Artificial Intelligence, vol. 137, pp. 239\u2013263, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z.-H. Zhou"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 6, pp. 1370\u20131382, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear dependency modeling for classifier fusion and feature combination", "author": ["A.J. Ma", "P.C. Yuen", "J.-H. Lai"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 5, pp. 1135\u20131148, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating accurate and diverse members of a neural network ensemble", "author": ["D.W. Opitz", "J.W. Shavlik"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201996). MIT Press, 1996, pp. 535\u2013541.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical mechanics of ensemble learning", "author": ["A. Krogh", "P. Sollich"], "venue": "Physical Review E, vol. 55, no. 1, pp. 811\u2013825, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Z.-H. Zhou"], "venue": "Boca Raton, FL: Chamman & Hall/CRC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Selective ensemble under regularization framework", "author": ["N. Li", "Z.H. Zhou"], "venue": "Proceedings of International Workshop on Multiple Classifier Systems, 2009, pp. 293\u2013303.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Pruning in ordered Bagging ensembles", "author": ["G. Mart\u0131\u0301nez-Mu\u00f1oz", "A. Su\u00e1rez"], "venue": "Proceedings of International Conference on Machine learning (ICML\u201906), 2006, pp. 609\u2013616.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "An analysis of ensemble pruning techniques based on ordered aggregation", "author": ["G. Martinez-Munoz", "D. Hernandez-Lobato", "A. Suarez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 245\u2013259, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The sources of increased accuracy for two proposed Boosting algorithms", "author": ["D.B. Skalak"], "venue": "Proceeding of American Association for Artificial Intelligence (AAAI\u201996), 1996.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Relationship between combination methods and measures of diversity in combining classifiers", "author": ["C.A. Shipp", "L. Kuncheva"], "venue": "Information Fusion, vol. 3, pp. 135\u2013148, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Measures of diversity in classifier ensembles", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine Learning, vol. 51, no. 2, pp. 181\u2013 207, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Analysis of the correlation between majority voting error and the diversity measures in multiple classifier systems", "author": ["D. Ruta", "B. Gabrys"], "venue": "Proceedings of International Symposium on Soft Computing and Intelligent Systems for Industry, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Hypothesis diveristy in ensemble classification", "author": ["L. Saitta"], "venue": "Proceedings of International Symposium ISMIS, 2006, pp. 662\u2013670.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["F. Radlinski", "R. Kleinberg", "T. Joachims"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201908), 2008, pp. 784\u2013791.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Redundancy, diversity and interdependent document relevance", "author": ["F. Radlinski", "P.N. Bennett", "B. Carterette", "T. Joachims"], "venue": "SIGIR Forum, vol. 43, no. 2, pp. 46\u201352, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Ensemble pruning via semi-definite programming", "author": ["Y. Zhang", "S. Burer", "W.N. Street", "K. Bennett", "E. Parradohern"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 1315\u20131338, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "A framework for merging and ranking of answers in DeepQA", "author": ["D. Gondek", "A. Lally", "A. Kalyanpur", "J. Murdock", "P. Duboue", "L. Zhang", "Y. Pan", "Z. Qiu", "C. Welty"], "venue": "IBM Journal of Research and Development, vol. 56, no. 3-4, pp. 14:1\u201314:12, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictive ensemble pruning by expectation propagation", "author": ["H. Chen", "P. Tino", "X. Yao"], "venue": "IEEE Trans. Knowledge and Data Engineering, vol. 21, no. 7, pp. 999\u20131013, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Design of effective neural network ensembles for image classification processes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image Vision and Computing, vol. 19, no. 9-10, pp. 699\u2013707, 2000.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "The Kappa statistic in reliability studies: Use, interpretation, and sample size requirements", "author": ["J. Sim", "C.C. Wright"], "venue": "Physical Therapy, vol. 85, no. 3, pp. 257\u2013268, 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Pruning adaptive Boosting", "author": ["D.D. Margineantu", "T.G. Dietterich"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201997), 1997, pp. 211\u2013218.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Diversity regularized ensemble pruning", "author": ["N. Li", "Y. Yu", "Z.-H. Zhou"], "venue": "ECML/PKDD, 2012, pp. 330\u2013345.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "On the combination of accuracy and diversity measures for genetic selection of Bagging fuzzy rule-based multiclassification systems", "author": ["K. Trawinski", "A. Quirin", "O. Cordon"], "venue": "Proceedings of International Conference on Intelligent Systems Design and Applications, 2009, pp. 121\u2013127.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Classifier ensemble using a heuristic learning with sparsity and diversity", "author": ["X.-C. Yin", "K. Huang", "H.-W. Hao", "K. Iqbal", "Z.-B. Wang"], "venue": "Proceedings of International Conference on Neural Information Processing (ICONIP\u201912), 2012, pp. 100\u2013107.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "A novel classifier ensemble method with sparsity and diversity", "author": ["\u2014\u2014"], "venue": "Neurocomputing, 2013, accepted.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013 271, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to kernel-based learning algorithms", "author": ["K.-R. Muller", "S. Mika", "G. Ratsch", "K. Tsuda", "B. Scholkopf"], "venue": "IEEE Trans. Neural Networks, vol. 12, no. 2, pp. 181\u2013201, 2001.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel methods: A survey of current techniques", "author": ["C. Campbell"], "venue": "Neurocomputing, vol. 48, no. 1-4, pp. 63\u201384, 2002.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Scholkopf", "A.J. Smola"], "venue": "The Ananls of Statistics, vol. 36, no. 3, pp. 1171\u20131220, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian generalized kernel mixed models", "author": ["Z. Zhang", "G. Dai", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 111\u2013139, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "The wave kernel signature: A quantum mechanical approach to shape analysis", "author": ["M. Aubry", "U. Schlickewei", "D. Cremers"], "venue": "ICCV Workshops, 2011, pp. 1626\u20131633.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelet kernel support vector machines forecasting techniques: Case study on water-level predictions during typhoons", "author": ["C.-C. Wei"], "venue": "Expert Systems and Applications, vol. 39, no. 5, pp. 5189\u20135199, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning, vol. 46, no. 1-3, pp. 389\u2013422, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Component selection and smoothing in multivariate nonparametric regression", "author": ["Y. Lin", "H.H. Zhang"], "venue": "The Annals of Statistics, vol. 34, no. 5, pp. 2272\u20132297, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Gaussian Processes in Machine Learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Adaptive scaling for feature selection in SVMs.", "author": ["Y. Grandvalet", "S. Canu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Feature selection for nonlinear kernel support vector machines.", "author": ["O.L. Mangasarian", "G. Kou"], "venue": "in ICDM Workshops,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "An efficient method for gradient-based adaptation of hyperparameters in SVM models.", "author": ["S.S. Keerthi", "V. Sindhwani", "O. Chapelle"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "More generality in efficient multiple kernel learning", "author": ["M. Varma", "B.R. Babu"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201909), 2009, pp. 1065\u20131072.  X.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED KERNELS FOR CLASSIFIER ENSEMBLE  9", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Are loss functions all the same?", "author": ["L. Rosasco", "D.V. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "[13], [14], [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14], [15].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[13], [14], [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 251, "endOffset": 255}, {"referenceID": 19, "context": "Some researchers even argued that in practice it was not possible to define and use a diversity measure that is clearly linked to the ensemble accuracy [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": ", Q statistics) performed well on artificial data, but was inadequate for realistic datasets [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "In information retrieval, given a list of documents or web pages, learning to diversify is to learn to rank and present a diverse set of results by considering both relevance and diversity [21], [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "In information retrieval, given a list of documents or web pages, learning to diversify is to learn to rank and present a diverse set of results by considering both relevance and diversity [21], [22].", "startOffset": 195, "endOffset": 199}, {"referenceID": 6, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 168, "endOffset": 171}, {"referenceID": 12, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 22, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 261, "endOffset": 265}, {"referenceID": 1, "context": ", Boosting [2] and Bagging [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": ", Boosting [2] and Bagging [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 23, "context": ", the merging framework in the Waston DeepQA [24].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "From the view of ensemble pruning, Zhou divided related methods into three categories: ordering-based pruning, clustering-based pruning, and optimizationbased pruning approaches [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "Main techniques of optimization-based pruning include heuristic optimization [7], mathematical programming (e.", "startOffset": 77, "endOffset": 80}, {"referenceID": 22, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 15, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 127, "endOffset": 130}, {"referenceID": 17, "context": "Specifically, Kuncheva and Whitaker compared 10 popular diversity measures (4 pair-wise and 6 non-pair-wise ones) on a variety of benchmark datasets [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 27, "context": "proposed a Kappa pruning approach, which aimed at maximizing the pair-wise diversity among the selected component classifiers [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "sorted component classifiers by diversity and selected the top 20% \u2212 40% classifiers for combining [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Following this analysis, they also proposed the diversity regularized ensemble pruning method [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "used Genetic Algorithm (GA) to search the best classifier subset by a linear combination of accuracy and diversity [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 30, "context": "proposed a heuristic learning approach with diversity and sparsity to learn classifiers\u2019 weights and combine multiple classifiers [31], [32].", "startOffset": 130, "endOffset": 134}, {"referenceID": 31, "context": "proposed a heuristic learning approach with diversity and sparsity to learn classifiers\u2019 weights and combine multiple classifiers [31], [32].", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "[23] proposed a Semi-Definite Programming (SDP) approach, which formulates the ensemble pruning problem as a quadratic integer programming problem and solve it by a semi-definite programming solution technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed Regularized Selective Ensemble (RSE) approach, which formulates the ensemble pruning problem as a quadratic programming problem, and learns classifier weights that have the optimal accuracydiversity trade-off.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "For example, Ruta and Gabrys [19] studied the relationship between majority voting errors and diversity measures operating on binary classifier outputs, and found that some measures were consistently correlated with majority vote error, but there was no clear correlation between diversity and accuracy.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Kuncheva and Whitaker pointed out that it was difficult to give a definitive connection between the diversity measures and the improvement of the accuracy, and the use of diversity measures for enhancing the design of classifier ensembles was still an open question [18].", "startOffset": 266, "endOffset": 270}, {"referenceID": 19, "context": "Saitta analyzed and experimented a numerous diversity measures, and concluded that all these measures have only a very loose relation with the expected accuracy of the classifier [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": ", the minimum margin of an ensemble was not monotonically increasing with respect to diversity [33].", "startOffset": 95, "endOffset": 99}, {"referenceID": 33, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 27, "endOffset": 31}, {"referenceID": 36, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 41, "endOffset": 45}, {"referenceID": 37, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 52, "endOffset": 56}, {"referenceID": 38, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "proposed SVM-RFE (Recursive Feature Elimination) to eliminate features from the full set sequentially, and used it to select genes for cancer classification [40].", "startOffset": 157, "endOffset": 161}, {"referenceID": 40, "context": "Lin and Zhang proposed COSSO (COmponent Selection and Smoothing Operator) to select features in smoothing spline regression by breaking up the regularization term into components on individual dimensions [41].", "startOffset": 204, "endOffset": 208}, {"referenceID": 41, "context": "One alternative solution for this problem is the weighted kernel method, which parameterizes the kernel with a weight on each feature [42].", "startOffset": 134, "endOffset": 138}, {"referenceID": 41, "context": "where \u03b2 are the weights and the Gaussian kernel is performed in Gaussian processes [42].", "startOffset": 83, "endOffset": 87}, {"referenceID": 42, "context": "Similarly, some researchers also used SVMs for feature selection [43], [44].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "Similarly, some researchers also used SVMs for feature selection [43], [44].", "startOffset": 71, "endOffset": 75}, {"referenceID": 44, "context": "proposed an efficient algorithm that alternates between learning an SVM and optimizing the feature weights [45].", "startOffset": 107, "endOffset": 111}, {"referenceID": 45, "context": "Varma and Babu proposed a projected gradient method with l1 regularization for multiple kernel learning (with weighted kernels) and reported encouraged results in a variety of benchmark vision and UCI databases [46].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "For example, the diversity matrixes with two common pairwise diversity measures (Disagreement [16] and Double Fault [26]) are shown below.", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "For example, the diversity matrixes with two common pairwise diversity measures (Disagreement [16] and Double Fault [26]) are shown below.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "We call such convex optimization as Quadratic Programming problem with Diversity (QPD) for classifier ensemble, the similar idea of which can be seen in [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": ", the damped factor \u03b2t \u2208 [0, 1] and \u03b2t \u2264 \u03b2t+1.", "startOffset": 25, "endOffset": 31}, {"referenceID": 46, "context": "One method, called L2DWK-Hinge, gets the idea from the hinge loss [47].", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "The other method, called L2DWK-Exp, gets the idea from the adaptive re-weighting step in Boosting [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Obviously, there are \u2211N i=1 \u03b1 t i = 1 (\u2200t) and \u03b1 t i \u2208 [0, 1] (\u2200 t, i).", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "\u03b2t: a parameter that \u03b2t \u2208 [0, 1], and \u03b2t \u2264 \u03b2t+1.", "startOffset": 26, "endOffset": 32}], "year": 2014, "abstractText": "Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers\u2019 outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.", "creator": "LaTeX with hyperref package"}}}