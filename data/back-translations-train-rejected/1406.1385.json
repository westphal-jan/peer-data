{"id": "1406.1385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2014", "title": "Learning the Information Divergence", "abstract": "Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the beta-divergence family. Selecting the best beta then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate alpha-divergence in terms of beta-divergence, which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence. Furthermore, we show the connections between gamma and beta-divergences as well as R\\'enyi and alpha-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.", "histories": [["v1", "Thu, 5 Jun 2014 13:44:25 GMT  (358kb)", "http://arxiv.org/abs/1406.1385v1", "12 pages, 7 figures"]], "COMMENTS": "12 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["onur dikmen", "zhirong yang", "erkki oja"], "accepted": false, "id": "1406.1385"}, "pdf": {"name": "1406.1385.pdf", "metadata": {"source": "CRF", "title": "Learning the Information Divergence", "authors": ["Onur Dikmen", "Zhirong Yang"], "emails": ["onur.dikmen@aalto.fi;", "rong.yang@aalto.fi;", "erkki.oja@aalto.fi"], "sections": [{"heading": null, "text": "In fact, it is not that it is a pure bugbear, but that it is a pure bugbear that focuses on itself. (...) It is not that it is a bugbear. (...) It is as if it is a bugbear. (...) It is a bugbear. (...), (...) It is a bugbear. (...), (...), (...), (...), (...), (...), (...), (...), (...), (...) (...), ((...), (...), ((...), (...), (...), (...), (...), (...) (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...)."}, {"heading": "III. DIVERGENCE SELECTION BY STATISTICAL LEARNING", "text": "A conventional automatic selection method is cross-validation [33], [34] in which the training uses only a portion of the x-entries and the remaining ones for validation. This method has a number of disadvantages: 1) It is only applicable to divergences where the entries are separable (e.g. \u03b1- or \u03b2-divergence); omitting some entries for \u03b3- and R\u00e9nyi-divergences is not feasible due to logarithm or normalization; 2) Entry separation is not applicable in some application areas 3, where all entries are required for learning, e.g. cluster analysis. Our proposal is to use the well-known and proven technique of maximum probability assessment for automatic divergence selection, whereby a suitable approach to divergence and very flexible divergence selection in other families is to be discussed."}, {"heading": "A. Selecting \u03b2-divergence", "text": "1) Tweeden Likelihood (MTL): \u03b2 = \u03b2 \u03b2????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "B. Selecting \u03b1-divergence", "text": "We extend the MEDAL method to \u03b1 divergence selection. This is done by relating \u03b1 divergence to \u03b2 divergence, with a nonlinear transformation between \u03b1 and \u03b2. We have D\u03b2 (yi | | mi) = 1\u03b2 (\u03b2 + 1) (y\u03b2 + 1i + \u03b2m \u03b2 + 1 i \u2212 (\u03b2 + 1) yim\u03b2i) = \u2212 \u03b12 \u03b1 \u2212 1 (xi \u03b12 + 1 \u2212 \u03b1 \u00b5i \u03b12 \u2212 \u03b1 \u03b12 \u03b12 \u03b11 \u03b1i \u03b12\u03b1 \u00b51 \u2212 \u03b1i \u03b12 (yi; mi, \u03b2)) = D\u03b1 (xi | \u00b5i). This relationship allows us to evaluate the probability of \u00b5 and \u03b1 by means of yi and \u03b2: p (xi; \u00b5i, \u03b1, \u0445, \u0445) = p (yi; mi, \u03b2) = p (yi; \u03b2 \u00b2) = p (xi), which is the best form."}, {"heading": "C. Selecting \u03b3- and R\u00e9nyi divergences", "text": "Above, we presented the selection methods for two families in which the divergence is divisible by the tensor entries. Next, we consider the selection between the two families in which their members are inseparable. Our strategy is to reduce the \u03b3 divergence to \u03b2 divergence with a connecting scalar, formally resulting in the following result. Theorem 2: For x divergence and R\u00e9nyi divergence families in which their members are inseparable, the Argmin divergence is reduced to \u03b2 divergence. Theorem 2: For x divergence, the derived right side is eroded with respect to c (details in Appendix C). Theorem 2 states that in a positive scale, the learning problem formulated by \u03b3 divergence corresponds to the same by the corresponding \u03b2 divergence, the latter is separable and can be solved by the methods described in Appendix C."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we present the proposed method for different types of data and learning tasks. First, we provide the results on synthetic data whose density is known to compare the behavior of MTL, MEDAL, and the score matching method [29]. Second, we illustrate the advantage of EDA density over ED. Third, we apply our method for \u03b1- and \u03b2-divergence selection in nonnegative matrix factorization (NMF) to real data such as music and stock prices. Fourth, we test MEDAL in selecting non-separable cases (e.g. \u03b3 divergence) for projective NMF and s-SNE visualization learning tasks using synthetic data, images, and a social dolphin network."}, {"heading": "A. Synthetic data", "text": "1) \u03b2 divergence selection: We use scalar data generated by the four specific cases of tweedie distributions (\u03b2 \u03b2 distributions), \u03b2 \u03b2 distributions, namely inverse Gaussian, Gamma, Poisson, and Gaussian distributions. We simply match the best tweedie, EDA, or ED density to the data using either the maximum probability method or score matching (SM). Figure 1 (first row) shows the results of the maximum tweedie likelihood (MTL) successfully. The \u03b2 value, which maximizes the probability in the tweedie distribution, is consistent with the true parameters, i.e., -2, -1, and 1 for the above distributions, respectively. Note that Tweedie distributions are not defined for \u03b2 (0, 1), but \u03b2 divergence is defined in this region, which will result in discontinuity in the third row probability over \u03b2."}, {"heading": "B. Divergence selection in NMF", "text": "The goal of non-negative matrix factorization (NMF) is to find a low-level approximation to the observed data by expressing it as the product of two non-negative matrices, i.e. V \u2248 V = WH with V-RF \u00b7 N +, W-RF \u00b7 K + and H-RK \u00b7 N +. This goal is pursued by minimizing the information divergence between the data and the approximation, i.e., D (V | V). The divergence can be pursued for the data / application such as \u03b2, \u03b1, \u03b3, R\u00e9nyi, etc. Here we have chosen \u03b2 and \u03b1 divergences to illustrate the MEDAL method for realistic data.The optimization of \u03b2-NMF was implemented using the usual multiplicative rules of updating [23], [38]. Similar multiplicative updating rules are also available for \u03b1-NMF [23]."}, {"heading": "C. Stock Prices", "text": "Next, we repeat the same experiment with a stock price record that contains the Dow Jones Industrial Average. There are 30 companies included in the data. These are large American companies from different sectors such as services (e.g. Walmart), consumer goods (e.g. General Motors) and healthcare (e.g. Pfizer). The data were collected from January 3, 2000 to July 27, 2011, a total of 2543 trading dates. We set K = 5 in NMF and mask 50% of the data according to the following pattern [39]. The corresponding best stock curves are represented in Figure 4 (left). The EDA probability curve is represented with \u03b2 [\u2212 2, 2] is shown in Figure 4 (bottom left). \u2212 The best deviation selected by MEDAL is \u03b2 = 0.4. The corresponding best stock curves = 0.006. These results are consistent with the results of the shares of Tan and F\u00e9votte [39] using the remaining 50% of the data. \u2212 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,"}, {"heading": "D. Selecting \u03b3-divergence", "text": "In this section we show that the proposed method can be applied to applications beyond the NMF and that inseparable divergence families go beyond the divergence. In our knowledge there are no other existing methods to handle these two cases. [We have a 1000-dimensional stochastic-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "V. CONCLUSIONS", "text": "We have presented a new method called MEDAL to automatically select the best information divergence in a parametric divergence method < \u03b2 divergence method is based on a statistical learning approach in which divergence is learned as a result of the standard density parameter estimation. Maximizing the probability of tweedie distribution is an easy way to select \u03b2 divergence, but it has some shortcomings. We have proposed a novel distribution, exponential divergence with augmentation (EDA), which overcomes these shortcomings and can therefore provide a more robust method. http: / www-personal.umich.edu / ~ mejn / netdata / selection for the parameters over a wide range. The new method has been extended to alpha divergence selection through a nonlinear transformation, and we have provided new results that link the two methods - the selectable and the non-selectable - to expand the divergence cases."}, {"heading": "VI. ACKNOWLEDGMENT", "text": "This work was financially supported by the Academy of Finland (Finnish Center of Excellence in Computational Inference Research COIN, grant number 251170; Zhirong Yang also under decision number 140398)."}, {"heading": "APPENDIX A INFINITE SERIES EXPANSION IN TWEEDIE DISTRIBUTION", "text": "In the series extension, an EDM random variable is represented as the sum of the G-independent gamma random variables x = \u2211 G g yg, where G is distributed with the parameters \u03bb = \u00b5 2 \u2212 p\u03c6 (2 \u2212 p); and the form and scale parameters of the gamma distribution are \u2212 a and b, with a = 2 \u2212 p1 \u2212 p and b = \u03c6 (p \u2212 1) \u00b5p \u2212 1. The pdf of the tweedie distribution is analytically obtained at x = 0 as e \u2212 \u00b52 \u2212 p\u03c6 (2 \u2212 p). For x > 0, the function f (x, \u03c6, p) = 1 x \u00b2 j = 1 Wj (x, \u03c6, p), where for 1 < p < 2Wj = x \u2212 yes (p \u2212 1) ja\u03c6j (1 \u2212 p) (1 \u2212 jaj! (25) and for ammp > 2Wj \u2212 summa (1 + ja) < more < more (x) j (1 \u2212 jaj)."}, {"heading": "APPENDIX B GAUSS-LAGUERRE QUADRATURES", "text": "With this method (e.g. [48]) definite integrals of the form \u0442 \u221e 0e \u2212 zf (z) dz \u2248 n \u2211 if (zi) wi, (27) can be evaluated, where zi is the ith root of the n-th order of the Laguerre polynomial Ln (z) and the weights of wi = zi (n + 1) 2L2n (zi) are given. (28) The recursive definition of Ln (z) results from Ln + 1 (z) = 1n + 1 [(2n + 1 \u2212 z) Ln (z) \u2212 nLn \u2212 1 (z)], (29) 11with L0 (z) = 1 and L1 (z) = 1 \u2212 z. In our experiments we used the Matlab implementation of angles3 with n = 5000."}, {"heading": "APPENDIX C PROOFS OF THEOREMS 2 AND 3", "text": "Lemon 4: argminz af (1 + 1) argminz a ln f (\u03b2) \u03b2 (\u03b2) \u03b2 (z) > 0. The proof of the problem is simply derived from the monotonicity of ln. (30) To attribute it to min\u00b5mincD\u03b2 (x), we get: min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min. (x) min."}], "references": [{"title": "A generalized divergence measure for nonnegative matrix factorization,", "author": ["R. Kompass"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Generalized nonnegative matrix approximations with Bregman divergences,", "author": ["I.S. Dhillon", "S. Sra"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Non-negative matrix factorization with \u03b1-divergence,", "author": ["A. Cichocki", "H. Lee", "Y.-D. Kim", "S. Choi"], "venue": "Pattern Recognition Letters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Stochastic neighbor embedding,", "author": ["G. Hinton", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Visualizing data using t-SNE,", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Rethinking collapsed variational bayes inference for lda,", "author": ["I. Sato", "H. Nakagawa"], "venue": "in International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Divergence measures and message passing,", "author": ["T. Minka"], "venue": "Microsoft Research, Tech. Rep.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A measure of asymptotic efficiency for tests of a hypothesis based on a sum of observations,", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "Differential-Geometrical Methods in Statistics", "author": ["S. Amari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "Robust and efficient estimation by minimising a density power divergence,", "author": ["A. Basu", "I.R. Harris", "N. Hjort", "M. Jones"], "venue": "Biometrika, vol", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Robust paramater estimation with a small bias against heavy contamination,", "author": ["H. Fujisawa", "S. Eguchi"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Families of alpha- beta- and gammadivergences: Flexible and robust measures of similarities,", "author": ["A. Cichocki", "S.-i. Amari"], "venue": "Entropy, vol. 12,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization,", "author": ["A. Cichocki", "S. Cruces", "S.-I. Amari"], "venue": "Entropy, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Eine informationstheoretische ungleichung und ihre anwendung auf den beweis der ergodizitat von markoffschen ketten,", "author": ["I. Csisz\u00e1r"], "venue": "Publications of the Mathematical Institute of Hungarian Academy of Sciences Series A,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Markov processes and the h-theorem,", "author": ["T. Morimoto"], "venue": "Journal of the Physical Society of Japan,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1963}, {"title": "The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming,", "author": ["L.M. Bregman"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1967}, {"title": "Kullback-leibler divergence for nonnegative for nonnegative matrix factorization,", "author": ["Z. Yang", "H. Zhang", "Z. Yuan", "E. Oja"], "venue": "Proceedings of 21st International Conference on Artificial Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Projective nonnegative matrix factorization with \u03b1-divergence,", "author": ["Z. Yang", "E. Oja"], "venue": "Proceedings of 19th International Conference on Artificial Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Nonnegative matrix factorization with the Itakura-Saito divergence. With application to music analysis,", "author": ["C. F\u00e9votte", "N. Bertin", "J.-L. Durrieu"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Exponential dispersion models,", "author": ["B. J\u00f8rgensen"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1987}, {"title": "Nonnegative Matrix and Tensor Factorization", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Alpha/beta divergences and Tweedie models,", "author": ["Y.K. Yilmaz", "A.T. Cemgil"], "venue": "CoRR, vol. abs/1209.4280,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Estimation of non-normalized statistical models using score matching,", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Learning the parts of objects by nonnegative matrix factorization,", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Projective nonnegative matrix factorization for image compression and feature extraction,", "author": ["Z. Yuan", "E. Oja"], "venue": "Proceedings of 14th Scandinavian Conference on Image Analysis,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Linear and nonlinear projective nonnegative matrix factorization,", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Selecting \u03b2-divergence for nonnegative matrix factorization by score matching,", "author": ["Z. Lu", "Z. Yang", "E. Oja"], "venue": "Proceedings of the 22nd International Conference on Artificial Neural Networks (ICANN", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Robust blind source separation by beta divergence,", "author": ["M. Minami", "S. Eguchi"], "venue": "Neural Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "On measures of information and entropy,", "author": ["A. R\u00e9nyi"], "venue": "Procedings of 4th Berkeley Symposium on Mathematics, Statistics and Probability,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1960}, {"title": "Robust prewhitening for ica by minimizing beta-divergence and its application to fastica,", "author": ["M. Mollah", "S. Eguchi", "M. Minami"], "venue": "Neural Processing Letters,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning alpha-integration with partially-labeled data,", "author": ["H. Choi", "S. Choi", "A. Katake", "Y. Choe"], "venue": "Proc. of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Series evaluation of Tweedie exponential dispersion model densities,", "author": ["P.K. Dunn", "G.K. Smyth"], "venue": "Statistics and Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Some extensions of score matching,", "author": ["A. Hyv\u00e4rinen"], "venue": "Comput. Stat. Data Anal.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Algorithms for nonnegative matrix factorization with the beta-divergence,", "author": ["C. F\u00e9votte", "J. Idier"], "venue": "Neural Computation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Automatic relevance determination in nonnegative matrix factorization with the \u03b2-divergence,", "author": ["C.F.V. Tan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Probabilistic latent semantic indexing,", "author": ["T. Hofmann"], "venue": "in International Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1999}, {"title": "Quadratic nonnegative matrix factorization,", "author": ["Z. Yang", "E. Oja"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts?", "author": ["D. Donoho", "V. Stodden"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}, {"title": "Automatic rank determination in projective nonnegative matrix factorization,", "author": ["Z. Yang", "Z. Zhu", "E. Oja"], "venue": "Proceedings of the 9th International Conference on Latent Variable Analysis and Signal Separation (LVA2010),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "On Estimation of a Probability Density Function and Mode,", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1962}, {"title": "Low-rank kernel learning with bregman matrix divergences,", "author": ["B. Kulis", "M.A. Sustik", "I.S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th ed", "author": ["M. Abramowitz", "I.A. Stegun", "Eds"], "venue": "New York: Dover,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1972}], "referenceMentions": [{"referenceID": 0, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 158, "endOffset": 161}, {"referenceID": 17, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 18, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 206, "endOffset": 210}, {"referenceID": 18, "context": "For example, Euclidean distance is suitable for data with Gaussian noise; Kullback-Leibler divergence has shown success for finding topics in text documents [7]; and Itakura-Saito divergence has proven to be suitable for audio signal processing [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": ", Score Matching (SM) [25], specifically proposed for nonnormalized densities.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 24, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 26, "context": "We also demonstrate that our method outperforms Score Matching on Exponential Divergence distribution (ED), a previous approach for \u03b2-divergence selection [29].", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "\u2022 \u03b1-divergence [10], [11] is defined as", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "\u2022 \u03b1-divergence [10], [11] is defined as", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "\u2022 \u03b2-divergence [30], [31] is defined as", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "\u2022 \u03b3-divergence [13] is defined as", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "\u2022 R\u00e9nyi divergence [32] is defined as", "startOffset": 19, "endOffset": 23}, {"referenceID": 29, "context": "A conventional automatic selection method is cross-validation [33], [34], where the training only uses part of the entries of x and the remaining ones are used for validation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "A conventional automatic selection method is cross-validation [33], [34], where the training only uses part of the entries of x and the remaining ones are used for validation.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Selecting \u03b2-divergence 1) Maximum Tweedie Likelihood (MTL): We start from the probability density function (pdf) of an exponential dispersion model (EDM) [22]:", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "The canonical parameter and the cumulant function that satisfy this property are [22]", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "The function can be expanded with infinite series [35] or approximated by saddle point estimation [36].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": ", [23], [24]): maximizing the likelihood of Tweedie distribution for certain p values is equivalent to minimizing the corresponding divergence with \u03b2 = 1\u2212 p.", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": ", [23], [24]): maximizing the likelihood of Tweedie distribution for certain p values is equivalent to minimizing the corresponding divergence with \u03b2 = 1\u2212 p.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "The existing software using the infinite series expansion approach [35] (see Appendix A) is prone to numerical computation problems especially for \u22120.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Obviously f(x) is continuous and bounded for x \u2208 [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 22, "context": "Finally, let us note that in addition to the maximum likelihood estimator, Score Matching (SM) [25], [37] can be applied to estimation of \u03b2 as a density parameter (see Section IV-A).", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "Finally, let us note that in addition to the maximum likelihood estimator, Score Matching (SM) [25], [37] can be applied to estimation of \u03b2 as a density parameter (see Section IV-A).", "startOffset": 101, "endOffset": 105}, {"referenceID": 26, "context": "[29] proposed a similar exponential divergence (ED) distribution pED(x;\u03bc, \u03b2) \u221d exp [\u2212D\u03b2(x||\u03bc)] , (21)", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "First we provide the results on synthetic data, whose density is known, to compare the behavior of MTL, MEDAL and the score matching method [29].", "startOffset": 140, "endOffset": 144}, {"referenceID": 26, "context": "The loglikelihood and negative score matching objectives [29] on the same four datasets are shown.", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "The ED distribution [29] has an advantage that it is defined also for \u03b2 \u2208 (0, 1).", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "The optimization of \u03b2-NMF was implemented using the standard multiplicative update rules [23], [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 33, "context": "The optimization of \u03b2-NMF was implemented using the standard multiplicative update rules [23], [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Similar multiplicative update rules are also available for \u03b1-NMF [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "1) A Short Piano Excerpt: We consider the piano data used in [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "The found maximum likelihood estimate \u03b2 = \u22121 corresponds to Itakura-Saito divergence, which is in harmony with the empirical results presented in [21] and the common belief that IS divergence is most suitable for audio spectrograms.", "startOffset": 146, "endOffset": 150}, {"referenceID": 34, "context": "We set K = 5 in NMF and masked 50% of the data by following [39].", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "These results are in harmony with the findings of Tan and F\u00e9votte [39] using the remaining 50% of the data as validation set, where they found that \u03b2 \u2208 [0, 0.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Our finding also justifies the usage of KL-divergence in topic models with the multinomial distribution [40], [7].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "Thus it is a special case of Quadratic Nonnegative Matrix Factorization (QNMF) [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "The block entries are uniformly drawn from [0, 10].", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "We then added uniform noise from [0, 1] to the all matrix entries.", "startOffset": 33, "endOffset": 39}, {"referenceID": 25, "context": "For each \u03b3, we ran the multiplicative algorithm of PNMF by Yang and Oja [28], [42] to obtain W and V\u0302.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "We also tested MEDAL on the swimmer dataset [43] which is popularly used in the NMF field.", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "3) Symmetric Stochastic Neighbor Embedding: Finally, we show an application beyond NMF, where MEDAL is used to find the best \u03b3-divergence for the visualization using Symmetric Stochastic Neighbor Embedding (s-SNE) [5], [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "3) Symmetric Stochastic Neighbor Embedding: Finally, we show an application beyond NMF, where MEDAL is used to find the best \u03b3-divergence for the visualization using Symmetric Stochastic Neighbor Embedding (s-SNE) [5], [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 39, "context": "The background illustrates the node density by the Parzen method [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "[47]) that are defined over eigenvalues of matrices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Dunn and Smyth [35] described an approach to select a subset of these infinite terms to accurately approximate f(x, \u03c6, p).", "startOffset": 15, "endOffset": 19}, {"referenceID": 41, "context": "[48]) can evaluate definite integrals of the form \u222b \u221e", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the \u03b2-divergence family. Selecting the best \u03b2 then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate \u03b1-divergence in terms of \u03b2-divergence, which enables automatic selection of \u03b1 by maximum likelihood with reuse of the learning principle for \u03b2-divergence. Furthermore, we show the connections between \u03b3and \u03b2-divergences as well as R\u00e9nyiand \u03b1-divergences, such that our automatic selection framework is extended to nonseparable divergences. Experiments on both synthetic and realworld data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.", "creator": "LaTeX with hyperref package"}}}