{"id": "1406.1827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Recursive Neural Networks Can Learn Logical Semantics", "abstract": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, but it remains an open question whether they can learn compositional semantic grammars that support logical deduction. We address this question directly by for the first time evaluating whether each of two classes of neural model --- plain RNNs and recursive neural tensor networks (RNTNs) --- can correctly learn relationships such as entailment and contradiction between pairs of sentences, where we have generated controlled data sets of sentences from a logical grammar. Our first experiment evaluates whether these models can learn the basic algebra of logical relations involved. Our second and third experiments extend this evaluation to complex recursive structures and sentences involving quantification. We find that the plain RNN achieves only mixed results on all three experiments, whereas the stronger RNTN model generalizes well in every setting and appears capable of learning suitable representations for natural language logical inference.", "histories": [["v1", "Fri, 6 Jun 2014 22:09:27 GMT  (112kb)", "http://arxiv.org/abs/1406.1827v1", null], ["v2", "Sun, 14 Dec 2014 20:37:33 GMT  (153kb)", "http://arxiv.org/abs/1406.1827v2", null], ["v3", "Tue, 3 Mar 2015 19:48:45 GMT  (102kb,D)", "http://arxiv.org/abs/1406.1827v3", null], ["v4", "Thu, 14 May 2015 19:37:38 GMT  (119kb,D)", "http://arxiv.org/abs/1406.1827v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["samuel r bowman", "christopher potts", "christopher d manning"], "accepted": false, "id": "1406.1827"}, "pdf": {"name": "1406.1827.pdf", "metadata": {"source": "CRF", "title": "Recursive Neural Networks for Learning Logical Semantics", "authors": ["Samuel R. Bowman", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.18 27v1 [cs.CL] 6 Jun 2"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to move, to move, to move and to move."}, {"heading": "2 Recursive neural network models", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "3 Reasoning about semantic relations", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "4 Recursive structure in propositional logic", "text": "This year, it is time for us to set out to find a solution that paves the way for the future, to pave the way for the future."}, {"heading": "5 Reasoning with natural language quantifiers and negation", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 General discussion", "text": "The results indicate that RNTNs, but not simple RNNs, are capable of meeting the challenges of these tasks with reasonably large training sets. These positive results are promising for the future of learned representation models in applied modeling of compositional semantics. Of course, challenges remain. In terms of our experimental data, even the RNTN lags behind the perfection of our more complex tasks, with performance steadily declining as the depth of recursion grows. It remains to be seen whether these deficits can be overcome by improvements in the model, optimization procedures, or linguistic representations. Furthermore, subtle questions remain about how these models can be assessed in a logical way in the logical structure that we are able to learn the underlying multiple functions so that we learn them with limited tension."}, {"heading": "Acknowledgments", "text": "We thank Jeffrey Pennington, Richard Socher and the audience at CSLI and Nuance."}], "references": [{"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "TACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "An efficient easily adaptable system for interpreting natural language queries", "author": ["David H.D. Warren", "Fernando C.N. Pereira"], "venue": "American Journal of Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1982}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M. Zelle", "Raymond J. Mooney"], "venue": "In Proceedings of AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Concrete sentence spaces for compositional distributional models of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh", "Stephen Clark", "Bob Coecke", "Stephen Pulman"], "venue": "In Proceedings of IWCS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In Proceedings of EACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of IWCS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A latent discriminative model for compositional entailment relation recognition using natural logic", "author": ["Yotaro Watanabe", "Junta Mizuno", "Eric Nichols", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192", "author": ["Samuel R. Bowman"], "venue": "Presented at ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A complete calculus of monotone and antitone higher-order functions", "author": ["Thomas F. Icard", "Lawrence S. Moss"], "venue": "Proceedings of Topology, Algebra, and Categories in Logic,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recent progress on monotonicity", "author": ["Thomas F. Icard", "Lawrence S. Moss"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Partee. Compositionality", "author": ["H. Barbara"], "venue": "Varieties of Formal Semantics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1984}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In Proceedings of ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 166, "endOffset": 172}, {"referenceID": 1, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 166, "endOffset": 172}, {"referenceID": 2, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 5, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 6, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 7, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 8, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 9, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 10, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 11, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 12, "context": "We address this question in the context of natural language inference (also known as recognizing textual entailment; [13]), in which the goal is to determine the core inferential relationship between two sentences.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Much of the theoretical work on this task (and some successful implemented models [14, 15]) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language.", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Much of the theoretical work on this task (and some successful implemented models [14, 15]) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language.", "startOffset": 82, "endOffset": 90}, {"referenceID": 15, "context": "Following [16], we use the natural logic developed by [14] as our formal model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Following [16], we use the natural logic developed by [14] as our formal model.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "The formal properties and inferential strength of this system are now well-understood [17, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "The formal properties and inferential strength of this system are now well-understood [17, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "Table 1: The seven natural logic relations of [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "classes of neural model \u2014 plain RNNs and recursive neural tensor networks (RNTNs, [2]) \u2014 can learn those relationships correctly.", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "We study neural models that adhere to the linguistic principle of compositionality, which says that the meanings for complex expressions are derived from the meanings of their constituent parts via specific composition functions [19, 20].", "startOffset": 229, "endOffset": 237}, {"referenceID": 15, "context": "We use the model architecture proposed in [16] and depicted in Figure 1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "A sigmoid nonlinearity (element-wise tanh) is applied to the output of either layer function, following [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Rather than use a tanh nonlinearity here, we follow [16] in using a rectified linear function.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "In particular, we use the leaky rectified linear function [22]: f(~x) = max(~x, 0) + 0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "We train the model using stochastic gradient descent (SGD) with learning rates computed using AdaGrad [23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "[24] show that a matrix-vector RNN model somewhat similar to our RNTN can learn a logic, but the logic discussed here is a much better approximation of the kind of expressivity needed for natural language.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "As opposed to the two-way contrasts seen in [24], this logic distinguishes between 2 = 64 possible assignments of truth values, and expressions of this logic define arbitrary conditions on these possible assignments, for a total of 2 possible statements that the intermediate vector representations need to be able to distinguish if we are to learn this logical system in its full generality.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Quantification is far from the only place in natural language where complex functional meanings are found, but it is a natural starting point, since it can be tested in sentences whose structures are otherwise quite simple, and since it has formed a standard case study in prior formal work on natural language inference [18].", "startOffset": 321, "endOffset": 325}, {"referenceID": 15, "context": "This experiment replicates similar work described in [16], which found that RNTNs can learn to reason well with quantifier meanings given sufficient training data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "It remains to be seen whether these deficiencies can be overcome with improvements to the model, the optimization procedures, or the linguistic representations [3, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 11, "context": "It remains to be seen whether these deficiencies can be overcome with improvements to the model, the optimization procedures, or the linguistic representations [3, 12].", "startOffset": 160, "endOffset": 167}], "year": 2014, "abstractText": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, but it remains an open question whether they can learn compositional semantic grammars that support logical deduction. We address this question directly by for the first time evaluating whether each of two classes of neural model \u2014 plain RNNs and recursive neural tensor networks (RNTNs) \u2014 can correctly learn relationships such as entailment and contradiction between pairs of sentences, where we have generated controlled data sets of sentences from a logical grammar. Our first experiment evaluates whether these models can learn the basic algebra of logical relations involved. Our second and third experiments extend this evaluation to complex recursive structures and sentences involving quantification. We find that the plain RNN achieves only mixed results on all three experiments, whereas the stronger RNTN model generalizes well in every setting and appears capable of learning suitable representations for natural language logical inference.", "creator": "LaTeX with hyperref package"}}}