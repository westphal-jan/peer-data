{"id": "1406.1831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Analyzing noise in autoencoders and deep networks", "abstract": "Autoencoders have emerged as a useful framework for unsupervised learning of internal representations, and a wide variety of apparently conceptually disparate regularization techniques have been proposed to generate useful features. Here we extend existing denoising autoencoders to additionally inject noise before the nonlinearity, and at the hidden unit activations. We show that a wide variety of previous methods, including denoising, contractive, and sparse autoencoders, as well as dropout can be interpreted using this framework. This noise injection framework reaps practical benefits by providing a unified strategy to develop new internal representations by designing the nature of the injected noise. We show that noisy autoencoders outperform denoising autoencoders at the very task of denoising, and are competitive with other single-layer techniques on MNIST, and CIFAR-10. We also show that types of noise other than dropout improve performance in a deep network through sparsifying, decorrelating, and spreading information across representations.", "histories": [["v1", "Fri, 6 Jun 2014 22:49:11 GMT  (749kb,D)", "http://arxiv.org/abs/1406.1831v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ben poole", "jascha sohl-dickstein", "surya ganguli"], "accepted": false, "id": "1406.1831"}, "pdf": {"name": "1406.1831.pdf", "metadata": {"source": "CRF", "title": "Analyzing noise in autoencoders and deep networks", "authors": ["Ben Poole", "Jascha Sohl-Dickstein", "Surya Ganguli"], "emails": ["poole@cs.stanford.edu", "sganguli}@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most of them will be able to show themselves that they are able, are able to achieve their goals, and that they are not able to achieve their goals. If they are able to achieve their goals, they will see themselves able to achieve their goals. If they are able to achieve their goals, they will be able to achieve their goals, they will be able to achieve the goals that they have set themselves. If they are able to achieve their goals, they will be able to achieve their goals. If they are able to achieve their goals, they will be able to achieve their goals."}, {"heading": "2 Autoencoder Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Autoencoders and Denoising Autoencoders", "text": "An autoencoder is a type of single-layer neural network that is trained to reconstruct its input. In the complete case, this can be achieved trivially through identity transformation. However, if the network is restricted in any way, the autoencoder tends to learn a more interesting representation of the input data, which can be useful for other tasks such as object detection. The autoencoder consists of an encoder that assigns input to a hidden representation: f (x) = sf (Wx + b), and a decoder that reproduces the hidden representation back to the input: g (h) = sg (W \u2032 h + d). The composition of the encoder and the decoder results in the reconstruction function: r (x) = g (f (x))). The typical training criterion for autoencoders is to minimize the reconstruction error (Es x, r (x) as a corrupt version."}, {"heading": "2.2 Noisy Autoencoder Model", "text": "Inspired by recent work on the dropout, we are expanding the denociation of autoencoders to allow the injection of additional noise at the input and output of the hidden units. We call these models noisy autoencoders (NAEs) because their hidden representations are stochastical, and no longer a deterministic function of the input. Injection of noise into the inputs and hidden representations of autoencoders has been proposed for linear networks in previous work by [11], but has not been analyzed in detail for nonlinear representations. We parameterize noise in the NAE as a tuple (I, Z, H) = f = sf (W (I) + b) (H)."}, {"heading": "2.3 Relationships between noise types", "text": "Due to the parameterization of noise in the noisy encoder, there are many possible possibilities of noise (I, Z, H) that result in an equivalent effective noise on the reconstruction. In particular, we can always rewrite a NAE so that the only source of noise is the hidden activations. To analyze the effects of introducing noise before the nonlinearity of the encoder, we perform a Taylor extension of the first-order encoder function: sf (Wx + Z) \u2248 sf (Wx) + diag (s \u00b2 f (Wx))))) Z. Thus, if the input noise is Gauss with a certain covariance, then the equivalent hidden input noise is Gauss with covariance. If the singular values of W are sufficiently small, then we can make the input noise an effective unit (H \u00b2) with a certain covariance (Wx)."}, {"heading": "2.4 Marginalized Noise Penalties", "text": "We assume that the decoder is linear (sg (x) = x), that the loss function used is squared error, and that all corruption is applied independently to each neuron. This allows us to precisely marginalize the noise on the hidden activation of the unit based on a result from [3]: E [ig x \u2212 r (x) x \u2212 r (x) x \u2212 r (x) x + tr (WWTVar (h))) (3) We can then apply the approximation results from above to obtain cost functions that correspond independently to the marginalization of any other type of noise: L (W) x x x x x x x x (r) x (r) + cH (x) + cZ (x) + cI (x) cH (x) = 1 Var (h) x x x x (W) x x x (W) x (W) x (W) x (W) x (W) x (W) x x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x)."}, {"heading": "3 Connections to prior work", "text": "The noisy autoencoder and associated marginalized noise provide a framework for comparing many types of regulated autoencoders and help explain the benefits of injecting noise into more general neural networks."}, {"heading": "3.1 Regularized Autoencoders", "text": "The marginalized input notes from the notebook provide an intuition into the success of the encoder. When we use bound weights, the noise is punished with the dislike of the dislike.) This penalty encourages the hidden units to strengthen the dislike of the dislike of the dislike of the dislike of the dislike of the dislike."}, {"heading": "3.2 Dropout", "text": "Dropout is a simple regulator that is used in the formation of neural networks and applies multiplicative Bernoulli noise to all units of a neural network [10]. This noise has been shown to effectively regulate deep neural networks and has been associated with preventing the co-adaptation of neurons and the model mean [9]. The primary motivation for NAEs was Dropout's success in improving generalization performance. We can analyze the effect of dropout noise in NAEs by calculating the corresponding Marginalized Noise Penalty: cH (x) = \u2211 d i = 1 Var (h-i | h). The shrinkage of the projective field helps to reduce the sensitivity of reconstruction to the elimination of hidden units."}, {"heading": "3.3 Other models", "text": "Recent neuroscientific work has shown that single-layer models trained and optimized with input and output noise to maximize mutual information produce receptive fields similar to those of biological systems [7, 12]. Similar to our work, these models demonstrate the importance of input noise and hidden activation noise for learning representations. In semantic hashing, Gaussian noise is injected onto an intermediate layer to learn binary representations [19]. These representations allow rapid adaptation in low-dimensional space using binary codes. To our knowledge, the advantage of adding noise is not discussed in terms of the accuracy of the fully trained autoencoder."}, {"heading": "4 Autoencoder Experiments", "text": "Our theoretical analysis of noisy auto-encoders shows that NAEs can implement a variety of regulated auto-encoder penalties. We evaluated the effectiveness of noisy auto-encoders in learning representations through a variety of experiments on natural images, MNIST and CIFAR-10. All experiments used stochastic gradient descent with impulses to train models. We found that this dynamics was critical in the formation of both auto-encoders and supervised deep networks. Learning rates, pile size and additional hyperparameters were selected by a combination of manual and automatic grid search on validation sets. We considered auto-encoders with a significant encoder, linear decoder and quadratic error losses. We experimented with isotropic auto-input noise with fixed variance \u03c32I, isotropic output noise with a significant encoder, linear decoder and quadratic error losses."}, {"heading": "4.1 Denoising Natural Images", "text": "In our first experiment, we investigated the effect of failure noise on the generalization performance of a noisy auto encoder. We trained two NAEs on 12x12 fields from the van Hateren image set [20]. The first NAE had noise on the input but no hidden activations (just a PCS), while the second additionally had failure noise with p = 0.5 on the hidden activations. We evaluated the failure performance on an independent set of image fields with a noise variance corresponding to the corrupting input noise of the NAE, and calculated the average reconstruction error over 1000 noisy inputs. The NAE with and without dropout had average reconstruction errors of 2.5 and 3.2, respectively. Thus, NAEs are able to improve the failure performance compared to typical DAEs."}, {"heading": "4.2 Effect on MNIST Features", "text": "In order to better understand the effects of dropouts on NAE functions, we trained a series of models with different input noise and failure noise on hidden activations. For this experiment, we used smaller networks with 250 hidden units and trained only the first 10,000 digits of MNIST. In Figure 2 (left), we show the effects of input noise and hidden activation noise on the learned characteristics. Without input or hidden activation noise, the noisy autoencode reduces to a vanilla autoencode and tends to learn characteristics that do not capture an interesting structure in the data. If we increase the input noise, we learn characteristics that capture an increasingly large local structure in the digits, as in DAEs [21]. Increasing hidden activation noise leads to more global characteristics that tend to resemble large strokes or digit segments. If we find both input noise and the larger activation noise, we increase the number noise."}, {"heading": "4.3 MNIST Classification", "text": "In order to better assess the effects of hidden input and activation noise on NAE classification performance, we trained larger models with 2000 hidden units and fixed the Gaussian input noise variance at 0.1. We looked at Gaussian noise at the hidden device inputs and both dropout and Gaussian noise at the hidden device activations. These models were used as an initialization for an MLP that was trained with standard backward propagation. We found that NAEs with a dropout of p = 0.25 had the lowest test error of 138.1http: / / github.com / lisa-lab / pylearn2Considering the pre-designed NAE, we can also perform the same backward noise as this model with 1.0."}, {"heading": "4.4 CIFAR-10 Classification", "text": "To test the usefulness of NAEs in other models, we also analyze the CIFAR-10 dataset. We train a NAE with \u03c32I = 0.25, dropout hidden activation noise with p = 0.5 and 800 units on 6x6 fields extracted from the CIFAR-10 dataset. Using these features, we extract a high-grade representation of full-size images using the pipeline from [6]. Training an SVM classifier on the resulting representation yielded an accuracy of 74.5%. This accuracy is slightly lower than that for the higher order CAE [18], but better than any other regulated autocode representation. Training a DAE yields a lower accuracy of 73.6%, suggesting that the addition of failure noise during learning functions is helpful."}, {"heading": "4.5 Purely Supervised MNIST", "text": "We have shown that different types of noise can be used to regulate hidden representations and fine-tune the classification performance on MNIST and CIFAR-10. In addition, we found that both dropouts and additive Gaussian noise can be applied to hidden activations, while fine-tuning can improve the classification error. Here, we are experimenting with a deep MNIST model from [10]. This model consists of two hidden layers of 1200 rectified linear units and was trained with dropouts on the inputs and hidden activations. Previously, it was the state-of-the-art result for individual models that do not incorporate prior knowledge or training, but have recently been surpassed by maxout networks [9]. Both networks use dropouts during training and the same scaling that we perform at test times. Instead of training with dropouts, we use other types of noise on the input and hidden units, the dropouts we add to noise and palate activations."}, {"heading": "5 Discussion", "text": "In summary, a unifying principle for auto-encoders, namely the robustness of auto-encoders for injecting noise at all levels of internal representation, we have created a framework for viewing a variety of previous training algorithms through a unified lens. Different decisions about where to inject noise, and what kind of noise, result in different algorithms. As a result, we can generate many new training algorithms by designing noise, not only in NAE's, but also in networks that are directly monitored. With these techniques, we can achieve very good performance in benchmark tasks. We are able to obtain the best-tuned layer models on MNIST, as well as the best-tuned model (a) (b) deep models that do not involve any prior knowledge or practice. However, we emphasize that we have not even begun to explore the full power of NAEs, as we do not systematically inject the type of space and where there is space for and where there is space."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["Guozhong An"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Deep Learning of Representations: Looking Forward", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["Chris M Bishop"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Marginalized Denoising Autoencoders for Domain Adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A Coates", "H Lee", "A Y Ng"], "venue": "Ann Arbor,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Efficient coding of spatial information in the primate retina", "author": ["E Doi", "J L Gauthier", "G D Field", "J Shlens"], "venue": "The Journal of ", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Quoc Le", "Andrew Saxe", "Honglak Lee", "Andrew Y Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "Pairwise Correlation", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Improved Generalization by Adding both Auto-Association and Hidden-Layer- Noise to Neural-Network-Based-Classifiers", "author": ["H Inayoshi", "T Kurita"], "venue": "Machine Learning for Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Ica with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S Rifai", "P Vincent", "X Muller", "X Glorot", "Y Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Adding noise to the input of a model trained with a regularized objective", "author": ["Salah Rifai", "Xavier Glorot", "Yoshua Bengio", "Pascal Vincent"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Higher order contractive auto-encoder", "author": ["Salah Rifai", "Gr\u00e9goire Mesnil", "Pascal Vincent", "Xavier Muller", "Yoshua Bengio", "Yann Dauphin", "Xavier Glorot"], "venue": "In ECML PKDD\u201911: Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "author": ["J Hans van Hateren", "Arjen van der Schaaf"], "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P Vincent", "H Larochelle", "I Lajoie", "Y Bengio", "P A Manzagol"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Regularization through noise [3, 1] has regained focus recently in the training of supervised neural networks.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Regularization through noise [3, 1] has regained focus recently in the training of supervised neural networks.", "startOffset": 29, "endOffset": 35}, {"referenceID": 10, "context": "Randomly dropping out units while performing backpropagation has been shown to consistently improve the performance of large neural networks [13, 10].", "startOffset": 141, "endOffset": 149}, {"referenceID": 8, "context": "Randomly dropping out units while performing backpropagation has been shown to consistently improve the performance of large neural networks [13, 10].", "startOffset": 141, "endOffset": 149}, {"referenceID": 18, "context": "Stochastic pooling, where a set of input units are gated based off their activations, has also been shown to improve performance in convolutional nets over noiseless max and average pooling [23, 15].", "startOffset": 190, "endOffset": 198}, {"referenceID": 12, "context": "Stochastic pooling, where a set of input units are gated based off their activations, has also been shown to improve performance in convolutional nets over noiseless max and average pooling [23, 15].", "startOffset": 190, "endOffset": 198}, {"referenceID": 17, "context": "The role of input noise in training unsupervised networks has also been extensively explored in recent years [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "The typical training criterion for autoencoders is minimizing the reconstruction error, \u2211 x\u2208X L(x, r(x)) with respect to some loss L, typically either squared error or the binary cross-entropy [2].", "startOffset": 193, "endOffset": 196}, {"referenceID": 17, "context": "Denoising autoencoders (DAEs) are an extension of autoencoders trained to reconstruct a clean version of an input from its corrupted version [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "Prior work has shown that DAEs can be stacked to learn more complex representations that are useful for a variety of tasks [5, 21, 4].", "startOffset": 123, "endOffset": 133}, {"referenceID": 17, "context": "Prior work has shown that DAEs can be stacked to learn more complex representations that are useful for a variety of tasks [5, 21, 4].", "startOffset": 123, "endOffset": 133}, {"referenceID": 3, "context": "Prior work has shown that DAEs can be stacked to learn more complex representations that are useful for a variety of tasks [5, 21, 4].", "startOffset": 123, "endOffset": 133}, {"referenceID": 9, "context": "Injecting noise into both the inputs and hidden representations of autoencoders has been proposed for linear networks in prior work by [11], but has not been analyzed in detail for nonlinear representations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "This allows us to exactly marginalize out the noise on the hidden unit activations using a result from [3]:", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "A similar type of penalty is found when learning overcomplete representations with reconstruction ICA, where they use \u2016WWT \u2212 I\u2016F to encourage a diverse set of filters [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "Contractive autoencoders aim to learn a representation that is insensitive to small changes in the input space [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "This result has been previously reported in [17], and motivated the contractive penalty.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "Sparse autoencoders force all hidden units to have similar mean activations[8] .", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "Dropout is a simple regularizer used in training neural networks that applies multiplicative bernoulli noise to all units in a neural network [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "Recent work in neuroscience has shown that single-layer models trained with input and output noise and optimized to maximize mutual information yield receptive fields resembling those found in biological systems [7, 12].", "startOffset": 212, "endOffset": 219}, {"referenceID": 16, "context": "We trained two NAEs on 12x12 patches drawn from the van Hateren natural image dataset [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "As we increase the input noise, we learn features that capture increasingly larger local structure in the digits, as found in DAEs [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "Using these features, we extract a high level representation of full size images using the pipeline from [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 15, "context": "This accuracy is slightly lower than that reported for the higher-order CAE [18], but better than all other regularized autoencoder representations.", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Here we experiment with a deep MNIST model from [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "We optimized these networks using SGD with momentum using the same parameters as in [10].", "startOffset": 84, "endOffset": 88}], "year": 2014, "abstractText": "Autoencoders have emerged as a useful framework for unsupervised learning of internal representations, and a wide variety of apparently conceptually disparate regularization techniques have been proposed to generate useful features. Here we extend existing denoising autoencoders to additionally inject noise before the nonlinearity, and at the hidden unit activations. We show that a wide variety of previous methods, including denoising, contractive, and sparse autoencoders, as well as dropout can be interpreted using this framework. This noise injection framework reaps practical benefits by providing a unified strategy to develop new internal representations by designing the nature of the injected noise. We show that noisy autoencoders outperform denoising autoencoders at the very task of denoising, and are competitive with other single-layer techniques on MNIST, and CIFAR10. We also show that types of noise other than dropout improve performance in a deep network through sparsifying, decorrelating, and spreading information across representations.", "creator": "LaTeX with hyperref package"}}}