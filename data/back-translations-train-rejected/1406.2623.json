{"id": "1406.2623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES", "abstract": "The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widely accepted as a robust derivative-free continuous optimization algorithm for non-linear and non-convex optimization problems. CMA-ES is well known to be almost parameterless, meaning that only one hyper-parameter, the population size, is proposed to be tuned by the user. In this paper, we propose a principled approach called self-CMA-ES to achieve the online adaptation of CMA-ES hyper-parameters in order to improve its overall performance. Experimental results show that for larger-than-default population size, the default settings of hyper-parameters of CMA-ES are far from being optimal, and that self-CMA-ES allows for dynamically approaching optimal settings.", "histories": [["v1", "Tue, 10 Jun 2014 16:44:32 GMT  (578kb)", "https://arxiv.org/abs/1406.2623v1", "13th International Conference on Parallel Problem Solving from Nature (PPSN 2014) (2014)"], ["v2", "Wed, 11 Jun 2014 09:54:27 GMT  (1304kb)", "http://arxiv.org/abs/1406.2623v2", "13th International Conference on Parallel Problem Solving from Nature (PPSN 2014) (2014)"]], "COMMENTS": "13th International Conference on Parallel Problem Solving from Nature (PPSN 2014) (2014)", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["ilya loshchilov", "marc schoenauer", "mich\\`ele sebag", "nikolaus hansen"], "accepted": false, "id": "1406.2623"}, "pdf": {"name": "1406.2623.pdf", "metadata": {"source": "CRF", "title": "Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES", "authors": ["Ilya Loshchilov", "Marc Schoenauer", "Mich\u00e8le Sebag", "Nikolaus Hansen"], "emails": ["Ilya.Loshchilov@epfl.ch", "FirstName.LastName@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.26 23v2 [cs.NE] 1 1"}, {"heading": "1 Introduction", "text": "The Covariance Matrix Adaptation Evolution Strategy (CMA-ES [5]) is a continuous optimizer that uses only the ranking of the estimated candidate solutions to achieve the optimum of an objective function f: Rn \u2192 R. CMA-ES is also invariant w.r.t. affine transformations of the decision space, which explains the well-known robustness of the algorithm. An important practical advantage of CMAES is that all hyperparameters are defined by default in relation to the problem dimension n. Practically, only the population size \u03bb is suggested to be adjusted by the user, e.g. when considering parallelization of the algorithm or describing the problem at hand as multimodal and / or noisy [1,8]. Other hyperparameters have been provided robust default settings (depending on n and \u043c), in the sense that their offline coordination supposedly hardly improves the CMA-ES performance."}, {"heading": "2 Covariance Matrix Adaptation Evolution Strategy", "text": "The covariance matrix adaptation evolution strategy [6,7,5] is acknowledged to be the most popular and efficient evolutionary strategy algorithm (1).The original (\u00b5 / \u00b5w, \u03bb) -CMA-ES (algorithm 1) runs as follows: At the next iteration, a Gaussian distribution N (mt, \u03c3t 2 C t), (1) where the mean mt-Rn of the distribution can be interpreted as the current estimate of function f. (line 5): xtk = N (mt, 2 C t) = mt + t tN (0, C t), (1) where the mean mt-Rn of the distribution can be interpreted as the current estimate of function f."}, {"heading": "3 The self-CMA-ES", "text": "The proposed self-CMA-ES approach is based on the intuition that the optimal hyperparameters of CMA-ES at the time t should favor the generation of the best individuals at the time t, under the (strong) assumption that optimal parameterization and performance of CMA-ES at any time t will lead to the overall optimal performance. Formally, this intuition leads to the following procedure. Let the hyperparameter vector that is used for optimizing the lens f at the time t (CMAES stores its state variables and internal parameters of iteration t in the. \"notation is used to access them."}, {"heading": "4 Experimental Validation", "text": "The experimental validation of self-CMA-ES deals with the performance of the algorithm compared to CMA-ES in terms of the noiseless BBOB problems [4]. Both algorithms are restarted in the IPOP scenario of reboots when the CMA-ES with double population size is restarted as soon as the stop criteria [3] met5.5 For the sake of reproducibility, the source code is https: / / sites.google.com / site / selfcmappsn / The population size is chosen to be 100 for both CMA-ES and self-CMAES. We choose this value (about 10 times greater than the default value, see the default parameters of CMA-ES in Algorithm 1) to investigate how suboptimal the other CMA-ES hyper parameters that can be derived from the default are, and whether the self-CMA-ES parameters can recover from this suboptimal."}, {"heading": "4.1 Results", "text": "Figures 1 and 2 show the comparative performance of CMA-ES (left) and self-CMA-ES (right) on the 10 and 20-dimensional sphere, Rosenbrock, Rotated Ellipsoid and Sharp ridge functions from the silent BBOB testbed [4] (medians of 15 runs).Each diagram shows the value of hyperparameters (left y-axis) along with the objective function (in the logarithmic scale, right y-axis).Hyperparameters c1, c\u00b5 and cc are constant and set to their default values for CMA-ES as they evolve for self-CMA-ES.In self-CMA-ES, the hyperparameters are uniformly initialized into [0, 0.9] (hence the medians are close to 0.45) and they gradually converge into values that are estimated to achieve the best update of the covariance matrix.r.The ability to generate the best individuals cnamics is always greater than those observed by the standard values, which are almost constant."}, {"heading": "4.2 Discussion", "text": "Self-CMA-ES provides conceptual evidence for the online adjustment of three CMA-ES hyperparameters in terms of feasibility and usefulness. Previous studies on parameter settings for CMA-ES mostly looked at offline tuning (see e.g. [16,11]) and theoretical analyses come from the first papers on evolutionary strategies. The main limitation of these studies is that the recommended hyperparameter values are usually also specific to the (class of) problems analyzed. Furthermore, the proposed values are fixed, provided that the optimal parameter values remain constant throughout evolution. However, when optimizing a function whose landscape gradually changes as it approaches the optimum, one can expect optimal hyperparameter values to reflect this change as well. Studies on online adjustment of hyperparameters (apart from the size of the center, m and C) consider the population size to be typically noisy modal [2, multiparameter] [1,12] [or expensive]."}, {"heading": "5 Conclusion and Perspectives", "text": "This paper proposes a principle-driven approach to the self-adjustment of CMA-ES hyperparameters, which is addressed as an auxiliary optimization problem: maximizing the likelihood of generating the most proven solutions. Experimental validation of self-CMA-ES shows that the learning rates involved in adjusting the covariance matrix can be efficiently adjusted online, with results comparable or better than CMA-ES. It should be emphasized that adjusting to the performance of CMA-ES, whose default setting represents a historical consensus between theoretical analysis and offline tuning, is not easy. The main novelty of the work is to offer an intrinsic evaluation of the internal state of the algorithm, based on retrospective considerations (given the best current solutions, how could the generation of these solutions be facilitated) and on an assumption (the optimal hyperparameter values at a given time are \"insufficient\"), based on retrospective considerations (given the best current solutions, how could the generation of these solutions be facilitated) and on an assumption (the optimal hyperparameter values at a certain point in time are \"insufficient\")."}], "references": [{"title": "A Restart CMA Evolution Strategy With Increasing Population Size", "author": ["A. Auger", "N. Hansen"], "venue": "In IEEE Congress on Evolutionary Computation, pages 1769\u2013 1776. IEEE Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Controlling population size and mutation strength by meta-es under fitness noise", "author": ["H.-G. Beyer", "M. Hellwig"], "venue": "In Proceedings of the Twelfth Workshop on Foundations of Genetic Algorithms XII, FOGA XII \u201913, pages 11\u201324. ACM,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Benchmarking a BI-population CMA-ES on the BBOB-2009 function testbed", "author": ["N. Hansen"], "venue": "In GECCO Companion, pages 2389\u20132396,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup", "author": ["N. Hansen", "A. Auger", "S. Finck", "R. Ros"], "venue": "Technical report, INRIA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)", "author": ["N. Hansen", "S. M\u00fcller", "P. Koumoutsakos"], "venue": "Evolutionary Computation, 11(1):1\u201318,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Adapting Arbitrary Normal Mutation Distributions in Evolution Strategies: The Covariance Matrix Adaptation", "author": ["N. Hansen", "A. Ostermeier"], "venue": "In International Conference on Evolutionary Computation, pages 312\u2013317,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Completely Derandomized Self-Adaptation in Evolution Strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation, 9(2):159\u2013195, June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Benchmarking a weighted negative covariance matrix update on the BBOB-2010 noisy testbed", "author": ["N. Hansen", "R. Ros"], "venue": "In GECCO \u201910: Proceedings of the 12th annual conference comp on Genetic and evolutionary computation, pages 1681\u2013 1688, New York, NY, USA,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Controlled Model Assisted Evolution Strategy with Adaptive Preselection", "author": ["F. Hoffmann", "S. Holemann"], "venue": "In International Symposium on Evolving Fuzzy Systems, pages 182\u2013187. IEEE,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Empirical evaluation of the improved rprop learning algorithms", "author": ["C. Igel", "M. H\u00fcsken"], "venue": "Neurocomputing, 50:105\u2013123,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Benchmark results for a simple hybrid algorithm on the CEC 2013 benchmark set for real-parameter optimization", "author": ["T. Liao", "T. St\u00fctzle"], "venue": "In IEEE Congress on Evolutionary Computation (CEC), pages 1938\u20131944. IEEE press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Alternative Restart Strategies for CMA-ES", "author": ["I. Loshchilov", "M. Schoenauer", "M. Sebag"], "venue": "In V. C. et al., editor, Parallel Problem Solving from Nature (PPSN XII), LNCS, pages 296\u2013305. Springer, September", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-Adaptive Surrogate-Assisted Covariance Matrix Adaptation Evolution Strategy", "author": ["I. Loshchilov", "M. Schoenauer", "M. Sebag"], "venue": "In Genetic and Evolutionary Computation Conference (GECCO), pages 321\u2013328. ACM Press, July", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Intensive Surrogate Model Exploitation in Self-adaptive Surrogate-assisted CMA-ES (saACM-ES)", "author": ["I. Loshchilov", "M. Schoenauer", "M. Sebag"], "venue": "In Genetic and evolutionary computation conference, pages 439\u2013446. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing natural evolution strategies to bipop-cma-es on noiseless and noisy black-box optimization testbeds", "author": ["T. Schaul"], "venue": "In Genetic and evolutionary computation conference companion, pages 237\u2013244. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Beating the \u2018world champion\u2019 Evolutionary Algorithm via REVAC Tuning", "author": ["S. Smit", "A. Eiben"], "venue": "In IEEE Congress on Evolutionary Computation, pages 1\u20138,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "1 Introduction The Covariance Matrix Adaptation Evolution Strategy (CMA-ES [5]) is a continuous optimizer which only exploits the ranking of estimated candidate solutions to approach the optimum of an objective function f : R \u2192 R.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "when a parallelization of the algorithm is considered or the problem at hand is known to be multi-modal and/or noisy [1,8].", "startOffset": 117, "endOffset": 122}, {"referenceID": 7, "context": "when a parallelization of the algorithm is considered or the problem at hand is known to be multi-modal and/or noisy [1,8].", "startOffset": 117, "endOffset": 122}, {"referenceID": 15, "context": "In the meanwhile, for multi-modal functions it is suggested that the overall performance can be significantly improved by offline tuning of \u03bb and multiple stopping criteria [16,11].", "startOffset": 173, "endOffset": 180}, {"referenceID": 10, "context": "In the meanwhile, for multi-modal functions it is suggested that the overall performance can be significantly improved by offline tuning of \u03bb and multiple stopping criteria [16,11].", "startOffset": 173, "endOffset": 180}, {"referenceID": 13, "context": "Additionally, it is shown that CMA-ES can be improved by a factor up to 5-10 by the use of surrogate models on unimodal ill-conditioned functions [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "The Covariance Matrix Adaptation Evolution Strategy [6,7,5] is acknowledgedly the most popular and the most efficient Evolution Strategy algorithm.", "startOffset": 52, "endOffset": 59}, {"referenceID": 6, "context": "The Covariance Matrix Adaptation Evolution Strategy [6,7,5] is acknowledgedly the most popular and the most efficient Evolution Strategy algorithm.", "startOffset": 52, "endOffset": 59}, {"referenceID": 4, "context": "The Covariance Matrix Adaptation Evolution Strategy [6,7,5] is acknowledgedly the most popular and the most efficient Evolution Strategy algorithm.", "startOffset": 52, "endOffset": 59}, {"referenceID": 5, "context": "The adaptation of the step-size \u03c3, inherited from the Cumulative Step-Size Adaptation Evolution Strategy (CSA-ES [6]), is controlled by the evolution path p \u03c3 .", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "The covariance matrix update consists of two parts (line 12): a rank-one update [7] and a rank-\u03bc update [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "The covariance matrix update consists of two parts (line 12): a rank-one update [7] and a rank-\u03bc update [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "matrix C itself is replaced by a weighted sum of the rank-one (weight c1 [7]) and rank-\u03bc (weight c\u03bc [5]) updates, with c1 and c\u03bc positive and c1 + c\u03bc \u2264 1.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "matrix C itself is replaced by a weighted sum of the rank-one (weight c1 [7]) and rank-\u03bc (weight c\u03bc [5]) updates, with c1 and c\u03bc positive and c1 + c\u03bc \u2264 1.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "While the optimal parameterization of CMA-ES remains an open problem, the default parameterization is found quite robust on noiseless unimodal functions [5], which explains the popularity of CMA-ES.", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "4 This scheme is actually inspired from the one proposed for surrogate-assisted optimization [13], where the auxiliary CMA-ES was in charge of optimizing the surrogate learning hyper-parameters.", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "4 Experimental Validation The experimental validation of self-CMA-ES investigates the performance of the algorithm comparatively to CMA-ES on the BBOB noiseless problems [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Both algorithms are launched in IPOP scenario of restarts when the CMA-ES is restarted with doubled population size once stopping criteria [3] are met.", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "Evolution of learning rates c1, c\u03bc, cc (lines with markers, left y-axis) and log10(objective function) (plain line, right y-axis) of CMA-ES (left column) and selfCMA-ES (right column) on 10- and 20-dimensional Sphere and Rosenbrock functions from [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 3, "context": "Evolution of learning rates c1, c\u03bc, cc (lines with markers, left y-axis) and log10(objective function) (plain line, right y-axis) of CMA-ES (left column) and selfCMA-ES (right column) on 10- and 20-dimensional Rotated Ellipsoid and Sharp Ridge functions from [4].", "startOffset": 259, "endOffset": 262}, {"referenceID": 0, "context": "9; the constraint is meant to enforce a feasible C update for the primary CMA-ES (the decay factor of C should be in [0, 1]).", "startOffset": 117, "endOffset": 123}, {"referenceID": 3, "context": "1 Results Figures 1 and 2 display the comparative performances of CMA-ES (left) and self-CMA-ES (right) on 10 and 20-dimensional Sphere, Rosenbrock, Rotated Ellipsoid and Sharp ridge functions from the noiseless BBOB testbed [4] (medians out of 15 runs).", "startOffset": 225, "endOffset": 228}, {"referenceID": 15, "context": ", [16,11]) and theoretical analysis dated back to the first papers on Evolution Strategies.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [16,11]) and theoretical analysis dated back to the first papers on Evolution Strategies.", "startOffset": 2, "endOffset": 9}, {"referenceID": 1, "context": "Studies on the online adaptation of hyper-parameters (apart from \u03c3, m and C) usually consider population size in noisy [2], multi-modal [1,12] or expensive [9] optimization.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "Studies on the online adaptation of hyper-parameters (apart from \u03c3, m and C) usually consider population size in noisy [2], multi-modal [1,12] or expensive [9] optimization.", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "Studies on the online adaptation of hyper-parameters (apart from \u03c3, m and C) usually consider population size in noisy [2], multi-modal [1,12] or expensive [9] optimization.", "startOffset": 136, "endOffset": 142}, {"referenceID": 8, "context": "Studies on the online adaptation of hyper-parameters (apart from \u03c3, m and C) usually consider population size in noisy [2], multi-modal [1,12] or expensive [9] optimization.", "startOffset": 156, "endOffset": 159}, {"referenceID": 14, "context": "A more closely related approach was proposed in [15] where the learning rate for step-size adaptation is adapted in a stochastic way similarly to Rprop-updates [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "A more closely related approach was proposed in [15] where the learning rate for step-size adaptation is adapted in a stochastic way similarly to Rprop-updates [10].", "startOffset": 160, "endOffset": 164}], "year": 2014, "abstractText": "The Covariance Matrix Adaptation Evolution Strategy (CMAES) is widely accepted as a robust derivative-free continuous optimization algorithm for non-linear and non-convex optimization problems. CMA-ES is well known to be almost parameterless, meaning that only one hyper-parameter, the population size, is proposed to be tuned by the user. In this paper, we propose a principled approach called selfCMA-ES to achieve the online adaptation of CMA-ES hyper-parameters in order to improve its overall performance. Experimental results show that for larger-than-default population size, the default settings of hyperparameters of CMA-ES are far from being optimal, and that self-CMAES allows for dynamically approaching optimal settings.", "creator": "LaTeX with hyperref package"}}}