{"id": "1406.2732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Deep Epitomic Convolutional Neural Networks", "abstract": "Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the small-image MNIST and CIFAR-10 datasets.", "histories": [["v1", "Tue, 10 Jun 2014 22:07:01 GMT  (3127kb)", "http://arxiv.org/abs/1406.2732v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["george papandreou"], "accepted": false, "id": "1406.2732"}, "pdf": {"name": "1406.2732.pdf", "metadata": {"source": "CRF", "title": "Deep Epitomic Convolutional Neural Networks", "authors": ["George Papandreou"], "emails": ["gpapan@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.27 32v1 [cs.CV] 1 0Ju n20 14"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Deep Epitomic Convolutional Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Mini-Epitomic deep networks", "text": "It is indeed the case that most of us are able to put ourselves at the top, \"he told the German Press Agency.\" But it is not the case that we are able to change the world, \"he said.\" But it is not the case that we have to put them in their place. \"He added:\" It is not the first time that we are able to change the world. \"He added:\" It is not the first time that we have been able to change the world. \"\" It is not the first time that we have been able to change the world. \""}, {"heading": "2.2 Topographic deep networks", "text": "We also experimented with a topographical variant of the proposed deep epitomical network, using a dictionary with only a few large epitomes of spatial size V x V pixels, with V \u2265 W + D \u2212 1. We maintain the local maximum responses via D x D neighborhoods at intervals of D pixels in each of the epitomes, resulting in two output values for each of the K epitomes in the dictionary. The mini-epitomical variant can be considered a special case of the topographic variant, if V = W + D \u2212 1."}, {"heading": "2.3 Optional mean and contrast normalization", "text": "Motivated by [28], we also studied the effect of filter mean and contrast normalization on deep epitomic network training. More specifically, we looked at a variant of the model in which the epitomic folding reactions are calculated as follows: (yi, k, pi, k) \u00b7 max p, p, Nepitomex T i w, p, w, k, p, p, p, p (3), where w, k, p is an average normalized version of the filters and vice versa. (w, Tk, pw, p + \u03bb) 1 / 2 is their contrast, where \u03bb = 0.01 is a small positive constant. This normalization requires only a minor modification of the stochastic downward formula and causes negligible computational effort. Note that the contrast normalization studied here is slightly different from that in [28], which reduces the filters only when their contrast exceeds a predefined limit."}, {"heading": "3 Image Classification Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Image classification tasks", "text": "We have done most of our experimental research on the Imagenet ILSVRC 2012 dataset [5], focusing on the task of image classification, which contains more than 1.2 million training images, 50,000 validation images and 100,000 test images. Each image is assigned to one of 1,000 possible object categories, and performance is evaluated on the basis of the Top 5 classification error. Large-format image datasets such as these have so far proved indispensable to successfully train large deep neural networks using monitored criteria. Similar to other recent work [4, 24, 28] we also evaluate deep epitomic networks trained on Imagenet as a black box visual feature frontend on the Caltech 101 benchmark [6], classifying images into one of 102 possible image classes. We also consider two standard classification standards for thumbnail-sized images, the MNIST number [18] and the possible CAR14 [10] classes."}, {"heading": "3.2 Network architecture and training methodology", "text": "In fact, most of them are able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "3.3 Weight visualization", "text": "In Figure 2, we visualize the layer weights on the first layer of the aforementioned networks. The networks get to know susceptible fields that are sensitive to edges, blotches, textures and color patterns."}, {"heading": "3.4 Classification results", "text": "In Table 4, we report our results on the Imagenet ILSVRC-2012 benchmark, including results previously published in the literature [15,26,28], all of which relate to the top 5 errors on the validation list and are achieved with a single network. Our best score of 13.6% with the proposed Epitomic standard network is 0.6% better than the basic score of 14.2%. Our topographic standard network scores less well, yielding 15.4% error rate, but is still better than [15,28]."}, {"heading": "3.5 Mean-contrast normalization and convergence speed", "text": "We comment on the learning speed and convergence characteristics of the various models we have experimented with on Imagenet. In Figure 3, we show how the top 5 validation error improves as learning progresses for the various models we tested with or without averages + contrast normalization. As a reference, we also include a corresponding chart that we reproduced for the original model by Krizhevsky et al. [15]. We observe that averages + contrast normalization significantly accelerate the convergence of both epitomical and max-pooled models, but without significantly affecting the final model quality."}, {"heading": "4 Conclusions", "text": "In this paper, we have explored the potential of epitomical representation as a building block for deep neural networks. We have shown that an epitomical layer can successfully replace a pair of successive folding and max pooling layers. We have proposed two deep epitomical variants, one with mini-epitomes that perform best empirically in image classification, and one with large epitomes and topographically organized feature maps. We have shown that the proposed epitomical model performs about 0.5% better than the maximum pooled baseline on the demanding Imagenet benchmark and other image classification tasks. In future work, we are very interested in developing methods for unattended or semi-supervised training of deep epitomical models, taking advantage of the fact that epitomical representation is more accessible than maximum merging to include image reconstruction objectives. Reproducibility We have applied the proposed methods by implementing the excellent framework of affaffixed software."}], "references": [{"title": "Sparse and redundant modeling of image content using an imagesignature-dictionary", "author": ["M. Aharon", "M. Elad"], "venue": "SIAM J. Imaging Sci., 1(3):228\u2013247", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse image representation with epitomes", "author": ["L. Beno\u0131\u0302t", "J. Mairal", "F. Bach", "J. Ponce"], "venue": "In Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. on Intel. Systems and Tech., 2(3)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li-Jia", "K. Li", "L. Fei-Fei"], "venue": "Proc. CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Proc. CVPR Workshop", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proc. CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Proc. ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P. Hoyer", "M. Inki"], "venue": "Neur. Comp., 13(7):1527\u20131558", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "ICCV, pages 2146\u20132153", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe.berkeleyvision.org/", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Epitomic analysis of appearance and shape", "author": ["N. Jojic", "B. Frey", "A. Kannan"], "venue": "Proc. ICCV, pages 34\u201341", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "Proc. CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Proc. NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "G. Corrado", "K. Chen", "J. Dean", "A. Ng"], "venue": "Proc. ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proc. ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV, 60(2):91\u2013110", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Topographic product models applied to natural scene statistics", "author": ["S. Osindero", "M. Welling", "G. Hinton"], "venue": "Neur. Comp., 18:381\u2013414", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint deep learning for pedestrian detection", "author": ["W. Ouyang", "X. Wang"], "venue": "Proc. ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling image patches with a generic dictionary of mini-epitomes", "author": ["G. Papandreou", "L.-C. Chen", "A. Yuille"], "venue": "Proc. CVPR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proc. CVPR Workshop", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature neuroscience, 2(11):1019\u20131025", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "arXiv", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Deconvolutional networks", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "Proc. CVPR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "[15] convincingly demonstrated that deep neural networks can be very effective in classifying images in the challenging Imagenet benchmark [5], significantly outperforming computer vision systems built on top of engineered features like SIFT [20].", "startOffset": 242, "endOffset": 246}, {"referenceID": 26, "context": "Subsequent work has improved our understanding and has refined certain aspects of this class of models [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 6, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 20, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 22, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 24, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 26, "context": "A number of different studies have further shown that the features learned by deep neural networks are generic and can be successfully employed in a black-box fashion in other datasets or tasks such as image detection [4, 7, 22, 24, 26, 28].", "startOffset": 218, "endOffset": 240}, {"referenceID": 14, "context": "Their recent success is partly due to the availability of large annotated datasets and fast GPU computing, and partly due to some important methodological developments such as dropout regularization and rectifier linear activations [15].", "startOffset": 232, "endOffset": 236}, {"referenceID": 16, "context": "However, the key building blocks of deep neural networks for images have been around for many years [17]: (1) convolutional multi-layer neural networks with small receptive fields that spatially share parameters within each layer.", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "(2) Gradual abstraction and spatial resolution reduction after each convolutional layer as we ascend the network hierarchy, most effectively via maxpooling [10, 25].", "startOffset": 156, "endOffset": 164}, {"referenceID": 23, "context": "(2) Gradual abstraction and spatial resolution reduction after each convolutional layer as we ascend the network hierarchy, most effectively via maxpooling [10, 25].", "startOffset": 156, "endOffset": 164}, {"referenceID": 11, "context": "In this work we build a deep neural network around the epitomic representation [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "We train the model by error backpropagation to minimize the classification log-loss, similarly to [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Note that the error rate of the original model in [15] is 18.", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "We also find that the proposed epitomic model converges faster, especially when the filters in the dictionary are mean- and contrast-normalized, which is related to [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Related work Our model builds on the epitomic image representation [12], which was initially geared towards image and video modeling tasks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Single-level dictionaries of image epitomes learned in an unsupervised fashion for image denoising have been explored in [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Single-level dictionaries of image epitomes learned in an unsupervised fashion for image denoising have been explored in [1, 2].", "startOffset": 121, "endOffset": 127}, {"referenceID": 21, "context": "Recently, single-level mini-epitomes learned by a variant of K-means have been proposed as an alternative to SIFT for image classification [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": "The proposed epitomic model is closely related to maxout networks [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Maxout is typically used in conjunction with max-pooling [8], while epitomes fully substitute for it.", "startOffset": 57, "endOffset": 60}, {"referenceID": 14, "context": "On the contrary, we have found that learning deep epitomic networks does not require dropout in the convolutional layers \u2013 similarly to [15], we only use dropout regularization in the fully connected layers of our network.", "startOffset": 136, "endOffset": 140}, {"referenceID": 25, "context": "Stochastic pooling [27] has been proposed in conjunction with supervised learning.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Probabilistic pooling [19] and deconvolutional networks [29] have been proposed before in conjunction with unsupervised learning, avoiding the theoretical and practical difficulties associated with building probabilistic models on top of max-pooling.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Probabilistic pooling [19] and deconvolutional networks [29] have been proposed before in conjunction with unsupervised learning, avoiding the theoretical and practical difficulties associated with building probabilistic models on top of max-pooling.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.", "startOffset": 74, "endOffset": 86}, {"referenceID": 15, "context": "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.", "startOffset": 74, "endOffset": 86}, {"referenceID": 19, "context": "This relates the proposed model to topographic ICA [9] and related models [13, 16, 21], which are typically trained to optimize unsupervised objectives.", "startOffset": 74, "endOffset": 86}, {"referenceID": 7, "context": "Computing the maximum response over filters rather than image positions resembles the maxout scheme of [8], yet in the proposed model the filters within the epitome are constrained to share values in their area of overlap.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "Similarly to [15], the final two layers of our network for Imagenet image classification are fully connected and are regularized by dropout.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Motivated by [28], we have also explored the effect of filter mean and contrast normalization on deep epitomic network training.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Note that the contrast normalization explored here is slightly different than the one in [28], who only scale down the filters whenever their contrast exceeds a pre-defined threshold.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "We have performed most of our experimental investigation on the Imagenet ILSVRC-2012 dataset [5], focusing on the task of image classification.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].", "startOffset": 32, "endOffset": 43}, {"referenceID": 22, "context": "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].", "startOffset": 32, "endOffset": 43}, {"referenceID": 26, "context": "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].", "startOffset": 32, "endOffset": 43}, {"referenceID": 5, "context": "Similarly to other recent works [4, 24, 28], we also evaluate deep epitomic networks trained on Imagenet as a black-box visual feature front-end on the Caltech-101 benchmark [6].", "startOffset": 174, "endOffset": 177}, {"referenceID": 13, "context": "We further consider two standard classification benchmarks involving thumbnail-sized images, the MNIST digit [18] and the CIFAR10 [14], both involving classification into 10 possible classes.", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "Similarly to [15], we apply local response normalization (LRN) to the output of the first two convolutional layers and dropout to the output of the two fully-connected layers.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "It has a similar structure with the Overfeat model [26], yet significantly fewer neurons in the convolutional layers 2 to 6.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "Another difference with [26] is the use of LRN, which to our experience facilitates training.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "We followed the methodology of [15] in training our models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Similarly to [4], we resized the training images to have small dimension equal to 256 pixels while preserving their aspect ratio and not cropping their large dimension.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "5, also injecting global color noise exactly as in [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "Model Krizhevsky Zeiler-Fergus Overfeat Max-Pool Max-Pool Epitomic Epitomic Topographic [15] [28] [26] + norm + norm + norm Top-5 Error 18.", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": "Model Zeiler-Fergus Max-Pool Max-Pool Epitomic Epitomic Topographic [28] + norm + norm + norm Mean Accuracy 86.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].", "startOffset": 133, "endOffset": 143}, {"referenceID": 24, "context": "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].", "startOffset": 133, "endOffset": 143}, {"referenceID": 26, "context": "We report at Table 4 our results on the Imagenet ILSVRC-2012 benchmark, also including results previously reported in the literature [15,26,28].", "startOffset": 133, "endOffset": 143}, {"referenceID": 14, "context": "4% error rate, which however is still better than [15,28].", "startOffset": 50, "endOffset": 57}, {"referenceID": 26, "context": "4% error rate, which however is still better than [15,28].", "startOffset": 50, "endOffset": 57}, {"referenceID": 24, "context": "The improved performance that we got with the Max-Pool baseline network compared to Overfeat [26] is most likely due to our use of LRN and aspect ratio preserving image resizing.", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "When preparing this manuscript, we became aware of the work of [4] that reports an even lower 13.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "We trained a 102-way SVM classifier using libsvm [3] and the default regularization parameter.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "Our results are comparable to maxout [8], which achieves state-of-art results on these tasks.", "startOffset": 37, "endOffset": 40}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Table 6: Classification error rates on small image datasets for maxout [8] and the proposed miniepitomic deep network: (a) MNIST.", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "Reproducibility We implemented the proposed methods by extending the excellent Caffe software framework [11].", "startOffset": 104, "endOffset": 108}], "year": 2014, "abstractText": "Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error backpropagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the smallimage MNIST and CIFAR-10 datasets.", "creator": "gnuplot 4.2 patchlevel 6 "}}}