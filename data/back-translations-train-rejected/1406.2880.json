{"id": "1406.2880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "POS Tagging and its Applications for Mathematics", "abstract": "Content analysis of scientific publications is a nontrivial task, but a useful and important one for scientific information services. In the Gutenberg era it was a domain of human experts; in the digital age many machine-based methods, e.g., graph analysis tools and machine-learning techniques, have been developed for it. Natural Language Processing (NLP) is a powerful machine-learning approach to semiautomatic speech and language processing, which is also applicable to mathematics. The well established methods of NLP have to be adjusted for the special needs of mathematics, in particular for handling mathematical formulae. We demonstrate a mathematics-aware part of speech tagger and give a short overview about our adaptation of NLP methods for mathematical publications. We show the use of the tools developed for key phrase extraction and classification in the database zbMATH.", "histories": [["v1", "Wed, 11 Jun 2014 12:25:26 GMT  (423kb,D)", "http://arxiv.org/abs/1406.2880v1", null]], "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.IR", "authors": ["ulf sch\\\"oneberg", "wolfram sperber"], "accepted": false, "id": "1406.2880"}, "pdf": {"name": "1406.2880.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wolfram Sperber"], "emails": [], "sections": [{"heading": "1 Methods and Tools", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Part of Speech Tagging and Noun Phrases", "text": "We describe our approach to the world in which we move, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which, in which, in which, in which, in which, in which, in which, in which, in which, we live, in which, in which, in which, in which, in which, we, in which, in which, in which, we, in which, in which, we, in which, in which, in which, in which, in which, we live, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, we live, in which, in which, in which, in which, in which, in which, in which, we, in which, in which, in which, in which, in which, in which, in which, in which, in which, we, in which, in which, in which, we, in which, in which, in which, we, in which, in which, in which, we, in which, in which, in which, we, in which, in which, in which, in which, we, in which, in which, in which, in which, we, in which, we, in which, we, in which, in which, in which, we, in which, in which, in which, we, in which, in which, we, in which, in which, we, in which, in which, in which, we, in which, in which, in which, in which,"}, {"heading": "1.2 Noun Phrase and Key Phrase Extraction", "text": "A key phrase in our context, information retrieval in mathematical literature, is a phrase that captures the essence of the subject of a document [8]. Mathematical publications, especially journal articles, have a more or less standardized metadata structure that covers important bibliographic data: authors, titles, abstracts, keywords (key sentences), and sometimes a classification that corresponds to Mathematical Subject Classification (MSC). Keyphrases are typically short phrases that characterize a publication in its general mathematical context as diophantine equations or optimal control - embedding special objects, methods, and results of a publication as bipartite complex networks, k-centric clusteringKeyphrases are often derived from the title, summary, or review, or full text, but this is not mandatory. Keyphrases of a document must not be part of the document."}, {"heading": "1.3 Classification with NPs", "text": "Classification is also an important task within NLP. The normal approach, which uses the full text of a document and prefers stem building and Term Frequency / Inverse Document Frequency (TF / IDF) to get rid of redundant words, took a different approach. We used the extracted noun phrases from the texts and then applied text classification methods. We tested several machine learning techniques. The best results came from a Support Vector Machine (SVM). The SVM we used was John Platt's sequential minimum optimization algorithm for support vector classifiers, the kernel is a polynomial kernel, the training data came from the database zbMATH from the beginning to the end of 2013. Specifically, we used WEKA's sequential minimum optimization (SMO) technique."}, {"heading": "1.4 The Big Picture", "text": "A few words about the block diagram Fig. 1: We start with an article, which is then converted into sentences. For each sentence, the extracted formulas have to be pre-processed: If an acronym is in the sentence, it has to be expanded, then the POS tagger runs and the noun phrases are extracted, the extracted NPs are sent to the classifier, the candidates for key phrases and classification codes are submitted to human experts, and their evaluations are used to improve our machine learning techniques. The POS block in the middle of the diagram is really large in terms of complexity. As I said, when a new release comes out of Stanford, the new block can be easily integrated into our system.General Note: There are some emerging machine learning techniques that have also been used for the semantic analysis of documents. They work with Deep Belief Networks (deep neural networks) and the results of these experiments are more than promising."}, {"heading": "2 Reviewing services in mathematics", "text": "The review of journals has a long tradition in mathematics and today takes the form of electronic databases. zbMATH and MathSciNet [2] are today the most important bibliographic databases in mathematics and are important tools of the mathematical community in the search for relevant publications. zbMATH and MathSciNet [2] offer the most comprehensive bibliographic information on mathematics, supplemented by in-depth content analysis of publications. In the Gutenberg era, all this information was created manually. The digital age has dramatically changed the situation. Digitisation of information allows automation such as the one we are developing to make the production of databases more efficient and uniform, in order to improve the quality of the database zbMATH.The mathematical databases have three different levels, which are directly oriented towards the content analysis of a publication: - lower level: reviews or abstracts - second level: key sentences - top level: classification Each level has its own characteristics, but these levels interact with each other."}, {"heading": "3 Key phrase extraction in zbMATH", "text": "The relevance of NPs for the identification of key terms also applies to our data. Central journal Math, today the database zbMATH, has responded to the increasing number of key terms in mathematical literature and collected them since the 1970s. The field UT Uncontrolled Terms was introduced to highlight individual terms or phrases of a publication, e.g. marginal function, quasi-differentiable function, direction-bound differentiability, distance function. This field lists key terms created by authors and / or reviewers and / or editors of zbMATH. Typically, key terms of authors are extended by reviewers and editors within the workflow of zbMATH. Key terms presented in zbMATH are searchable (by the specification ut: in the search field) and clickable."}, {"heading": "3.1 Problems", "text": "Relevance: The extracted NPs have different values for content analysis. Phrases such as those in the following work (chapters, etc.) or an important theorem are of marginal value for content analysis. Therefore, we assign weight to each extracted noun sentence. A noun phrase receives a very high score if it is a designated mathematical unit defined in a mathematical vocabulary such as Wikipedia, PlanetMath, Encyclopedia of Mathematics, etc. The number of designated mathematical units in these vocabularies is limited, and no more than 50,000 units. Typically, such phrases are important when the publication is assigned to its mathematical context - identical to a proposed key phrase in mathematics, etc. Most mathematical publications have a (limited) number of key phrases created by the author (s)."}, {"heading": "3.2 Processing of NPs:", "text": "The following are the methods used: - Weighting: The weighting of key phrases is done as described above. - Redundancy: Very often, some of the extracted NPs are similar. - A simple measure of similarity is the number of different tokens between two phrases. The selection of a representative is based on the basic vocabulary. Existing key phrases and other resources (e.g. the names of the MSC classes) are used to select the most appropriate phrase. - Expert evaluation: The resulting list of possible key phrases is shown to human experts, e.g. editors or reviewers who can remove, change or add phrases."}, {"heading": "3.3 Results", "text": "The average number of extracted NPs is significantly higher: 10-20 NPs. Through the methods described above, the number of candidates is reduced to 7-10 phrases for a publication. So far, the evaluation of key phrase extraction by human experts has only been started for certain classes, because this entails additional costs for human involvement. In the first phase, less than 40% of the phrases were accepted by the experts. Feedback led to a redesign and significant improvements of the methods. The acceptable proportion of automatically generated key phrases increased to more than 60% by removing irrelevant phrases. It is planned to convert machine-assisted key phrase extraction into a semi-automatic workflow for zbMATH.Of course, the quality of the proposed key phrases depends on the title and the review (abstract).Controlled vocabulary: We applied our tools to the complete zbMATH database. All resulting keys and all changes are classified and controlled based on the first phrases."}, {"heading": "3.4 Further remarks", "text": "Key Phrases and Classification: Automatically generated key phrases have also been used for classification, as described in detail below. Classification based on extracted key phrases rather than reviews has significantly improved the quality of automatic classification. Structuring Keyphrases: With our method, we only obtain key phrases located within a text. To further improve key phrases and controlled vocabulary, we need to know additional relationships between phrases, such as synonyms, hypernyms, hyponyms, meronyms. Such ontological relationships could be used to structure and improve the extracted key phrases.Deeper analysis of mathematical formulas: Mathematical symbols and formulas are an important part of mathematical publications, but they are more important in the full texts of publications than in reviews. An analysis of symbols and formulas found in zbMATH has shown that reviews, or summaries, are, for example, a combination of formulas well as forms."}, {"heading": "4 Classification in zbMATH", "text": "Although this is a well-known method, it is not a trivial task. There are many reasons for the difficulties, two main reasons being the classification scheme and the classification process; classes are defined by one or more common characteristics of the members; the abstraction of individual objects, classification systems that assign the objects to the classes; classification systems are not a priori given; they are intellectually designed and depend on the subject, objectives, interests and views of the developers of the classification; and the MSC was designed by the American Mathematical Society in the 1970s to support subjectively oriented access to the increasing number of mathematically relevant publications, e.g. MATH lists 35,958 journal articles, books and other publications in mathematics and fields of application in the 1970s."}, {"heading": "4.1 Quality", "text": "Precision is the proportion of all publications of a class that are assigned to a particular class by the automatic classifier. As a reminder, we look at all publications that do not belong to a class (in addition to the class: i.e. the publication is in a different class) but are assigned to that class by the automatic classifier."}, {"heading": "4.2 Results", "text": "The results of the classification give a differentiated picture. Roughly speaking, the precision is sufficient (for 26 of the 63 top-level classes, the precision is higher than 0.75 and only for 4 classes it is less than 0.5), the recall is not. In other words, publications classified as elements of a certain MSC class i are for the most part correctly classified. The classifier is precision-weighted: for all MSC classes, the precision is higher than remembered. In the following, we discuss the results in more detail. A central idea in the discussion about quality is the overlap of the classes. Therefore, we have built a matrix indexed by the top-level classes of the MSC, which lists the number of publications according to the following definition: Let aii the number of publications classified exclusively with the MSC classes i and aij, the number of publications is overlaid with the primary and secondary classes."}, {"heading": "4.3 Remarks", "text": "Use of Classification: High precision means high reliability that releases of class i are automatically assigned to this class as well. This is important for pre-classification, where precision is more important than memory. So far, we have used the classification tool for pre-classification of publications. We propose to improve retrieval by a second step of classification analysis. The key sentences of each publication assigned to class i by the automatic classifier are analysed in more detail.Controlled vocabulary and classification: Classification is - in addition to key phrases - an important piece of metadata in the content analysis of a publication. Each e.g. MATH element can carry more than one classification code. The database e.g. MATH does not contain any relationship between key phrases and classification codes. It is an n: m relationship. The hierarchical structure of the MSC is also a problem. Initially, we have assigned an MSC class to a key phrase when the MSC classification is different to a MSC classification (for a higher level)."}, {"heading": "5 Conclusion and next steps", "text": "The machine-aided methods we have developed for the extraction and classification of keywords already seem to be useful to improve the content analysis of mathematical publications and to make the workflow at zbMATH more efficient. We note some positive effects: - The quantity and quality of keywords is increased by automatic keyword extraction. - The integration of formulas into keywords creates the basis for the inclusion of formulas in content analysis. This could significantly improve the content analysis of mathematical documents. - Results of the classification can be used to redesign and improve the MSC. - The use of standardized methods guarantees a balanced and standardized quality of content analysis in zbMATH."}], "references": [{"title": "Part-of-Speech-Tagging guidelines for the Penn Treebank Project (3rd Revi4ghulsion, 2nd printing)", "author": ["B. Santorini"], "venue": "ftp://ftp.cis.upenn.edu/pub/ treebank/doc/tagguide.ps.gz", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Keyphrase extraction in scientific publications. Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers", "author": ["Nguyen", "Thuy Dung", "Min-Yen Kan"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The WEKA Data Mining Software: An Update; SIGKDD Explorations", "author": ["Mark Hall", "Frank Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Ian H. Witten"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Fast Training of Support Vector Machines", "author": ["Platt", "John C"], "venue": "Using Sequential Minimal Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Comparing a linguistic and a stochastic tagger", "author": ["C. Samuelsson", "A. Voutilainen"], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}], "referenceMentions": [{"referenceID": 4, "context": "In our project, the Stanford POS tagger [10] is used.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "The Stanford tagger uses the Penn Treebank POS scheme [4], a classification scheme of linguistic types with 45 tags for tokens and punctuation symbols.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "[6]", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "Content analysis of scientific publications is a nontrivial task, but a useful and important one for scientific information services. In the Gutenberg era it was a domain of human experts; in the digital age many machine-based methods, e.g., graph analysis tools and machine-learning techniques, have been developed for it. Natural Language Processing (NLP) is a powerful machinelearning approach to semiautomatic speech and language processing, which is also applicable to mathematics. The well established methods of NLP have to be adjusted for the special needs of mathematics, in particular for handling mathematical formulae. We demonstrate a mathematics-aware part of speech tagger and give a short overview about our adaptation of NLP methods for mathematical publications. We show the use of the tools developed for key phrase extraction and classification in the database zbMATH.", "creator": "LaTeX with hyperref package"}}}