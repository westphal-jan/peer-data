{"id": "1406.3010", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "\"Mental Rotation\" by Optimizing Transforming Distance", "abstract": "The human visual system is able to recognize objects despite transformations that can drastically alter their appearance. To this end, much effort has been devoted to the invariance properties of recognition systems. Invariance can be engineered (e.g. convolutional nets), or learned from data explicitly (e.g. temporal coherence) or implicitly (e.g. by data augmentation). One idea that has not, to date, been explored is the integration of latent variables which permit a search over a learned space of transformations. Motivated by evidence that people mentally simulate transformations in space while comparing examples, so-called \"mental rotation\", we propose a transforming distance. Here, a trained relational model actively transforms pairs of examples so that they are maximally similar in some feature space yet respect the learned transformational constraints. We apply our method to nearest-neighbour problems on the Toronto Face Database and NORB.", "histories": [["v1", "Wed, 11 Jun 2014 19:38:05 GMT  (217kb,D)", "https://arxiv.org/abs/1406.3010v1", null], ["v2", "Fri, 5 Dec 2014 17:55:09 GMT  (210kb,D)", "http://arxiv.org/abs/1406.3010v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["weiguang ding", "graham w taylor"], "accepted": false, "id": "1406.3010"}, "pdf": {"name": "1406.3010.pdf", "metadata": {"source": "CRF", "title": "\u201cMental Rotation\u201d by Optimizing Transforming Distance", "authors": ["Weiguang Ding", "Graham W. Taylor"], "emails": ["wding@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 Introduction", "text": "It has long been assumed that when people recognize the identity of an object or assess the similarity between the objects, they simulate transformations of these objects, showing that the response time for the participants is the same (except for spatial rotation) or that they refer to the angle between the two objects."}, {"heading": "2 Background", "text": "Our work attempts to link two areas of the study that have remained disparate to date: the unattended learning of image transformations and the learning of similarity measurement, so that objects that are perceptibly similar exhibit a high measurable similarity. The latter field has focused on methods that are computationally comprehensible and often learn a Mahalanobis distance [5] or other functional mappings in which distances can be calculated for tasks such as the closest neighborhood classification or retrieval of documents. Such methods usually use distance information that is intrinsically available: binary or real similarities or dissimilarities, user input or class-based information. Rarely has the amount of known transformations of an object been included in learning, with the exception of Hadsell et al. [11], although they do not attempt to model transformations."}, {"heading": "3 Methods", "text": "A relational model captures the transformation between significant pairs of images (x, y), such as photos taken from different views of the same object, and images with different expressions of the same face. It encodes the transformation information from x to y as a hidden representation h, which can also be used to transform the \"source\" x in the direction of the \"target\" y."}, {"heading": "3.1 Factored gated restricted Boltzmann machine", "text": "The factored gated RBM (fgRBM) [25] sets x, y, and h in relation to the following energy function: \u2212 E (y, h; x) = N \u2211 n = 1 (i, j = 1 vinxi) (J, j = 1 wjnyj) (M, m = 1 umnhm) + 1 2 J, j = 1 (yj \u2212 bj) 2 + M, l = m cmhm, (1) where I, J, and M are the dimensionality of x, y, and h; vin, wjn, and uln are a series of filters that project x, y, and h onto N factors. bj and cm are the bias coefficients for y and h. Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM that is able to model real evaluated data."}, {"heading": "3.1.1 Factored gated RBM training", "text": "To train the fgRBM, we would like to maximize the average log probability of y for a number of training pairs {(x\u03b1, y\u03b1): L (\u03b8) = \u2211 \u03b1 logp (y\u03b1; x\u03b1) (5), where \u03b8 represents all the parameters involved. The partial derivation of the probability L (\u03b8) with respect to any parameter is \u2212 \u2202 L \u2202 \u03b8 = \u2211 \u03b1 < \u2202 E (y \u03b1, h; x\u03b1) \u2202 \u03b8 > h \u2212 < \u2202 E (y, h; x\u03b1) \u2202 \u03b8 > y, h = \u0445 \u03b1 \u2202 F (y\u03b1; x\u03b1) \u0445 \u0432 - < \u2202 F (y; x \u03b1) 20s \u03b8 > y (6), where < > z denotes the expectation with respect to z. The first term is the derivation of free energy, which is comprehensible."}, {"heading": "3.2 Transforming distance", "text": "We can use a trained fgRBM to transform an image x with a specific h that represents a transformation (\u03b2 \u03b2). The transformation function can be derived from the conditional p (y | h; x) by taking the conditional expectation of y | x. We use t (x, h) to transform x in relation to h. t (x, h), we can define the transforming distance D between a pair of images (xx\u03b2) and we express the jth element of t astj (x, h), which is represented by h\u03b2 and h\u03b2. (7) With \u03b1 (x, h) we can define the transforming distance D between a pair of images (x\u03b2, x\u03b2), as their transformations are represented by h\u03b1 and h\u03b2."}, {"heading": "4 Experiments", "text": "We looked at two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19]. A qualitative visual evaluation was performed to demonstrate the validity of optimizing the transforming distance. For quantitative evaluation, we replaced the L2 distance in the K nearest neighbor algorithm (KNN) with the proposed transformation distance. We refer to this variant of KNN as transforming KNN. Specifically, in the training phase, an fgRBM is trained; during the test period, this fgRBM defines a transforming distance that is used to compare test images with examples in the KNN database."}, {"heading": "4.1 Data", "text": "The people mentioned are able to hide without being able to play by the rules."}, {"heading": "4.2 Model training", "text": "Before training the fgRBM and performing transformation distances, the TFD images were pre-processed by Local Contrast Normalization (LCN) [28] with core size 9. For the NORB dataset, we used only the first image from each stereo pair. First, the images were sampled to 32 \u00d7 32 and then pre-processed by LCN with core size 3. Examples are in Fig. 1 (b) and (c).For TFD, the fgRBM is trained on pairs of images with the same identity. fgRBM encodes valid expression transformation information. for NORB, 2 images share the same class, instance, illumination state, and altitude within each image pair. The fgRBM encodes the valid azimuth transformation information. based on the allowed azimuth difference between the images, we traced 2 types of difference: 1) AnyTrans, which show the difference between a shorter pair and an azimuth within the two images."}, {"heading": "4.2.1 Hyperparameter selection", "text": "For the display of the features, we cross-validated the choice of PCA and CAE with 64, 128 and 256 hidden units on regular KNN. The best of these 6 combinations was chosen. For fgRBM, we cross-validated the hidden dimensionality of 64, 128 and 256 by performing a one-sided transforming KNN with pixel pitch. For NORB, the K value for KNN was cross-validated in the range 1 to 30. Whereas for TFD K, due to the nature of the task, the number of factors in fgRBM is set to double the dimensionality of the hidden units. For space reasons, details of the remaining settings are provided in supplementary material."}, {"heading": "4.3 Qualitative evaluation", "text": "In order to be able to assess the transforming distance optimization process, we observed the transformed image t (x\u03b1, h \u043a) with each optimization step. Figure 3 shows the optimization process of image pairs with the same identity and different identity. We see that the transformed images are all valid transformations of the original images during the optimization process: The Facebook images retain their identities even when an image is transformed to match a face image with different identity; the car and the human figure are only rotated, even if the human figure image is transformed to match the autoimage. This is an interesting generalization result, since the fgRBM was trained on identity pairs only."}, {"heading": "4.4 Quantitative evaluation", "text": "For quantitative evaluation, the transforming distance was combined with KNN as transforming KNN and compared with two basic methods: the regular KNN and a proposed extended KNN."}, {"heading": "4.4.1 Augmented KNN", "text": "It is interesting to see if this \"expansion effect\" can be achieved by extending the KNN database with the learned transformation and then performing regular KNN. This comparison is particularly useful if the data space is not well covered by the KNN database (e.g. in the TFD dataset) so we suggested the \"extended KNN\" and tested it for TFD. Additional training images were scanned by a trained fgRBM. We used each image in the KNN database as a \"source image\" and took turns scanning Gibbs between hidden and target units. We kept a sample every 100 iterations. By repeating these 9 times for each image in the KNN database, we acquired a database ten times larger than the original database. We are not aware of the work this model used for common purposes such as relation 1."}, {"heading": "4.4.2 KNN performance", "text": "This year, the number of job-related redundancies has fallen by 20 per cent compared to the previous year, while the number of job-related redundancies has increased by 20 per cent."}, {"heading": "4.4.3 KNN with a reduced database", "text": "As already mentioned, under Transformation KNN, each example in the database can include a variety and cover a \"volume.\" We examine this assumption by performing KNN on reduced databases and show the relationship between accuracy and missing data rate (randomly discarded) in Fig. 5 For both TFD and NORB, the transformation of KNN works relatively well with missing data. Its performance is still comparable to that of regular KNN with full database, when 70% (TFDs1), 80% (TFDs2) and even 99% (NORB, AnyTrans) data fail. For TFD, missing data leads to missing identities, no matter how good the relativity model is, a performance drop is inevitable. In comparison, NORB contains only 5 classes that have a very high error rate."}, {"heading": "5 Conclusion", "text": "The most important innovation in our work is the idea of augmenting learned similarity functions with latent variables that capture variation factors. Although we were inspired by the evidence of a mental ability to simulate spatial transformations, we believe this is just one example of a richer class of dynamic similarity models that can be achieved within this framework. That is, the proposed framework is generic because it consists of interchangeable components. Nevertheless, performing inferences or optimizations using latent variables when comparing examples of learned similarity models is much more compositional than the typical \"test-time\" application of learned similarity models, which can be mitigated in part by a reduced need for large databases. Unfortunately, our method excludes the use of harmless neighboring techniques that are typically used for large-scale problems of the type we are looking at. However, we have made modest gains from parallelizing GPUs."}], "references": [{"title": "Deep learners benefit more from out-of-distribution", "author": ["Y. Bengio", "F. Bastien", "A. Bergeron", "N. Boulanger-Lewandowski", "T. Breuel", "Y. Chherawala", "M. Cisse", "M. C\u00f4t\u00e9", "D. Erhan", "J. Eustache"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci", "L. Gambardella", "J. Schmidhuber"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["G. Dahl", "M. Ranzato", "A. Mohamed", "G. E Hinton"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Disentangling factors of variation via generative entangling", "author": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1210.5474,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Object detection with discriminatively trained part-based models", "author": ["Pedro F Felzenszwalb", "Ross B Girshick", "David McAllester", "Deva Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning invariance from transformation sequences", "author": ["P. F\u00f6ldi\u00e1k"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images", "author": ["Michael U Gutmann", "Valero Laparra", "Aapo Hyv\u00e4rinen", "Jes\u00fas Malo"], "venue": "PloS one,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1936}, {"title": "On autoencoder scoring", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "In ICML, pages 720\u2013728,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Emergence of complex cell properties by learning to generalize in natural scenes", "author": ["Y. Karklin", "M. Lewicki"], "venue": "Nature, 457(7225):83\u201386,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The role of spatio-temporal synchrony in the encoding of motion", "author": ["K. Konda", "R. Memisevic", "V. Michalski"], "venue": "In Proc. of International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "Fu J. Huang", "L. Bottou"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Learning invariant representations and applications to face verification", "author": ["Q. Liao", "J. Leibo", "T. Poggio"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Appearance models based on kernel canonical correlation analysis", "author": ["Thomas Melzer", "Michael Reiter", "Horst Bischof"], "venue": "Pattern recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1961}, {"title": "Gradient-based learning of higher-order image features", "author": ["R. Memisevic"], "venue": "In ICCV. IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Learning to relate images", "author": ["R. Memisevic"], "venue": "IEEE TPAMI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["R. Memisevic", "G. Hinton"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Transformation Pursuit for Image Classification", "author": ["M. Paulin", "J. Revaud", "Z. Harchaoui", "F. Perronnin", "C. Schmid"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D. Cox", "J. DiCarlo"], "venue": "PLoS compbiol,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Modeling pixel means and covariances using factorized third-order boltzmann machines", "author": ["M. Ranzato", "G. Hinton"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Mental rotation of three-dimensional objects", "author": ["R. Shepard", "J. Metzler"], "venue": "Science, 171(3972):701,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1971}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Novel morphometric based classification via diffeomorphic based shape representation using manifold learning", "author": ["Rachel Sparks", "Anant Madabhushi"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "The toronto face database", "author": ["J. Susskind", "A. Anderson", "G. Hinton"], "venue": "U of Toronto, Tech. Rep,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "R. Memisevic", "G. Hinton", "M. Pollefeys"], "venue": "In CVPR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Convolutional learning of spatio-temporal features", "author": ["G. Taylor", "R. Fergus", "Y. LeCun", "C. Bregler"], "venue": "In ECCV,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": "Shepard and Metzler [32] were the first to formalize this phenomenon, and assess it experimentally.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Although much progress on object recognition by machines has been inspired by human biology [28], these models have rarely accounted for the explicit transformation of internal representations analogous to human mental rotation.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "One example where invariance is engineered into the model is convolutional networks [18], which gain invariance to small translations in the input because they pool features over local regions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 2, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 26, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 7, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 25, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 19, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 31, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 249, "endOffset": 253}, {"referenceID": 13, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 9, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "Another related stream includes various deformable part models [7] and diffeomorphism models [34], where knowledge about specific spatial relationships or classes of transformations are encoded.", "startOffset": 63, "endOffset": 66}, {"referenceID": 32, "context": "Another related stream includes various deformable part models [7] and diffeomorphism models [34], where knowledge about specific spatial relationships or classes of transformations are encoded.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "domains with large intra-class variance, one either needs to maintain a database of essentially all different appearances of objects, or learn an invariant similarity measure [9, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 10, "context": "domains with large intra-class variance, one either needs to maintain a database of essentially all different appearances of objects, or learn an invariant similarity measure [9, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 22, "context": "A class of relational unsupervised learning techniques [23] use multiplicative interactions between inputs to represent correlation patterns across multiple images.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "One application of these methods has been to learn to represent transformations between image pairs [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "Such a model can be used to assess the similarity between images and used for nearest-neighbor classification [36].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "Another related line of research focuses on disentangling different attributes [6, 30], for example face identity and expression.", "startOffset": 79, "endOffset": 86}, {"referenceID": 22, "context": "In contrast, relational models [23, 24] learn to encode \u2018relative\u2019 transformations between multiple groups of visible units, where each group represents one transformed instance of an image.", "startOffset": 31, "endOffset": 39}, {"referenceID": 23, "context": "In contrast, relational models [23, 24] learn to encode \u2018relative\u2019 transformations between multiple groups of visible units, where each group represents one transformed instance of an image.", "startOffset": 31, "endOffset": 39}, {"referenceID": 4, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 147, "endOffset": 154}, {"referenceID": 10, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 147, "endOffset": 154}, {"referenceID": 10, "context": "[11], though they do not attempt to model transformations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The field of representation learning [2] is concerned with learning features which untangle unknown underlying factors of variation in data.", "startOffset": 37, "endOffset": 40}, {"referenceID": 23, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 24, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 15, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 28, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 82, "endOffset": 89}, {"referenceID": 3, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 82, "endOffset": 89}, {"referenceID": 23, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 34, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 21, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 35, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 16, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 23, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 19, "endOffset": 27}, {"referenceID": 24, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 19, "endOffset": 27}, {"referenceID": 21, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Recent work also suggests a means of scoring inputs under an autoencoder [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "The factored gated RBM (fgRBM) [25] relates x, y and h by the following energy function:", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM capable of modeling real-valued data, which has a slightly different energy function from that of the original binary fgRBM in [25].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM capable of modeling real-valued data, which has a slightly different energy function from that of the original binary fgRBM in [25].", "startOffset": 189, "endOffset": 193}, {"referenceID": 11, "context": "In this paper, we used the contrastive divergence learning algorithm [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "In this paper, for distance metrics, we simply consider the Euclidean distance; for feature representations, we consider raw pixels, principal component analysis (PCA) and the contractive autoencoder (CAE) [31].", "startOffset": 206, "endOffset": 210}, {"referenceID": 33, "context": "We considered two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "We considered two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "Images were taken under different lighting conditions, elevations and azimuths [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "Before training the fgRBM and performing transforming distance, TFD images were preprocessed by local contrast normalization (LCN) [28] with kernel size 9.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Provided the data to train the relational model is available, this seems like an attractive option to the usual approach of hand-coding perturbations [1, 3].", "startOffset": 150, "endOffset": 156}, {"referenceID": 2, "context": "Provided the data to train the relational model is available, this seems like an attractive option to the usual approach of hand-coding perturbations [1, 3].", "startOffset": 150, "endOffset": 156}], "year": 2014, "abstractText": "The human visual system is able to recognize objects despite transformations that can drastically alter their appearance. To this end, much effort has been devoted to the invariance properties of recognition systems. Invariance can be engineered (e.g. convolutional nets), or learned from data explicitly (e.g. temporal coherence) or implicitly (e.g. by data augmentation). One idea that has not, to date, been explored is the integration of latent variables which permit a search over a learned space of transformations. Motivated by evidence that people mentally simulate transformations in space while comparing examples, so-called \u201cmental rotation\u201d, we propose a transforming distance. Here, a trained relational model actively transforms pairs of examples so that they are maximally similar in some feature space yet respect the learned transformational constraints. We apply our method to nearest-neighbour problems on the Toronto Face Database and NORB.", "creator": "LaTeX with hyperref package"}}}