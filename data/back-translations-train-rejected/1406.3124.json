{"id": "1406.3124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2014", "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and Reasoning", "abstract": "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning under structural restrictions. All these problems involve two tasks: (i) identifying the structure in the input as required by the restriction, and (ii) using the identified structure to solve the reasoning task efficiently. We show that for most of the considered problems, task (i) admits a polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, in contrast to task (ii) which does not admit such a reduction to a problem kernel of polynomial size, subject to a complexity theoretic assumption. As a notable exception we show that the consistency problem for the AtMost-NValue constraint admits a polynomial kernel consisting of a quadratic number of variables and domain values. Our results provide a firm worst-case guarantees and theoretical boundaries for the performance of polynomial-time preprocessing algorithms for the considered problems.", "histories": [["v1", "Thu, 12 Jun 2014 05:44:06 GMT  (41kb)", "http://arxiv.org/abs/1406.3124v1", "arXiv admin note: substantial text overlap witharXiv:1104.2541,arXiv:1104.5566"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1104.2541,arXiv:1104.5566", "reviews": [], "SUBJECTS": "cs.AI cs.CC cs.DS", "authors": ["serge gaspers", "stefan szeider"], "accepted": false, "id": "1406.3124"}, "pdf": {"name": "1406.3124.pdf", "metadata": {"source": "CRF", "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and Reasoning", "authors": ["Serge Gaspers", "Stefan Szeider"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 6.31 24v1 [cs.AI] 12 Jun 2014Keywords: traceability of fixed parameters; kernelization; satisfaction with limitations; reason; computational complexity"}, {"heading": "1 Introduction", "text": "The question that arises in this context is how to measure the quality of the proposed processing rules. \"What is it actually?,\" he asked, \"what is it actually?,\" \"What is it actually?,\" \"What is it actually?,\" \"What is it actually?,\" \"What is it actually?,\" \"What is it actually?,\" \"What is it?,\" \"What is it?,\" \"\" What is it?, \"\" \"What is it?,\" \"\" \"What is it actually?,\" \"\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\", \"\", \"\", \",\", \",\" \",\", \",\", \"\", \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \"\" \",\", \"\" \"\", \"\", \"\" \"\" \",\" \",\" \"\" \"\" \",\" \"\" \",\" \"\" \"\", \",\" \"\" \"\" \"\" \"\", \",\" \"\", \"\", \"\" \"\", \"\" \"\" \"\" \"\", \"\" \",\" \"\" \"\" \",\" \"\" \"\", \",\" \"\" \"\", \"\" \"\" \"\" \"\", \",\" \",\", \"\" \"\" \",\" \"\" \",\", \"\" \"\", \"\" \"\", \",\" \"\" \"\", \"\" \"\", \",\" \"\" \",\", \"\" \"\", \"\" \"\" \",\", \"\" \"\" \",\" \"\" \",\", \"\", \"\" \",\" \"\" \"\", \"\", \"\" \"\", \"\", \"\" \",\" \",\" \",\" \"\" \"\" \",\", \"\", \",\" \"\" \",\" \"\" \",\" \",\" \"\" \"\" \",\" \"\" \"\" \",\", \"\" \"\" \",\", \"\" \"\" \",\", \"\" \"\" \"\", \"\" \"\" \""}, {"heading": "2 Formal Background", "text": "A parameterized problem P is a subset of the problem. We assume that the parameter NP is represented in singular. However, for the parameterized problems considered in this paper, the parameter is a function of the main part, i.e., k = \u03c0 (x) for a function. We then refer to the problem as P (\u03c0), i.e., U-CSP (width) denotes the problem U-CSP, which is parameterized by the width of the given tree. A parameterized problem P is fixable if there is an algorithm that solves each problem (x, k)."}, {"heading": "3 Tools for Kernel Lower Bounds", "text": "Subsequently, we will use recently developed tools to obtain lower kernel boundaries. Our lower kernel boundaries are subject to the widely accepted complex theoretical assumption NP 6 coNP / poly. In other words, the tools allow us to show that a parameterized NP problem does not allow a polynomized nucleus unless NP coNP / poly. NP coNP / poly in particular would imply the collapse of the polynomial hierarchy to the third level: NP = 3p [51]. A composition algorithm for a parameterized problem P p is an algorithm that receives a sequence as input (x1, k),. (xt, k)."}, {"heading": "4 Constraint Networks", "text": "We assume that this is a solution that requires an assignment of the individual sizes to the individual sizes. We assume that we have an assignment of the individual sizes to the individual sizes. We assume that we have an assignment of the different sizes to the different sizes. We assume that we have an assignment of the sizes to the different sizes. We assume that it is an assignment of the sizes. We assume that an assignment of the variables Si. A solution is an assignment of the sizes Si. We assume that it is an assignment of the sizes Si and Si. We assume that it is an assignment of the sizes Si and Si. We assume that it is an assignment of the sizes Si and Si. We assume that it is an assignment of the sizes Si and Si."}, {"heading": "5 Satisfiability", "text": "The probable satisfaction of the problem (SAT) was the first problem that proved to be NP difficult or satisfactory; despite its hardness, SAT solver is increasingly recognized as a general purpose tool in areas such as software and hardware verification. (SAT solvers are capable of exploiting the hidden structure in the real world.) The concept of backdoors, developed by Williams et al. [65], provides a means of establishing the vague notion of a hidden structure. Backdoors are defined as a \"sub-solver,\" which is a polynomial algorithm that correctly determines satisfaction for a Class C of CNF forms. [40] We define a sub-solver to be an algorithm that takes a CNF formula F as input and has the following properties: 1. Totomy: Either rejectives that prove to be a sub-solver or a satisfactory solver [40] to be an algorithm."}, {"heading": "6 Global Constraints", "text": "In their AAI '08 paper, they demonstrated that a full spread of several intolerable constraints can be solved as efficiently as they can. [5] At the heart of efficient CP solutions, certain variables must assume different values (e.g., activities requiring the same resource must be allocated to all different times). [6] Therefore, most constraint solvers offer a globalAllAllAllAllAllAllAllende constraint and algorithms for their propagation. [8] In their AAAAAAAAAI' 08 paper, they demonstrated that a full spread of several intolerable constraints can be efficiently resolved."}, {"heading": "6.1 Kernel Lower Bounds", "text": "We show that it is unlikely that most of the FPT results of Bessi\u00e8re et al. [5] can be improved to polynomial kernel. Theorem 6. The problems NValue-Cons (dx), Disjoint-Cons (dxy), Uses-Cons (dy) do not allow polynomial cores, however, unless there is a polynomial parameter transformation of SAT (vars). We use a construction of Bessi\u00e8re et al. [8]. Let F = {C1,., Cm} be a CNF formula about variables x1,., xn. We consider the clauses and variables of F as the variables of a global constraint with domains (xi) = {\u2212 i}, and dom (Cj) = {i: xi \u00b2 Cj}."}, {"heading": "6.2 A Polynomial Kernel for NValue Constraints", "text": "Most of them are not able to keep to the limits of their possibilities. (Most of them are not able to keep to the limits of their possibilities.) (Most of them are not able to keep to the limits of their possibilities. (Most of them are not able to keep to the limits of their possibilities.) Most of them are not able to keep to the limits of their possibilities. (Most of them are not able to keep to the limits of their possibilities. (Most of them are not able to keep to the limits of their possibilities.) (Most of them are not able to keep to the limits of their possibilities.) (Most of them are not able to keep to the limits of their possibilities.)"}, {"heading": "6.3 Improved FPT Algorithm using the Kernel", "text": "Using the kernel of Theorem 8 and the simple algorithm described at the beginning of this section, we arrive at an O (2kk2 + | I |) time algorithm for verifying the consistency of an AtMost-NValue algorithm. If we take ideas from the kerelization algorithm, we now reduce the exponential dependence on k at runtime. The speed-ups resulting from this branch algorithm and the kernel-NValue algorithm result in a speed-up for enforcing HAC-NValue constraints (by Corollary 2) and enforcement of HAC for NValue constraints (by decomposition of [6]). The consistency problem for AtMost-NValue constraints allows an O to (kk2 + | I) time algorithm where k is the number of holes in the input instance I."}, {"heading": "7 Bayesian Reasoning", "text": "The BN variables and dependencies are characterized in the BN by a direct acyclic diagram G = (V, A), when the common probability distribution is given by a table Tv for each node V, which defines a probability Tv. (U) for each possible instantiation U = (d1,., ds) for the parents v1, vs in G. The probability Pr (U) for a complete instantiation U = (d1,.) of the variables of the Tv variables is given by the product of the Tv. We consider the problematic BN inference, which we consider to be a complete instantiation. (U) of the variables of the Tv variables is given by the product of the Tv | U over all variables v. We consider the problematic BN inference, which we consider to be a problem."}, {"heading": "8 Nonmonotonic Reasoning", "text": "I would like to thank all those who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me through my struggles, who have supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles."}, {"heading": "9 Conclusion", "text": "We have provided the first theoretical evaluation of the guarantees and limits of polynomial time pre-processing for hard AI problems. Specifically, we have set super polynomial kernel lower limits for many problems, which set firm limits on the power of polynomial time pre-processing for these problems. On the positive side, we have developed an efficient linear kernel algorithm for the consistency problem for AtMost-NValue constraints, and demonstrated how it can be used to accelerate the full spread of NValue and related constraints. Following our work, Fellows et al. [29] investigated the parameterized complexity and kerelization for various parameter risks of abductive reasoning. Their kernel licensing results were largely negative, showing that many parameterization risks for the abduction problem do not have polynomial cores unless NP-copoly."}, {"heading": "A Appendix: Proof of Theorem 8", "text": "In this appendix we can assume that S nice.duction le - is because there is a solution for a solution required for the size of the core, and that there is a solution for a solution based on a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution. \"A solution for a solution for a solution for a solution for a solution for a solution for a solution for a solution is nice.\""}, {"heading": "Acknowledgments", "text": "Serge Gaspers is the recipient of the Australian Research Council Discovery Early Career Researchers Award (project number DE120101761). NICTA is funded by the Australian Government, represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council under the ICT Centre of Excellence Programme."}], "references": [{"title": "A kernelization algorithm for d-hitting", "author": ["Faisal N. Abu-Khzam"], "venue": "set. J. of Computer and System Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Parameterized complexity and kernel bounds for hard planning problems", "author": ["Christer B\u00e4ckstr\u00f6m", "Peter Jonsson", "Sebastian Ordyniak", "Stefan Szeider"], "venue": "Algorithms and Complexity, 8th International Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Pruning for the minimum constraint family and for the number of distinct values constraint family", "author": ["Nicolas Beldiceanu"], "venue": "Principles and Practice of Constraint Programming - CP 2001, 7th International Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "The parameterized complexity of global constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Claude-Guy Quimper", "Toby Walsh"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Filtering algorithms for the NValue", "author": ["Christian Bessiere", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Toby Walsh"], "venue": "constraint. Constraints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Range and roots: Two common patterns for specifying and propagating counting and occurrence constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Zeynep Kiziltan", "Toby Walsh"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "The complexity of global constraints", "author": ["Christian Bessi\u00e8re", "Emmanuel Hebrard", "Brahim Hnich", "Toby Walsh"], "venue": "Proceedings of the Nineteenth National Conference on Artificial Intelligence, July 25-29,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Cutset sampling for Bayesian networks", "author": ["Bozhena Bidyuk", "Rina Dechter"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A linear-time algorithm for finding tree-decompositions of small treewidth", "author": ["Hans L. Bodlaender"], "venue": "SIAM J. Comput.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "On problems without polynomial kernels", "author": ["Hans L. Bodlaender", "Rodney G. Downey", "Michael R. Fellows", "Danny Hermelin"], "venue": "J. of Computer and System Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Kernel bounds for disjoint cycles and disjoint paths", "author": ["Hans L. Bodlaender", "St\u00e9phan Thomass\u00e9", "Anders Yeo"], "venue": "ESA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Preprocessing the MAP problem", "author": ["J.H. Bolt", "L.C van der Gaag"], "venue": "Proceedings of the Third European Workshop on Probabilistic Graphical Models,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Nondeterminism within P", "author": ["Jonathan F. Buss", "Judy Goldsmith"], "venue": "SIAM J. Comput.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Preprocessing of intractable problems", "author": ["Marco Cadoli", "Francesco M. Donini", "Paolo Liberatore", "Marco Schaerf"], "venue": "Information and Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "On feedback vertex set new measure and new structures", "author": ["Yixin Cao", "Jianer Chen", "Yang Liu"], "venue": "Algorithm Theory - SWAT 2010, 12th Scandinavian Symposium and Workshops on Algorithm Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Improved upper bounds for vertex cover", "author": ["Jianer Chen", "Iyad A. Kanj", "Ge Xia"], "venue": "Theoretical Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "The complexity of theorem-proving procedures", "author": ["Stephen A. Cook"], "venue": "In Proc. 3rd Annual Symp. on Theory of Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["Gregory F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1990}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Constraint satisfaction. In The CogNet Library: References Collection", "author": ["Rina Dechter"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Tree clustering for constraint networks", "author": ["Rina Dechter", "Judea Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "Linear-time algorithms for testing the satisfiability of propositional Horn formulae", "author": ["William F. Dowling", "Jean H. Gallier"], "venue": "J. Logic Programming,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1984}, {"title": "Parameterized complexity: A framework for systematically confronting computational intractability", "author": ["R. Downey", "M.R. Fellows", "U. Stege"], "venue": "Contemporary Trends in Discrete Mathematics: From DI- MACS and DIMATIA to the Future, volume 49 of AMS-DIMACS, pages 49\u201399. American Mathematical Society", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Parameterized Complexity", "author": ["R.G. Downey", "M.R. Fellows"], "venue": "Monographs in Computer Science. Springer Verlag, New York", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "New limits to classical and quantum instance compression", "author": ["Andrew Drucker"], "venue": "In 53rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Effective preprocessing in SAT through variable and clause elimination", "author": ["Niklas E\u00e9n", "Armin Biere"], "venue": "In Fahiem Bacchus and Toby Walsh, editors, Theory and Applications of Satisfiability Testing, 8th International Conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "The lost continent of polynomial time: Preprocessing and kernelization", "author": ["Michael R. Fellows"], "venue": "IWPEC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "The parameterized complexity of abduction", "author": ["Michael R. Fellows", "Andreas Pfandler", "Frances A. Rosamond", "Stefan R\u00fcmmele"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Kernel(s) for problems with no kernel: On out-trees with many leaves", "author": ["Henning Fernau", "Fedor V. Fomin", "Daniel Lokshtanov", "Daniel Raible", "Saket Saurabh", "Yngve Villanger"], "venue": "26th International Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Backdoors to tractable answer-set programming", "author": ["Johannes Klaus Fichte", "Stefan Szeider"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Exact Exponential Algorithms", "author": ["F.V. Fomin", "D. Kratsch"], "venue": "Springer Verlag", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Infeasibility of instance compression and succinct PCPs for NP", "author": ["Lance Fortnow", "Rahul Santhanam"], "venue": "Proceedings of the 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Exponential Time Algorithms - Structures, Measures, and Bounds", "author": ["Serge Gaspers"], "venue": "VDM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Kernels for global constraints", "author": ["Serge Gaspers", "Stefan Szeider"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Backdoors to satisfaction", "author": ["Serge Gaspers", "Stefan Szeider"], "venue": "Essays Dedicated to Michael R. Fellows on the Occasion of His 60th Birthday,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Advanced preprocessing for answer set solving", "author": ["Martin Gebser", "Benjamin Kaufmann", "Andr\u00e9 Neumann", "Torsten Schaub"], "venue": "ECAI 2008 - 18th European Conference on Artificial Intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "The stable model semantics for logic programming", "author": ["Michael Gelfond", "Vladimir Lifschitz"], "venue": "Logic Programming, Proceedings of the Fifth International Conference and Symposium,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1988}, {"title": "Satisfiability solvers. In Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence, pages 89\u2013134", "author": ["Carla P. Gomes", "Henry Kautz", "Ashish Sabharwal", "Bart Selman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Fixed-parameter complexity in AI and nonmonotonic reasoning", "author": ["Georg Gottlob", "Francesco Scarcello", "Martha Sideri"], "venue": "Artificial Intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Invitation to data reduction and problem kernelization", "author": ["Jiong Guo", "Rolf Niedermeier"], "venue": "ACM SIGACT News,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Treewidth: Computations and Approximations", "author": ["T. Kloks"], "venue": "Springer Verlag, Berlin", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "The decision problem for a class of first-order formulas in which all disjunctions are binary", "author": ["Melven R. Krom"], "venue": "Zeitschrift fu\u0308r Mathematische Logik und Grundlagen der Mathematik,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1967}, {"title": "Kernelization - preprocessing with a guarantee", "author": ["Daniel Lokshtanov", "Neeldhara Misra", "Saket Saurabh"], "venue": "The Multivariate Algorithmic Revolution and Beyond - Essays Dedicated to Michael R. Fellows on the Occasion of His 60th Birthday,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Stable models and an alternative logic programming paradigm. In The Logic Programming Paradigm: a 25-Year Perspective, pages 169\u2013181", "author": ["V. Wiktor Marek", "Miroslaw Truszczynski"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1999}, {"title": "Maximum matchings via Gaussian elimination", "author": ["Marcin Mucha", "Piotr Sankowski"], "venue": "In Proceedings of the 45th Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["Ilkka Niemel\u00e4"], "venue": "Ann. Math. Artif. Intell.,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1999}, {"title": "Detecting backdoor sets with respect to Horn and binary clauses", "author": ["Naomi Nishimura", "Prabhakar Ragde", "Stefan Szeider"], "venue": "In Proceedings of SAT 2004 (Seventh International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference. The Morgan Kaufmann Series in Representation and Reasoning", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1988}, {"title": "Bayesian networks. In The CogNet Library: References Collection", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "P", "author": ["C.-G. Quimper", "A. L\u00f3pez-Ortiz"], "venue": "van Beek, and A. Golynski. Improved algorithms for the global cardinality constraint. In M. Wallace, editor, Principles and Practice of Constraint Programming ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "The problem of simplifying truth functions", "author": ["W.V. Quine"], "venue": "Amer. Math. Monthly, 59:521\u2013531", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1952}, {"title": "Generalized arc consistency for global cardinality constraint", "author": ["J.-C. R\u00e9gin"], "venue": "14th National Conference on Artificial Intelligence ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1996}, {"title": "Table of races", "author": ["Frances Rosamond"], "venue": "http://fpt.wikidot.com/", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "P", "author": ["F. Rossi"], "venue": "van Beek, and T. Walsh, editors. Handbook of Constraint Programming. Elsevier", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "On the hardness of approximate reasoning", "author": ["Dan Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1996}, {"title": "Fixed-parameter tractability", "author": ["Marko Samer", "Stefan Szeider"], "venue": "Handbook of Satisfiability,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "Constraint satisfaction with bounded treewidth revisited", "author": ["Marko Samer", "Stefan Szeider"], "venue": "J. of Computer and System Sciences,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Limits of preprocessing", "author": ["Stefan Szeider"], "venue": "Proceedings of the Twenty-Fifth Conference on Artificial Intelligence,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2011}, {"title": "Multiplying matrices faster than Coppersmith-Winograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Symposium on Theory of Computing Conference (STOC", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2012}, {"title": "On the connections between backdoors, restarts, and heavy-tailedness in combinatorial search", "author": ["Ryan Williams", "Carla Gomes", "Bart Selman"], "venue": "In Informal Proc. of the Sixth International Conference on Theory and Applications of Satisfiability Testing, S. Margherita Ligure - Portofino,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": ", [27]), CSP solvers make use of various local consistency algorithms that filter the domains of variables (see, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 35, "context": ", [38, 13], respectively).", "startOffset": 2, "endOffset": 10}, {"referenceID": 11, "context": ", [38, 13], respectively).", "startOffset": 2, "endOffset": 10}, {"referenceID": 50, "context": "The history of preprocessing, like applying reduction rules to simplify truth functions, can be traced back to the 1950\u2019s [55].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "With the advent of parameterized complexity [25], a new theoretical framework became available that provides suitable tools to analyze the power of preprocessing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 33, "context": "Thus, for FPT problems, the combinatorial explosion can be Preliminary and shorter versions of this paper appeared in the proceedings of IJCAI 2011 [36] and AAAI 2011 [62].", "startOffset": 148, "endOffset": 152}, {"referenceID": 57, "context": "Thus, for FPT problems, the combinatorial explosion can be Preliminary and shorter versions of this paper appeared in the proceedings of IJCAI 2011 [36] and AAAI 2011 [62].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "It is known that a problem is fixed-parameter tractable if and only if every problem input can be reduced by polynomial-time preprocessing to an equivalent input whose size is bounded by a function of the parameter [24].", "startOffset": 215, "endOffset": 219}, {"referenceID": 39, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 39, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 42, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 52, "context": ", the references in [28, 42, 45, 57]).", "startOffset": 20, "endOffset": 36}, {"referenceID": 26, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 39, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 42, "context": "Kernelization can be seen as a preprocessing with performance guarantee that reduces a problem instance in polynomial time to an equivalent instance, the kernel, whose size is a function of the parameter [28, 33, 42, 45].", "startOffset": 204, "endOffset": 220}, {"referenceID": 9, "context": "Some NP-hard combinatorial problems such as k-Vertex Cover admit polynomially sized kernels, for others such as k-Path an exponential kernel is the best one can hope for [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Buss\u2019 kernelization algorithm for k-Vertex Cover (see [14]) computes the set U of vertices with degree at least k+1 in G.", "startOffset": 54, "endOffset": 58}, {"referenceID": 38, "context": "Constraint satisfaction problems (CSP) over a fixed universe of values, parameterized by the induced width [41].", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "Consistency and generalized arc consistency for intractable global constraints, parameterized by the cardinalities of certain sets of values [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 46, "context": "Propositional satisfiability (SAT), parameterized by the size of backdoors [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets [52, 9].", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets [52, 9].", "startOffset": 117, "endOffset": 124}, {"referenceID": 38, "context": "Nonmonotonic reasoning with normal logic programs, parameterized by feedback width [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "As in [5], the parameter is the number of holes in the domains of the variables, measuring how close the domains are to being intervals.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "exponential time) preprocessed (\u201ccompiled\u201d), such that in a second phase various queries can be answered in polynomial time [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "Theorem 1 ([11, 34]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 31, "context": "Theorem 1 ([11, 34]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "Theorem 2 ([12]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "Constraint networks have proven successful in modeling everyday cognitive tasks such as vision, language comprehension, default reasoning, and abduction, as well as in applications such as scheduling, design, diagnosis, and temporal and spatial reasoning [21].", "startOffset": 255, "endOffset": 259}, {"referenceID": 20, "context": "The induced width of a constraint network is the treewidth of its constraint graph [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 90, "endOffset": 98}, {"referenceID": 56, "context": "It is well known that U-CSP(width) is fixed-parameter tractable over any fixed universe U [22, 41] (for generalizations see [61]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 40, "context": ", [43]), we can put the given width w tree decompositions T1, .", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "By Bodlaender\u2019s Theorem [10], the problem is fixed-parameter tractable.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "[11] showed that the related problem of testing whether a graph has treewidth at most w does not have a polynomial kernel (taking w as the parameter), unless a certain \u201cAND-conjecture\u201d fails.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In turn, Drucker [26] showed that a failure of the AND-conjecture implies NP \u2286 coNP/poly.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "The propositional satisfiability problem (SAT) was the first problem shown to be NP-hard [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 37, "context": "Despite its hardness, SAT solvers are increasingly leaving their mark as a general-purpose tool in areas as diverse as software and hardware verification, automatic test pattern generation, planning, scheduling, and even challenging problems from algebra [40].", "startOffset": 255, "endOffset": 259}, {"referenceID": 59, "context": "[65], provides a means for making the vague notion of a hidden structure explicit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] define a sub-solver to be an algorithm A that takes as input a CNF formula F and has the following properties:", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The smaller the backdoor B, the more useful it is for satisfiability solving, which makes the size of the backdoor a natural parameter to consider (see [37] for a survey on the parameterized complexity of backdoor problems).", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "We will devise polynomial parameter transformations from the following parameterized problem which is known to be compositional [34] and therefore unlikely to admit a polynomial kernel.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Sub-solvers for Horn and 2CNF follow from [23] and [44], respectively.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "Sub-solvers for Horn and 2CNF follow from [23] and [44], respectively.", "startOffset": 51, "endOffset": 55}, {"referenceID": 46, "context": "We therefore consider the cases 3SAT(Horn-backdoor) and 3SAT(2CNF-backdoor) separately, these cases are important since the detection of Horn and 2CNF-backdoors is fixed-parameter tractable [50].", "startOffset": 190, "endOffset": 194}, {"referenceID": 46, "context": "\u22a3 We now turn to the recognition problem Rec-SAT(A-backdoor), in particular for A \u2208 {Horn, 2CNF} for which, as mentioned above, the problem is known to be fixed-parameter tractable [50].", "startOffset": 181, "endOffset": 185}, {"referenceID": 55, "context": "It is well-known and easy to see that the vertex covers of G(F ) are exactly the Horn-backdoor sets of F [60].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "Now, we apply the known kernelization algorithm for vertex covers [17] to (G(F ), k) and obtain in polynomial time an equivalent instance (G\u2032, k\u2032) where G\u2032 has at most 2k vertices.", "startOffset": 66, "endOffset": 70}, {"referenceID": 55, "context": "Again, it is well-known and easy to see that the hitting sets of H(F ) are exactly the 2CNF-backdoor sets of F [60].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Now we apply a known kernelization algorithm for the hitting set problem on 3-uniform hypergraphs (3HS) [1] to (H(F ), k) and obtain in polynomial time an equivalent instance (H \u2032, k\u2032) where H \u2032 has at most O(k) vertices.", "startOffset": 104, "endOffset": 107}, {"referenceID": 53, "context": "Constraint programming (CP) offers a powerful framework for efficient modeling and solving of a wide range of hard problems [58].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "Unfortunately, for several important global constraints a complete propagation is NPhard, and one switches therefore to incomplete propagation such as bound consistency [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "[5] showed that a complete propagation of several intractable constraints can efficiently be done as long as certain natural problem parameters are small, i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": ", the propagation is fixed-parameter tractable [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "[5] in their concluding remarks: whether intractable global constraints admit a reduction to a problem kernel or kernelization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This includes the 5 global constraints NValue, AtMost-NValue, Disjoint, Uses, and EGC defined above (see [8]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "[5] considered dx = |dom(X)| as parameter for NValue, dxy = |dom(X) \u2229 dom(Y )| as parameter for Disjoint, and dy = |dom(Y )| as parameter for Uses.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] also showed that polynomial time algorithms for enforcing bounds consistency imply that the corresponding consistency problem is fixed-parameter tractable parameterized by the number of holes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] can be improved to polynomial kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7], F can be encoded as a Disjoint constraint with X = {x1, .", "startOffset": 0, "endOffset": 3}, {"referenceID": 49, "context": "The Consistency problem for EGC constraints is NP-hard [54].", "startOffset": 55, "endOffset": 59}, {"referenceID": 51, "context": "However, if all sets dom(\u00b7) are intervals, then consistency can be checked in polynomial time using network flows [56].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "[5], the Consistency problem for EGC constraints is fixed-parameter tractable, parameterized by the number of holes in the sets dom(\u00b7).", "startOffset": 0, "endOffset": 3}, {"referenceID": 49, "context": "[54]: Given a CNF formula F on k variables, one can construct in polynomial time an EGC constraint CF such that 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Beldiceanu [3] and Bessi\u00e8re et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "[6] decompose NValue constraints into two other global constraints: AtMost-NValue and AtLeast-NValue, which require that at most N or at least N values are used for the variables in X , respectively.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A greedy algorithm by Beldiceanu [3] checks the consistency of an AtMost-NValue constraint in linear time when all domains are intervals (i.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "[5] have shown that Consistency (and Enforcing HAC) is fixed-parameter tractable, parameterized by the number of holes, for all constraints for which bound consistency can be enforced in polynomial time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A simple algorithm for checking the consistency of AtMost-NValue goes over all instances obtained from restricting the domain of each variable to one of its intervals, and executes the algorithm of [3] for each of these 2 instances.", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "As in [6], we determine the largest possible value for N if its domain were the set of all integers.", "startOffset": 6, "endOffset": 9}, {"referenceID": 44, "context": "This can be done in O((|X |+ |D|)) time [48, 64] by computing a maximum matching in the graph whose vertices are X \u222a D with an edge between x \u2208 X and v \u2208 D iff v \u2208 dom(x).", "startOffset": 40, "endOffset": 48}, {"referenceID": 58, "context": "This can be done in O((|X |+ |D|)) time [48, 64] by computing a maximum matching in the graph whose vertices are X \u222a D with an edge between x \u2208 X and v \u2208 D iff v \u2208 dom(x).", "startOffset": 40, "endOffset": 48}, {"referenceID": 4, "context": "The speed-ups due to this branching algorithm and the kernelization algorithm lead to a speed-up for enforcing HAC for AtMost-NValue constraints (by Corollary 2) and for enforcing HAC for NValue constraints (by the decomposition of [6]).", "startOffset": 232, "endOffset": 235}, {"referenceID": 48, "context": "Bayesian networks (BNs) have emerged as a general representation scheme for uncertain knowledge [53].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "The problem is NP-complete [19] and moves from NP to #P if we ask to compute Pr(v = true) [59].", "startOffset": 27, "endOffset": 31}, {"referenceID": 54, "context": "The problem is NP-complete [19] and moves from NP to #P if we ask to compute Pr(v = true) [59].", "startOffset": 90, "endOffset": 94}, {"referenceID": 47, "context": "e, if there is at most one undirected path between any two variables [52].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "The reduction is based on the reduction from 3SAT given by Cooper [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Hence we can apply a known kernelization algorithm for feedback vertex sets [16] to G and obtain a kernel (G\u2032, k) with at most O(k) many vertices.", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning [39, 46].", "startOffset": 114, "endOffset": 122}, {"referenceID": 43, "context": "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning [39, 46].", "startOffset": 114, "endOffset": 122}, {"referenceID": 38, "context": "[41] considered the following parameterization of the problem and showed fixed-parameter tractability (see [31] for generalizations).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[41] considered the following parameterization of the problem and showed fixed-parameter tractability (see [31] for generalizations).", "startOffset": 107, "endOffset": 111}, {"referenceID": 38, "context": "[41] cannot be strengthened towards a polynomial kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "We give a polynomial parameter transformation from SAT(vars) to SME(feedback width) using a construction of Niemel\u00e4 [49].", "startOffset": 116, "endOffset": 120}, {"referenceID": 45, "context": "The formula F is satisfiable if and only if P has a stable model [49].", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "[29] investigated the parameterized complexity and kernelization for various parameterizations of Abductive Reasoning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] for planning problems, parameterized by the length of the plan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "In the area of optimization, parameterized problems are known that do not admit polynomial kernels but admit polynomial Turing kernels [30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": ", [20]), the initial sorting of the n + k intervals can be done in time O(n + |D| + k).", "startOffset": 2, "endOffset": 6}], "year": 2014, "abstractText": "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning under structural restrictions. All these problems involve two tasks: (i) identifying the structure in the input as required by the restriction, and (ii) using the identified structure to solve the reasoning task efficiently. We show that for most of the considered problems, task (i) admits a polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, in contrast to task (ii) which does not admit such a reduction to a problem kernel of polynomial size, subject to a complexity theoretic assumption. As a notable exception we show that the consistency problem for the AtMost-NValue constraint admits a polynomial kernel consisting of a quadratic number of variables and domain values. Our results provide a firm worst-case guarantees and theoretical boundaries for the performance of polynomial-time preprocessing algorithms for the considered problems.", "creator": "LaTeX with hyperref package"}}}