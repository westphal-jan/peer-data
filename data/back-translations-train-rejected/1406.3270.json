{"id": "1406.3270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Kalman Temporal Differences", "abstract": "Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features.", "histories": [["v1", "Thu, 16 Jan 2014 05:02:28 GMT  (812kb)", "http://arxiv.org/abs/1406.3270v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthieu geist", "olivier pietquin"], "accepted": false, "id": "1406.3270"}, "pdf": {"name": "1406.3270.pdf", "metadata": {"source": "CRF", "title": "Kalman Temporal Differences", "authors": ["Matthieu Geist", "Olivier Pietquin"], "emails": ["matthieu.geist@supelec.fr", "olivier.pietquin@supelec.fr"], "sections": [{"heading": "1. Introduction", "text": "The automatic learning response to this recurring problem is the Reinforcement Learning (RL) paradigm (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Sigaud & Buffet, 2010).In this general example, an artificial agent learns optimal control policies through interactions with the dynamic system (which is also considered its environment).After each interaction, the agent receives immediate scalar reward information and the optimal reward he is seeking is the one that maximizes cumulative reward over the long run.Traditionally, the dynamic system to be controlled is modeled as a Markov Decision Process (MDP).An MDP is a tuple {S, A, P, R, \u03b3} where S is the state space, A the action space, P: s, an \"S \u00d7 P.\""}, {"heading": "1.1 Formalism", "text": "The value function V \u03c0 of a given policy associates to each state the expected discounted cumulative reward for the reward starting in that state and then the following function Q (Q function): V \u03c0 (s) = E [\u221e \u2211 i = 0 \u03b3iri | s0 = s, \u03c0] (1), where ri is the reward observed in due course i. The Q function adds a degree of freedom for choosing the first action: Q\u03c0 (s, a) = E [\u221e i = 0 \u03b3iri | s0 = s, a0 = a, \u03c0] (2), which maximizes the policy \u03c0, which maximizes the value function for each state: \u03c0 (s), is the value function for each state: p = argmaximizes. Despite the partial order (value functions are vectors), this maximum exists (Puterman, 1994). Two schemes (among others) can lead to the solution. First, political iteration implies learning the value function of a given policy, then the improvement of the policy, or the learning function is the new nature."}, {"heading": "1.2 State of the Art", "text": "Standard RL algorithms such as TD evaluation, SARSA and Q-Learning (Sutton & Barto, 1998) share the same characteristics and a unified view of Equation (7).In this equation, the term \"Belli\" is the TD error. Suppose that at step i a transition (si, ai, ri, si + 1, ai + 1), the TD-like RL algorithms aimed at evaluating the value function of a given policy, the TD error is: \"i = ri + a transition.\" (si + 1) \u2212 V-like RL algorithms aimed at evaluating the Q function of a given policy, the TD error is: \"i = ri + i + i error.\" (si + 1) \u2212 V-like RL algorithms aimed at evaluating the Q function of a given policy."}, {"heading": "1.3 Paper Outline", "text": "The next section introduces an alternative point of view on value function approximation, informally introducing Kalman filtering and the representation of the noise state on which our contribution is based. MDP determinism is adopted in Section 3 and the general Kalman time differential frame is derived. Deterministic transitions are to be associated with an assumption of white noise state necessary for KTD derivation. Subsequently, we specialize in an approximation scheme, the Unscented Transform (UT) by Julier and Uhlmann (2004), to derive a family of practical algorithms. In Section 4, a colored noise model, originally introduced by Engel, Mannor and Meir (2005), is used to extend the KTD framework to the case of stochastic transitions. An eXtended KTD (XKTD) framework is proposed, and its combination with non-political learning is analyzed as a kind of value."}, {"heading": "2. An Alternative Point of View", "text": "In the previous section, the standard vision of the problem of enhanced learning and its formulation within the MDP framework was presented, offering an alternative viewpoint. 3. LSTD and GPTD could certainly be extended to the non-stationary case, for example by introducing a certain forgetfulness factor. However, they were not originally designed in this way, and the aim of this paper is not to provide LSTD or GPTD variations."}, {"heading": "2.1 Informal Idea", "text": "This paper proposes a novel approach based on an alternative standpoint. A stochastic dynamic system is considered to possess the underlying value functions V-RS and the underlying value functions Q-RS-A. When an agent performs an action, it provokes a change of state and the generation of a reward. Indeed, this reward is a local observation of the underlying value functions that determine the behavior of the system. From a sequence of such observations, the agent can derive information about each of the value functions. A good estimate of the value function V-RS (s) (or state value function Q-RS, a) is derived from the conditional expectation of all possible paths of V (s) (or Q (s, a)) given the sequence of observed reward functions: V-i (s) = E-RS (state value = R1, R1)."}, {"heading": "2.2 Kalman Filtering", "text": "Originally, the Kalman filter paradigma aims at tracking the hidden state X (modeled as a random vector) of a non-stationary dynamic system by indirect observations (Y1,.., Yi) of this state. To do this, the algorithm calculates a prediction of the state (X, i, i, i, 1) and the observation (Y, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "2.3 State-space Formulation for the Value Function Evaluation Problem", "text": "Before providing the general framework, underlying ideas are introduced by the value function V \u03c0 (s) evaluation problem. As the provision of some uncertainty information about estimates is considered a desirable feature, a statistical view is taken and the parameter vector \u03b8 is modeled as a series of random variables. Another desired feature is to pursue the solution rather than approach it. This suggests the assumption of an evolutionary model for the value function (by the parameters). However, the dynamics of the value function is difficult to model since it depends on whether the dynamic system is controlled or evaluated the value function taking place in a generalized political iteration.4 Here, a heuristic evolutionary model is adopted according to the occam shaving principle and the parameters evolution is modeled as a random gait is not stationary or the value evaluation occurs in a generalized political iteration scheme."}, {"heading": "3. KTD: the Deterministic Case", "text": "From now on, and throughout the rest of this section, the focus is on deterministic Markov decision-making processes. Transitions become deterministic and Bellman equations (4-6) simplify as follows: V \u03c0 (s) = R (s, \u03c0 (s), s \") + GP (40) Q\u03c0 (s, a) = R (s, a, s\") + \u03b3Q\u03c0 (s, \"p\"), GP (s \"), a\" s, \"a\" 41 \"Q\" (s, \"a) = R\" (s, \"a\") + \u03b3maxb \"A Q\" (s, \"b), GP\" s, \"a\" (42) This section presents the derivation of the most common KTD algorithm as well as specializations on practical implementations."}, {"heading": "3.1 The General Framework", "text": "In fact, the two are a \"conspiracy theory,\" a \"conspiracy theory,\" a \"conspiracy theory,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"conspiracy,\" a \"a\" conspiracy, \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" a \"conspiracy,\" a \"a\" conspiracy, \"a\" a \"a\" a \"conspiracy, a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"conspiracy, a\" a \"a\" a \"a\" a \"a\" conspiracy, a \"a\" a \"a\" a \"conspiracy, a\" a \"a\" a \""}, {"heading": "3.1.1 Minimized Cost Function", "text": "One goal might be to estimate the total distribution of parameters based on past observed rewards that can be handled by Bayesian filtering. However, it is a difficult problem in the general case. \u2212 \u2212 \u2212 Here, a simpler goal is chosen: the estimation of the (deterministic) parameter vector, which minimizes the expectation of \"true\" parameters of the average square error due to past observed rewards. \u2212 The idea is that information is provided by observed transitions and associated rewards, and that knowledge of the mean of the posterior distribution should suffice. Related costs can be written as follows: Ji (conform) = E [constant error] 2 | r1: i] with r1: i = r1,., ri (47) Note that if behavii is a random vector (the distribution of which is not known), the drop vector is a deterrent vector, with a minimal estimate of the optimum error of the square (MSE)."}, {"heading": "3.1.2 Optimal Gain", "text": "Using classical equations, the cost function can be redefined as a consequence of the matrix variance of parameters. (Using the matrix variance of parameters error: Ji (V) = E (V) = V (V) = V (V) = V (V) = V (V) = V (V) = V (V) = V (V) = V (V) = V (K) = V (K) = V (K) = V (K) = V) = V (K) = V (K) = V (K) = V). Therefore, the cost function Ji (V) should be considered, and the unknown is the gain Ki: Ji (V) = V (K) = V (K) = V) = V (K) = V)."}, {"heading": "3.1.3 General Algorithm", "text": "The first step is the calculation of predicted quantities. (The first step is the calculation of predicted quantities. (The second step is the calculation of predicted quantities.) The first step is the calculation of predicted quantities. (The first step is the calculation of predicted quantities. (The second step is the calculation of predicted quantities.) These quantities are made from past estimates, the algorithm must indeed be initialized with previous quantities. (The third step is the calculation of quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-quantities-"}, {"heading": "3.2 Specializations", "text": "The main difficulty in applying the KTD consists in the calculation of the interest statistics r \u0192i | i \u2212 1, P\u03b8ri and Pri (for which the statistics \u03b8 \u0441i | i \u2212 1 and Pi | i \u2212 1 are required). First, the evaluation of the value function in the case of a linear parameterization is taken into consideration. The associated Bellman equation is (40), in which case an analytical derivation is possible. Then, an approximation scheme, the odorless transformation (UT) by Julier and Uhlmann (2004) is introduced, which enables the solution of the same problem for a nonlinear parameterization, followed by the evaluation of the Q function and direct optimization."}, {"heading": "3.2.1 KTD-V: Linear Parameterization", "text": "Here the linear parameterization of the equation (8) is assumed, i.e. V-shaped (s) -shaped (s) -shaped (s) -spatial) formulation (46) can therefore be rewritten as: - \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p = (si) \u2212 -shaped (si + 1) T-shaped (si + 1) -shaped (s) -shaped (s) -shaped (s) -shaped (s) -shaped (s) -shaped (s) -shaped (si + 1) -shaped) T-shaped (si + 1) -shaped (si) -shaped (s) -shaped (S-shaped) policy is fixed, the MDP is reduced to an estimated Markov chain. To shorten the notations, Hi is derived as: Hi = -shaped (si) -shaped (si) -shaped (s) -shaped (si) -shaped (+ 1) -shaped (66) -shaped observation (i) -shaped (i): \u2212 -1."}, {"heading": "3.2.2 The Unscented Transform", "text": "The problem is to calculate the mean and covariance of Y (that is, the number of rows in the first and second columns of the X column), but if the mapping is linear, the relationship between X and Y can be analytically calculated as E [Y] = AE [X] and E [Y], where A is a matrix of the ad hoc dimension (that is, the number of rows in the Y column), in which case the required mean and covariance is analytically described as E [Y] = AE [X] and E [Y]. This result was used to determine the KTD-V algorithms of Section 3.2.1.If the mapping is nonlinear, the relationship between X and Y can be described as: Y (71)."}, {"heading": "3.2.3 KTD-V: Nonlinear Parameterization", "text": "This section considers a generic parameterization of the value function V \u2212 \u2212 \u2212 si, 2008): It may be a neural network (Bishop, 1995), a semi-parametric core representation (mind, pietquin \u2212 \u2212 \u2212 \u2212 si, 2008), or any functional representation of interest, as long as it can be described by a set of p-parameters. Still, the problem is to calculate the statistic of interest that becomes traceable with the unscented transformation. The first thing to calculate is the set of sigma points from known statistics. (si + 1) \u2212 i \u2212 1 and Pi \u2212 1, as well as the associated weights using the equations (74-77) that become traceable with the unscented transformation."}, {"heading": "3.2.4 KTD-SARSA", "text": "This section is about the evaluation of the Q function of a fixed predetermined policy. The associated algorithm is called KTD-SARSA, which can be misleading. In fact, SARSA is eventually understood to be a \"Q function\" evaluation algorithm associated with optimistic political iteration (e.g., greedy politics), where the focus is on the Q function evaluation problem, and the control part is disregarded. For a general parameterization Q question, and taking into account the Bellman evaluation equation (41), the state-space model (46) can be rewritten as follows: \"[2] Kant i \u2212 1 + vi ri = Q question (si, ai) \u2212 i + 1)."}, {"heading": "3.2.5 KTD-Q", "text": "This section focuses on the Q-square optimization, i.e. on finding an approximate solution for the Q-square optimization Q = 84. It is assumed a general parametrization Q-square square. The state-room model (46) can be specialized as follows: {\u03b8i = \u03b8i \u2212 1 + vi = Q-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square-square"}, {"heading": "3.3 Algorithmic Complexity", "text": "s specify the number of parameters. Unscented transformation involves a cholesky decomposition whose computational complexity is generally O (p3). However, since the variance update (60) is an update, the cholesky decomposition can be performed in O (p2) (e.g., see Gill, Golub, Murray, & Saunders, 1974). The various algorithms imply that 2p + 1 times the gti function must be evaluated in each time step. For KTD-V or KTD-SARSA and general parameterization, any evaluation is limited by O (p). For KTD-Q, the maximum must be calculated by the actions. Notation A represents the cardinality of the action space when the computational complexity of the algorithm is used to search for the maximum."}, {"heading": "4. KTD: the Stochastic Case", "text": "The KTD framework presented so far assumes deterministic transitions. If this is not the case, the observation noise ni cannot be assumed to be white (since it would include both stochasticity of the MDP and inductive bias), while it is a necessary condition for the KTD derivation. First, it is shown that the use of KTD in a stochastic MDP is associated with bias, then a color noise model is introduced to alleviate this problem, and it is used to extend the KTD. Also, the problem caused by non-political learning, which prevents the derivation of an XKTD-Q algorithm, is discussed."}, {"heading": "4.1 Stochastic Transitions and Bias", "text": "Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-Ki-."}, {"heading": "It is clear that this bias is zero for deterministic transitions.", "text": "The assumption that the reward does not depend on the transiting state is made for the technical simplification of the demonstration (due to the conditioning of the cost function on past observed rewards). However, it is done without loss of universality. \u2212 si [[Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si si [Es] si si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si] si \"si si\" si \"si [Es] si\" si \"si\" si \"si\" si [Es] si [Es] si [Es] si [Es] si [Es] si [Es] si] si [Es] si [Es] si [Es] si [Es] si] si [Es] si [Es] si [Es] si \"i) i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i"}, {"heading": "4.2 A Colored Noise Model", "text": "The extension to Q function evaluation is straightforward, and the optimization of Q function is discussed later because it has a non-political aspect (the learned policy is not behavior-oriented).However, the Bellman equation, initially proposed by Engel et al. (2005) (the basis of the so-called Monte Carlo GPTD algorithm), is presented first before being adapted to extend the KTD model.The policy set for evaluation reduces the MDP in an estimated Markov chain of probability transition, the PDP (. | s) = p (. | s, \u03c0 (s) and the reward R\u03c0 (s, s \u00b2 s \u00b2 s \u00b2) distribution process, and the function can be defined as such."}, {"heading": "4.3 Extending KTD", "text": "It is quite easy to use an autoregressive (AR) process noise in a Kalman filter by extending the evolutionary equation (see, for example, Simon, 2006). However, as far as we know, the case of MA observation noise has never been dealt with in the literature, whereas it is necessary to extend the KTD. Note that this noise model is taken into account in a completely different way in the GPTD framework. Basically, it is done using the formula of partitioned matrix inversion, which is not possible here due to a lack of linearity assumption."}, {"heading": "4.3.1 eXtended Kalman Temporal Differences", "text": "It is proposed instead to express the scalar MA noise ni as vector AR noise, which allows the extension of the state-space model (46) to a new one, for which algorithm 1 applies fairly directly. However, let me say that it can be a helpfully random variable. Scalar MA noise (105) is equivalent to the following vector AR noise: (0 1) (1) (1) (1) (1) (1) Ui (106) In fact, of this vectorial AR noise, ni = 1 (1) Ki noise and sp (1) Ki noise, so ni = \u2212 XUi \u2212 1, which is the correct MA model. Noise u-type i is (ui \u2212 ui) T is also centered and its variance matrix: Pu type i = 2 i i i."}, {"heading": "4.3.2 XKTD and Off-policy Learning", "text": "OffPolicy Learning is the problem of learning the value of one policy (target policy) while following another (behavioral policy).OffPolicy Learning is the problem of learning the value of one policy (target policy) while following another (behavioral policy).KTD-Q (or more generally Q-Learning-like algorithms) is an example of off-policy learning: Behavioral policy is a sufficiently explorative policy, while the policy learned is the optimal one. More generally, OffPolicy Learning is of interest (Sutton & Barto, 1998).As classic entitlement trace algorithms applied to off-policy learning, XKTD should fail because it involves some effects of multi-step transitions that are contaminated by behavioral policy and are not compensated in any way."}, {"heading": "5. Convergence Analysis", "text": "This section provides a convergence analysis for both KTD (deterministic MDPs) and XKTD (stochastic MDPs)."}, {"heading": "5.1 Deterministic Case", "text": "It leads to a result similar to that of the residual algorithms (Baird, 1995), which means the minimization of the residual square costs. This theorem makes some strong assumptions (in fact, the same as the GPTD framework, but without the linear hypothesis), but it is important to note that even if these hypotheses are not fulfilled, the cost function (47) is still minimized. The goal of this result is to link KTD to more classical RL algorithms. Theorem 2: Among the assumptions that posterior and noise distributions are Gaussian and that the previous Gaussian also (of mean duration 0 and variance P0) when the Kalman Temporal Differences algorithms (white noise assumptions) minimizes the following regulated empirical costs: Ci."}, {"heading": "5.2 Stochastic Case", "text": "Here, a convergence analysis for the XKTD in stochastic MDPs is provided, which can easily be extended to room i. Again, some strong assumptions are made without compromising the minimization of the cost function (47) if they are not satisfactory. \u2212 Relying on the posterior and noise distribution being Gaussian, as well as the previous distribution (of mean 0 and variance P0), the XKTD estimator minimizes the (weighted and regulated) square linkage values back to Monte Carlo. \u2212 Sich: Ci (error) = 11. \u2212 2j \u2212 1V (of mean 0 and variance P0). \u2212 Sich estimator minimizes the (weighted and regulated) square error distribution back to Monte Carlo. Again, the result of van der Merwe (2004, ch. 4.5) is used. The corresponding evidence is used for a random model (the space can be easily expanded to the identity matrix)."}, {"heading": "6. An Active Learning Scheme", "text": "The parameters modeled as random variables, and the value (or Q) function, which is a function of these parameters, are a random variable for a given state (or state-action pair). First, it is shown how to calculate one's expectation and the associated uncertainty due to the odorless transformation. The dilemma between exploration and exploitation should benefit from such uncertainty information. Few approaches in the literature allow dealing with the problem of value approximation and calculating uncertainty about values in the meantime. Engel's work (2005) is one such approach, but the effective use of the received uncertainty information remains for future work. Here is a proposed form of active learning, which is a kind of fully exploratory policy in the context of KTD-Q. This paper shows that learning in Section 7 is effectively accelerated."}, {"heading": "6.1 Computing Uncertainty over Values", "text": "In order to propagate the uncertainty of the parameters to the value function, a first step consists in calculating the sigma points of the parameter vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector."}, {"heading": "6.2 A Form of Active Learning", "text": "A simple active learning scheme that uses this uncertainty information is provided here. KTDQ (determinism of transitions is assumed here) is a non-political algorithm: the optimal policy is learned while another behavior policy is pursued. A natural question is which behavior policy is to be chosen to accelerate learning. Here, an answer is given. I am the current temporal index. The system is in a state si, and the actor must select an action ai. The algorithm under consideration is KTD-Q, with the estimated imbalances between i \u2212 1 | i \u2212 1 and Pi \u2212 1 | i \u2212 1 available. They can be used to approximate the uncertainty of the Qfunction parameterized by \u0445i \u2212 1 in the state si, and for each action a. Instead, let us be the corresponding deviation from si \u2212 1 (si, a). The action ai is selected according to the following random behavior policy: Maneuver (1 | si) (si = si \u2212 1)."}, {"heading": "7. Experiments", "text": "This section offers a series of classical RL benchmarks aimed at comparing KTD and variants with state-of-the-art algorithms and highlighting their various aspects. \"Atomic\" benchmarks have been selected to separately highlight uniform characteristics of KTD (see Table 1), which should be quite complex for a more difficult task. Comparison algorithms are TD, SARSA and Q-Learning with function approximation, as well as (recursive form of) LSTD and (MC) GPTD. For reasons of reproducibility, all parameter values are provided for each experiment. Their extensions to authorization histories are not taken into account here, as LSTD performs better than TD (\u03bb) and variation of \u03bb has little impact on the performance of the LSTD, according to Boyan (1999)."}, {"heading": "7.1 Choosing KTD Parameters", "text": "In order to use the (X) KTD framework, parameters must be selected: the variance of observation noise (or the residual variance for XKTD), the priors, and the variance of process noise. As they are rarer and perhaps less intuitive than, for example, selecting a learning rate, they are discussed here. Evolutionary noise for KTD and residuals for XKTD translate the user's confidence in the ability of the chosen parameterization to represent the true value function. If it is known in advance that the value function is in hybrid space (as is the case in the table case, for example), the corresponding variance can be selected very small (but never zero for reasons of numerical stability). Another way to choose these variances is to interpret them by their weighting of the samples, see Equation of Samples (119) and (129)."}, {"heading": "7.2 Tsitsiklis Chain", "text": "This first experiment aims to illustrate the ability of KTD to handle nonlinear parameters and their convergence properties. It consists of a 3-state Markov chain, first proposed by Tsitsiklis and Roy (1997). State i transit in state i with probability 0.5 and in state i \u2212 1 with probability 0.5 (state 1 transits into state 1 or 3 with equation probability). The reward is always zero, so the optimal value function is zero. This chain is very simple, however, a nonlinear parameterization that causes TD with function approximation divergence is taken into account. Let us parameterize the 3-3 identity matrix and M the 3-3-3 matrix as: M = 12 323 2 1 1 21 2 2 2 1 (147) The value function is parameterized by a single scalar convergence of TIS parameters, their parameterization is called TIS-3 (note that this is a VIS-3)."}, {"heading": "7.3 Boyan Chain", "text": "In this section, KTD and XKTD are compared with two other second value algorithms that do not allow the functions to converge, namely (recursively) LSTD and (parametrically) MC-GPTD on a simple estimated Markov chain, but the Boyan (1999) chain is threefold: It shows the effectiveness of the procedure and the reward of the procedure (of XKTD compared to KTD) and shows the non-stationary nature of the handling. The Boyan chain is a 13-state Markov chain in which the state s0 is an absorbing state, s0 with probability 1 and a reward of -2, and si transmits either si \u2212 1 or si \u2212 2 with probability 0.5 and reward. The function vector-vector-vector-vector-ver (s) for the states s12, s4 and s0 are each [1, 0, 0] T, 0, 0, 0, 0, 0, 0."}, {"heading": "7.4 Simple Maze", "text": "With the KTD frame, the parameters are modeled as random variables. As it is a function of the parameters, the approximate (or Q-) function is a random function. It is therefore possible to calculate a variance associated with the value of each state, as shown in Section 6.1. However, it is a necessary condition to handle the exploration exploitation dilemma in a value (or Q-) function approximation context. In this section, the uncertainty information that can be obtained from the KTD frame is illustrated by a simple labyrinth problem. The 2d continuous state space is the unit square: (x, y), [0, 1] 2. Deviations are to be moved left, right, up or down, the magnitude of 0.05 in any case. The reward is + 1 if the agent leaves the labyrinth in 1 and x."}, {"heading": "7.5 Inverted Pendulum", "text": "The last experiment is the reverse pendulum, as described by Lagoudakis and Parr (2003). The aim is to compare two value-oriented iteration-like algorithms, namely KTD-Q and Qlearning, which are aimed directly at learning the optimal policy. LSTD and GPTD cannot be taken into account here: since they are unable to handle non-linearity (where non-linearity is the maximum operator), they cannot be used with the Bellman optimality operator. Three actions are also permitted: left force (-1), right force (+ 1) or no force (0). The associated state space consists of a pendulum of unknown length and mass at the upright position by applying forces to the cart to which it is attached."}, {"heading": "7.5.1 Learning the Optimal Policy", "text": "First, the ability of algorithms to learn optimal policies is compared; for Q-Learning, the learning rate is set to \u03b1i = \u03b10 n0 + 1n0 + i with \u03b10 = 0.5 and n0 = 200, according to Lagoudakis and Parr (2003); for KTD-Q, the parameters are set to P0 | 0 = 10I, Pni = 1 and Pvi = 0I. For all algorithms, the initial parameter vector is set to zero; training samples are collected online with random episodes; the agent begins in a randomly disturbed state close to equilibrium (0, 0) and then follows a policy that randomly selects actions; the average length of such episodes was about 10 steps, and both algorithms learn from the same trajectories; the results are summarized in Figure 5.For each study, learning is carried out over 1000 episodes."}, {"heading": "7.5.2 A Form of Active Learning", "text": "The parameters are random variables, as explained in Section 6 and illustrated in Section 7.4, but the Q function is a random function, and the KTD framework allows the calculation of a variance associated with the value of each state. Here, an experiment is proposed that aims to use this uncertainty information to speed up learning, and learning is still done from random paths. However, the form of active learning described in Section 6 is now being considered, and the environment is randomly weighted as before. If the system is in a certain state, the standard deviation of the Q function is calculated for each action. These deviations are normalized, and the new action is randomly scanned according to the probabilities weighted by the deviations, making an uncertain action more sampled. The average length of such episodes of such episodes was about 11 steps that are not very different from the equally random transitions."}, {"heading": "8. Discussion and Perspectives", "text": "In this section, the proposed framework is discussed and combined with some related approaches."}, {"heading": "8.1 Discussion", "text": "The approaches related to the KTD framework have already been proposed. Engel (2005) proposes an approximation of the value of the function that cannot be approximated. However, as previously explained, its principle is to model the value function as a Gaussian process and adopt a generative model that is linked to the Bellman evaluation equation. However, connections between Engels approach and the proposed approach have been discussed throughout the work, especially when we realize that GPTD could probably be extended to handle it, and more importantly, to handle non-linear arrangements in a derivative-free manner that makes it possible to consider non-linear parameterization and the Bellman optimization operators. Engels framework enables the construction automatically and online of kernel-based linear parameterization, which is an advantage over the proposed framework."}, {"heading": "8.2 Conclusion and Perspectives", "text": "In fact, it is so that most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) In fact, it is so that they are able to determine for themselves what they want. (...) \"It is not so that they abide by the rules. (...)\" \"It is so that they abide by the rules. (...)\" (...) \"(...)\" (...) \"((...)\" (...) \"(...)\" ((...) \"((...)\" (...) \"((...)\" (...) (\") ((\") ((It is). () \"(It is). (\" (It is). (\") (It is). (\" (It is). (\") (It is) (\" (it is)."}, {"heading": "Acknowledgments", "text": "The authors thank the European Community (FP7 / 2007-2013, Funding Agreement 216594, CLASSiC project: www.classic-project.org) and the Lorraine Region for their financial support. Matthieu Geist thanks ArcelorMittal Research for their financial support during his doctoral thesis 2006-2009."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features.", "creator": "TeX"}}}