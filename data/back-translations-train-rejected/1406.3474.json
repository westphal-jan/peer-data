{"id": "1406.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network", "abstract": "We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.", "histories": [["v1", "Fri, 13 Jun 2014 10:11:18 GMT  (3676kb,D)", "http://arxiv.org/abs/1406.3474v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sijin li", "zhi-qiang liu", "antoni b chan"], "accepted": false, "id": "1406.3474"}, "pdf": {"name": "1406.3474.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network", "authors": ["Sijin Li", "Zhi-Qiang Liu", "Antoni B. Chan"], "emails": ["sijin.li@my.cityu.edu.hk", "SMZLIU@cityu.edu.hk", "abchan@cityu.edu.hk"], "sections": [{"heading": null, "text": "We work on assessing human body positions using monocular images with a deep convolutionary neural network. In particular, we learn simultaneously a pose joint regressor and a sliding window detector for body parts in a deep network architecture. We show that the recognition of body parts helps regulate the network and lead to a good solution. We report competitive and state-of-the-art results on several data sets. We also show empirically that the learned neurons in the middle layer of our network are aligned with localized body parts."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Related work", "text": "Subsequently, we will briefly compare previous multi-task approaches and regression networks that have most to do with our work. [26] will train a heterogeneous multi-task model by encouraging the parameters for the regression task and the classification task to share the same genre pattern. You will find that joint training tends to find the most useful features in the input for both tasks. Instead of sharing a genre pattern, our framework forces the heterogeneous tasks to share the common functionality that is good for both tasks."}, {"heading": "3. Heterogeneous Multi-task Learning", "text": "Our heterogeneous multi-task framework consists of two types of tasks: 1) a pose regression task, in which the goal is to predict the positions of human body joints in an image; 2) a series of body part detection tasks, in which the goal is to classify whether a window in the image contains the specific body part. In the following, we assume that a boundary box has already been provided around the human being, e.g. with the help of an upper body detector [1]. All coordinates refer to the boundary box that contains the human. Our framework is summarized in Figure 1."}, {"heading": "3.1. Joint point regression", "text": "The regression task consists in predicting the position of the joint points for each human body part. The coordinates of each joint point are assumed as target values. We normalize all coordinates with the size of the boundary field, so that their values are in the range of [0, 1]. We use the square error as a cost function for our regression task, Er (J-i, Ji) = [Ji-J-i] 22, (1), where Ji and J-i are the basic truth or the predicted positions for the i-th joint."}, {"heading": "3.2. Body part detection", "text": "In the body part recognition tasks, the goal is to determine whether a specific window in the image contains a specific body part > relative to the total length. Let P be the total number of body parts and let L be the number of overlapping windows within the bounding box. For the p-th body part, we train L classifiers, namely Cp, 1,..., Cp, L, to determine whether the l-th window contains a body part. Note that for each location, we train a separate classifier that allows the part detector to learn a site-specific appearance for the part, as well as site-specific contextual information with other parts. For example, a lower arm in the upper corner of the bounding box will be more likely to be vertical or diagonal. In our training set, the commented body parts are presented as sticks. Therefore, we must match the body part detector by first identifying the windows in the training set that contain each body part."}, {"heading": "3.3. Global cost function", "text": "Our global cost function is the linear combination of the regression cost function for all joints and the detection cost function for all parts and windows across all training patterns, \u03a6 = \u03bbr \u2211 t \u2211 i Er (J (t) i, J (t) i) + \u03bbd \u0445 t \u2211 p \u2211 l Ed (y (t) p, l, y (t) p, l), (4) where \u03bbr or \u03bbd are the weights for regression or detection tasks and the high rate (t) indicates the index of the training pattern."}, {"heading": "3.4. Network Structure", "text": "The design of our network is based on the following considerations: \u2022 Low-level feature sharing: We leave the detection tasks and regression tasks to share the same learned feature representation. This is motivated by the following two reasons. First, functions learned for the detection task should also be helpful to identify parts or joints in the regression task. Second, feature sharing will reduce the number of parameters and encourage the network to generalize to a wider range of samples. \u2022 The detection task is to determine whether a local window penetrates the specific body part, while the regression task is to predict the coordinates of the common position."}, {"heading": "3.5. Training", "text": "We jointly train the regression and recognition networks with the global cost function in (4). We use back propagation [16] to update the weights. In the light of a training pattern, predictions for both tasks are calculated, and the corresponding gradients are propagated back through the network. In layers with multiple output layers, the gradient from their output layers is added to the weight update. \"Dropout\" [12] is also used in the first fully connected layers for regression and detection tasks to prevent overfitting. In the experiments, the failure probability will be 0.5. In each iteration, neurons are randomly selected in failure layers, with a 0.5 probability to forward their activation to the output units, and only the selected neurons will participate in the return propagation during this iteration. In the test phase, all neurons are multiplied by their activation value by 0.5 for the normalization, \"as this proves to be very effective as a strategy."}, {"heading": "4. Experiments", "text": "We present experiments with our method HMLPE (heterogeneous multi-task learning for pose estimation)."}, {"heading": "4.1. Training data", "text": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronical Activities Stickmen (SA [6]), Frames Labeled In Cinema (FLIC [19]), We Are Family (WAF) [5]. For Buffy, LSP, FLIC, we use only their respective training sets, while we use the entire ETHZ, SA, and WAF data sets for training. In total, we have collected 8427 images for training. We depict the human body with a series of joints and use the segments between these joints to represent body parts. For data sets with stick-only labels, we use the next end of the stick or the average of the next ends as the common point. We define 8 joints (nose, neck, left and right shoulders, left and right elbows), and 7 body parts (head, left and right shoulder, left arms and the), and we only use them for training."}, {"heading": "4.2. Experiment setup", "text": "For our HMLPE, the Pose Regression Task predicts 8 common positions (16 outputs in total), and the detection task consists of 7 body parts. For the detection task in HMLPE, we use 64 local windows of the same size, evenly distributed in the boundary field. In all experiments, the window size is set to 30 x 30, which is comparable to the size of a body part in the training set. We train the network on the basis of the training data discussed in the previous section in order to obtain an initial network. We then use the initial network as a starting point for training the network based on the training data of a particular data set, either Buffy or FLIC. The initial network serves as a preliminary stage for regulating the network. We train and evaluate our network on a Dell T3400 with GTX 770 4G. Training the network takes 1 to 2 days, while evaluating 4000 images takes 5-6 seconds."}, {"heading": "4.3. Evaluation on Buffy Set", "text": "We use the same upper body detector as [7, 11]. To obtain the human Bounding Box, the width and height of the upper body detection windows are scaled by a fixed factor (swidth = 1.7, sheight = 0.5), which was empirically determined according to the training set. The scaled detection window is used as a human Bounding Box, and the imageis trimmed and reduced to 112 x 112. We use the percentage of the correct PCP part (PCP) to measure the accuracy of the pose estimation. As shown in [11], the upper PCP assessment size is not correctly calculated as the upper PCP assessment of the upper arms, i.e. we use the evaluation tool provided by [11] to calculate the correct PCP part, where an estimated body part with endpoints (e1, e2) is considered correct if the upper PCP assessment size of the upper arms is not calculated correctly."}, {"heading": "4.4. Evaluation on FLIC Data set", "text": "Next, we evaluate using the FLIC test set. We use the same torso field as [19] with scale factors (swidth = 3.5, sheight = 4.5) set empirically from the training set. [19] uses the following accuracy to evaluate their performance, accJi (r) = 100Nsample Nsample \u2211 t = 1 (100 \u00b7 \u0441\u0435J (t) i \u2212 J (t) i \u01922 \u0445 J (t) lhip \u2212 J (t) rsho \u0432 2 \u2264 r). (7) where J (t) i and J (t) i are the ground-level remark and predicted position for the i-th point of the joint test pattern t. Since [19] compares their methods with several previous approaches and shows that their model performs best according to these criteria, we compare only with [19]. The accuracy results are shown in Figure 4. HMLPE has better accuracy with a looser point of the DEC test image in general; for the accuracy of the LE = on the MP20 side, the accuracy of the MP20 is on the other side."}, {"heading": "4.5. Effect of multi-task training", "text": "All parameters except the weights of the cost function remain the same. We show training and test errors in Figure 5 and Table 3.Firstly, the network performs poorly with only the detection task in both the training and the test.1 Even the use of tiny weights in the detection tasks helps to improve convergence, which leads to a significant increase in performance. Within a certain range, increasing weights in the detection tasks result in lower errors. If the detection tasks are heavier, performance decreases. This is reasonable, as the gradient is then dominated by the detection task. These results suggest that the detection task greatly benefits from the feature representation caused by the detection tasks. Gradations in the detection tasks not only result in the network achieving a better minimum in the detection task."}, {"heading": "5. Visualization of features", "text": "In this section, we will examine the characteristics learned by the network. As the first layer relies on the input image, the filter response may reflect what affects the lower layers of the image to which the neurons are sensitive; the learned filters are in Figure 6a, and as expected, they look like edge or gradient detectors for different orientations; that is, the activation of some neurons in the center is influenced by a different approach that maximizes the input. Instead, we use the property that our network is connected only in the first 6 layers, which is the activation of some neurons in the center."}, {"heading": "6. Conclusion", "text": "In this paper, we have proposed a heterogeneous multi-task learning framework with deep convolutional neural network for human pose estimation. Our framework consists of two tasks: pose regression and body-part detection via sliding window classifiers. We demonstrate empirically that the joint training represents a regression with the recognition tasks that guide the network to learn meaningful features for pose estimation, and that the network can be generalized well using test data. Finally, we visualize the mean and high characteristics based on the average of the tracked patches from the maximum reacting neurons. We have also found that these neurons are selective to form patterns that resemble localized human body parts. In future work, we will expand our network for learning positions with occlusion and combine our framework with unattended learning to train the network before training. In addition, we would like to expand our framework for assessing human poses from video sequences."}], "references": [{"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Human pose estimation from still images using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L. van Gool"], "venue": "In CVPR. IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Better appearance models for pictorial structures", "author": ["M. Eichner", "V. Ferrari"], "venue": "In BMVC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "We are family: Joint pose estimation of multiple persons", "author": ["M. Eichner", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Human pose co-estimation and applications", "author": ["M. Eichner", "V. Ferrari"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "2d articulated human pose estimation and retrieval in (almost) unconstrained still images", "author": ["M. Eichner", "M. Marin-Jimenez", "A. Zisserman", "V. Ferrari"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE TPAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Pictorial structures for object recognition", "author": ["P.F. Felzenszwalb", "Huttenlocher", "D.P"], "venue": "IJCV, pages 55\u201379,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Computationally efficient regression on a dependency graph for human pose estimation", "author": ["K. Hara", "R. Chellappa"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning effective human pose estimation from inaccurate annotation", "author": ["S. Johnson", "M. Everingham"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Cor-  rado", "J. Dean", "A. Ng"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Poselet conditioned pictorial structures", "author": ["L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Modec: Multimodal decomposable models for human pose estimation", "author": ["B. Sapp", "B. Taskar"], "venue": "In In Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Cascaded models for articulated pose estimation", "author": ["B. Sapp", "A. Toshev", "B. Taskar"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Real-time human pose recognition in parts from single depth", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep convolutional network cascade for facial point detection", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Pose-sensitive embedding by nonlinear nca regression", "author": ["G.W. Taylor", "R. Fergus", "G. Williams", "I. Spiro", "C. Bregler"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Heterogeneous multitask learning with joint sparsity constraints", "author": ["X. Yang", "S. Kim", "E.P. Xing"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Articulated pose estimation with flexible mixtures-of-parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Articulated human detection with flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "IEEE TPAMI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Learning gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "Some algorithms [21] based on depth maps have already been used in practice.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 5, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 8, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 11, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 18, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 25, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 8, "context": "One popular graphical model for human pose estimation is the pictorial structure model [10] (PSM), which uses pairwise connections between parts to form a tree.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Exact inference is possible and the solution is guaranteed to be globally optimal [10], but the inference is still very expensive for real-time applications.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "For both definitions of parts, the appearance model is critical for learning a good PSM [3, 7].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": "For both definitions of parts, the appearance model is critical for learning a good PSM [3, 7].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": "Several methods have been proposed to alleviate this problem by truncating the pose space [7, 20].", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "Several methods have been proposed to alleviate this problem by truncating the pose space [7, 20].", "startOffset": 90, "endOffset": 97}, {"referenceID": 26, "context": "On the other hand, [28] extends the traditional PSM by allowing each body part to have multiple modes.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 11, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 17, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 0, "context": "In the second approach, pose estimation is viewed as a regression task [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 12, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 20, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 6, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 24, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 27, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 6, "context": "We refer reader to [8, 29] for a review.", "startOffset": 19, "endOffset": 26}, {"referenceID": 27, "context": "We refer reader to [8, 29] for a review.", "startOffset": 19, "endOffset": 26}, {"referenceID": 24, "context": "In [26], a heterogeneous multi-task model is trained by encouraging the parameters for the regression task and the classification task to share the same sparsity pattern.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [9], a deep convolutional network is trained for scene labeling, by defining a multi-category classification task for each pixel.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "[23] trains a deep CNN to learn a pose-sensitive embedding with nonlinear NCA (neighbourhood components analysis) regression, and predicts the location of the head and hands by finding the nearest neighbor with the learned embedding features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In contrast to [23], we introduce accessory tasks for learning shared \u201cpose features\u201d, and output the joint locations directly from the regression network.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "In [22], a multi-stage system with deep convolutional networks is built for predicting facial point locations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Similarly, [24] trained cascaded convolutional networks for human pose estimation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "In [25] semi-supervised learning is used to guide the network to learn an internal representation that reflects the similarity between training samples.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Finally, in order to investigate the feature representation learned by the neural network, [15] estimates the \u201coptimal\u201d input that maximizes the activation of a selected neuron, and find that the \u201coptimal\u201d input resembles a human face.", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "In contrast to [15], we visualize a feature by averaging image patches that are associated with the neurons with maximum responses in an upper-layer, and obtain similar results.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Most of the neurons in our network are Rectified Linear Units (ReLu) [17], where fact(x) = max(0, x).", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "[17] showed that ReLus are good for recognition tasks and fast to train.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We use backpropagation [16] to update the weights.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "\u201cDropout\u201d [12] is also used in the first fully connected layers for the regression and detection tasks to prevent over-fitting.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "We refer reader to [14] for more details about the training procedure.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 93, "endOffset": 96}, {"referenceID": 11, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 161, "endOffset": 164}, {"referenceID": 17, "context": "Frames Labeled In Cinema (FLIC [19]), We Are Family(WAF) [5].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Frames Labeled In Cinema (FLIC [19]), We Are Family(WAF) [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "We use the same upper body detector as [7, 11].", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "We use the same upper body detector as [7, 11].", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "RoDG-Boost[11] 51.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Eichner[7] 50.", "startOffset": 7, "endOffset": 10}, {"referenceID": 26, "context": "MoP [28] M = 6 51.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] M = 9 56.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] M = 12 60.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] 57.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "As pointed out in [11], the previous PCP evaluation measure does not compute PCP correctly.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "We use the evaluation tool provided by [11] to calculate the corrected PCP, where an estimated body part with end points (e1, e2) is considered as correct if", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "On the whole Buffy test set (276 images), HMLPE achieves better results than [7, 11] on the more difficult parts, lower arms (4.", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "On the whole Buffy test set (276 images), HMLPE achieves better results than [7, 11] on the more difficult parts, lower arms (4.", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "8% improvement), but gets a slightly worse result than [11] on upper arms (1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "In this case, HMLPE achieves slightly better results than [28] (0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "We also run the code from [28] on our full training set, using different number of components per part (denoted as M = {6, 9, 12}).", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Using M = 12, [28] has better PCP (4%) on the lower arms compared to HMLPE.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "5%) than [28] on upper arms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "We use the same torso box as [19] with scale factors (swidth = 3.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "[19] uses the following accuracy to evaluate their performance,", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Since [19] compares their methods with several previous approaches, and show that their model performs the best under this criteria, we only compare with [19].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Since [19] compares their methods with several previous approaches, and show that their model performs the best under this criteria, we only compare with [19].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "For the 2nd and 3rd layers (mid- and high-level features), we use a different approach than [15], which finds the input that maximizes one specific neuron.", "startOffset": 92, "endOffset": 96}], "year": 2014, "abstractText": "We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a slidingwindow body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.", "creator": "LaTeX with hyperref package"}}}