{"id": "1406.3497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation Supplementary Material", "abstract": "This paper is about learning a continuous approximation of the Pareto frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy-based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy-gradient multi-objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient-ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient-based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs.", "histories": [["v1", "Fri, 13 Jun 2014 10:49:38 GMT  (398kb,D)", "http://arxiv.org/abs/1406.3497v1", null], ["v2", "Tue, 18 Nov 2014 21:31:32 GMT  (1262kb,D)", "http://arxiv.org/abs/1406.3497v2", "AAAI-15 Supplement. Updated upon acceptance at the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["matteo pirotta", "simone parisi", "marcello restelli"], "accepted": false, "id": "1406.3497"}, "pdf": {"name": "1406.3497.pdf", "metadata": {"source": "CRF", "title": "Multi\u2013objective Reinforcement Learning with Continuous Pareto Frontier Approximation", "authors": ["Matteo Pirotta", "Simone Parisi", "Marcello Restelli", "Leonardo Da Vinci"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "2 Preliminaries", "text": "In practice, a MOMDP model is described by a tuple < S, A, P, R, D >, where S'Rn is the continuous space for action, A Rm is the continuous space for action, P is a Markovian transitional model in which P (s \"s, a) defines the transitional density between state s and s\" under action a, R = [R1,. Rq] T and D is the continuous space for action, in which P (s \"s,\" a) defines the transitional density between state s and s \"under action a, R = [R1,. Rq] T and D is a distribution from which the original state is drawn. In MOMDPs, any policy is associated with expected returns."}, {"heading": "3 Gradient on Policy Manifold for Continuous Pareto Front", "text": "ApproximationIn this section, we first present a general definition of the optimization problem we want to solve, and then explain how to solve it with a gradient-based approach in the MOMDP case."}, {"heading": "3.1 Parametric Pareto Front in MOO", "text": "The idea behind this work, however, is to parameterise the local Pareto-optimal solution curve in objective space in order to create a continuous representation of the Pareto boundary. Let T in Rb be open with b \u2264 q. However, the high-dimensional analogy of a parameterised curve is a smooth map in which the Pareto-optimal solution curve has a smooth map: T \u2192 Rq of class Cl (l \u2265 1), in which t-T and P Rk are the free variable and the parameters. F = E value (T), together with the map [R value] represents the optimal boundary."}, {"heading": "3.2 Parametric Pareto Front in MOMDP", "text": "D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)"}, {"heading": "4 Gradient Estimation from Sample Trajectories", "text": "In this section we present standard results with respect to the approaches used in the Hessian estimation literature and we provide a theoretical analysis of the Hessian estimation methods. As the map of the Hessian estimation can be freely designed, the corresponding terms (e.g. Dtn) and the corresponding terms (e.g. Dtn) can be calculated exactly. On the other hand, the term can be estimated in connection with the MDP (JV) and H J (JV). While the estimate of the expected discounted reward and the associated gradient is an old topic in the Hessian literature and several results have been proposed."}, {"heading": "5 Metrics for Multi\u2013objective Optimization", "text": "This section discusses some of the pros and cons of the literature in order to examine the properties of the hypervolume indicators. Lately, we have focused on the use of performance indicators to obtain an approximate measure of the discrepancy between the candidate and the Pareto. Since the indicator-based algorithms aim to assign a single objective measure to each point, or in other words, to ask a natural question about the correctness of this optimization process and the pareto-one. Instead of directly optimizing the objective functions, they aim to find a solution that maximizes the indicator metrically, a natural question arises about the correctness of the optimization process and about the properties of the indicator functions. For example, hypervolume indicators and their weighted version are among the most widely used metrics in literature. These metrics have gained popularity because they are refleximized by parameters."}, {"heading": "6 Experiments", "text": "In this section, the results in connection with the numerical simulations of the proposed algorithms are led in a continuous and discrete way to the divergence of the primary system. (In particular, the performance is compared against some existing algorithms [8, 10]. In all experiments, the learning rate \u03b1 is determined by hand tuning and for the settings, a multi-objective version of the standard discrete-time system Linear-Quadratic Gaussian Regulator (LQG) is compared with multidimensional and continuous state and action spaces. For a complete description of the LQG problem and for the settings, we refer to [10]. This scenario is particularly informative, as all terms can be precisely calculated, we focus on the properties of parameterization and we demand the analysis of the estimation phase for the water reservoir domain. First, we present the results for a 2-dimensional LQG domain. The LQG is a problematic domain, as it is defined only in the control area."}, {"heading": "7 Conclusions", "text": "In this paper, we have proposed a novel gradient-based approach to learn a continuous approach to the Pareto boundary in the MOMDPs. The idea is to define a parametric function \u03c6\u03c1 that describes a multiple of the political parameter space mapped to a multiple in the objective space. Considering a metric that measures the quality of the multiple in the objective space (i.e. the candidate boundary), we have shown how to calculate its gradient w.r.t. (and from the trajectory samples), the parameters of \u03c6\u03c1. Updating the parameters along the gradient direction generates a new political diversity that is linked to an improved (w.r.t. of the chosen metric), continuous boundary in the objective space. Although we have provided a derivative that is independent of the specific metric used to measure the quality of candidate solutions, the choice of such a metric influences the empirical nature of the projects presented, the outcome of each one of the analyses, and the outcome of each one of the analyses are likely to be strongly discussed by the other, and the alternatives we have presented in this paper."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Multiobjective reinforcement learning: A comprehensive overview", "author": ["C. Liu", "X. Xu", "D. Hu"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A survey of multiobjective sequential decision-making", "author": ["Diederik M. Roijers", "Peter Vamplew", "Shimon Whiteson", "Richard Dazeley"], "venue": "JAIR, 48:67\u2013113,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["Peter Vamplew", "Richard Dazeley", "Adam Berry", "Rustam Issabekov", "Evan Dekker"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Uniform sampling of local paretooptimal solution curves by pareto path following and its applications in multi-objective ga", "author": ["Ken Harada", "Jun Sakuma", "Shigenobu Kobayashi", "Isao Ono"], "venue": "In Proceedings of GECCO", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Performance assessment of multiobjective optimizers: an analysis and review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V.G. da Fonseca"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Linear fitted-q iteration with multiple reward functions", "author": ["Daniel J. Lizotte", "Michael Bowling", "Susan A. Murphy"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A multiobjective reinforcement learning approach to water resources systems operation: Pareto frontier approximation in a single run", "author": ["A Castelletti", "F Pianosi", "M Restelli"], "venue": "Water Resources Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Importance Sampling for Reinforcement Learning with Multiple Objectives", "author": ["Christian Robert Shelton"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Policy gradient approaches for multi-objective sequential decision making", "author": ["Simone Parisi", "Matteo Pirotta", "Nicola Smacchia", "Luca Bascetta", "Marcello Restelli"], "venue": "IJCNN", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Local search for multiobjective function optimization: pareto descent method", "author": ["Ken Harada", "Jun Sakuma", "Shigenobu Kobayashi"], "venue": "In GECCO,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics", "author": ["J.R. Magnus", "H. Neudecker"], "venue": "Wiley Ser. Probab. Statist.: Texts and References Section", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Analysis On Manifolds", "author": ["J.R. Munkres"], "venue": "Adv. Books Classics Series. Westview Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Optimizing average reward using discounted rewards", "author": ["Sham Kakade"], "venue": "In COLT/EuroCOLT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Adaptive step-size for policy gradient methods", "author": ["Matteo Pirotta", "Marcello Restelli", "Luca Bascetta"], "venue": "In NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Simultaneous perturbation algorithms for batch off-policy", "author": ["Raphael Fonteneau", "Prashanth L. A"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "Monte Carlo statistical methods, volume 319", "author": ["Christian P Robert", "George Casella"], "venue": "New York: Springer,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "On set-based multiobjective optimization", "author": ["Eckart Zitzler", "Lothar Thiele", "Johannes Bader"], "venue": "Trans. Evol. Comp,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Multiplicative approximations and the hypervolume indicator", "author": ["Tobias Friedrich", "Christian Horoba", "Frank Neumann"], "venue": "In GECCO", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A critical survey of performance indices for multi-objective optimisation", "author": ["T. Okabe", "Y. Jin", "B. Sendhoff"], "venue": "In CEC \u201903.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "In the last decades, Reinforcement Learning (RL) [1] has established as an effective and theoretically-grounded framework that allows to solve single-objective MDPs whenever either no (or little) prior knowledge is available about system dynamics, or the dimensionality of the system to be controlled is too high for classical optimal control methods.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Despite the successful developments in RL theory and a high demand for multi-objective control applications, Multi-Objective Reinforcement Learning (MORL) [2, 3] is still a relatively young and unexplored research topic.", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "Despite the successful developments in RL theory and a high demand for multi-objective control applications, Multi-Objective Reinforcement Learning (MORL) [2, 3] is still a relatively young and unexplored research topic.", "startOffset": 155, "endOffset": 161}, {"referenceID": 3, "context": "MORL approaches can be divided into two main categories, based on the number of policies they learn [4]: single\u2013policy and multiple policy.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": ", policy parameters) is greater than or equal to the number q of objectives, the local Pareto\u2013optimal solutions form a (q \u2212 1)\u2013dimensional manifold [5].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Building the exact frontier is generally impractical in real-world problems, the goal is thus to compute an approximation of the Pareto frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the one of the actual front [6].", "startOffset": 267, "endOffset": 270}, {"referenceID": 6, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 85, "endOffset": 91}, {"referenceID": 7, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 85, "endOffset": 91}, {"referenceID": 8, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Among multiple\u2013policy algorithms it is possible to identify two classes: value\u2013based [7, 8] and gradient approaches [9, 10].", "startOffset": 116, "endOffset": 123}, {"referenceID": 10, "context": "In MOMDPs for each policy parameter \u03b8, q gradient directions are defined [11]", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "1As done in [12], we suppose that local Pareto-optimal solutions that are not Pareto-optimal do not exist.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Refer to [13] for details.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "1 Parametric Pareto Front in MOO It has been shown [5] that local Pareto\u2013optimal solutions locally form a (q \u2212 1)\u2013dimensional manifold, assuming d > q.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "The set F = \u03c8\u03c1(T ), together with the map \u03c8\u03c1 constitute a parametrized manifold of dimension b, denoted by F\u03c1(T ) [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "Recently [4], several metrics have been defined, but every candidate presents some intrinsic limits that prevent the definition of a unique superior metric.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "the volume of the manifold [14] and I : F (T )\u2192 R is a continuous indicator function that for each point of F (T ) measures its Pareto\u2013optimality.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "2 Parametric Pareto Front in MOMDP Given a continuous indicator function I, the integral of I over F (T ), with respect to volume, is defined by the equation [14] \u222b", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "where T = D\u03b8J(\u03b8)Dt\u03c6\u03c1(t), \u2297 is the Kronecker product, Nb = 12 (Ib2 +Kbb) is a symmetric (b 2 \u00d7 b) idempotent matrix with rank 1 2 b(b+ 1) and Kbb is a permutation matrix [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "A first analysis was performed in [15] where the authors provided a formulation based on the policy gradient theorem [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "A first analysis was performed in [15] where the authors provided a formulation based on the policy gradient theorem [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "While the estimate of the expected discounted reward and the associated gradient is an old topic in RL literature and several results have been proposed [15, 17], the estimate of the Hessian is not addressed in literature.", "startOffset": 153, "endOffset": 161}, {"referenceID": 16, "context": "While the estimate of the expected discounted reward and the associated gradient is an old topic in RL literature and several results have been proposed [15, 17], the estimate of the Hessian is not addressed in literature.", "startOffset": 153, "endOffset": 161}, {"referenceID": 17, "context": "Recently, the simultaneous perturbation stochastic approximation technique was exploited to estimate the Hessian [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "This formulation resemble the definition of REINFORCE estimate given in [19] for the gradient \u2207\u03b8J(\u03b8).", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Several statistical bounds have been proposed in literature, we refer to [20] for a survey on Monte\u2013Carlo methods.", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "These metrics have gained popularity because they are refinements of the Pareto dominance relation [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Recently, several works have been proposed in order to theoretically investigate the properties of hypervolume indicator [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "Several other metrics have been defined in the field of MOO, we refer to [23] for an extensive survey.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "Recall that the goal of MOO is to compute an approximation of the frontier that includes solutions that are accurate, evenly distributed and covering a range similar to the actual one [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 21, "context": "[22]", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In particular, the performance are compared against some existing algorithms [8, 10].", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "In particular, the performance are compared against some existing algorithms [8, 10].", "startOffset": 77, "endOffset": 84}, {"referenceID": 10, "context": "We start considering a multi\u2013objective version of the standard discrete-time Linear-Quadratic Gaussian regulator (LQG) with multidimensional and continuous state and action spaces [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 9, "context": "For a complete description of the LQG problem and for the settings, we refer to [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "Our primary concern was related to the boundness of the control actions, leading to the following parametrization of the manifold in the policy space: \u03c6\u03c1(t) = \u2212 1/1 + exp(\u03c11 + \u03c12t) and \u03c6 2 \u03c1(t) = \u2212 1/1 + exp(\u03c13 + \u03c14t) with t \u2208 [0, 1].", "startOffset": 227, "endOffset": 233}, {"referenceID": 0, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 1, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 2, "context": "5 and starting from \u03c1 = [1, 2, 0, 3] is shown in Figure 2(a).", "startOffset": 24, "endOffset": 36}, {"referenceID": 9, "context": "In order to compare our frontier with the one obtained by other algorithms, we consider the domain, settings and policy parametrization as described in [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "A simple second\u2013order polynomial in t \u2208 [0, 1] with 5 parameters has been used to parametrize the policy manifold.", "startOffset": 40, "endOffset": 46}], "year": 2017, "abstractText": "This paper is about learning a continuous approximation of the Pareto frontier in Multi\u2013Objective Markov Decision Problems (MOMDPs). We propose a policy\u2013based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy\u2013gradient multi\u2013objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient\u2013ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient\u2013based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non\u2013trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs.", "creator": "LaTeX with hyperref package"}}}