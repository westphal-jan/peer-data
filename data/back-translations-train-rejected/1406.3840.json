{"id": "1406.3840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2014", "title": "Optimal Resource Allocation with Semi-Bandit Feedback", "abstract": "We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden.", "histories": [["v1", "Sun, 15 Jun 2014 18:41:47 GMT  (28kb)", "http://arxiv.org/abs/1406.3840v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "koby crammer", "csaba szepesv\\'ari"], "accepted": false, "id": "1406.3840"}, "pdf": {"name": "1406.3840.pdf", "metadata": {"source": "CRF", "title": "Optimal Resource Allocation with Semi-Bandit Feedback", "authors": ["Tor Lattimore", "Csaba Szepesv\u00e1ri"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 6.38 40v1 [cs.LG] 1 5Ju nWe are investigating a sequential resource allocation problem involving a fixed number of recurring jobs. At each stage, the manager should distribute available resources among the jobs to maximize the expected number of completed jobs. Allocating additional resources to a particular job increases the likelihood that it will be completed, but with a cut-off. Specifically, we are assuming a linear model where the probability increases linearly until it is the same, whereupon the allocation of additional resources is wasteful. We assume that the difficulty of each job is unknown and present the first algorithm for this problem, showing upper and lower limits of regret. Despite its apparent simplicity, the problem has a rich structure: We show that a suitable optimistic algorithm can dramatically improve its learning speed beyond the outcomes normally expected for similar problems when the problem becomes resource-consuming."}, {"heading": "1 INTRODUCTION", "text": "Assuming there are K-jobs and in every work step a learner has to use the available resources with Mk, t-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o, o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o, o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o, o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o, o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-"}, {"heading": "2 PRELIMINARIES", "text": "In each time step t, the learner chooses Mk, t \u2265 0 is subject to the constraint \"\u03b2\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" = \"K\" K \"=\" K. \"After that, all jobs are executed and Xk\", K \"=\" K \"K\" (K, t / \"K\") indicates the success or failure of the work k in the time step t and is taken from a Bernoullanii distribution with the parameter \u03b2 (Mk, t / \"K). We define the gaps\" J, \"k =\" K \"\u2212 1\" Vanish \"\u2212 1k\" as unstructured action spaces, except for unstructured and unstructured spaces."}, {"heading": "3 OVERVIEW OF ALGORITHM", "text": "We take inspiration from the optimal policy for the known implications, which consists of fully allocating the jobs with the smallest implications (simplest jobs) and allocating the rest / overflow to the next simplest job. At each time step t, we replace the unknown implant with a lower boundary between implants, t-1 \u2264 implants. This corresponds to the optimistic strategy, which assumes that every job is as simple as possible. Building a confidence interval about implants is surprisingly delicate. There are two major challenges. First, the function \u03b2 (Mk, t / implants) is indistinguishable for Mk, t = implants k, and for Mk, t implants the job is always complete and little information is gained. This is addressed by using a lower estimate of implants in the algorithm. The second challenge is that Mk, t will vary over time so that the samples Xk, t are not distributed identically."}, {"heading": "4 UPPER BOUNDS ON THE REGRET", "text": "There are four natural cases that will appear in our main result. (Case 1: Insufficient budgets for all jobs.) In this case, the problem can be reduced in a K-poor way by limiting the scope of action to Mk, t = 1 for all. Then, a bandit algorithm like UCB1 [Auer et al., 2002] becomes logarithmic (problem dependent) remorse with some dependence on the lapses 1, k = 1 \u2212 1 for all problems. In particular, the regret looks like Rn. (K = 2 log n: 1, k).Algorithm 1: Optimistic Algorithm1: Input: n, K, 0} Kk = 1: 1."}, {"heading": "5 ESTIMATION", "text": "We start by showing that the confidence intervals are highly likely to contain the truth, and then analyze the rate at which the intervals shrink as more data is observed. \u2212 What is surprising is that the rate is heavily dependent on the data with larger mappings, leading to faster convergence. Let's analyze the confidence intervals that lead to faster convergence."}, {"heading": "6 PROOF OF THEOREM 2", "text": "The first step is to divide the regret into two cases, depending on whether the confidence intervals contain the truth or not. \u2212 The probability that they are not low is negligible to regret. The second component involves the analysis of the overflow process, the approach being reminiscent of the analysis of the UCB algorithm for stochastic bandits. [Auer et al., 2002].Let Fk, t denote the event when none of the confidence intervals basic job k fail to time t: Fk = {s = s \u2264 t to the UCB algorithm for stochastic bandits [Auer et al, 2002]."}, {"heading": "7 INITIALISATION", "text": "In particular, the following algorithm calculates a lower limit of 0 \u2264 \u03bd for a single job with an unknown parameter \u03bd.Algorithm 2 Initialization of \u03bd0 1: for the lower limits {1,.} k is simple: Allocation Mt = 2 \u2212 t and observe Xt 3: if Xt = 0 then again 0 \u2190 2 \u2212 t. 4: end forA naive way to eliminate the need for the lower limits (\u03bdk, 0) k is easy to execute algorithm 2 sequentially for each job \u2212 \u2212 the following statement shows that the procedure of O (1) is reasonable, which justifies the assertion that the terms occurring in Theorem 2 are O (1). Proposition 11. If the procedure of 1 = min {1, the procedure = min = 4.Proof. Let there be the probability that the job will be executed after \u03b2 = time."}, {"heading": "8 MINIMAX LOWER BOUNDS", "text": "Despite the continuous scope for action, the techniques used in testing minimax lower limits for the standard stochastic bandits (Auer et al., 1995) must be adapted to our attitude. Theorem 12. Given the fixed n and 8n \u2265 K \u2265 2 and an arbitrary algorithm, there is an assignment problem (for which the expected regret Rn \u2265 \u221a nK16 \u221a n. Proof. Consider a number of K assignment problems where in Problem, \u03bdj = 2 for all j 6 = k and imp = 21 +. The optimal action in Problem k is to allocate all available resources to the work k if the expected reward is 1 + 2. Interaction between the algorithm and a problem k defines a measurement Pk on the set of results (successes, allocations). We attribute Ek to expectations regarding the measurement Pk. We haveEk."}, {"heading": "9 EXPERIMENTS", "text": "The data points were generated using the modified algorithm described in Section 7, using the mean of 300 samples. In these many samples, the standard error is relatively small (and omitted for readability).We should note that the variance in the regret of the modified algorithm is relatively large, since the regret linearly depends on the random regret. At known lower limits, the variance is extremely small. To illustrate the behavior of the algorithm, we performed four experiments with synthetic data with K = 2 plotted below TL (top left), TR, BL, BR (bottom right).In TL, we specified n = 104, \u03bd1 = 2 and plotted regret as a function of \u03bd2 [2, 10].The experiment shows the usual band-like dependence on the gap 1 / \u0445 1,2. In TR, we weighted the error 1 = 4 / 10 (2 = 6 / log2 n as a function of n)."}, {"heading": "10 CONCLUSIONS", "text": "The simulations confirm the theory and highlight the practical behavior of the new algorithm. There are many open questions and possibilities for future research. Most important is whether the log2 n can be reduced to logn. Problem-dependent lower limits would be interesting. The algorithm is not at all times (although a double trick probably works in theory). The development and analysis of algorithms when the horizon is unknown and high probability limits are of interest. We also wonder whether Thompson sampling can be implemented efficiently for a reasonable period of time (although a double trick probably works in theory)."}, {"heading": "A TECHNICAL INEQUALITIES", "text": "The proof for the following theorem is in the supplementary material.Theorem 13 (r) that we both register an increase (r =). Let us know that Xn will be a sequence of random variables that are adapted to filtration. \u2212 Definition S = [Xt | Ft \u2212 1] = 0. Let Rt be so that | Xt | \u2264 Rt is almost certain, R \u2212 maxt n Rt. Definition S = [Xt] = 1 Xt, 2 = [t = 1 Var [Xt | Ft \u2212 1], andr, v = 3 (r = 1) 2 (v \u2212 1), f (r, v) = 13 log2\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "B TABLE OF NOTATION", "text": "K Number of jobsn time-horizontal parameter, which characterizes the difficulty of the job k \u03b2 (p) function \u03b2 (p): = min {1, p} Mk, t Resources assigned to the job k in time step tXk, t Result of the job k in time step tXk, t Lower limit of the job k in time step t\u03bd k, t Upper limit of the job k in time step t\u03b4 tied to the probability that some confidence intervals will fail. t (i) ith Easiest job in time step t sorted by number of fully assigned jobs with optimal assignmentS \u2022 Optimal amount of resources assigned to the overflow process A \u0445 contains the simplest jobs (sorted by number of jobs with Mk, t = number of jobs with Mk, t \u2212 1 in time step tBt equals number of fully assigned jobs with optimal assignmentS \u2022 Optimal amount of resources assigned to the overflow process."}], "references": [{"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam"], "venue": "IEEE Transaction on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1989}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Bandits with knapsacks", "author": ["Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "In FOCS, pages 207\u2013216,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2013}, {"title": "A near-optimal algorithm for finite partialmonitoring games against adversarial opponents", "author": ["G\u00e1bor Bart\u00f3k"], "venue": "In COLT,", "citeRegEx": "Bart\u00f3k.,? \\Q2013\\E", "shortCiteRegEx": "Bart\u00f3k.", "year": 2013}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G\u00e1bor Bart\u00f3k", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2011}, {"title": "The Theory of Probabilities (Russian)", "author": ["Sergei Bernstein"], "venue": "Moscow,", "citeRegEx": "Bernstein.,? \\Q1946\\E", "shortCiteRegEx": "Bernstein.", "year": 1946}, {"title": "Dynamic pricing under a general parametric choice model", "author": ["Josef Broder", "Paat Rusmevichientong"], "venue": "Operations Research,", "citeRegEx": "Broder and Rusmevichientong.,? \\Q2012\\E", "shortCiteRegEx": "Broder and Rusmevichientong.", "year": 2012}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Multi-armed bandit with budget constraint and variable costs", "author": ["Wenkui Ding", "Tao Qin", "Xu-Dong Zhang", "Tie-Yan Liu"], "venue": "In AAAI,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "No internal regret via neighborhood watch", "author": ["Dean P. Foster", "Alexander Rakhlin"], "venue": "Journal of Machine Learning Research - Proceedings Track (AISTATS),", "citeRegEx": "Foster and Rakhlin.,? \\Q2012\\E", "shortCiteRegEx": "Foster and Rakhlin.", "year": 2012}, {"title": "On tail probabilities for martingales", "author": ["David A. Freedman"], "venue": "The Annals of Probability, 3(1):100\u2013118,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Continuous time associative bandit problems", "author": ["Andr\u00e1s Gy\u00f6rgy", "Levente Kocsis", "Ivett Szab\u00f3", "Csaba Szepesv\u00e1ri"], "venue": "In IJCAI-07,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Self-optimizing memory controllers: A reinforcement learning approach", "author": ["Engin Ipek", "Onur Mutlu", "Jos\u00e9 F. Mart\u0131\u0301nez", "Rich Caruana"], "venue": "SIGARCH Comput. Archit. News,", "citeRegEx": "Ipek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ipek et al\\.", "year": 2008}, {"title": "Organizing the last line of defense before hitting the memory wall for cmps", "author": ["Chun Liu", "Anand Sivasubramaniam", "Mahmut Kandemir"], "venue": "In Software, IEE Proceedings-,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Concentration. In Probabilistic methods for algorithmic discrete mathematics, pages 195\u2013248", "author": ["Colin McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1998\\E", "shortCiteRegEx": "McDiarmid.", "year": 1998}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "In NIPS,", "citeRegEx": "Russo and Roy.,? \\Q2013\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2013}, {"title": "Minimizing regret: The general case", "author": ["Aldo Rustichini"], "venue": "Games and Economic Behavior,", "citeRegEx": "Rustichini.,? \\Q1999\\E", "shortCiteRegEx": "Rustichini.", "year": 1999}, {"title": "A new memory monitoring scheme for memory-aware scheduling and partitioning", "author": ["G Edward Suh", "Srinivas Devadas", "Larry Rudolph"], "venue": "In High-Performance Computer Architecture,", "citeRegEx": "Suh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Suh et al\\.", "year": 2002}, {"title": "Knapsack based optimal policies for budget-limited multi-armed bandits", "author": ["Long Tran-Thanh", "Archie C. Chapman", "Alex Rogers", "Nicholas R. Jennings"], "venue": "In AAAI,", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "In fact, we anticipate numerous extensions and The name was invented later by (perhaps) [Rustichini, 1999].", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": "Given its information structure, the problem belongs to the class of stochastic partial monitoring problems, which was first studied by Agrawal et al. [1989]1, where in each time step the learner receives noisy information about a hidden \u201cparameter\u201d while trying to maximise the sum of rewards and both the information received and the rewards depend in a known fashion on the actions and the hidden parameter.", "startOffset": 136, "endOffset": 158}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989].", "startOffset": 252, "endOffset": 274}, {"referenceID": 6, "context": "adaptations for specific applications, such as in the case of bandits (see, Bubeck and Cesa-Bianchi [2012] for an overview of this rich literature).", "startOffset": 76, "endOffset": 107}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data.", "startOffset": 253, "endOffset": 589}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache.", "startOffset": 253, "endOffset": 721}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources.", "startOffset": 253, "endOffset": 948}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors.", "startOffset": 253, "endOffset": 1056}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors. 2 PRELIMINARIES In each time-step t the learner chooses Mk,t \u2265 0 subject to the constraint, \u2211K k=1 Mk,t \u2264 1. Then all jobs are executed and Xk,t \u2208 {0, 1} indicates the success or failure of job k in time-step t and is sampled from a Bernoulli distribution with parameter \u03b2(Mk,t/\u03bdk) := min {1,Mk,t/\u03bdk}. The goal is to maximise the expected number of jobs that successfully complete, \u2211Kk=1 \u03b2(Mk,t/\u03bdk). We define the gaps \u2206j,k = \u03bd \u22121 j \u2212 \u03bd\u22121 k . We assume throughout for conveBesides Badanidiyuru et al. [2013], all works consider finite action spaces and unstructured reward functions.", "startOffset": 253, "endOffset": 1630}, {"referenceID": 2, "context": "Then a bandit algorithm such as UCB1 [Auer et al., 2002] will achieve logarithmic (problem dependent) regret with some dependence on the gaps \u22061,k = 1 \u03bd1 \u2212 1 \u03bdk .", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "The second component involves analysing the selection of the overflow process, where the approach is reminiscent of the analysis for the UCB algorithm for stochastic bandits [Auer et al., 2002].", "startOffset": 174, "endOffset": 193}, {"referenceID": 1, "context": "For the second term we need the following lemma, which uses Theorem 5 and a reasoning analogues to that of Auer et al. [2002] to bound the regret of the UCB algorithm for stochastic bandits: Lemma 10.", "startOffset": 107, "endOffset": 126}, {"referenceID": 1, "context": "8 MINIMAX LOWER BOUNDS Despite the continuous action space, the techniques used when proving minimax lower bounds for standard stochastic bandits [Auer et al., 1995] can be adapted to our setting.", "startOffset": 146, "endOffset": 165}], "year": 2014, "abstractText": "We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden.", "creator": "Creator"}}}