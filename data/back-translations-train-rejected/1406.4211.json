{"id": "1406.4211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2014", "title": "Mapping the Economic Crisis: Some Preliminary Investigations", "abstract": "In this paper we describe our contribution to the PoliInformatics 2014 Challenge on the 2007-2008 financial crisis. We propose a state of the art technique to extract information from texts and provide different representations, giving first a static overview of the domain and then a dynamic representation of its main evolutions. We show that this strategy provides a practical solution to some recent theories in social sciences that are facing a lack of methods and tools to automatically extract information from natural language texts.", "histories": [["v1", "Tue, 17 Jun 2014 01:34:22 GMT  (1834kb,D)", "http://arxiv.org/abs/1406.4211v1", "Technical paper describing the Lattice submission to the 2014 PoliInformatics Unshared task"]], "COMMENTS": "Technical paper describing the Lattice submission to the 2014 PoliInformatics Unshared task", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pierre bourreau", "thierry poibeau"], "accepted": false, "id": "1406.4211"}, "pdf": {"name": "1406.4211.pdf", "metadata": {"source": "CRF", "title": "Mapping the Economic Crisis: Some Preliminary Investigations", "authors": ["Pierre Bourreau", "Maurice Arnoux", "Thierry Poibeau"], "emails": ["pierre.bourreau@gmail.com", "thierry.poibeau@ens.fr"], "sections": [{"heading": "1 Introduction", "text": "The organizers of this challenge had provided a series of documents on the 2007-2008 financial crisis, and the common task was to come up with solutions to questions such as \"Who was the financial crisis?\" or \"What was the financial crisis?.\" Of course, these questions are too complex to have a simple and direct answer. Consequently, our strategy has been to provide tools to process and visualize the most relevant data so that experts can easily navigate this flow of information and use the data wisely. While we believe in semi-automated research of the corpus, we do not believe it is possible or even desirable to provide fully automated answers to the above questions. We have mainly used available tools to extract and visualize information. More specifically, we have used the Stanford Named Entity Recognizer (Finkel et al., 2005) and the Cortext Platform (http: / / www.cortext.net /) for data visualization."}, {"heading": "2 Corpora Selected for the Analysis", "text": "For this reason, we worked on three different files: the Congressional Reports \"Wall Street and the Financial Crisis: Anatomy of a Financial Collapse\" (referred to as AoC in the rest of the document), \"The Stock Market Plunge: What Happened and What is Next?\" (hereinafter referred to as SMP), and the \"Financial Crisis Inquiry Report\" (hereinafter referred to as FCIC), each of which was available as a PDF or alternatively as a list of HTML pages, with each page corresponding to a page of the PDF. The first task we performed was to convert the HTML files into a single text document (thanks to a Python script). In the case of AoC, an option was added to remove page numbers that were present in the content of the HTML file."}, {"heading": "3 Named Entity Recognition and Normalization", "text": "The next steps in this direction are expected shortly: the EU Commission, the EU Commission, the EU Commission, the European Central Bank and the European Central Bank (ECB)."}, {"heading": "4 Vizualizing entities", "text": "We used the Gephi software (Bastian et al., 2009) to create diagrams for each corpus so that: \u2022 a node corresponds to a cluster of persons or organizations in the corresponding corpus (each node is labeled with the most common unit in the corresponding cluster); \u2022 an edge between two nodes corresponds to the number of simultaneous occurrences of the two nodes within the same set in the corpus. We chose to look at individuals and organizations together as they can play a similar role in the event, and metonymy is often used so that a person can point to an enterprise (and vice versa); we then used the Force Atlas algorithm so that only pairs of interconnected nodes attract each other. We assign a measure between the centers (BC) to each node that makes the highest-degree nodes (i.e. nodes with the highest number of connections)."}, {"heading": "5 Visualizing temporal evolution", "text": "We have tried to explore and visualize the temporal evolution of the financial crisis, or more precisely, the evolution of the perceived role of organizations over time. To this end, we produced the charts of the correlation of organizations and domain-related terms in the corpus. With this strategy, we take into account the temporal evolution of companies and actions along the 1DAs. Due to their size and complexity, the numbers presented at the end of this paper are small and not fully legible; however, we hope that they give the reader a taste of the kind of representation that is possible. See http: / www. lattice.fr / PoliInformatics2014 to visualize the original maps are small and not fully legible."}, {"heading": "6 Discussion", "text": "In this paper, we examined the use of natural language processing tools to extract and visualize information about the financial crisis of 2007-2008. First, we presented a static view of the area based on the convergence of designated entities (mainly persons and organizations) within the corpus. Then, we showed how the development of the area can be presented in the light of time. In this second experiment, we focused on the juxtaposition of organizations and domain-specific terms and showed that the terms evolve over time as well as their association with company names, which makes it possible to get an idea of the area without first having to read all the documentation. These representations are inspired by newer social science theories that do not propose to define stakeholders a priori, but to directly consider stakeholders and observe regular patterns or regular behaviors (Law and Hassard, 1999; Latour, 2005)."}, {"heading": "7 Future Work", "text": "The perspectives are twofold: on the one hand, they improve data analysis to provide more relevant maps and representations, and on the other hand, they work closely with domain experts and provide interactive ways to navigate the data. On the technical side, several improvements could be achieved. For example, it would be interesting to complete the extraction of technical terms with relevant verbs in order to obtain more relevant connections between entities. The aim would be to better characterize the role of the different entities. In addition, data taken into account in the temporal analysis could be scaled within months or days. In the same vein, it might be interesting to test normalization strategies for different named entities and to test our methods on more texts from different sources. In terms of interactions with experts, it is clear that end users could make a very valuable contribution in the selection of relevant data, as well as in the way they are linked and mapped. Some experiments are currently being conducted with a focus group that brings together social scientists and information scientists."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Elisa Omodei for her help in using the Cortext platform. We would also like to thank the two reviewers of the PoliInformatics Challenge for their useful feedback and suggestions on the work they have done. We are also grateful to the experts of our focus group, which was partly funded by the CNRS Mastodon's Aresos project."}], "references": [{"title": "Gephi: an open source software for exploring and manipulating networks", "author": ["Sebastien Heymann", "Mathieu Jacomy"], "venue": "In International AAAI Conference on Weblogs and Social Media (ICWSM),", "citeRegEx": "Bastian et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bastian et al\\.", "year": 2009}, {"title": "Fast unfolding of communities in large networks", "author": ["Jean-Loup Guillaume", "Renaud Lambiotte", "Etienne Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "Blondel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2008}, {"title": "Incorporating non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of the conference of the Association", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Linking entities to a knowledge base with query expansion", "author": ["Gottipati", "Jiang2011] Swapna Gottipati", "Jing Jiang"], "venue": "In Proc. of Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Gottipati et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gottipati et al\\.", "year": 2011}, {"title": "A generative entity-mention model for linking entities", "author": ["Han", "Sun2011] Xianpei Han", "Le Sun"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Han et al\\.", "year": 2011}, {"title": "Reassembling the Social: An Introduction to Actor-Network-Theory", "author": ["Bruno Latour"], "venue": null, "citeRegEx": "Latour.,? \\Q2005\\E", "shortCiteRegEx": "Latour.", "year": 2005}, {"title": "Entity linking: Finding extracted entities in a knowledge base", "author": ["Rao et al.2011] Delip Rao", "Paul McNamee", "Mark Dredze"], "venue": "In Multi-source, Multilingual Information Extraction and Summarization,", "citeRegEx": "Rao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2011}, {"title": "Once upon a text : an ant tale in text analysis", "author": ["Venturini", "Guido2012] Tommaso Venturini", "Daniele Guido"], "venue": "Sociologica (online: http://www.sociologica.mulino.it/journal/ issue/index/Issue/Journal:ISSUE:18),", "citeRegEx": "Venturini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Venturini et al\\.", "year": 2012}, {"title": "Entity linking with effective acronym expansion instance selection and topic modeling", "author": ["Zhang et al.2011] Wei Zhang", "Yan Chuan Sim", "Jian Su", "Chew Lim Tan"], "venue": "In Proc. of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "More precisely, we have used the Stanford Named Entity Recognizer (Finkel et al., 2005) and the Cortext platform (http://www.", "startOffset": 66, "endOffset": 87}, {"referenceID": 0, "context": "As for data visualization, we have used Gephi (Bastian et al., 2009) to observe semantic and social networks, and the Cortext platform to observe the evolution of the domain over time.", "startOffset": 46, "endOffset": 68}, {"referenceID": 2, "context": "In order to do so, we used the Stanford NER, based on Conditional Random Fields, with MUC tags (Time, Location, Organization, Person, Money, Percent, Date) (Finkel et al., 2005).", "startOffset": 156, "endOffset": 177}, {"referenceID": 0, "context": "We used the Gephi software (Bastian et al., 2009) so as to create graphs for each corpus, such that:", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "nodes with the highest number of links) be bigger; finally, we used a clustering algorithm: the Louvain Method (Blondel et al., 2008), to detect communities in the graph, and we colored the nodes of each community with a distinct", "startOffset": 111, "endOffset": 133}, {"referenceID": 5, "context": "fine a priori groups of interest but to directly take into account actors and observe regular patterns or regular behaviors (Law and Hassard, 1999; Latour, 2005).", "startOffset": 124, "endOffset": 161}], "year": 2014, "abstractText": "In this paper we describe our contribution to the PoliInformatics 2014 Challenge on the 2007-2008 financial crisis. We propose a state of the art technique to extract information from texts and provide different representations, giving first a static overview of the domain and then a dynamic representation of its main evolutions. We show that this strategy provides a practical solution to some recent theories in social sciences that are facing a lack of methods and tools to automatically extract information from natural language texts.", "creator": "LaTeX with hyperref package"}}}