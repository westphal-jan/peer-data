{"id": "1406.4296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2014", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams", "abstract": "Learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. This is impractical when dealing with many different sources (e.g., in camera networks), or constantly changing ones such as mobile cameras (e.g., in robotics or driving assistant systems). In this paper, we address the problem of self-learning detectors in an autonomous manner, i.e. (i) detectors continuously updating themselves to efficiently adapt to streaming data sources (contrary to transductive algorithms), (ii) without any labeled data strongly related to the target data stream (contrary to self-paced learning), and (iii) without manual intervention to set and update hyper-parameters. To that end, we propose an unsupervised, on-line, and self-tuning learning algorithm to optimize a multi-task learning convex objective. Our method uses confident but laconic oracles (high-precision but low-recall off-the-shelf generic detectors), and exploits the structure of the problem to jointly learn on-line an ensemble of instance-level trackers, from which we derive an adapted category-level object detector. Our approach is validated on real-world publicly available video object datasets.", "histories": [["v1", "Tue, 17 Jun 2014 09:51:18 GMT  (1261kb,D)", "https://arxiv.org/abs/1406.4296v1", null], ["v2", "Wed, 18 Jun 2014 12:33:22 GMT  (1261kb,D)", "http://arxiv.org/abs/1406.4296v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["adrien gaidon", "gloria zen", "jose a rodriguez-serrano"], "accepted": false, "id": "1406.4296"}, "pdf": {"name": "1406.4296.pdf", "metadata": {"source": "CRF", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams", "authors": ["Adrien Gaidon", "Gloria Zen", "Jose A. Rodriguez-Serrano"], "emails": ["adrien.gaidon@xrce.xerox.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to go in search of a solution that puts us in the position in which we are. (...) In fact, it is the case that we are able to find a solution. (...) In fact, it is the case that we are able to find a solution. (...) It is the case that we are able to find a solution. (...) It is the case that we are able to find a solution. (...) It is as if we are able to find a solution. (...) It is as if we are able to find a solution. (...) \"(...)\" (... \"(...)\" (...) \"(...)\" (... \"(...)\" (...). (...) \"(...\" (...) (...). (...) (...). (...) (...). (...). (...). (...). () (...). (...). \"(). (...). (). (). (...). (). (). (). (). (...). (). (). (). (). (...). (). (). ().). (). (). ().). (). ().). (). ().). ().). (). ().). (). ().). ().). ().). (). (). ().).). (). (). (). ().). ().).). ().). (). (). ().).). (). (). ().). (). (). (). ().).). (). (). ().).). ().). (). (). (). ().). ().). ().).). (). (). ().).). (). ().). ().). (). (). ().).). ().).).). (). ()"}, {"heading": "2 Related work", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3 Multi-task learning of an ensemble of trackers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tracking-by-detection", "text": "The term \"object detector\" refers to a binary classifier represented by a (learned) vector w & # 246; re. & # 8222; This classifier calculates the probability that a window - a subregion x - is represented by a characteristic vector. & # 8220; The first step of our method is to call a safe but laconic oracle that will automatically label a few (or none) samples (image regions with object labels in our case). & # 8222; We assume that the automatic laboratories of these seeds are in order. & # 8220; The first step of our method is to return a few (or none) positive samples. & # 8220; & # 8220; & # 8220; & # 8220;"}, {"heading": "3.2 The Ensemble of Instance Trackers (EIT) model", "text": "n: \"It is not the first time that we have faced the question of the extent to which we are able to change the world.\" (\"It is not the first time that we are able to reform the world.\") (\"It is the second time that we are able to reform the world.\") (\"It is the second time that we are able to understand the world.\") (\"It is the second time that we are able to understand the world.\") (\"It is the second time that we are able to understand the world.\") (\"It is the second time that we are able to understand the world.\") (\"It is the second time that we are able to understand the world.\" (\"It is the first time, the second time we are able to understand the world.\") (\"It is the second time that we see the world in the world.\") (\"It is the second time that we.\" It is the third time that we see the world in the world. \"(\" It is the first time, the second time we will see the second time. \"It is the second time.) It is the second time that we are able to change the world in the world."}, {"heading": "4 Continuous self-tuning on-line adaptation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Streaming asynchronous optimization", "text": "We solve the optimization problem in equivalents. (4) By using the average stochastic gradient Descent (ASGD), also known as the Polyak-Rupert mean. [16] This method of first order achieves an optimal convergence rate with only one pass over the data and a constant learning rate with a logistic regression model [13]. The update rule for each model wi is: w (t + 1, k) i = w (t + 1, k \u2212 1) i \u2212 \u03b7 (x (i) x (i) k, y (i) k, w (t) k (t + 1, k \u2212 1) i) + \u03bbNt + 1 (wi \u2212 w (t))), (7), showing the learning rate, (x (i) k (i) k (i) k) a training sample, k = 1, \u00b7, ni, and w (t + 1.0) i (t) the current course of the model."}, {"heading": "4.2 Self-tuning of hyper-parameters", "text": "A key issue in our unguarded online learning environment is how to select the values of the various detection strategies used. (4) This is especially important as we need to continuously update the appearance models in a streaming mode where the data is likely to be non-i.d., and the video stream non-stationary (e.g., in long-term surveillance scenarios). The learning rate, in particular, is a critical parameter of ASGD, and should be on a non-stationary case.Cross-validation, typically used to tune hyper-parameters, is not applicable in our setting as we only have one positive example at a time."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "To validate our approach, we use publicly available benchmarks: five video scenes from CAVIAR1 and a long HD video sequence from VIRAT2. Objects of interest in CAVIAR are persons, while VIRAT objects of interest are cars. Sample images from the two scenarios are shown in Figure 3. Some data set details are shown in Table 1. On CAVIAR, we use the same sequences as Wang et al. [18] - Ols1, Ols2, Olsr1, Olsr2 and Ose - and also the image size by a factor of 2.0. We conduct independent experiments on each of the five sequences. Each sequence is used for both unattended learning and evaluation (i.e., the same setting as in [18]). In our case, this corresponds to a scenario in which the object detector is continuously adjusted along a finite video stream and later applied to the totality of that video."}, {"heading": "5.2 Implementation details", "text": "Generic DetectorororacleLike in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector. This generic detector is pre-trained on Pascal VOC 2007, and is publicly available online3. The performance achieved by DPM is reported in Table 2. To use DPM as a safe but laconic oracle, we use only its top 5% detection methods.Tracking-by-detection object detectorOur formulation is applicable to any detector based on a linear classifier, and is therefore usable with most state-of-the-art object detection and window display methods [3, 17, 19]. In our experiments we use Fisher Vectors (FV) [20], as this representation is particularly suitable for our problem for the following reasons: firstly, the FV is among the most advanced representations for object detection [17] decoding and window display methods. Secondly, we use Fisher Vectors (FV) [20] in our experiments, because this representation is particularly suitable for our problem: First, the FV is one of the most modern representations for object detection and window display methods."}, {"heading": "5.3 Results", "text": "This year, it has come to the point where it only takes one year to get to the next round."}], "references": [{"title": "Self-paced learning for latent variable models", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010. 2, 3", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005. 2, 3", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "Dand Ramanan"], "venue": "PAMI, 2010. 2, 3, 7, 8", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning on the test data: Leveraging unseen features", "author": ["B. Taskar", "M.-F.Wong", "D. Koller"], "venue": "ICML, 2003. 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Reshaping visual datasets for domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "NIPS, 2013. 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "ICML, 1999. 2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning object class detectors from weakly annotated video", "author": ["A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari"], "venue": "CVPR, 2012. 3", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Shifting weights: Adapting object detectors from image to video", "author": ["K. Tang", "V. Ramanathan", "L. Fei-Fei", "D. Koller"], "venue": "NIPS, 2012. 3, 8", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised incremental learning for improved object detection in a video", "author": ["P. Sharma", "C. Huang", "R. Nevatia"], "venue": "CVPR, 2012. 3, 8", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Tracking-learning-detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "PAMI, 2012. 3", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust visual tracking via structured multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "IJCV, 2013. 3", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning by borrowing examples for multiclass object detection", "author": ["J.J. Lim", "R. Salakhutdinov", "A. Torralba"], "venue": "NIPS, 2011. 3", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O ( 1 / n )", "author": ["F. Bach", "E. Moulines"], "venue": "NIPS, 2013. 4, 5", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "SIGKDD, 2004. 4", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Ensemble of Exemplar-SVMs for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A. Efros"], "venue": "ICCV, 2011. 5", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B. Polyak", "A. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 1992. 5", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Segmentation driven object detection with Fisher vectors", "author": ["R. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "ICCV, 2013. 6, 7, 8", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection by detections: Non-parametric detector adaptation for a video", "author": ["X. Wang", "G. Hua", "T. Han"], "venue": "CVPR, 2012. 6, 7, 8", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014. 7", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "CVPR, 2007. 7", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Image Classification with the Fisher Vector: Theory and Practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "IJCV, 2013. 7", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "PAMI, 2012. 7", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient Action Localization with Approximately Normalized Fisher Vectors", "author": ["D. Oneata", "J. Verbeek", "C. Schmid"], "venue": "CVPR, 2014. 8", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 2010. 8 9", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1]), and we are not limited to the stationary and transductive learning settings (see Section 2 for a more detailed discussion).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "First, in Section 3, we propose to use multi-task learning in conjunction with tracking as a way to automatically elicit a useful set of target positives and negatives, especially \u201chard negatives\u201d, which are crucial for high detection performance [2, 3].", "startOffset": 247, "endOffset": 253}, {"referenceID": 2, "context": "First, in Section 3, we propose to use multi-task learning in conjunction with tracking as a way to automatically elicit a useful set of target positives and negatives, especially \u201chard negatives\u201d, which are crucial for high detection performance [2, 3].", "startOffset": 247, "endOffset": 253}, {"referenceID": 3, "context": "[4] leverage \u201cunseen\u201d features from instances classified with high confidence, learning which features are useful for classification, and Gong et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] \u201creshape\u201d datasets to minimize their distribution mismatch.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "These methods assume that the target domain is stationary, with many unlabeled target samples readily available at each update (transductive setting [6]).", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "[7] learn object detectors by relying on joint motion segmentation of a set of videos containing the object of interest moving differently from the background.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] are inspired by the self-paced learning approach of Kumar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] iteratively re-weights labeled source samples by using the tracks of the closest target samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] propose a similarly inspired multiple instance learning algorithm, also relying on self-paced learning and off-line iterative re-training of a generic detector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and the multi-task learning approach of Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "\u201cdifficult\u201d target samples, also called hard negatives [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "[8]), they are key to the successful update of the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] for instance).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "(1), and enjoys useful theoretical properties for on-line optimization [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "Note that this formulation is closely related to the mean-regularized multi-task learning formulation of Evgeniou and Pontil [14], with the difference that it is designed for on-line learning in streaming scenarios.", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "(4) using Averaged Stochastic Gradient Descent (ASGD), also called Polyak-Rupert averaging [16].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "This on-line first order method achieves optimal convergence rate with only one pass over the data and a constant learning rate with a logistic regression model [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Furthermore, optimizing the window classification performance is not guaranteed to result in optimal detection performance due to the additional post-processing steps applied in most detection methods [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "[18] \u2014 Ols1, Ols2, Osow1, Olsr2, and Ose \u2014 and also upscale the image size by a factor of 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "the same setting as in [18]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18], a state-of-the-art unsupervised video adaptation method also relying only on a pre-trained generic detector, and assuming the most confident detections are true positives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose a \u201clazy learning\u201d method consisting in re-ranking the low scoring detections using the similarity to high scoring ones.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Like in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "Like in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 16, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 18, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 19, "context": "In our experiments, we use Fisher Vectors (FV) [20], as this representation is particularly suited to our problem for the following reasons.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "First, FV are among state-of-the-art representations for object detection [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Second, they proved to be efficient for both category-level image classification [21] and instance-level retrieval problems [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Second, they proved to be efficient for both category-level image classification [21] and instance-level retrieval problems [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "[17], which yields competitive detection results on Pascal VOC, with the difference that we use a more efficient but approximate sliding window approach similar to the one described in Oneata et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Ols1 Ols2 Olsr2 Osow1 Ose2 VIRAT-0401 DPM [3] 30.", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "0 DbD [18] 32.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "\u201cDPM\u201d is the pre-trained generic detector (DPM [3], using all detections, not just seeds).", "startOffset": 47, "endOffset": 50}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "It is the standard metric used to measure object detection performance [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "[18]4; (iii) our EIT algorithm without multi-task regularization, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", [8, 9]), as our contribution is on autonomous on-line self-adaptation, i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [8, 9]), as our contribution is on autonomous on-line self-adaptation, i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": "we do not assume access to a large fixed labeled dataset related to the target videos, or that all target video frames are available at once (in contrast to [8, 9]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 8, "context": "we do not assume access to a large fixed labeled dataset related to the target videos, or that all target video frames are available at once (in contrast to [8, 9]).", "startOffset": 157, "endOffset": 163}], "year": 2014, "abstractText": "Learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. This is impractical when dealing with many different sources (e.g., in camera networks), or constantly changing ones such as mobile cameras (e.g., in robotics or driving assistant systems). In this paper, we address the problem of self-learning detectors in an autonomous manner, i.e. (i) detectors continuously updating themselves to efficiently adapt to streaming data sources (contrary to transductive algorithms), (ii) without any labeled data strongly related to the target data stream (contrary to self-paced learning), and (iii) without manual intervention to set and update hyper-parameters. To that end, we propose an unsupervised, on-line, and self-tuning learning algorithm to optimize a multi-task learning convex objective. Our method uses confident but laconic oracles (high-precision but low-recall off-the-shelf generic detectors), and exploits the structure of the problem to jointly learn on-line an ensemble of instance-level trackers, from which we derive an adapted category-level object detector. Our approach is validated on real-world publicly available video object datasets.", "creator": "LaTeX with hyperref package"}}}