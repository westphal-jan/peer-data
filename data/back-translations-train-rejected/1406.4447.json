{"id": "1406.4447", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2014", "title": "Automatic Fado Music Classification", "abstract": "In late 2011, Fado was elevated to the oral and intangible heritage of humanity by UNESCO. This study aims to develop a tool for automatic detection of Fado music based on the audio signal. To do this, frequency spectrum-related characteristics were captured form the audio signal: in addition to the Mel Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the signal was further analysed in two frequency ranges, providing additional information. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy). The good results reflect the fact that Fado is a very distinctive musical style.", "histories": [["v1", "Tue, 17 Jun 2014 17:44:45 GMT  (183kb,D)", "http://arxiv.org/abs/1406.4447v1", "4 pages, 1 figure, 5 tables"]], "COMMENTS": "4 pages, 1 figure, 5 tables", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["pedro gir\\~ao antunes", "david martins de matos", "ricardo ribeiro", "isabel trancoso"], "accepted": false, "id": "1406.4447"}, "pdf": {"name": "1406.4447.pdf", "metadata": {"source": "CRF", "title": "Automatic Fado Music Classification", "authors": ["Pedro Gir\u00e3o Antunes", "David Martins de Matos", "Ricardo Ribeiro", "Isabel Trancoso"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTIONFado music was born in the popular 19th century context of Lisbon, Portugal. It encompasses both music and poetry and was originally introduced among the poorest social groups in the city and was often performed on the streets by amateurs (Figure 1). Later, it became a real profession, and fado performers - fadistas - are now found in specialized casas de fado (\"houses of fado\"), typical Portuguese places dedicated to fado. Fado songs are typically performed by a solo singer, male or female, traditionally accompanied by a classical guitar and the Portuguese guitar - a pear-shaped lemon with twelve wire strings, which is unique to Portugal. In recent decades, the instrumental accompaniment has been expanded to include two Portuguese guitars, a classical guitar and an acoustic bass guitar. The limited instrumental equipment helps to better define fado from a signal analysis perspective."}, {"heading": "II. RELATED WORK", "text": "The tags of music genres are often generated by individual users, which is the cheapest way to do this, but can lead to contradictions, as people do not have exactly the same perception of a particular genre. Songs can be marked manually by experts, but this is an expensive task, especially for large music collections. Finally, automatic genre marking from an acoustic analysis makes the most of the above methods, since the marking is done in a consistent manner, but it is cheaper to run on large music collections. The problem with this approach is how well a machine can be set to determine the genre of a track. McKay and Fujinaga [9] introduce arguments for and against automatic genre classification. In the next section, some of the most important works in this area are presented."}, {"heading": "A. Automatic Audio-based Music Genre Classification", "text": "In their work, they presented a dataset called GTZAN, which was also used by Li [8] and Panagakis [11] to represent the timbral content, rhythmic content, and statistical recognition."}, {"heading": "B. Evaluation Concerns", "text": "Terms [16] indicate that final accuracy varies inversely with the number of genres used, depending on the regression method used. The data presented in Table I correspond to Equation 1. This fact raises the question of the validity of the classification of music genres, i.e., how is genre marking defined in terms of individual perception? For some users, a song is considered pop, while for others it may be rock, pop-rock or some other type of similar keyword. This is the cause of the inverse relationship between accuracy and the number of genres considered. Ultimately, it would be the same for any manual human marking [13]. Of course, there are also some arguments about what is and what is not music. However, due to the traditional origin and the limited musical instrument in the specific genre, fado music is much better defined than, for example, rock classification [3]."}, {"heading": "III. FADO CLASSIFIER", "text": "Our Fado classifier was implemented with two main tools: MIRtoolbox [7] for Matlaba and libsvm [5], the first was used to extract the audio features and the second to classify songs. Before introducing the audio features, it should be noted that the audio files used were songs in WAV format that were sampled down to 22050Hz, normalized according to the RMS, and that each feature was calculated for a 10-second snippet of each song. The snippet size is discussed in the next section."}, {"heading": "A. Audio Features", "text": "We extracted three characteristics: one that relates to the rhythmic information, one that relates to the timbre, and another that relates to the musical dynamics; the rhythmic characteristic was based on the work of Mitri, Uitdenbogerd, and Ciesielski [10]. This characteristic is composed of two 9-dimensional vectors: one that relates to low frequencies, and another that relates to high frequencies. The first characteristic is calculated on the basis of the FFT coefficients on the frequency range from 20 Hz to 100 Hz, the other is on 8000Hz to 11025Hz. Each set of FFT coefficients was extracted from 50-ms frames that half overlap. Considering this, each component of the 9-dimensional vector is presented in Table II.The purpose of this characteristic is to capture the rhythmic information that is present in both, the low frequencies, and the high frequencies, and the fact that in general, the Fado is a lot of information."}, {"heading": "B. Support Vector Machines", "text": "SVMs (Support Vector Machines) are a useful technique for data classification that is widely used in the bibliography [8], [2], [3]. In essence, SVMs aim to search for a hyperplane that separates the positive data points and the negative data points with maximum marginality. SVMs try to map the original training data into a higher (perhaps infinite) dimensional space by using a function \u03c6. To do this, SVMs create a linear separating hyperplane with the maximum margin in this higher dimensional space. From a mathematical point of view, a set of instance label pairs (xi, yi), i = 1... l in which xi, Rn and y, 1l, SVMs seek the solution of the following optimization problem: minw, b, \u042012 wTw + C, i = 1l\u043fito subject: yi (wT\u03c6) + b, in which xi (xi) + b, where xi, www and y, 1l, SVMs seek the solution of the following optimization problem: SVi \u043fi) + b minym + b, in which xi wtw, 1xi), SVM (SVM + wxi) are displayed as a higher dimension."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we present the experiments performed and the data set used. One of the questions raised was: \"How long would the sample have to be to characterize the music genre?\" Generally, a value of 10 to 30 seconds is used. In practice, we observed that the average person could identify the music genre in less than 10 seconds in most cases, so we used 10-second music samples."}, {"heading": "A. Dataset", "text": "The goal is to classify fado music specifically, then to use a simple binary classifier, one class being fado and the other any other genre of music (i.e. not fado), but it is virtually impossible to classify each genre and its subgenres into a series of songs, even the fado itself can be divided into several different subgenres [6]. In this case, it makes sense to use the largest number of different other genres as a non-fado song: 250 fado songs and 250 non-fado songs.When we look at 10-second samples, a question arises: where should we take it when we use four hypotheses: from the beginning of the track to the end, from the middle to the end of the sequence."}, {"heading": "V. RESULTS AND DISCUSSION", "text": "The results show that the maximum RMS value and the beginning of the song are the best samples to distinguish fado from other music genres by taking advantage of these characteristics. This suggests that fado actually has a characteristic beginning and that it is relatively more uniform than other music genres. However, the middle sample also gets a high result. To analyze song-specific results, the data set was randomly divided into two sets, with a traditional move / test setup implemented: about 2 / 3 for training (334 songs) and 1 / 3 for testing (166 songs). Training was performed using the parameters C (equivalent 2) and 3 (equivalent 3), estimated using a grid algorithm from libsvm [5]. The results are shown in Table IV.4, where the false negatives for the maximum RMS sample music that received the best score are wrong vocals."}, {"heading": "VI. CONCLUSION", "text": "We introduced an audio-based Fado classifier using an SVM classifier [5] and the results are comparable to those in [8], [3], although an exact comparison is quite difficult due to the different data sets used and different characteristics and genres of music. However, since the accuracy results are over 90% (in general) and over 95% (in our best constellations, even taking cross-validation into account), we think that they can be considered state of the art for this type of task. We introduced a comparative study of sample collection and a detailed analysis of false negatives and false positives. This work can be considered as a first step towards Fado classification, one possible direction would be the classification of various subgenres of Fado music that would find application in musicological studies."}, {"heading": "Acknowledgments", "text": "A special thanks goes to Daniel Gouveia, who provided his text about Fado music and a large collection of Fado soundtracks."}], "references": [{"title": "Music Genre Classification using the multivariate AR feature integration model. MIREX genre classification contest", "author": ["Peter Ahrendt", "Anders Meng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Audio feature engineering for automatic music genre classification", "author": ["Paolo Annesi", "Roberto Basili", "Raffaele Gitto"], "venue": "Conference RIAO2007, Pittsburgh PA, U.S.A.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Music genre classification using explicit semantic analysis. Proceedings of the 1st international ACM workshop on Music information retrieval with user-centered and multimodal strategies - MIRUM", "author": ["Kamelia Aryafar", "Ali Shokoufandeh"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Aggregate features and ADABOOST for music classification", "author": ["James Bergstra", "Norman Casagrande", "Dumitru Erhan", "Douglas Eck", "Bal\u00e1zs K\u00e9gl"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "MIRtoolbox 1.4 User\u2019s Manual", "author": ["Olivier Lartillot"], "venue": "Finnish Centre of Excelence in Interdisciplinary Music Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A comparative study on contentbased music genre classification", "author": ["Tao Li", "Mitsunori Ogihara", "Qi Li"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR \u201903,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Musical genre classification: Is it worth pursuing and how can it be improved", "author": ["Cory Mckay", "Ichiro Fujinaga"], "venue": "Proc. of the 7th Int. Conf. on Music Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Automatic music classification problems", "author": ["G Mitri", "AL Uitdenbogerd", "V Ciesielski"], "venue": "Proceedings of the 27th,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Music genre classification via sparse representations of auditory temporal modulations", "author": ["Yannis Panagakis", "Constantine Kotropoulos", "GR Arce"], "venue": "Proc. XVII European Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Musical Genre Classification Using Melody Features Extracted from Polyphonic Music Signals", "author": ["J Salamon", "B Rocha", "E G\u00f3mez"], "venue": "mtg.upf.es,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Automatic genre classification of music content: a survey", "author": ["Nicolas Scaringella", "Giorgio Zoia", "Daniel Mlynek"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "A machine learning approach to automatic music genre classification", "author": ["Carlos N. Silla Jr.", "Alessandro L. Koerich", "Celso a. a. Kaestner"], "venue": "Journal of the Brazilian Computer Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "On Automatic Music Genre Recognition by Sparse Representation Classification using Auditory Temporal Modulations", "author": ["Bob L Sturm", "Pardis Noorzad"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Audio content processing for automatic music genre classification: descriptors, databases, and classifiers", "author": ["EG Termens"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "McKay and Fujinaga [9] introduce arguments for and against automatic genre classification.", "startOffset": 19, "endOffset": 22}, {"referenceID": 15, "context": "Tzanetakis and Cook [17] were among the first to approach the problem of automatic classification of musical genre based on the audio signal.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "In their work they introduced a dataset called GTZAN also used by Li [8] and Panagakis [11].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "In their work they introduced a dataset called GTZAN also used by Li [8] and Panagakis [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "[8] introduced a new feature extraction method for music genre classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] used a multivariate autoregressive model of the first 6 MFCCs and a generalized linear model classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] used a variety of timbral related features, including FFT coefficients, MFCCs, zero-crossing rate, among others.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], in addition to the timbral, rhythmic, and pitch features, introduced a new one, which they called Volume Reverse.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] introduced musical genre classification using multiple feature vectors, from the beginning, middle, and", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However their results were recently contested by Sturm and Noorzad [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "[12] presented a classifier based on highlevel melodic features that are extracted directly from the audio signal of polyphonic music.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Aryafar and Shokoufandeh [3] used Explicit Semantic Analysis of textual documents to represent audio samples to feed an support vector machine (SVM) and a k-nearest neighbor (kNN) clustering classifier.", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 185, "endOffset": 188}, {"referenceID": 12, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 229, "endOffset": 233}, {"referenceID": 9, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 311, "endOffset": 315}, {"referenceID": 2, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 331, "endOffset": 334}, {"referenceID": 14, "context": "Termens [16] notes that the final accuracy varies inversely with the number of genres used, according to the equation:", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Eventually, it would be the same for any manual human tagging [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "Two works present genre classification in a one against all setup [8], [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Two works present genre classification in a one against all setup [8], [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Our Fado classifier was implemented using two main tools: MIRtoolbox [7] for Matlab\u00e5nd libsvm [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Our Fado classifier was implemented using two main tools: MIRtoolbox [7] for Matlab\u00e5nd libsvm [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "The rhythmic feature was based on the work of Mitri, Uitdenbogerd and Ciesielski [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The implementation of SVM used was the libsvm, which is an integrated software for support vector classification, regression and distribution estimation [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We introduced an audio-based Fado classifier using an SVM classifier [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "The results are comparable to those presented in [8], [3], although it is quite difficult to make an accurate comparison, because of the different datasets used, and different features and musical genres in question.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "The results are comparable to those presented in [8], [3], although it is quite difficult to make an accurate comparison, because of the different datasets used, and different features and musical genres in question.", "startOffset": 54, "endOffset": 57}], "year": 2014, "abstractText": "In late 2011, Fado was elevated to the oral and intangible heritage of humanity by UNESCO. This study aims to develop a tool for automatic detection of Fado music based on the audio signal. To do this, frequency spectrum-related characteristics were captured form the audio signal: in addition to the Mel Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the signal was further analysed in two frequency ranges, providing additional information. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy). The good results reflect the fact that Fado is a very distinctive musical style.", "creator": "LaTeX with hyperref package"}}}