{"id": "1406.4619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "A Generalized Markov-Chain Modelling Approach to $(1,\\lambda)$-ES Linear Optimization: Technical Report", "abstract": "Several recent publications investigated Markov-chain modelling of linear optimization by a $(1,\\lambda)$-ES, considering both unconstrained and linearly constrained optimization, and both constant and varying step size. All of them assume normality of the involved random steps, and while this is consistent with a black-box scenario, information on the function to be optimized (e.g. separability) may be exploited by the use of another distribution. The objective of our contribution is to complement previous studies realized with normal steps, and to give sufficient conditions on the distribution of the random steps for the success of a constant step-size $(1,\\lambda)$-ES on the simple problem of a linear function with a linear constraint. The decomposition of a multidimensional distribution into its marginals and the copula combining them is applied to the new distributional assumptions, particular attention being paid to distributions with Archimedean copulas.", "histories": [["v1", "Wed, 18 Jun 2014 06:58:38 GMT  (124kb)", "http://arxiv.org/abs/1406.4619v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG cs.NE", "authors": ["alexandre chotard", "martin holena"], "accepted": false, "id": "1406.4619"}, "pdf": {"name": "1406.4619.pdf", "metadata": {"source": "CRF", "title": "A Generalized Markov-Chain Modelling Approach to (1, \u03bb)-ES Linear Optimization: Technical Report", "authors": ["Alexandre Chotard"], "emails": ["alexandre.chotard@lri.fr", "martin@cs.cas.cz"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.46 19v1 [cs.NA] 1 8Ju nKeywords: evolutionary strategies, continuous optimization, linear optimization, linear constraint, linear function, Markov chain models, archimedean copula"}, {"heading": "1 Introduction", "text": "In such a context, it is natural to proceed from the normality of the random distribution steps, since the normal distribution for given means and variance, which means that there are the most general assumptions that can be made without the use of additional information about f. However, such additional information may be available, and then the use of normal steps may not be optimal. Cases in which different distributions have been studied include so-called fast-evolution strategies [1], which determine the separability of f, or severe tail distributions on multimodal problems [4,3]. In several recent publications, attention has been paid to the market chains of linear modeling."}, {"heading": "2 Problem setting and algorithm definition", "text": "In the course of this work, we will examine a (1,) ES optimization of a linear function f: Rn \u2192 R, in which \u03bb 2 and n \u2265 2, with a linear constraint g: Rn \u2192 R, solving the constraint by re-sample until a practicable solution is found. Let us take (ek) k [1.. n] an orthonormal basis of R n. We can assume that the behavior of an ES is immutable to the composition of the objective function by a strictly increasing function (e.g. h: x 7 \u2192 x /... f), and the same applies to our random method of handling, since it only depends on the imbalance g (x) \u2264 0, which is immutable to the composition of g by a homothetic transformation."}, {"heading": "3 Distribution of the feasible and selected steps", "text": "In this section we link the distributions of the random vectors M it and M it with the distribution of the random steps M i, jt and give another way to optimize the distribution of the random steps M i, jt and for the distribution of the samples. Lemma 1. Let a (1, \u03bb) -ES optimize the problem defined in (2) Handling constraints by resampling. Take H the distribution of the random step M i, jt, and for the distribution R + stands L.L: = {x x Rn (x). Provided that H is absolutely continuous and that H (L.M) > 0 is for all cases R +, the distribution H is the feasible step and H the distribution of the selected step, if there is no solution."}, {"heading": "5 Application to More Specific Distributions", "text": "The following problem shows an equivalence between a non-identity-covariance matrix for H and a different standard and limitation of the approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach-approach"}, {"heading": "6 Discussion", "text": "The paper presents a generalization of the recent results of the first author [8] regarding linear optimization by a (1, \u03bb) -ES in the case of constant step size. Generalization consists in replacing the assumption of normality of the random steps involved in the evolutionary strategy with much more general distribution assumptions. This generalization shows that isotropic distributions solve the linear problem. Although the conditions for the ergodicity of the studied Markov chain accept some heavy tail distributions, an exponentially vanishing tail allows geometric ergodicity, which implies a faster convergence to its stationary distribution, and a faster convergence of the Monte Carlo simulations. We believe that these conditions increase insight into the role that different types of distributions play in the evolutionary calculation, and widen the spectrum of possibilities for designing multievolutionary algorithms based on their common steps, while increasing the theoretical steps of applying the copying strategy."}, {"heading": "10. C. A. Coello Coello, \u201cConstraint-handling techniques used with evolutionary al-", "text": "In: Proceedings of the 2008 GECCO conference companion on Genetic and evolutionary computation, GECCO '08, (New York, NY, USA), pp. 2445-2466, ACM, 2008. 11. D. Arnold and D. Brauer, \"On the behavior of the (1 + 1) -ES for a simple confined problem,\" in: Parallel Problem Solving from Nature - PPSN X (I. G. R. et al., ed.), pp. 1-10, Springer, 2008. 12. A. Cuesta-Infante, R. Santana, J. Hidalgo, C. Bielza, and P. Larran \u0439aga, \"Bivariate empirical and n-variate archimedean copulas in estimation of distribution algorithms,\" in: IEEE Congress on Evolutionary Computation, pp."}], "references": [{"title": "Fast evolution strategies,", "author": ["X. Yao", "Y. Liu"], "venue": "Evolutionary Programming VI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Benchmarking Separable Natural Evolution Strategies on the Noiseless and Noisy Black-box Optimization Testbeds,\u201d in Black-box Optimization Benchmarking Workshop, Genetic and Evolutionary Computation", "author": ["T. Schaul"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "High dimensions and heavy tails for natural evolution strategies,", "author": ["T. Schaul", "T. Glasmachers", "J. Schmidhuber"], "venue": "Genetic and Evolutionary Computation Conference (GECCO),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "When do heavy-tail distributions help?,", "author": ["N. Hansen", "F. Gemperle", "A. Auger", "P. Koumoutsakos"], "venue": "P. Runarsson et al., eds.),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "On the behaviour of the (1,\u03bb)-ES for a simple constrained problem,", "author": ["D. Arnold"], "venue": "in Foundations of Genetic Algorithms - FOGA", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the behaviour of the (1, \u03bb)-\u03c3SA-ES for a constrained linear problem,\u201d in Parallel Problem Solving from Nature ", "author": ["D. Arnold"], "venue": "PPSN XII,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Cumulative step-size adaptation on linear functions,\u201d in Parallel Problem Solving from Nature - PPSN XII", "author": ["A. Chotard", "A. Auger", "N. Hansen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Markov chain analysis of evolution strategies on a linear constraint optimization problem,", "author": ["A. Chotard", "A. Auger", "N. Hansen"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Completely derandomized self-adaptation in evolution strategies,", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Constraint-handling techniques used with evolutionary algorithms,", "author": ["C.A. Coello Coello"], "venue": "Proceedings of the 2008 GECCO conference companion on Genetic and evolutionary computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "On the behaviour of the (1 + 1)-ES for a simple constrained problem,\u201d in Parallel Problem Solving from Nature ", "author": ["D. Arnold", "D. Brauer"], "venue": "PPSN X (I. G. R. et al.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Copula estimation of distribution algorithms based on exchangeable archimedean copula,", "author": ["L. Wang", "X. Guo", "J. Zeng", "Y. Hong"], "venue": "International Journal of Computer Applications in Technology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Using copulas in estimation of distribution algorithms,", "author": ["R. Salinas-Gutierrez", "A. Hern\u00e1ndez Aguirre", "E. Villa Diharce"], "venue": "MICAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Markov chains and stochastic stability", "author": ["S.P. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Toward a theory of evolution strategies: Self-adaptation,", "author": ["H.-G. Beyer"], "venue": "Evolutionary Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "Cases where different distributions have been studied include so-called Fast Evolution Strategies [1] or SNES [2,3] which exploits the separability of f , or heavy-tail distributions on multimodal problems [4,3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "Cases where different distributions have been studied include so-called Fast Evolution Strategies [1] or SNES [2,3] which exploits the separability of f , or heavy-tail distributions on multimodal problems [4,3].", "startOffset": 110, "endOffset": 115}, {"referenceID": 2, "context": "Cases where different distributions have been studied include so-called Fast Evolution Strategies [1] or SNES [2,3] which exploits the separability of f , or heavy-tail distributions on multimodal problems [4,3].", "startOffset": 110, "endOffset": 115}, {"referenceID": 3, "context": "Cases where different distributions have been studied include so-called Fast Evolution Strategies [1] or SNES [2,3] which exploits the separability of f , or heavy-tail distributions on multimodal problems [4,3].", "startOffset": 206, "endOffset": 211}, {"referenceID": 2, "context": "Cases where different distributions have been studied include so-called Fast Evolution Strategies [1] or SNES [2,3] which exploits the separability of f , or heavy-tail distributions on multimodal problems [4,3].", "startOffset": 206, "endOffset": 211}, {"referenceID": 4, "context": "In several recent publications [5,6,7,8], attention has been paid to Markovchain modelling of linear optimization by a (1, \u03bb)-ES, i.", "startOffset": 31, "endOffset": 40}, {"referenceID": 5, "context": "In several recent publications [5,6,7,8], attention has been paid to Markovchain modelling of linear optimization by a (1, \u03bb)-ES, i.", "startOffset": 31, "endOffset": 40}, {"referenceID": 6, "context": "In several recent publications [5,6,7,8], attention has been paid to Markovchain modelling of linear optimization by a (1, \u03bb)-ES, i.", "startOffset": 31, "endOffset": 40}, {"referenceID": 7, "context": "In several recent publications [5,6,7,8], attention has been paid to Markovchain modelling of linear optimization by a (1, \u03bb)-ES, i.", "startOffset": 31, "endOffset": 40}, {"referenceID": 6, "context": "This unconstrained case was studied in [7] for normal steps with cumulative step-size adaptation (the step-size adaptation mechanism in CMA-ES [9]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "This unconstrained case was studied in [7] for normal steps with cumulative step-size adaptation (the step-size adaptation mechanism in CMA-ES [9]).", "startOffset": 143, "endOffset": 146}, {"referenceID": 9, "context": "Many techniques to handle constraints in randomised algorithms have been proposed (see [10]).", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We chose this method as it makes the algorithm easier to study, and is consistent with the previous studies assuming normal steps [11,5,6,8], studying constant step-size, self adaptation and cumulative step-size adaptation mechanisms (with fixed covariance matrix).", "startOffset": 130, "endOffset": 140}, {"referenceID": 4, "context": "We chose this method as it makes the algorithm easier to study, and is consistent with the previous studies assuming normal steps [11,5,6,8], studying constant step-size, self adaptation and cumulative step-size adaptation mechanisms (with fixed covariance matrix).", "startOffset": 130, "endOffset": 140}, {"referenceID": 5, "context": "We chose this method as it makes the algorithm easier to study, and is consistent with the previous studies assuming normal steps [11,5,6,8], studying constant step-size, self adaptation and cumulative step-size adaptation mechanisms (with fixed covariance matrix).", "startOffset": 130, "endOffset": 140}, {"referenceID": 7, "context": "We chose this method as it makes the algorithm easier to study, and is consistent with the previous studies assuming normal steps [11,5,6,8], studying constant step-size, self adaptation and cumulative step-size adaptation mechanisms (with fixed covariance matrix).", "startOffset": 130, "endOffset": 140}, {"referenceID": 4, "context": "We want to extend the results obtained in [5,8] using the theory of Markov chains.", "startOffset": 42, "endOffset": 47}, {"referenceID": 7, "context": "We want to extend the results obtained in [5,8] using the theory of Markov chains.", "startOffset": 42, "endOffset": 47}, {"referenceID": 11, "context": "Such distributions have been recently considered in the Estimation of Distribution Algorithms [12,13], continuing the trend of using copulas in that kind of evolutionary optimization algorithms [14].", "startOffset": 94, "endOffset": 101}, {"referenceID": 12, "context": "Such distributions have been recently considered in the Estimation of Distribution Algorithms [12,13], continuing the trend of using copulas in that kind of evolutionary optimization algorithms [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 4, "context": "Following [5,6,11,8] we define \u03b4t as \u03b4t := \u2212 g(Xt) \u03c3t .", "startOffset": 10, "endOffset": 20}, {"referenceID": 5, "context": "Following [5,6,11,8] we define \u03b4t as \u03b4t := \u2212 g(Xt) \u03c3t .", "startOffset": 10, "endOffset": 20}, {"referenceID": 10, "context": "Following [5,6,11,8] we define \u03b4t as \u03b4t := \u2212 g(Xt) \u03c3t .", "startOffset": 10, "endOffset": 20}, {"referenceID": 7, "context": "Following [5,6,11,8] we define \u03b4t as \u03b4t := \u2212 g(Xt) \u03c3t .", "startOffset": 10, "endOffset": 20}, {"referenceID": 0, "context": "n]) \u2208 R+ \u00d7 [0, 1] 7\u2192 Q \uf8eb", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "\u03bb]) \u2208 R+ \u00d7 [0, 1] 7\u2192 argmax G\u2208{G(\u03b4,vi)|i\u2208[1.", "startOffset": 11, "endOffset": 17}, {"referenceID": 13, "context": "We may now use these results to show the divergence of the algorithm when the step-size is constant, using the theory of Markov chains [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 7, "context": "Following the first part of [8], we restrict our attention to the constant step size in the remainder of the paper, that is for all t \u2208 N we take \u03c3t = \u03c3 \u2208 R+.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "We now show ergodicity of the Markov chain (\u03b4t)t\u2208N, which implies that the t-steps transition kernel (the function A 7\u2192 Pr(\u03b4t \u2208 A|\u03b40 = \u03b4) for A \u2208 B(R+)) converges towards a stationary measure \u03c0, generalizing Propositions 3 and 4 of [8].", "startOffset": 232, "endOffset": 235}, {"referenceID": 13, "context": "We want to show a drift condition (see [15]) on V .", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "8 of [15], the Markov chain (\u03b4t)t\u2208N is Harris recurrent.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "1 of [15] the Markov chain (\u03b4t)t\u2208N is positive and is r1-ergodic.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "We now want to show a drift condition (see [15]) on V\u03b1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "1 of [15] which with Theorem 14.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "1 of [15] proves that the Markov chain (\u03b4t)t\u2208N is V\u03b1-geometrically ergodic.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "\u2293\u2294 We now use a law of large numbers ([15] Theorem 17.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "1 of [15], we may apply a law of large numbers on the right hand side of Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "1 of [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": ", copulas defined (\u2200u \u2208 [0, 1]) C\u03c8(u) = \u03c8(\u03c8([u]1) + \u00b7 \u00b7 \u00b7+ \u03c8([u]n)), (20) where \u03c8 : [0,+\u221e] \u2192 [0, 1] is an Archimedean generator, i.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": ", copulas defined (\u2200u \u2208 [0, 1]) C\u03c8(u) = \u03c8(\u03c8([u]1) + \u00b7 \u00b7 \u00b7+ \u03c8([u]n)), (20) where \u03c8 : [0,+\u221e] \u2192 [0, 1] is an Archimedean generator, i.", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": ", \u03c8(0) = 1, \u03c8(+\u221e) = limt\u2192+\u221e \u03c8(t) = 0, \u03c8 is continuous and strictly decreasing on [0, inf{t : \u03c8(t) = 0}), and \u03c8 denotes the generalized inverse of \u03c8, (\u2200u \u2208 [0, 1]) \u03c8(u) = inf{t \u2208 [0,+\u221e] : \u03c8(t) = u}.", "startOffset": 155, "endOffset": 161}, {"referenceID": 0, "context": ", (\u2200u \u2208 [0, 1]) C\u03c8(Qu) = C\u03c8(u).", "startOffset": 8, "endOffset": 14}, {"referenceID": 7, "context": "The paper presents a generalization of recent results of the first author [8] concerning linear optimization by a (1, \u03bb)-ES in the constant step size case.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "small contribution to the research into applicability of copulas in evolutionary computation, complementing the more common application of copulas to the Estimation of Distribution Algorithms [12,14,13].", "startOffset": 192, "endOffset": 202}, {"referenceID": 11, "context": "small contribution to the research into applicability of copulas in evolutionary computation, complementing the more common application of copulas to the Estimation of Distribution Algorithms [12,14,13].", "startOffset": 192, "endOffset": 202}, {"referenceID": 7, "context": "The most important results in [8] actually concern that case.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "A generalization of those results for non-Gaussian distributions of random steps for cumulative step-size adaptation ([9]) is especially difficult as the evolution path is tailored for Gaussian steps, and some careful tweaking would have to be applied.", "startOffset": 118, "endOffset": 121}, {"referenceID": 14, "context": "The \u03c3 self-adaptation evolution strategy ([16]), studied in [6] for the same problem, appears easier, and would be our direction for future research.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "The \u03c3 self-adaptation evolution strategy ([16]), studied in [6] for the same problem, appears easier, and would be our direction for future research.", "startOffset": 60, "endOffset": 63}], "year": 2014, "abstractText": "Several recent publications investigated Markov-chain modelling of linear optimization by a (1, \u03bb)-ES, considering both unconstrained and linearly constrained optimization, and both constant and varying step size. All of them assume normality of the involved random steps, and while this is consistent with a black-box scenario, information on the function to be optimized (e.g. separability) may be exploited by the use of another distribution. The objective of our contribution is to complement previous studies realized with normal steps, and to give sufficient conditions on the distribution of the random steps for the success of a constant step-size (1, \u03bb)-ES on the simple problem of a linear function with a linear constraint. The decomposition of a multidimensional distribution into its marginals and the copula combining them is applied to the new distributional assumptions, particular attention being paid to distributions with Archimedean copulas.", "creator": "LaTeX with hyperref package"}}}