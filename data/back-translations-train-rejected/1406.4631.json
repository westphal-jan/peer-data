{"id": "1406.4631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "A Sober Look at Spectral Learning", "abstract": "Spectral learning recently generated lots of excitement in machine learning, largely because it is the first known method to produce consistent estimates (under suitable conditions) for several latent variable models. In contrast, maximum likelihood estimates may get trapped in local optima due to the non-convex nature of the likelihood function of latent variable models. In this paper, we do an empirical evaluation of spectral learning (SL) and expectation maximization (EM), which reveals an important gap between the theory and the practice. First, SL often leads to negative probabilities. Second, EM often yields better estimates than spectral learning and it does not seem to get stuck in local optima. We discuss how the rank of the model parameters and the amount of training data can yield negative probabilities. We also question the common belief that maximum likelihood estimators are necessarily inconsistent.", "histories": [["v1", "Wed, 18 Jun 2014 08:25:03 GMT  (128kb,D)", "http://arxiv.org/abs/1406.4631v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["han zhao", "pascal poupart"], "accepted": false, "id": "1406.4631"}, "pdf": {"name": "1406.4631.pdf", "metadata": {"source": "META", "title": "A Sober Look at Spectral Learning", "authors": ["Han Zhao", "Pascal Poupart"], "emails": ["HAN.ZHAO@UWATERLOO.CA", "PPOUPART@UWATERLOO.CA"], "sections": [{"heading": "1. Introduction", "text": "Spectral learning is a general approach that uses spectral compositions (e.g. Singular Value Decomposition and Tensor Decomposition) for parameter estimation based on the Method of Moments (Hsu et al., 2012; Parikh & Xing, 2011; Anandkumar et al., 2012a; c; b) Spectral learning has generated a lot of excitement in recent years due to its performance guaranteed in latently variable models. Generally, the presence of discrete latent variables leads to a non-concave log likelihood function that is problematic for maximum liquidity. Spectral learning is the first known method of being consistent (under appropriate conditions) for several latent variable models, including mixtures of Gaussern (MoGs), hidden Markov models (HMMs) and latent dirichlet allocation (LDA)."}, {"heading": "2. Spectral Learning for HMMs", "text": "Consider an HMM which is described as follows: Let x1, x2, x3,.. denotes a sequence of discrete observations in which xt x (n) = {1,.., n} is the observation in time step t, and h1, h2, h3,.. denotes a sequence of hidden states in which there is a sequence of hidden states in which there is a sequence of hidden states, in which there is a sequence of hidden states in which there is a sequence of hidden states. (1,.,.,.) The parameters of an HMM are (n, T, O) in which there is a sequence of hidden states in which there is a sequence of hidden states in which there is a sequence of hidden states."}, {"heading": "3. Negative Probabilities", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "4. Empirical Comparison with EM", "text": "We compared empirically spectral learning with expectation maximization (EM) on synthetic HMMs. Theory suggests that spectral learning should do better as it is consistent, while EM is subject to local optimism, but the results are mixed. On the small synthetic HMM (Fig. 1c), with true rank and sufficient data, spectral learning outperforms EM, but on the large synthetic HMM (Fig. 1f), EM outperforms spectral learning. The amount of training data was the same for both problems. We suspect that the amount of training data for spectral learning was insufficient to estimate reasonable operators for the larger model. Moreover, since spectral learning inverts a matrix to restore a similarity transformation of the observable operators, it is unstable and sensitive to noise."}, {"heading": "5. Local Optima", "text": "It was a surprise that the EM as good as never before has an unstandardized mixture of these components. \"It is a very good and consistent solution,\" he says, \"but it is a very good and consistent solution.\" \"It is a very good idea.\" \"It is a good idea.\" \"It is a good idea.\" \"It is a good idea.\" \"It is a very good idea.\" \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"\" It is a good idea. \"..\" It is a good idea...... \"It is a good idea....\" It is a good idea....... \"It is a good idea....\" \"It is a good idea......\" It is a good idea...... \"\" It is a good idea......... \"\" It is a good idea is a good idea. \""}, {"heading": "6. Conclusion", "text": "Spectral learning is an exciting and promising field of research. In this paper we showed that there is an important gap between theory and practice. We highlighted several outstanding problems with negative probabilities and suspected that EM might be consistent in some areas."}], "references": [{"title": "A Spectral Algorithm for Latent Dirichlet Allocation", "author": ["Anandkumar", "Anima", "Foster", "Dean P", "Hsu", "Daniel"], "venue": "In NIPS, pp", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor Decompositions for Learning Latent Variable Models", "author": ["Anandkumar", "Anima", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "In arXiv preprint arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Method of Moments for Mixture Models and Hidden Markov Models", "author": ["Anandkumar", "Animashree", "Hsu", "Daniel", "Kakade", "Sham M"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Pattern recognition and machine learning, volume 1. springer", "author": ["Bishop", "Christopher M"], "venue": "New York,", "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "A spectral algorithm for learning Hidden Markov Models", "author": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "A Spectral Algorithm for Latent Tree Graphical Models", "author": ["Parikh", "Ankur", "Xing", "Eric P"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "A Spectral Algorithm for Latent Junction Trees", "author": ["Parikh", "Ankur", "Teodoru", "Gabi", "Tech", "Georgia", "Ishteva", "Mariya", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1210.4884,", "citeRegEx": "Parikh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2012}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["Vavasis", "Stephen A"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Vavasis and A.,? \\Q2009\\E", "shortCiteRegEx": "Vavasis and A.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Furthermore, finite sample bounds guarantee that the approach will find nearly optimal parameters or make nearly optimal predictions with high probability given a sufficient amount of training data (Hsu et al., 2012).", "startOffset": 198, "endOffset": 216}, {"referenceID": 5, "context": "Bx1b1 The classic parameters (\u03c0,O, T ) can also be recovered from the operators (Hsu et al., 2012).", "startOffset": 80, "endOffset": 98}, {"referenceID": 5, "context": "Hsu et al. (2012) proved that joint probability estimates are consistent in the sense that", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "They simply reflect the fact that the theoretical guarantees are expressed in terms of bounds on additive error (see Theorem 6 in Hsu et al. (2012)).", "startOffset": 130, "endOffset": 148}, {"referenceID": 1, "context": "The problem of negative probabilities is well-known in the literature on observable operator models and was acknowledged by Boots et al. (2011) who rounded up all negative outputs to a number slightly above zero followed by normalization.", "startOffset": 124, "endOffset": 144}], "year": 2014, "abstractText": "Spectral learning recently generated lots of excitement in machine learning, largely because it is the first known method to produce consistent estimates (under suitable conditions) for several latent variable models. In contrast, maximum likelihood estimates may get trapped in local optima due to the non-convex nature of the likelihood function of latent variable models. In this paper, we do an empirical evaluation of spectral learning (SL) and expectation maximization (EM), which reveals an important gap between the theory and the practice. First, SL often leads to negative probabilities. Second, EM often yields better estimates than spectral learning and it does not seem to get stuck in local optima. We discuss how the rank of the model parameters and the amount of training data can yield negative probabilities. We also question the common belief that maximum likelihood estimators are necessarily inconsistent.", "creator": "LaTeX with hyperref package"}}}