{"id": "1406.4682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard", "abstract": "Latent variable conditional models, including the latent conditional random fields as a special case, are popular models for many natural language processing and vision processing tasks. The computational complexity of the exact decoding/inference in latent conditional random fields is unclear. In this paper, we try to clarify the computational complexity of the exact decoding. We analyze the complexity and demonstrate that it is an NP-hard problem even on a sequential labeling setting. Furthermore, we propose the latent-dynamic inference (LDI-Naive) method and its bounded version (LDI-Bounded), which are able to perform exact-inference or almost-exact-inference by using top-$n$ search and dynamic programming.", "histories": [["v1", "Wed, 18 Jun 2014 11:17:58 GMT  (204kb)", "http://arxiv.org/abs/1406.4682v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CC cs.LG", "authors": ["xu sun"], "accepted": false, "id": "1406.4682"}, "pdf": {"name": "1406.4682.pdf", "metadata": {"source": "CRF", "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard", "authors": ["Xu Sun"], "emails": ["xusun@pku.edu.cn"], "sections": [{"heading": null, "text": "In this context, it should be noted that this is a very complex issue, an attempt to find a solution, which is not a solution, but a solution."}, {"heading": "II. LATENT CONDITIONAL RANDOM FIELDS", "text": "Considering the training data, the task is to make an illustration between a sequence of observations x = x1, x2,.., xm and a sequence of labels y = y1, y2,.., each yj is a class name for the j'th character of a word sequence, and is a member of a number of possible class names. For each sequence, the model also assumes a sequence of latent variables h = h1, h2,., hm, which is not observable in training examples. See Figure 1 for comparison between CRFs and LCRFs.2 h1h2h4h4h4h4x2 x4 xy1y2,., where w represents the parameter vector of the model. To make the illustration efficient, a limitation is made for the model: for each label, the associated latent variables have no intersection with the latent variables from other labels."}, {"heading": "III. COMPLEXITY OF EXACT INFERENCE", "text": "In this section, we will perform a formal analysis of the problem's computational complexity based on conclusions in LCRFs. We assume that the reader has a basic background in complexity theory, including the terms NP and NP hardness [15], [16], [17], [18]. Normally, a minimum requirement for an algorithm that must be considered efficient is that its runtime is polynomial: O (nc), where c is a constant real value and n is the size of the input. It is assumed that the NP hard problems cannot be solved in polynomial time, although there has been no evidence of a superpolynomial lower limit. The known maximum clique problem is a NP hard problem."}, {"heading": "A. Proof", "text": "Here we prove the theorem 1 / 21 / 18 / 18 * 1 / 222N244444. For an undirected graph G = {V, D}, we present the construction of a corresponding latent conditional model MG of G, so that the size of the maximum clique of G is proportional to the probability of the optimal label of MG. We determine the length of the input sequence: m = | V |}, and the fragmented association between Y and H is as follows: H (E) = {E1, E2, E2,}, and H (N) = {N1, N2, N2, N2, N2., N | V}, and the fragmented association between Y and H is as follows: H (E), E2, E2, E2, E2."}, {"heading": "IV. A PRACTICAL SOLUTION BASED ON PROBABILITY CONCENTRATION", "text": "We have shown that the exact conclusion in latently conditioned models is a NP-hard problem. Nevertheless, we are trying to solve this difficult problem on the basis of an interesting observation. In the real world, we have an observation about LCRFs: They usually have a highly concentrated probability distribution. That is, most of the probability mass is distributed on latently upstream labels."}, {"heading": "A. Probability Concentration from Optimization", "text": "In order to formally analyze the reason for the probability concentration on LCRFs, we first analyze the optimization process on LCRFs. Since the optimization process on LCRFs is based on the gradient information of its objective function, it is crucial to analyze the trends in the gradient formulation of the LCRF objective function. In the training of LCRFs, people perform the gradient ascent to maximize the objective function. The log probability part of the objective function is as follows: L = log {f (h, x)] objective structures exp \u00b2 [w f (h, x)] objective tasks exp [w f (h, x)] objective tasks exp [h] objective tasks exp [w f (h, x)] objective tasks exp (h, x)] objective exp (h, exh) objective abilities exp \u00b2 objective tasks exp (h, h) objective tasks exh (h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h, h"}, {"heading": "B. Latent-Dynamic Inference", "text": "Based on the highly (but not completely) concentrated probabilities in LCRFs, we propose a novel sequencing method that is efficient and accurate in most real-world applications. 5 1: Definitions: 2: \"n\" stands for the current search step (# of the latent labels searched for). 3: \"ProbGap\" is a real value that records the difference between P (y) and Premain. 4: \"S\" refers to a series of \"already searched labels.\" 5: \"FindLatentLabeling (n)\" uses a search result list to find the latent label. 6: \"FindParentLabeling (h)\" finds the corresponding label from the latent label: FindParentLabeling (h) = y (hyn) = hyn (yyn): H (yj): Labeling (yj) for j = 1.."}, {"heading": "C. Latent-Dynamic Inference (LDI-Naive)", "text": "In the conclusion stage, where a test sequence x is given, we want to find the most likely label sequence until there is sufficient accuracy. (5) For latent models such as LCRFs, the y sequence cannot be produced directly by the Viterbi algorithm (see Figure 6). In short, the algorithm generates the best latent labelings in the order of their probabilities. Then, the algorithm maps each of these latent labelings to its associated labels and uses a dynamic programming method (the forward-backward algorithm) to calculate the probabilities of the corresponding labels."}, {"heading": "D. Admissible and Tight Heuristics for Efficient Search", "text": "We have presented the framework of the LDI conclusion. Here, we describe the details of implementing its critical component (DI = > cost): designing the heuristic function for the A \u0445 heuristic search. Our heuristic search aims to find the most likely latent-labelings. Remember that the probability of latent-labelings is defined as asP (h | x, w), an easier way to achieve the same goal is to find out where the uppermost latent-labelings are defined with the score. (h, w) = w f (h, x).In the A search, the cost function is usually defined as follows: f (i) = g (i)."}, {"heading": "E. A Bounded Version (LDI-Bounded)", "text": "By simply setting a threshold in the search step n, we can derive a limited version of the LDI, i.e., LDIBounded (see Figure 7). This method is a simple method of approximation to the LDI. We have also tried other methods of approximation. Intuitively, an alternative method is to design an approximate \"exact state\" by using a factor, \u03b1, to estimate the distribution of the remaining probability: P (y \u2032 | x, w) \u2212 \u03b1 (1 \u2212 \u2211 yk-SnP (yk | x, w)) \u2265 0.For example, if at most 50% of the unknown probability, 1 \u2212 \u2211 yk-Sn P (yk | x, w), can be distributed on a single sheet, we can set \u03b1 = 0.5 to create a loose condition to stop the inference. At first glance, however, this seems quite intuitive when we compare this alternative method with the LDI-bounded method, we find that the performance of the latter and the speed of the latter method was worse than the latter."}, {"heading": "F. Existing Inference Methods on Latent Conditional Models", "text": "In [13], the optimal labeling is approximated by using a modified viterbi inference (MVI method). In the MVI inference, there are two steps. First, the MVI searches for the optimal latent labeling using the Viterbi algorithm: h * = argmax h P (h | x, w). Then, a labeling y is derived by directly localizing the corresponding labeling of the latent labeling h: y = FindParentLabeling (h *), which means that hj * H (yj) for j = 1... m. The MVI inference can be regarded as a simple adaptation of the traditional viterbi inference in the case of latent conditional models. In [4], y * is estimated by a point-by-marginal inference (PMI)."}, {"heading": "G. Comparison with MAP Algorithms", "text": "The MAP problem relates to finding the maximum-a-posteriori hypothesis, which aims to find the most likely configuration of a set of variables in a Bayesian network, since some partial results are available on the complementarity of this theorem. Several researchers have suggested algorithms to solve the MAP problem [24], [20], [25], [26].In [20] an efficient approximate local search algorithm is proposed for approximation to MAP: Mountaineering and Taboo Searches. Compared to the approximate local search algorithm, the LDI algorithm can perform exact conclusions under a reasonable number of search steps (at reasonable cost). In the case that accuracy is required, this property of the LDI algorithm is important. In [26] a dynamic A weighting algorithm (DWA) is proposed for the search to solve the MAP solution in Bayesian networks."}, {"heading": "V. CONCLUSIONS", "text": "We performed a formal analysis of the conclusion in latent conditional models and showed that it is a NP-hard problem, even if latent conditional models have a disjunctive assumption and linear chain structures. More importantly, based on an observation of the probability concentration, we proposed the latent dynamic inference method (LDI naive) and its limited version (LDI bounded), which are able to perform exact and fast conclusions in latent conditional models, even though the original problem is NP-hard."}], "references": [{"title": "Discriminative log-linear grammars with latent variables", "author": ["S. Petrov", "D. Klein"], "venue": "Proceedings of NIPS\u201908. MIT Press, 2008, pp. 1153\u2013 1160.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition.", "author": ["S.B. Wang", "A. Quattoni", "L. Morency", "D. Demirdjian", "T. Darrell"], "venue": "Proceedings of CVPR\u201906,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Conditional random fields for object recognition.", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "Proceedings of NIPS\u201904,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L. Morency", "A. Quattoni", "T. Darrell"], "venue": "Proceedings of CVPR\u201907, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling latentdynamic in shallow parsing: A latent conditional model with improved inference", "author": ["X. Sun", "L.-P. Morency", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of COLING\u201908, Manchester, UK, 2008, pp. 841\u2013848.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust approach to abbreviating terms: A discriminative latent variable model with global information", "author": ["X. Sun", "N. Okazaki", "J. Tsujii"], "venue": "Proceedings of the ACL\u201909, Suntec, Singapore, August 2009, pp. 905\u2013913.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Hidden conditional random field with distribution constraints for phone classification", "author": ["D. Yu", "L. Deng", "A. Acero"], "venue": "Proceedings of InterSpeech\u201909, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Max-margin hidden conditional random fields for human action recognition", "author": ["Y. Wang", "G. Mori"], "venue": "Proceedings of CVPR\u201909, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Products of random latent variable grammars", "author": ["S. Petrov"], "venue": "Proceedings of NAACL\u201910, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative latent variable chinese segmenter with hybrid word/character information", "author": ["X. Sun", "Y. Zhang", "T. Matsuzaki", "Y. Tsuruoka", "J. Tsujii"], "venue": "Proceedings of NAACL-HLT\u201909, 2009, pp. 56\u201364.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["X. Sun", "T. Matsuzaki", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of IJCAI\u201909, 2009, pp. 1236\u20131242.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation", "author": ["X. Sun", "J. Tsujii"], "venue": "Proceedings of EACL\u201909, Athens, Greece, March 2009, pp. 772\u2013780.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic CFG with latent annotations", "author": ["T. Matsuzaki", "Y. Miyao", "J. Tsujii"], "venue": "Proceedings of ACL\u201905, June 2005, pp. 75\u201382.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1848\u20131852, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1848}, {"title": "Introduction to Automata Theory, Languages, and Computation", "author": ["J.E. Hopcroft", "J.D. Ullman"], "venue": "Addison Wesley,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1979}, {"title": "The consensus string problem and the complexity of comparing hidden markov models.", "author": ["R.B. Lyngs\u00f8", "C.N.S. Pedersen"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Complexity results and approximation strategies for MAP explanations.", "author": ["J. Park", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Computational complexity of probabilistic disambiguation", "author": ["K. Sima\u2019an"], "venue": "Grammars, vol. 5, no. 2, pp. 125\u2013151, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "A formal basis for the heuristic determination of minimum cost path", "author": ["P. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Trans. On System Science and Cybernetics, vol. SSC-4(2), pp. 100\u2013107, 1968.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1968}, {"title": "Solving MAP exactly using systematic search.", "author": ["J.D. Park", "A. Darwiche"], "venue": "Proceedings of UAI\u201903. Morgan Kaufmann,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Finding the m most probable configurations using loopy belief propagation", "author": ["C. Yanover", "Y. Weiss"], "venue": "Proceedings of NIPS\u201903. MIT Press, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic weighting a* searchbased MAP algorithm for bayesian networks.", "author": ["X. Sun", "M.J. Druzdzel", "C. Yuan"], "venue": "Proceedings of IJ-", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, in the syntactic parsing task for natural language, the hidden structures can be refined grammars that are unobservable in the supervised training data [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "In the gesture recognition task of the computational vision area, there are also hidden structures which are crucial for successful gesture recognition [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 3, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 0, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 213, "endOffset": 216}, {"referenceID": 4, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 223, "endOffset": 226}, {"referenceID": 9, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 228, "endOffset": 232}, {"referenceID": 10, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 234, "endOffset": 238}, {"referenceID": 11, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 240, "endOffset": 244}, {"referenceID": 8, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 3, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "For example, [4] demonstrated that LCRF models can learn latent structures of vision recognition problems efficiently, and outperform several widely-used conventional models, such as support vector machines (SVMs), conditional random fields (CRFs) and hidden Markov models (HMMs).", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "[1] and [9] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than models that use conventional techniques without latent variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[1] and [9] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than models that use conventional techniques without latent variables.", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The LCRF model is defined as follows [4]:", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "To make the training efficient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [4], [1].", "startOffset": 190, "endOffset": 193}, {"referenceID": 0, "context": "To make the training efficient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [4], [1].", "startOffset": 195, "endOffset": 198}, {"referenceID": 13, "context": "This simplification is also a popular practice in other latent conditional models, including hidden-state conditional random fields (HCRF) [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "We assume that the reader has basic background in the complexity theory, including the notions of NP and NP-hardness [15], [16], [17], [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Inspired by the consensus string problem on hidden Markov models [19], we establish the hardness analysis of the problem by a reduction from the maximum clique problem.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The proof is extended from related work on complexity analysis [20], [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "The proof is extended from related work on complexity analysis [20], [21].", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "In [20], the complexity analysis is based on Bayesian networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [21], the analysis is based on grammar models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In detail, an A search algorithm [22], [23] with a Viterbi heuristic function is adopted to produce top-n latent-labelings, h1,h2, .", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "Existing Inference Methods on Latent Conditional Models In [13], the optimal labeling is approximated by using a modified Viterbi inference (MVI) method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "In [4], y is estimated by a point-wise marginal inference (PMI) method.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "In [20], an efficient approximate local search algorithm is proposed for approximating MAP: hill climbing and taboo search.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [26], a dynamic weighting A (DWA) search algorithm is proposed for solving MAP in Bayesian networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [24], an effective method is proposed to compute a relatively tight upper-bound on the probability of a MAP solution.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "Latent variable conditional models, including the latent conditional random fields as a special case, are popular models for many natural language processing and vision processing tasks. The computational complexity of the exact decoding/inference in latent conditional random fields is unclear. In this paper, we try to clarify the computational complexity of the exact decoding. We analyze the complexity and demonstrate that it is an NP-hard problem even on a sequential labeling setting. Furthermore, we propose the latent-dynamic inference (LDI-Naive) method and its bounded version (LDI-Bounded), which are able to perform exact-inference or almost-exactinference by using top-n search and dynamic programming.", "creator": "LaTeX with hyperref package"}}}