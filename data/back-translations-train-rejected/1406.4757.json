{"id": "1406.4757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "An Experimental Evaluation of Nearest Neighbour Time Series Classification", "abstract": "Data mining research into time series classification (TSC) has focussed on alternative distance measures for nearest neighbour classifiers. It is standard practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a straw man for comparison. As part of a wider investigation into elastic distance measures for TSC~\\cite{lines14elastic}, we perform a series of experiments to test whether this standard practice is valid.", "histories": [["v1", "Wed, 18 Jun 2014 15:09:21 GMT  (117kb,D)", "http://arxiv.org/abs/1406.4757v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anthony bagnall", "jason lines"], "accepted": false, "id": "1406.4757"}, "pdf": {"name": "1406.4757.pdf", "metadata": {"source": "CRF", "title": "Technical Report CMP-C14-01: An Experimental Evaluation of Nearest Neighbour Time Series Classification", "authors": ["Anthony Bagnall"], "emails": ["ajb@uea.ac.uk", "jason.lines@uea.ac.uk"], "sections": [{"heading": null, "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Time Series Classification (TSC)", "text": "We define time series as the problem of building a classifier from a collection of described training time series. We limit our attention to problems in which each time series has the same number of observations. We define a time series xi as a series of ordered observations. (xn, yn).For traditional classification, the order of attributes is irrelevant and the interaction between the variables is considered independent of their relative positions. For time series data, the arrangement of the variables is often crucial in order to find the best discriminating characteristics. (xn, yn) The order of attributes is unimportant and the interaction between the variables."}, {"heading": "B. Classification Algorithms", "text": "The closest classifier is a lazy classifier (i.e., it does not require training) that classifies new cases by finding the closest case in the training set with a distance function and then using the class of the closest case as the predicted class for the new case. Given the focus on distance functions in data mining research in time series, it is perhaps not surprising that the majority of the classification has used 1-NN. Although 1-NN is often very effective, it is known to be vulnerable to problems such as outliers in the training set and a large number of redundant features. Outliers can be compensated by using k nearest neighbors and a voting scheme. Redundant features can be solved by filtering or using one of the plethora variety of alternative classifiers. Filtering proved ineffective in [9]. We compare the nearest neighbourhood classifiers with C4.5 [18], Random Forest [19], Forest [20] Rotation [22] and Bayesian [23] machines."}, {"heading": "C. Dynamic Time Warping", "text": "Suppose we want to measure the distance between two series, a = {a1, a2,..., am} and b = {b1, b2,.., bm}. Suppose M (a, b) is the m \u00b7 m pointer distance matrix between a and b, where Mi, j = (ai \u2212 bj) 2.A delay path P = < (e1, f1), (e2, f2),.., (es, fs) > is a series of points (i.e. pairs of indexes) defining a traverse of matrix M. For example, the euclidean distance dE (a, b) = 1 piff mi = 1 (ai \u2212 bi) 2 is the path along the diagonal of M, i.e. Pe = & lt.i (1, ei), ei (ei), ei (1, ei), i, i, i, i, ei (1), ei, ei, ei, ei (1), ei, ei, ei (1), ei, ei (1), ei (1), ei (1), ei (1), ei (1), ei (1), ei (1), ei (1), ei, ei (1), ei (1), ei (1)."}, {"heading": "D. Longest Common Subsequence", "text": "The longest shared sub-sequence distance (LCSS) is based on solving the longest shared sub-sequence problem Li Li Li Li b > Li Li b > j = 1 CDJ = 1 CDJ = 1 CDJ = 1. The typical problem is to find the longest sub-sequence that is shared by two discrete sub-sequences, based on the processing distance. This approach can be extended to include real time series by using a distance threshold that defines the maximum difference between a value pair that can be considered to match them [25]. LCSS finds the optimal alignment between two rows by inserting gaps to find the largest number of matching pairs. For example, consider the discrete case where we have two strings: LSS = \"ABCADACDAB\" and T = \"BCDBCACB.\" If we simply observe the dot-by-point matches between the two sequences, we can find the matching pairs for the sub-sequence \"ABCCDB 1.ACDB\" as in ACDB--CDB."}, {"heading": "E. Derivative Dynamic Time Warping", "text": "Keogh and Pazzani proposed a modification of the DTW called Derivative Dynamic Time Warping (DDTW) [13], which initially transforms the series into a series of initial differences. At a series a = {a1, a2,..., am}, the difference series is a \u2032 = {a \u00b2 2, a \u00b2 2,..., a \u00b2 m \u2212 1}, where a \u00b2 i is defined as the average of the slope between ai \u2212 1 and ai and ai and ai and ai + 1, i.e. a \u2032 i = (ai \u2212 1) + (ai + 1 \u2212 ai \u2212 1) / 22, for 1 < i < m. DDTW is designed to reduce the noise in the series that can affect DTW."}, {"heading": "F. Weighted Dynamic Time Warping", "text": "A weighted form of DTW (WDTW) was proposed by Jeong et al. [8]. WDTW adds a multiplicative weight penalty based on the distortion distance between the points in the distortion path. It favors a reduced distortion and is a gentle alternative to the cutoff point approach of using a distortion window. When creating the distance matrix M, a weight penalty is imposed w | i \u2212 j | for a distortion distance of | i \u2212 j | (ai \u2212 bj) 2.A logistic weight function is proposed in [8], so that a distortion of a location imposes a weight of w (a) = wmax1 + e \u2212 g \u00b7 (a \u2212 m / 2), with wmax representing an upper limit of the weight (set to 1), m is the series length and g is a parameter that controls the penalty for large distortions. The greater the distortion, the higher the penalty is for the distortion."}, {"heading": "III. DATA SETS", "text": "We have collected 77 data sets whose names are listed in Table I. 43 of these are available from the UCR repository [15], 29 have been used in other published work [11], [5], [16] and 5 are new data sets that we are presenting for the first time. Further information and the data sets we are allowed to circulate are available from [14]. We have grouped the problems into categories to facilitate interpretation, and the group of sensor measurements is the largest category, with 31 data sets. If we had more data sets, it would make sense to divide the sensor categories into subtypes such as human sensors and spectrographs. However, at this point, such sub-typing would result in groups that are too small. The classification of image outlines is the second largest category with 29 problem sets. Many of the image outlines, such as BeetleFly, are not rotational, and it would be expected that classifiers in the time range will not necessarily perform well with these data."}, {"heading": "IV. RESULTS", "text": "We have performed our classification experiments with WEKA [26] source code adapted for time series classification, using the 77 datasets described in Section III.All datasets are divided into a training and test set, and all parameter optimizations are performed only on the training set. We use a pull / test split for several reasons. First, it is common practice to do this with UCR datasets. Second, some of the datasets are designed to eliminate the pull / test split distortion.A combination to perform a cross validation would reintroduce the distortion effect. Finally, it is mathematically impossible to cross everything. We have performed over 3 million experiments on a 4148-core high-performance cluster (with a theoretical peak performance of 65Tflops) over a period of month.Adding another level of cross validation would have increased the time required to perform the number of experiments and increase the complexity."}, {"heading": "A. Are 1-NN Classifiers are hard to beat?", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "D. Which is better for DTW, setting window size or setting weights?", "text": "The results proposed for the weighting algorithm in DTW are in the critical differential diagram in Figure 6.Their claim that weighting leads to a clear improvement is not supported by these results. Figure 6 shows that although weighted versions have the highest rank, the result cannot be described as significant; the only significant difference is between the weighted versions of DTW and Euclidean distances. This shows the need for testing on a large number of data sets. Furthermore, the comparisons they make are distorted. First, they compare the full DTW window with the weight scheme when it would be more appropriate to compare against the window size by cross-validation. Second, they use half of the tested data to validate the weighting parameters, but do not allow the other classifiers access to this data."}, {"heading": "V. CONCLUSIONS", "text": "We have conducted extensive experiments (to the best of our knowledge) on the largest set of time series classification problems ever used in literature. We have conducted these tests to confirm common assumptions, evaluate a recently published algorithm, and evaluate ensemble classification methods.First, we conclude that comparisons against 1-NN at euclidean distance are not particularly meaningful for new TSC algorithms, since three standard classifiers applied to the raw data perform significantly better without parameter tuning. We think that a new algorithm is of interest only when it can significantly exceed 1-NN DTW with a full delay window. Second, when using an NN classifier with DTW for a new problem, we would point out that it is not particularly important to cross-validate k, but that setting the distortion window size is worthwhile."}], "references": [{"title": "Time series classification with ensembles of elastic distance measures", "author": ["J. Lines", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery, vol. Accepted for publication, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines of intervalbased features for time series classification", "author": ["J. Rodriguez", "C. Alonso"], "venue": "Knowledge-Based Systems, vol. 18, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Querying and mining of time series data: Experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proc. 34th VLDB, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Time series clustering and classification by the autoregressive metric", "author": ["M. Corduas", "D. Piccolo"], "venue": "Computational Statistics and Data Analysis, vol. 52, no. 4, pp. 1860\u20131872, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1860}, {"title": "Time series shapelets: A new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proc. 15th ACM SIGKDD, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "An integrated framework for simultaneous classification and regression of time-series data", "author": ["Z. Abraham", "P. Tan"], "venue": "Proc. 10th SDM, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A complexity-invariant distance measure for time series", "author": ["G. Batista", "X. Wang", "E. Keogh"], "venue": "Proc. 11th SDM, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y. Jeong", "M. Jeong", "O. Omitaomu"], "venue": "Pattern Recognition, vol. 44, pp. 2231\u20132240, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Transformation based ensembles for time series classification", "author": ["A. Bagnall", "L. Davis", "J. Hills", "J. Lines"], "venue": "Proc. 12th SDM, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification trees for time series", "author": ["A. Douzal-Chouakria", "C. Amblard"], "venue": "Pattern Recognition, vol. 45, no. 3, pp. 1076\u20131091, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of time series by shapelet transformation", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery, vol. online first, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A time series forest for classification and feature extraction", "author": ["H. Deng", "G. Runger", "E. Tuv", "M. Vladimir"], "venue": "Information Sciences, vol. 239, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Derivative dynamic time warping", "author": ["E.Keogh", "M.Pazzani"], "venue": "Proc. 1st SDM, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series classification", "author": ["A. Bagnall"], "venue": "http://www.uea.ac.uk/computing/machine-learning/time-seriesclassification.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 0}, {"title": "The UCR time series classification/clustering homepage", "author": ["E. Keogh", "Q. Zhu", "B. Hu", "Y. Hao", "X. Xi", "L. Wei", "C. Ratanamahatana"], "venue": "http://www.cs.ucr.edu/ eamonn/time_series_data/, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Shapelet based time-series classification", "author": ["A. Bagnall"], "venue": "http://www.uea.ac.uk/computing/machine-learning/shapelets.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient Similarity Search In Sequence Databases", "author": ["R. Agrawal", "C. Faloutsos", "A. Swami"], "venue": "Proc. 4th FODO, 1993.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "C4.5: programs for machine learning", "author": ["J. Quinlan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Rotation forest: A new classifier ensemble method", "author": ["J. Rodriguez", "L. Kuncheva", "C. Alonso"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 28, no. 10, pp. 1619\u20131630, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Naive (bayes) at forty: The independence assumption in information retrieval", "author": ["D. Lewis"], "venue": "Proc. 10th ECML, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1988}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Three myths about dynamic time warping data mining", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "Proc. 5th SDM, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Hand shape classification using DTW and LCSS as similarity measures for vision-based gesture recognition system", "author": ["A. Kuzmanic", "V. Zanchi"], "venue": "2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 11, no. 1, pp. 10\u201318, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "As part of a wider investigation into elastic distance measures for TSC [1], we perform a series of experiments to test whether this standard practice is valid.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "In [9], Bagnall et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The main conclusion in [9] is that for problems where the discriminatory features are based on similarity in change and similarity in shape, operating in a different data space produces a better performance improvement than designing a more complex classifier for the time domain.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "However, the issue of what is the best technique in a single data domain is not addressed in [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 195, "endOffset": 199}, {"referenceID": 11, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 6, "context": "The general consensus amongst data mining researchers is that \u201csimple nearest neighbor classification is very difficult to beat\u201d [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "For problems with few training cases, an elastic distance measure such as dynamic time warping (DTW) or longest common subsequence (LCSS) is often superior to Euclidean distance, but as the number of series increases \u201cthe accuracy of elastic measures converge with that of Euclidean distance\u201d [3].", "startOffset": 293, "endOffset": 296}, {"referenceID": 7, "context": "A version of DTW that weights against large warpings (WDTW) is described in [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "The weighting scheme can be used in conjunction with dynamic time warping and an alternative version based on first order differences (DDTW), described in [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "We extend the experiments described in [8] to test whether their conclusions hold over a large number of data sets with parameter optimisations for all of the algorithms considered.", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "All datasets and code to reproduce experiments and results are available online [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 16, "context": "If the common shape involves the whole series, but is phase shifted between instances of the same class, then transformation into the frequency domain is often the best approach (for example, see [17]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "If the common shape is local and embedded in confounding noise, then subsequence techniques such as Shapelets can be employed [5], [11].", "startOffset": 126, "endOffset": 129}, {"referenceID": 10, "context": "If the common shape is local and embedded in confounding noise, then subsequence techniques such as Shapelets can be employed [5], [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "The most common approach in this situation is to fit an ARMA model, then base similarity on differences in model parameters [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "This can be quantified by measures such as Euclidean distance or correlation [10], [6].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "This can be quantified by measures such as Euclidean distance or correlation [10], [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "A classic example of this type of similarity is the Cylinder-Bell-Funnel artificial data set, where there is noise around the underlying shape, but also noise in the index of where the underlying shape transitions [10].", "startOffset": 214, "endOffset": 218}, {"referenceID": 2, "context": "In a comprehensive study [3], DTW was found to be as least as good as other elastic measures based on edit distance, and constraining the warping window was found to speed up computation, \u201cwhile yielding the same or even better accuracy\u201d [3].", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "In a comprehensive study [3], DTW was found to be as least as good as other elastic measures based on edit distance, and constraining the warping window was found to speed up computation, \u201cwhile yielding the same or even better accuracy\u201d [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 2, "context": "The experimentation in [3] addresses the issue of what distance measure to use and is the starting point for our research.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Filtering was found to be not effective in [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 17, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 23, "context": "For similarity in shape, Dynamic Time Warping (DTW) is commonly used to mitigate against distortions in the time axis [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "This approach can be extended to consider real-valued time series by using a distance threshold , which defines the maximum difference between a pair of values that is allowed for them to be considered a match [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "Keogh and Pazzani proposed a modification of DTW called Derivative Dynamic Time Warping (DDTW) [13] that first transforms the series into a series of first differences.", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "A logistic weight function is proposed in [8], so that a warping of a places imposes a weighting of", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Further information and the data sets we have permission to circulate are available from [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "We conducted our classification experiments with WEKA [26] source code adapted for time series classification, using the 77 data sets described in Section III.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "All the results are available on an Excel spreadsheet from [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "We used a paired T test for the mean and a Wilcoxon signed rank test for the median, full results are available from [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "The results published for the weighting algorithm proposed in [8] are summarised in the critical difference diagram in Figure 6.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Data taken directly from [8].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "LCSS is in many ways closer to a Shapelet approach [5] than DTW, and this result suggests that subsequence matching techniques such as LCSS and Shapelets may be better for image outline classification.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Thirdly, we conclude that the weighting algorithm for DTW described in [8] is significantly better than DTW with a full warping window, but not significantly different to DTW with the window set through cross validation.", "startOffset": 71, "endOffset": 74}], "year": 2014, "abstractText": "Data mining research into time series classification (TSC) has focussed on alternative distance measures for nearest neighbour classifiers. It is standard practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a straw man for comparison. As part of a wider investigation into elastic distance measures for TSC [1], we perform a series of experiments to test whether this standard practice is valid. Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to standard classifiers, examine whether the performance of 1-NN Euclidean approaches that of 1-NN DTW as the number of cases increases, assess whether there is any benefit of setting k for k-NN through cross validation whether it is worth setting the warping path for DTW through cross validation and finally is it better to use a window or weighting for DTW. Based on experiments on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to beat but 1-NN with DTW is not, if window size is set through cross validation.", "creator": "LaTeX with hyperref package"}}}