{"id": "1406.4802", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2014", "title": "Homotopy based algorithms for $\\ell_0$-regularized least-squares", "abstract": "Sparse signal approximation can be formulated as the mixed $\\ell_2$-$\\ell_0$ minimization problem $\\min_x J(x;\\lambda)=\\|y-Ax\\|_2^2+\\lambda\\|x\\|_0$. We propose two heuristic search algorithms to minimize J for a continuum of $\\lambda$-values, yielding a sequence of coarse to fine approximations. Continuation Single Best Replacement is a bidirectional greedy algorithm adapted from the Single Best Replacement algorithm previously proposed for minimizing J for fixed $\\lambda$. $\\ell_0$ regularization path track is a more complex algorithm exploiting that the $\\ell_2$-$\\ell_0$ regularization path is piecewise constant with respect to $\\lambda$. Tracking the $\\ell_0$ regularization path is done in a sub-optimal manner by maintaining (i) a list of subsets that are candidates to be solution supports for decreasing $\\lambda$'s and (ii) the list of critical $\\lambda$-values around which the solution changes. Both algorithms gradually construct the $\\ell_0$ regularization path by performing single replacements, i.e., adding or removing a dictionary atom from a subset. A straightforward adaptation of these algorithms yields sub-optimal solutions to $\\min_x \\|y-Ax\\|_2^2$ subject to $\\|x\\|_0\\leq k$ for contiguous values of $k\\geq 0$ and to $\\min_x \\|x\\|_0$ subject to $\\|y-Ax\\|_2^2\\leq\\varepsilon$ for continuous values of $\\varepsilon$. Numerical simulations show the effectiveness of the algorithms on a difficult sparse deconvolution problem inducing a highly correlated dictionary A.", "histories": [["v1", "Fri, 31 Jan 2014 22:26:17 GMT  (723kb)", "https://arxiv.org/abs/1406.4802v1", "28 pages"], ["v2", "Wed, 18 Mar 2015 16:37:16 GMT  (183kb)", "http://arxiv.org/abs/1406.4802v2", "38 pages"]], "COMMENTS": "28 pages", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["charles soussen", "j\\'er\\^ome idier", "junbo duan", "david brie"], "accepted": false, "id": "1406.4802"}, "pdf": {"name": "1406.4802.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Charles Soussen", "J\u00e9r\u00f4me Idier", "Junbo Duan"], "emails": ["charles.soussen@univ-lorraine.fr,", "david.brie@univ-lorraine.fr.", "jerome.idier@irccyn.ec-nantes.fr.", "junbo.duan@mail.xjtu.edu.cn."], "sections": [{"heading": null, "text": "It is well known that the interpretation of a term leads to a problem where the convex relaxation methods have received considerable attention, where the term \"0-norm\" is replaced by the term \"1-norm.\" Among the many efficient \"1\" -solvers, the homotopy algorithm is minimized. \"In this paper, we address the minimization problem\".A \".A\" \".A\".A \"\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \"A\".A \".A\" A \".A\" A \".A\".A \"A\".A \"A\".A \".A\" A \".A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \"A\".A \".A\" A \"A\".A \".A\" A \".A\" A \".A\" A \".A\".A \"A\".A \"A\".A \".A\" A \"A\".A \".A\" A \"A\".A \".A\" A \".A\" A \".A\" A \"A\".A \".A\".A \".A\" A \"\" A \"A\".A \"A\" A \".A\" A \".A\" A \".A\".A \"A\".A \"A\" A \".A\" has received considerable attention."}, {"heading": "A. Classification of methods", "text": "The individual algorithms that address the problems (1) - (2) can be divided into two classes. (2) The forward-looking algorithms explore sub-areas of increasing complexity. (4) The forward-looking algorithms explore sub-areas of complexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], Orthogonal LeastMarch 19, 2015 DRAFTSquares (OLS). (5), OrderRecursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP)."}, {"heading": "B. Main idea", "text": "Our approach is dedicated to the \"0-penalized smallest squares.\" It is based on the following geometric interpretations.First, we can define a linear function \u03bb 7 \u2192 E (S) + \u03bb | S | for each subset S, where E (S) = \u0442 y \u2212 Ax \u00b2 22 is the corresponding smallest square error, and | S | for the cardinality of S. For each subset S, this function results in a line in the 2D domain (\u03bb, J), as in Fig. 1. Second, the set of solutions of (3) is piecemeal with respect to \u03bb (see Appendix A for a proof).Geometrically, this result is easy to understand by observing that the minimum of J (x; \u03bb) is reached with respect for x for all thresholds, taking into account the concave envelope of the line of (3) (see Appendix A for a proof)."}, {"heading": "C. Related works", "text": "1) Bi-objective optimization: The formulations (1), (2) and (3) can be interpreted as the same biobjective problem because they all aim to minimize both the approximation errors and the different solutions. (2) Although x-objective optimization problem is actually an ongoing one, the bi-objective optimization problem should rather be considered as a discrete one, where both objective representations represent a classic bi-objective representation, where each axis is related to a single objective goal, namely S-objective optimization. (2) In bi-objective optimization, a point S is considered optimal when no other point can reduce S-objective. (32)"}, {"heading": "A. Definitions, terminology and working assumptions", "text": "We assume that all min (m, n) columns of A are linearly independent, so that for each subset S {1,.., n} the submatrix of A collects the columns indexed by S. We then use the alternative notations \"S + {i}\" and \"S \u2212 {i}\" for preselection S \u00b2 and the backward elimination S\\ {i}. We can then introduce the generalization of a subset S \u00b2 \"and\" S \u2212 {i} \"for preselection S \u00b2 and the backward elimination S\\ {i} as the generalization of the generalization S \u00b1 {i} for individual substitutions."}, {"heading": "B. Definition and properties of the \u21130-regularized paths", "text": "The persistent problems (1), (2) and (3) can be converted as the discrete problems: min S (S) are subject to the constant of | \u2264 k, (4) min S | definition of E (S) \u2264 \u03b5, (5) min S {J (S; \u03bb), E (S) + \u03bb | S |}, (6) where S stands for the support of x. The optimal solutions x for problems (1), (2) and (3) can simply be read from the elements of (4), (5) and (6), each x as the least square minimizers of all the vectors supported by S. In the following, the formulation will be omitted from the formula (5), because it leadsto follows the same path of 0 regulation of piepiepie.i as formulation (4) [2].Let us first define the solution approaches to (4) and (6) and (0-curve) that relate to the minimum value."}, {"heading": "C. Approximate \u21130-penalized regularization path", "text": "Let us introduce notations for the approximate path of 0 given by our heuristic search algorithms. Results of our algorithms are composed of a list \u03bb = {\u03bb1,..., \u03bbJ + 1} of decreasing \u03bb values and a list S = {S0,..., SJ} of candidates, with S0 = \u2205. Sj is a suboptimal solution for (6) decreasing \u03bb (\u03bbj + 1, \u03bbj). In the first interval, the solution is S0 > \u03bb1. The reader must bear in mind that each output Sj produces a suboptimal solution xj to (3) for (1) for (2 + 1, \u03bbj). This vector is the least square solution supported by Sj. It can be calculated using the pseudo-indication of the J segments."}, {"heading": "III. GREEDY CONTINUATION ALGORITHM (CSBR)", "text": "Our starting point is the Single Best Replacement algorithm [3], which is dedicated to the minimization of J (x; \u03bb) in relation to x or equivalent to J (S; \u03bb) = E (S) + \u03bb | S | in relation to S. First, we describe SBR for a certain \u03bb. Afterwards, the CSBR extension for the reduction and adaptation of \u03bb's is presented."}, {"heading": "A. Single Best Replacement", "text": "An SBR iteration consists of three steps: 1) Calculation J (S \u00b1 {i}; \u03bb) for all possible single substitutes S \u00b1 {i} (n insertion and removal attempts); 2) Select the best replacement Sbest = S \u00b1 {e} when no single substitute can reduce the cost function; (7) 3) Update S \u2190 Sbest.SBR ends when J (Sbest; \u00f6) the best replacement Sbest (S; \u00f6) when no single substitute can reduce the cost function. This happens after an finite number of iterations, because SBR is a descending algorithm and there is a finite number of possible subsets."}, {"heading": "B. Principle of the continuation search", "text": "Our continuation strategy is inspired by the \"1 homotopia,\" which recursively calculates the minimizers of \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - \"BR\" - BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"-\" - BR \"-\" BR \"-\" BR \"-\" BR \"-\" BR \"- BR\" - \"- BR\" - BR \"- BR\" - BR \"-\" - BR \"-\" BR \"-\" - BR \"-\" BR \"-\" - \"BR\" - \"-\" - \"BR\" - \"-\" BR \"-\" - \"BR\" - \"-\" - \"BR\" - \"-\" - \"BR\" - \"-\" - \"-\" BR \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" BR \"-\" - \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" BR \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" - \"-\" BR \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"BR\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" BR \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-"}, {"heading": "C. CSBR algorithm", "text": "In the first phase we have S0 = \u2205, and (11) - (12) reread: (12) reread: (13) reread: (13) reread: (13) reread: (13) reread: (13) \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "A. Descent of the concave polygon", "text": "This descendancy procedure is illustrated in Fig. 7 using two examples (upper and lower sub-figures). For each example, the unitial polygon is represented in (a). It is updated if its intersection with the line Snew is not empty (b). The new concave polygon (c) is obtained as the concave shell of the former polygon and the line Snew. All subsets in S whose edges are above the line Snew are removed from S. This procedure is formally presented in Fig. III. Let us now specify how the new candidate subdivisions Snew are structured.March 19, 2015"}, {"heading": "B. Selection of the new candidate support", "text": "It corresponds to 1 if Sj has already been \"explored\" and 0 otherwise. The following exploration process is performed against a subsetS = Sj: all possible single substitutions S \u00b1 {i} are tested; the best insertion and removal values are stored in memory, defining the unexplored subset Sj of the lowest cardinality (i.e. the lowest index j) in (11) and similar; attempts to include Sj = Sj + (S \u2212 {i}) \u2212 E (S)} are selected; (14) The unexplored subset Sj of the lowest cardinality (i.e. the lowest index j) is included in 0-PD."}, {"heading": "C. \u21130-PD algorithm", "text": "The resulting concave polygon is reduced to a single horizontal edge, the corresponding endpoints being \u03bb1 = 0 and (by extension) \u03bb0, + \u221e. In the first iteration, S0 is explored: The best insertion Sadd = {\u043e add} is calculated in (13) and included in S. The updated group S is now composed of S0 = \u2205 (explored) and S1 = Sadd (unexplored). The new concave polygon has two edges delimited by \u03bb2 = 0, \u03bb1 and \u03bb0 = + \u221e, with the alternative 1 in (13). Generally, either 0, 1 or 2 new unexplored Sadd and Srmv are supported, where Sadd and Srmv can be included in S on a given iteration, while a variable number of backups can be removed from S."}, {"heading": "D. Fast implementation", "text": "The concave polygon S and the line Snew. Lemma 1 states that this intersection point is empty in two simple situations. Therefore, the request to intersect is not required in these situations. This implementation detail is excluded in Tab. III for brevity reasons. Lemma 1 Let S = {Sj, j = 0,..., J} is a list of columns associated with a continuous concave polygon. \u2022 If [Sj] < \u03bbj + 1, then the line Sadd = Sj + [...] is above the current concave polygon. \u2022 If Ermv (Sj) > & ltj, then the line Srmv = Sj < \u03bbj + [...], then the line Sadd = Sj + [...] is above the current concave polygon."}, {"heading": "E. Main differences between CSBR and \u21130-PD", "text": "First, we emphasize that the index j in \u03bbj, unlike CSBR, no longer identifies with the iteration number for H0-PD. In fact, the current iteration of H0-PD is associated with an edge of the concave polygon, i.e. with an entire interval (HJ + 1, HJ), whereas the current iteration of CSBR is dedicated to a single value that decreases as the iteration number j increases. Second, the calculation of the next value HJ + 1 \u2264 HJ in CSBR is based only on violating the lower limit of (9), which corresponds to the selection of atoms. H0-PD also takes into account the upper limit, which is why the \u03bb values are no longer scanned in decreasing order. This could improve the very sparse solutions found in the early iterations within an increased computation time, as we will see later."}, {"heading": "V. NUMERICAL RESULTS", "text": "The algorithms are evaluated on the basis of two types of problems with poorly conditioned dictionaries: The behaviour of CSBR and H0-PD is first analysed on the basis of simple examples and then compared in detail with other non-convex algorithms for many scenarios."}, {"heading": "A. Two generic problems", "text": "The sparse unfolding problem takes the form y = h * x + n, where the impulse response h is a Gaussian filter of standard deviation \u03c3, and noise n is assumed i.i.d. and Gaussian. The problem rereads y = Ax + n, where A is a folding matrix. In the default setting, y and x are sampled at the same frequency. h is approximated by a finite impulse response of length 6\u03c3 by swelling of the smallest values. A is a Toeplitz matrix of the selected dimensions, so that each Gaussian feature h * x *, when fully enclosed within the observation window..., m} This implies that A is slightly undercomplete: m > n with m \u00b2 n,. Two simulated data vectors y are shown in the figure. 8 (a, b), where x are the x-sparse vectors of the observation window."}, {"heading": "B. Empirical behavior of CSBR and \u21130-PD", "text": "Since CSBR and PD produce very similar results, we show only the smallest approaches to solving the first solution (the lowest value of Sj, c), only the seven most important jumps are detected (Figure 9 (a)). The cardinality of the Sj jumps with most other jumps is achieved together with possible incorrect jumps (Figure 9, c). It can often be useful to select a single solution xj. The proposed algorithms are compatible with most classical methods of model selection [46], because they are greedy algorithms."}, {"heading": "C. Extensive comparisons", "text": "We do not consider the proposed algorithms to be forward-looking algorithms, nor do we consider the BPDN estimates to be forward-looking; we have already shown that the BPDN estimates are surprisingly efficient. (In fact, we have admitted that the BPDN estimates are less accurate than the unforeseen estimates.) The proposed algorithms are not forward-looking algorithms. (In fact, we have admitted that the BPDN estimates are less accurate than the unforeseen estimates.) The proposed algorithms are based on comparison with algorithms based on non-convex penalties.We have actually admitted that the BPDN estimates are less accurate than thrifty estimates. (We consider the BPDN estimates based on non-convex penalties.) We do not consider the proposed algorithms to be forward-looking algorithms. (We have actually admitted that the BPDN estimates are less accurate than the BPDN estimates based on BDN estimates.)"}, {"heading": "VI. SOFTWARE", "text": "The Matlab implementation of the proposed CSR and 0 PD algorithms is available at www.cran.univ-lorraine.fr / perso / charles.soussen / software.html, including programs that show how to call these functions."}, {"heading": "VII. CONCLUSION", "text": "The choice of a relevant, sparse approximation algorithm is based on a trade-off between the desired performance and the computing time you are willing to spend. The proposed algorithms are relatively expensive, but very suitable for inverse problems that cause highly correlated dictionaries, one reason being that they are able to escape the local minimizers of J (x; \u03bb) = 0. This behavior is in contrast to other classic, sparse algorithms. We have shown the usefulness and efficiency of the two SBR extensions when the level of sparseness is moderate to high, i.e., k / min (m, n) is lower than 0.1. They remain competitive when k / min (m, n) ranges between 0.1 and 0.2, and their performance gradually deteriorates for weaker levels of sparseness, while they represent an expected behavior for such greedy algorithms."}, {"heading": "APPENDIX A", "text": "PROPERTIES OF REGULARIZATION In this appendix, we prove that the 0% penalized path S-P (see definition 2) is constant bit by bit (theorem 1) and represents a subset of the 0% limited regulation path S-C (theorem 2). We will denote the 0% curve as 7% J = minS {J (S; \u03bb)}. Remember that this function is concave and affine at each interval (\u03bb-i + 1, \u03bb-i), where i-i {0,..., I} (definition 1)."}, {"heading": "A. Proof of Theorem 1", "text": "We prove theorem 1 together with the following problem, which gives information about the content of S-P (\u03bb) about the breakpoints of S-P (\u03bb) and Lemma 2 Let i-P (..., I-1). Then we have for the first and last periods: \u2022 For all periods (0, \u03bb-I), S-P (\u03bb), S-P (\u03bb-I + 1), S-P (\u03bb-I), S-P (\u03bb-I), S-P (\u03bb-I), S-P (\u03bb-I) and S-P (\u03bb-I). \u2022 For all periods (\u03bb-1, + 1), S-P (\u03bb-P) and S-P (\u03bb-I)."}, {"heading": "B. Proof of Theorem 2", "text": "The first result is simple: For each result and for each result we have S-S-C (| S-S |). Otherwise, there would be S-S-S-S-S-S and E-S-S (S) < E-S (S). To prove the second result, we first show that S-S-P (S-S) for each i-S-S-S-S (S-S), S-P (S-S), S-P (S-S), S-P (S-S), S-S (i + I), S-P (S), S-S (i), S-P (ki) and S-S (S). Therefore, we imply J-S-P-P (S-S) = J-S-S (S-S)."}], "references": [{"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Description of the minimizers of least squares regularized with l0 norm. Uniqueness of the global minimizer", "author": ["M. Nikolova"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Computational methods for sparse solution of linear inverse problems\u201d, Proc. IEEE, invited paper (Special Issue \u201cApplications of sparse representation and compressive sensing\u201d)", "author": ["J.A. Tropp", "S.J. Wright"], "venue": "vol. 98,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "in Proc. 27th Asilomar Conf. on Signals, Systems and Computers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Orthogonal least squares methods and their application to non-linear system identification", "author": ["S. Chen", "S.A. Billings", "W. Luo"], "venue": "Int. J. Control,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Subset selection in regression", "author": ["A.J. Miller"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Forward sequential algorithms for best basis selection", "author": ["S.F. Cotter", "J. Adler", "B.D. Rao", "K. Kreutz-Delgado"], "venue": "IEE Proc. Vision, Image and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Optimized orthogonal matching pursuit approach", "author": ["L. Rebollo-Neira", "D. Lowe"], "venue": "IEEE Signal Process. Lett., vol. 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Iterative thresholding for sparse approximations", "author": ["T. Blumensath", "M.E. Davies"], "venue": "J. Fourier Anal. Appl.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Appl. Comp. Harmonic Anal., vol. 26,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multiple regression analysis", "author": ["M.A. Efroymson"], "venue": "Mathematical Methods for Digital Computers, A. Ralston and H. S. Wilf, Eds., vol. 1, pp. 191\u2013203. Wiley, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1960}, {"title": "Forward and backward stepping in variable selection", "author": ["K.N. Berk"], "venue": "J. Statist. Comput. Simul.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Adaptive forward-backward greedy algorithm for learning sparse representations", "author": ["T. Zhang"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["M.A.T. Figueiredo", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE J. Sel. Top. Signal Process.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "l1 \u2212 l2 optimization in signal and image processing", "author": ["M. Zibulevsky", "M. Elad"], "venue": "IEEE Sig. Proc. Mag., vol. 27,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Minimizing nonconvex functions for sparse vector reconstruction", "author": ["N. Mourad", "J.P. Reilly"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Iterative reweighted l1 and l2 methods for finding sparse solutions", "author": ["D.P. Wipf", "S. Nagarajan"], "venue": "IEEE J. Sel. Top. Signal Process. (Special issue on Compressive Sensing),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A general framework for sparsity-based denoising and inversion", "author": ["A. Gholami", "S.M. Hosseini"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Universal regularizers for robust sparse coding and modeling", "author": ["I. Ram\u0131\u0301rez", "G. Sapiro"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Sparse signal recovery by difference of convex functions algorithms", "author": ["H.A. Le Thi", "B.T. Nguyen Thi", "H.M. Le"], "venue": "Intelligent Information and Database Systems, A. Selamat, N. T. Nguyen, and H. Haron, Eds., Berlin", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse signal estimation by maximally sparse convex optimization", "author": ["I. Selesnick", "I. Bayram"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA Journal of Numerical Analysis, vol. 20, no. 3, pp. 389\u2013403", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist., vol. 32,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria optimization problems", "author": ["I. Das", "J.E. Dennis"], "venue": "Structural optimization,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Survey of multi-objective optimization methods for engineering", "author": ["R.T. Marler", "J.S. Arora"], "venue": "Structural and Multidisciplinary Optimization,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Probing the Pareto frontier for basis pursuit solutions", "author": ["E. van den Berg", "M.P. Friedlander"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Subset regression with stepwise directed search", "author": ["P.M.T. Broersen"], "venue": "J. R. Statist. Soc. C, vol. 35, no. 2, pp. 168\u2013177", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1986}, {"title": "A bidirectional greedy heuristic for the subspace selection problem\u201d, in Engineering stochastic local search algorithms. Designing, implementing and analyzing effective heuristics", "author": ["D. Haugland"], "venue": "vol. 4638 of Lect. Notes Comput. Sci.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Projection-based and look-ahead strategies for atom selection", "author": ["S. Chatterjee", "D. Sundman", "M. Vehkaper\u00e4", "M. Skoglund"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A continuation approach to estimate a solution path of mixed L2-L0 minimization problems\u201d, in Signal Processing with Adaptive Sparse Structured Representations (SPARS workshop)", "author": ["J. Duan", "C. Soussen", "D. Brie", "J. Idier"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Multipath matching pursuit", "author": ["S. Kwon", "J. Wang", "B. Shim"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "The Viterbi algorithm for subset selection", "author": ["S. Maymon", "Y. Eldar"], "venue": "IEEE Signal Process. Lett., vol. 22,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Numerical solutions by the continuation method", "author": ["E. Wasserstrom"], "venue": "SIAM Rev., vol. 15,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1973}, {"title": "Homotopy continuation for sparse signal representation", "author": ["D.M. Malioutov", "M. \u00c7etin", "A.S. Willsky"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic  l0minimization", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Trans. Medical Imaging,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "A fast approach for overcomplete sparse decomposition based on smoothed l norm", "author": ["G.H. Mohimani", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "SparseNet: Coordinate descent with nonconvex penalties", "author": ["R. Mazumder", "J.H. Friedman", "T. Hastie"], "venue": "J. Acoust. Soc. Amer.,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Practical approximate solutions to linear operator equations when the data are noisy", "author": ["G. Wahba"], "venue": "SIAM J. Num. Anal., vol. 14, no. 4, pp. 651\u2013667", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1977}, {"title": "Generalized cross-validation as a method for choosing a good ridge parameter", "author": ["G.H. Golub", "M. Heath", "G. Wahba"], "venue": "Technometrics, vol. 21,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1979}, {"title": "On the relation between sparse reconstruction and parameter estimation with model order selection", "author": ["C.D. Austin", "R.L. Moses", "J.N. Ash", "E. Ertin"], "venue": "IEEE J. Sel. Top. Signal Process.,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Improved iteratively reweighted least squares for unconstrained smoothed lq minimization", "author": ["M.-J. Lai", "Y. Xu", "W. Yin"], "venue": "SIAM J. Num. Anal.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Acoust. Soc. Amer., vol. 101,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Sparse coloured system identification with guaranteed stability", "author": ["A.J. Seneviratne", "V. Solo"], "venue": "IEEE Conference on Decision and Control,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Robust-sl0 for stable sparse representation in noisy settings", "author": ["A. Eftekhari", "M. Babaie-Zadeh", "C. Jutten", "H.A. Moghaddam"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "lq sparsity penalized linear regression with cyclic descent", "author": ["G. Marjanovic", "V. Solo"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Sparse channel estimation of mimo-ofdm systems with unconstrained smoothed l0-norm-regularized least squares compressed sensing", "author": ["X. Ye", "W.-P. Zhu", "A. Zhang", "J. Yan"], "venue": "EURASIP J. Wireless Comm. and Networking,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Superresolution of noisy band-limited data by data adaptive regularization and its application to seismic trace inversion", "author": ["N. Saito"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1990}, {"title": "A novel spike distance", "author": ["M.C. van Rossum"], "venue": "Neural Computation,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2001}, {"title": "Multivariate adaptive regression splines", "author": ["J.H. Friedman"], "venue": "Ann. Statist.,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "They are known to be NP-hard except for specific cases [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "is worth being considered, where \u03bb expresses the trade-off between the quality of approximation and the sparsity level [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "At each iteration, a new atom is appended to the current subset, therefore gradually improving the quality of approximation [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Greedy algorithms include, by increasing order of complexity: Matching Pursuit (MP) [5], Orthogonal Matching Pursuit (OMP) [6], and Orthogonal Least", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "Squares (OLS) [7], also referred to as forward selection in statistical regression [8] and known as Order Recursive Matching Pursuit (ORMP) [9] and Optimized Orthogonal Matching Pursuit (OOMP) [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 8, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Popular thresholding algorithms include Iterative Hard Thresholding [11], Subspace Pursuit [12] and CoSaMP [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Forward-backward algorithms include the so-called stepwise regression algorithms which are OLS extensions [8], [15], [16], and OMP based algorithms of lower complexity [14], [17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": ", [18], [19] and [20]\u2013[27] for convex (l1) and nonconvex relaxation, respectively.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]\u2013[30], a forward-backward greedy search whose complexity is close to that of OMP.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "It is noticeable that BPDN leads to stepwise algorithms [18], [28] including the popular l1-homotopy [28]\u2013[30], a forward-backward greedy search whose complexity is close to that of OMP.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "It is referred to as \u201cLARS with the LASSO modification\u201d in [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "2 is a classical bi-objective representation where each axis is related to a single objective [31], namely |S| and E(S).", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "In bi-objective optimization, a point S is called Pareto optimal when no other point S\u2032 can decrease both objectives [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "On the contrary, the non-supported solutions cannot [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "\u2016x\u20161 \u2264 t for all t [33].", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Now, the weighted sum formulation (3) may not yield the same solutions as the constrained formulations (1) and (2) because the l0-norm is nonconvex [2].", "startOffset": 148, "endOffset": 151}, {"referenceID": 17, "context": "Many authors actually discourage the direct optimization of J because there are a very large number of local minimizers [20], [23].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "3) Positioning with respect to other stepwise algorithms: In statistical regression, the word \u201cstepwise\u201d originally refers to Efroymson\u2019s algorithm [15], proposed in 1960 as an empirical extension of forward selection (i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "Other stepwise algorithms were proposed in the 1980\u2019s [8, Chapter 3] among which Berk\u2019s and Broersen\u2019s algorithms [16], [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "Other stepwise algorithms were proposed in the 1980\u2019s [8, Chapter 3] among which Berk\u2019s and Broersen\u2019s algorithms [16], [34].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Recent stepwise algorithms were designed as either OMP [14], [17] or OLS extensions [35], [36].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "CSBR and l0-PD both read as descent algorithms in different senses: CSBR, first sketched in [37], repeatedly minimizes J (x;\u03bb) for decreasing \u03bb\u2019s.", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": "The idea of maintaining a list of support candidates was recently developed within the framework of forward selection [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 31, "context": "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "In contrast, the supports in the list are all candidate solutions to solve the same problem in [38], [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "tuning some continuous hyperparameter [40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "BPDN is solved for decreasing hyperparameter values using the solution for each value as a warm starting point for the next value [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 23, "context": "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "l1-homotopy [28], [30], [41] exploits that the l1 regularization path is piecewise affine and tracks the breakpoints between consecutive affine pieces.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.", "startOffset": 70, "endOffset": 74}, {"referenceID": 36, "context": "Second, the continuous approximation of the (discrete) l0 pseudo-norm [42] using a Graduated Non Convexity (GNC) approach [43]: a series of continuous concave metrics is considered leading to the resolution of continuous optimization problems with warm start initialization.", "startOffset": 122, "endOffset": 126}, {"referenceID": 37, "context": "Although the full reconstruction of the l0-regularization path has been rarely addressed, it is noticeable that a GNC-like approach, called SparseNet, aims to gradually update some estimation of the regularization path induced by increasingly non-convex sparsity measures [44].", "startOffset": 272, "endOffset": 276}, {"referenceID": 26, "context": "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "Because the influence of the grid is critical [33], it is useful to adapt the grid while the nonconvex measure is modified [44].", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "The \u03bb-values are rather adaptively computed similar to the l1-homotopy principle [28], [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "In the following, the formulation (5) will be omitted because it leads to the same l0-regularization path as formulation (4) [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Therefore, both l0-regularization paths may not coincide [2], [31].", "startOffset": 57, "endOffset": 60}, {"referenceID": 24, "context": "Therefore, both l0-regularization paths may not coincide [2], [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "SBR coincides with the well-known OLS algorithm [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 23, "context": "Our continuation strategy is inspired by l1-homotopy which recursively computes the minimizers of \u2016y\u2212Ax\u20162 +\u03bb\u2016x\u20161 when \u03bb is continuously decreasing [28]\u2013[30].", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "The second are cross-validation criteria [48], [49].", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "The second are cross-validation criteria [48], [49].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].", "startOffset": 188, "endOffset": 191}, {"referenceID": 40, "context": "The sparse approximation framework allows one to derive simplified expressions of the latter up to the storage of intermediate solutions of greedy algorithms for consecutive cardinalities [8], [47], [50].", "startOffset": 199, "endOffset": 203}, {"referenceID": 41, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 209, "endOffset": 213}, {"referenceID": 42, "context": "Among the popular nonconvex algorithms, we consider: 1) Iterative Reweighted Least Squares (IRLS) for lq minimization, q < 1 [52]; 2) Iterative Reweighted l1 (IRl1) coupled with the penalty log(|xi|+ \u03b5) [20], [23], [53];", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "3) l0 penalized least squares for cyclic descent (L0LS-CD) [54]; 4) Smoothed l0 (SL0) [43], [55].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.", "startOffset": 109, "endOffset": 113}, {"referenceID": 45, "context": "Moreover, the cyclic descent approach is becoming very popular in the recent sparse approximation literature [44], [56] although its speed of convergence is sensitive to the quality of the initial solution.", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "1 as suggested in [52].", "startOffset": 18, "endOffset": 22}, {"referenceID": 36, "context": "The basic SL0 implementation is dedicated to noise-free problems [43].", "startOffset": 65, "endOffset": 69}, {"referenceID": 44, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 53, "endOffset": 57}, {"referenceID": 46, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 59, "endOffset": 63}, {"referenceID": 47, "context": "There exist several adaptations in the noisy setting [55], [57] including the precursory work [58].", "startOffset": 94, "endOffset": 98}, {"referenceID": 46, "context": "We chose the efficient implementation of [57] in which the original pseudo-inverse calculations are replaced by a quasi-Newton strategy using limited memory BFGS updates.", "startOffset": 41, "endOffset": 45}, {"referenceID": 42, "context": "We have tested two l1 solvers: the incrowd algorithm [59] together with an empirical setting of \u03b5 > 0, and l1 homotopy in the limit case \u03b5 \u2192 0, following [53].", "startOffset": 154, "endOffset": 158}, {"referenceID": 36, "context": "For SL0, we have followed the default setting of [43] for the rate of deformation of the nonconvex penalty.", "startOffset": 49, "endOffset": 53}, {"referenceID": 48, "context": "More sophisticated localization tests are non binary and would take into account the distance between the location of the true spikes and their wrong estimates [60].", "startOffset": 160, "endOffset": 164}, {"referenceID": 49, "context": "The generalized version [3] is inspired from the regression spline modeling in [61].", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "Extensions of OMP and OLS were recently proposed in this spirit [36] and deserve consideration for proposing efficient forward-backward algorithms.", "startOffset": 64, "endOffset": 68}], "year": 2015, "abstractText": "Sparse signal restoration is usually formulated as the minimization of a quadratic cost function \u2016y\u2212Ax\u201622, where A is a dictionary and x is an unknown sparse vector. It is well-known that imposing an l0 constraint leads to an NP-hard minimization problem. The convex relaxation approach has received considerable attention, where the l0-norm is replaced by the l1-norm. Among the many efficient l1 solvers, the homotopy algorithm minimizes \u2016y \u2212Ax\u2016 2 + \u03bb\u2016x\u20161 with respect to x for a continuum of \u03bb\u2019s. It is inspired by the piecewise regularity of the l1-regularization path, also referred to as the homotopy path. In this paper, we address the minimization problem \u2016y \u2212Ax\u2016 2 + \u03bb\u2016x\u20160 for a continuum of \u03bb\u2019s and propose two heuristic search algorithms for l0-homotopy. Continuation Single Best Replacement is a forward-backward greedy strategy extending the Single Best Replacement algorithm, previously proposed for l0-minimization at a given \u03bb. The adaptive search of the \u03bb-values is inspired by l1-homotopy. l0 Regularization Path Descent is a more complex algorithm exploiting the structural properties of the l0-regularization path, which is piecewise constant with respect to \u03bb. Both algorithms are empirically evaluated for difficult inverse problems involving ill-conditioned dictionaries. Finally, we show that they can be easily coupled with usual methods of model order selection. This work was carried out in part while C. Soussen was visiting IRCCyN during the academic year 2010-2011 with the financial support of CNRS. C. Soussen and D. Brie are with the Universit\u00e9 de Lorraine and CNRS at the Centre de Recherche en Automatique de Nancy (UMR 7039).Campus Sciences, B.P. 70239, F-54506 Vand\u0153uvre-l\u00e8s-Nancy, France. Tel: (+33)-3 83 59 56 43, Fax: (+33)-3 83 68 44 62. E-mail: charles.soussen@univ-lorraine.fr, david.brie@univ-lorraine.fr. J. Idier is with L\u2019UNAM Universit\u00e9, Ecole Centrale Nantes and CNRS at the Institut de Recherche en Communications et Cybern\u00e9tique de Nantes (UMR 6597), 1 rue de la No\u00eb, BP 92101, F-44321 Nantes Cedex 3, France. Tel: (+33)-2 40 37 69 09, Fax: (+33)-2 40 37 69 30. E-mail: jerome.idier@irccyn.ec-nantes.fr. J. Duan was with CRAN. He is now with the Department of Biomedical Engineering, Xi\u2019an Jiaotong University. No. 28, Xianning West Road, Xi\u2019an 710049, Shaanxi Province, China. Tel: (+86)-29-82 66 86 68, Fax: (+86)-29 82 66 76 67. E-mail: junbo.duan@mail.xjtu.edu.cn. March 19, 2015 DRAFT", "creator": "LaTeX with hyperref package"}}}