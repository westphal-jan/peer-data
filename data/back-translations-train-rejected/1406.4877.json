{"id": "1406.4877", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "On the Application of Generic Summarization Algorithms to Music", "abstract": "Several generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization's performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier's performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing.", "histories": [["v1", "Wed, 18 Jun 2014 20:10:22 GMT  (12kb)", "http://arxiv.org/abs/1406.4877v1", "12 pages, 1 table; Submitted to IEEE Signal Processing Letters"]], "COMMENTS": "12 pages, 1 table; Submitted to IEEE Signal Processing Letters", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SD", "authors": ["francisco raposo", "ricardo ribeiro", "david martins de matos"], "accepted": false, "id": "1406.4877"}, "pdf": {"name": "1406.4877.pdf", "metadata": {"source": "CRF", "title": "On the Application of Generic Summarization Algorithms to Music", "authors": ["Francisco Raposo"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of us are able to play by the rules that they have imposed on themselves and are able to play by the rules that they have imposed on themselves."}, {"heading": "II. SUMMARIZATION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "A. Average Similarity", "text": "This method was introduced in [6] and later applied in other research efforts such as [7].The method consists of creating a similarity matrix for the song and calculating an aggregated similarity measurement between the entire song and each L-second long segment.In [6] 45 Mel Frequency Cepstral Coefficient (MFCC) s are calculated, but only the 15 with the highest variance are retained.The cosine distance is used to calculate pair similarities. In [7] the first 13 MFCCs and the spectral gravity center of gravity (sound \"brightness\") are used, but only the 15 with the highest variance are retained. Cosmic distance is used to calculate pair similarities."}, {"heading": "B. Maximal Marginal Relevance", "text": "MMR [17] selects sentences from the signal according to their relevance and diversity compared to the sentences already selected in order to produce low-redundancy summaries, an approach used in the language summary [14], [15]. It is a query-specific summary method, where generic summaries are possible by assuming the centric vector of all sentences (as in [15]) as a query. MMR iteratively selects the sentence Si, which maximizes the following mathematical model: Si are the unselected sentences and Sj are the previously selected ones; Q is the query and \u03bb is a configurable parameter that allows the selection of the next sentence based on its relevance and diversity (quency) and frequency."}, {"heading": "C. LexRank", "text": "LexRank [10] is a centrality-based method based on the similarity for each set pair. This centrality-based method is based on Google's PageRank [12] algorithm for ranking web pages. The output is a list of ranked sentences from which we can extract the most central sentences to create a summary.First, we compare all sentences that are normally represented as TF-IDF vectors with each other using a measure of similarity. LexRank uses cosine-like similarity. After this step, we create a graph in which each sentence is a vertex and the edges between each sentence are generated according to their pair-wise similarity. Usually, the similarity value must be higher to produce an edge. LexRank can be used with both weighted and unweighted edges. Then, we perform the following calculation: VRank [VRank] is the highest edge (VRank) and Rank the highest edge (VRank) is a high one."}, {"heading": "D. Latent Semantic Analysis", "text": "SVD is used to reduce the dimensionality of an original matrix representation of the text. To perform an LSA-based text summary, we begin by constructing a T term by N-sentence matrix A. Each element of A, aij = LijGi, has two weight components: a local weight and a global weight. The local weight is a function of the number of times a term occurs in a particular sentence, and the global weight is a function of the number of sentences containing a specific term. When SVD is applied to matrix A, it results in a decomposition consisting of three matrices: U, a T-N matrix of left singular vectors (their columns); \u03a3, an N-diagonal matrix of singular values containing the first relevant matrix of singular values; and V T, an N-N matrix of singular values (their rows are used to ascend their series: V.K)."}, {"heading": "III. EXPERIMENTS", "text": "It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it is. (it.) It is. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It is. (it. (it.) It. (it. (it.) It is. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it. (it.) It. (it. (it.) It. (it. It. (it.) It. (it. (it. It.) is. (it. (it. (it.) It. (it. (it.) It is. (it.) It. It. (it. (it. (it.) It. (it. (it.) It is. (it. (it.) It is. (it. It. It. (it.) It. (it.) It is. (it. (it. It. (it.) It. It is. It is. It is. (it. (it.) It is. It is. (it. (it. It. It. It. It. (it. It."}, {"heading": "IV. RESULTS", "text": "We present only the most interesting results as we have tried many different parameter combinations for each algorithm. The Frame / Hop Size columns show the frame / Hop sizes in seconds, which can be interpreted as overlaps (e.g. the pair 0.5, 0.25 stands for frames of 0.5 s duration with a Hop Size of 0.25 s, which corresponds to a 50% overlap between frames).The classification accuracy of the results for the 30 contingent segments that represent the baseline are 95.8%, and 94% for the beginning, middle, and end sections, respectively, on Dataset 1 and 85.2%, 92%, and 90.4%, on Dataset Algorithms, which are successful in improving the classification performance on Dataset 1 (98.8% as maximum accuracy, the sections that are achieved with the frame size of 0.5 s)."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "We examined summaries using classifications for MMR, LexRank, and LSA in the music field. Further experiments should be undertaken to find a number of parameter combinations suitable for most music contexts. Future work will include testing other summary algorithms, other similarity metrics, other characteristic types, and other classifiers. Using Gaussian blending models could also help to find more \"natural\" vocabulary, and beat detection could be used to find better values for fixed segmentation."}], "references": [{"title": "Semantic Segmentation and Summarization of Music: Methods Based on Tonality and Recurrent Structure", "author": ["W. Chai"], "venue": "Signal Processing Magazine, IEEE, vol. 23, no. 2, pp. 124\u2013132, March 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing Popular Music via Structural Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Applications of Signal Processing to Audio and Acoustics, 2003 IEEE Workshop on., Oct 2003, pp. 127\u2013130.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward Automatic Music Audio Summary Generation from Signal Analysis", "author": ["G. Peeters", "A.L. Burthe", "X. Rodet"], "venue": "Proc. Intl. Conf. on Music Information Retrieval, 2002, pp. 94\u2013100.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Signal-based Music Structure Discovery for Music Audio Summary Generation", "author": ["G. Peeters", "X. Rodet"], "venue": "Proc. of the Intl. Computer Music Conf. (ICMC), 2003, pp. 15\u201322.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Music Summary Using Key Phrases", "author": ["S. Chu", "B. Logan"], "venue": "Hewlett-Packard Cambridge Research Laboratory, Cambridge MA 02139, Tech. Rep. CRL 2000/1, April 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic Music Summarization via Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. Int. Conf. Music Information Retrieval, 2002, M. Fingerhut, Ed., 2002, pp. 81\u201385.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic Music Summarization: A \u201dThumbnail\u201d Approach", "author": ["J. Glaczynski", "E. Lukasik"], "venue": "Archives of Acoustics, vol. 36, no. 2, pp. 297\u2013309, Jan. 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio Thumbnailing of Popular Music using Chroma-based Representations", "author": ["M.A. Bartsch", "G.H. Wakefield"], "venue": "IEEE Transactions on Multimedia, vol. 7, no. 1, pp. 96\u2013104, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic Fado Music Classification", "author": ["P.G. Antunes", "D.M. de Matos", "R. Ribeiro", "I. Trancoso"], "venue": "CoRR, 2014, arXiv:1406.4447.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research, vol. 22, no. 1, pp. 457\u2013479, Dec. 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "TextRank: Bringing Order into Texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Proc. of EMNLP 2004. Barcelona, Spain: Association for Computational Linguistics, July 2004, pp. 404\u2013411.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine", "author": ["S. Brin", "L. Page"], "venue": "Seventh Intl. World-Wide Web Conf. (WWW 1998), 1998. June 20, 2014  DRAFT  SUBMITTED TO IEEE SIGNAL PROCESSING LETTERS, VOL. X, NO. X  12", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving Diversity in Ranking using Absorbing Random Walks", "author": ["X. Zhu", "A.B. Goldberg", "J.V. Gael", "D. Andrzejewski"], "venue": "Proc. of NAACL HLT, pp. 97\u2013104, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Minimizing word error rate in textual summaries of spoken language", "author": ["K. Zechner", "A. Waibel"], "venue": "Proc. of the 1st North American chapter of the Association for Computational Linguistics conference. Stroudsburg, PA, USA: Association for Computational Linguistics, 2000, pp. 186\u2013193.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Extractive Summarization of Meeting Recordings", "author": ["G. Murray", "S. Renals", "J. Carletta"], "venue": "Proc. of the 9th European Conf. on Speech Communication and Technology, 2005, pp. 593\u2013596.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of the 24th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. New York, NY, USA: ACM, 2001, pp. 19\u201325.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of the 21st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval. New York, NY, USA: ACM, 1998, pp. 335\u2013336.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. of ISIM 2004, 2004, pp. 93\u2013100.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent Developments in openSMILE, the Munich Open-source Multimedia Feature Extractor", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "Proc. of the 21st ACM Intl. Conf. on Multimedia, 2013, pp. 835\u2013838.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Armadillo: An open source c++ linear algebra library for fast prototyping and computationally intensive experiments", "author": ["C. Sanderson"], "venue": "NICTA, Australia, Tech. Rep., October 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "MARSYAS: A Framework for Audio Analysis", "author": ["G. Tzanetakis", "P. Cook"], "venue": "Organised Sound, vol. 4, no. 3, pp. 169\u2013175, Dec. 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "MLPACK: A Scalable C++ Machine Learning Library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 801\u2013805, 2013. June 20, 2014  DRAFT", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Several algorithms to summarize music have been published [1]\u2013[8], mainly for popular music songs whose structure is repetitive enough.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Several algorithms to summarize music have been published [1]\u2013[8], mainly for popular music songs whose structure is repetitive enough.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "We evaluate the summarization\u2019s contribution by comparing the performance of a Portuguese music style Fado Genre Classifier [9] using the extracted summaries of the songs against using contiguous clips (truncated from the beginning, middle and end of the song).", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "LexRank [10] and TextRank [11] are centrality-based methods that rely on the similarity between every sentence.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "LexRank [10] and TextRank [11] are centrality-based methods that rely on the similarity between every sentence.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "These are based on Google\u2019s PageRank [12] algorithm for ranking web pages and are successfully applied in text summarization.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "GRASSHOPPER [13] is another method applied in text summarization, as well as social network analysis, focusing on improving diversity in ranking sentences.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "MMR [14], [15], applied in speech summarization, is a query-specific method that selects sentences", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "MMR [14], [15], applied in speech summarization, is a query-specific method that selects sentences", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "LSA [16] is another", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "[1] presents two approaches for segmentation: using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In [2], segmentation is achieved by correlating a Gaussian-tempered \u201ccheckerboard\u201d kernel along the main diagonal of the similarity matrix of the song, outputting segment boundaries.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [3], [4], songs are segmented in 3 stages.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3], [4], songs are segmented in 3 stages.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "[5] groups (based on the Kullback-Leibler (KL) divergence) and labels similar segments of the song and then the summary is generated by taking the longest sequence of segments belonging to the same cluster.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In [6], [7], a method called Average Similarity is used to extract a thumbnail L seconds long that is most similar to the whole piece.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [6], [7], a method called Average Similarity is used to extract a thumbnail L seconds long that is most similar to the whole piece.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Another method for this task is the Maximum Filtered Correlation [8] which starts by building a similarity matrix and then a filtered time-lag matrix, which has the similarity between extended segments embedded in it.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "This method was introduced in [6] and later used in other research efforts such as [7].", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "This method was introduced in [6] and later used in other research efforts such as [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "In [6], 45 Mel Frequency Cepstral Coefficient (MFCC)s are computed but only the 15 with highest variance are kept.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the first 13 MFCCs and the spectral centre of gravity (sound \u201cbrightness\u201d) are used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The evaluations of this method in the literature are subjective (human) evaluations that take into account whether the generated summaries include the most memorable part(s) of the song [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": "are averages of scores given by test subjects, regarding specific qualities of the summary such as Clarity, Conciseness and Coherence [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "MMR [17], selects sentences from the signal according to their relevance and to their diversity against the already selected sentences in order to output low-redundancy summaries.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "This approach has been used in speech summarization [14], [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "This approach has been used in speech summarization [14], [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "It is a query-specific summarization method, though it is possible to produce generic summaries by taking the centroid vector of all the sentences (as in [15]) as the query.", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "LexRank [10] is a centrality-based method that relies on the similarity for each sentence pair.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "This centrality-based method is based on Google\u2019s PageRank [12] algorithm for ranking web pages.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "LSA is based on the mathematical technique SVD that was first used for text summarization in [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "In [18], two limitations of this approach are discussed: the fact that K is equal to the number of sentences in the summary, which, as it increases, tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included in the summary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "The classifier is a Support Vector Machine (SVM) [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "For feature extraction we used OpenSMILE\u2019s [20] implementation, namely, to extract MFCC feature vectors.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "We also used the Armadillo library [21] for matrix operations and the Marsyas library [22] for synthesizing the summaries.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "We also used the Armadillo library [21] for matrix operations and the Marsyas library [22] for synthesizing the summaries.", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "For each piece being summarized, we cluster all of its frames using the mlpack\u2019s [23] K-Means algorithm implementation which calculates the vocabulary for that song (i.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "We used Armadillo\u2019s [21] implementation of the SVD operation to implement LSA.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "We used MFCC vectors (of size 12) as features for these experiments, they are widely used in many MIR tasks including music summarization in [2], [5]\u2013[7].", "startOffset": 150, "endOffset": 153}], "year": 2014, "abstractText": "Several generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization\u2019s performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier\u2019s performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing.", "creator": "LaTeX with hyperref package"}}}