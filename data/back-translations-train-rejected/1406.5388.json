{"id": "1406.5388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Learning computationally efficient dictionaries and their implementation as fast transforms", "abstract": "Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a sparse representation. The sparser the representation, the better the dictionary. The resulting dictionary is in general a dense matrix, and its manipulation can be computationally costly both at the learning stage and later in the usage of this dictionary, for tasks such as sparse coding. Dictionary learning is thus limited to relatively small-scale problems. In this paper, inspired by usual fast transforms, we consider a general dictionary structure that allows cheaper manipulation, and propose an algorithm to learn such dictionaries --and their fast implementation-- over training data. The approach is demonstrated experimentally with the factorization of the Hadamard matrix and with synthetic dictionary learning experiments.", "histories": [["v1", "Fri, 20 Jun 2014 13:52:36 GMT  (202kb,D)", "https://arxiv.org/abs/1406.5388v1", null], ["v2", "Mon, 30 Jun 2014 18:47:09 GMT  (269kb,D)", "http://arxiv.org/abs/1406.5388v2", null], ["v3", "Thu, 26 Feb 2015 19:53:03 GMT  (529kb,D)", "http://arxiv.org/abs/1406.5388v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luc le magoarou", "r\\'emi gribonval"], "accepted": false, "id": "1406.5388"}, "pdf": {"name": "1406.5388.pdf", "metadata": {"source": "CRF", "title": "Learning computationally efficient dictionaries and their implementation as fast transforms", "authors": ["Luc Le Magoarou", "R\u00e9mi Gribonval"], "emails": ["luc.le-magoarou@inria.fr", "remi.gribonval@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it is all about one person who will be able to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to see the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world around the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see the world, to see, to see the world, to see the world, to see"}, {"heading": "2 Problem formulation and related works", "text": "In fact, it is so that most of us are able to adapt to the real situation in which they find themselves. (...) In the real world, it is so that most of us are able to live in the real world. (...) In the real world, it is so that people in the real world are not able to live in the real world. (...) In the real world, it is so that people in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world in the real world, in the real world in the real world, in the real world, in the real world, in the real situation, in the real world, in the real world in the real world, in"}, {"heading": "3 Optimization framework", "text": "In this section, we will explain the optimization problem under consideration and describe an algorithm that is guaranteed to converge to a stationary point of the objective function."}, {"heading": "3.1 Objective function", "text": "In order to learn a dictionary that is well adapted to the data, while it is quick to manipulate and cheap to store, we will minimize an objective function of the form of eq. (4). We will use as distance measurement the square Frobenius norm of difference d (X, VP + 1 j = 1 Sj): = 1 2 x X \u2212 1 x M + 1 j = 1 Sj \u00b2 2F and, as an economical search for penalties, some indicator functions of the sets of sparse matrices: gj: = \u03b4Ej, with \u03b4T (S) = 0 if S \u00b2 T and \u03b4T (S) = 1 otherwise. The avid reader may have noticed that this basic formulation of the object is invariant under relative scales of factors when the factual constraints themselves are invariant, and we deal with this problem. The choice of factual constraints is crucial because they completely determine the storage and multiplication costs of the learned dictionary."}, {"heading": "3.2 Algorithm overview", "text": "The formulation of the problem in eq. (6) is unfortunately highly non-convex, and the sparseness that pervades the part is non-smooth. Based on recent advances in non-convex optimization, we next propose an algorithm with convergence that guarantees a stationary point of the problem. In [6], the authors consider the cost functions depending on N blocks of variables of the form: (x1,. \u2212, xN): = H (x1,.,. \u00b7 N = 1 fj (xj), (7) where the function H is smooth, and the Fjs are correct and lower semi-continuous (the exact assumptions are given below). It should be stressed that no convergence of any kind is assumed. Here, for simplicity, we assume that the Fjs are indicator functions of constraints Tj."}, {"heading": "3.3 Algorithm details", "text": "It is quite easy to see that there is a correspondence between eq. (6) and eq. (7). (6). (6). (7)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.4 Practical strategy", "text": "However, there is no guarantee that the algorithm will converge to such a stationary point. Unfortunately, this is illustrated by a very simple experiment in which algorithm 2 is applied to a data matrix X = D with a known factorization in the M factors, while there is no guarantee that the algorithm will converge to such a stationary point. The naive approach is to take Q = M directly into algorithm 2 and set the constraints to reflect the actual thriftiness of the turf factors. This simple strategy performs rather poorly in practice, and the achieved local minimum is very often unsatisfactory (the data fidelity part of the objective function is large)."}, {"heading": "4 Experiments", "text": "In all experiments we consider square dictionaries and square factors."}, {"heading": "4.1 Learning a fast implementation of the Hadamard transform", "text": "Let us begin with a factorization experiment in the dictionary. Consider a data matrix X = D with a known factorization in M factors, D = VP M j = 1 Sj: In Section 3.4 we have highlighted the failure of algorithm 2 for this factorization problem. In contrast, Figure 2 illustrates the result of the proposed hierarchical strategy (algorithm 3) with the Hadamard dictionary in dimension n = 32. The factorization obtained is accurate and as good as the reference in terms of complexity savings in Figure 4. The runtime is less than one second. Factorizing the Hadamard matrix in dimension up to n = 1024 showed identical performance, with the runtime O (n2) being up to ten minutes."}, {"heading": "4.2 Learning computationally efficient dictionaries", "text": "We test Algorithm 3 in a more realistic framework for a dictionary learning problem with synthetic data, and compare it to classic learned dictionaries and analytical dictionaries in terms of approximate quality and relative complexity."}, {"heading": "5 Conclusion", "text": "Building on recent advances in nonconvex optimization, we have developed an algorithm with a convergence guarantee to learn efficient dictionaries and experimentally demonstrated its ability to create trade-offs between complexity and accuracy that modern dictionary learning methods have not been able to achieve. Besides the obvious need to further test the approach to real data and redundant dictionaries, and to better understand the role of its parameters in controlling the desired goal, a particular challenge is to use the acquired complexity to speed up the learning process itself in order to learn efficient dictionaries efficiently."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the PLEASE project of the European Research Council (ERC-StG2011-277906) and the authors thank Franc ois Malgouyres and Olivier Chabiron for the fruitful discussions that contributed to this work."}, {"heading": "A Factorizations examples", "text": "In this appendix we show that the matrices of the usual transformations connected with fast algorithms can be factored in sparse factors.A.1 The discrete Fourier transformation We will look at the DFT matrix in dimension 8 and show that it can be factored in sparse matrices. (The discrete Fourier transformation We will look at the DFT matrix in dimension 8: C = W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W"}], "references": [{"title": "Dictionaries for Sparse Representation Modeling", "author": ["Ron Rubinstein", "A.M. Bruckstein", "Michael Elad"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "An algorithm for the machine calculation of complex Fourier series", "author": ["James Cooley", "John Tukey"], "venue": "Mathematics of Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1965}, {"title": "A theory for multiresolution signal decomposition : the wavelet representation", "author": ["St\u00e9phane Mallat"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Double sparsity: learning sparse dictionaries for sparse signal approximation", "author": ["Ron Rubinstein", "Michael Zibulevsky", "Michael Elad"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Toward fast transform learning", "author": ["Olivier Chabiron", "Francois Malgouyres", "Jean-Yves Tourneret", "Nicolas Dobigeon"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Proximal alternating linearized minimization for nonconvex and nonsmooth problems", "author": ["J\u00e9r\u00f4me Bolte", "Shoham Sabach", "Marc Teboulle"], "venue": "Mathematical Programming,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Treelets - an adaptive multi-scale basis for sparse unordered data", "author": ["Ann B. Lee", "Boaz Nadler", "Larry Wasserman"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The sparse matrix transform for covariance estimation and analysis of high dimensional signals", "author": ["Guangzhi Cao", "L.R. Bachega", "C.A. Bouman"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On algorithms for sparse multi-factor NMF", "author": ["Siwei Lyu", "Xin Wang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Sparse matrix factorization", "author": ["Behnam Neyshabur", "Rina Panigrahy"], "venue": "CoRR, abs/1311.3315,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "CoRR, abs/1310.6343,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["Karol Gregor", "Yann LeCun"], "venue": "In Proceedings of the 27th Annual International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Learning efficient sparse and low rank models", "author": ["Pablo Sprechmann", "Alexander M. Bronstein", "Guillermo Sapiro"], "venue": "CoRR, abs/1212.3631,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Zhifeng Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein. K"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Efficient Implementation of the K- SVD Algorithm using Batch Orthogonal Matching Pursuit", "author": ["Ron Rubinstein", "Michael Zibulevsky", "Michael Elad"], "venue": "Technical report,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Dictionaries designed this way are called analytic dictionaries [1] (e.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Due to the relative simplicity of analytic dictionaries, they are often associated with a fast algorithm such as the Fast Fourier Transform (FFT) [2] or the Discrete Wavelet Transform (DWT) [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Due to the relative simplicity of analytic dictionaries, they are often associated with a fast algorithm such as the Fast Fourier Transform (FFT) [2] or the Discrete Wavelet Transform (DWT) [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 0, "context": "A survey on dictionaries, analytic or learned, can be found in [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? Such an objective can seem unrealistic, but in [4], and more recently in [5], the authors introduced new dictionary structures that seek to bridge the gap between the two categories.", "startOffset": 196, "endOffset": 199}, {"referenceID": 4, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? Such an objective can seem unrealistic, but in [4], and more recently in [5], the authors introduced new dictionary structures that seek to bridge the gap between the two categories.", "startOffset": 222, "endOffset": 225}, {"referenceID": 5, "context": "We will express this as an highly non-convex optimization problem, and rely on recent advances in optimization such as the PALM algorithm proposed in [6] to address it.", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "In [4], the authors propose to learn a dictionary which atoms are sparse linear combinations of atoms of a so-called base dictionary.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions with sparse kernels, so that the dictionary is fast to manipulate.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Recent representative works in this direction are [7] and [8].", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Recent representative works in this direction are [7] and [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "In [9], the authors introduce the sparse multi-factor NMF, that can be seen as modelling the data as in eq.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [10] and [11], the authors assume that the data come from a deep neural network, assuming that consecutive layers are sparsely connected and neglecting the non-linearities, they provide some strategies to recover the structure of the network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [10] and [11], the authors assume that the data come from a deep neural network, assuming that consecutive layers are sparsely connected and neglecting the non-linearities, they provide some strategies to recover the structure of the network.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Models addressing this problematic have been proposed in [12] and [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "Models addressing this problematic have been proposed in [12] and [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "In order to avoid scaling ambiguities, it is common [5, 9] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "In order to avoid scaling ambiguities, it is common [5, 9] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 52, "endOffset": 58}, {"referenceID": 5, "context": "In [6], the authors consider cost functions depending on N blocks of variables of the form:", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM)[6], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step.", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Since fM+2(\u03bb) = 0 is a convex penalty, it is enough to check (v) with a non-strict inequality [6], this leads to the update rule:", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "All methods involve a coefficient update step which is performed using Orthogonal Matching Pursuit (OMP) [14]:", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "\u2022 K-SVD [15], one of the most used algorithm that provides a learned dictionary.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "We use the implementation described in [16], running 300 iterations (which proved empirically sufficient to ensure convergence).", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "\u2022 Sparse K-SVD [4], a method that seeks to bridge the gap between learned dictionaries and analytic dictionaries.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "The implementation of [4] is used, with Dbase the Discrete Cosine Transform (DCT) matrix and 100 iterations, ensuring convergence in practice.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "The quality of approximation is expressed using the Root-Mean-Square Error (RMSE)[4, 15]: RMSE := 1 \u221a dn \u2016X\u2212D\u0393\u2016F .", "startOffset": 81, "endOffset": 88}, {"referenceID": 14, "context": "The quality of approximation is expressed using the Root-Mean-Square Error (RMSE)[4, 15]: RMSE := 1 \u221a dn \u2016X\u2212D\u0393\u2016F .", "startOffset": 81, "endOffset": 88}], "year": 2015, "abstractText": "Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a sparse representation. The sparser the representation, the better the dictionary. The resulting dictionary is in general a dense matrix, and its manipulation can be computationally costly both at the learning stage and later in the usage of this dictionary, for tasks such as sparse coding. Dictionary learning is thus limited to relatively small-scale problems. In this paper, inspired by usual fast transforms, we consider a general dictionary structure that allows cheaper manipulation, and propose an algorithm to learn such dictionaries \u2013and their fast implementation\u2013 over training data. The approach is demonstrated experimentally with the factorization of the Hadamard matrix and with synthetic dictionary learning experiments.", "creator": "LaTeX with hyperref package"}}}