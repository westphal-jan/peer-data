{"id": "1406.5600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2014", "title": "From conformal to probabilistic prediction", "abstract": "This paper proposes a new method of probabilistic prediction, which is based on conformal prediction. The method is applied to the standard USPS data set and gives encouraging results.", "histories": [["v1", "Sat, 21 Jun 2014 11:47:21 GMT  (13kb)", "http://arxiv.org/abs/1406.5600v1", "12 pages, 2 tables"]], "COMMENTS": "12 pages, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir vovk", "ivan petej", "valentina fedorova"], "accepted": false, "id": "1406.5600"}, "pdf": {"name": "1406.5600.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["volodya.vovk@gmail.com", "ivan.petej@gmail.com", "alushaf@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.56 00v1 [cs.LG] 2 1This paper proposes a new method of probabilistic prediction based on compliant prediction applied to the standard USPS dataset and provides encouraging results."}, {"heading": "1 Introduction", "text": "In fact, most people who are able, are able, are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2 Criteria of efficiency for label-conditional con-", "text": "Formal predictors and transducerLet X be a measurable space (the object space) and Y be a finite set with the discrete \u03c3-algebra (the label space); the observation space is defined to be Z: = X \u00b7 Y. A measure of conformity is a measurable function A which assigns to each sequence (z1,.., zn) an equally long sequence (\u03b11,.., \u03b1n) of real numbers and which is equivalent in terms of permutations: for each n and each permutation \u03c0 of {1,.., n}, (\u03b11,.,., \u03b1n) = A (z1,.,., zn) = (\u03b1 (1),."}, {"heading": "Four criteria of efficiency", "text": "Suppose, in addition to the educational sequence, that we obtain a test sequence and then want to measure the performance of a label-conditioned predictor or converter. As usual, we define the performance on the test set as the average performance (or, equivalent, the sum of the performances) based on the individual test observations. In the following [12] we will discuss the following four efficiency criteria for each test observation; all criteria will operate in the same direction: the smaller the better. \u2022 The sum of the y y y y y y y y y y y y y y y y of the p values; referred to as the S criterion. \u2022 The sum of the p values apart from the true designation: the OF (\"observed blur\") value is the N criterion. It is applicable to conform predictors (depending on the size of the device). \u2022 The sum of the p values apart from the true designation: the OF (\"observed blur\") value, this criterion is the one (the) level (the Y) that contains the Signification (the two), the Signification (the one), the Signification (the other) is the one."}, {"heading": "3 Optimal idealized conformity measures for a", "text": "Known probability distribution In this section we consider the idealized case in which the probability distribution Q = Q = Q = Q = Y = Y (Q = Q = Q = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y ="}, {"heading": "Four idealized criteria of efficiency", "text": "In this subsection, we will take the four criteria of efficiency that we discussed in the previous section to the idealized level of Q (Q = Q = any test sequences; since the sequences are infinite, they carry all the information about the data-generating distribution Q. We will write a uniform probability measurement for the interval [0, 1]. (An idealized conformity measurement A is: \u2022 S-optimal, if E (x, 4) x-optimal, if E (x, y). An idealized conformity measurement A is: \u2022 S-optimal, if E (x, 5)."}, {"heading": "4 Criteria of efficiency for probabilistic predic-", "text": "torsIn view of a training set (z1,.., zl) and a test object x, a probabilistic predictor prints a probability quantity (zl \u00b2), which is interpreted as its probabilistic prediction for the label y of x; we leave P (Y) for the set of all probability measures on Y. The two standard methods for measuring the performance of P on the actual label y are the logarithmic (or log) loss \u2212 lnP ({y}) and the Brier loss [y \u00b2]. Suppose we have a test sequence (zl \u00b2) \u2212 P ({y \u2032) 2, where 1E stands for the indicator of an event E: 1E = 0 when E happens and 1E = 0 otherwise. The efficiency of the probabilistic predictors is measured by these two loss functions."}, {"heading": "5 Calibration of p-values into conditional prob-", "text": "The argument of this section will be somewhat heuristic, and we will not try to formalize it in this work. Suppose that q: = P (y-x) has an absolutely continuous distribution with the density f if x-x is QX. (In other words, f is the density of the image of QX (q-x). (7) Algorithm type type probability forecast p (y-x). For the idealized conformity measurement of the CP we can rewrite (3) asp (q): = q value f (q-q) dq. \"/ D (7) Algorithm type type probability forecast p.\" Input: Training sequence (z1, zl) Input: Calibration sequence (xl + 1,."}, {"heading": "6 Experiments", "text": "In our experiments, we use the standard USPS dataset of handwritten digits. The size of the training set is 7291, and the size of the test set is 2007; however, instead of using the original split of the data into two parts, we randomly divide all available data (the union of the original training and test groups) into a training set of size 7291 and a test set of size 2007. (Therefore, our results are somewhat dependent on what the random generator uses, but the dependence is low and does not affect our conclusions at all.) We always report a powerful algorithm for the USPS dataset."}, {"heading": "Acknowledgments.", "text": "In our experiments we used the R-package e1071 (by David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, Friedrich Leisch, ChihChung Chang and Chih-Chen Lin) and the implementation of the tangential distance by Daniel Keysers. This work was partially supported by the EPSRC (scholarship EP / K033344 / 1, first author) and Royal Holloway, University of London (third author)."}], "references": [{"title": "A Course in Density Estimation", "author": ["Luc Devroye"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "On the theory of mortality", "author": ["Ulf Grenander"], "venue": "measurement. Part II. Skandinavisk Aktuarietidskrift,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1956}, {"title": "John E", "author": ["G.H. Hardy"], "venue": "Littlewood, and George P\u00f3lya. Inequalities. Cambridge University Press, Cambridge, England, second edition", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1952}, {"title": "Testing Statistical Hypotheses", "author": ["Erich L. Lehmann"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1986}, {"title": "Distribution free prediction bands for nonparametric regression", "author": ["Jing Lei", "Larry Wasserman"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Probabilities for SV machines", "author": ["John C. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Calibration of p-values for testing precise null hypotheses", "author": ["Thomas Sellke", "M.J. Bayarri", "James Berger"], "venue": "American Statistician,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Efficient pattern recognition using a new transformation distance", "author": ["Patrice Simard", "Yann LeCun", "John Denker"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A logic of probability, with application to the foundations of statistics (with discussion)", "author": ["Vladimir Vovk"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Conditional validity of inductive conformal predictors", "author": ["Vladimir Vovk"], "venue": "Technical Report arXiv:1209.2673 [cs.LG], arXiv.org e-Print archive,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Criteria of efficiency for conformal prediction, On-line Compression Modelling project (New Series), http://alrw.net", "author": ["Vladimir Vovk", "Valentina Fedorova", "Alex Gammerman", "Ilia Nouretdinov"], "venue": "Working Paper", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Algorithmic Learning in a Random World", "author": ["Vladimir Vovk", "Alex Gammerman", "Glenn Shafer"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Venn\u2013Abers predictors. Technical Report arXiv:1211.0025v2 [cs.LG], arXiv.org e-Print archive", "author": ["Vladimir Vovk", "Ivan Petej"], "venue": "To appear in the UAI 2014 Proceedings", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Probability estimates for multi-class classification by pairwise coupling", "author": ["Ting-Fan Wu", "Chih-Jen Lin", "Ruby C. Weng"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": ", [10], Sect.", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "9, and [7]) to conformal prediction: the p-values produced by conformal predictors will be transformed into probabilities.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "It was observed in [12] that some criteria of efficiency for conformal prediction (called \u201cprobabilistic criteria\u201d) encourage using the conditional probability Q(y | x) as the conformity score for an observation (x, y), Q being the data-generating distribution.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "Following [10] and [7], we will say that at this step we calibrate the p-values into probabilities,", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "Following [10] and [7], we will say that at this step we calibrate the p-values into probabilities,", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": ", [13], Chap.", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "6, or [14]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "This method does have a guaranteed property of validity (perhaps the simplest being Theorem 1 in [14]); however, the price to pay is that it outputs multiprobabilistic predictions rather than sharp probabilistic predictions.", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": ", [14], Sect.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "For a general discussion of conditionality in conformal prediction, see [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "Objectconditional conformal prediction has been studied in [5] (in the case of regression).", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": ", l + 1 | yi = y}| , (2) where \u03c4 is a random number distributed uniformly on the interval [0, 1] and the corresponding sequence of conformity scores is defined by (\u03b1y1 , .", "startOffset": 90, "endOffset": 96}, {"referenceID": 11, "context": "Following [12], we will discuss the following four criteria of efficiency for individual test observations; all the criteria will work in the same direction: the smaller the better.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "Equivalently, the S criterion can be defined as the arithmetic mean 1 |Y| \u2211 y\u2208Y p y of the p-values; the proof of Theorem 1 below will show that, in fact, we can replace arithmetic mean by any mean ([3], Sect.", "startOffset": 199, "endOffset": 202}, {"referenceID": 11, "context": "is known (as in [12]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "main result of this section, Theorem 1, is the label-conditional counterpart of Theorem 1 in [12]; the proof of our Theorem 1 is also modelled on the proof of Theorem 1 in [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "main result of this section, Theorem 1, is the label-conditional counterpart of Theorem 1 in [12]; the proof of our Theorem 1 is also modelled on the proof of Theorem 1 in [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 0, "context": "For each potential label y \u2208 Y for x define the corresponding label-conditional p-value as p = p(x, y) := Q({(x, y) | x \u2208 X & A((x, y), Q) < A((x, y), Q)}) QY({y}) + \u03c4 Q({(x, y) | x \u2208 X & A((x, y), Q) = A((x, y), Q)}) QY({y}) (3) (this is the idealized analogue of (2)), where QY is the marginal distribution of Q on Y and \u03c4 is a random number distributed uniformly on [0, 1].", "startOffset": 369, "endOffset": 375}, {"referenceID": 0, "context": "Let U be the uniform probability measure on the interval [0, 1].", "startOffset": 57, "endOffset": 63}, {"referenceID": 11, "context": "(Notice that this definition, being labelconditional, is different from the one given in [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "[4], Sect.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "where we have used the fact that p(x, y) is distributed uniformly on [0, 1] when ((x, y), \u03c4) \u223c Q\u00d7 U (see [13] and [12]).", "startOffset": 69, "endOffset": 75}, {"referenceID": 12, "context": "where we have used the fact that p(x, y) is distributed uniformly on [0, 1] when ((x, y), \u03c4) \u223c Q\u00d7 U (see [13] and [12]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "where we have used the fact that p(x, y) is distributed uniformly on [0, 1] when ((x, y), \u03c4) \u223c Q\u00d7 U (see [13] and [12]).", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "Indeed, for any significance level \u01eb, Ex,\u03c4 |\u0393 (x)| = E(x,y),\u03c4 |\u0393 (x) \\ {y}|+ (1\u2212 \u01eb), again using the fact that p(x, y) is distributed uniformly on [0, 1] and so P(x,y),\u03c4 (y \u2208 \u0393 (x)) = 1\u2212 \u01eb.", "startOffset": 147, "endOffset": 153}, {"referenceID": 13, "context": ", [14], (12)) and the standardized Brier loss coincides with the root mean square error (used in, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": ", [14], (13)).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": ", xl+k) \u2208 X k Input: test object x0 Output: probabilistic prediction P \u2208 P(Y) for the label of x0 for y \u2208 Y do for each xi in the calibration sequence find the p-value p y i by (2) (with l + i in place of l + 1) let gy be the antitonic density on [0, 1] fitted to p y l+1, .", "startOffset": 247, "endOffset": 253}, {"referenceID": 1, "context": "estimator (see [2] or, e.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": ", [1], Chap.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ") A powerful algorithm for the USPS data set is the 1-Nearest Neighbour (1NN) algorithm using tangent distance [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "On the other hand, there is a very natural and standard way of extracting probabilities from support vector machines, which we will refer to it as Platt\u2019s algorithm in this paper: it is the combination of the method proposed by Platt [6] with pairwise coupling [15] (unlike our algorithm, which is applicable to multi-class problems directly, Platt\u2019s method is directly applicable only to binary problems).", "startOffset": 234, "endOffset": 237}, {"referenceID": 14, "context": "On the other hand, there is a very natural and standard way of extracting probabilities from support vector machines, which we will refer to it as Platt\u2019s algorithm in this paper: it is the combination of the method proposed by Platt [6] with pairwise coupling [15] (unlike our algorithm, which is applicable to multi-class problems directly, Platt\u2019s method is directly applicable only to binary problems).", "startOffset": 261, "endOffset": 265}, {"referenceID": 12, "context": "There is a standard way of turning a distance into a conformal predictor ([13], Sect.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "We chose the polynomial kernel of degree 3 (since it is known to produce the best results: see [9], Sect.", "startOffset": 95, "endOffset": 98}], "year": 2014, "abstractText": "This paper proposes a new method of probabilistic prediction, which is based on conformal prediction. The method is applied to the standard USPS data set and gives encouraging results.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}