{"id": "1406.5675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions", "abstract": "Many kernel methods suffer from high time and space complexities, so they are prohibitive in big-data applications. To tackle the computational challenge, the Nystr\\\"om method has been extensively used to reduce time and space complexities by sacrificing some accuracy. The Nystr\\\"om method speedups computation by constructing an approximation of the kernel matrix in question using only a few columns of the matrix. Recently, a variant of the Nystr\\\"om method called the modified Nystr\\\"om method has demonstrated significant improvement over the standard Nystr\\\"om method in approximation accuracy, both theoretically and empirically.", "histories": [["v1", "Sun, 22 Jun 2014 05:02:06 GMT  (257kb,D)", "http://arxiv.org/abs/1406.5675v1", null], ["v2", "Thu, 15 Jan 2015 10:05:36 GMT  (533kb,D)", "http://arxiv.org/abs/1406.5675v2", null], ["v3", "Sat, 28 Mar 2015 07:43:49 GMT  (543kb,D)", "http://arxiv.org/abs/1406.5675v3", null], ["v4", "Sun, 11 Oct 2015 08:49:11 GMT  (814kb,D)", "http://arxiv.org/abs/1406.5675v4", null], ["v5", "Sun, 20 Dec 2015 09:47:37 GMT  (888kb,D)", "http://arxiv.org/abs/1406.5675v5", null], ["v6", "Fri, 20 May 2016 06:32:12 GMT  (348kb,D)", "http://arxiv.org/abs/1406.5675v6", "Journal of Machine Learning Research, 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shusen wang", "luo luo", "zhihua zhang"], "accepted": false, "id": "1406.5675"}, "pdf": {"name": "1406.5675.pdf", "metadata": {"source": "CRF", "title": "The Modified Nystro\u0308m Method: Theories, Algorithms, and Extension", "authors": ["Shusen Wang", "Zhihua Zhang"], "emails": ["wss@zju.edu.cn", "zhihua}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "In this thesis, we provide theoretical analysis, efficient algorithms, and a simple but highly precise extension of the modified Nystro-m method. First, we prove that the modified Nystro-m method is accurate under certain conditions, and we determine a lower error limit for the modified Nystro-m method. Second, we develop two efficient algorithms to make the modified Nystro-m method efficient and practicable under certain conditions. We design a simple column selection algorithm with a detectable error limit. With the selected columns at hand, we propose an algorithm that calculates the modified Nystro-m approach in less time complexity than the approach in the previous work. Third, the extension, which we call the SS-Nystro-Nystro-m method, exhibits a much stronger error binding than the modified Nystro-m method, especially when the spectrum of the nuclear matrix is slowly being calculated."}, {"heading": "1. Introduction", "text": "The kernel methods are important tools in machine learning, computer vision and data mining (Schoolhead and Smola, 2002, Shawe-Taylor and Christianini, 2004). However, many kernel methods require matrix calculations of high time and space complexity. Let's be the number of data instances, but the Gaussian process regression, kernel feedback, and least square feedback, all of which cost the inversion of some n-n matrices, time O (n3) and space O (n2); the kernel PCA, isomap, and laplacian eigenmaps all perform the abbreviated eigenvalue decomposition, which takes time, and space O (n2), where k is the target of decomposition. In addition to high time complexity, these matrix operations also have high spatial complexities and are difficult to implement."}, {"heading": "1.1 Our Contributions", "text": "Our papers cover three main aspects: theoretical analysis, computationally efficient algorithms, and extensions.1 They are summarized as follows: 1. An early version of the results in Section 4 and Section 5 is published in (Wang and Zhang, 2014); the ISS Nystro Method described in Section 6.2 is published in (Wang et al., 2014); an improved model - the SS Nystro m Method - is formulated and analyzed in this paper."}, {"heading": "1.1.1 Our Contributions: Theories", "text": "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) have already shown that the standard Nystro-m approach is correct precisely when the original nuclear matrix has a low rank. In Section 4.1, we show that the modified Nystro-m method accurately restores the original SPSD matrix under the same conditions. Furthermore, Wang and Zhang (2013) have demonstrated the lower error limits of the standard Nystro-m method. Similarly, in Section 4.2, we find a lower error limit for the modified Nystro-m method. The lower limit of the modified Nystro-m method bears a strong resemblance to the lower limit of the column selection problem, which is notoriously narrow, so we suspect that our established lower limit is narrow."}, {"heading": "1.1.2 Our Contributions: Algorithms", "text": "Although the modified Nystro-m method is more accurate than the standard Nystro-m method, the modified Nystro-m approach is more expensive to calculate. In this paper, we try to make the modified Nystro-m method efficient and practicable. In Section 5.1, we provide an efficient algorithm for calculating the intersection matrix of the modified Nystro-m method. Under certain conditions, this algorithm can significantly reduce time costs.In Section 5.2, we design a simple and efficient column selection algorithm for the modified Nystro-m method. We call it the uniform + adaptive2 algorithm. Our uniform + adaptive2 algorithm is more efficient and much easier to implement than the approximately optimal + adaptive algorithm by Wang and Zhang (2013), but its margin of error is comparable to the approximately optimal + adaptive algorithm."}, {"heading": "1.1.3 Our Contributions: Extension", "text": "The standard / modified Nystro-m methods produce minor approximations to kernel matrices, and their approximation errors can be no better than the rank-c truncated SVDs, where c is the number of columns selected by the Nystro-m methods. If the spectrum of a kernel matrix decays slowly (i.e., the c + 1 to n largest eigenvalues are not small enough), we propose in this paper a new method constructed by partial eigenvalue substitution or the standard / modified Nystro-m methods to make the approximation effective even if the spectrum of the original kernel matrix decays slowly. Unlike the standard / modified Nystro-m methods, which approach the kernel matrix K-Rn by low factization, we significantly improve the K-K-Ustro-K-K-USS-N method."}, {"heading": "1.2 Paper Organization", "text": "The rest of this paper is structured as follows: In Section 2, we define the notation to be used in the essay; in Section 3, we describe the Nystro \ufffd m approximation models, the column selection algorithm, and the applications to core methods; and then we present our work - theories, algorithms, and extensions - in Section 4, 5, and 6 respectively. Finally, in Section 7, we evaluate empirically the models and algorithms proposed in this essay; all the evidence is moved to the appendix."}, {"heading": "2. Notation", "text": "The notation used in this work follows that of Wang and Zhang (2013). For a m \u00b7 n matrix A = [aij], let a (i) be its i-th series, aj be its j-th column. (A) If we let the condensed singular value A (SVD) be 1 / 2 of its Frobenius norm, and b) A \u00b2 2 = maxx 6 = 0 \u00b2 Ax \u00b2 x x \u00b2 2 of its spectral norm. (A), let us write the condensed singular value of decomposition (SVD) of A = UA \u00b2 AV T A, where the (i) -th input of A \u2212 R\u03c1 \u00b2 x \u00b2 2 is its spectral norm. (A) Let us also let the intrinsic composition of A and VA, k \u00b2 be the first k (< B) column of UA and VA."}, {"heading": "3. The Nystro\u0308m Methods", "text": "In Section 3.1, we formally describe the Nystro \ufffd m approximation methods, including the standard Nystro \ufffd m method of Nystro \ufffd m (1930), Williams and Seeger (2001), the modified Nystro \ufffd m method of Wang and Zhang (2013), and the SS Nystro \ufffd m method proposed in this paper. Since the Nystro \ufffd m approximation is constructed using a small portion of columns, we will introduce some popular column sampling algorithms in Section 3.2, especially those with theoretical guarantees. In Section 3.3, we will discuss how to apply the Nystro \ufffd m methods to make kernel methods scalable."}, {"heading": "3.1 Models", "text": "Suppose we get a n \u00b7 n symmetric matrix K, our goal is to calculate a fast factorization of K so that the matrix inverse (K + \u03b1In) \u2212 1 and / or the eigenvalue substitution of K can be calculated highly efficiently. Nystrom methods solve this problem by approximating K with respect to a subset of its columns called C-Rn \u00b7 c. Without loss of generality, K and C can be defined as: K = [W KT21 K22] and C = [W K21], (1) where W is of size c \u00b7 c. Based on the above notation, three types of Nystrom approximation models are defined: In addition to the standard Nystrom mealto death, the modified / SS Nystro\u017em methods can be trivially applied to the Nystro\u017em Method group (Kumar et al., 2012), the SPSD sketching model (2013, the Kernel Method, and the Apatoni Method)."}, {"heading": "3.1.1 The standard Nystro\u0308m method", "text": "The (standard) Nystro method was proposed by Nystro-m (1930) and first introduced into the machine learning community by Williams and Seeger (2001). With the selected columns at hand, the standard Nystro method does not need to see the entire matrix K, and it only needs O (c3) + TMultiple (nc2) time and O (c2) space to calculate the intersection matrix Unys."}, {"heading": "3.1.2 The Modified Nystro\u0308m Method", "text": "The modified Nystro-M method is defined by K-Modc, CU-ModCT = C (C \u2020 K (C \u2020) T) CT, which is essentially the projection of K onto the column space of C and the row space of CT. This model was proposed by Wang and Zhang (2013) and is not necessarily the Nystro-M method, since it uses a different intersection matrix Umod, C \u2020 A (C \u2020) T. Although the selected columns are more expensive to calculate, the modified Nystro-M method only has to go through the data one pass, and the time and space costs are generally O (nc2) + TMultiple (n2c) and O (nc). Although the modified Nystro-M method is more expensive, it is a more accurate approximation. Since Umod = C \u2020 K (C \u2020) T is the minimizer of the optimization problem in U-K \u2212 UCT-de, the modified M method is a more accurate one."}, {"heading": "3.1.3 The SS-Nystro\u0308m Method", "text": "In this paper, we propose an extension of the modified Nystro-m method, which we call the modified Nystro-m method by spectral shift (SS-Nystro-m).The SS-Nystro-m approach of K is called K-ssc = C-U ssC-T + \u03b4ssIn.Here, the spectral shift of the term is called \"K-K.\" This approximation is calculated in three steps. First, the initial spectral shift of the term \"K\" = 1n-k (tr (K) -k-j = 1 \u03c3j (K), (3) and then the spectral shift \"K-K.\" In fact, the exact specification of the initial spectral shift of the term to \"K\" is unnecessary. Later in Section 6, we will show that the SS-Nystro-Uss-M method has a better upper margin of error than the modified Nystro-m method when the modified beginning of the term is K."}, {"heading": "3.2 Column Sampling Algorithms", "text": "The column selection problem has been widely studied in theoretical computer science (Boutsidis et al., 2011, Mahoney, 2011, Guruswami and Sinop, 2012) and the numerical linear algebra community (Gu and Eisenstat, 1996, Wang, 1999), and numerous algorithms have been developed and analyzed. Here, we focus on some theoretically guaranteed algorithms that have been studied in theoretical computer science. Much attention has been paid in previous work to improving the slit sampling algorithms so that the Nystro-m approach is more accurate. Uniform sampling is the simplest and most time-efficient sampling method, and it has detectable errors when applied to the standard Nystro method (Gittens, 2011, Kumar et al., 2012, Jin et al., 2013, Gittens and Mahoney, 2013). To improve approximation accuracy, many important sampling algorithms have been proposed that are adaptive among those."}, {"heading": "3.3 Applications to Kernel Methods", "text": "In this section we will discuss how to perform matrix inverse and eigenvalue decomposition using Nystro-M. Many core methods become scalable if the matrix inverse and eigenvalue decomposition can be efficiently solved. \u2022 Gaussian Process Regression (Williams and Seeger, 2001), least squares SVM (Suykens and Vandewalle, 1999), and kernel ridge regression (Saunders et al., 1998) all requisites of this type of linear system: (K + In) b = y, (6) which amounts to the matrix inverse problem b = (K + thox In) \u2212 1y. Here it is a constant. \u2022 Spectral Clustering (UCLkes et al, 2004, Li et al., 2011), Kernel PCA (Zhang and Kwok, 2010), and many manifold learning processes (Zhang et al, 2008, Talkar et al, 2013, we need to perform)."}, {"heading": "4. Theories", "text": "In Section 4.1, we show that the modified Nystro \ufffd m approach is accurate if K has a low rank. In Section 4.2, we show a lower error margin of the modified Nystro \ufffd m method."}, {"heading": "4.1 Theoretical Justifications", "text": "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) showed that the standard Nystrom method is accurate when rank (W) = rank (K). We provide a similar result for the modified Nystrom approximation in Theorem 2.Theorem 2 For a symmetrical matrix K defined in (1), the following three statements are equivalent: (i) rank (W) = rank (K), (ii) K = CW \u2020 CT, (iii) K = CC \u2020 K (C \u2020) TCT.Theorem 2 implies that the standard and modified Nystrom methods are equivalent when rank (W) = rank (K), i.e. the nuclear matrix K is lower. Generally, however, rank (K) c (K), rank (W) TCT. Theorem 2 implies that the standard and modified Nystrom methods are equivalent when rank (W) = rank (K), i.e. the nuclear matrix is lower (K)."}, {"heading": "4.2 Lower Error Bounds", "text": "We find a lower error limit of the modified Nystro-m method in Theorem 3. Theorem 3 shows that regardless of how a column sampling algorithm is used to construct the modified Nystro-m approach, at least c \u2265 2k \u2212 1 columns must be selected to reach the 1 + limit. Theorem 3 (Lower error limit of the modified Nystro-m method) Regardless of how a column sampling algorithm is used, there is an n \u00d7 n SPSD matrix K so that the error caused by the modified Nystro-m method obeys."}, {"heading": "5. Algorithms", "text": "In Section 5.1, we propose a quick approach to calculating the intersection matrices of the modified / SS Nystro \ufffd m methods. In Section 5.2, we design a simple and efficient column selection algorithm that is almost as accurate as the state-of-the-art Wang and Zhang algorithm (2013)."}, {"heading": "5.1 Fast Computation of the Intersection Matrices", "text": "In this section we propose a more efficient algorithm for calculating the intersection matrix, which only takes time O (c3) to change the processing method of (Ben-Israel and Greville, 2003, page 179).Theorem 4 For an n symmetrical matrix K, if the submatrix W is not singular, the intersection matrix of C with the modified Nystro method is Umod = C \u2020 K."}, {"heading": "5.2 An Efficient Column Sampling Algorithm", "text": "The algorithms consist of a uniform sampling step and two adaptive sampling steps, so we call the idea behind the uniform + adaptive2 algorithm quite intuitive. As the modified Nystro method is the simultaneous projection of K onto the column space of C and the row space of CT, the approximation error is lower if it is better approximated (C). After initialization by the uniform sampling method, the columns of K are far away (C1) have large residuals and are therefore likely to be chosen by the adaptive sampling method."}, {"heading": "6. The SS-Nystro\u0308m Approximation", "text": "In this section we propose a variant of the modified Nystro-m method, which is still effective when the spectrum of K slowly decays. We call the proposed method the modified Nystro-m () () () () Algorithm 3 The Modified Nystro-m by Spectral Shifting (SS-Nystro-m).1: Input: an n \u00b7 n SPSD matrix K, a target rank k, the oversampling parameter l. 2: / / approximately we calculate the initial spectral shift concept."}, {"heading": "6.1 Model Formulation", "text": "As discussed in Section 1.1.3, when the lowest eigenvalues of a kernel matrix are large, low matrix approximation methods - the standard / modified Nystro-M methods, and even partial eigenvalue decomposition - work poorly. To improve the accuracy of kernel approximation, Zhang (2014) proposed a kernel approximation model called matrix comb approximation (MRA). MRA approximation approaches each SPSD matrix using K \u2248 AAT + \u03b4In the case where A is an n \u00b7 c matrix and vice versa > 0 is the average of the lowest n \u2212 c eigenvalues. MRA-AAT + \u03b4In has a better state number than K, so it works well regardless of whether the lowest eigenvalues are large or small, but MRA is solved by an iterative algorithm, so it is not consistent."}, {"heading": "6.2 Error Analysis", "text": "It is not easy to analyze the theoretical error, so we have a variant of SS-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "6.3 Efficient Algorithm for Computing \u03b4\u0304", "text": "The SS-Nystro-m method uses manual as the initial spectral displacement concept. However, the calculation according to (14) requires partial eigenvalue substitution, which costs time O (n2k) and space O (n2). This can be accelerated by calculating the top-k singular values, which are roughly repeated with random projection techniques (Boutsidis et al., 2011, Halko et al., 2011). We present the algorithm for approximate calculation based on random projections in lines 2-6 of algorithm 3. The performance of the approximation will be defined in the following theorem 15 Nyo et al., 15 Let us define the spectral computing power in (14), k, l, n in algorithm 3. The following inequality applies in expectation: E [...] [...]. We can call the computing power in the native theorem [...] n."}, {"heading": "7. Experiments on Kernel Approximation", "text": "In this section, we compare empirically between the three Nystro approximation models: the Standard Nystro Method of Nystro \ufffd m (1930), Williams and Seeger (2001), the modified Nystro \ufffd m Method of Wang and Zhang (2013), and the SS Nystro \ufffd m Method proposed in this paper. In addition, we evaluate our Uniform + Adaptive2 Column Sample Algorithm proposed in Section 5.2; for comparison, the Uniform Sample and the Nearly Optimal + Adaptive Column Sample Algorithm of Wang and Zhang (2013) are used."}, {"heading": "7.1 The Setup", "text": "We conduct experiments on several data sets published by UCI (Frank and Asuncion, 2010) and Statlog (Michie et al., 1994), obtaining the data collected on the LIBSVM website, where the data is approximated to [0,1]. We summarize the data sets in Table 2. For each data set, we generate a radial base function (RBF) kernel matrix K, defined by kij = exp (\u2212 12\u03b3 xi \u2212 xj \u00b2 2). Here > 0 is the scaling parameter; the larger the scaling parameters, the faster the spectrum of the kernel decay (Gittens and Mahoney, 2013). Experience from previous work shows that for the same data sets, with different settings of the Nystro methods and the sampling algorithms we use, are very different."}, {"heading": "7.2 Comparisons among the Kernel Approximation Models", "text": "The experiments show that our SS-Nystro \ufffd m achieves the highest kernel approximation accuracy among the three Nystro \ufffd m approximation methods. For the kernel matrices with \u03b7 = 0.5, where the spectrum decays slowly and the lower eigenvalues are large, our SS-Nystro \ufffd m method is vastly more accurate than the standard / modified Nystro \ufffd m methods, which is in line with our theoretical analysis. If \u03b7 = 0.9, where the spectrum of the kernel matrix decays rapidly, our SS-Nystro \ufffd m method is still more accurate than the other two methods, but the advantage is not as obvious as the \u03b7 = 0.5 cases. In terms of runtime, our SS-Nystro \ufffd m is slightly slower than the modified Nystro \ufffd m, since SS-Nystro \ufffd m has to roughly calculate its time by random SVD, which costs time O (nk2) + T2k (as we have set it)."}, {"heading": "7.3 Comparisons among the Column Selection Algorithms", "text": "The empirical results in the figures show that our uniform + adaptive2 algorithm achieves an accuracy comparable to the state-of-the-art algorithm - the near-optimal + adaptive algorithm developed by Wang and Zhang (2013). In particular, when c is large, these two algorithms have practically the same accuracy, which is consistent with our analysis in the last paragraph of Section 5.2: large c implies a small error date, and the error limits of the two algorithms coincide when it is small. In terms of runtime, we can see that our uniform + adaptive2 algorithm performs column selection very efficiently and the elapsed time slowly grows into c. In comparison, our algorithm is much more efficient than the near-optimal + adaptive algorithm."}, {"heading": "8. Concluding Remarks", "text": "In this time we have the opportunity to embark on the search for new ways that will put us in a position to change the world and change the world, \"he told the German press agency in an interview with the German press agency.\" We have the opportunity to change the world, \"he told the German press agency.\" We have the feeling that the world in which we find ourselves is pushed into the background. \"He added:\" We have the feeling that the world in which we live, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the people, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the"}, {"heading": "Appendix A. Proof of Theorem 2", "text": "Suppose this rank (W) = rank (K) =. We have this rank (W) = rank (TB) = rank (K) = rank (K) \u2265 rank (W). (16) So there is a matrix X, so that [KT21 K22] = CXT = [WXT] = [WXTK21X T], and it follows that K21 = XW and K22 = K21X T = XWXT. Then we have this matrix X = [W) TXW XWXT] = [I X] W [I XT], (17) CW \u2020 T = [WXW] W = [WXW] W = [I X] W [I XT]. (18) Here follows the second equality in (18) TVW = W. We get this K = CW C. Then we show that K = CC \u2020 K (K) K (K) TCT (K)."}, {"heading": "Appendix B. Proof of Theorem 3", "text": "In section B.1 we provide several key lemmas, and then in section B.2 we prove theorem 3 with lemmas 19 and 18.B.1 key lemmas 17 provides a useful tool for extending the Moore-Penrose formula \u2212 \u2212 Moore \u2212 \u2212 \u2212 n of the distribution formula, and the problem is used to prove lemmas 19 and theorem 3.Lemma 17 (page 179 of Ben-Israel and Greville (2003). (page 179 of Ben-Israel and Greville (page 179 of Ben-Israel and Greville).)) Given a matrix X-Rm \u00b7 n in the order of at least c that has a non-insignificant c \u00b7 c submatrix X11. By rearranging columns and rows by permutation P and Q, the submatrix X11 can be bought into the upper left corner of X, that is, PXQ = [X11 X12 X21 X22 X22]. Then we will perform the Moore-Penrose inversion of X isc \u2020 TQ = TIT + 111 (TT] \u2212 IT \u2212 IQ."}, {"heading": "Appendix C. Proof of Theorem 4", "text": "The proof that C-Rm \u00b7 c consists of a subset of columns of K. By series permutation, C can be expressed as PC = [W K21]. Then, according to Lemma 17, the Moore-Penrose inverse of C can be written as C \u2020 = W \u2212 1 (Ic + S TS) \u2212 1 [Ic S T] P, where S = K21W \u2212 1. Then, the cross-section matrix of the modified Nystroem approach to K can be written as U = C \u2020 K (C \u2020) T = W \u2212 1 (Ic + S TS) \u2212 1 [Ic S T] PKPT [Ic + S TS) \u2212 1 W \u2212 1 W \u2212 1 (Ic + S TS) \u2212 1 (Ic + S TS) \u2212 1 [W KT21K21 K22] [Ic S + 1 TW = 1 (Ic + 1 W = 1 W = 1 W)."}, {"heading": "Appendix D. Proof of Theorem 7", "text": "The error analysis for the uniformity parameters A \u2212 K (2011), Gittens (2011), Gittens (2011), Gittens and Mahoney (2013), Troponey (2013), Troponey (2013), Troponey (2013), Troponey (2013), Troponey (2013), Troponey (2013), Troponey (2013), Troponey (2013), Camplingc (2013). \u2212 K \u2212 K (2013). \u2212 K (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), Gittens (2011), K (2011), Gittens (2011)."}, {"heading": "Appendix E. Proof of Theorem 9", "text": "In Section E.1, we derive the solution of the optimization problem (5). In Section E.2, we demonstrate that the resulting solution is SPSD if K is the SPSD.E.1 solution to the optimization problem. (5) We designate the objective function of the optimization problem (5) by f (U). (5) We designate the objective function of the optimization problem (5) by f (U). (5) We designate the objective function of the optimization problem (5) by f (U). (D). (C). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D. (D). (D). (D). (D. (D). (D. (D). (D. (D). (D. (D). (D). (D. (D. (D). (D. (D). (D. (D). (D. (D. (D). (D). (D. (D). (D. (D). (D. (D).). (D. (D. (D.). (D. (D. (D. (D). (D). (D). (D. (D. (D. (D). (D). (D. (D). (D). (D.). (D. (D).). (D."}, {"heading": "Appendix F. Proof of Theorem 10", "text": "Evidence: Since the right side of (13) is the convex and the right side is the minimizer of (13), the following applies to each (0): k + 1 (K) \u2212 \u03b4) 2 \u2264 n \u0441j = k + 1 (K) \u2212 0 2 = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k = k"}, {"heading": "Appendix G. Proof of Theorem 12", "text": "Proof The error made by SS-Nystro \ufffd m is that the sum of n eigenvalues of K \u00b2 2 is contained in the set {(K) + 1 (K) + 1 (K) + 1 (K) + 1 (K). Here, the inequality results from the property of the column selection algorithm Acol. The i-th largest eigenvalue of K \u00b2 is that the n eigenvalues of K \u00b2 2 are all contained in the set {(K) + 1 (K \u00b2). The sum of the smallest n \u2212 k of n eigenvalues of K \u00b2 must be smaller than \u2212 or equal to the sum of n \u00b2 (K) \u2212 K \u00b2 (K) \u2212 2 (K) = 1 \u00b2 (K \u00b2) (K \u00b2) = 1 \u00b2 (K \u00b2) (K \u00b2) = 1 \u00b2 K \u00b2 (K \u00b2) = 1 \u00b2 K \u00b2 (K \u00b2)."}, {"heading": "Appendix H. Proof of Theorem 15", "text": "The proof Let K = Q (QTK) k, where Q is defined in line 4 in algorithm 3. (Boutsidis et al. (2011) showed that the random Gaussian matrix (2011) follows from Lemma 23 that \"K \u2212 K \u2212 K\" 2F (1 + k / l) 2F, where \"K \u2212 K\" and \"K\" contain the singular values in descending order. (Since \"K\" has a ranking order of at most k, the entries of \"K \u2212 K \u2212 2\" are in vectors of length k and n \u2212 k: \"Johnson,\" where \"K\" contains the singular values in descending order. (Since \"K\" has a ranking order of at most k, the k + 1 to n entries of \"K \u2212 zero.\" We divide \"K \u2212 K\" into vectors of length k and n \u2212 k: \"Johnson\" K, \"where\" K \"contains the singular values in descending order of at most k,\" k, \"k\" K and K at most. \""}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Many kernel methods suffer from high time and space complexities, so they are prohibitive<lb>in big-data applications. To tackle the computational challenge, the Nystr\u00f6m method<lb>has been extensively used to reduce time and space complexities by sacrificing some<lb>accuracy. The Nystr\u00f6m method speedups computation by constructing an approximation<lb>of the kernel matrix in question using only a few columns of the matrix. Recently, a<lb>variant of the Nystr\u00f6m method called the modified Nystr\u00f6m method has demonstrated<lb>significant improvement over the standard Nystr\u00f6m method in approximation accuracy,<lb>both theoretically and empirically.<lb>In this paper we provide theoretical analysis, efficient algorithms, and a simple but<lb>highly accurate extension for the modified Nystr\u00f6m method. First, we prove that the<lb>modified Nystr\u00f6m method is exact under certain conditions, and we establish a lower error<lb>bound for the modified Nystr\u00f6m method. Second, we develop two efficient algorithms to<lb>make the modified Nystr\u00f6m method efficient and practical. We devise a simple column<lb>selection algorithm with a provable error bound. With the selected columns at hand,<lb>we propose an algorithm that computes the modified Nystr\u00f6m approximation in lower<lb>time complexity than the approach in the previous work. Third, the extension which we<lb>call the SS-Nystr\u00f6m method has much stronger error bound than the modified Nystr\u00f6m<lb>method, especially when the spectrum of the kernel matrix decays slowly. Our proposed<lb>SS-Nystr\u00f6m can be computed nearly as efficiently as the modified Nystr\u00f6m method.<lb>Finally, experiments on real-world datasets demonstrate that the proposed column selection<lb>algorithm is both efficient and accurate and that the SS-Nystr\u00f6m method always leads to<lb>much higher kernel approximation accuracy than the standard/modified Nystr\u00f6m method.<lb>", "creator": "LaTeX with hyperref package"}}}