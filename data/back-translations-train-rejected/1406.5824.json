{"id": "1406.5824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "VideoSET: Video Summary Evaluation through Text", "abstract": "In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.", "histories": [["v1", "Mon, 23 Jun 2014 07:56:23 GMT  (4078kb,D)", "http://arxiv.org/abs/1406.5824v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR", "authors": ["serena yeung", "alireza fathi", "li fei-fei"], "accepted": false, "id": "1406.5824"}, "pdf": {"name": "1406.5824.pdf", "metadata": {"source": "CRF", "title": "VideoSET: Video Summary Evaluation through Text", "authors": ["Serena Yeung", "Alireza Fathi", "Li Fei-Fei"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Previous Work", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "3 Evaluation Framework", "text": "In paragraph 3.1, we give an overview of VideoSET and describe how it can be used to evaluate video summaries. In paragraph 3.2, we then describe the video data sets for which we have received text comments and ground truth summaries that can be used in VideoSET. Finally, in paragraphs 3.2-3.5, we explain each component of the framework in detail: text comments, summaries, creating a text representation of a video summary, and sounding of the video summary."}, {"heading": "3.1 Overview of VideoSET", "text": "Fig. 2 gives an overview of VideoSET. A video is presented as a sequence of M-subshots V = {vi} Mi = 1, and a video summary is a subset of C-V of these subshots. A user constructs a video summary using a summary algorithm and provides it to VideoSET as input. VideoSET then generates a text representation T (C) of the summary using text comments from the original video. Text representation is compared to a set of text summaries G written by humans to determine the ideal semantic content of a video summary. We have released all the necessary text annotations and soil truth summaries for a number of video datasets. Comparison with the text summaries on the ground is performed using a scoring function (C, G) = max gi-G S (T (C), gi) (1), where S (x, y) is a function that measures the text summary on the ground and then used for GE."}, {"heading": "3.2 Datasets", "text": "We have published text commentary and basic truth commentary that can be used in VideoSET for two publicly available egocentric video datasets and four TV episodes. Each of these episodes is described in more detail below, and representative images and text commentary are shown in Fig. 3.The egocentric dataset of daily life [11] This dataset consists of 4 egocentric videos of 3-5 hours each. Each video records a topic through natural daily activities such as eating, shopping, and cooking. The videos were recorded with a Looxcie portable camera at 15 frames per second and 320 x 480 resolution. We provide text commentary and basic truth commentary for all videos in this dataset. Disneyworld egocentric dataset [4] This dataset consists of 8 egocentric videos of 6-8 hours each. Each video records a theme during a day at Disneyworld."}, {"heading": "3.3 Obtaining text annotations", "text": "We segmented egocentric videos from the records in Sec. 3.2 into 5-second subshots, and TV episodes into 10-second subshots. We then received 1-sentence descriptions of each subshot using Amazon's Mechanical Turk. Workers were asked to write a simple and factual sentence about what was happening in each subshot. They were instructed to write from the first-person perspective of past times for the egocentric videos, and from a third-person present perspective for the TV episodes. Workers who commented on the TV episodes had to be familiar with the episode and use the TV characters names in their descriptions. The descriptions were edited by additional workers for vocabulary and grammatical consistency. To select the subshot length, we first received text annotations for an egocentric video in 3, 5 and 10 seconds, and for a TV episode in the subversion."}, {"heading": "3.4 Obtaining ground-truth summaries", "text": "It is also easier for people to write down the information they think should be in a summary than to comb through a long video and select the ideal subshots. For example, it may be clear that a summary should show that the camera carrier was \"walking on the sidewalk.\" However, as the examples in Fig. 1 show, many visually different and equally good subshots can illustrate this, and it is unclear what should be included in a basic truth. We asked a small group of workers to write a summary in words about what was happening in each video. Workers were provided with the text annotations for the video so that a similar vocabulary could be used. They were asked to write simple sentences with a level of content similar to the text annotations. They were also asked to rank their sentences by importance. Then, during the evaluation process, a video summary of C."}, {"heading": "3.5 Generating the text representation of a video summary", "text": "Faced with a video summary C that needs to be evaluated, VideoSET first generates a text representation T (C) of the summary. This representation can be determined by concatenating the already existing text annotations (paragraph 3.3) to each summary partial representation, since the summary is a collection of partial images of the original video. We have published text annotations of the videos in paragraph 3.2 so that no effort is required on the part of the user and the process is illustrated in Figure 5."}, {"heading": "3.6 Scoring the video summary", "text": "To get the video summary, a similarity function S (x, y) is used to compare the text representation of the summary with the soil-truth-text summaries. We use the ROUGE-SU metric from the publicly available ROUGE toolbox [14]. ROUGE-SU measures unirams and skip-bigrams coexistence between a candidate and the soil-truth summary, after preprocessing, to contain words and remove stopwords. Skip-bigrams are arbitrary pairs of words in their sentence order that allow arbitrary gaps. For example, the skip bigrams for the phrase \"I went for a walk with my dog in the park,\" \"walked in the park,\" and \"dog park,\" where stopwords are not included. Unigrams and skip-bigrams are treated equally as counters."}, {"heading": "4 Experiments", "text": "To evaluate the effectiveness of VideoSET, we conducted two different experiments. In the first experiment, we generated a number of video summaries using existing video summaries and correlated their VideoSET results with human judgment. In the second experiment, we analyzed VideoSET's performance throughout the space of possible video summaries. We randomly sampled pairs of video summaries and subshots and compared VideoSET assessments with human judgment. To confirm our intuition that text distance as a measure of semantic similarity is more appropriate than visual distance, we also compared our experiments with a pixel-based distance metric."}, {"heading": "4.1 VideoSET evaluation of existing summarization methods", "text": "We generated video summaries using the following existing keyframe methods. 2 minutes of summaries (N = 24 subshots of egocentric video and N = 12 subshots for TV episodes) were generated using each method. 1 minute summaries of the results were generated using different methods. 2 minutes of video sequences were selected, each located in the middle of the individual N clusters. 2. Color histogram clustering: frames located in the middle of the individual N clusters were selected for the video summary. 3. Video MMR [12]: frames were extracted from the original video using video sequences. In each of the N iterations, a keyframe was selected that came closest to the frames in the middle of the individual N clusters, the frames were not yet selected as keyframes, and at the same time different from the previously selected keyframes. In other words, each of the keyframes was selected a keyframe that resembled the frames most similar to those not yet selected frames."}, {"heading": "4.2 VideoSET Evaluation of Randomly Sampled Summaries and Subshots", "text": "To better understand VideoSET's performance in the full space of possible summaries, we randomly sampled video summaries as well as subshots and compared VideoSET judgments with human judgments. We first randomly generated 100 pairs of 2-minute summaries (24 subshots) for a video in daily life of egocentric datasets. We asked two people to observe each pair of summaries and judgments that are semantically closer to a provided ground truth summary. In 40% of the comparisons, the two human judges disagree, suggesting that the difference was too ambiguous for humans. For the remaining 60% of the comparisons, we have automated judgments with VideoSES truth summaries."}, {"heading": "5 Conclusion", "text": "Our approach is based on generating a text representation of the video summary and measuring the semantic distance of the text to text summaries written by humans. Our experiments show that this approach correlates well with human judgment and outperforms pixel-based distance measurements. In addition, our framework can be expanded to evaluate any kind of video summary and accommodate future extensions of our semantic distance measurement."}, {"heading": "6 Acknowledgements", "text": "This research is supported in part by an ONR-MURI scholarship and an Intel gift, as well as a Stanford Graduate Fellowship at S.Y."}], "references": [{"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["G. Doddington"], "venue": "In Proceedings of the Second International Conference on Human Language Technology Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Automatically segmenting lifelog data into events", "author": ["A.R. Doherty", "A.F. Smeaton"], "venue": "In International Workshop on Image Analysis for Multimedia Interactive Services,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Key-frame selection to represent a video", "author": ["F. Dufaux"], "venue": "In IEEE International Conference on Multimedia and Expo,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Social interactions: A first-person perspective", "author": ["A. Fathi", "J.K. Hodgins", "J.M. Rehg"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Summarization system evaluation revisited: N-gram graphs", "author": ["G. Giannakopoulos", "V. Karkaletsis", "G. Vouros", "P. Stamatopoulos"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Schematic storyboarding for video visualization and editing", "author": ["D.B. Goldman", "B. Curless", "D. Salesin", "S.M. Seitz"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Automated summarization evaluation with basic elements", "author": ["E. Hovy", "C. Lin", "L. Zhou", "J. Fukumoto"], "venue": "In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Large-scale video summarization using web-image priors", "author": ["A. Khosla", "R. Hamid", "C-J Lin", "N. Sundaresan"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "An integrated scheme for object-based video abstraction", "author": ["C. Kim", "J. Hwang"], "venue": "In Proceedings of ACM Multimedia,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Discovering important people and objects for egocentric video summarization", "author": ["Y.J. Lee", "J. Ghosh", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Multi-video summarization based on av-mmr", "author": ["Y. Li", "B. Maerialdo"], "venue": "In ContentBased Multimedia Indexing (CBMI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Automatic evaluation of video summaries", "author": ["Y. Li", "B. Maerialdo. Vert"], "venue": "In ACM International Conference on Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C-Y. Lin"], "venue": "In Workshop on Text Summarization Branches Out (WAS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "A hierarchical visual model for video object summarization", "author": ["D. Liu", "G. Hua", "T. Chen"], "venue": "In PAMI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Story-driven summarization for egocentric video", "author": ["Z. Lu", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Recent advances in content-based video analysis", "author": ["C. Ngo", "H. Zhang", "T. Pong"], "venue": "In International Journal of Image and Graphics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T. Berg"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Key frame selection by motion analysis", "author": ["W. Wolf"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "An integrated system for content-based video retrieval and browsing", "author": ["H.J. Zhang"], "venue": "In Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Paraeval: Using paraphrases to evaluate summaries automatically", "author": ["L. Zhou", "C. Lin", "D. Munteanu", "E. Hovy"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}], "referenceMentions": [{"referenceID": 15, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 1, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 9, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "Most previous work has performed a diverse range of user comparison studies [17,2,11,16] ar X iv :1 40 6.", "startOffset": 76, "endOffset": 88}, {"referenceID": 11, "context": "that are difficult to replicate, while a few have used pixel-based comparison with a ground truth [13,9].", "startOffset": 98, "endOffset": 104}, {"referenceID": 7, "context": "that are difficult to replicate, while a few have used pixel-based comparison with a ground truth [13,9].", "startOffset": 98, "endOffset": 104}, {"referenceID": 19, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Methods for video summarization: Previous methods for video summarization have used low-level features such as color [21] and motion [20,6], or a combination of both [3].", "startOffset": 166, "endOffset": 169}, {"referenceID": 8, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 38, "endOffset": 45}, {"referenceID": 13, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 38, "endOffset": 45}, {"referenceID": 9, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 68, "endOffset": 75}, {"referenceID": 14, "context": "Some other works have modeled objects [10,15] and their interaction [11,16] to select key subshots.", "startOffset": 68, "endOffset": 75}, {"referenceID": 8, "context": "Kim and Hwang [10] segment the objects in video and use the distance between the objects for video summarization.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "[15] summarize a video by finding the frames that contain the object of interest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] find the important objects and people in egocentric video and select the events that contain them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Lu and Grauman [16] model video summarization as a story that relates frames to each other based on the objects they contain.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "[9] use web images as a prior to summarize user generated videos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 1, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 9, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 14, "context": "Techniques for evaluating video summaries: Most previous work evaluate the performance of their video summarization techniques using user studies [17,2,11,16].", "startOffset": 146, "endOffset": 158}, {"referenceID": 13, "context": "[15] measure the performance based on the presence of objects of interest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Li and Maerialdo [12] and Khosla et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "[9] use pixel-based distance of a summary to the original video for evaluation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Li and Maerialdo [13] introduce VERT, which evaluates video summaries given a ground-truth video summary by counting the number of sub-shots that overlap between the two.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The first techniques in NLP were created in order to evaluate the quality of text which had been machine translated from one language to another [1,19].", "startOffset": 145, "endOffset": 151}, {"referenceID": 17, "context": "The first techniques in NLP were created in order to evaluate the quality of text which had been machine translated from one language to another [1,19].", "startOffset": 145, "endOffset": 151}, {"referenceID": 12, "context": "Later on, Lin [14] introduced ROUGE for evaluating video summaries.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 20, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 4, "context": "There have been other more recent techniques for evaluating text summaries [8,22,5], but ROUGE still remains the standard evaluation algorithm.", "startOffset": 75, "endOffset": 83}, {"referenceID": 9, "context": "Daily life egocentric dataset [11] This dataset consists of 4 egocentric videos of 3-5 hours each.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Disneyworld egocentric dataset [4] This dataset consists of 8 egocentric videos of 6-8 hours each.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "Our annotations may also be of interest to researchers working in the intersection between images or video and text, similar to [18] and [7].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "TV#episodes# Disneyworld#egocentric#dataset#[4]# I#walked#around#a#market#and# looked#at#tents.", "startOffset": 44, "endOffset": 47}, {"referenceID": 9, "context": "# Daily#life#egocentric#dataset#[11]#", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "We use the ROUGE-SU metric from the publicly available ROUGE toolbox [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "The ROUGE toolbox is a collection of n-gram comparison metrics that measure text content similarity, and more detail can be found in [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Video-MMR [12]: Frames were extracted at 1fps from the original video.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Object-driven summarization [11]: The method of Lee et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "[11] chooses keyframes containing important people and objects based on a learned metric for importance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Daily life dataset [11] Disney dataset [4] TV episodes", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Daily life dataset [11] Disney dataset [4] TV episodes", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "summaries (24 subshots) for a video in the Daily life egocentric dataset [11].", "startOffset": 73, "endOffset": 77}], "year": 2014, "abstractText": "In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community.", "creator": "LaTeX with hyperref package"}}}