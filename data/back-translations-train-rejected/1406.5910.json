{"id": "1406.5910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions", "abstract": "Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.", "histories": [["v1", "Mon, 23 Jun 2014 14:06:24 GMT  (1860kb,D)", "http://arxiv.org/abs/1406.5910v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["roman shapovalov", "dmitry vetrov", "anton osokin", "pushmeet kohli"], "accepted": false, "id": "1406.5910"}, "pdf": {"name": "1406.5910.pdf", "metadata": {"source": "CRF", "title": "Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions", "authors": ["Roman Shapovalov", "Dmitry Vetrov", "Anton Osokin"], "emails": ["shapovalov@graphics.cs.msu.ru", "vetrovd@yandex.ru", "anton.osokin@gmail.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "It is one of the largest nodes in the world that has been in the United States in recent years to try to resolve questions that have arisen in recent years. It is one of the largest nodes that has ever existed to explore the world, and it is one of the largest nodes that has existed in recent years."}, {"heading": "2 Latent-variable SSVM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Structured-output learning", "text": "In the following we will consider only the mappings, which can be expressed as maximizing a discriminatory function (x, y), (1) where the vector function (x, y) depends linearly on its parameters, w: H (x) = argmax y (x, y; w) is defined in a problem-specific way (x, y), while the weights w are learned from the training data. We address a wide class of so-called labeling problems, where the structured marking is a vector of discrete variables: Y = KV, where K = {1,., K}. Their length V may vary for individual instances."}, {"heading": "2.2 Learning with weak annotations", "text": "Consider the case where, in addition to fully labeled objects, the pull set M contains weakly labeled objects: (xm, zm) N + Mm = N + 1. In the following, we assume that the weak note zm defines a subset of complete labels L (zm) and Y that are consistent with it, and thus zm is less informative than an individual complete label ym. Examples of such weak annotations for the image segmentation problem are (1) Bounding Boxes of segments of a given label; (2) a value for some global statistics (area, average intensity, number of connected components, etc.) for the segments of a given label; (3) subsets of superpixels belonging to a given label (seeds). We now generate the default SSVM formulation to make them both complete and weakly labeled."}, {"heading": "3 Weak annotation for semantic image segmentation", "text": "Semantic image segmentation is aimed at assigning category labels to image pixels. We assume that an image is represented as a series of superpixels, i.e. groups of pixels that look similar. Consider a graph G = (V, E), whose nodes V correspond to superpixels of the image. Edge group E represents a neighborhood system on V that includes the pairs of nodes that correspond to all adjacent superpixels. Let's leave xi Rd as a vector of superpixel characteristics associated with some nodes i, V."}, {"heading": "3.1 Image-level labels", "text": "We start with the definition of loss functions K (y, z) for some arbitrary names K (c). We assume that z (c) is a series of names that appear in the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the description of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the caption of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the description of the"}, {"heading": "3.2 Bounding boxes", "text": "We assume that within a particular image, each category can be defined either with image-level labels or with bounding boxes, although the type of annotation for a category can vary from picture to picture (see Section 4.3 for an example where it can be useful). We assume that within a particular image, each category can be defined either with image-level labels or with bounding boxes, although the type of annotation for a category can be varied from picture to picture (see Section 4.3 for an example where it can be useful)."}, {"heading": "3.3 Objects\u2019 seeds", "text": "Another form of weak annotation, which is natural for the object categories, is the annotation of the seed (fig. 1d). Generally, a seed for a segment of a category is a subset of its pixels. We consider a specific case in which only one pixel that may be near the segment center is labeled. During the annotation-consistent conclusion, we need the superpixel in which this point is located to have the fixed seed denomination. We now model the weak annotation z as a pair (zil, zos), where zos is a series of 2D dots with the corresponding denominations: (p, k). The assumption of seed centrality allows us to determine the Gaussian penalty for deriving a non-seed label in the vicinity of each seed, which leads us to the following loss function: kil-os (y) = label of the first image is equal to the second."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and metrics", "text": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23]. MSRC contains 276 training images and 256 test images, which are fully labeled with 23 categories; a substantial portion of the pixels remain unlabeled. SIFT-flow is a more sophisticated dataset: it is a subset of the LabelMe database [19], which contains 2488 training images and 200 test images; they are crowd-sourced into 33 categories. For MSRC, we get superpixels with the original implementation of the gPb edge detector [1]. The uncommon features are the following: a histogram of the SIFT [12] visual words created with a dictionary of size 512 by hard mapping the descriptors to the containers; we get superpixels with the original implementation of the gb edge detector [1]."}, {"heading": "4.2 Image-level labels", "text": "We generate weak annotations. We automatically obtain image-level labels from complete labels by enumerating the unique labels for each image. Each MSRC image typically shows one or more objects of a specific target category (e.g. \"characters,\" \"auto\") on any background. However, not every background category falls into the labels used, so it can be left without labels. Thus, some images contain only one category label. In this case, the image-level label clearly defines the complete label. To avoid this knowledge (unrealistic in the real world), we could model the \"other\" label that contains anything but the designated 23 categories. However, the labels usually have uncertain boundaries between segments of different labels, i.e. the boundaries are not labeled either. (Fig. 1b). If we model these boundaries as a separate category, it would violate segmentation performance."}, {"heading": "4.3 Adding bounding boxes and seeds", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "5 Conclusion", "text": "We introduced the framework for learning structural classification from different types of annotations by minimizing comment-specific loss functions; we applied it to semantic image segmentation by introducing weak loss functions for image plane, Bounding Box, and object seed annotations; and the use of poorly commented training data consistently improves labeling; and the results of the semantic segmentation data sets show that the common annotation, where background is given by imaginary labels and objects are given by Bounding Box, is the best compromise between segmentation quality and annotation effort."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbel\u00e1ez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Structured output learning with indirect supervision", "author": ["Chang", "M.-W", "V. Srikumar", "D. Goldwasser", "D. Roth"], "venue": "In International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Fast Approximate Energy Minimization with Label Costs", "author": ["A. Delong", "A. Osokin", "H.N. Isack", "Y. Boykov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Efficient Graph-Based Image Segmentation", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "International Journal of Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C. Yu"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning specific-class segmentation from diverse data", "author": ["M.P. Kumar", "H. Turki", "D. Preston", "D. Koller"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Image segmentation with a bounding box prior", "author": ["V. Lempitsky", "P. Kohli", "C. Rother", "T. Sharp"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Rapid and accurate large-scale coestimation of sequence alignments and phylogenetic trees", "author": ["K. Liu", "S. Raghavan", "S. Nelesen", "C.R. Linder", "T. Warnow"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Structured Learning from Partial Annotations", "author": ["X. Lou", "F.A. Hamprecht"], "venue": "In International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Learning low-order models for enforcing high-order statistics", "author": ["P. Pletscher", "P. Kohli"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Semantic texton forests for image categorization and segmentation", "author": ["J. Shotton", "M. Johnson", "R. Cipolla"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Structured Output Learning with High Order Loss Functions", "author": ["D. Tarlow", "R.S. Zemel"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning associative Markov networks", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "SuperParsing: Scalable Nonparametric Image Parsing with Superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "In European Conference on Computer", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "LabelMe: Online Image Annotation and Applications", "author": ["A. Torralba", "B.C. Russel", "J. Yuen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Weakly Supervised Semantic Segmentation with a Multi-Image Model", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In IEEE International Conference on Computer", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Weakly Supervised Structured Output Learning for Semantic Segmentation", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning structural SVMs with latent variables", "author": ["Yu", "C.-N. J", "T. Joachims"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "The concave-convex procedure (CCCP)", "author": ["A. Yuille", "A. Rangarajan"], "venue": "In NIPS", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "Figure 1: Types of annotation for an image from the MSRC dataset [15]", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "\u2018things\u2019 in terms of Heitz and Koller [6]) are better described by bounding-box annotations, while the background categories (i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "\u2018stuff\u2019 [6])\u2014which tend to fill significant parts of an image\u2014 by image-level labels.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "[22, 23] use a multi-image probabilistic graphical model to propagate image-level annotations across different training images.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[22, 23] use a multi-image probabilistic graphical model to propagate image-level annotations across different training images.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 7, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 10, "context": "Our work extends recent research on using latent-variable structural support vector machines (LV-SSVM) for weakly-supervised learning [3, 8, 11] by incorporating annotation-specific loss functions, which measure the inconsistency of some labelling predicted by the algorithm with the ground-truth weak annotation.", "startOffset": 134, "endOffset": 144}, {"referenceID": 7, "context": "[8], who use a sequential method to learn semantic segmentation from different types of annotations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], at the training stage we minimize our annotation-specific loss functions simultaneously.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "This relates us to the recent work on supervised learning with non-decomposable loss functions [13, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 15, "context": "This relates us to the recent work on supervised learning with non-decomposable loss functions [13, 16].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "Pletscher and Kohli [13] use a higher-order loss function that penalizes the difference in the area of the target category between binary segmentations.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Tarlow and Zemel [16]", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 19, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 6, "context": "In this paper we follow the maxmargin formulation of structured-output learning (also called structural support vector machine, SSVM) [17, 20, 7]:", "startOffset": 134, "endOffset": 145}, {"referenceID": 12, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 15, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 3, "context": "the individual variables [13, 16, 4].", "startOffset": 25, "endOffset": 36}, {"referenceID": 19, "context": "Problem (2)\u2013(3) is convex and can be solved by the cutting-plane method [20, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "Problem (2)\u2013(3) is convex and can be solved by the cutting-plane method [20, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 23, "context": "Note that for M = 0 the above formulation degenerates to the standard SSVM formulation, while for N = 0 it reduces to the latent-variable SSVM [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "We follow Yu and Joachims [24] and use the concave-convex procedure (CCCP) [25] to solve it approximately.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "We follow Yu and Joachims [24] and use the concave-convex procedure (CCCP) [25] to solve it approximately.", "startOffset": 75, "endOffset": 79}, {"referenceID": 16, "context": "We restrict pairwise weights w and pairwise features xij to be nonnegative and thus obtain an associative discriminative function (with only attractive pairwise potentials) [17].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "\u03b1-expansion [2].", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "We use the efficient modification of \u03b1-expansion for accounting label costs [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "[8], but it did not affect the performance significantly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We treat them the same way as the image-level loss: we modify \u03b1-expansion with label costs [4] to penalize each clique of superpixels, which contains at least one superpixel labelled with label(z).", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "[9] showed that this definition is natural).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To provide tightness, we use a variation of the pinpointing algorithm [9], adapted for the multi-class segmentation.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "[9], we do not perform further dilation, since it is unclear, which label we should use for expansion move(s); neither of the heuristics we tried improved the result significantly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used a different criterion during the annotation-consistent inference: they penalize the empty rows and columns within bounding boxes (the opposite to what we do in loss-augmented inference).", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 56, "endOffset": 64}, {"referenceID": 21, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 56, "endOffset": 64}, {"referenceID": 9, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 17, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 22, "context": "We test the proposed framework on two datasets: MSRCv23 [15, 22] and SIFT-flow4 [10, 18, 23].", "startOffset": 80, "endOffset": 92}, {"referenceID": 18, "context": "SIFT-flow is a more challenging dataset: it is a subset of the LabelMe database [19], which contains 2488 training and 200 test images; they are labelled to 33 categories using crowd-sourcing.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "For MSRC, we obtain superpixels using the original implementation of the gPb edge detector [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "The unary features are the following: a histogram of SIFT [12] visual words built using a dictionary of size 512 by hard assignment of the descriptors to the bins; a histogram of the RGB colors on a dictionary of size 128; a histogram of locations over a uniform 6\u00d76 grid.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "We L2-normalize the joint feature vector and map it into a higher-dimensional space where the inner product approximates the \u03c7-kernel in the original space (the dimensionality of the space triples after the transformation) [21].", "startOffset": 223, "endOffset": 227}, {"referenceID": 22, "context": "[23] and obtain superpixels and features using the code by Tighe and Lazebnik [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[23] and obtain superpixels and features using the code by Tighe and Lazebnik [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "It runs graph-based segmentation of Felzenszwalb and Huttenlocher [5] followed by feature extraction.", "startOffset": 66, "endOffset": 69}, {"referenceID": 21, "context": "Following the previous work [22, 14], we exclude the pixels of rare categories (\u2018horse\u2019 and \u2018mountain\u2019) from recall computation for MSRC, but include the \u2018other\u2019 label, see Section 4.", "startOffset": 28, "endOffset": 36}, {"referenceID": 13, "context": "Following the previous work [22, 14], we exclude the pixels of rare categories (\u2018horse\u2019 and \u2018mountain\u2019) from recall computation for MSRC, but include the \u2018other\u2019 label, see Section 4.", "startOffset": 28, "endOffset": 36}, {"referenceID": 22, "context": "[23], who also reached 21% on that dataset with the same superpixels and features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] applied to three images from the MSRC test set", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] since the type of input data for their framework is unorthodox.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] could look like this: \u2022 train SSVM using the fully-labelled part of the training set, \u2022 use the trained model to infer the labelling of all images consistent with the weak annotation, \u2022 train SSVM using the hallucinated labelling obtained in the previous step.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] uses the strong loss function w.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.", "creator": "LaTeX with hyperref package"}}}