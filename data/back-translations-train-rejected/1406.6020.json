{"id": "1406.6020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "Stationary Mixing Bandits", "abstract": "We study the bandit problem where arms are associated with stationary phi-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve.", "histories": [["v1", "Mon, 23 Jun 2014 18:48:59 GMT  (19kb)", "http://arxiv.org/abs/1406.6020v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["julien audiffren", "liva ralaivola"], "accepted": false, "id": "1406.6020"}, "pdf": {"name": "1406.6020.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 6,60 20v1 [cs.LG] 2 3Ju n20 14"}, {"heading": "1 Introduction", "text": "In this case, the agent's goal is to maximize his long-term reward. It is traditionally assumed that the stochastic process associated with each arm is a sequence of independently and identically distributed (i.i.d) random variables. In this case, the challenge the agent must face is the well-known exploration / exploitation of the problem: at the same time, it must ensure that it collects information from all arms to identify the most rewarding - and maximize the rewards along the sequence."}, {"heading": "2 Overview of the problem", "text": "Let us recall the definitions of stationary and non-negative integers and s, random subsequences (Xt) and (Xt + s,.., Xt + m + s,.,.,.,.,.,.,...,...,...,.....,.........,............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Mixing Bandits in the Rested Case", "text": "Our goal is to show that a simple algorithm derived from UCB works by selecting blocks, and the order accessed is as follows: (Xkst,.), X k (s + 1) t \u2212 1) t \u2212 1) t \u2212 1. The reward accessed is in due course t (with a slight misuse of notation that leads us to use t as a time index if we do not use the full information provided by this order, but instead km (X k t) s, b) whereXkt, s (Xkst, s)."}, {"heading": "4 m + b | s: Expressing the Independence Trade-Off in the", "text": "This section introduces another way to encode the trade-off between exploration, exploitation and independence. As before, we are looking at sequences of s-studies, but among the s-results, we are trying to optimize the number of m-results that we are using to update the empirical value of weapons and the number of b-results that we are ignoring to improve the independence between the realization of our random variables under consideration. In addition to the s = m + b cases, we are also looking at the situation where \u03b2 (m + b) = s with \u03b2-N and \u03b2 > 1. The sequence of s-studies can then be interpreted as \u03b2-consecutive sequences of m + b studies, and the value of this particular (m, b) distribution is therefore multiplied by \u03b2."}, {"heading": "4.1 Hypotheses and Regrets", "text": "In this section, we will assume the following additional assumptions about the mixing processes: (Xkt) t \u2265 0: 1 \u2264 k \u2264 K, (B) n, Mk (B). = 1 + \u2211 i \u2265 1k (B (i + 1)) < + \u221e. Note that this corresponds to the widely held assumption that the k (i) can be summed up over i (see, for example, the case of the aforementioned algebraic mix sequence). Also, note that Mk (1) is an upper limit of (B) m that appears in Theorem 1, and Mk (\u00b7) is a decreasing function, so that Mk (B). The setting is as follows: At each step, the agent pulls an arm s times and must choose how to split these s elements between a significant portion of m elements that are used to update the empirical value of the arm, and the non-significant portion of the b element that is used to strengthen the independence between the variables."}, {"heading": "4.2 Concentration inequality and algorithm", "text": "We now introduce a concentration inequality based on the 2 km, b = 2 km, b = 2 km, b = 3 km, b = 4 km, b = 4 km, b = 4 km, b = 4 km, b = 4 km, b = 4 km, b = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5 km, c = 5, c = 5, c = 5, c = 5, c = 5, c = 5, c = 5 km."}, {"heading": "4.3 Regret Analysis", "text": "In this subsection we set an upper limit for regretting the algorithm 2nd sentence 2. Let R be regret as defined in (15), and leave \u03b7: N 7 \u2192 N, \u03b7 (s). = \u2211 si = 1 i1i | s Then the main difference to the standard technique for proving regret results from the fact that the value of each arm is the maximum of its coordinate: as suchs we must consider the following event levels: k) \"t,\" b \"m + b,\" b \"k,\" s \"m + b\" (\"km,\" b \"- 1 \u2212 b\"), c \"c,\" c, \"c\" s, \"c,\" c, \"c\" m, \"c\" m, \"b\" k, \"c\" s, \"c,\" c, \"c,\" k, \"c,\" c, \"k,\" c, \"c,\" c, \"m m, m, m,\" s."}, {"heading": "5 Restless \u03d5 mixing bandits", "text": "In this section, we provide an analysis for the restless-mixing bandit, based on the results obtained so far. Let's remember that in the realized case, the agent was obliged to ignore some of the insights he received from one arm in order to enforce the independence and thus the accuracy of his predictor. In the restless case, instead of pulling an arm to increase the independence of the realization of an arm, the agent can pull another arm to gather information about the independence of the k while enforcing the independence of the k. As in Section 4, we assume that an agent is willing to increase the independence of the realization of an arm. \u00b7 K, the stochastic process Xk is a process, and its mixed coefficients (i) are summable, and we define the following upper function."}, {"heading": "6 Conclusion", "text": "We have presented both a theoretical analysis in a general framework as well as a more practical investigation of the problem in the case of fast mixing sequences (with the difference that the mixing coefficients are all 0).For each of these cases, we have provided algorithms and accompanying regret analyses, which represent strict extensions of the methods that exist for the i.i.d situation, as to how ordinary results can be recovered from our limits if the mixing coefficients are all 0. Future work could include a study of the restless case in which the mk must be calculated from the data, as well as a study in the more difficult case of \u03b2 mixing processes."}, {"heading": "A Appendix", "text": "Suggestion 5: Leave m, b \u2212 N and s = m + b, Xt is a process of stochastic mixing according to the value in R, with the mixing coefficient X (\u00b7), and and and and and: Rm 7 \u2192 R is a measurable function. Then the stochastic process Zt (Xst + 1,.., Xst + m) is also a process of mixing with the mixing coefficient Z = X (Rm), defining the stochastic process Zt = bt + m (t \u2212 1). Proof. In the following, we will use delta (A) to denote the \u03c3 algebra produced by A. First note: Since then it is a measurable process with the coefficient Z \u2212 1 (R), and consequently it is (Rm), and consequently (Zt) a process of change (Xst + 1,."}, {"heading": "B Proof of Proposition 2", "text": "The proof: If the elected arm at a given time k, k + k, k + k, k + k, k + k, k + k, k + k, k + m, k = b, k = b, k = b, k, k, k, k, k, k, k, k, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b"}, {"heading": "C Proof of Proposition 5", "text": "Proof: If at the time t the selected arm is k instead of k, then one of the following values must be true: \u0435k \u0445 t \u2264 \u00b5k \u0445 (21) \u00b5k \u2264 (\u0432 \u0441kt \u2212 1 \u2212 \u221a \u03b1J (k, t \u2212 1)) (22) 2 \u221a 2\u03b1 log (t) M2k (\u03b7k, t) \u03c4k, t \u2212 1 \u2265 \u00b5k \u0432 \u2212 \u00b5k (23) From the last inequality we deduce that: \u03c4k, t \u2212 1 \u2264 8\u03b1 log (t) M2k (\u03b7k, t) \u0445 2k \u2264 8\u03b1 log (t) \u0445 2k (24) since M2k (\u03b7k, t) \u2264 1."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6:422", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multi- armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning Journal, 47(23):235\u2013256", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica, 61:5565", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "volume 5 of Foundation and Trends in Machine Learning. NOW", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning of rested and restless bandits", "author": ["C. Tekin", "M. Liu"], "venue": "IEEE Transactions on Information Theory, 58(8):5588\u20135611", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Concentration inequalities for dependent random variables via the martingale method", "author": ["L. Kontorovich", "K. Ramanan"], "venue": "The Annals of Probability, 36(6):2126\u20132158", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Stability bounds for stationary -mixing and -mixing processes", "author": ["M. Mohri", "A. Rostamizadeh"], "venue": "Journal of Machine Learning Research, 11:789\u2013814", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 2, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 3, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 0, "context": "Many algorithms have been proposed to solve this trade-off between exploration and exploitation [2, 3, 4, 1].", "startOffset": 96, "endOffset": 108}, {"referenceID": 4, "context": "A closely related setup that addresses the bandit problem with dependent rewards is when they are distributed according to Markov processes, such as Markov chains and Markov decision process (MDP) [5, 6], where the dependences between rewards are of bounded range, which is what distinguishes those works with ours.", "startOffset": 197, "endOffset": 203}, {"referenceID": 5, "context": "In the case of stationary \u03c6-mixing distributions, we have the following theorem from [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Theorem 1 ([7, 8]).", "startOffset": 11, "endOffset": 17}, {"referenceID": 6, "context": "Theorem 1 ([7, 8]).", "startOffset": 11, "endOffset": 17}], "year": 2014, "abstractText": "We study the bandit problem where arms are associated with stationary \u03c6-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve.", "creator": "LaTeX with hyperref package"}}}