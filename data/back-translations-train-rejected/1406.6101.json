{"id": "1406.6101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2014", "title": "Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states", "abstract": "The purpose of speech emotion recognition system is to classify speakers utterances into different emotional states such as disgust, boredom, sadness, neutral and happiness. Speech features that are commonly used in speech emotion recognition rely on global utterance level prosodic features. In our work, we evaluate the impact of frame level feature extraction. The speech samples are from Berlin emotional database and the features extracted from these utterances are energy, different variant of mel frequency cepstrum coefficients, velocity and acceleration features.", "histories": [["v1", "Mon, 23 Jun 2014 22:21:17 GMT  (427kb)", "http://arxiv.org/abs/1406.6101v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["imen trabelsi", "dorra ben ayed", "noureddine ellouze"], "accepted": false, "id": "1406.6101"}, "pdf": {"name": "1406.6101.pdf", "metadata": {"source": "CRF", "title": "Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states", "authors": ["Imen Trabelsi", "Dorra Ben Ayed", "Noureddine Ellouze"], "emails": [], "sections": [{"heading": null, "text": "In our work we evaluate the effects of questions and acceleration mechanisms. The idea is to investigate the success of questions in literature that can be derived from these statements is energy, another variant of questions. The idea behind this is that the question of questions is put in the foreground. The question is that the question of questions is put in the foreground. The question of questions is whether the question, the question, the question, the question, the question, the question, the question and the question, the question and the question, the question and the question, the question and the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the question, the question, the question and the"}, {"heading": "A. Feature extraction", "text": "The first problem that occurs when trying to create a recognition framework is discrimination of the characteristics to be used. Common acoustic characteristics used to create the emotion model are pitch, intensity, speech quality characteristics and formants [9]. Others include cepstral analysis [4]. These characteristics can be divided into two categories: characteristics at the statement level [10] and characteristics at the frame level [11]. In this paper, our feature extractor is based on: Mel Frequency Cepstral Coefficient (MFCCs), MFCC-low, energy, speed and acceleration coefficients. They are extracted at the image level. MFCCs were the most popular characteristics at the low level. They show good performance in speech and speaker recognition. We use the advantage of this representation for our emotion identification task. MFCC-Low is a variant of MFCC-Low, energy, acceleration coefficient, and image level."}, {"heading": "B. The acoustic emotion gaussians model", "text": "The probability density function of the feature space for each emotion is modelled using a weighted mixture of simple Gaussian components.,, 1 () (;).Ni i ii P x W N x (1), where N (;,) is the Gaussian density function, wi, \u00b5i and i are the weight, mean and covariance matrix of the i-th Gaussian component or weight. This module is ensured by the construction of a universal background model (UBM) that is trained across all emotional classes. There are a number of different parameters involved in the UBM training process, namely the mean vector, covariance matrix and weight. These parameters are estimated using the iterative expectation maximization model (EM), whereby the thermodeling of emotional expression is performed separately by specifying only the mean value vectors of the UBM transformations using the GMM criterion."}, {"heading": "C. SVM Classification Algorithm", "text": "The support vector machines (SVM) [19] are supervised learning machines that find the maximum margin hyperplane that separates two classes of data. SVM solve nonlinear problems by projecting the input characteristics vectors into a higher dimensional space using a Mercer core. This powerful tool is researched for the distinction of emotions by means of GMM mean supervectors. The reason for choosing the SVM classifier for this task is that it will provide a better distinction even in a high-dimensional feature space. In our research, we give each training supervector sample with the corresponding emotion class label. Then, we enter it into the SVM classifier and get an SVM emotion model. The output of each model is given to the decision logic. The model with the best score determines the emotion statue. The output of the matching supervector is a posteriori sample with the corresponding emotion class designation. In this work, we examine two SVM kernel SVM suggested in the SVM:"}, {"heading": "A. Emotional speech database", "text": "The database used in this paper is the Berlin database of emotional language (EMO-DB), which is recorded by a language working group led in the anechoic room of the Technical University of Berlin. It is a simulated open source language database. This database contains about 500 language samples, which were detected by ten professional German actors (5 actors and 5 actresses) to simulate 7 different emotions. The length of the language samples varies between 2 seconds and 8 seconds. Table 1 summarizes the different emotions."}, {"heading": "B. System Description", "text": "The data was recorded at a sampling rate of 16 KHZ and a resolution of 16 bits. First, the signal is segmented into speech and silence, then silence segments are discarded and the speech segments are pre-emphasized with a coefficient of 0.95. From pre-emphasized language, each feature vector is extracted from 8 ms layer with a 16 ms analysis window. Each signal window is hammered to reduce signal discontinuity. Our base system consists of 128 UBM Gaussian components from the acoustic data of various emotional sets. Individual emotion models are MAP-adjusted. Only the central vectors are adjusted with a relevance factor of 16."}, {"heading": "C. Results and discussion", "text": "1) Categorical Emotion ResultsIn these experiments we have different emotions, which are marked to different states."}, {"heading": "TABLE.2 RECOGNITION RATE FROM DIFFERENT VARIANT OF MFCC", "text": "The combination of MFCC and MFCC-low resulted in an accuracy of 81.35%. MFCC-low features perform well compared to the small scale of filter banks used, possibly due to its ability to detect fluctuations in voice quality. For the rest of the work, we choose the combined MFCC (CMFCC).The table below (Table 3) shows the full range of features used for the assessment."}, {"heading": "TABLE3. DIFFERENT SPEECH FEATURE VECTORS", "text": "We can observe that negative emotions (sadness, boredom, disgust, fear) received the highest classification rate; this could be attributed to the exaggerated expression of emotions by the actors; the lowest rate was for neutral synthesized speech at 50%, which can be explained by the fact that neutral language does not contain specific emotional information.The addition of energy significantly improves the recognition rate for emotions such as anger (from 89.47% to 97.37%), disgust (from 92.31% to 100%) and sadness (from 81.47% to 84.21%).The addition of derivatives improves the recognition rate for happy emotions (from 71.43% to 80.95%).We also conclude that GMM SVM achieves a higher detection rate even when the training data size is small (45 expressions for sadness).More detailed results in the confusion matrix (Table 5) further show that the confusion between different emotions are linked to MFCs."}, {"heading": "Recognized As Ang Disg Fear Happ Neut Sad Bor", "text": "Ang 34 1 0 2 1 0 0Disg 0 12 0 0 1 0Fear 0 0 20 1 0 0Happ 5 0 1 15 0 0Neut 0 1 0 5 0 8Sad 0 0 0 0 17 2Bor 0 0 0 0 0 0 1 232) Dimensional emotions result In these experiments we have various emotions indicating binary arousal and valence. The confusion matrix shown in Table 6 illustrates the classifications of the two arousal classes individually (high versus low).The detection rate is 98.24% for low arousal and 97.84% for high arousal. As can be seen, high and low emotions are easy to classified.TABLE. 6 CONFUSION MATRIX OF AROUSAL CLASSIFICATIONRecognized as High LowHigh 91 2 Low 1 56Accuracy (%) 97.85 98.24In Valence there are 3 classes that are positive, we rank as neutral, and the detection classes as negative for the detection classes."}, {"heading": "Recognized as Negative Neutral Positive", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "TABLE.8 CONFUSION MATRIX :NEGATIVE VS POSITIVE", "text": "Finally, we distinguish between emotional and neutral language. As can be seen in Table 9, phrases belonging to a neutral state are completely misclassified."}, {"heading": "TABLE.9 CONFUSION MATRIX :EMOTIONAL VS NEUTRAL", "text": "In this context, it should be noted that these two emotional states are the same emotional traits that make it difficult to distinguish between these emotions. Positive emotions are insufficiently recognized, due to the fact that happiness, which is a positive expression, is generally confused with anger, which is a negative expression. Considering that these two emotions have exactly the same highest value in the dimension of arousal, which indicates that arousal plays an important role in the recognition of emotions, this is one of the reasons why acoustic discrimination in the dimension of valence is still problematic: there are no strong discriminatory speech functions that distinguish between positive speech and negative effect."}, {"heading": "Emotion Negative Positive", "text": "In recent years, it has become clear that the two countries are not only one country, but also a country in which people are able to integrate themselves, in a society in which they support each other, in which they support each other, in which they support each other and in which they support each other. In recent years, it has become clear that this is a country in which people are able to integrate themselves. In recent years, it has become clear that this is not only a country, but also a country in which people are able to integrate themselves, but in which they live and live, in which they live and live."}, {"heading": "Imen Trabelsi", "text": "I. Trabelsi holds a degree in Computer Science from the High Institute of Management of Tunis (ISG-Tunisia) in 2009 and an MS degree in Signal Processing from the Institute of Computer Science of Tunis (ISI-Tunisia) in 2011. Currently, she works at the National School of Engineer of Tunis (ENIT) for a PhD in Electrical Engineering (Signal Processing), specialising in speech processing, pattern recognition, emotion recognition and speaker recognition.E-mail: trabelsi.imen1 @ gmail.com"}, {"heading": "Dorra Ben Ayed Mezghani", "text": "D. Ayed Mezghani studied computer science at the National School of Computer Science (ENSI-Tunisia) in 1995, at the National School of Computer Science in Tunis (ENITTunisia) in 1997 and at the National School of Signal Processing (ENIT-Tunisia) in 2003. Currently, she is an associate professor in the Department of Computer Science at the High Institute of ComputerScience of Tunis (ISI-Tunisia). Her research interests include fuzzy logic, support vector machines, artificial intelligence, pattern recognition, speech recognition and speaker identification. E-mail: Dorra.mezghani @ isi.rnu.tn, DorraInsat @ yahoo.fr"}, {"heading": "Noureddine Ellouze", "text": "N. Ellouze obtained a doctorate from the Institut National Polytechnique at the Paul Sabatier University (Toulouse-France) in 1977 and a diploma in electrical engineering at the ENSEEIHT at the same university in 1968. In 1978, Dr. Ellouze joined the Department of Electrical Engineering at the National School of Engineering of Tunis (ENIT-Tunisia) as Assistant Professor of Statistics, Electronics, Signal Processing and Computer Architecture. In 1990, he became Professor of Signal Processing and Stochastic Processes. He was also Director of the Electricity Department at ENIT from 1978 to 1983, Director General and President of the Research Institute of Computer Science and Telecommunications IRSIT from 1987 to 1990 and President of the Institute from 1990 to 1994. He is now Director of the Signal Processing Research LaboratoryLSTS at ENIT and is responsible for Control and SignalProcessing Master at ENIT."}], "references": [{"title": "Ensemble methods for spoken emotion recognition in callcentres", "author": ["M. Donn", "W. Ruili", "C. Liyanage"], "venue": "Speech communication, vol. 49", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A Robust Multi-Modal Emotion Recognition Framework for Intelligent Tutorig Systems", "author": ["L. Xiao", "J. Yadegar", "N. Kamat"], "venue": "IEEE International Conference on Advanced Learning Technologies (ICALT)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus", "author": ["K. Forbes", "D. Litman"], "venue": "Proc. Of 6th SIGdial Workshop on Discourse and Dialogue, Lisbon, Portugal", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Emotional speech recognition: Resources", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": "features, and methods. Speech Communication, vol. 48, no. 9, pp. 1162\u20131181", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech emotion recognition using hidden Markov models", "author": ["T.New", "S. Foo", "L.D. Silva"], "venue": "Speech Communication,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Emotion recognition in spontaneous speech using GMMs", "author": ["D. Neiberg", "K. Elenius", "K. Laskowski"], "venue": "Proc. INTERSPEECH", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Mandarin emotional speech recognition based on SVM and NN", "author": ["T.-L. Pao", "Y.-T. Chen", "J.-H. Yeh"], "venue": "Proc of the 18th International Conference on Pattern Recognition (ICPR'06), vol. 1, pp. 1096- 1100", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech emotion recognition based on HMM and SVM", "author": ["Y. Lin", "G. Wei"], "venue": "In Proc. of 2005 International Conference on Machine Learning and Cybernetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Evaluation d\u2019une approche hybride GMM-SVM pour l\u2019identification de locuteurs", "author": ["I. Trabelsi", "D. BenAyed"], "venue": "La revue e-STA, 8(1), 61-65", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "On the use of different feature extraction methods for linear and non linear kernels", "author": ["I. Trabelsi", "D. Ben Ayed"], "venue": "Proc. Of Sciences of Electronics, Technologies of Information and Telecommunications ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["J.S. Park", "J.H. Kim", "Y.H. Oh"], "venue": "Feature Vector Classification based Speech Emotion Recognition for Service Robots, IEEE Trans. on Consumer Electronics. 55, 1590-1596", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["D. Bitouk", "R. Verma"], "venue": "Nenkova, Class-level spectral features for emotion recognition,Speech Communication. 52, pp. 613-625", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Classical and Novel Discriminant Features for Affect Recognition from Speech", "author": ["R. Fernandez", "R.W. Picard"], "venue": "Proc. Of InterSpeech , pp. 1-4, Lisbon, Portugal", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech and crosstalk detection in multichannel audio", "author": ["S.N. Wrigley", "G.J. Brown", "V. Wanand", "S. Renals"], "venue": "IEEE Transactions on Speech and Audio Processing", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Localized Spectro-Temporal Cepstral Analysis of Speech", "author": ["J. Bouvrie", "J. Ezzat", "T. Poggio"], "venue": "Proc. ICASSP 2008, pp. 4733-4736", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Frame vs. turn-level: emotion recognition from speech considering static and dynamic processing", "author": ["B. Vlasenko", "B.Schuller", "A. Wendemuth", "G. Rigoll"], "venue": "In Proc. of Affective Computing and Intelligent Interaction,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Maximum Likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laid", "D. Durbin"], "venue": "J. Royal Statistical Soc, vol. 39, pp. 1-38", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1977}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "DSP, Vol. 10, No. 3, pp. 19\u201341", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Spring-verlag, New York", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "LIBSVM : a library for support vector machines", "author": ["C.C. Chang", "C. J . Lin"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.", "startOffset": 175, "endOffset": 178}, {"referenceID": 1, "context": "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.", "startOffset": 210, "endOffset": 213}, {"referenceID": 2, "context": "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.", "startOffset": 240, "endOffset": 243}, {"referenceID": 3, "context": "Many kind of acoustic features have been explored to build the emotion models [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 6, "context": "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "In our paper, we investigate the relationship between generative method based GMM and discriminative method based SVM [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 9, "context": "In our paper, we investigate the relationship between generative method based GMM and discriminative method based SVM [9, 10].", "startOffset": 118, "endOffset": 125}, {"referenceID": 8, "context": "Common acoustic features used to build the emotion model include pitch, intensity, voice quality features and formants [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "Others include cepstral analysis [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "These features can be divided into two categories: utterance-level features [10] and framelevel features [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "These features can be divided into two categories: utterance-level features [10] and framelevel features [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Mel filter banks are placed in [20-300] Hz.", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "It is, often referred to as the volume or intensity of the speech, is also known to contain valuable information [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "\uf0b7 Velocity (delta) and acceleration (delta-delta) parameters have been shown to play an important role in capturing the temporal characteristics between the different frames that can contribute to a better discrimination [14].", "startOffset": 221, "endOffset": 225}, {"referenceID": 14, "context": "The acoustic emotion gaussians model GMMs have been successfully employed in emotion recognition [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "These parameters are estimated using the iterative expectation-maximization (EM) algorithm [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "Each emotional utterance is then modeled separately by adapting only the mean vectors of UBM using Maximum A Posteriori (MAP) criterion [18], while the weights and covariance matrix were set to the corresponding parameters of the UBM.", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "SVM Classification Algorithm The support vector machines (SVM) [19] are supervised learning machines that find the maximum margin hyperplane separating two classes of data.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Our experiments are implemented using the LibSvm [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 1, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 2, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 3, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 4, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 5, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 6, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 7, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 8, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 9, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 12, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 14, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 15, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 16, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 18, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}, {"referenceID": 19, "context": "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.", "startOffset": 54, "endOffset": 62}], "year": 2014, "abstractText": "The purpose of speech emotion recognition system is to classify speaker's utterances into different emotional states such as disgust, boredom, sadness, neutral and happiness. Speech features that are commonly used in speech emotion recognition (SER) rely on global utterance level prosodic features. In our work, we evaluate the impact of frame-level feature extraction. The speech samples are from Berlin emotional database and the features extracted from these utterances are energy, different variant of mel frequency cepstrum coefficients (MFCC), velocity and acceleration features. The idea is to explore the successful approach in the literature of speaker recognition GMM-UBM to handle with emotion identification tasks. In addition, we propose a classification scheme for the labeling of emotions on a continuous dimensional-based approach.", "creator": "Microsoft\u00ae Office Word 2007"}}}