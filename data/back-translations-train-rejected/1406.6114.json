{"id": "1406.6114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform", "abstract": "In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.", "histories": [["v1", "Tue, 24 Jun 2014 00:48:23 GMT  (487kb,D)", "http://arxiv.org/abs/1406.6114v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sakthithasan sripirakas", "russel pears"], "accepted": false, "id": "1406.6114"}, "pdf": {"name": "1406.6114.pdf", "metadata": {"source": "CRF", "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform", "authors": ["Sakthithasan Sripirakas"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to find themselves without being able to play by the rules. [7] This approach removes the need to re-learn the model, thereby improving the accuracy and calculation of costs. [8] A number of methods have been proposed that deal with the collection and use of recurring data in place of the current model. [9]"}, {"heading": "2 Related Research", "text": "In fact, the fact is that most of them are not a \"normal\" person, but a person who is able to assert himself, and who is able to assert himself."}, {"heading": "3 Application of the Discrete Fourier Transform on Decision Trees", "text": "Discrete Fourier Transformation (DFT) can be applied to very different areas of application, such as time series analysis, signal processing, image processing, and so on. It turns out that DFT is very effective in terms of classification when applied to a decision tree model. Kargupta and Park in [9] have studied the use of DFT in a distributed environment, but not its use in a data stream environment, as this research on do.Kargupta and Park in [9] showed that the Fourier spectrum, consisting of a series of Fourier coefficients, fully captures a decision tree in algebraic form, meaning that the Fourier representation maintains the same classification power as the original decision power. A decision tree can be represented in compact algebraic form by applying the DFT to the paths of the tree."}, {"heading": "4 Exploitation of the Fourier Transform for Recurrent Concept Capture", "text": "We will first introduce the basic algorithm used in Section 5.1, and then address an optimization that we used in Section 5.2 for the energy threshold."}, {"heading": "4.1 The FCT algorithm", "text": "We use CBDT [6] as the basic algorithm that sustains a forest of trees 1. This forest of trees is dynamic in the sense that it can adapt to changing concepts at drift detection points. Thus, we define the memory consumed by that forest as active. We integrate the basic CBDT algorithm with the ADWIN [2] drift detector to signal concept drift. At the first concept drift point, the most powerful tree (in terms of accuracy) is identified and the DFT is applied according to the energy threshold by which the resulting spectrum is stored in the repository for future use when the current concept is recursed. The spectra stored in the repository are fixed with the intention of capturing past concepts. At each subsequent drift point, a winning model is selected by querying both the active memory and the repository."}, {"heading": "4.2 Optimizing the Energy Thresholding Process", "text": "In order to avoid unnecessary calculation of coefficients of higher order, which provide increasingly lower returns on the accuracy of the classification, an energy threshold is highly desirable. To achieve a threshold for energy, a subset S of the coefficient (lower order) must be determined in such a way that E (S) E (T) >, where E (T) indicates the total energy across the entire spectrum and is the desired energy threshold.In our optimized threshold, we first calculate the cumulative threshold CEi in the order i (i + 1) CEi, since the exponential decay property ensures that the energy is expressed in each of the orders i + 1, i + 2, \u00b7 \u00b7, d less than the cumulative energy in the rest of the spectrum."}, {"heading": "5 Experimental Study", "text": "This section discusses our empirical study of the following learning systems: CBDT, FCT (Fourier Concept Trees) and MetaCT. FCT includes the Fourier-compressed trees in a repository in addition to the tree populations maintained by the standard CBDT. We are implementing Gamma's meta-learning approach with CBDT as a basis for learning, namely MetaCT. The main focus of the study is to assess the extent to which relapses are detected on the basis of old models preserved in classification pools."}, {"heading": "5.1 Parameter Values", "text": "All experiments were performed with the following parameter values: \u2022 Hoeffding Tree Parameter The desired probability to choose the correct split attribute = 0.99, Equilibrium Threshold = 0.01, Growth Test Interval = 32 \u2022 Baumwald Parameter Maximum Node Number = 5000, Maximum Number of Fourier Trees = 50, Accuracy Threshold Equilibrium Threshold \u03c4 = 0.01 \u2022 ADWIN Parameter Drift Significance Value = 0.01, Warning Significance Value = 0.3 (MetaCT only) All experiments were performed using the same software with C #.net runtime and hardware with Intel i5 CPU and 8GB RAM, freeing memory in each run to allow a fair comparison."}, {"heading": "5.2 Datasets used for the experimental study", "text": "We experimented with data generated by three data generators commonly used in drift detection and recurring concept mining, namely SEA Concept [16], RBF and Rotating Hyperplane Generators. In addition, we used two real data sets, Spam and NSW Electricity, which were also widely used in previous research. In synthetic data sets, each of the four concepts comprised 5,000 instances and appeared 25 times in one data set, resulting in a total of 500,000 instances of 100 true concept drift points. To challenge the concept recognition process, we added a 10% noise level to all synthetic data sets to ensure that concepts recur in a similar but not exact form."}, {"heading": "5.2.1 Synthetic Data Sets", "text": "We used MOA [3] as a tool to generate these datasets.1. SEA: The concepts are defined by the function Feature1 + Feature2 > Threshold. We arranged the concepts as Concept1, Concept2, Concept3 and Concept4, which were generated based on the thresholds 8,7,9 and 9,5 respectively on the first data segment of the size 20,000. We generated 96 repetitions of a modified form of these concepts by using different seed values in MOA for each repetition sequence. Thus, our version of this data set differed from that of Gama and Kosina [4], who simply used 3 concepts, the third being an exact copy of the first. 2. RBF: The number of centrification parameters was adjusted to generate different concepts for the RBF dataset. Concept1, Concept2, Concept3 and Concept4 were generated with the number of centrations."}, {"heading": "5.2.2 Real World datasets", "text": "1. Spam record: The spam record was used in its original form 1, which summarises a development of spam messages. There are 9,324 cases and 499 informative attributes, which differ from the version used by Gama with 850 attributes. 1 from http: / / www.liaad.up.pt / kdus / products / datasets-for-concept-drift2. Electricity record: NSW Electricity dataset is also in its original form 2. There are two classes up and down, which indicate the price change in relation to the moving average of prices over the past 24 hours."}, {"heading": "5.3 Tuning MetaCT Key Parameters", "text": "In our preliminary experiments, we found optimal values for the two parameters, delays in receiving labels for short-term memory instances, and a percentage threshold of 200 and 80%, respectively. The latter parameter reflects the estimated similarity of the current concept to one from the past, thus controlling the degree of use of classifiers from the pool."}, {"heading": "5.4 Comparative study: CBDT vs FCT vs MetaCT", "text": "In this series of experiments, we focused on evaluating the models for accuracy, memory consumption, and processing times. None of the earlier studies described in the recurring concept mining literature performed a comparative study against other approaches, and therefore we believe that our empirical study is the first of its kind. Moreover, all previous studies focused exclusively on accuracy without recording memory and execution time, and therefore this study would be the first of its kind."}, {"heading": "5.4.1 Accuracy", "text": "Figure 2 clearly shows that FCT significantly outperforms its two rivals in terms of classification accuracy, and the main reason for FCT's superior performance was its ability to reuse earlier classifiers, as shown in the segment 20k-25k on the RBF dataset. A similar situation occurs at the interval 25k-35k, where the concept is similar to the concept that occurred at interval 1-5K, in contrast to MetaCT, which was unable to detect the repetition of concept1. A similar situation occurs at the interval 25k-35k, where the concept is similar to previous concepts, concept2 and 3. MetaCBDT, which operate in their own way without support for the repetition of the concept, had a relatively flat orbit across the stream segment. A similar trend towards the RBF dataset was observed in Rotating Hyperplane and SEA datasets."}, {"heading": "5.4.2 Memory", "text": "Our accuracy experiments, particularly in the case of FCT, have demonstrated the key role that capturing and reusing concepts has played in improving accuracy, and the question is what price has to be paid in terms of the amount of memory required to store these recurring concepts. Table 1 clearly shows that the Fourier-transformed trees consume a small fraction of the memory used by the pool of trees in FCT's active memory, although these models collectively outperform their decision tree counterparts at a greater number of points in the current path. Comparing total memory consumption between FCT and MetaCT is complicated by the fact that the latter tended to have immature trees in their classification pool that correspond to concepts. Nevertheless, Table 1 shows that the memory consumption of FCT competes with that of MetaCT. The only case where MetaCT had significantly lower consumption was the spam dataset with lower active memory for overhead."}, {"heading": "5.4.3 Processing Speed", "text": "FCT and MetaCT have two very contrasting methods of classification. The former route each blank instance to a single tree, which is the most powerful tree selected at the last concept drift point. In contrast, MetaCT classifies by passing an blank instance to all referees to obtain their assessment of their corresponding models, and in general FCT potentially has more effort on concept drift points if the winning tree is one selected from the active forest, as that tree needs to be converted into its Fourier representation. Therefore, it is interesting to compare the runtime performance of the two approaches. Table 1 shows that FCT generally has a higher processing speed (measured in instances processed per second); the only exception was the electricity dataset, where MetaCT was faster. Current data contain a relatively larger number of drift points compared to the other datasets, and this in turn required a larger number of DFT operations to perform this concept."}, {"heading": "5.5 Sensitivity Analysis on FCT", "text": "Once we had established the superiority of FCT, we were interested in investigating the sensitivity of the accuracy of FCT in two key factors.5.5.1 FCT's energy threshold parameter controls the extent to which it captures recurring contexts. We conducted experiments with all the datasets we experimented with and tracked the accuracy across four different thresholds: 95%, 80%, 40% and 20%. Trends observed for all datasets were very similar, and therefore we show the results for the datasets of the SEA concepts due to space constraints. Figure 3 clearly shows that there is very little difference in acceptance between trajectories for 40% and 95%, which shows DFT's resilience in capturing the classification power of concepts at low energy levels such as 40%."}, {"heading": "5.5.2 Noise Level", "text": "In Section 5.4, we observed that FCT exceeded MetaCT by recognizing concepts from the past, even though the concepts did not exactly recur in their original form, partly due to noise and partly due to different instances of data produced as a result of the re-seeding of concept generation functions. In this experiment, we explicitly tested FCT's resistance to noise levels by exposing it to three different levels of noise - 10%, 20% and 30%. For completeness, we also included MetaCT in its experiments to help interpret the results. Figure 4 shows three interesting pieces of information. First, FCT is still able to detect recurring concepts at the 20% noise level, although the models it reuses do not have quite the same classification power (compared to the 10% noise level) that the current concept is relatively higher due to data instances, the noise level."}, {"heading": "6 Conclusions and Future Work", "text": "Our experiments showed that the discrete Fourier transformation, when applied to decision trees, captured concepts very effectively, both in terms of information content and conciseness. Overall, our proposed approach exceeded the meta-learning approach of Gama and Kosina [4] in terms of classification accuracy, while it was competitive in terms of memory and processing speed. We were able to optimize the derivation of the Fourier spectrum through an efficient threshold process, but there is further scope for optimizations in calculating low-order coefficients in streams that exhibit frequent drifts, as our experiment with the Celsius electricity dataset shows. Our future work will focus on two areas. Firstly, we plan to run the multi-processing processes in parallel to each other, using two DFT processes independently of each other."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.", "creator": "LaTeX with hyperref package"}}}