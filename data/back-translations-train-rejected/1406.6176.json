{"id": "1406.6176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Composite Likelihood Estimation for Restricted Boltzmann machines", "abstract": "Learning the parameters of graphical models using the maximum likelihood estimation is generally hard which requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation which are higher-order generalizations of the maximum pseudo-likelihood estimation. In this paper, we propose a composite likelihood method and investigate its property. Furthermore, we apply our composite likelihood method to restricted Boltzmann machines.", "histories": [["v1", "Tue, 24 Jun 2014 09:32:10 GMT  (110kb)", "http://arxiv.org/abs/1406.6176v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["muneki yasuda", "shun kataoka", "yuji waizumi", "kazuyuki tanaka"], "accepted": false, "id": "1406.6176"}, "pdf": {"name": "1406.6176.pdf", "metadata": {"source": "CRF", "title": "Composite Likelihood Estimation for Restricted Boltzmann machines", "authors": ["Muneki Yasuda", "Shun Kataoka", "Yuji Waizumi"], "emails": ["muneki@yz.yamagata-u.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 140 6.61 76v1 [cs.LG] 2 4Ju n20 14"}, {"heading": "1. Introduction", "text": "Learning the parameters of graphical models using the maximum probability (ML) is generally difficult due to the intractability of calculating the normalizing constant and its gradients. The maximum pseudo-probability (PL) estimate [2] is a statistical approximation to the ML estimate. In contrast to the ML estimate, the maximum PL estimate is mathematically fast, but the estimates obtained with this method are not very accurate.Composite probabilities (CLs) [6] are generalizations of higher order of PL. The asymptotic analysis shows that the maximum CL estimate is statistically more efficient than the maximum PL estimate [5]. It is known that the maximum PL estimate is asymptotically consistent [2]. Thus, the maximum CL estimate is also asymptotically consistent [6]."}, {"heading": "2. Composite Likelihood Estimation", "text": "For the n-dimensional discrete random variable x: = {xi | i \u00b2 = {1, 2,. lnsianer (n}), let us consider the probability model expressed asP (x | \u03b8): = Z (\u03b8) \u2212 1 exp (\u2212 E (x | \u03b8))), (1) where E (x | \u03b8) is the energy function that has any form of function and Z (\u03b8): = x exp (\u2212 E (x | \u03b8))) is the normalizing constant. Suppose the data set consists of M data, D: = {d (\u00b5) | \u00b5 = 1, 2,., M} is statistically independent of each other. From the perspective of the ML estimation, we determine the optimal solution by maximizing the log likelihood function defined by LML."}, {"heading": "2.1. Systematic Choice of Blocks", "text": "In this section we present a specific selection of blocks in which the CL has a good property. For 1 \u2264 k \u2264 Q = n we define the family Fk, whose elements are all possible blocks composed of k different variables, i.e. Fk: = {i1, i2,..,., i1 < i2 < i2 < i2). E.g. if n = 4, F2 = {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}, {4}, andF3 = {{1, 2, 3}, {1, 4}, {2, 3, 4}, {2, 4}. For the Fk family, the CL is expressed as LFk and the difference between the individual positions (LFk) is true."}, {"heading": "3. Application to Restricted Boltzmann Machines", "text": "In this section, we apply the CL estimation to RBMs. RBMs consist of visible variables, whose states can be observed, and hidden variables, whose states are not specified by observed data. There are connections between visible variables and hidden variables, and all linkages between the individual levels are not allowed.We refer to the names of visible variables and hidden variables, whose state variables are interpreted by H or H, respectively, the state variables of visible variables. All state variables are binary random variables, which are + 1 or \u2212 1. The common distribution of RBM is represented by PRBM (x, h)."}, {"heading": "3.1. Numerical Experiments", "text": "In this section we present the results of numerical experiments using synthetic data. We use an RBM consisting of 5 visible variables and 10 hidden variables as a learning machine, and we generate M = 70 data from an RBM consisting of 5 visible estimates and 17 hidden variables using the Markov chain Monte Carlo method. In generative RBM we set \u03b1i = 0.1, \u03b2j = -0.1 and wi, j = 0.2 for all i and j. We compare the first, second and third CL estimates with the exact ML estimate. We maximize the CLs, i.e. LF1 = -0.1 and wi, j = 0.2 for all i and j. We compare the first, second and third CL estimates with the exact ML estimate with the exact ML estimate."}, {"heading": "4. Conclusion", "text": "In this paper, we have introduced the systematic selection of blocks for the maximum CL estimation, which guarantees that the CL kth-ter order approaches the true log probability with the increase of k monotonous. This property does not depend on the details of graphical models. Furthermore, we have applied our CL method to the learning of RBMs and explicitly formulate a learning algorithm. In our numerical experiments for synthetic data, we have ensured that the higher-value CLs perform better. As we have seen in Section 3, the computing costs increase when higher-value CLs are used. Nevertheless, it is possible to exchange the computing time for increased accuracy by switching to higher-value CLs. Recognition This work was partially supported by Grants-In-Aid (No. 21700247, No. 22300078 and No. 23500075) for scientific research of the Japanese Ministry of Education, Culture, Sports, Science and Technology."}], "references": [{"title": "Learning with blocks: composite likelihood and contrastive divergence", "author": ["A. Asuncion", "Q. Liu", "A.T. Ihler", "P. Smyth"], "venue": "Proceedings of 13th International Conference on AI and Statistics (AISTAT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society D (The Statistician),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1975}, {"title": "Stochastic composite likelihood", "author": ["J. Dillon", "G. Lebanon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "Proceedings of 25th International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Composite likelihood methods", "author": ["B.G. Lindsay"], "venue": "Comtemporary Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Inductive principles for restricted boltzmann machine learning", "author": ["B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas"], "venue": "Proceedings of the 13th International Conference on Ar-  tificial Intelligence and Statistics (AISTATS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Maximum pseudo-likelihood (PL) estimation [2] is a statistical approximation of the ML estimation.", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "Composite likelihoods (CLs) [6] are higher-order generalizations of the PL.", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "Asymptotic analysis shows that maximum CL estimation is statistically more efficient than the maximum PL estimation [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "It has been known that the maximum PL estimation is asymptotically consistent [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Like this, the maximum CL estimation is also asymptotically consistent [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "Furthermore, the maximum CL estimation has an asymptotic variance that is smaller than the maximum PL estimation but larger than ML estimation [5, 3].", "startOffset": 143, "endOffset": 149}, {"referenceID": 2, "context": "Furthermore, the maximum CL estimation has an asymptotic variance that is smaller than the maximum PL estimation but larger than ML estimation [5, 3].", "startOffset": 143, "endOffset": 149}, {"referenceID": 0, "context": "Recently, it has been found that the maximum CL estimation corresponds to a block-wise contrastive divergence learning [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "In the maximum CL estimation, one can freely choose the size of \u201cblocks\u201d which contain several variables, and it is widely believed that by increasing the size of blocks, one can capture more dependency relations in the model and increase the accuracy of the estimates [1].", "startOffset": 269, "endOffset": 272}, {"referenceID": 3, "context": "In the latter part of this paper, we apply our maximum CL estimation to restricted Boltzmann machines (RBMs) [4] and show results of numerical experiments using synthetic data.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "The maximum CL estimation is a statistical approximation technique of the ML estimation [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "ci = {i} and r = n, the CL is reduced to the PL [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "Application to Restricted Boltzmann Machines In this section, we apply the CL estimation to RBMs [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "Indeed, the PL estimation for the RBM was introduced [7].", "startOffset": 53, "endOffset": 56}], "year": 2014, "abstractText": "Learning the parameters of graphical models using the maximum likelihood estimation is generally hard which requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation which are higher-order generalizations of the maximum pseudolikelihood estimation. In this paper, we propose a composite likelihood method and investigate its property. Furthermore, we apply our composite likelihood method to restricted Boltzmann machines.", "creator": "LaTeX with hyperref package"}}}