{"id": "1406.6973", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2014", "title": "Communicating and resolving entity references", "abstract": "Statements about entities occur everywhere, from newspapers and web pages to structured databases. Correlating references to entities across systems that use different identifiers or names for them is a widespread problem. In this paper, we show how shared knowledge between systems can be used to solve this problem. We present \"reference by description\", a formal model for resolving references. We provide some results on the conditions under which a randomly chosen entity in one system can, with high probability, be mapped to the same entity in a different system.", "histories": [["v1", "Thu, 26 Jun 2014 18:25:53 GMT  (303kb,D)", "http://arxiv.org/abs/1406.6973v1", "18 pages, 4 figures"]], "COMMENTS": "18 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["r v guha"], "accepted": false, "id": "1406.6973"}, "pdf": {"name": "1406.6973.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "References to things / entities (people, places, events, products, etc.) are omnipresent in almost all communications, from expressions of natural language to structured data feeds, and the correct resolution of these references is critical to the proper functioning of many systems. Variations of this problem have been studied in areas ranging from philosophy and linguistics to database integration and artificial intelligence. In this paper, we propose a framework for investigating the reference problem. One of the earliest descriptions of the problem comes from Shannon's groundbreaking paper [7]. Shannon writes, \"The fundamental communication problem is to reproduce at one point either accurately or roughly a message selected at another point. Frequently, the messages have meaning; that is, they relate to or correlate to a system with specific physical or conceptual entities.\" In other words, the symbols in a message are often intended to relate to specific entities."}, {"heading": "1.1 Problem Model", "text": "In this paper, we use the model and terminology of communication theory. In classical information theory, the two parties agree on the amount of messages that can be sent. Xiv: 140 6,69 73v1 [cs.AI] 26 Jun 2014 selects one of these messages and transmits it via a channel. Communication is supposed to be successful if the recipient has a unique identification or name (henceforth, simply referred to as a name) and the two parties share the names for all entities that might have a reference in the message. In the trivial case, if each entity has a unique identification or name (advanced, simply referred to as a name) and has the names for the entities that assume successful communication of the message, the recipient can trivially decipher intended entity."}, {"heading": "1.2 Summary of Results", "text": "There are many interesting questions that can be formulated within our framework. We list some of them here, along with informal descriptions of the results presented in the rest of the paper.1. The minimum number of names that must be shared for the sender to successfully communicate references to all other units: Our most interesting result is that the amount of shared knowledge required to decrypt the intended designations in a message is inversely proportional to the required channel capacity to transmit the message. 2. the minimum length / information content of the description. We find that the average length of the required description is inversely proportional to the required channel capacity. This is closely related to the minimum number of names that need to be shared. 3. Different classes of descriptions and their sharing requirements. We find that as we allow more complex descriptions that are more difficult to decipher computationally, the amount that needs to be shared. In other words, analogy to the intersection of space that is found in the overlapping space that is required."}, {"heading": "2 Outline of paper", "text": "First, we present our model of correlation or communication of references as an extension of Shannon's communication model. Then, we review previous work in relation to this framework. Then, we formalize the concept of descriptions and the entropy of these descriptions. Finally, we provide some results on the conditions under which communication can take place."}, {"heading": "3 Communication model", "text": "In this section we describe our advanced model of communication. We start with the traditional model of information theory, in which the sender selects one of the possible messages, encrypts it, and transmits one of them through a potentially noisy channel to a receiver. We add that the world is modeled on these entities as a group of entities and a series of N-tuples. We use this model to represent the underlying world. As arbitrary N-tuples can be constructed from databases and artificial intelligence, we can model the world as a group of entities. We use this model to represent the underlying world."}, {"heading": "3.1 Simplifying Assumptions", "text": "We assume that sender and receiver share the grammar with which the graph is encoded; the details of the grammar are not relevant as long as the receiver can analyze the message. 2. The transmitted graphs can be expressed as source, arc, target triples, i.e. there are no quantifiers. Disjunctions and negations are allowed in principle. We map1 Given a graph that allows multiple directed slurs between arbitrary nodes, we map it to a corresponding graph that has at most one undirected slur between arbitrary nodes, with the same amount of nodes but different arc labels. The amount of arc labels in this reduced graph are the various possible combinations of arc labels and arc directions in the original graph that can occur between arbitrary nodes."}, {"heading": "3.2 Examples", "text": "In all of these examples, the graphs have a single arc label (let's call it P) 1. In the example shown in Figure 1, both parties observe the same graph. Names for the nodes B and D. The sender sends the order of the symbols \"P (Q, T).\" Given the underlying graph, since Q and T are known to be not B or D, the receiver can assign Q to either S (which would be correct) or R (which would be wrong). The sender understands the potential for this confusion and adds the description \"P (B, Q),\" eliminating the wrong assignment as a possibility. If either B or D are not shared, there will be at least one node whose reference cannot be communicated."}, {"heading": "4 Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "5 Graph model", "text": "As we have seen in Examples 1, 2, and 3, the graphs differ in their ability to support different descriptions; the identity of the nodes can be communicated by the uniqueness of the shape of the graph around them and by their relationship to one or more common nodes. If the structure around each node looks like the structure around each other node, it becomes more difficult to construct unique descriptions. As the wealth of the graph increases, the number of candidates increases unique descriptions for a given set of common names. The entropy of the graph is a measure of its wealth. First, we need a mathematical model for our graph. We assume that our graph is generated by a stochastic process. There is extensive work to model the graphs produced by stochastic processes, most of which can be easily extended in described ways."}, {"heading": "6 Shared knowledge", "text": "When the sender describes a node X as L1 (X, S1), i.e. by indicating that between X and the common node S1 there is an arc with the label L1, he expects the receiver to know both the common name of the node S1 and which nodes have an arc with the label L1 to S1. If one of these two conditions is not met, the description does not fulfill its purpose. We distinguish between the two types of common knowledge: common name and shared knowledge of the graph."}, {"heading": "6.1 Sharing Names", "text": "We are interested in determining the minimum number of knots whose names must be divided. We assume that the names for the arc labels are divided. We are interested in the case where the structure is very large and there is a small fixed number of arc labels, so the number of arc labels is very small compared to the number of knots. In such cases, the assumption that the arc labels are divided should have a very small effect. The quantitative measure of splitting is very simple - it is simply the number of knots whose names are divided."}, {"heading": "6.2 Shared knowledge of the graph", "text": "Quantifying the exchange of graphs is more subtle than quantifying the exchange of names. What is shared is as important as the amount that is shared. Differences in the views of sender and receiver alter the effective graph entropy that descriptions can exploit. For example, if \"color\" is one of the attributes of nodes and the receiver is blind, the usable entropy of the graph, i.e. the number of candidate descriptions, decreases. If the receiver is color-blind, some of the color values (such as black and white) can be correctly recognized, while other values such as red or green may have limited ambiguities. We use the mutual information between the sender's version and receiver of the graph's adaptation matrix as a measure of how much knowledge about the underlying world is shared. M = H (sender | receiver) = H (receiver) \u2212 H (receiver)"}, {"heading": "7 Descriptions", "text": "In this section we will look at different types of descriptions with different descriptions that we impose in the structure of allowed descriptions."}, {"heading": "7.1 Entropy of complex descriptions", "text": "As the amount of possible substitutes for L0 grows with the richness of the possible intermediate diagram, the entropy of the description string grows with the richness of the descriptions. We are interested in the growth of entropy of descriptions (Hd), where the number of nodes in a intermediate description is D as a function of D and entropy of the diagram is Hg. For the sake of this analysis, we ignore the automorphisms. Although there are 2D possible graphs of size D (of potentially unconnected rows and columns), each subblock (of potentially unconnected rows and columns) is the adoption matrix of the diagram, which is D columns wide and D rows high. Although there are 2D possible graphs (ignoring automorphisms of size D), according to the AEP, only 2HgD 2 is likely to occur between HgD 2 and HgD 2 with any probability and any probability of H2 \u2212 occurring."}, {"heading": "8 Minimum Sharing Required", "text": "In this section we calculate the minimum number of nodes to be divided depending on the entropy of the (associated probability distribution) descriptions, i.e. HD. Suppose sender and receiver share the names for a set of K-nodes S1, S2,... and let the description string for X be Lx1Lx2Lx3... LxK. The K-nodes are selected to maximize HD. In the case of an Earth Renyi random rate, we can select any set of K-nodes. In other graphs, the descriptions associated with different sets of K-nodes have different entropies. If the receiver receives the description Lx1Lx2Lx3... LxK, he can easily consider them by searching for the nodes related to Lx1 and Lx2 to K2, etc. Depending on the size of the description, we may have more than one such node."}, {"heading": "8.1 Case 1: Identical views of the graph", "text": "Theorem: Allow the sender and receiver name forGlog (N) / HD node, whereHD is the entropy of the descriptions used by the sender to identify entropy and N is the number of nodes in the graph. For large graphs, if G \u2265 2 then with high probability, the sender and receiver can communicate references to all other nodes."}, {"heading": "8.2 Different views of the graph", "text": "It is possible for sender and receiver references to communicate correctly, even if there are differences between their views on the underlying graph. (We use the mutual information (MD) between the two graphs (the one that they see as a measure of common knowledge.) Our evidence is very similar to the proof for Shannon's theory. Theorem: Let sender and receiver share the names forGlog (N) / MD nodes, where MD is the mutual information between the views of the graph that sender and receiver communicate about, and N is the number of nodes in the graph. For large graphs, if G \u2265 2 can then most likely communicate references to all but a constant number of other nodes, where MD is the reference to all but a constant number of other nodes. If G < 2, then there will most likely be more than a constant number of nodes that sender and receiver cannot communicate with each other."}, {"heading": "8.3 Description Length", "text": "In the event that sender and receiver share the same worldview, it follows from the evidence of the earlier theorem that the description must be at least 2log (N) / HD long. The information content of the description must be at least 2log (N) HD / MD. In cases (e.g. when linking records and catalogs) where the graph is reduced to a series of units with literal attribute values because the literals are divided, there is no shortage of common symbols. The length of the description / information content can be used to determine whether we have enough information about one unit to assign it to another. We can also use it to determine how much information we can disclose about someone without revealing their identity."}, {"heading": "8.4 Discussion", "text": "1. The number of nodes that need to be divided is inversely proportional to entropy and thus to the required channel capacity to send the message. Messages that are more compressible need more common names to resolve all references correctly. In extreme cases, for a clique that has zero entropy, each name needs to be shared. 2. As the richness of the description language grows, HD grows and the minimum number of nodes that need to be shared decreases. If HD = 2log (N), only one name needs to be shared. If D = N, then HD = HgN2. So, if Hg > 0, if the recipient is able to decode sufficiently large graphs, we need no more than a constant number of nodes with common names. 3. We can see this as a problem that addresses the addresses: If we use protocol (N) bits optimally, we can construct unique addresses for N objects."}, {"heading": "8.5 Communication overhead", "text": "In this section, we will consider the communication overhead when using descriptions. Consider the overhead when sending a single triple with 2 nodes whose names are not shared. In addition to the triple itself, we have 2 descriptions, each of which is of the size 2log (M) log (N) / HD, where M is the set of possible entries in the adjacence matrix, in the class of permitted descriptions (i.e. the vocabulary or the number of alphabets in the description language). As the descriptions themselves are strings from the adjacence matrix, they can be compressed during communication. As their entropy is HD, their size after compressing is each 2log (M) log (N) log (N) and the total effort is 4log (M) log (N) log (N).If the names of the two nodes have been split, we need 2log (N) bits to express the names of the two nodes."}, {"heading": "9 Acknowledgements", "text": "I would like to thank Andrew Tomkins and Phokion Kolaitis for providing me with a home at IBM Research to begin this work. I would also like to thank Bill Coughran for encouraging me to finish this work at Google. Finally, I would like to thank Vineet Gupta, Andrew Moore and Andrew Tomkins for their feedback on the drafts of this paper."}], "references": [{"title": "Hardening soft information sources", "author": ["W.W. Cohen", "H. Kautz", "D. McAllester"], "venue": "In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201900, pages 255\u2013259, New York, NY, USA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to Algorithms, pages 106\u2013108", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "MIT Press, Cambridge, MA, second edition,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley-Interscience,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Duplicate record detection: A survey", "author": ["A.K. Elmagarmid", "P.G. Ipeirotis", "V.S. Verykios"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 19:1\u201316,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The structure and function of complex networks", "author": ["M.E.J. Newman"], "venue": "SIAM Review, 45(2):167\u2013 256,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Identity uncertainty and citation matching", "author": ["H. Pasula", "B. Marthi", "B. Milch", "S. Russell", "I. Shpitser"], "venue": "In In NIPS. MIT Press,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "The mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal, 27:379\u2013423,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1948}], "referenceMentions": [{"referenceID": 6, "context": "One of the earliest descriptions of this problem was in Shannon\u2019s seminal paper [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "The research by ([4], [1] and [6]) are archetypal of the approaches that have been followed for solving this class of problems.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "The research by ([4], [1] and [6]) are archetypal of the approaches that have been followed for solving this class of problems.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "The research by ([4], [1] and [6]) are archetypal of the approaches that have been followed for solving this class of problems.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Recently here has been considerable work on other random graph models [5], such as those involving preferential attachment, which can be useful for modelling structures such as the web.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "More concretely, randomly chosen long enough samples from these rows in the adjacency matrix should obey the asymptotic equipartition property (AEP) [3].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "From [2] we know that the estimated number of collisions is C = N/2J .", "startOffset": 5, "endOffset": 8}], "year": 2014, "abstractText": "Statements about entities occur everywhere, from newspapers and web pages to structured databases. Correlating references to entities across systems that use different identifiers or names for them is a widespread problem. In this paper, we show how shared knowledge between systems can be used to solve this problem. We present \u201dreference by description\u201d, a formal model for resolving references. We provide some results on the conditions under which a randomly chosen entity in one system can, with high probability, be mapped to the same entity in a different system.", "creator": "LaTeX with hyperref package"}}}