{"id": "1406.7444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Learning to Deblur", "abstract": "We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.", "histories": [["v1", "Sat, 28 Jun 2014 21:56:31 GMT  (8880kb,D)", "http://arxiv.org/abs/1406.7444v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["christian j schuler", "michael hirsch", "stefan harmeling", "bernhard sch\\\"olkopf"], "accepted": false, "id": "1406.7444"}, "pdf": {"name": "1406.7444.pdf", "metadata": {"source": "META", "title": "Learning to Deblur", "authors": ["Christian J. Schuler", "Michael Hirsch", "Stefan Harmeling", "Bernhard Sch\u00f6lkopf"], "emails": ["cschuler@tuebingen.mpg.de", "mhirsch@tuebingen.mpg.de", "harmeling@tuebingen.mpg.de", "bs@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "Learning to DeblurChristian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Sch\u00f6lkopf {cschuler, mhirsch, harmeling, bs} @ tuebingen.mpg.deMax Planck Institute for Intelligent SystemsWe describe a learning-based approach to deconvolution of blind images. It uses a deeply layered architecture that is partly borrowed from recent work on learning neural networks and partly incorporates image-specific calculations. The system is continuously trained on a series of artificially generated training examples that enable a competitive performance in blind deconvolution, both in terms of quality and runtime."}, {"heading": "1. Introduction", "text": "In photography, long exposure times could cause the camera to shake, often in combination with image noise caused by low light conditions. Lens deviations can especially lead to blur. We assume that the blurred image y is generated by linear conversion of the underlying images x (sometimes referred to as the \"latent image\"), through a confrontation that is particularly difficult to resolve because it is not only the underlying image, but also the blur."}, {"heading": "2. Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3. Blind deconvolution as a layered network", "text": "Existing methods of rapid blind deconvolution work by alternating between the following three steps [XZJ13; XJ10; CL09]: 1. Feature extraction computes image representations that are useful for core estimation; these representations can simply be the gradient of the blurred image and the gradient of the current estimate, possibly in flat regions [XJ10]; they can also be preliminary estimates of the sharp image, for example, calculated by heuristic nonlinear filtering [CL09].2. The kernel estimate uses the extracted features to estimate the convolution nucleus.3. The image estimation attempts to compute an approximation of the sharp image using the current kernel estimation.We represent these steps by means of a traceable deep neural network, which gives them more flexibility and allows them to adapt optimally to the problem.3. The layers of the network switch between (1) a local evolutionary network estimation, resulting in a blurring of the good properties (2) and the extraction becomes a complete problem (2)."}, {"heading": "3.1. Architecture layout", "text": "In the following we describe the different parts of the network."}, {"heading": "3.1.1. Feature extraction module", "text": "What constitutes a good feature for core evaluation can reasonably be considered a translation invariant problem, i.e. independent of the position within the image. Therefore, we model the feature extractors based on common weights that apply to all image positions, i.e., as a Convolutionary NN Layer, 1 which generates multiple feature representations of the image, followed by two layers that introduce nonlinearity into the model. First, each value is transformed by a tanh unit, then the feature representations are combined pixel by pixel linearly into new hidden images, with the function tanh formally speaking."}, {"heading": "3.1.2. Kernel estimation module", "text": "FOR the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RONG for the RON"}, {"heading": "3.1.3. Image estimation module", "text": "Before another feature extraction module is added, the estimated Core is used to receive an update of the sharp latent image (3): analog to equation (3), we resolve for x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "3.2. Iterations as stacked networks", "text": "The Feature Extraction Module, the Kernel Estimation Module, and the Image Estimation Module can be stacked multiple times, corresponding to multiple iterations for unlearned blind deconvolution methods. This results in a single NN that can be trained end-to-end with back propagation by taking the derivatives of all steps (see Appendix A for the derivatives of the solutions to equivalents. (3) and the analog equivalent. (5)), which increases performance, as shown in Fig. 3 (but simultaneously increases runtime). Similar to other blind deconversion approaches, we also use a multi-scale method. The first (and coarse) scale is just a network as described above, trained for a specific kernel size. For the second scale, the previous scale network is applied to a scaled down version of the blurred image, and its estimated lateral image is scaled back to the second scale."}, {"heading": "3.3. Training", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4. Implementation", "text": "We provide the code for both training and testing our downloadable method. For training, we use our own C + + / CUDA-based neural network toolbox. After a one-time training for a specific blur class (e.g. camera shaking), which lasts about two days per level, the use of the network is very fast and can be done in Matlab without any additional dependencies. Running times on an Intel i5 that only uses Matlab are shown in Table 1. The most expensive calculation is the creation of multiple hidden representations in the wavelength layer of the NN (in this example: 32)."}, {"heading": "5. Experiments", "text": "Unless otherwise stated, all experiments were performed with a multi-scale, three-stage architecture. We use up to three scales for cores of size 17 x 17, 25 x 25, 33 x 33. On each scale, each feature extraction module consists of a folding layer with 32 filters, a tanh layer, a linear recombination to 32 new hidden images, another tanh layer, and a recombination to four gradient-like images, two each for x-i and y-i (in the third stage: eight gradient-like images). In the case of the network with blur cores of size 33 x 33, we uncouple the estimated cores with a small Gaussian with \u03c3 = 0.5 to counteract the smoothing effect of the L2 standard used during the training. For the specific choice of architecture, we refer to the influence of model parameters on the kernel estimation performance in Figures 3 to 4."}, {"heading": "5.1. Image content specific training", "text": "In a number of recent works [HY12; Sun + 13; Wan + 13], it was pointed out the inadequacy of modern algorithms, which rely on the presence of strong prominent edges and their reduced performance in images with structured scenes such as natural landscapes, due to the absence of the so-called image prediction step, which uses a combination of bilateral and shock filtering to restore latent edges used for a subsequent core assessment. In this context, learning the latent image prediction step offers a major advantage: by training our network with a particular class of images, we can focus on those features that are informative for the particular type of image. In other words, the network learns content-specific non-linear filters that provide improved performance. To demonstrate this, we have used the same training procedure as described above, but we reduced the image set to images from a particular image category within the image category, which we have coached the 13,000 images in total image categories, which we have compared with the 13,000 images in total."}, {"heading": "5.2. Noise specific training", "text": "To counteract noise in blurry images, current exposure algorithms during latent image prediction use such2ImageNet 2011 Fall Release > Geological Education > Natural Depression, Depression > Valley, vale (http: / / www.image-net.org / synset? wnid = n028468604) 3ImageNet 2011 Fall Release > Artefact > Leaf, Flat, Solid > Blackboard, Chalk Board (http: / / www.image-net.org / synset? wnid = n02846511) as bilateral filtration [CL09] or Gaussian filtration [XJ10]. However, in a recent paper [Zho + 13], the authors show that current methods do not provide satisfactory results for increased levels of image noise and suggest a novel robust exposure algorithm."}, {"heading": "5.3. Spatially-varying blur", "text": "Since the predictive step of our trained NN is independent of the convolution model, we can also use it in conjunction with the recently proposed fast-forward model of [Hir + 11] to restore images with spatially varying blur. To this end, we replace the objective function Eq. (5) with Eq. (8) of [Hir + 11] in our kernel estimation module (Hir + 10); in a second step, we project the blur kernel onto a motion base aka [Hir + 11], as explained below. Figure 11 shows a comparison between recentric art algorithms for spatially varying blur along with our declassifying results of comparable quality."}, {"heading": "5.4. Comparisons", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4.1. Benchmark Datasets", "text": "We evaluate our method using the standard test sets of [Lev + 09; Sun + 13]. The four images of [Lev + 09] are 255 x 255 pixels in size and artificially blurred with eight different blur cores each, and contain 1% additional Gaussian noise. Performance is shown in Figure 13 on the left, with the blur cores sorted by increasing size. We compare with Levin et al. [Lev + 11], Cho and Lee [CL09] and Xu et al. [XZJ13]. While our method competes with the state of the art for small blur cores, our method lags behind performance in blur grain size by more than 25 x 25 pixels. We discuss the reasons for this in Section 6.3. The second benchmark of [Sun + 13] expands this data set to 80 new images, each about one megapixel in size, using the same blur core size as in [Lev + 11] and the results of [Sun + 13]."}, {"heading": "5.4.2. Real-World Images", "text": "In Fig. 14 and 15, we show results of our method on images from the real world. Fig. 14 shows examples of invariant blur, while Fig. 15 shows images with spatially varying camera wobble. In both examples, our approach is able to restore images that are qualitatively comparable to the state of the art."}, {"heading": "6. Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Learned filters", "text": "The purpose of the Feature Extraction Module is to highlight and enhance those image features that contain information about the unknown blur core. Figure 16 shows the learned folding layer filters for each of the three levels within a single scale of a trained NN for the core size of 17 x 17 pixels. While the first stage has a single (possibly downscanned) version of a blurred image as input, the subsequent stages take both the restored latent image (obtained by non-blind deconvolution with the current kernel estimate) and the blurred image as input. The outputs of each level are non-linear versions of the input images. In Figure 18 we visualize the effect of the first stage of a NN on the image with two predicted output images and a toy example consisting of four disks that are blurred."}, {"heading": "6.2. Dependence on the size of the observed image", "text": "As noted by Hu and Yang [HY12], blind baring methods are most successful in predicting the nucleus in regions of an image that have strong prominent edges. Other regions are less informative about the nucleus and have shown that larger input would even affect the kernel estimate if included in the blind unfolding algorithm input. Ideally, an estimation method should weight its input according to its information content. In the worst case, larger input would not improve the results, but would not cause any deterioration. We are investigating the behavior of our method in terms of the size of the observed image. It is possible that the NN has learned to ignore image contents that are harmful to the core estimation. In Fig. 19, the predicted cores are shown for varying sized portions of a blurred image. In fact, this example suggests that for our learned algorithm, the core overlaps with the input images, the state of the art does not magnify during learning."}, {"heading": "6.3. Limitations", "text": "One limitation of our current approach is the drop in performance at larger blur cores. Figure 21 shows an example error from the [Lev + 09] benchmark dataset. We believe this is due to the suboptimal architecture of our multi-scale approach at higher resolution scales. While a multi-scale approach performs better than a single-scale network, the observed drop in performance at larger blurs suggests that using the same architecture at all scales is not optimal."}, {"heading": "7. Conclusion", "text": "We have shown that it is possible to learn blind deconvolution automatically by reformulating the task as a single major nonlinear regression problem, mapping between blurred input and predicted core. The basic idea is to incorporate the properties of the generative forward model into our algorithm, namely that the image is interwoven with the same blur core everywhere. While functions are extracted locally in the image, the Core Estimation module combines them globally. Next, the Image Estimation module propagates the information onto the overall image, reducing the difficulty of the problem for the subsequent iteration. Our approach can adapt to different settings (e.g. blurred images with strong noise or certain image classes), and it could be expanded further to combine blurring with other steps in the image pipeline, including oversaturation, Bayer filtering, HDR, or superresolution."}, {"heading": "Appendix A Quotient Layer", "text": "(2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (2) F. (4) F. (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4).). (4). (4).). (4)."}], "references": [{"title": "Blur identification using neural network for image restoration", "author": ["I. Aizenberg", "D. Paliy", "C. Moraga", "J. Astola"], "venue": "Computational Intelligence, Theory and Applications. Vol. 38. Springer,", "citeRegEx": "Aizenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aizenberg et al\\.", "year": 2006}, {"title": "A Framework for the Cooperation of Learning Algorithms", "author": ["L. Bottou", "P. Gallinari"], "venue": "Advances Neural Information Processing Systems", "citeRegEx": "Bottou and Gallinari.,? \\Q1991\\E", "shortCiteRegEx": "Bottou and Gallinari.", "year": 1991}, {"title": "Blur identification and image restoration using a multilayer neural network", "author": ["C.M. Cho", "H.S. Don"], "venue": "IEEE Int. Joint Conf. Neural Networks", "citeRegEx": "Cho and Don.,? \\Q1991\\E", "shortCiteRegEx": "Cho and Don.", "year": 1991}, {"title": "Fast motion deblurring", "author": ["S. Cho", "S. Lee"], "venue": "ACM Trans. Graphics", "citeRegEx": "Cho and Lee.,? \\Q2009\\E", "shortCiteRegEx": "Cho and Lee.", "year": 2009}, {"title": "Nonlinear image processing using artificial neural networks", "author": ["D. De Ridder"], "venue": "Advances Imaging and Electron Physics", "citeRegEx": "Ridder,? \\Q2003\\E", "shortCiteRegEx": "Ridder", "year": 2003}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Deng,? \\Q2009\\E", "shortCiteRegEx": "Deng", "year": 2009}, {"title": "Image processing with neural networks\u2014a review", "author": ["M. Egmont-Petersen", "D. de Ridder", "H. Handels"], "venue": "Pattern recognition", "citeRegEx": "Egmont.Petersen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Egmont.Petersen et al\\.", "year": 2002}, {"title": "Removing camera shake from a single photograph", "author": ["R. Fergus"], "venue": "ACM Trans. Graphics", "citeRegEx": "Fergus,? \\Q2006\\E", "shortCiteRegEx": "Fergus", "year": 2006}, {"title": "Single Image Deblurring Using Motion Density Functions", "author": ["A. Gupta"], "venue": "IEEE Europ. Conf. Computer Vision", "citeRegEx": "Gupta,? \\Q2010\\E", "shortCiteRegEx": "Gupta", "year": 2010}, {"title": "Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.", "author": ["S. Harmeling", "M. Hirsch", "B. Sch\u00f6lkopf"], "venue": "Advances Neural Information Processing Systems", "citeRegEx": "Harmeling et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Harmeling et al\\.", "year": 2010}, {"title": "Single image deblurring with adaptive dictionary learning", "author": ["Z. Hu", "J.-B. Huang", "M.-H. Yang"], "venue": "IEEE Int. Conf. Image Processing", "citeRegEx": "Hu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2010}, {"title": "Efficient filter flow for spacevariant multiframe blind deconvolution.", "author": ["M. Hirsch", "S. Sra", "B. Sch\u00f6lkopf", "S. Harmeling"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Hirsch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2010}, {"title": "Fast removal of nonuniform camera shake", "author": ["M. Hirsch", "C.J. Schuler", "S. Harmeling", "B. Sch\u00f6lkopf"], "venue": "IEEE Int. Conf. Computer Vision", "citeRegEx": "Hirsch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2011}, {"title": "Good Regions to Deblur", "author": ["Z. Hu", "M.-H. Yang"], "venue": "Lecture Notes in Computer Science. Springer,", "citeRegEx": "Hu and Yang.,? \\Q2012\\E", "shortCiteRegEx": "Hu and Yang.", "year": 2012}, {"title": "Image deblurring using inertial measurement sensors", "author": ["N. Joshi", "S.B. Kang", "C.L. Zitnick", "R. Szeliski"], "venue": "ACM Trans. Graphics", "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Blind deconvolution using a normalized sparsity measure", "author": ["D.Krishnan", "T. Tay", "andR. Fergus"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "D.Krishnan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "D.Krishnan et al\\.", "year": 2011}, {"title": "Understanding and evaluating blind deconvolution algorithms", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Efficient marginal likelihood optimization in blind deconvolution", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Levin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2011}, {"title": "The matrix cookbook", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": null, "citeRegEx": "Petersen and Pedersen.,? \\Q2012\\E", "shortCiteRegEx": "Petersen and Pedersen.", "year": 2012}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Discriminative Non-blind Deblurring", "author": ["U. Schmidt"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Schmidt,? \\Q2013\\E", "shortCiteRegEx": "Schmidt", "year": 2013}, {"title": "Blind deconvolution of images by use of neural networks", "author": ["R.J. Steriti", "M.A. Fiddy"], "venue": "Optics letters", "citeRegEx": "Steriti and Fiddy.,? \\Q1994\\E", "shortCiteRegEx": "Steriti and Fiddy.", "year": 1994}, {"title": "High-quality motion deblurring from a single image", "author": ["Q. Shan", "J. Jia", "A. Agarwala"], "venue": "ACM Trans. Graphics", "citeRegEx": "Shan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shan et al\\.", "year": 2008}, {"title": "Edge-based blur kernel estimation using patch priors", "author": ["L. Sun", "S. Cho", "J. Wang", "J. Hays"], "venue": "IEEE Int. Conf. Computational Photography", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Neural network image deconvolution", "author": ["J.E. Tansley", "M.J. Oldfield", "D.J. MacKay"], "venue": "Maximum Entropy and Bayesian Methods. Fundamental Theories of Physics. Springer,", "citeRegEx": "Tansley et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Tansley et al\\.", "year": 1996}, {"title": "Nonedge-specific adaptive scheme for highly robust blind motion deblurring of natural images", "author": ["C.Wang"], "venue": "IEEE Trans. Image Processing", "citeRegEx": "C.Wang,? \\Q2013\\E", "shortCiteRegEx": "C.Wang", "year": 2013}, {"title": "Non-uniformDeblurring for Shaken Images", "author": ["O.Whyte", "J. Sivic", "A. Zisserman", "J. Ponce"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "O.Whyte et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Whyte et al\\.", "year": 2010}, {"title": "Revisiting Bayesian Blind Deconvolution", "author": ["D. Wipf", "H. Zhang"], "venue": "eprints", "citeRegEx": "Wipf and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Wipf and Zhang.", "year": 2013}, {"title": "Two-Phase Kernel Estimation for Robust Motion Deblurring", "author": ["L. Xu", "J. Jia"], "venue": "Computer Vision \u2013 ECCV 2010. Lecture Notes in Computer Science. Springer,", "citeRegEx": "Xu and Jia.,? \\Q2010\\E", "shortCiteRegEx": "Xu and Jia.", "year": 2010}, {"title": "Unnatural l0 sparse representation for natural image deblurring", "author": ["L. Xu", "S. Zheng", "J. Jia"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "IEEEConf. Computer Vision and Pattern Recognition", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}, {"title": "ADADELTA:AnAdaptive Learning RateMethod", "author": ["M.D. Zeiler"], "venue": "In:ArXiv e-prints", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Handling Noise in Single Image Deblurring Using Directional Filters", "author": ["L. Zhong"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Zhong,? \\Q2013\\E", "shortCiteRegEx": "Zhong", "year": 2013}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "IEEE Int. Conf. Computer Vision", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}], "referenceMentions": [], "year": 2014, "abstractText": "We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.", "creator": "LaTeX with hyperref package"}}}