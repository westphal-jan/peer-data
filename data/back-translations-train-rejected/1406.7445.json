{"id": "1406.7445", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Contrastive Feature Induction for Efficient Structure Learning of Conditional Random Fields", "abstract": "Structure learning of Conditional Random Fields (CRFs) can be cast into an L1-regularized optimization problem. To avoid optimizing over a fully linked model, gain-based or gradient-based feature selection methods start from an empty model and incrementally add top ranked features to it. However, for high-dimensional problems like statistical relational learning, training time of these incremental methods can be dominated by the cost of evaluating the gain or gradient of a large collection of candidate features. In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which only evaluates a subset of features that involve both variables with high signals (deviation from mean) and variables with high errors (residue). We prove that the gradient of candidate features can be represented solely as a function of signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real data sets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting.", "histories": [["v1", "Sat, 28 Jun 2014 22:13:52 GMT  (511kb)", "http://arxiv.org/abs/1406.7445v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ni lao", "jun zhu"], "accepted": false, "id": "1406.7445"}, "pdf": {"name": "1406.7445.pdf", "metadata": {"source": "CRF", "title": "Contrastive Feature Induction for Efficient Structure Learning of Conditional Random Fields", "authors": ["Ni Lao", "Jun Zhu"], "emails": ["nlao@cs.cmu.edu", "junzhu@cs.cmu.edu"], "sections": [{"heading": null, "text": "We demonstrate that the gradient of candidate characteristics can only be represented as a function of signals and errors, and that CFI represents an efficient approximation of gradient-based evaluation methods. Experiments with synthetic and real data sets show that CFI exhibits competitive learning speed and accuracy in paired CRFs compared to modern structure learning methods such as full optimization across all characteristics and grafting. Interestingly, CFI is not only faster than other methods, but also produces models with higher predictive accuracy by focusing on large prediction errors during introduction."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Prel iminaries", "text": "Conditional fields (CRFs [12]) are Markov Random Fields (MRFs) that are globally conditioned on observations. We use uppercase letters like X, Y for random variables, and lowercase letters like x, y for approximate assignments. Bolded letters are vectors of variables or assignments. Let G = (V, E) be an undirected model, using observed variables O, labeled variables Y, and hidden variables H. Here we look at the general case of hidden variables, and it subsumes regular CRFs with no hidden variables when H is empty. Let X = (Y, H) be the joint of hidden and labeled variables. Then X has the distribution (1) according to the model. Feature functions fk (x, o) count how often a feature fires in the CRFs."}, {"heading": "3. Contrastive Feature Induction (CFI)", "text": "Although we define CRFs with hidden variables in the previous section, CFI can be applied to CRFs with or without hidden variables = = 500q = i. We only refer to each training sample as (x i, o i) to subsume both cases. First, let's consider pairs of Markov networks in which each trait function is an indicator function for a specific mapping to either a pair of nodes or a single node (x x). Here, we present a quick approximation of the method. Before we delve into the exposure of the algorithm, let us first define two key concepts: Letq0 i = x) and q1 (X = x) will define the estimates of the variable X characteristics of the instance in qi (x) and qi (x)."}, {"heading": "4. Experiment", "text": "In this study, we report on empirical results for paired networks by comparing CFI with state-of-the-art structural learning methods, including complete optimization across all characteristics (Full-L1) [1] and transplantation [17]. The original transplantation method optimizes the model to convergence after each batch of new characteristics has been added. Empirically, we found that it is more efficient to update only one iteration after each batch of characteristics has been added, and the quality of the learned models is not sacrificed. In accordance with the Kok and Domingos methodology [10], we set our experiment to prediction tasks. Ten-fold cross-validation occurs as follows: We first mark a randomly selected 10% mark in the data as hidden during the training, and then evaluate on these labels during the test, depending on the observed training labels. The performance of the systems is calculated based on training time, the pre-error rate, and the liquidity quidity (L)."}, {"heading": "4 .1 . Sy nthe t i c Da ta", "text": "Following the method described by Lee et al. [13], we generated synthetic data by Gibbs scanning (10000 iteration burn-in, 1000 iteration thinning) on synthetic networks. A network structure with N nodes was generated by treating every possible edge as a Bernoulli random variable and scanning the edges. We opted for the parameter of the Bernoulli distribution so that each node had on average K neighbors. The weights of the selected edges are drawn from an even distribution within [-5, 5]. We fix 1 = 2, M = 200 (number of samples), k = 5, and vary N. We also compare with TrueGraph, for which characteristics are optimized in the true models and no function induction is applied. To check the distribution of signals and errors, we draw their histograms in Figure 1. We can see that signals and errors are mostly concentrated around 0, by ignoring essential calculations of small terms."}, {"heading": "4 .2 . Re la t io na l Lea rning Da ta", "text": "In this section, we compare different structural learning methods for TR-MRFs and compare their performance with other learning states. However, we have no prior work on the learning structure of TR-MRFs. We have no prior work on the learning structure of TR-MRFs. In this section, we compare different structural learning methods for TR-MRFs. We have no prior work on the learning structure of TR-MRFs. We have no prior work on the learning structure of TR-MRFs. We have no prior work on the learning structure of TR-MRFs. We have no prior work on the learning structure of TR-MRFs. In this section, we compare different structural learning methods for TR-MRFs and compare their performance with other learning states."}, {"heading": "5. Conclusion", "text": "In this study, we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which evaluates a subset of features that include variables with high signal (deviation from the mean) or error (prediction arrears). We prove that CFI is an efficient approximation of gradient-based evaluation methods. Experiments with synthetic and real data sets show that CFI is not only faster than grafting and full optimization of all features, but also produces models of higher prediction accuracy by focusing on large prediction errors."}], "references": [{"title": "Scalable training of L1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In ICML", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Inducing Features of Random Fields", "author": ["S. Della Pietra", "V. Della Pietra", "Lafferty"], "venue": "IEEE Transactions Pattern Analysis And Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Markov Logic: A Unifying Framework for Statistical Relational Learning", "author": ["P. Domingos", "M. Richardson"], "venue": "Proceedings of the ICML-2004 Work-shop on Statistical Relational Learning and its Connections to Other Fields (pp. 49-54)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Ideal Parent\u2016 Structure Learning for Continuous Variable Bayesian Networks", "author": ["G. Elidan", "G. Nachman", "N. Friedman"], "venue": "Journal of Machine Learning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Introduction to statistical relational learning", "author": ["L. Getoor", "B. Taskar"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Discriminative Structure and Parameter Learning for Markov Logic Networks", "author": ["T. Huynh", "R. Mooney"], "venue": "In Proceedings of the 25th Inter-national Conference on Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Template based inference in symmetric relational Markov random fields", "author": ["A. Jaimovich", "O. Meshi", "N. Friedman"], "venue": "In Proc", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "Saul L. K"], "venue": "Learning in Graphical Models,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Statistical Predicate Invention", "author": ["S. Kok", "P. Domingos"], "venue": "Proceedings of the Twenty-Fourth International Conference on Machine Learning (pp. 433-440)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning Markov Logic Network Structure via Hypergraph Lifting", "author": ["S. Kok", "P. Domingos"], "venue": "Proceedings of the Twenty-Sixth International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Efficient Structure Learning of Markov Networks using L1-Regularization.\" Advances in Neural In-formation Processing Systems", "author": ["Lee", "S.-I", "V. Ganapathi", "D. Koller"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Efficiently inducing features of conditional random fields", "author": ["A. McCallum"], "venue": "In Proc. UAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Loopy belief propagation for approximate inference: an empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1988}, {"title": "Grafting: Fast, incremental feature selection by gradient descent in function spaces", "author": ["S. Perkins", "K. Lacker", "J. Theiler"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Combinatorial stochastic processes (Technical Report 621)", "author": ["J. Pitman"], "venue": "Department of Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Joint inference in information extraction", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. AAAI-07", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Stat.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1978}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "P. Abbeel", "D. Koller"], "venue": "Proc. UAI-02", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "A New Learning Algorithm for Mean Field Boltzmann Machines", "author": ["M. Welling", "G.E. Hinton"], "venue": "ICANN", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "A generalized mean field algorithm for variational inference in exponential families, Uncertainty in Artificial Intelligence (UAI2003), (eds", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "Introduction Conditional Random Fields (CRFs) [12] are widely used in applications like computer vision, natural language processing, information extraction, relational learning, computational biology etc.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "For a very long time, the dominant solution to this structure learning problem has been greedy local search [2][14].", "startOffset": 108, "endOffset": 111}, {"referenceID": 13, "context": "For a very long time, the dominant solution to this structure learning problem has been greedy local search [2][14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Recent work has formulated this structure selection problem as an optimization problem by applying L1-regularization to a fully linked model [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 12, "context": "However, full optimization requires inference on potentially very dense graphs, which is time prohibitive and inaccurate [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "Approximate inference methods such as loopy Belief Propagation (BP) [16] are likely to give highly inaccurate estimates of the gradient, leading to poorly learned models [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Approximate inference methods such as loopy Belief Propagation (BP) [16] are likely to give highly inaccurate estimates of the gradient, leading to poorly learned models [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 16, "context": "Gain-based or gradient-based feature selection methods avoid this problem by starting from an empty model and incrementally adding top ranked features to it [17][13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "Gain-based or gradient-based feature selection methods avoid this problem by starting from an empty model and incrementally adding top ranked features to it [17][13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "Based on this insight, we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI) based on Mean Field Contrastive Divergence (CD MF ) [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "\u2019s \"ideal parent\" algorithm [4] for continuous variable Bayes networks provides a nice principle for detecting potential graph structures.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "It uses a monotonic function of the cosine similarity between signal vectors and error vectors of variables to filter candidate structures before scoring them with more costly BIC [20].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "Prel iminaries Conditional Random Fields (CRFs [12]) are Markov Random Fields (MRFs) that are globally conditioned on observations.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "In this case, p(x|o;\u03b8) can be approximated by methods like belief propagation [15], or mean field approximation [8].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "In this case, p(x|o;\u03b8) can be approximated by methods like belief propagation [15], or mean field approximation [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "(5), where \u03bb1 controls L1-regularization to help structure selection [1], and \u03bb2 controls L2- regularization to prevent over fitting.", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Since l(\u03b8) and g(\u03b8) are intractable to compute, we use 1-step Mean Field Contrastive Divergence (CD MF ) [22] as an approximation.", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "By starting from q0, q1 always goes to the same mode as q0 [22].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "We use orthant-wise L-BFGS [1] to tune \u03b8.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "The CFI algorithm shares the same principle suggested by the \u2015ideal parent\u2016 algorithm [4] \u2014 if one variable has high correlation to another variable\u2019s prediction error, a link should be formed between them.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Experiment In this study, we report empirical results for pairwise networks by comparing CFI with state-of-the-art structure learning methods including full optimization over all features (Full-L1) [1], and Grafting [17].", "startOffset": 198, "endOffset": 201}, {"referenceID": 16, "context": "Experiment In this study, we report empirical results for pairwise networks by comparing CFI with state-of-the-art structure learning methods including full optimization over all features (Full-L1) [1], and Grafting [17].", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "Following the methodology of Kok and Domingos [10], we situate our experiment in prediction tasks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "Performances of the systems are measured by training time, prediction error rate, average Conditional Log-Likelihood (CLL), and Area Under precision-recall Curve (AUC) [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "[13] we generated synthetic data through Gibbs sampling (10000 iteration burn-in, 1000 iteration thinning) on synthetic networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Weights of the chosen edges are drawn from an even distribution within [-5, 5].", "startOffset": 71, "endOffset": 78}, {"referenceID": 4, "context": "Re la t io na l Lea rning Da ta There has been a surge of interests in Statistical Relational Learning (SRL [5]) driven by applications like collective classification, information extraction and integration, bioinformatics, et al.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "Among many proposed SRL models, Template-Based Relation Markov Random Fields (TR-MRFs [7]) is a powerful model.", "startOffset": 86, "endOffset": 89}, {"referenceID": 18, "context": "The binary representation of MLN causes deterministic dependencies when modeling multi-category concepts, and requires special treatment like slice-sampling during inference to help find separate modes of distribution [19].", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "[7]; we leave out implementation details here due to the limitation of space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Note that instead of using lifted belief propagation for inference [7], we use generalized mean field approximation [23], which is fast and easier to implement.", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "Note that instead of using lifted belief propagation for inference [7], we use generalized mean field approximation [23], which is fast and easier to implement.", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "edu [10]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "reported in [10].", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "The MLN structure learning algorithm (MSL [11]) creates candidate clauses by adding literals to the current clauses.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Infinite Relational Model (IRM [9]) simultaneously clusters objects and relations.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "It uses a Chinese restaurant process prior (CRP [18]) on the cluster assignments to automatically control number of clusters.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "Multiple Relational Clusterings (MRC [10]) is based on MLN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "Table 1 compares performance of different methods on the two data sets , where the result of MSL/IRM/MRC are directly cited from [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "Kok & Domingos [10] did not report error rates.", "startOffset": 15, "endOffset": 19}], "year": 2012, "abstractText": "Structure learning of Conditional Random Fields (CRFs) can be cast into L1-regularized optimization problems, which can be solved by efficient gradient-based optimization methods. However, optimizing a fully linked model may require inference on dense graphs which can be time prohibitive and inaccurate. Gain-based or gradient-based feature selection methods avoid this problem by starting from an empty model and incrementally adding top ranked features to it. However, training time with these incremental methods can be dominated by the cost of evaluating the gain or gradient of candidate features for high-dimensional problems. In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI) based on Mean Field Contrastive Divergence (CD MF ). CFI only evaluates a subset of features which involve variables with high signals (deviation from mean) or errors (prediction residue). We prove that the gradient of candidate features can be represented solely as a function of signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real datasets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting. More interestingly, CFI is not only faster than other methods but also produces models with higher prediction accuracy by focusing on large prediction errors during induction.", "creator": "Microsoft\u00ae Word 2010"}}}