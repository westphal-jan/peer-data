{"id": "1407.0107", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2014", "title": "Randomized Block Coordinate Descent for Online and Stochastic Optimization", "abstract": "Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent (OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, for the first time, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.", "histories": [["v1", "Tue, 1 Jul 2014 05:57:43 GMT  (223kb)", "http://arxiv.org/abs/1407.0107v1", null], ["v2", "Sat, 12 Jul 2014 21:03:06 GMT  (223kb)", "http://arxiv.org/abs/1407.0107v2", null], ["v3", "Sat, 26 Jul 2014 19:16:39 GMT  (232kb)", "http://arxiv.org/abs/1407.0107v3", "The errors in the proof of ORBCD with variance reduction have been corrected"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huahua wang", "arindam banerjee"], "accepted": false, "id": "1407.0107"}, "pdf": {"name": "1407.0107.pdf", "metadata": {"source": "CRF", "title": "Randomized Block Coordinate Descent for Online and Stochastic Optimization", "authors": ["Huahua Wang"], "emails": ["huwang@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 140 7.01 07v1 [cs.LG] 1 Jul 2"}, {"heading": "1 Introduction", "text": "In fact, most of them are in a position to decide whether they will be able to leave the country, or whether they will be able to leave it."}, {"heading": "2 Related Work", "text": "In this section we will briefly discuss the two types of methods with low cost-per-iteration gradient descent (GD), i.e. OGD / SGD and RBCD. Applying GD to (1) results in the following iteration: xt + 1 = argmin x < \u0394f (xt), x > + g (x) + \u03b7t2 \u0445 x \u2212 xt \"22. (2) In some cases, e.g. g (x) is a standard, (2) a solution may be available in closed form."}, {"heading": "2.1 Online and Stochastic Gradient Descent", "text": "In (2), it requires the calculation of the complete gradient of m samples in each iteration, which could be arithmetically expensive if m is too large. (3), if the S sample is a solution + 1 + 1 + 1 = argmin x < / 2), if the S sample (3), where the S sample is given and assumed to be convex. (3), then a function + 1 is revealed that causes the loss (xt). OGD's performance is measured by repentance, which is the discrepancy between the cumulative loss over T rounds and the best decision in Hindsight, R (T) = T sample = 1 [x t) + g [ft (x t)]. (4), where the best decision in Hindsight is R (x) + g (x)."}, {"heading": "2.2 Randomized Block Coordinate Descent", "text": "Let us assume that xj (1 \u2264 j) are non-overlapping blocks. In iteration t, RBCD [23, 24, 17] randomly selects the jt coordinate and triggers the following problem: xt + 1jt = argminxjt < stjtf (x t), xjt > + gjt (xjt) + \u03b7t 2 stjt = argminxjt < stjt > gjt (xjt) + \u03b7t 2 stjt = argminxjt < stjt > gjt (xjt) stjt = stjt = stjt = stjt = stjt = t > t > t = xt > t > t > t > t > t = xt > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t > t &ltt >"}, {"heading": "3 Online Randomized Block Coordinate Descent", "text": "In this section, our goal is to combine OGD / SGD and RBCD together to solve a problem (1). We call the algorithm online randomized block coordinate descent (ORBCD), which calculates a block coordinate of the gradient of a sample block in each iteration. ORBCD essentially performs RBCD in online and stochastic setting. Let {x1, \u00b7, xJ}, xj block coordinates in x. For each partition of x and U, x = J sums in x. Let Uj-Rn-nj nj columns be an n-n permutation matrix U, corresponding to j block coordinates in x. For each partition of x and U, x = J sums in x."}, {"heading": "3.1 ORBCD for Online Learning", "text": "In the online configuration, ORBCD considers the worst case and runs in rounds. In due course, specifying a function ft that may be agnostic, ORBCD randomly selects the jt-block coordinate and presents the solution by solving the following problem: xt + 1jt = argminxjt < jtft (x t), xjt > + gjt (xjt) + 2-block coordinate by solving: xt + 1jt = argminxjt < jtft (x t), xjt > gjt (xjt) + 2-block coordinate: xt + 1jt = argminxjt < jtt (x t)"}, {"heading": "3.2 ORBCD for Stochastic Optimization", "text": "In the stochastic setting, ORBCD first randomly takes the it-th block sample and then randomly selects the jt-th block coordinate. The algorithm has the following iteration: xt + 1jt = argminxjt < p > + gjt (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p) p (6) p) p (6) p (6) p) p (6) p (6) p) p (4) p (4) p (4) p (4) p (4) p (4) p) p) p (4) p (4) p (4) p) p (4) p (4) p (4) p (4) p (p) p) p (4) p (4) p (4) p) p (4) p (4) p (4) p) p (4) p (4) p (p) p) p (4) p (4) p (4) p) p (4) p (p) p (4) p (4) p (p) p (4) p) p (4) p (4) p (p) p (4) p (4) p (p) p (4) p (4) p (4) p (p) p (4) p (4) p (4) p (p (p) p (4) p (4) p (4) p (p (p) p (4) p (4) p (p (4) p (4) p (p (p (p) p (4) p (p (p) p (4) p (4) p (p (p) p (4) p (4) p (p (p \"p (p) p (p (p) p (4)."}, {"heading": "3.3 ORBCD with variance reduction", "text": "In the stochastic setting, we apply the variance reduction technology [36, 13] to accelerate the convergence rate of ORBCD (abbreviated as ORBCDVD) (44). \u2212 In the inner stage, the variance reduction is performed by k = 0, \u00b7 \u00b7, m \u2212 1. In the k-th stage, an estimate x = xt of the optimal x-th sample and updates x-th iterations is required. \u2212 In the inner stage, an iteration is performed by k = 0, \u00b7, m \u2212 1. In the k-th iteration, the ORBCDVD is randomly selected with x-th samples and updates x-th of the x-th. \u2212 In the third stage, the iteration is performed by k = 0, \u00b7, m \u2212 1. In the k-th iteration, the ORBCDVD is randomly selected with x-th samples and updates. \u2212 In the fourth stage, the ORBCDVD is randomly selected. \u2212 in the third stage, the jate-kart (jate-kart x) and \u2212 kart (\u2212 x). \u2212"}, {"heading": "4 The Rate of Convergnce", "text": "The following problem is a key block for convergence of ORBCD in both ranges < < < / p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ p) p (+ p) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p (+ 1) p) p (+ 1) p) p (+ 1) p (+ 1) p) p (+ 1) p (+ 1) p) p (+ 1) p) p (+ 1) p (+ 1) p"}, {"heading": "4.1 Online Optimization", "text": "The following theorem sets the regrettable limit of ORBCD.Theorem 1: \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "4.2 Stochastic Optimization", "text": "In the stochastic setting, ORBCD first selects the it-th block sample and then the jt-th block coordinate. jt and it are independent. xt depends on the observed realization of the random variables. (i1, j1), (it \u2212 1, jt \u2212 1), (it \u2212 1), (it \u2212 1, jt \u2212 1), (it \u2212 1, jt \u2212 1), (it \u2212 1), (it \u2212 1), (it \u2212 1, jt \u2212 1), (it \u2212 1), (it \u2212 t), (it \u2212 1, jt \u2212), (it \u2212 1, jt), jt (it \u2212 1, jt \u2212 1), jt (jt \u2212), jt (jt), jt (jt), jt (, jt, jt (), jt (jt, jt, jt (), jt (jt, jt, jt (), jt (, jt, jt, jt (), jt (, jt, jt, jt (), jt (, jt),"}, {"heading": "4.3 ORBCD with Variance Reduction", "text": "The following results usually follow [36, 13].Lemma 2 Define h (x) = f (x) + g (x). Let x (x) be an optimal solution and L = maxi Li, we have1II (x) \u2212 fi (x) \u2212 fi (x). \u2212 fi (x). \u2212 f (x) \u2212 f (x). (61) Lemma 3 Let vikjk and x \u2212 f (x). \u2212 f (28). (62) Proof: Conditioned on xk, we haveE vikjk \u2212 jkf (x). (x). (x). (x). (x). (xk). (xk)."}, {"heading": "5 Conclusions", "text": "We proposed an online randomized block coordinate decrease (ORBCD), which combines online / stochastic gradient decrease and randomized block coordinate decrease. ORBCD is well suited for large-scale high-dimensional problems with non-overlapping composite regulators. We determined the convergence rate for ORBCD, which has the same sequence as OGD / SGD. For stochastic optimizations with strongly convex functions, ORBCD can converge in anticipation of a geometric rate by decreasing the variance of the stochastic gradient."}], "references": [{"title": "Convex Optimization with Sparsity-Inducing Norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Coordinate descent method for large-scale l2-loss linear support vector machines", "author": ["K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Proximal splitting methods in signal processsing. Fixed-Point Algorithms for Inverse Problems in Science and Engineering", "author": ["P. Combettes", "J. Pesquet"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A note on the group lasso and sparse group lasson", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "S. Keerthi C.-J. Lin", "S. Sundararajan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Juditsky", "A. Nemirovski", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Richtarik. Semi-stochastic gradient descent methods", "author": ["P.J. Konecny"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Coordinate descent optimization for l1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Z. Lu", "L. Xiao"], "venue": "ArXiv,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "On the convergence of the coordinate descent method for convex differentiable minimization", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Mixed optimization for smooth functions", "author": ["L. Zhang M. Mahdavi", "R. Jin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["N. Le Roux M. Schmidt", "F. Bach"], "venue": "Technical Report HAL 00860051,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A", "author": ["Y. Nesterov"], "venue": "Basic Course. Kluwer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical Report 76, Center for Operation Research and Economics (CORE), Catholic University of Louvain (UCL),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization methods", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richtarik", "M. Takac"], "venue": "Mathematical Programming,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richtarik", "M. Takac"], "venue": "ArXiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N. Le Roux", "M. Schmidt", "F. Bach"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "On the non-asymptotic convergence of cyclic coordinate descent methods", "author": ["A. Saha", "A. Tewari"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Stochastic methods for l1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Mini-batch primal and dual methods for svms", "author": ["M. Takac", "A. Bijral", "P. Richtarik", "N. Srebro"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["L. Zhang", "M. Mahdavi", "R. Jin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 30, "context": "For example, in lasso [32], fi is a square loss or logistic loss function and g(x) = \u03bb\u2016x\u20161 where \u03bb is the tuning parameter.", "startOffset": 22, "endOffset": 26}, {"referenceID": 35, "context": "In group lasso [37], gj(xj) = \u03bb1\u2016xj\u20162, which enforces group sparsity among variables.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "To induce both group sparsity and sparsity, sparse group lasso [9] uses composite regularizers gj(xj) = \u03bb1\u2016xj\u20162 + \u03bb2\u2016xj\u20161.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 32, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 25, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 13, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 3, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 37, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 10, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 6, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 7, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 33, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 22, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 2, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 24, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 23, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 29, "context": "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].", "startOffset": 75, "endOffset": 87}, {"referenceID": 28, "context": "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].", "startOffset": 75, "endOffset": 87}, {"referenceID": 21, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 5, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 32, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 1, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 6, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 7, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 33, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 26, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 19, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 34, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 12, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 18, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 36, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 26, "context": "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 12, "context": "To address the issue of storage and better explain the faster convergence, [13] proposed an explicit variance reduction scheme into SGD.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "The similar idea was also proposed independently by [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "The results of SVRG is then improved in [15].", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "In [36], SVRG is generalized to solve composite minimization problem by incorporate the variance reduction technique into proximal gradient method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 23, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 16, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 28, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 4, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 11, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 15, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 22, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 120, "endOffset": 128}, {"referenceID": 16, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 120, "endOffset": 128}, {"referenceID": 22, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 23, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 16, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 27, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 31, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 17, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 31, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 60, "endOffset": 68}, {"referenceID": 17, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 60, "endOffset": 68}, {"referenceID": 27, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "To accelerate the convergence speed of ORBCD, we adopt the varaince reduction technique [13] to alleviate the effect of randomness.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "Instead, \u03bc\u0303 can be partially computed at each iteration and then stored for the next retrieval, which may be useful when the computation of full gradient is expensive or data is partially available at the moment [23].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "To accelerate the SGD by reducing the variance of stochastic gradient, stochastic variance reduced gradient (SVRG) was proposed by [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 34, "context": "[36] extends SVRG to composite functions (1), called prox-SVRG.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "(8) By reduding the variance of stochastic gradient, x can converge to x\u2217 at the same rate as GD, which has been proved in [13, 36].", "startOffset": 123, "endOffset": 131}, {"referenceID": 34, "context": "(8) By reduding the variance of stochastic gradient, x can converge to x\u2217 at the same rate as GD, which has been proved in [13, 36].", "startOffset": 123, "endOffset": 131}, {"referenceID": 34, "context": "For strongly convex functions, prox-SVRG [36] can converge linearly in expectation if \u03b7 > 4L and m satisfy the following condition:", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 23, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 16, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 22, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 23, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 16, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 6, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 7, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 33, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 34, "context": "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "The computation of full gradient may require substantial computational eorts, let alone data may be only partially available at the moment [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "For prox-SVRG [36], setting \u03b7 = 10L and m = 100L/\u03b3, \u03c1 \u2248 5/6 in (9).", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 12, "context": "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].", "startOffset": 68, "endOffset": 76}], "year": 2017, "abstractText": "Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent ( OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, for the first time, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}