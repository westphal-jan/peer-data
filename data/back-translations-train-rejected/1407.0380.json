{"id": "1407.0380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2014", "title": "A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech", "abstract": "Several speaker identification systems are giving good performance with clean speech but are affected by the degradations introduced by noisy audio conditions. To deal with this problem, we investigate the use of complementary information at different levels for computing a combined match score for the unknown speaker. In this work, we observe the effect of two supervised machine learning approaches including support vectors machines (SVM) and na\\\"ive bayes (NB). We define two feature vector sets based on mel frequency cepstral coefficients (MFCC) and relative spectral perceptual linear predictive coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture Model (GMM). Several ways of combining these information sources give significant improvements in a text-independent speaker identification task using a very large telephone degraded NTIMIT database.", "histories": [["v1", "Fri, 27 Jun 2014 20:34:05 GMT  (235kb)", "http://arxiv.org/abs/1407.0380v1", "10 pages, 4 figures, International Journal of Signal Processing, Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013"]], "COMMENTS": "10 pages, 4 figures, International Journal of Signal Processing, Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["imen trabelsi", "dorra ben ayed"], "accepted": false, "id": "1407.0380"}, "pdf": {"name": "1407.0380.pdf", "metadata": {"source": "CRF", "title": "A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech", "authors": ["Imen Trabelsi", "Dorra Ben Ayed"], "emails": ["trabelsi.imen1@gmail.com,", "Dorra.mezghani@isi.rnu.tn"], "sections": [{"heading": null, "text": "To solve this problem, we study the use of complementary information at different levels to calculate a combined hit point for the unknown loudspeaker. In this paper, we observe the effect of two monitored machine learning approaches, including support vectors (SVM) and naive Bayes (NB). We define two feature vector sets based on mel-frequency receiver coefficients (MFCC) and relative spectral linear prediction coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture Model (GMM). Several ways to combine these information sources result in significant improvements to a text-independent speaker recognition task using a very large telephone-degraded NTIMIT database. Keywords: Fusion, GMM, SVM, Na\u00efve Bayes, MFCC, RASTA-PLP"}, {"heading": "1. Introduction", "text": "The task of automatic speaker identification is to identify an unknown voice sample as one of several known voice samples. A speaker's identification methods can be divided into text-dependent and text-independent methods. In text-dependent methods, the test expression is known, while it is not known in text-independent methods [9]. In this study, we conducted experiments for text-independent tasks in telephone quality. Generally, the speaker identification system is considered to consist of two important units, namely enrollment (modeling) and identification (matching) as shown in (Figure 1). In the enrollment phase, all samples of a speaker are trained and stored in a database. The goal of enrollment is to construct a model for each speaker based on the characteristics extracted from his / her speech samples. Identification is a process of calculating a matching score between the entered voice functions vector and a model of the speaker's voice phases."}, {"heading": "2. Speaker Modeling", "text": "The function vectors extracted from the training data are used to create a series of loudspeaker models.The modeling of a loudspeaker can be implemented using various techniques.The most common is the Gaussian Mixture Model (GMM).A GMM aims to approximate a complex nonlinear distribution using a mixture of simple Gaussian models.A GMM is a weighted sum of M component densities according to (1): 1 (/) () ().Mi ip x Where x is a d-dimensional vector, bi (x) are the component densities, pi are the mixed weights. Each component density is a d-variant Gaussian function with the form: 11221 1 () exp {() ().2 (2) ti i id iib x x x With the midvectors \u00b5i and covariance matrix i, the mixing weights 11.:"}, {"heading": "3. Classifiers", "text": "In this section, two methods of supervised learning for classifying GMM loudspeaker models are briefly described: NB and SVM."}, {"heading": "3.1. Na\u00efve Bayes (NB)", "text": "The naive classification model of Bayes is a simple classification technique based on Bayes \"theorem. This classifier assumes that the effect of an attribute value on a given class label is independent of the values of the other attributes. This assumption is called class conditional independence. This classifier simply calculates the conditional probabilities of the different classes taking into account the values of attributes and then selects the class with the highest conditional probabilities.When describing an instance with n attributes ai (i = 1... n), then the class assigned to an instance of a class c from possible classes C is according to a Maximum a Posteriori (MAP) Naive Bayes classifier: 1 arg max () ((() jnj ic Cc p p p a c Despite its simplicity, Naive Bayes can often surpass more complex classification methods. For speech recognition, NB algorithm is widely used, as we suggest in our [8] combining the two."}, {"heading": "3.2. Support Vector Machines (SVM)", "text": "Support vector machines are a new technique of the statistical learning theory proposed by Vapnick in 1995 [6] and are based on the principle of structural risk minimization [11] from computer-aided learning theory. SVMs are a new core method that is suitable for binary classification tasks and makes its decisions by constructing a hyperplane that optimally separates two classes. In order to solve non-linear classification problems, the input notation space is typically transformed via a core function into a higher dimensional space where it is possible to define a hyperplane to separate the two classes with maximum marginality."}, {"heading": "4. Multi Level Data Fusion", "text": "Data fusion techniques cover all areas in which the application of a combination of different information sources is concerned. The basic idea is to evaluate the classification results more accurately by efficiently merging the final results of different experts (characteristics, classifiers...) so that the results of such combinations can be superior to all individual experts."}, {"heading": "4.1. Feature Level Fusion", "text": "4.1.1. Vector concatenation: The idea of merging different sources of characteristics in loudspeaker identification is not new. A well-known strategy of data fusion is to merge the vectors with their delta vectors and delta vectors into a single characteristic vector, as shown in Figure 2. Generally, vector concatenation is referred to as classification input fusion [10].4.1.2. Fusion of GMM supervectors: We propose another multi-input fusion by associating the GMM supervectors with characteristics. In this case, we combine the results of the modeling stage as summarized in Figure 3. The multi-supervector is then submitted to the loudspeaker independent classifier for evaluation."}, {"heading": "4.2. Scores Level Fusion", "text": "Research in recent years has shown that fusion can be done at different levels, and the score level fusion is the best in terms of simplicity and the amount of information that should be combined. The basic idea behind the classification combination is that decision-making should not rely on a single classifier, but that classifiers must be involved in decision-making by setting their individual scores. In our case, all data is handled separately and each classifier makes their decisions independently, as shown in Figure 4."}, {"heading": "5. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Corpus Description", "text": "The NTIMIT database consists of TIMIT utterances transmitted over a variety of telephone lines. Each speaker's sentence is transmitted over a different telephone line to have realistic conditions. All utterances are different from speaker to speaker, except for \"SA1\" and \"SA2,\" which occur frequently. We conducted experiments with the DR1 (New England dialect) dialect on 28 speakers (14 female and 14 male). Each speaker uttered ten utterances, eight utterances are selected for training, and two utterances are selected for the identification test. Data was digitized at 16 kHz sampling frequency with 16 bits per sample."}, {"heading": "5.2. Feature Extraction", "text": "The extraction of speech characteristics (front-end) is a central focus of robust speech recognition, which significantly influences recognition performance.In this study, we use the mel-scale frequency cepstral coefficient (MFCC), these coefficients were used as standard acoustic characteristics for the identification system.In fact, MFCC coefficients are the less susceptible coefficient to noise disturbances [15]. We also use relative spectral linear prediction coefficients (RASTA-PLP).This choice is underscored by the fact that these coefficients prove to be more robust for noisy environments [4] and have shown good detection performance in a previous study [16]. In addition, we have added the dynamic cepstrum parameters due to their popularity to automatic speaker recognition systems that accompany delta coefficients."}, {"heading": "5.3. Baseline System", "text": "Our basic system consists of a 128-component GMM-UBM, based on the training statements of all 28 speakers. Individual speaker models are MAP-adapted; only meaning vectors with a relevance factor of 16. After modelling the speakers according to the individual characteristic data, we use the SVM and NB individually for evaluation and as a final step; we combine these two classifiers for each characteristic. We construct three systems: System 1: SVM classifier. System 2: NB classifier. System 3: Combination of SVM and NB classifier. To train SVM, we use a new learning algorithm called sequential minimum optimization (SMO) [14]. SVM algorithm was evaluated using a linear core given by (5). (,)."}, {"heading": "5.4. Results", "text": "The results of each feature are shown in (Table 1) for the three systems.Table 1 shows that the identification rate varies between (50%) and (75%).We note that feature 2 (RASTA-PLP) is better for the three systems.We also note that feature 1 (MFCC) is better than System2 (NB).The table also shows that the maximum IR is generated by combined classifiers for the two characteristics.The results of the combined features are shown in (Table 2) for the three systems.Table 2 shows that the identification rate varies between (50%) and (71, 42%).For feature 3, the results show that the SVM classifier (System 1) is better (60, 71%) than the NB classifier (System 2) (53, 57%)."}, {"heading": "6. Conclusion", "text": "In this paper, we have proposed several methods for identifying text speakers that are very difficult to detect through the telephone network, combining different methods for extracting characteristics and different classifiers to improve detection accuracy; the best detection results are obtained by concatenating GMM supervectors from Rasta PLP characteristics and MFCC characteristics with combined classifiers; in further work, we will attempt to examine the performance of the proposed systems across the entire NTIMIT corpus."}], "references": [{"title": "Robust Text Independent Speaker Identification Using Hybrid GMM-SVM System", "author": ["S.Z. Boujelbene", "D.B.A. Mezghani", "N. Ellouze"], "venue": "Journal of Convergence Information Technology \u2013 JDCTA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Evaluation d\u2019une approche hybride GMM-SVM pour l\u2019identification de locuteurs", "author": ["I. Trabelsi", "D.B. Ayed"], "venue": "La revue e-STA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Rasta plp speech analysis", "author": ["H. Hermansky", "N. Morgan", "A. Bayya", "P. Kohn"], "venue": "International Computer Science Institute,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1947}, {"title": "Speakerverification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "DSP, vol. 10,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Speaker Identification via Support Vector Machies", "author": ["M. Schmidt", "H. Gish"], "venue": "in ICASSP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "LIBSVM: a library for support vector machines", "author": ["C. -C. Chang", "C. -J. Lin"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "On naive bayes in speech recognition", "author": ["L. Toth", "A. Kocsor", "J. Csirik"], "venue": "nt. J. Appl. Math. Comput. Sci.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "An Overview of Automatic Speaker Recognition Technology", "author": ["D. Reynolds"], "venue": "the International Conference on Acoustics, Speech, and Signal Processing ICASSP 02,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A Comparison of fusion techniques in mel-cepstral based speaker idenficication", "author": ["S. Slomka", "S. Sridharan", "V. Chandran"], "venue": "Proc. ICSLP", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Statistical learning theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "An Acoustic-Phonetic Data Base", "author": ["W. Fisher", "V. Zue", "J. Bernstein", "D. Pallet"], "venue": "J. Acoust. Soc. Amer. Suppl. (A),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Maximum Likelihood from incomplete data via the EMalgorithm", "author": ["A.P.N. Dempster", "M.D. Laid", "B. Durbin"], "venue": "J. Royal Statistical Soc., vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1977}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J.C. Platt"], "venue": "Rapport interne, Microsoft Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Comparison of Parametric Representation for Monosyllabic Word Recognition in Continuously Spoken Sentences", "author": ["S.B. Davis", "P. Mermelstein"], "venue": "IEEE Trans. On ASSP, vol. ASSP 28,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1980}, {"title": "Strat\u00e9gies de fusion de param\u00e8tres pour une t\u00e2che d'identification du locuteur en mode ind\u00e9pendant du texte en environnement bruit\u00e9: Application sur le corpus Ntimit", "author": ["I. Trabelsi", "D. Ben Ayed"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "On the Use of Instantaneous and Transitional Spectral Information in Speaker Recognition", "author": ["F.K. Soong", "A.E. Rosenberg"], "venue": "IEEE Trans. Acoustics, Speech and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Sole-Casals, \u201cMaximum Likelihood Linear Programming Data Fusion for Speaker Recognition", "author": ["E. Monte-Moreno", "M. Chetouani", "J.M. Faundez-Zanuy"], "venue": "Speech Communication,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "On Combining Classifiers", "author": ["J. Kittler", "M. Hatef", "R.P.W. Duin", "J. Matas"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "For text dependent methods, the test utterance is known while for text independent methods, it is not known [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 16, "context": "This technique can be divided in two main categories: systems based on features diversity [18] and systems based on classifiers diversity [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "This technique can be divided in two main categories: systems based on features diversity [18] and systems based on classifiers diversity [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "34 experiments the NTIMIT database [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "A standard approach in estimating the parameters of GMM-UBM aims to learn the mean vector, covariance matrix and the weight through expectation-maximization (EM) algorithm [13] from a background dataset.", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "Each speaker is then modeled and referred by adapting only the mean vectors of UBM using maximum a posteriori (MAP) criteria [5], while the weights and covariance matrix were set to the corresponding parameters of the UBM.", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "For speaker recognition, NB algorithm is largely used, such in [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Support Vector Machines (SVM) Support vector machines are a new technique of the statistical learning theory proposed by Vapnick in 1995 [6] and are based on the structural risk minimization principle [11] from computational learning theory.", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "Support Vector Machines (SVM) Support vector machines are a new technique of the statistical learning theory proposed by Vapnick in 1995 [6] and are based on the structural risk minimization principle [11] from computational learning theory.", "startOffset": 201, "endOffset": 205}, {"referenceID": 4, "context": "For speaker recognition, the first approach in using SVM classifier was implemented by Schmidt where SVM were trained directly on the acoustic space [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "Another approach became recently more popular; consist of using an hybrid GMM-SVM [1, 2] in a way that the", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "Another approach became recently more popular; consist of using an hybrid GMM-SVM [1, 2] in a way that the", "startOffset": 82, "endOffset": 88}, {"referenceID": 8, "context": "In general, vector concatenation is termed as classifier input fusion [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "Actually MFCC coefficients are the less vulnerable coefficient to noise perturbation [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "We also use relative spectral perceptual linear predictive (RASTA-PLP) coefficients, this choice is emphasized by the fact that these coefficients are proved to be more robust for noisy environment [4] and have shown good recognition performance in a previous study [16].", "startOffset": 198, "endOffset": 201}, {"referenceID": 14, "context": "We also use relative spectral perceptual linear predictive (RASTA-PLP) coefficients, this choice is emphasized by the fact that these coefficients are proved to be more robust for noisy environment [4] and have shown good recognition performance in a previous study [16].", "startOffset": 266, "endOffset": 270}, {"referenceID": 15, "context": "We furthermore added the dynamic cepstrum parameters due to their popularity in automatic speaker recognition systems [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "To train SVM, we use a new learning algorithm called sequential Minimal Optimization (SMO) [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "The SVM algorithm is implemented using the library LIBSVM [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "Only for SVM, data were scaled to [0, 1] before the classification process.", "startOffset": 34, "endOffset": 40}], "year": 2013, "abstractText": "Several speaker identification systems are giving good performance with clean speech but are affected by the degradations introduced by noisy audio conditions. To deal with this problem, we investigate the use of complementary information at different levels for computing a combined match score for the unknown speaker. In this work, we observe the effect of two supervised machine learning approaches including support vectors machines (SVM) and na\u00efve bayes (NB). We define two feature vector sets based on mel frequency cepstral coefficients (MFCC) and relative spectral perceptual linear predictive coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture Model (GMM). Several ways of combining these information sources give significant improvements in a text-independent speaker identification task using a very large telephone degraded NTIMIT database.", "creator": "Microsoft\u00ae Word 2010"}}}