{"id": "1407.1165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2014", "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition", "abstract": "Automatic Speech Recognition (ASR) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area. In recent year, there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions. The objective of audio-visual speech recognition system is to improve recognition accuracy. In this paper we computed visual features using Zernike moments and audio feature using Mel Frequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of Independent Standard Words) dataset which contains collection of isolated set of city names of 10 speakers. The visual features were normalized and dimension of features set was reduced by Principal Component Analysis (PCA) in order to recognize the isolated word utterance on PCA space.The performance of recognition of isolated words based on visual only and audio only features results in 63.88 and 100 respectively.", "histories": [["v1", "Fri, 4 Jul 2014 09:32:10 GMT  (637kb)", "http://arxiv.org/abs/1407.1165v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["prashant bordea", "amarsinh varpeb", "ramesh manzac", "pravin yannawara"], "accepted": false, "id": "1407.1165"}, "pdf": {"name": "1407.1165.pdf", "metadata": {"source": "CRF", "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio Visual Speech Recognition", "authors": ["Prashant Borde", "Amarsinh Varpe", "Ramesh Manza", "Pravin Yannawar"], "emails": ["borde.prashantkumar@gmail.com,", "pravinyannawar@gmail.com."], "sections": [{"heading": null, "text": "Automatic speech recognition (ASR) by machines is an attractive research topic in the field of signal processing and has led many researchers to contribute in this area. In recent years, there has been much progress in the automated speech reading system with the inclusion of audiovisual and linguistic features to detect words under loud conditions. The goal of the audio-visual speech recognition system is to improve recognition accuracy. In this post, we calculated visual features using Zernike moments and audio features using Mel Frequency Cepstral Coefficient (MFCC) to \"vVISWa\" (Visual Vocabulary of Independent Standard Words) dataset, which contains a collection of isolated city names of 10 speakers. Visual features have been normalized and the dimension of the features has been reduced by Principal Component Analysis (PCA) to detect isolated word expression in PCA space.The performance of the detection of 100 isolated words, based on only 688% of words, and only 688% of words."}, {"heading": "1. Introduction", "text": "In recent years, the number of people who are able to understand and understand the language has multiplied. (...) In recent years, the number of people who are able to understand the language has multiplied. (...) The number of people who are able to understand the language has multiplied. (...) The number of people who are able to understand the language is very high. (...) The number of people who are able to understand the language is very high. (...) The number of people who are able to understand the language is very high. (...) The number of people who are able to speak the language is very high. (...) The number of people who are able to understand the language is very high. \"(...)"}, {"heading": "2. 'vVISWa\u2019 Dataset", "text": "The video sequences used in this study were collected in the laboratory in a closed environment. \"vVISWa\" (Visual Vocabulary of Independent Standard Words) database consists of independent / isolated standard words from Marathi, Hindi and English script. The data set of isolated city words such as \"Aurangabad,\" \"Beed,\" \"Hingoli,\" \"Jalgaon,\" \"Kolhapur,\" \"Mumbai,\" \"Osmanabad,\" \"Parbhani,\" \"Pune,\" \"Solapur\"} in Marathi were considered for these experiments. Each visual expression is recorded for 2 seconds. The database consists of 10 individual speakers, 4 male and 6 female speakers."}, {"heading": "3. Methodology", "text": "The typical audiovisual speech recognition system accepts the audiovisual and visual input as shown in Figure 3.1. The audio input is captured using standard audio microphones and visual expressions are captured using standard cameras. Space between camera and individual speakers is kept constant to obtain a correct visual utterance. Once the input is captured, it is processed separately for acoustic and visual feature extraction and further used for the detection and integration of utterance. Visual features can be grouped into three general categories: form-based, optical and combinatorial approaches. All three types require the localization and tracking of Region of Interest (ROI). Areas of interest for the calculation of visual features focus on the movement of the lips (opening and closing of the mouth) over a very complex timeframe. Considering this calculating good and discriminating visual feature of the mouth plays a crucial role in detection."}, {"heading": "3.1 Region-of-Interest (ROI) Detection / Localization", "text": "The visual information relevant to language is largely contained in the movement of visible articulators such as lips, tongue and jaw. To extract this information from a sequence of video images, it is beneficial to track the complete motion of face recognition and mouth localization, which helps to extract visual features. To achieve robust and real-time face recognition, we used the \"Viola Jones\" detector based on \"AdaBoost,\" a binary classifier that uses cascades of weak classifiers to increase its performance by G. Bradski and A. Kaehler, 2008 and C. M. Bishop, 2006. In our study, we used a detector to detect the face in each image from the sequence and then detect the mouth area of the face, which is achieved by finding the mean value of the coordinates of the ROI object bounding box of frames."}, {"heading": "3.2 Visual Feature Extraction", "text": "ndU \"s rf\u00fc ide rf\u00fc nde nlrfhUeee,\" so rf\u00fc ide rf\u00fc ide nlrfhUeaeaeetnn-eaeaJnlrsrgVnlrgne\u00fceaeaeaeaeoiugnnlrgVnlrsrsrteeaeaeoiuugnngn nvo rf\u00fc ide nlrfhteeaeaeaeaeFnlrgn nvo nlrf\u00fc ide nlrfteeaeaeaeaeaeaeaeaeaeaeaeFnlrf\u00fc nvo nlrfteeaeaeaeaeaeSrmnlrf\u00fc nlrf\u00fc ide nlrf\u00fc ide nlrf\u00fc ide nlrfteeaeaeaeaeaeaeaeaeaeaeaeaeaeI nI-rf\u00fc"}, {"heading": "3.3 Principal Component Analysis", "text": "The visual expression was captured for two seconds and results in 52 images, so the Zernike characteristics for a visual expression result in 468x1 columns for a single word. Likewise, all visual words from city names are passed from \"vVISWa\" for visual feature extraction, resulting in a size matrix of 468x72. This feature set was called a \"training set\" and the dimension of this data set is to be reduced and interpreted using Principal Component Analysis. PCA was applied to Zernike characteristics to extract the most important components of characteristics corresponding to the set of isolated words. PCA converts all original variables into independent linear variables sets. These independent linear variables have the most information in the original data designated as the main components."}, {"heading": "3.4 Acoustic Features Extraction using MFCC", "text": "The acoustic signals of \"vVISWa,\" which correspond to isolated words of city names, have been removed for functional extraction. Recognition of the word uttered is based on information contained in the speech signal at the time of pronunciation of the word. MFCC is the most popular reason for selecting MFCC for recognition purposes, because it uses the spectral base as the parameter for recognition. MFCCs are the coefficients based on the perception of human auditory systems. The reason for selecting MFCC for recognition purposes lies in its peculiar difference between the operations of FFT / DCT. In the MFCC, the frequency bands are positioned logarithmically (on the scale) that the human auditory system responds to."}, {"heading": "4. Experiment and Result", "text": "Similarly, MFCC features were extracted for audio-only detection of uttered isolated words. These features were calculated for all samples of the training set and stored for detection purposes. The entire data set of isolated city names (visual and acoustic only) was divided into 70-30 ratios, i.e. 70% (training samples) and 30% (test samples). The detection of isolated city names was divided into two phases: the first visual language detection and the second audiovisual language detection."}, {"heading": "4.1 Visual Speech Recognition", "text": "The \"Euclidean\" distance provides information between each pair (one vector from the test set and another vector from the training set) of observations. The distance matrix of training sample and test sample using Zernike Moment and PCA was shown in Table 3.a.) It was found that out of 36 samples, 23 samples were correctly identified and 13 samples were incorrectly classified. Samples from \"Latur\" and \"Solapur\" were not recognized, while samples from \"Mumbai,\" \"Osmanabad,\" \"Parbhani\" and \"Satara\" were fully recognized."}, {"heading": "5. Conclusion", "text": "In this paper, we described a complete Audio-Visual Speech Recognition System, which includes facial and lip recognition, with extraction and recognition. This experiment was conducted using our own database, which was developed for analysis purposes. We used a new technique for visual speech recognition, the Zernike Moment. The Zernike Moment features proved to be more reliable and accurate features for visual speech recognition and PCA for reducing the data dimension."}, {"heading": "Acknowledgements", "text": "The authors thank the Department of Science and Technology (DST) for the financial support of the large-scale research project approved under the Fast Track Scheme for Young Scientist, see sanction number SERB / 1766 / 2013 / 14 and the authorities of Dr. Babasaheb Ambedkar Marathwada University, Aurangabad (MS) India for providing the infrastructure for this research."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Improving connected letter recognition by Lip-reading", "author": ["Christopher Bergler"], "venue": null, "citeRegEx": "Bergler,? \\Q1993\\E", "shortCiteRegEx": "Bergler", "year": 1993}, {"title": "PoliteknikSeberangPerai, JalanPermatangPauh, \u201cRobust Computer Voice Recognition Using Improved MFCC Algorithm", "author": ["Clarence Goh Kok Leon"], "venue": "InternationalConference on New Trends in Information and Service Science,", "citeRegEx": "Leon,? \\Q2009\\E", "shortCiteRegEx": "Leon", "year": 2009}, {"title": "An Improved automatic Lip-reading system to enhance speech Recognition", "author": ["Eric Petjan", "Bradford Bischoff", "David Bodoff"], "venue": "Technical Report TM 11251-871012-11,", "citeRegEx": "Petjan et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Petjan et al\\.", "year": 1987}, {"title": "Learning OpenCV: Computer Vision with the OpenCV Library\u201d. O\u2019Reilly Media, 1 edition", "author": ["G. Bradski", "A. Kaehler"], "venue": "IEEETransaction on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bradski and Kaehler.,? \\Q2008\\E", "shortCiteRegEx": "Bradski and Kaehler.", "year": 2008}, {"title": "MacGurk, \"Visual influences on speech perception process, Perception and Psychophysics", "author": ["J H Macdonald"], "venue": "voices\", Nature,", "citeRegEx": "Macdonald,? \\Q1978\\E", "shortCiteRegEx": "Macdonald", "year": 1978}, {"title": "A Natural Speech recognition system", "author": ["Meisel W.S"], "venue": "Proceeding Speech Tech", "citeRegEx": "W.S,? \\Q2008\\E", "shortCiteRegEx": "W.S", "year": 2008}, {"title": "Produced under stress and in noise", "author": ["Sun-Kyoo Hwang", "Whoi-Yul Kim"], "venue": "Proceeding Speech Tech", "citeRegEx": "Hwang et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 1987}, {"title": "Intelligent Information hiding and multimedia signal", "author": ["M. \u017dELEZN\u00dd", "Z. KR\u0147OUL", "P. C\u00cdSA\u0158", "J. MATOU\u0160EK"], "venue": null, "citeRegEx": "\u017dELEZN\u00dd et al\\.,? \\Q2006\\E", "shortCiteRegEx": "\u017dELEZN\u00dd et al\\.", "year": 2006}], "referenceMentions": [], "year": 2014, "abstractText": "Automatic Speech Recognition (ASR) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area. In recent year, there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions. The objective of audio-visual speech recognition system is to improve recognition accuracy. In this paper we computed visual features using Zernike moments and audio feature using Mel Frequency Cepstral Coefficients (MFCC) on \u2018vVISWa\u2019 (Visual Vocabulary of Independent Standard Words) dataset which contains collection of isolated set of city names of 10 speakers. The visual features were normalized and dimension of features set was reduced by Principal Component Analysis (PCA) in order to recognize the isolated word utterance on PCA space.The performance of recognition of isolated words based on visual only and audio only features results in 63.88% and 100% respectively.", "creator": "Microsoft\u00ae Word 2013"}}}