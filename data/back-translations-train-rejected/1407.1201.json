{"id": "1407.1201", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2014", "title": "Improving Performance of Self-Organising Maps with Distance Metric Learning Method", "abstract": "Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern Recognition tasks. Their major advantage over other architectures is human readability of a model. However, they often gain poorer accuracy. Mostly used metric in SOM is the Euclidean distance, which is not the best approach to some problems. In this paper, we study an impact of the metric change on the SOM's performance in classification problems. In order to change the metric of the SOM we applied a distance metric learning method, so-called 'Large Margin Nearest Neighbour'. It computes the Mahalanobis matrix, which assures small distance between nearest neighbour points from the same class and separation of points belonging to different classes by large margin. Results are presented on several real data sets, containing for example recognition of written digits, spoken letters or faces.", "histories": [["v1", "Fri, 4 Jul 2014 12:14:48 GMT  (148kb)", "http://arxiv.org/abs/1407.1201v1", "9 pages, 2 figures"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["piotr p{\\l}o\\'nski", "krzysztof zaremba"], "accepted": false, "id": "1407.1201"}, "pdf": {"name": "1407.1201.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Krzysztof Zaremba"], "emails": ["pplonski@ire.pw.edu.pl", "zaremba@ire.pw.edu.pl"], "sections": [{"heading": null, "text": "ar Xiv: 140 7.12 01v1 [cs.LG] 4 Jul 2Keywords: Self-organizing maps, Distance metric learning, LMNN, Mahalanobis distance, classification"}, {"heading": "1 Introduction", "text": "They are mathematical models inspired by biology, and this type of data analysis has been tested as an efficient tool in many applications, both academic and industrial, such as character recognition, face recognition, word analysis [12], document analysis [9], visualization [9], and bioinformatics (9]."}, {"heading": "2 Methods", "text": "Describe the dataset as D = {(xi, yi)}, where xi is an attribute vector of the i-th sample, and yi is a class vector, where yij = 0 is for j 6 = Classi and yij = 1 is for j = Classi, where Classi is the class number of the i-th sample."}, {"heading": "2.1 SOM model", "text": "In this thesis, we used the SOM architecture in a controlled manner so-called \"Learning Associations by Self-Organization\" (LASSO), first described in [13]. The main difference between this SOM architecture and Kohon's original SOM architecture [10] is that during the learning phase, the LASSO method takes into account the class vector, in addition to the attributes. Here, we used the two-dimensional grid of neurons \u2212 \u2212 values. Each neuron is represented by a weight vector Wpq, consisting of a vector that comes closest to the attributes Apq and a class Cpq (Wpq; Cpq]), with (p, q) indices of the neurons in the grid. In a learning phase, all samples are shown to the network in an epoch. For each sample, we look for a neuron that comes closest to the i-th sample. Distance is calculated by Disttrain (Di, Wpq)."}, {"heading": "2.2 LMNN method", "text": "In many cases, the most commonly used metric is Euclidean. It often gives poor accuracy because it records all dimensions with the same contribution and does not assume correlations between dimensions. Mahalanobi's distance seems to be a better metric choice because it is scale invariant and takes into account the correlations of the input dimensions. It is defined by: DistM (xi, xj) = (xi \u2212 xj) TM (xi \u2212 xj), (7) where M is usually an inverse of a covariance matrix. In the case where M is an identity matrix, the distance (7) is equal to the euclidean distance. In this thesis we learned that Mahalanobi's matrix is an inverse of a covariance matrix using the method described in [16]. Matrix coefficients are calculated in a way that ensures a large margin separation of points from different classes and a small distance between points of the same class."}, {"heading": "2.3 SOM+DML model", "text": "We are interested in such a linear transformation of sample attributes that ensures that the Euclidean distance calculated on the transformed attributes is equal to the Mahalanobis distance calculated on the original attributes. Mahalanobis matrix M can be written as M = LTL, where L is the transformation sought. (13) The distance between the transformed attributes in the euclidean distance should correspond to the distance between the original attributes: DistM (xi, xj) = DistE (ui, uj). (14) 1 Matalb attributes of the LMNN algorithm are available at http: / / www.cse.wustl. edu / ~ kilian / code / code.htmlWe will now search for L."}, {"heading": "3 Results", "text": "The performance of the SOM + DML method has been compared with the SOM models on six real data sets. As a measure of accuracy, we take the percentage of incorrect classifications. It is worth noting that the highest number of correct classifications is therefore not the goal of this paper. Data sets are originally described in Table 1. Sets \"Wine,\" \"Ionosphere,\" \"Isolet,\" \"Ionosphere\" are sentences from the \"UCI Machine Learing Repository\" 2, set \"Faces\" are from the \"ORL Database of Faces\" 3.Now we briefly present the origin of the sets. Data sets \"Wine,\" \"Ionosphere\" are classic benchmark sets, which are often used in tests of newly developed classification algorithms. \"Isolet\" Dataset represents a spoken character recognition www.task.2 http: / / archive.uics.ci.ac.ac.ac.ac..com / Archive"}, {"heading": "4 Conclusions", "text": "A method for improving the performance of the Self-Organising Maps in classification tasks was described, the linear transformation of the data took place before the SOM training phase, and the matrix for the transformation was derived from the LMNN algorithm, which calculates the Mahalanobis matrix and ensures a great differentiation between the points of different classes at the same time. We called our method SOM + DML. The test of the method was demonstrated on several data sets, with the emphasis on the recognition of: faces, handwritten numbers and spoken letters. Test results confirm that the distance metric learning method improves the performance of the SOM network. Finding the optimal matrix for the linear transformation plays a crucial role in achieving improved outcomes. The implementation of the SOM + DML model in the Matlab is available at http: / / home.elka.pw. edulon.pl / ~ ppski / som _ ml."}], "references": [{"title": "Dynamic Self Organizing Maps With Controlled Growth for Knowledge Discovery", "author": ["D. Alahakoon", "S.K. Halgamuge", "B. Sirinivasan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Face Recognition under Varying Illumination Using Mahalanobis Self-organizing Map", "author": ["S. Aly", "N. Tsuruta", "R. Taniguchi"], "venue": "Artificial Life and Robotics, vol.13, no. 1, pp 298-301", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving the K-NN Classification with the Euclidean Distance Through Linear Data Transformations", "author": ["L. Bobrowski", "M. Topczewska"], "venue": "In Industrial Conference on Data", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Phylogenetic reconstruction using an unsupervised growing neural network that adopts the topology of a phylogenetic tree", "author": ["J. Dopazo", "J.M. Carazo"], "venue": "Journal of Molecular Evolution,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "On Global Self-Organizing Maps", "author": ["W. Duch", "A. Naud"], "venue": "ESANN", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Comparison of Supervised SelfOrganizing Maps Using Euclidian or Mahalanobis Distance in Classification Context", "author": ["F. Fessant", "P. Aknin", "L. Oukhellou", "S. Midenet"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "The impact of network topology on selforganizing maps", "author": ["F. Jiang", "H. Berry", "M. Schoenauer"], "venue": "In GEC Summit(2009),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Create Self-organizing Maps of Documents in a Distributed System", "author": ["M.A. K lopotek", "T. Pachecki"], "venue": "Intelligent Information Systems, Siedlce, pp 315-320,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological Cybernetics, vol.43 pp 59-69", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}, {"title": "Contextually Self-Organized Maps of Chinese Words", "author": ["T. Kohonen", "H. Xing"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Learning Associations by Self-Organization: The LASSO model", "author": ["S. Midenet", "A. Grumbach"], "venue": "Neurocomputing vol.6(3) pp 343-361", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "parSOM: A Parallel Implementation of the Self-Organizing Map Exploiting Cache Effects: Making the SOM Fit for Interactive High-Performance Data Analysis", "author": ["A. Rauber", "P. Tomsich", "D. Merkl"], "venue": "International Joint Conference on Neural Networks(2000),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience, vol.3, no.(1), pp 71\u201386,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Distance Metric Learning with Application to Clustering with Side-Information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "Kohonen presented architecture called Self-Organising Maps (SOM) [10], which provides a method of feature mapping from multi-dimensional space to usually a two-dimensional grid of neurons in an unspervised way.", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 230, "endOffset": 233}, {"referenceID": 0, "context": "Some of them concentrated on finding an optimal size of a network[1], faster", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "learning [14] or applying different neighbourhood functions [8].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "learning [14] or applying different neighbourhood functions [8].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The first one [6], [2] uses Mahalanobis metric instead of the Euclidean one.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "The first one [6], [2] uses Mahalanobis metric instead of the Euclidean one.", "startOffset": 19, "endOffset": 22}, {"referenceID": 11, "context": "The second improvement [13] shows how to use SOM in a supervised manner.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "Several algorithms exist for distance metric learning (DML) [17], [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "Several algorithms exist for distance metric learning (DML) [17], [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 14, "context": "In this paper we use so-called Large Margin Nearest Neighbour (LMNN) method [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "In this paper, we used the SOM architecture in a supervised manner so-called \u2019Learning Associations by Self-Organisation\u2019 (LASSO), first described in [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "The main difference between this SOM architecture and the original Kohonen\u2019s SOM architecture [10] is that during the learning phase the LASSO method takes into consideration class vector, additionally to attributes.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "In this paper, we learned Mahalanobis matrix using the method described in [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "SPD can be solved using general purpose solvers, however in our approach we used Matalb implementation code described in [16], which is finely tuned to efficiently solve this kind of problems.", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "The original data - 92x112 pixels images in 256 gray levels was projected by PCA to 50 leading components (83% of variance), the so-called egienfaces method[15].", "startOffset": 156, "endOffset": 160}], "year": 2014, "abstractText": "Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern Recognition tasks. Their major advantage over other architectures is human readability of a model. However, they often gain poorer accuracy. Mostly used metric in SOM is the Euclidean distance, which is not the best approach to some problems. In this paper, we study an impact of the metric change on the SOM\u2019s performance in classification problems. In order to change the metric of the SOM we applied a distance metric learning method, so-called \u2019Large Margin Nearest Neighbour\u2019. It computes the Mahalanobis matrix, which assures small distance between nearest neighbour points from the same class and separation of points belonging to different classes by large margin. Results are presented on several real data sets, containing for example recognition of written digits, spoken letters or faces.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}