{"id": "1407.1538", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jul-2014", "title": "Large-Scale Multi-Label Learning with Incomplete Label Assignments", "abstract": "Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some ground-truth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called MPU, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our MPU model consistently outperform other commonly-used baselines.", "histories": [["v1", "Sun, 6 Jul 2014 20:13:48 GMT  (154kb,D)", "http://arxiv.org/abs/1407.1538v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiangnan kong", "zhaoming wu", "li-jia li", "ruofei zhang", "philip s yu", "hang wu", "wei fan"], "accepted": false, "id": "1407.1538"}, "pdf": {"name": "1407.1538.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Multi-Label Learning with Incomplete Label Assignments", "authors": ["Xiangnan Kong", "Zhaoming Wu", "Li-Jia Li", "Ruofei Zhang", "Philip S. Yu", "Hang Wu", "Wei Fan"], "emails": ["xkong4@uic.edu", "lijiali@yahoo-inc.com", "psyu@cs.uic.edu"], "sections": [{"heading": null, "text": "Traditional multi-level learning approaches mainly focus on taking advantage of label correlations. Normally, it is assumed, explicitly or implicitly, that the label sets for training instances are completely labeled without missing labels. However, in many multi-level label records in the real world, the label assignments for training instances can be incomplete. In this paper, we examine the problem of large-scale multi-level learning with incomplete label assignments, especially when the number of label instances is very large and the label costs are very high, making it almost impossible to obtain a fully labeled training set. In this paper, we examine the problem of large-level multi-level learning with incomplete label assignments. We propose an approach called Mpu, which relies on positive and unlabeled stochastic progression models. Unlike previous work, our method can effectively and efficiently eliminate the absence of labels and label correlations simultaneously, while considering the usual label size and the real-time correlations are very different."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "2 Problem Formulation", "text": "Given n data points of the form (xi, yi), where xi-X-RD is a D-dimensional vector that marks the characteristics of the i-th instantiation. yi = (y-i, \u00b7 \u00b7, y q i) >, where xi-X-RD is a D-dimensional vector that marks the characteristics of the i-th instantiation. yki = 1 if the k-th label is in the label set of the instance i, otherwise yki = \u2212 1. In many real multi-label learning tasks, training data is usually not fully labeled, with each instance being labeled with a subset of the labels of the ground truth labels. We refer to such settings as multi-label learning with incomplete label assignments or weak label problems that follow the definition in [21]. Specifically in the training set, the ground truth labels are not available for each instance."}, {"heading": "2.1 Handling Missing Labels via PU Stochastic", "text": "In this subsection, we will first address the problem of missing label assignments, while assuming that all labels are independent of each other = > In the next subsection, we will show how to expand the model to include label assignments. A simple solution to multi-label learning is to treat a multi-label problem as multiple binary classification problems (1 for each label): Pr (yi | xi) = 1Pr (yki = 1 | xi) Inspired by positive and unlabeled learning in single-label classification [6], we propose a method called PU Stochastic Gradient Descent that can handle large-scale datasets with missing label assignments. Let {wk} qk = 1 be a set of parameters in our classification, based on the principle of the maximum probability that we will find optimized label assignments."}, {"heading": "2.2 Handling Label Correlations via Stacked", "text": "Graphic Models In the previous subsection, we discussed how to deal with incomplete label mappings in multi-label learning. Now, we show how to use the previous model to consider further label correlations. Inspired by the collective classification methods in [15, 7] based on stack methods, we proposed a multi-label learning method called Mpu. Mpu can effectively look at label correlations using a stacked multi-label model that does not rely on common conclusions for all labels. Stacking [23] is a type of ensemble method that forms a chain of models that form a chain of labels."}, {"heading": "3 Experiments", "text": "In order to judge the results of the study, we have to deal with the question of what the quality of work is like. (...) We have to deal with the question of what the quality of work is like. (...) We have to deal with the question of what the quality of work is like. (...) We have to deal with the question of what the quality of work is like. (...) We have to deal with the question of what the quality of work is like. (...) We have to deal with the question of what the quality of work is like. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work. (...) We have to deal with the quality of work."}, {"heading": "4 Related Work", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "5 Conclusion", "text": "We examined two real-world data sets, one small and one large, to evaluate the performance of our proposed method. Unlike previous work on multi-label learning, we look at missing labels in the training set and label correlations at the same time. By explicitly looking at the missing labels using a positive and unlabeled learning model and label correlations using batch models, our method can effectively increase the performance of multi-label classification with partially labeled training data."}, {"heading": "6 Acknowledgements", "text": "This work is partially supported by NSF through grants CNS-1115234, DBI-0960443 and OISE-1129076, and the US Department of the Army through grants W911NF-12-1-0066 and Huawei Grant."}], "references": [{"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Multi-label learning with incomplete class assignments", "author": ["S. Bucak", "R. Jin", "A.K. Jain"], "venue": "In The 24th IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Combining instancebased learning and logistic regression for multilabel classification", "author": ["W. Cheng", "E. H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K. Dembczy\u0144ski", "W. Cheng", "E. H\u00fcllermeier"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "A kernel method for multilabelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Why stacked models perform effective collective classification", "author": ["A. Fast", "D. Jensen"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Random k-labelsets: An ensemble method for multilabel classification", "author": ["I.V.G. Tsoumakas"], "venue": "In Proceedings of 18th European Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Collective multi-label classification", "author": ["N. Ghamrawi", "A. McCallum"], "venue": "In Proceedings of ACM CIKM International Conference on Information and Knowledge Management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Discriminative methods for multi-labeled classification", "author": ["S. Godbole", "S. Sarawagi"], "venue": "In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Adaptive large margin training for multilabel classification", "author": ["Y. Guo", "D. Schuurmans"], "venue": "In Proceedings of the 25th AAAI Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Large scale max-margin multi-label classificaiton with priors", "author": ["B. hariharan", "L. Zelnik-Manor", "S.V.N. Vishwanathan", "M. Varma"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Correlated label propagation with application to multi-label learning", "author": ["F. Kang", "R. Jin", "R. Sukthankar"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Maximal margin labeling for multi-topic text categorization", "author": ["H. Kazawa", "T. Izumitani", "H. Taira", "E. Maeda"], "venue": "In NIPS", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Stacked graphical models for ecient inference in markov random fields", "author": ["Z. Kou", "W. Cohen"], "venue": "In Proceedings of the 7th SIAM International Conference on Data", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Building text classifiers using positive and unlabeled examples", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "P.S. Yu"], "venue": "In Proceedings of the 3rd IEEE International Conference on Data Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Semi-supervised multilabel learning by constrained non-negative matrix factorization", "author": ["Y. Liu", "R. Jin", "L. Yang"], "venue": "In The 21st National Conference on Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Mining partially annotated images", "author": ["Z. Qi", "M. Yang", "Z. Zhang"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Multi-label classification using ensembles of pruned sets", "author": ["J. Read", "B. Pfahringer", "G. Holmes"], "venue": "In Proceedings of the 8th IEEE International Conference on Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "In The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Multi-label learning with weak label", "author": ["Y. Sun", "Y. Zhang", "Z. Zhou"], "venue": "In The 24th AAAI Conference on Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Parametric mixture models for multi-labeled text. In NIPS, pages 721\u2013728", "author": ["N. Ueda", "K. Saito"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Multi-label learning by exploiting label dependency", "author": ["M.-L. Zhang", "K. Zhang"], "venue": "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Multi-label learning by instance differentiation", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "In Proceedings of the 21nd AAAI Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 21, "context": "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.", "startOffset": 49, "endOffset": 69}, {"referenceID": 9, "context": "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.", "startOffset": 49, "endOffset": 69}, {"referenceID": 13, "context": "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.", "startOffset": 49, "endOffset": 69}, {"referenceID": 25, "context": "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.", "startOffset": 49, "endOffset": 69}, {"referenceID": 19, "context": "Conventional approaches for multi-label learning [22, 10, 14, 26, 20] mainly focus on utilizing the correlations among different class labels to facilitate the learning process.", "startOffset": 49, "endOffset": 69}, {"referenceID": 24, "context": "Most existing multi-label learning methods, such as Ml-knn [25] and Rank-svm [5], assume that the training data are fully labeled.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Most existing multi-label learning methods, such as Ml-knn [25] and Rank-svm [5], assume that the training data are fully labeled.", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "Positive and Unlabeled learning methods [6, 16] can usually handle the cases when the label assignments are partially missing under single label settings.", "startOffset": 40, "endOffset": 47}, {"referenceID": 15, "context": "Positive and Unlabeled learning methods [6, 16] can usually handle the cases when the label assignments are partially missing under single label settings.", "startOffset": 40, "endOffset": 47}, {"referenceID": 20, "context": "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.", "startOffset": 78, "endOffset": 89}, {"referenceID": 1, "context": "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.", "startOffset": 78, "endOffset": 89}, {"referenceID": 17, "context": "Previous approaches on multi-label learning with incomplete label assignments [21, 2, 18] are mainly designed for small/moderatesized datasets.", "startOffset": 78, "endOffset": 89}, {"referenceID": 20, "context": "We call such settings as multilabel learning with incomplete label assignments or weak labeling problems, following the definition in [21].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "Inspired by the positive and unlabeled learning in single-label classification [6], we propose a method, called PU Stochastic Gradient Descent, which can handle large-scale datasets with missing label assignments.", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Following the assumption in [6], we assume that annotated labels are randomly sampled from the groundtruth label set with a constant rate c, where the sampling process is totally independent everything else, such as the feature of the instance.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Here c can be directly estimated from the training set using cross validation process in [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "Inspired by the collective classification methods in [15, 7] based on stacking, we proposed a multi-label learning method called Mpu.", "startOffset": 53, "endOffset": 60}, {"referenceID": 6, "context": "Inspired by the collective classification methods in [15, 7] based on stacking, we proposed a multi-label learning method called Mpu.", "startOffset": 53, "endOffset": 60}, {"referenceID": 22, "context": "Stacking [23] is one type of ensemble methods which build a chain of models.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "A stacked multi-label model allows inferences about one label to influence inferences about other labels but uses a different mechanism than other approaches to multilabel ensemble [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Such design can permit exact inference, while other ensemble methods require approximate inference techniques [4], such as classifier chains.", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.", "startOffset": 121, "endOffset": 132}, {"referenceID": 12, "context": "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.", "startOffset": 121, "endOffset": 132}, {"referenceID": 16, "context": "2 Evaluation Metric In order to evaluate the performance of multi-label learning by the models, we follow previous works [9, 13, 17] by using Micro-F1 as the performance measure.", "startOffset": 121, "endOffset": 132}, {"referenceID": 11, "context": "M3L Large-Scale Multi-Label Learning \u00ac Label Correlations [12] \u00ad Large-Scale Data", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "Elkan08 Positive & Unlabeled Learning \u00ae Missing Label Assignments [6]", "startOffset": 66, "endOffset": 69}, {"referenceID": 20, "context": "Well Multi-Label Learning with Missing Labels \u00ac Label Correlations [21] \u00ae Missing Label Assignments", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "\u2022 Large-scale Multi-label Learning (M3L) [12]: The first baseline method is a multi-label classification method for large scale problems.", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "\u2022 Positive and Unlabeled Learning (Elkan08) [6]: The second baseline method is a PU learning method which handle the cases where some positive instances can be missed by the labeler.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "We use the binary decomposition method to solve multi-label classification problems by training one model over each class [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 20, "context": "\u2022 Multi-label Learning with Missing Labels (Well) [21]: we also compare with another baseline which 0.", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].", "startOffset": 133, "endOffset": 152}, {"referenceID": 9, "context": "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].", "startOffset": 133, "endOffset": 152}, {"referenceID": 13, "context": "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].", "startOffset": 133, "endOffset": 152}, {"referenceID": 7, "context": "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].", "startOffset": 133, "endOffset": 152}, {"referenceID": 10, "context": "Multi-label learning corresponds to the classification problem where each instance can be assigned to multiple labels simultaneously [22, 10, 14, 8, 11].", "startOffset": 133, "endOffset": 152}, {"referenceID": 0, "context": "Conventional multi-label learning approaches can be roughly categorized as follows: (a) one-vs-all approaches: This type of approaches treat different labels as independent by converting the multi-label problem into multiple binary classification problems (one for each label) [1].", "startOffset": 277, "endOffset": 280}, {"referenceID": 24, "context": "Zhang and Zhou[25] proposed Ml-knn, a binary method by extending the kNN algorithm to a multi-label problems using maximum a posteriori (MAP) principle to determine the label set predictions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "(b) pairwise correlations: This type of approaches mainly use the pairwise relationships among different labels to facilitate the learning process [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "Elisseeff and Weston [5] proposed Ranksvm, a kernel method by minimizing ranking loss to rank label pairs.", "startOffset": 21, "endOffset": 24}, {"referenceID": 23, "context": "(c) High-order correlations [24, 24]: This type of approaches can utilize higher order relationships among different labels.", "startOffset": 28, "endOffset": 36}, {"referenceID": 23, "context": "(c) High-order correlations [24, 24]: This type of approaches can utilize higher order relationships among different labels.", "startOffset": 28, "endOffset": 36}, {"referenceID": 18, "context": "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].", "startOffset": 118, "endOffset": 124}, {"referenceID": 3, "context": "Examples include random subset ensemble approaches [19, 20], Bayesian network approach [24] and full-order approaches [3, 4].", "startOffset": 118, "endOffset": 124}, {"referenceID": 11, "context": "The work in [12] studied the large-scale multilabel leanring problems, and proposed an efficient approach M3L.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].", "startOffset": 121, "endOffset": 132}, {"referenceID": 1, "context": "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].", "startOffset": 121, "endOffset": 132}, {"referenceID": 17, "context": "In addition, multi-label learning with incomplete label assignment has also been studied on small/moderate-size datasets [21, 2, 18].", "startOffset": 121, "endOffset": 132}, {"referenceID": 20, "context": "The work in [21] proposed an approach Well to infer missing labels in multi-label learning under transductive settings.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Our work is also related to another line of research, called positive and unlabeled learning, or PU learning [16, 6].", "startOffset": 109, "endOffset": 116}, {"referenceID": 5, "context": "Our work is also related to another line of research, called positive and unlabeled learning, or PU learning [16, 6].", "startOffset": 109, "endOffset": 116}, {"referenceID": 15, "context": "Many previous works on PU learning focus on estimating reliable negative examples from the unlabeled dataset, and utilize the estimated labels to improve the classification performances, The work in [16] proposed a method based upon biased SVM.", "startOffset": 199, "endOffset": 203}, {"referenceID": 5, "context": "Elkan and Noto [6] provided a statistic model for positive and unlabeled learning.", "startOffset": 15, "endOffset": 18}], "year": 2014, "abstractText": "Multi-label learning deals with the classification problems where each instance can be assigned with multiple labels simultaneously. Conventional multi-label learning approaches mainly focus on exploiting label correlations. It is usually assumed, explicitly or implicitly, that the label sets for training instances are fully labeled without any missing labels. However, in many real-world multi-label datasets, the label assignments for training instances can be incomplete. Some groundtruth labels can be missed by the labeler from the label set. This problem is especially typical when the number instances is very large, and the labeling cost is very high, which makes it almost impossible to get a fully labeled training set. In this paper, we study the problem of large-scale multi-label learning with incomplete label assignments. We propose an approach, called Mpu, based upon positive and unlabeled stochastic gradient descent and stacked models. Unlike prior works, our method can effectively and efficiently consider missing labels and label correlations simultaneously, and is very scalable, that has linear time complexities over the size of the data. Extensive experiments on two real-world multi-label datasets show that our Mpu model consistently outperform other commonly-used baselines.", "creator": "LaTeX with hyperref package"}}}