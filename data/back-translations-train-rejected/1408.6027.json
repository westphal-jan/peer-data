{"id": "1408.6027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2014", "title": "Label Distribution Learning", "abstract": "Although multi-label learning can deal with many problems with label ambiguity, it does not fit some real applications well where the overall distribution of the importance of the labels matters. This paper proposes a novel learning paradigm named \\emph{label distribution learning} (LDL) for such kind of applications. The label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is a more general learning framework which includes both single-label and multi-label learning as its special cases. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare their performance, six evaluation measures are suggested for LDL algorithms, and the first batch of label distribution datasets are collected and made publicly available. Experimental results on one artificial and two real-world datasets show clear advantage of the specialized algorithms, which indicates the importance of special design for the characteristics of the LDL problem.", "histories": [["v1", "Tue, 26 Aug 2014 06:48:58 GMT  (8264kb)", "http://arxiv.org/abs/1408.6027v1", null], ["v2", "Tue, 5 Apr 2016 09:47:09 GMT  (10286kb)", "http://arxiv.org/abs/1408.6027v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xin geng"], "accepted": false, "id": "1408.6027"}, "pdf": {"name": "1408.6027.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Quan Zhao"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 8,60 27v1 [cs.LG] 2 6A ug2 014 1Index Terms - Multi-label learning, label distribution learning, learning with Ambiguity"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "2 FORMULATION OF LDL", "text": "In the label distribution definition given in Section 1, both single and multi-label annotations are considered special cases of label distribution. In the label distribution definition, there is an example of the distribution of labels, multiple labels, and general cases. In the label definition, the label Y2 is fully the instance, so the label descriptions of the label terms Y2 and Y4 each describe 50% of the instance, so that the label descriptions of the label terms Y2 and Y2 are complete. The label descriptions of the label terms Y2 and Y4 of the label terms Y4 of the label terms Labels Descriptions of the label terms Y2 and Y4 of the label terms Labels Descriptions of the label terms Y4 Labels Descriptions of the label terms Labels Descriptions of the label terms Y2 and Y4 of the label terms Y2 and Y4 descriptions of the label terms Labels Descriptions of the label terms Y2-2"}, {"heading": "3 LDL ALGORITHMS", "text": "We have three strategies for designing algorithms for LDL: the first strategy is problem transformation, i.e. transforming the LDL problem into existing learning paradigms; the second strategy is algorithm adaptation, i.e. extending existing learning algorithms to label distributions; and the third strategy is to design specialized algorithms according to the characteristics of LDL. Following each of the three strategies, two typical algorithms are proposed in this section."}, {"heading": "3.1 Problem Transformation", "text": "A simple way to turn an LDL problem into a labeling learning problem is to convert the training examples into weighted labeling examples. In detail, each training example (xi, Di) is converted into c labeling examples (xi, yj) with the weight d yj xi, where i = 1,..., n and j = 1,... The training set is then re-scanned according to the weight of each example. The newly scanned training set becomes a standard label training set with c \u00b7 n examples, and then any label learning algorithms can be applied to the training set. To predict the label distribution of a hitherto invisible instance x, the learner must be able to output the confidence / probability for each label yj, which can be regarded as the description level of the respective label, i.e. d yj x = P (yj | x). Two representative algorithms are used here as fixes."}, {"heading": "3.2 Algorithm Adaptation", "text": "Certain existing learning algorithms can, of course, be expanded to deal with label distributions, under which two customized algorithms are proposed. The first algorithm is k-NN. In a new instance x, its nearest neighbors are first found in the training set. Then, the mean of the label distributions of all k nearest neighbors is calculated as the label distribution of x, i.e. p (yj | x) = 1k x. This customized algorithm is called AA-kNN, where \"AA\" is the abbreviation for \"algorithm customization.\" (4), where Nk (x) is the index set of k nearest neighbors of x in the training set. This customized algorithm is called AA-kNN, where \"AA\" is the abbreviation of \"algorithm customization.\" The second algorithm is backpropagation (BP)."}, {"heading": "3.3 Specialized Algorithms", "text": "Deviating from the indirect strategy of problem transformation and algorithm customization, the specialized algorithms may directly correspond to the LDL problem, e.g. by directly solving the optimization problem in Eq. (1) A good start in this direction could be our previous work on the estimation of facial age [11], [12] where, in order to solve the insufficient training problem, each facial image is labeled not only by its chronological age, but also by the adjacent age. Indeed, this forms a special denomination distribution with the highest degree of description in chronological age and gradually decreasing degrees of description on both adjacent sides of the chronological age. We have proposed two algorithms: IIS-LLD and CPNN for such specific data, both designed to solve an optimization problem similar to Eq. (1) Unfortunately, CPNN is only suitable for fully ordered labels (such as age) and therefore cannot be applied to the general LDL problem."}, {"heading": "3.4 Evaluation Measures", "text": "The output of an LDL algorithm is a label distribution that differs from both the single label and single label learning output. A natural choice of such a measurement variable is the average distance or similarity between the predicted label distribution and the real label distribution. There are many measurements of distance / similarity between probability distributions [2] that can be easily borrowed to measure the distance / similarity between label distributions. In this paper, six representative measurements are selected as evaluation variables for the LDL algorithms, four of which are distance measurements (the smaller the better), i.e., Euclidean, S\u00f8rensen, Squared and Kullback Leibler (K-L)."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Methodology", "text": "There are three datasets used in the experiments: an artificial toy dataset, a moderate real data pattern, a moderate real data pattern, a moderate data pattern (1 / 2 x 2 x 1 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x"}, {"heading": "4.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Artificial Dataset", "text": "In order to visually represent the label distributions of the test samples of the artificial dataset, the description degrees of the three labels are considered as the three color channels of the RGB color space. Subsequently, each label distribution is converted to color, and the predictions made by the LDL algorithms can be visually enhanced by observing the color patterns on the manifold on which the test samples are located. To facilitate comparison, the images are visually enhanced by applying a decoration stretching process. Results are shown in Fig. 4. It can be seen that the two specialized LDL algorithms, IIS-LLD and BFGS-LLD, predict nearly identical color patterns with the basic truth. PT-Bayes and AA-kNN can also detect similar color patterns with the basic truth. However, the PT-SVM and AA-BP miss the visual performance."}, {"heading": "4.2.2 Yeast Gene Dataset", "text": "To give a direct idea of the predictions of the LDL algorithms, some typical examples of the predicted label distributions on the Yeast Gene dataset given by the six LDL algorithms are first shown in Table 4. The first row shows the real label distributions of four typical test instances with 18, 7, 4 and 2 labels. Each of the following lines shows the corresponding predictions of an LDL algorithm. The evaluation yardsticks are given under each predicted label distribution in two sets. However, the first set contains the four distance yardsticks {Euclidean, S\u00f8rensen, Squared \u03c72, K-L}, and the second set contains the two similarity yardsticks {Intersection, Fidelity}. The best performance in each case is highlighted by obesity. As you can see, the specialized algorithms (IIS-LLLD and BFGS-LLD) are the average measurements of five algorithms each."}, {"heading": "4.2.3 Human Gene Dataset", "text": "The experimental results of the six LDL algorithms of the large-scale Human Gene dataset are presented in tabular form in Table 6. Again, the algorithms are ranked according to each criterion in descending order of performance. As shown in Table 6, the performance of all algorithms is generally inferior to that of the Yeast Gene dataset, due to the fact that the Human Gene dataset is much more complex than the Yeast Gene dataset in terms of the number of examples, characteristics and designations. However, the relative performance, i.e. the rankings, of the algorithms is similar to that of the Yeast Gene dataset. Specialized LDL algorithms (IIS-LLD and BFGS-LLD) once again perform remarkably better than the two \"PT\" algorithms (PT-Bayes and PT-SVM) and the \"AA LLLalgorithms.\""}, {"heading": "5 SUMMARY AND DISCUSSIONS", "text": "This paper proposes to use and make publicly available six LDL algorithms in three ways: problem transformation, algorithm customization, and specialized algorithm design. To compare these algorithms, six assessment benchmarks are used, and the first batch of label distribution data sets is prepared and made publicly available. Experimental results on one artificial and two real data sets show a clear advantage of specialized algorithms, showing that the properties of LDL require a special design to achieve good performance. This paper is motivated by the real data of description degrees (e.g. gene expression level). As a general learning framework, label distribution learning could be used to solve other types of problems. Generally speaking, there are at least three scenarios in which label distribution experiences could be helpful."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by the National Science Foundation of China (61273300, 61232007) and the Jiangsu Natural Science Funds for Distinguished Young Scholar (BK20140022)."}], "references": [{"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.D. Pietra", "V.J.D. Pietra"], "venue": "Computational Linguistics, vol. 22, no. 1, pp. 39\u201371, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Comprehensive survey on distance/similarity measures between probability density functions", "author": ["S.-H. Cha"], "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, vol. 1, pp. 300\u2013307, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Graded multilabel classification: The ordinal case", "author": ["W. Cheng", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "Proc. 27th Int\u2019l Conf. Machine Learning, Haifa, Israel, 2010, pp. 223\u2013230.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining instance-based learning and logistic regression for multilabel classification", "author": ["W. Cheng", "E. H\u00fcllermeier"], "venue": "Machine Learning, vol. 76, no. 2-3, pp. 211\u2013225, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge discovery in multi-label phenotype data", "author": ["A. Clare", "R.D. King"], "venue": "Proc. European Conf. Principles of Data Mining and Knowledge Discovery, Freiburg, Germany, 2001, pp. 42\u201353.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Handling possibilistic labels in pattern classification using evidential reasoning", "author": ["T. Denoeux", "L.M. Zouhal"], "venue": "Fuzzy Sets and Systems, vol. 122, no. 3, pp. 409\u2013424, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Cluster analysis and display of genome-wide expression patterns", "author": ["M.B. Eisen", "P.T. Spellman", "P.O. Brown", "D. Botstein"], "venue": "Proceedings of the National Academy of Science, vol. 95, no. 25, pp. 14 863\u201314 868, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E.L. Menc\u0131\u0301a", "K. Brinker"], "venue": "Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008.  13", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Label distribution learning", "author": ["X. Geng", "R. Ji"], "venue": "Proc. 2013 Int\u2019l Conf. Data Mining Workshops, Dallas, TA, 2013, pp. 377\u2013383.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Facial age estimation by learning from label distributions", "author": ["X. Geng", "K. Smith-Miles", "Z.-H. Zhou"], "venue": "Proc. 24th AAAI Conf. Artificial Intelligence, Atlanta, GA, 2010, pp. 451\u2013456.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Facial age estimation by learning from label distributions", "author": ["X. Geng", "C. Yin", "Z.-H. Zhou"], "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 35, no. 10, pp. 2401\u20132412, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Functional cartography of complex metabolic networks", "author": ["R. Guimer\u00e0", "L.A.N. Amaral"], "venue": "Nature, vol. 433, pp. 895\u2013900, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-label classification using conditional dependency networks", "author": ["Y. Guo", "S. Gu"], "venue": "Proc. 22nd Int\u2019l Joint Conf. Artificial Intelligence, Barcelona, Spain, 2011, pp. 1300\u20131305.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Label ranking by learning pairwise preferences", "author": ["E. H\u00fcllermeier", "J. F\u00fcrnkranz", "W. Cheng", "K. Brinker"], "venue": "Artif. Intell., vol. 172, no. 16-17, pp. 1897\u20131916, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1897}, {"title": "Correlated label propagation with application to multi-label learning", "author": ["F. Kang", "R. Jin", "R. Sukthankar"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, New York, NY, 2006, pp. 1719\u20131726.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-label ensemble based on variable pairwise constraint projection", "author": ["P. Li", "H. Li", "M. Wu"], "venue": "Information Sciences, vol. 222, pp. 269\u2013 281, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of algorithms for maximum entropy parameter estimation", "author": ["R. Malouf"], "venue": "Proc. 6th Conf. Computational Natural Language Learning, Taipei, Taiwan, 2002, pp. 49\u201355.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Numerical Optimization, 2nd ed", "author": ["J. Nocedal", "S. Wright"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Uncovering the overlapping community structure of complex networks in nature and society", "author": ["G. Palla", "I. Derenyi", "I. Farkas", "T. Vicsek"], "venue": "Nature, vol. 435, pp. 814\u2013818, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Inducing features of random fields", "author": ["S.D. Pietra", "V.J.D. Pietra", "J.D. Lafferty"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 4, pp. 380\u2013393, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning from data with uncertain labels by boosting credal classifiers", "author": ["B. Quost", "T. Denoeux"], "venue": "Proc. 1st ACM SIGKDD Workshop on Knowledge Discovery from Uncertain Data, Paris, France, 2009, pp. 38\u201347.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning from crowds", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "Journal of Machine Learning Research, vol. 11, no. Apr, pp. 1297\u20131322, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable and efficient multi-label classification for evolving data streams", "author": ["J. Read", "A. Bifet", "G. Holmes", "B. Pfahringer"], "venue": "Machine Learning, vol. 88, no. 1-2, pp. 243\u2013272, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "Machine Learning, vol. 85, no. 3, pp. 333\u2013 359, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-label classification using ensembles of pruned sets", "author": ["J. Read", "B. Pfahringer", "G. Holmes"], "venue": "Proc. IEEE Int\u2019l Conf. Data Mining, Pisa, Italy, 2008, pp. 995\u20131000.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Art-based neural networks for multi-label classification", "author": ["E.P. Sapozhnikova"], "venue": "Int\u2019l Symposium Intelligent Data Analysis, Lyon, France, 2009, pp. 167\u2013177.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with probabilistic supervision", "author": ["P. Smyth"], "venue": "Computational Learning Theory and Natural Learning System, T. Petsche, Ed. MA: MIT Press, 1995, vol. III, pp. 163\u2013182.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Multi-label learning with weak label", "author": ["Y.-Y. Sun", "Y. Zhang", "Z.-H. Zhou"], "venue": "Proc. 24th AAAI Conf. Artificial Intelligence, Atlanta, GA, 2010, pp. 593\u2013598.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Tutorial on learning from multi-label data", "author": ["G. Tsoumakas", "M.-L. Zhang", "Z.-H. Zhou"], "venue": "European Conf. Machine Learning and Principles and Practice of Knowledge Discovery in Databases, Bled, Slovenia, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Int\u2019l Journal of Data Warehousing and Mining, vol. 3, no. 3, pp. 1\u201313, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A generative probabilistic model for multi-label classification", "author": ["H. Wang", "M. Huang", "X. Zhu"], "venue": "Proc. 8th IEEE Int\u2019l Conf. Data Mining, Pisa, Italy, 2008, pp. 628\u2013637.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Probability estimates for multiclass classification by pairwise coupling", "author": ["T.-F. Wu", "C.-J. Lin", "R.C. Weng"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 975\u20131005, 2004.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Proteomic applications for the early detection of cancer", "author": ["J.D. Wulfkuhle", "L.A. Liotta", "E.F. Petricoin"], "venue": "Nature Reviews Cancer, vol. 3, no. 4, pp. 267\u2013275, 2003.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Discriminate the falsely predicted proteinccoding genes in aeropyrum pernix k1 genome based on graphical representation", "author": ["J.-F. Yu", "D.-K. Jiang", "K. Xiao", "Y. Jin", "J.-H. Wang", "X. Sun"], "venue": "MATCH Commun. Math. Comput. Chem., vol. 67, no. 3, pp. 845\u2013866, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 18, no. 10, pp. 1338\u20131351, 2006.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-label learning by instance differentiation", "author": ["\u2014\u2014"], "venue": "Proc. 22nd AAAI Conf. Artificial Intelligence, Vancouver, Canada, 2007, pp. 669\u2013 674.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Practical Applications of Fuzzy Technologies", "author": ["H.-J. Zimmermann", "Ed"], "venue": "Quan Zhao received the B.Sc. degree in computer science and technology from Southeast University, China,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1999}], "referenceMentions": [{"referenceID": 30, "context": "Multi-label learning (MLL) [31] allows the training instances to be labeled in the second way, and thus can deal with the ambiguity at the label side.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "Generally speaking, current MLL algorithms have been developed with two strategies [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 128, "endOffset": 131}, {"referenceID": 25, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "For example, the MLL problem could be transformed into binary classification problems [14], [25], a label ranking problem [15], [9], or an ensemble learning problem [26], [17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 36, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 67, "endOffset": 70}, {"referenceID": 23, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 72, "endOffset": 76}, {"referenceID": 35, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "For example, it can be extended from k-NN [37], [5], decision tree [6], [24], or neural networks [36], [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 7, "context": "For example, in many scientific experiments [8], the result is not a single output, but a series of numerical outputs (e.", "startOffset": 44, "endOffset": 47}, {"referenceID": 33, "context": "It was discovered [34] that one particular kind of protein could be related to several cancers, and the expression levels of the protein in these related cancer cells are generally different.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Yet another example is from the overlapping community analysis of complex networks [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Such importance can be represented by certain criterion, such as the z-score [13], which indicates the role of the corresponding community when describing the node.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Without loss of generality, assume that d x \u2208 [0, 1].", "startOffset": 46, "endOffset": 52}, {"referenceID": 32, "context": ") is associated with each label [33], [37], [15], [4].", "startOffset": 32, "endOffset": 36}, {"referenceID": 36, "context": ") is associated with each label [33], [37], [15], [4].", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": ") is associated with each label [33], [37], [15], [4].", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": ") is associated with each label [33], [37], [15], [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "This paper extends our preliminary work [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Finally, (c) represents a general case of label distribution, which satisfies the constraints d x \u2208 [0, 1] and \u2211", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "As for a label distribution learner, there are infinite possible outputs as long as they satisfy that d x \u2208 [0, 1] and \u2211", "startOffset": 108, "endOffset": 114}, {"referenceID": 37, "context": "It is worthwhile to distinguish description degree from the concept membership used in fuzzy classification [38].", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "Based on fuzzy set theory, a recent extension of multi-label classification, namely graded multilabel classification [4], allows for graded membership of an instance belonging to a class.", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "The strategy in [4] is to reduce graded multilabel problems to conventional multi-label problems, while LDL aims to directly model the mapping from instances to label distributions.", "startOffset": 16, "endOffset": 19}, {"referenceID": 27, "context": "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one \u2018correct\u2019 label for each instance.", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one \u2018correct\u2019 label for each instance.", "startOffset": 113, "endOffset": 116}, {"referenceID": 21, "context": "Recognizing this, one can distinguish label distribution from the previous studies on probabilistic labels [28], [7], [22], where the basic assumption is that there is only one \u2018correct\u2019 label for each instance.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": ", d x \u2208 [0, 1] and \u2211", "startOffset": 8, "endOffset": 14}, {"referenceID": 30, "context": "In fact, this is equivalent to first applying Entropy-based Label Assignment (ELA) [31] to transform the multi-label instances into the weighted single-label instances, and then optimizing the ML criterion based on the weighted single-label instances.", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "As to SVM, the probability estimates are obtained by a pairwise coupling method proposed in [33] for multi-class classification, which has also been implemented in the software LIBSVM [3].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "As to SVM, the probability estimates are obtained by a pairwise coupling method proposed in [33] for multi-class classification, which has also been implemented in the software LIBSVM [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": ", zc} satisfies that zj \u2208 [0, 1] for j = 1, 2, .", "startOffset": 26, "endOffset": 32}, {"referenceID": 10, "context": "One good start toward this end might be our previous work on facial age estimation [11], [12], where in order to solve the insufficient training data problem, each face image is labeled by not only its chronological age, but also the neighboring ages.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "One good start toward this end might be our previous work on facial age estimation [11], [12], where in order to solve the insufficient training data problem, each face image is labeled by not only its chronological age, but also the neighboring ages.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "The optimization process of IIS-LLD, however, has been evidenced to be not very efficient [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "IIS-LLD assumes the parametric model p(y|x; \u03b8) to be the maximum entropy model [1], i.", "startOffset": 79, "endOffset": 82}, {"referenceID": 20, "context": "The optimization in IIS-LLD uses a strategy similar to Improved Iterative Scaling (IIS) [21], which is a well-known algorithm for maximizing the likelihood of the maximum entropy model.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "However, it has been reported in the literature [18] that IIS often performs worse than several other optimization algorithms such as conjugate gradient and quasi-Newton methods.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "Here we follow the idea of an effective quasi-Newton method BFGS [19] to further improve IIS-LLD.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "where the step length \u03b1 is obtained from a line search procedure to satisfy the strong Wolfe conditions [19]:", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "(18) Thus it performs much more efficiently than the standard line search Newton method, and based on previous studies [18], it stands a good chance of outperforming the IIS-based algorithm IIS-LLD.", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "There are many measures for the distance/similarity between probability distributions [2], which can be well borrowed to measure the distance/similarity between label distributions.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 15, "endOffset": 24}, {"referenceID": 1, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 15, "endOffset": 24}, {"referenceID": 0, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 15, "endOffset": 24}, {"referenceID": 0, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 32, "endOffset": 41}, {"referenceID": 1, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 32, "endOffset": 41}, {"referenceID": 3, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 32, "endOffset": 41}, {"referenceID": 0, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 49, "endOffset": 58}, {"referenceID": 3, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 49, "endOffset": 58}, {"referenceID": 1, "context": "2, d = 1, w1 = [4, 2, 1]T, w2 = [1, 2, 4]T, w3 = [1, 4, 2] T, and \u03bb1 = \u03bb2 = 0.", "startOffset": 49, "endOffset": 58}, {"referenceID": 7, "context": "The second dataset is a real-world dataset collected from ten biological experiments on the budding yeast Saccharomyces cerevisiae [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "There are in total 30, 542 human genes included in this dataset, each of which is represented by the 36 numerical descriptors for a gene sequence proposed in [35].", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "In such case, it is usually better for the learning algorithm to integrate the labels from all the sources rather than to decide one or more \u2018winning label(s)\u2019 via majority voting [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "Utilizing such correlation is one of the most important approaches to improve the learning process [16], [32], [29].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Although multi-label learning can deal with many problems with label ambiguity, it does not fit some real applications well where the overall distribution of the importance of the labels matters. This paper proposes a novel learning paradigm named label distribution learning (LDL) for such kind of applications. The label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is a more general learning framework which includes both single-label and multi-label learning as its special cases. This paper proposes six working LDL algorithms in three ways: problem transformation, algorithm adaptation, and specialized algorithm design. In order to compare their performance, six evaluation measures are suggested for LDL algorithms, and the first batch of label distribution datasets are collected and made publicly available. Experimental results on one artificial and two real-world datasets show clear advantage of the specialized algorithms, which indicates the importance of special design for the characteristics of the LDL problem.", "creator": "LaTeX with hyperref package"}}}