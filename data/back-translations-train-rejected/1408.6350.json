{"id": "1408.6350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2014", "title": "Definition and properties to assess multi-agent environments as social intelligence tests", "abstract": "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.", "histories": [["v1", "Wed, 27 Aug 2014 08:56:09 GMT  (2078kb,D)", "http://arxiv.org/abs/1408.6350v1", "53 pages + appendix"]], "COMMENTS": "53 pages + appendix", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["javier insa-cabrera", "jos\\'e hern\\'andez-orallo"], "accepted": false, "id": "1408.6350"}, "pdf": {"name": "1408.6350.pdf", "metadata": {"source": "CRF", "title": "Definition and properties to assess multi-agent environments as social intelligence tests", "authors": ["Javier Insa-Cabrera", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["jinsa@dsic.upv.es", "jorallo@dsic.upv.es"], "sections": [{"heading": null, "text": "Keywords: social intelligence, artificial intelligence, multi-agent systems, cooperation, competition, interaction, game theory, teams, rewards, intelligence tests, universal psychometries.ar Xiv: 140 8.63 50v1 [cs.MA] 2 7A ug2 01"}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Background 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Defining Social Intelligence Universally 6", "text": "3.1 Multi-Agent Environments and Team Rewards..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Properties about social intelligence testbeds 12", "text": "In the second half of the 19th century the country developed."}, {"heading": "5 Degree of compliance of several multi-agent and social scenarios 26", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Conclusions and Future work 48", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Matching Pennies properties 53", "text": ".................................................................................................................."}, {"heading": "B Prisoner\u2019s Dilemma properties 86", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}, {"heading": "C Predator-prey properties 108", "text": "......,......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "1 Introduction", "text": "In fact, artificial intelligence is a paradigmatic case of how useful these tools would be and how much they impede this deficiency. Of course, there are some tools, benchmarks and competitions aimed at measuring humanoid intelligence or performance in a particular set of tasks. But, the evolution and state of artificial intelligence focus more on social skills, and here the measurement tools are even more incidental. In the past two decades, the notion of agents and the performance of multi-agent systems has been shifted to problems and solutions where \"social\" intelligence is more relevant. This shift toward a more social intelligence is related to modern intelligence as highly social, and in fact one of the peculiarities of human intelligence over other types of intelligence."}, {"heading": "2 Background", "text": "This year, most of them will be able to establish themselves in the region."}, {"heading": "3 Defining Social Intelligence Universally", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3.1 Multi-agent environments and team rewards", "text": "Before we deal with a formal integration of definitions 1 and 2, we need to give a definition of the (multi-agent) environment, which we are both able to interact with through actions, rewards and observations. This general view of the interaction between an agent and an environment can be extended to multi-agent systems by letting different agents interact with the environment at the same time, as shown in Figure 1 (right). A multi-agent environment is an interactive scenario with multiple agents. An environment that accepts n parameters (one for each agent), which are referred to as slots. We use i = 1,., n to designate the options for action for agent slots."}, {"heading": "3.2 Teams", "text": "This actually means that we have to decide how the environment distributes rewards among actors. A simple possibility is that each actor gets his rewards through other rewards without further constraints. On the other hand, if we determine that the totality of rewards is in any way limited, we will promote competition, as happens in zero-sum games. However, in each of these cases, cooperation is unlikely to take place. Alliances could take place sporadically between at least two actors in order to disrupt (or defend against) a third actor. However, it seems unlikely that this will happen with low levels of social intelligence."}, {"heading": "3.3 A formal definition of social intelligence", "text": "With these ideas in mind, we can now try to formulate an initial definition of social intelligence. First, we fix the pattern and vary on the possible environment.Definition 5: We define the ability of an active substance to interact with at least i-agents and at most i-agents in a series of environments. (Definition 5: We define the ability of an active substance to interact with at least i-agents in a series of environments.) Definition 6: We define the ability of an active substance in an environment that interacts with at least i-agents in an environment. (Definition 1: 1) We can think about a definition of the ability of an active substance to interact with at least i-agents. (Definition 6: We define the ability of an active substance in an environment that accepts at least i-agents with a series of action patterns that interact in an environment. (Definition) We define the ability of an active substance in an environment that accepts at least i-agents."}, {"heading": "3.4 Tests", "text": "A definition that is particularly important because many definitions go beyond infinite distribution patterns or an infinite number of distribution patterns. (A test must be a finite process that may be independent of previous distribution patterns.) A test is therefore defined using the definition of non-adaptive tests based on a finite number of finite experiments or experimental series (episodes), the number of distribution steps for each experiment being in some way limited. (It makes no sense to repeat the same episode if the result is already known), but is understood as a substitute for non-deterministic agents or environments."}, {"heading": "4 Properties about social intelligence testbeds", "text": "In order to evaluate social intelligence and distinguish it from general intelligence, we need tests where social skills actually have to be used and also where we can perceive their consequences. This means that not every environment is useful to measure social intelligence and not every subset of agents is useful. We want tests where the evaluated agent has to use his social intelligence to understand and / or influence the policies of other agents, so that this is useful to achieve the goals of the evaluated agent. We also need situations where general general intelligence is not sufficient. In a way, we want (from the sum of all environments and line patterns) those problems (as defined by classes of environments and agents) where general intelligence is sufficient (and social intelligence is useless) and those in which intelligence (of any kind, social or non-social) is useless. We will examine some characteristics that are desirable (or necessary) for a test bed of environments and agents."}, {"heading": "4.1 Boundedness", "text": "One trait we need to impose in order to make many of the previous definitions meaningful is that rewards must be limited (otherwise some sums will diverge); any arbitrary choice of upper and lower limits can be scaled to any other choice, so we can assume without loss of generality that all of them are considered between \u2212 1 and 1. Formally, however, there is no assurance that the measurement is limited by definition 8. To ensure a limited result, we also need to take into account that weights are limited, i.e. there are constants cM, cL, and cL, so that we can only use one set for opponents and team players."}, {"heading": "4.2 Interactivity", "text": "By interactivity, we mean the attribute that the actions of the actors have an effect on the actions (and rewards) of the other actors. This is a key attribute, because the existence of multiple actors in an environment does not per se guarantee social behavior. In fact, it is important to realize that the use of multiple actors and their arrangement into5Note, that we are talking about the measure, can be a measure that represents the way out of a labyrinth without other actors. Rewards depend on whether the actor finds a way out or not. While this is clearly anti-social, we cannot ensure that any social behavior can ever take place. Let's imagine a non-social environment, like the way out of a labyrinth with other actors."}, {"heading": "4.2.1 Action Dependency", "text": "The key idea defines the interaction with respect to sensitivity to other actors or, in other words, whether the inclusion of different actors in the environment has an impact on what the evaluated actor does. A formalisation of this idea is as follows: definition 12. The degree of dependence on the evaluated actors playing in slot i in the environment has an impact on the weight class of opponents and team players weighing the patterns of action wL is given by the following factors: ADi (Phillips, wL, wL, wL, wL, \u00b5, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV, IV,"}, {"heading": "4.3 Non-neutralism", "text": "In fact, in ecology, there are seven possible combinations of positive, negative, or no effects between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0, -), commensalism (+, 0), competition (-, -), mutualism (+, +), and predation / parasitism (+, -). In our case, since we want to characterize environments that may contain individuals (possibly more than two), we can simplify this into neutralism, cooperation (including praise and mutualism), and competition (including the rest). In other words, we want to analyze whether interaction has no effect on rewards, has a positive or a negative relationship."}, {"heading": "4.3.1 Reward Dependency", "text": "The first thing we need to determine, therefore, is whether there is a dependence in rewards. This is very similar to the above-seen action dependence: Definition of reward dependence for evaluated agents playing in a slot environment, in patterns of opponents and team players, in which reward patterns and weight of placement patterns are given by the following factors: RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi (RDi), RDi), RDi (RDi), RDi), RDi (RDi), RDi), RDi), RDi (RDi), RDi)"}, {"heading": "4.3.2 Slot Reward Dependency", "text": "Both definitions 14 and 17 are necessary, because we can have a reward dependence without action dependence and action dependence without reward dependence."}, {"heading": "4.4 Secernment", "text": "It is an important feature of a test to be able to give different values for different rated agents. Otherwise, if the results are the same (or very similar) for most rated agents, we get little information. In other words, we want tests (i.e. the environment and group of agents that populate them) to hedge themselves to be discriminatory. Although there are many approaches to the idea of distinctiveness (see e.g. [26]), a simple idea that explains this concept quite well is the variance of the results."}, {"heading": "4.4.1 Fine and Coarse Discrimination", "text": "Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition:"}, {"heading": "4.4.2 Strict Total and Partial Grading", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4.5 Anticipation", "text": "Anticipation is a crucial characteristic related to social intelligence, which means that evaluated actors, both in competition and in cooperation, can benefit from the anticipation of the movements of other actors or, more generally, from a theory of the mind of others. Although it is very difficult to imagine a formalization of this concept, we can at least introduce an approximation."}, {"heading": "4.5.1 Competitive Anticipation", "text": "The first thing to do is to distinguish between competition and cooperation. In competition planning, we usually expect evaluated agents to perform better when their opponents are well predictable, for example, a random, not random situation. This phenomenon is generally difficult to define, but we can introduce a simplified approach based on the idea that a evaluated agent is competitive when his expected average reward, which competes against a (generally) non-random agent, is higher than his expected average reward, which competes against a random agent. This can be summarized as follows: Definition 31. The expectation advantage for evaluated agents 1 versus Agent 2, who plays in slots i or j in different teams (with i and j in different teams) when competing with a class of opponents and team players."}, {"heading": "4.5.2 Cooperative Anticipation", "text": "On the other hand, it is difficult to find a general definition for all possible coordination situations, but we can introduce a simplified approach based on the following intuitive definition: Two agents can cooperate cooperatively if the sum of the expected average rewards of 1 and 2 is higher when they interact with each other than the sum of each person interacting with a random agent. Definition 34: The expectation advantage for rated agents 1 and 2 playing in slots i and 2 is higher when they work in the environment with a class of opponents and team players. Definition 34: The expectation advantage for rated agents 1 and 2 playing in slots i and 2 (with i and j respectively in the same team) when they cooperate with a class of opponents and team players in the environment."}, {"heading": "4.6 Symmetry", "text": "In game theory, a symmetrical game is a game in which the payouts for playing a particular strategy depend only on the other strategies used by the rest of the agents, not on who plays them. This feature is very useful for evaluating targets. However, since we can change the positions of the agents and they maintain their results, we only need to evaluate the agent playing in a position of the environment, not on who plays them. In our definition of multi-agent environments, this definition of symmetry environments is not appropriate, but with the inclusion of teams, this definition of multi-agent environments does not apply. For example, using an environment with the division of slots between teams is not appropriate. The previous definition means that for each pair of line elements with the same agents, but in a different order, the agents get exactly the same results."}, {"heading": "4.7 Validity", "text": "(...). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.).). (.).). (.).). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.).). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.). (.). (.). (.).). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.). (.). (.). (.).).). (.).). (.).). (. (.).).). (.).).). (.). (.).).). (.).).). (.).). (. (.).).). (. (.).).). (.).). (.).).). (.).)."}, {"heading": "4.8 Reliability", "text": "Another key problem in psychometric tests is the concept of reliability, which means that the measurement is close to the actual value. (Note: This differs from validity, which refers to the true identification or definition of the actual value.) In other words, if we start from validity, i.e. that our definition is correct7, reliability refers to the quality of the measurement in relation to the actual value. (Note: The cause of the divergence can be systematic (bias), not systematic (deviation) or both.First, we must take into account that reliability applies to tests as introduced in Section 3.4. Reliability is then defined by repeating a test many times so that we become a random variable that we can compare with the true value."}, {"heading": "4.9 Efficiency", "text": "This property refers to how efficient a test is in terms of the (computational) time required to obtain a reliable evaluation. It is easy to see that efficiency and reliability are contradictory to each other. If we were able to perform an infinite number of infinite episodes, we would usually have the variance component of the reliability decomposition, which is affected if it is possible to keep the bias close to 0, even with very low values of the number of episodes in the definition."}, {"heading": "4.10 Summary of properties", "text": "In Tables 1 and 2 we see a summary of all previous properties. Table 1 shows the quantitative properties, while Table 2 shows the qualitative 8 properties, which completes our picture along with Figure 2. With these properties we have managed to show how appropriate an environment \u00b5 and the group of agents we use are to evaluate the social intelligence of a given set of evaluated agents. The set of properties we propose provides key information about the test bed we are analyzing. First, we can measure the influence that a group of agents has on a group of evaluated agents. Second, we can analyze to what extent the anticipation capabilities are useful for a group of evaluated agents, and third, we can determine whether cooperation or competition in the test bed is given greater importance. Fourth, we appreciate the discriminatory power that the test bed has for evaluating different agents. Fifth, the gradation power of the test bed shows how effective it is to rank agents. And we can better integrate some of these properties into the system."}, {"heading": "5 Degree of compliance of several multi-agent and social scenarios", "text": "Many games and environments have been proposed as test beds to evaluate performance in a multi-agent environment [45, 64, 53, 66]. Typically, these games and environments are created or selected to analyze or solve a specific problem or family. Since we are interested in developing tests for social intelligence, it is imperative to first assess whether these other earlier test beds could be as valid as they are (or with minor modifications). If not, they can still be a good source of inspiration to find new environment classes by reusing some of their ideas or hybridizing some of their properties. We will focus on some test beds whose specification is complete so that we can analyze the level of many games and environments, but we can practically only perform a selection of the most common and representative in the field of multi-agent systems, game theory and (social) computer games."}, {"heading": "5.1 Graphical analysis for the properties", "text": "Before we start with the games and environments, we will present some indicators and a graphical representation that are presented on a pictorial environment. In Table 3, we will show a summary of the most important elements we use in this section. In order to assess compliance with interactivity, non-neutrality and other properties for an environment, we must indicate a possible class of active agents that can be expressed in a particular political language. However, this would make it difficult (if not impossible) to calculate most properties. A better approach would be to use a (representative) sample of all active substances or a meaningful class."}, {"heading": "5.2 Matching pennies", "text": "This game consists of two players (or agents) who each spin one coin. If both coins match, each player can see the actions that the other player has performed. The game is usually repeated during K-steps (i.e., it is the repeated clash of pennies), which means that players can use past steps to predict the other player's strategy. Following the definition, each player can see the actions that the other player has performed. Following the definition, this environment allows only two actions Ai = {Head, Tail} and only offers two rewards Ri = {1, 1} that correspond to each losing and winning."}, {"heading": "5.3 Prisoner\u2019s dilemma", "text": "The two prisoners (or agents) are suspected of a crime and are asked if the other prisoner is guilty of the crime. If both sides cooperate and do not blame the other, both can spend a short time in jail. If one of them cooperates but not the other, the other reduces his time in prison to the minimum sentence, but the other prisoner receives the maximum sentence. Finally, if both prisoners blame the other, both can spend a long time in prison. As happens with the Matching Pennies, this game is played as a replay, meaning that the game is played on a single iteration, and the game repeats for multiple iterations. Each player can see the actions of the other player, which are normally repeated during the game (i.e. it is the iterated prisoner dilemma), so players can use past steps to predict the other player's strategy."}, {"heading": "5.4 Predator-prey (Pursuit game)", "text": "A typical cooperation environment that adds a variety of social complexity to the game is a chase game called predator prey, or cooperation with other two predators to hunt prey. If they succeed in hunting prey, the goal is achieved. Figure 9 shows an example of a predator-predator environment that agents can perceive in different environments, the number of predators or predators that offer a high variety of environments that have a high diversity of environments. Some examples include spaces with and without obstacles or boundaries, and many variations on the parameters were considered: the distance of the scenario that agents can perceive, the number of predators or predators, the speed of agents, etc. Also, the definition of how the predator is hunted has been modified, e.g. the prey is pursued by the predators, or a predator is pursued by occupying the prey by occupying the position."}, {"heading": "5.5 Pac-Man", "text": "In fact, it's not like you're able to surpass yourself, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" It's not like the world is OK. \"And then:\" It's not like the world is OK. \"And then:\" It's not like the world is OK, as if the world is OK, as if the world is OK, as if the world is OK. \"And then:\" It's not like the world is OK. \"And then:\" It's not like the world is OK, as if the world is OK, as if the world is OK, as if the world is OK, as if the world is OK, the world is OK, okay, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, fine, \"fine, fine, fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\" fine, \"fine,\""}, {"heading": "5.6 RoboCup Soccer", "text": "This year, we will be able to look for a solution that we are able to find, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "5.7 Summary", "text": "It is indeed the case that most of us are unable to play by the rules they have imposed on ourselves. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \"And further:\" It is not as if they play by the rules. \""}, {"heading": "6 Conclusions and Future work", "text": "This year, the time has come for us to be able to try to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "Acknowledgements", "text": "This work was supported by the MEC projects EXPLORA-INGENIO TIN 2009-06078-E, CONSOLIDERINGENIO 26706 and TINs 2010-21062-C02-02, 2013-45732-C4-1-P and the GVA projects PROMETEO / 2008 / 051 and PROMETEO / 2011 / 052. Javier Insa-Cabrera was sponsored by the Spanish MEC FPU grant AP2010-4389."}, {"heading": "Appendix", "text": "Before we start with each of the environments, we will prove a problem that will be helpful for the left and right ranges. We could calculate left and right with the help of a high number of agents. However, the more agents we include, the more difficult the calculation becomes. Instead, and to simplify the calculations, we can simply use the minimum number of agents for this property to calculate the maximum / minimum value according to the idea of lemma 1: Lemma 1. To calculate the maximum / minimum value for a property prop, the length of the set of evaluated agents can be used."}, {"heading": "A Matching Pennies properties", "text": "In this section we will demonstrate how we obtained the values for the properties for the matching pennies (Section 5.2). To calculate some of the values for the properties, we will use the problem 2.Lemma 2. In the matching pennies environment and for each slot, the introduction of a random reward factor in a row will always have an expected average reward of 0 for both agents.Proof. A random agent has a probability of p j r, h = p j r, t = 1 2 to reward both head and tail during iteration j. Let us specify the agent who interacts with, and denote with p 1 s, h the probability of performing head and p1s, t = 1 \u2212 p1s, h the probability of performing tail at the first iteration for the expected rewards. To calculate the expected reward of an agent, we will get the possible rewards that this agent can receive multiplied by the probability that these will occur."}, {"heading": "A.1 Action Dependency", "text": "We start with action dependence (AD). Since the environment is not symmetrical, we need to know whether the evaluated agents behave differently depending on action dependence (AD). We use \"S\" (a, b) = 1 if the distributions a and b are equal and 0 otherwise.Proposition 2: \"general min\" for action dependence (AD) property is equal to 0 for the matching pennies environment. Proof. To find a \"general min\" (Eq.40), we need to find a triad that minimizes the property as much as possible. We can have this situation by selecting \"S\" by selecting \"E\" = {zept \"with action dependence (B) = 1 and\" L \"p\" (A). \""}, {"heading": "A.2 Reward Dependency", "text": "As stated in Section 4.3.1, we want to know if the rated agents receive different expected average rewards (depending on which line they interact with). We use \"Q\" (a, b) = 1 if the numbers a and b are equal and 0 otherwise.Proposition 6. General min for reward dependence (RD) property is equal to 0 for the matching pennies. Prooft: To find a \"general min\" (Eq.40), we need to find a \"trio\" that minimizes the property as much as possible. We can have this situation by selecting \"L\" by selecting \"L\" (4) = 1 and \"L\" (a) = {h1, \u03c0h1} (a). \""}, {"heading": "A.3 Fine Discrimination", "text": "As indicated in Section 4.4.1, we want to know if different rated agents receive different expected average rewards when they interact in the environment. We use Q (a, b) = 1 if the numbers a and b are equal and 0 are otherwise. Proposal 10. General min for fine discrimination (FD) property is equal to 0 for the appropriate pennies environment.Proof. To find General min (Eq.40), we need to find a trio that minimizes the property as much as possible. We can have this situation by selecting this result by selecting this 2 with equal weight."}, {"heading": "A.4 Strict Total Grading", "text": "We come to the strict overall classification (STG) < < As specified in Section 4.4.2, we want to know if there is a strict order between the assessed agents when they interact in the environment. < To simplify the notation, we use the next table to represent the STO: Ri (l, j, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "A.5 Partial Grading", "text": "To simplify the notation, we use the following table to represent the PO: Ri (l) (l) (l), j), p), p), p), p), p), p), p), p) (l), p) (l), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p) (l), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p),"}, {"heading": "A.6 Slot Reward Dependency", "text": "Next we see the Slot Reward Dependency (SRD) property. As specified in Section 4.3.2, we want to know how much competitiveness or cooperation the environment is. Proposition 22: General Slot Reward Dependency (SRD) property is equal to [\u2212 1, \u2212 1] for the associated pennies of environment.Proof. The following definition 20: We get the SRD value for each pair of slots. The following definition 19: We can calculate the SRD value for each pair of slots. We start with slots 1 and 2: SRD1,2 (SRD1,2), as the environment is not symmetrical, we have to calculate this property for each pair of slots."}, {"heading": "A.7 Competitive Anticipation", "text": "As stated in Section 4.5.1, we would like to know how much benefit the evaluated agents will receive if they expect competing agents. Proposal 23. General Meeting for Competitive Expectation (AComp) property is equal \u2212 12 for the appropriate pennies of environment.Proof. To find a total (equation 40), we need to find a trio that minimizes the property as much as possible. We can have this situation by selecting this situation by selecting the number of people (equation 40). Proof. To find a total (equation 40), we need to find a total (an agent) that always executes the property header and a total (an agent) when we play in slot 1 and always execute the same sum. Following the definition 33, we get the AComp value for this sum that we receive. Since the environment is not symmetrical, we must calculate this property equally for each team."}, {"heading": "B Prisoner\u2019s Dilemma properties", "text": "In this section we will show how we obtained the values for the properties of the prisoner dilemma (Section 5.3)."}, {"heading": "B.1 Action Dependency", "text": "We start with the Action Dependency (AD) property. As indicated in Section 4.2.1, we want to know whether the evaluated agents behave differently depending on the Action Dependency (AD)."}, {"heading": "B.2 Reward Dependency", "text": "As explained in Section 4.3.1, we want to know if the rated agents receive different expected average rewards depending on which line they interact with. We use Q (a, b) = 1 if the numbers a and b are equal and 0 otherwise.Proposition 29. General min for reward dependence (RD) property is equal to 0 for the prisoner's dilemma. Proof. To find a general definition (Eq.40), we need to find a trio that minimizes the property as much as possible. We can have this situation by selecting the following definition: E = {perspecb} with which we obtain such a definition."}, {"heading": "B.3 Fine Discrimination", "text": "As indicated in Section 4.4.1, we want to know if different rated agents receive different expected average rewards when they interact in the environment. We use Q (a, b) = 1 when the numbers a and b are equal and 0 are otherwise. Proposal 33. General min for the Fine Discrimination (FD) property is equal to 0 for the trapped dilemma. To find this situation, we need to find a trio that minimizes the property as much as possible."}, {"heading": "B.4 Strict Total Grading", "text": "We come to the strict overall valuation (STG) < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "B.5 Partial Grading", "text": "As stated in Section 4.4.2, we want to know if there is a partial order between the rated agents when they interact in the environment. \u2212 To simplify the notation, we use the following table to represent the PO: Ri (\u00b5 [l, j, p, p, p, p, p, p, p, p, p, p, p, p, p, p), Ri (p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "B.6 Slot Reward Dependency", "text": "Next, we see the Slot Reward Dependency (SRD) as a result of the SRD definition. As stated in Section 4.3.2, we want to know how much competitiveness or cooperative ability the environment requires (Eq.40). Proposition 44: Generalmin for the Slot Reward Dependency (SRD) property is equal to \u2212 1 for the prisoner's dilemma. Proof. To find the Slot Reward (Eq.40), we need to find a trio (Eq.40) that minimizes the property as much as possible. We can have this situation by selecting the SRD definition. Proof. e (1) = 1 and [Eq.40) we need to find a trio that minimizes the Slot Reward Dependency (SRD Definition) -1 for property (SRD Definition) as much as possible. We get the SRD value for this SRD definition."}, {"heading": "B.7 Competitive Anticipation", "text": "As stated in Section 4.5.1, we want to know how much benefit the evaluated agents will receive if they expect competing agents."}, {"heading": "C Predator-prey properties", "text": "In this section we show how we obtained the values for the properties of the predator-prey environment (Section 5.4). To calculate some of the values for the properties, we use Lemma 3. When three well-coordinated predators try to hunt the prey, it is always hunted at 5 intervals or less, regardless of the behavior of the prey. As there are many variants to hunt the prey, we cannot show them all. Instead, we show one of the largest sequences of actions to hunt the prey at 5 intervals when the prey is trying to escape and the predators are well coordinated. \"Other behaviors of the prey bring them closer to the boundaries, which would be easier for the predators to hunt them."}, {"heading": "C.1 Action Dependency", "text": "We start with the action dependency (AD) property. As stated in Section 4.2.1, we want to know whether the evaluated agents behave differently (depending on which action dependency they interact with). We use \"S\" (a, b) = 1 when distributions a and b are equal and 0 are otherwise. Proposition 48. General term for action dependence (AD) property is equal to 0 for action dependence (AD). Proofu) = 1 and 2 when we need to find a tripartite relationship that minimizes the property as much as possible. We can have this situation by selecting \"S\" by selecting \"S.\" = 1 and vice versa \"S\" (a)."}, {"heading": "C.2 Reward Dependency", "text": "As stated in Section 4.3.1, we want to know if the rated agents receive different expected average rewards (depending on which line they interact with). We use Q (a, b) = 1 if the numbers a and b are equal and 0 are otherwise. Suggestion 52. General min for reward dependence (RD) is equal to 0 for the predatory / predatory environment. To find out if the numbers a and b are equal and 0 are otherwise, we need to find a trio that minimizes the property as much as possible. We can have this situation by having this situation by finding E = {\u03c0u} with the robbery (Eq.40) = 1 and vice versa (Eq.40), we need to find a trio that minimizes the properties as much as possible."}, {"heading": "C.3 Fine Discrimination", "text": "Since the environment is not symmetrical, we have to calculate this property for each subsequent definition. We want to know if different rated agents receive different expected average rewards when they interact in the environment. We use Q (a, b) = 1 if the numbers a and b are equal and 0 are otherwise. Proposition 54. Generalmin for Fine Discrimination (FD) property is equal to 0 for the predator-prey environment. Proof. To find such a situation, we have to find a trio that minimizes the property as much as possible. We can have this situation by selecting this situation by running this 2 with a uniform weight for w\u00b5 (Equation 40)."}, {"heading": "C.4 Strict Total Grading", "text": "We come to the strict overall valuation (STG) < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "C.5 Partial Grading", "text": "As indicated in section 4.4.2, we want to know whether there is a partial classification between the evaluated agents when they interact in the environment. To simplify the notation, we use the following table to represent the PO: Ri (l \u00b2 i, j \u00b2 p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p p p p p p p p p p p = p \u00b2 p p p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p p \u00b2 p = p p \u00b2 p \u00b2 p = p p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p p p = p p p \u00b2 p p = p p p \u00b2 p p = p p p \u00b2 p = p p p \u00b2 p p = p \u00b2 p p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p \u00b2 p \u00b2 p \u00b2"}, {"heading": "C.6 Slot Reward Dependency", "text": "Next, we see the Slot Reward Dependency (SRD) property. As stated in Section 4.3.2, we want to know how much competitiveness or cooperation the environment is. Proposition 62: General area for Slot Reward Dependency (SRD) property is equal to [0, 0] for the Predator-Prey Environment. Proof. Since the environment is not symmetrical, we need to calculate this property for each pair of slots. Following definition 19: We can calculate the SRD value for each pair of slots. We start with slots 1 and 2: SRD1,2 (SRD1,2), we know that we are not symmetrical with this slot-1, we need to calculate this property for each pair of slots."}, {"heading": "C.7 Competitive Anticipation", "text": "As stated in Section 4.5.1, we would like to know how much benefit the rated agents will receive if they expect the competing agents."}, {"heading": "C.8 Cooperative Anticipation", "text": "As stated in Section 4.5.2, we want to know how much benefit the evaluated agents will receive if they expect to cooperate with the agents. Proposition 65: General Minutes for Cooperative Expectation (ACoop) Property is equivalent to \u2212 1 for the predatory environment. Proof. to find a General Minutes (Eq.40), we need to find a trio that minimizes property as much as possible. We can have this situation by selecting the property by selecting it as far as possible."}], "references": [{"title": "Expertness based cooperative q-learning. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["Majid Nili Ahmadabadi", "Masoud Asadpour"], "venue": "IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "The arcade learning environment", "author": ["MG Bellemare", "Y Naddaf", "J Veness", "M Bowling"], "venue": "J. Artificial Intelligence Res,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On optimal cooperation of knowledge sources", "author": ["Miroslav Benda"], "venue": "Technical Report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1985}, {"title": "The first international roshambo programming competition", "author": ["Darse Billings"], "venue": "International Computer Games Association Journal,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Evolutionary online learning of cooperative behavior with situationaction pairs", "author": ["Joerg Denzinger", "Michael Kordt"], "venue": "InMultiAgent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Experiments in learning prototypical situations for variants of the pursuit game", "author": ["J\u00f6rg Denzinger", "Matthias Fuchs"], "venue": "In Proc. ICMAS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "AI - What is this? A definition of artificial intelligence", "author": ["D. Dobrev"], "venue": "PC Magazine Bulgaria (in Bulgarian, English version at http://www.dobrev.com/AI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "A non-behavioural, computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "In Intl. Conf. on Computational Intelligence & multimedia applications (ICCIMA\u201998), Gippsland,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "IQ tests are not for machines", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "yet. Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "How universal can an intelligence test be", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Adaptive Behavior,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On interaction complexity, (space-time) resolution and intelligence", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "In ReteCog interaction Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The rating of chessplayers", "author": ["A.E. Elo"], "venue": "past and present,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1978}, {"title": "On a pursuit game on cayley", "author": ["Peter Frankl"], "venue": "graphs. Combinatorica,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "Learning to play pac-man: An evolutionary, rule-based approach", "author": ["Marcus Gallagher", "Amanda Ryan"], "venue": "In Evolutionary Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "National Institute of Standards and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Universal psychometrics: Measuring cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "David L. Dowe", "M.Victoria Hern\u00e1ndez-Lloreda"], "venue": "Cognitive Systems Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Turing tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard"], "venue": "The Alan Turing Centenary Conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Turing machines and recursive Turing tests", "author": ["J. Hernandez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard"], "venue": "AISB/IACAP 2012 Symposium \u201dRevisiting Turing and his Test\u201d,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "In Proc. Intl Symposium of Engineering of Intelligent Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "In Artificial General Intelligence, 3rd Intl Conf,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Complexity distribution of agent policies", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "CoRR, abs/1302.2056,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "On environment difficulty and discriminating power", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "David L. Dowe"], "venue": "Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Humans have evolved specialized skills of social cognition: The cultural intelligence hypothesis", "author": ["E. Herrmann", "J. Call", "M.V. Hern\u00e1ndez-Lloreda", "B. Hare", "M. Tomasello"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "The structure of individual differences in the cognitive abilities of children and chimpanzees", "author": ["E. Herrmann", "M.V. Hern\u00e1ndez-Lloreda", "J. Call", "B. Hare", "M. Tomasello"], "venue": "Psychological Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Adversarial sequence prediction", "author": ["B. Hibbard"], "venue": "In Proceeding of the 2008 conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Measuring agent intelligence via hierarchies of environments", "author": ["B. Hibbard"], "venue": "Artificial General Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "Artificial General Intelligence 2011,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Evaluating a reinforcement learning algorithm with a general intelligence test", "author": ["J. Insa-Cabrera", "D.L. Dowe", "J. Hernandez-Orallo"], "venue": "Advances in Artificial Intelligence - 14th Conference of the Spanish Association for Artificial Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Interaction settings for measuring (social) intelligence in multiagent systems", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo"], "venue": "In ReteCog interaction Workshop,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Hernandez-Lloreda. The anynt project intelligence test : Lambda - one", "author": ["J. Insa-Cabrera", "J. Hernandez-Orallo", "D.L. Dowe", "S. Espaa", "M.V"], "venue": "AISB/IACAP 2012 Symposium \u201dRevisiting Turing and his Test\u201d,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "On measuring social intelligence: experiments on competition and cooperation", "author": ["Javier Insa-Cabrera", "Jos\u00e9-Luis Benacloch-Ayuso", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "In Proceedings of the 5th international conference on Artificial General Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Indirect interaction in environments for multi-agent systems", "author": ["David Keil", "Dina Q. Goldin"], "venue": "editors, E4MAS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Robocup: The robot world cup initiative", "author": ["Hiroaki Kitano", "Minoru Asada", "Yasuo Kuniyoshi", "Itsuki Noda", "Eiichi Osawa"], "venue": "In Proceedings of the first international conference on Autonomous agents,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Environmental science: systems and solutions", "author": ["Michael L Mac Kinney", "Robert Milton Schoch"], "venue": "Jones & Bartlett Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Are you socially intelligent", "author": ["F.A. Moss", "T. Hunt"], "venue": "Scientific American,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1927}, {"title": "Game theory: analysis of conflict", "author": ["Roger B Myerson"], "venue": "Harvard university press,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "An introduction to game theory", "author": ["Martin J Osborne"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Emergent bucket brigading: a simple mechanisms for improving performance in multi-robot constrained-space foraging tasks", "author": ["Esben H Ostergaard", "Gaurav S Sukhatme", "Maja J Matari"], "venue": "In Proceedings of the fifth international conference on Autonomous agents,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}, {"title": "A strategic metagame player for general chess-like games", "author": ["Barney Pell"], "venue": "Computational Intelligence,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Using genetic programming with multiple data types and automatic modularization to evolve decentralized and coordinated navigation in multiagent systems", "author": ["Alan Robinson", "Lee Spector"], "venue": "In Late Breaking Papers at the Genetic and Evolutionary Computation Conference", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2002}, {"title": "The Shapley value: essays in honor of Lloyd S", "author": ["Alvin E Roth"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1988}, {"title": "Behaviour of trading automata in a computerized double auction", "author": ["John Rust", "Richard Palmer", "John H Miller"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1992}, {"title": "Learning to coordinate without sharing information", "author": ["Sandip Sen", "Ip Sen", "Mahendra Sekaran", "John Hale"], "venue": "Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1994}, {"title": "Fundamentals of Comparative Cognition", "author": ["S.J. Shettleworth", "P. Bloom", "L. Nadel"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Shettleworth. Cognition, evolution, and behavior", "author": ["J Sara"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Multiagent systems: algorithmic, game-theoretic, and logical foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": "Cambridge Univ Pr,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT press,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1998}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Ming Tan"], "venue": "In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1993}, {"title": "A monte-carlo aixi approximation", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Some characteristics of the good judge of personality", "author": ["Philip E. Vernon"], "venue": "The Journal of Social Psychology,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1933}, {"title": "Comparative cognition: Experimental explorations of animal intelligence", "author": ["Edward A Wasserman", "Thomas R Zentall"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2006}, {"title": "The measurement and appraisal of adult intelligence", "author": ["David Wechsler"], "venue": "Academic Medicine,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1958}, {"title": "Evolutionary game theory", "author": ["J\u00f6rgen W Weibull"], "venue": "The MIT press,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1995}, {"title": "Theory and Measurement of Social Intelligence as a Cognitive Performance Construct", "author": ["Susanne Weis"], "venue": "PhD thesis, Otto-von-Guericke-Universita\u0308t Magdeburg, Universita\u0308tsbibliothek,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Designing the market game for a trading agent competition", "author": ["Michael P. Wellman", "Peter R. Wurman", "Kevin O\u2019Malley", "Roshan Bangera", "S-d Lin", "Daniel Reeves", "William E. Walsh"], "venue": "Internet Computing,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2001}, {"title": "Environments for multi-agent systems, state-of-the-art and research challenges", "author": ["D. Weyns", "H.V.D. Parunak", "F. Michel", "T. Holvoet", "J. Ferber"], "venue": "In Environments for MAS,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2005}, {"title": "Generalized measures of information transfer", "author": ["P.L. Williams", "R.D. Beer"], "venue": "arXiv preprint arXiv:1102.1507,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Learning mazes with aliasing states: An LCS algorithm with associative perception", "author": ["Z. Zatuchna", "A. Bagnall"], "venue": "Adaptive Behavior,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "co-evolutionary fitness switching: Learning complex collective behaviors using genetic programming", "author": ["Byoung-Tak Zhang", "Dong-Yeon Cho"], "venue": "Advances in genetic programming,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1999}], "referenceMentions": [{"referenceID": 40, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 193, "endOffset": 197}, {"referenceID": 57, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 243, "endOffset": 247}, {"referenceID": 55, "context": "Many definitions have been proposed such as the \u201cability to understand and manage men and women, boys and girls \u2013 to act wisely in human relations\u201d [56], the \u201cability to get along with others\u201d [41], the \u201cfacility in dealing with human beings\u201d [60], or more specific definitions including \u201c[the] ability to get along with people in general, social technique or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into the temporary moods or the underlying personality traits of friends and of strangers\u201d [58].", "startOffset": 573, "endOffset": 577}, {"referenceID": 59, "context": "Despite the ambiguity of what social intelligence is, many tests have been proposed to measure social intelligence in humans (see [62] for a survey).", "startOffset": 130, "endOffset": 134}, {"referenceID": 52, "context": "This is the same configuration as in reinforcement learning (RL) [54], where rewards are provided in order to encourage agents to perform tasks.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].", "startOffset": 131, "endOffset": 139}, {"referenceID": 28, "context": "For instance, some recent work has shown that social abilities can be compared in a systematic way between human children and apes [28, 29].", "startOffset": 131, "endOffset": 139}, {"referenceID": 56, "context": ", [59, 51]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 49, "context": ", [59, 51]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 42, "context": "For instance, prey-predator interaction and behaviour have been studied from many different points of view (including game theory [43]), but it is not clear how the ability of each subject can be objectively evaluated, especially because the interaction depends on the cognitive abilities of both prey and predator.", "startOffset": 130, "endOffset": 134}, {"referenceID": 56, "context": "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.", "startOffset": 50, "endOffset": 58}, {"referenceID": 49, "context": "Despite these difficulties, comparative cognition [59, 51] is more and more concerned about performing tests that compare the abilities of many different species and also the abilities of individuals of different species.", "startOffset": 50, "endOffset": 58}, {"referenceID": 26, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 18, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 9, "context": "From this point of view, it should be more natural to provide a single test (with possibly many different customised interfaces and rewards) to assess every kind of species (or, in other words, a more general, or universal [27, 19, 10], test).", "startOffset": 223, "endOffset": 235}, {"referenceID": 2, "context": "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).", "startOffset": 277, "endOffset": 284}, {"referenceID": 37, "context": "The environments used in social tests for machines tend to represent tasks that the agents must perform by interacting with other agents, so the performance is calculated as their capability to successfully cooperate with and/or compete against them to achieve some goals (see [3, 38] for two testbeds in multi-agent environments).", "startOffset": 277, "endOffset": 284}, {"referenceID": 42, "context": "In the context of social sciences (stretching from economics to AI), game theory [43] has also studied the interaction of different agents in formalised structures (called games).", "startOffset": 81, "endOffset": 85}, {"referenceID": 42, "context": "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].", "startOffset": 253, "endOffset": 261}, {"referenceID": 41, "context": "Several kinds of games try to represent or to analyse a variety of properties: cooperative or non-cooperative games, simultaneous or sequential games, normal-form or extensive-form games, zero-sum or general-sum games, and symmetric or asymmetric games [43, 42].", "startOffset": 253, "endOffset": 261}, {"referenceID": 47, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 37, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 60, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 111, "endOffset": 130}, {"referenceID": 2, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 53, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 48, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 45, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 64, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 43, "context": "Multi-agent systems (MAS), on the contrary, present richer and more diverse possibilities, both in competition [49, 4, 38, 63, 46] and cooperation [3, 55, 50, 47, 67, 44].", "startOffset": 147, "endOffset": 170}, {"referenceID": 7, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 22, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 15, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 16, "context": "Formal approaches started in the late 1990s using notions from Kolmogorov complexity, Solomonoff prediction and the MML principle [8, 23, 16, 17].", "startOffset": 130, "endOffset": 145}, {"referenceID": 6, "context": "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name \u201cUniversal Intelligence\u201d.", "startOffset": 7, "endOffset": 10}, {"referenceID": 38, "context": "Dobrev [7] suggested that machine intelligence should be measured by evaluating agent performance in a range of worlds, an idea that was independently developed in [39] under the name \u201cUniversal Intelligence\u201d.", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Following this definition, a framework to evaluate intelligence [27] and an environment following the framework [24] were proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 31, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 34, "context": "In order to show their effectiveness, some experiments were performed [33, 32, 35], but their results suggested that the framework still has some limitations.", "startOffset": 70, "endOffset": 82}, {"referenceID": 35, "context": "This was the goal in [36], where other agents were directly included in the environment.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "These experiments were performed using the framework in [27], which was originally designed to evaluate general intelligence, by simply including other agents in the environment.", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "This circular problem is turned into a recursive one in [18], where different levels of distributions are recursively provided by constructing a new distribution of agents from a prior distribution 1Not only as an alternative to MAS scenarios, but also to the Turing Test, CAPTCHAs and IQ tests (see [9] for a discussion).", "startOffset": 300, "endOffset": 303}, {"referenceID": 18, "context": "This is in the spirit of universal psychometrics [19], where we must consider any kind of agent (natural or artificial).", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "This can take us to definitions such as \u201cperformance of an agent in a wide range of environments while interacting with other agents\u201d [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "oi,k) \u2192 [0, 1] denotes its probability to perform action ai,k after the sequence of events oi,1ai,1ri,1 .", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "ok\u22121ak\u22121rk\u22121) \u2192 [0, 1].", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "okak) \u2192 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 46, "context": "Also, we do not use the term alliance as we do not use any sophisticated mechanism to award rewards, related to the contribution of each agent in the team, as it is done with the Shapley Value [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 37, "context": "Another example is RoboCup Soccer [38], denoted by \u03bcrc, whose \u03c4 would be {{1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}}, which represents that there are two teams, with slots {1, 2, 3, 4, 5} in the first team and slots {6, 7, 8, 9, 10} in the second team.", "startOffset": 34, "endOffset": 38}, {"referenceID": 62, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 10, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 33, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 11, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 60, "endOffset": 76}, {"referenceID": 36, "context": "In fact, this is at the roots of definitions of interaction [65, 11, 34, 12], and the distinction between several kinds of interaction [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "However, as pointed out by [12], \u201cthis may originate from a common source\u201d, so common or mutual information is not sufficient for interaction to have taken place.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "In fact, in ecology, given two species, there are seven possible combinations of positive, negative or no effect between them, leading to six forms of symbiosis [40]: neutralism (0,0), amensalism (0,-), commensalism (+,0), competition (-,-), mutualism (+,+), and predation/parasitism (+,-).", "startOffset": 161, "endOffset": 165}, {"referenceID": 46, "context": "The previous definition may slightly resemble the Shapley Value [48] in cooperative game theory, but here we are not concerned about how relevant each agent is in a team (whether its contribution is higher than the contribution of its teammates), but to see whether there is effect on the rewards.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": ", [26]), one simple notion that accounts for this concept quite well is the variance of results.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "So the idea we will pursue is to evaluate how close an environment and set of agents populating it are to this ideal situation from the expected average rewards of the evaluated agents (without an aggregated rating system): 6A common approach is to create a rating when we have many experiments, as done with sport ratings, such as the ELO rating [13] in chess.", "startOffset": 347, "endOffset": 351}, {"referenceID": 21, "context": "5][22], showing an agent set for the matching pennies game that is non-monotonic.", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "Nonetheless, a partial order can still be constructed for the agent set of all finite state machines for this game [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 38, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 26, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 23, "context": "One possibility is to consider all environments (as done by [39, 27]), and another is to find an environment class that is sufficiently representative (as attempted in [24]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.", "startOffset": 96, "endOffset": 104}, {"referenceID": 19, "context": "The set of all possible agents (either artificial or biological) is known as machine kingdom in [19, 20] and raises many questions about the feasibility of any test considering this astronomically large set.", "startOffset": 96, "endOffset": 104}, {"referenceID": 24, "context": "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "In this way, we could aim at social intelligence relative to a smaller (and well-defined) set of agents, possibly specialising the definition by limiting the resources, the program size [25] or the intelligence of the agents [18].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 18, "context": "4 is not adaptive (for adaptive versions of universal tests, the reader is referred to [27, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 44, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 61, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 51, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 63, "context": "Many games and environments have been proposed as testbeds to evaluate performance in a multi-agent environment [45, 64, 53, 66].", "startOffset": 112, "endOffset": 128}, {"referenceID": 0, "context": "[0, 1] The evaluated agents do not take into account other agents\u2019 actions in their behaviour.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Each evaluated agent obtains the same expected average reward independently of the line-up pattern.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Every evaluated agent obtains the same expected average reward for each line-up pattern.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] Every evaluated agent obtains the same (social) intelligence value.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] There is no strict total order between the evaluated agents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] There is no partial order between the evaluated agents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 58, "context": "2 Matching pennies Matching pennies [61] can be considered the simplest game in game theory featuring competition.", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name \u2018Adversarial Sequence Prediction\u2019 [30, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 30, "context": "Nonetheless, there are different opinions about this, as it has been suggested that matching pennies could be an intelligence test on its own, under the name \u2018Adversarial Sequence Prediction\u2019 [30, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 20, "context": "In [21] there is an example of an agent set for matching pennies that is non-monotonic (so PG < 1).", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "But, a good selection of \u03a0o can restrict the Right range making it equal to [1, 1] (proposition 32).", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "But, a good selection of \u03a0o can restrict the Right range making it equal to [1, 1] (proposition 32).", "startOffset": 76, "endOffset": 82}, {"referenceID": 2, "context": "4 Predator-prey (Pursuit game) One typical environment for cooperation that uses a 2D discrete space is a pursuit game called Predator-prey [3], where the evaluee acts as a predator and has to cooperate/coordinate with other two predators in order to chase a prey.", "startOffset": 140, "endOffset": 143}, {"referenceID": 53, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 5, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 4, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 13, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 0, "context": ", [55, 6, 5, 14, 1]), but, in our opinion, no thorough study about their properties has been developed so far.", "startOffset": 2, "endOffset": 19}, {"referenceID": 1, "context": "One example of the use of games for evaluating AI is the ALE (Arcade Learning Environment) [2], a framework where a set of arcade computer games are used to evaluate the performance of current AI algorithms.", "startOffset": 91, "endOffset": 94}, {"referenceID": 54, "context": ", [57, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [57, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 37, "context": "6 RoboCup Soccer As an example of a 3D space game we find the RoboCup Soccer competition [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": ", as in the spirit of the Darwin-Wallace distribution [18]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "[1] Majid Nili Ahmadabadi and Masoud Asadpour.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] MG Bellemare, Y Naddaf, J Veness, and M Bowling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Miroslav Benda.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Darse Billings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joerg Denzinger and Michael Kordt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] J\u00f6rg Denzinger and Matthias Fuchs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Peter Frankl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Marcus Gallagher and Amanda Ryan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Jos\u00e9 Hern\u00e1ndez-Orallo and David L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Javier Insa-Cabrera, Jos\u00e9-Luis Benacloch-Ayuso, and Jos\u00e9 Hern\u00e1ndez-Orallo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] David Keil and Dina Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, and Eiichi Osawa.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] Shane Legg and Marcus Hutter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] Michael L Mac Kinney and Robert Milton Schoch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] Roger B Myerson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] Martin J Osborne.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] Esben H Ostergaard, Gaurav S Sukhatme, and Maja J Matari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] Barney Pell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[47] Alan Robinson and Lee Spector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] Alvin E Roth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49] John Rust, Richard Palmer, and John H Miller.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[50] Sandip Sen, Ip Sen, Mahendra Sekaran, and John Hale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] Sara J Shettleworth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[53] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[55] Ming Tan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[57] Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[58] Philip E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[59] Edward A Wasserman and Thomas R Zentall.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[60] David Wechsler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[61] J\u00f6rgen W Weibull.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[62] Susanne Weis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[63] Michael P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[64] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[65] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[66] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "[67] Byoung-Tak Zhang and Dong-Yeon Cho.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour. The amalgamation of these traits is then used to configure the intuitive notion of social intelligence. Instead, in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents, and we assess and derive tests from it. This definition makes several dependencies explicit: (1) the definition depends on the choice (and weight) of environments and agents, (2) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams, (3) the definition mostly depends on the abilities of other agents, and (4) the actual difference between social intelligence and general intelligence (or other abilities) depends on these choices. As a result, we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour (such as action and reward dependency and anticipation on competitive/cooperative behaviours) are met as well as some other more instrumental properties (such as secernment, boundedness, symmetry, validity, reliability, efficiency), which are convenient to convert the definition into a practical test. From the definition and the formalised properties, we take a look at several representative multi-agent environments, tests and games to see whether they meet these properties.", "creator": "LaTeX with hyperref package"}}}