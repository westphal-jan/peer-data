{"id": "1408.6515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2014", "title": "Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform", "abstract": "This paper describes the solution of Bazinga Team for Tmall Recommendation Prize 2014. With real-world user action data provided by Tmall, one of the largest B2C online retail platforms in China, this competition requires to predict future user purchases on Tmall website. Predictions are judged on F1Score, which considers both precision and recall for fair evaluation. The data set provided by Tmall contains more than half billion action records from over ten million distinct users. Such massive data volume poses a big challenge, and drives competitors to write every single program in MapReduce fashion and run it on distributed cluster. We model the purchase prediction problem as standard machine learning problem, and mainly employ regression and classification methods as single models. Individual models are then aggregated in a two-stage approach, using linear regression for blending, and finally a linear ensemble of blended models. The competition is approaching the end but still in running during writing this paper. In the end, our team achieves F1Score 6.11 and ranks 7th (out of 7,276 teams in total).", "histories": [["v1", "Wed, 27 Aug 2014 06:32:21 GMT  (702kb,D)", "https://arxiv.org/abs/1408.6515v1", "Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014"], ["v2", "Tue, 9 Sep 2014 16:30:51 GMT  (702kb,D)", "http://arxiv.org/abs/1408.6515v2", "Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014"], ["v3", "Wed, 4 Mar 2015 15:11:18 GMT  (702kb,D)", "http://arxiv.org/abs/1408.6515v3", "Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014"]], "COMMENTS": "Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuyu zhang", "liang pang", "lei shi", "bin wang"], "accepted": false, "id": "1408.6515"}, "pdf": {"name": "1408.6515.pdf", "metadata": {"source": "CRF", "title": "Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform", "authors": ["Yuyu Zhang", "Liang Pang", "Lei Shi", "Bin Wang"], "emails": ["shilei1025}@gmail.com,", "wangbin@ict.ac.cn"], "sections": [{"heading": null, "text": "Categories and Subject Descriptions H.2.8 [Database Applications]: Data MiningGeneral Terms Algorithms, ExperimentationKeywords Recommended Price for Shopping Malls, Purchase Forecast, Recommendation System"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2. DATA PREPROCESSING", "text": "In this section, we first perform basic data analysis and accordingly introduce data splitting for offline validation and data cleansing."}, {"heading": "2.1 Data Analysis", "text": "In the dataset, the record date ranges from April to July (from 04-15 to 08-15, year unknown).For the sake of simplicity, we have a naming convention for the 4-month data, as shown in Table 1.Table 2, the basic statistics of the dataset. This table shows that the data is distributed almost evenly during these 4 months. On average, each user has about 50 action records, while each brand has about 20,000 action records. Comparatively abundant information can be very helpful for brand-level aggregation.For the four types of action, it is obvious that click is the \"cheapest.\" The total ratio between click and purchase is about 39: 1."}, {"heading": "2.2 Data Splitting", "text": "Figure 1 illustrates the details. For online setting, we use data sets localized in the first 4 months, and the August data is an invisible data set used to evaluate predictions. In terms of local data allocation, we use data sets in the first three months (April, May and June) as local data and data sets in July as local response sets that are invisible and used to evaluate local predictions."}, {"heading": "2.3 Data Cleansing", "text": "One possible explanation is that these users may be crawlers who automatically access the destination web page several times a day. Obviously, such a \"user\" should not be taken into account in the model. We use a simple filter rule to eliminate this noise in the data: if a user has more than 500 clicks but buys 0, all of that user's action records will be removed. According to our statistics, approximately 22 million action records are filtered out, accounting for 4% of the record."}, {"heading": "3. PURCHASE PREDICTION MODELING", "text": "In this section, we present the modeling of the problem of forecasting purchases, including how to build instances and how to design time spans of characteristics and objectives."}, {"heading": "3.1 Instance Building", "text": "We model the task of purchase forecasting as a standard problem of machine learning. We take a pair (user ID, brand ID) as an instance. User actions are obviously non-i.e. and highly dependent on historical actions. For example, it is much more likely that the user buys a certain brand if he has previously clicked on it and put it in the basket. Therefore, we choose to build an instance in a time-dependent manner: The objective of the instance is determined by future actions. Here, goal stands for the basic truth of the instance. If the task is modeled as a classification problem, the goal is a binary purchase label or not in the future. While the goal is modeled as a regression problem, the goal may be either a binary label or the purchase times in the future. Let's take the classification modeling as an example of each (user ID, brand ID) pair, what we must predict is a conditional action probability of the next month (including the next month of purchase or not the corresponding action)."}, {"heading": "3.2 Time Span", "text": "As discussed above, the periods of function and goal are separated from each other. We have two different ways of constructing time periods: \u2022 Fixed periods. In training, we use the last month as the target range and the previous months as the target range. In training, we use all available months as the target range. Figure 3 illustrates the details of fixed periods both in the local and online environment. \u2022 Moving periods. In training, the trait range becomes expandable, and the target range follows the moving end of Table 2: Basic statistics of the data set.April May July Date Total (User ID, Brand ID) 52,909,766 59,293 49,776,687 50,188,852 195,303,359 195,006 User ID 7,513,602 8,024,680 7,415,736 7,497,824 12,500,984 Days1 Days1 Days-, Brand ID 23,305 23,742 725,703 50,803 User ID 7,003 933,936 Action 95,950,957 950,950,950 93ID 3,950,93ID 3,949,9379 3,949,079 3,702 Action 950,950,957 950,950 950,950 93,950 93,93ID 3,949,950 3,950 3,9479 39,949,949,950 3,950 3,950 3ID 3,079 23,742 25,702 Action 950,950,950 93,950 93,950 950 93,950 93,950 950 93,950 93,93,950 93,950 93,93,950 3,950 3,950 3,950 3,93,950 3,950 3,950 3,950 3,950 3,950 3,93,93,950 3,950 3,950 3,950 3,950 3,950 3,950 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1 Days1, Brand ID 29,909,299,299,203 - 203 - 203 - 203 -"}, {"heading": "4. FEATURE DESIGN", "text": "In this section, we will present our feature design. Basically, for an instance pair (User ID, Brand ID), we build its features in three parts: Pair Features, User Features, and Brand Features. For each part, we will design 5 groups of features: Counter Features, Ratio Features, Flag Features, and Global Features. We will list each feature in a detailed feature list as follows, to which parts it will be applied."}, {"heading": "4.1 Counting Features", "text": "We use several types of counting functions as basic statistics on different granularities listed below. \u2022 Action counting, including the number of individual types of action and the sum of all types of action. This counting function is applied to pair, user and brand. \u2022 Action counting, including the daily count of each type of action. This is an aggregation of action counting at the daily level. This counting function is applied to pair, user and brand. \u2022 Valid click counting, which counts only the number of clicks after the last purchase. This counting function is applied only to pairs. \u2022 Differentiated action user or brand counting, which counts the number of different users or brands in each type of action. This counting function is applied to brand and user. \u2022 First-time action user or brand counting, which counts the number of initial actions by user or brand. This counting function is applied to brand and user."}, {"heading": "4.2 Ratio Features", "text": "We essentially build two types of ratios. \u2022 Conversion ratio, where the purchased number (number of days, number of days) is divided by the total number of actions. \u2022 Conversion ratio reflects the probability of buying after the click. This ratio is applied to pair, user and brand. \u2022 Count ratio, where the number of clicks or purchases is divided by a different type of click or purchase number. Count ratio is between counts of the same action type. For example, number of clicks / click day, number of purchases / unique purchaser, etc. This ratio is applied to user and brand. \u2022 Cross ratio, which is built between pair and user narrative features. For example, click number / number of clicks. Cross ratio indicates the user's preference for a particular brand in pairs."}, {"heading": "4.3 Flag Features", "text": "Rules can be simple but very useful as additional predictions that can be merged into model predictions. So a simple rule can be to predict pairs with more than 10 clicks. However, our offline experiments show that adding rule predictions directly to model output does not work. Therefore, we apply effective rules to binary flags and elegantly integrate flag characteristics into the learning model, which works better than directly adding rule predictions. Our flag characteristics include: \u2022 Whether that pair or that user or that brand already has a certain type of action or not. This flag characteristic is applied to pair, user and tag. \u2022 Whether that pair or that user has consecutive purchases in adjacent two months or not. This flag characteristic is applied to pair, user and tag."}, {"heading": "4.4 Global Features", "text": "Count, ratio and flag functions are all based on date groups, i.e. each attribute has a copy in each date group and the value of the function is determined by the scope of the date group. We also build global attributes that are not dependent on date groups and are calculated over the entire attribute range. Global attributes have more information than date-dependent attributes. They are more robust and can describe instances more precisely. We mainly use the following global function: \u2022 First and last active day, which is the distance between the first or last active day and the end of the attribute range. This global function is applied to pair and user. \u2022 Last purchase day, which is the distance between the last purchase day and the end of the attribute range. This global function is applied to pair and user. \u2022 Length of active time span, which is the number of days between the first and the last active day. This global function is applied to pair, user and tag. \u2022 Percentage of frequent users means that a user has purchased more than one brand."}, {"heading": "5. BLENDING AND ENSEMBLE", "text": "In this section we will first simply present our individual models and then the mix and ensemble approach for the aggregation of these models."}, {"heading": "5.1 Individual Models", "text": "We mainly use three types of individual models: regression, classification and global scoring. For regression, we use Gradient Boosting Regression Tree (GBRT) as a single model. For classification, we use Logistic Regression (LR) and Random Forest (RF) as individual models. For global evaluation, we use a time span that is applied to all three models, and moving time span is applied to GBRT and RF. The organizers of this competition ensure the distributed implementation of these classic learning models. For global evaluation, we use a time-lapse function to directly evaluate each instance: Score (userID, brandID) = \u2211 N \u2211 T \u03b1t \u00b7 I (t) \u00b7 day2 (n), (5) where N is the total number of instance actions, T the number of action types, \u03b1t the weight of action types t, I (t) is an indicator that shows whether the action is of type t, and Day (n) of the ninth index of the day."}, {"heading": "5.2 Overall Framework", "text": "For each model, we use a two-step approach to mixing models and ensembles. First, we use individual models to predict the training set and apply logistic regression (LR) to merge these models group by group. To avoid overfitting, we simply use a simple linear ensemble to the mixed models and get our final prediction model. The general framework is shown in Figure 5."}, {"heading": "5.3 Post Analysis", "text": "We include all hits in the local forecast and draw the daily distribution from it, as shown in Figure 6. This analysis shows that our model is good at predicting user purchases in the first 3 days. It is clear that the prediction softness increases significantly after the first 3 days. This is understandable as useful information becomes weaker, but the noise becomes stronger in the distant future."}, {"heading": "6. CONCLUSION", "text": "In this article, we present our solution for the Tmall Recommendation Prize 2014. We construct instances of training and prediction in a time-dependent manner and model the task of predicting purchases as a standard machine learning problem. Two types of time span construction serve to generate characteristics and objectives of instances. We mainly use three types of individual models: regression, classification and global assessment. We use a two-step approach to mixing models and ensembles that effectively improves prediction accuracy."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "This paper describes the solution of Bazinga Team for Tmall Recommendation Prize 2014. With real-world user action data provided by Tmall, one of the largest B2C online retail platforms in China, this competition requires to predict future user purchases on Tmall website. Predictions are judged on F1Score, which considers both precision and recall for fair evaluation. The data set provided by Tmall contains more than half billion action records from over ten million distinct users. Such massive data volume poses a big challenge, and drives competitors to write every single program in MapReduce fashion and run it on distributed cluster. We model the purchase prediction problem as standard machine learning problem, and mainly employ regression and classification methods as single models. Individual models are then aggregated in a two-stage approach, using linear regression for blending, and finally a linear ensemble of blended models. The competition is approaching the end but still in running during writing this paper. In the end, our team achieves F1Score 0.0611 and ranks 7th (out of 7,276 teams in total).", "creator": "LaTeX with hyperref package"}}}