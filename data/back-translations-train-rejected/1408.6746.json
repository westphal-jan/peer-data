{"id": "1408.6746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2014", "title": "Non-Standard Words as Features for Text Categorization", "abstract": "This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. Non-Standard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.", "histories": [["v1", "Thu, 28 Aug 2014 15:06:50 GMT  (673kb)", "http://arxiv.org/abs/1408.6746v1", "IEEE 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419"], ["v2", "Sun, 16 Nov 2014 21:33:22 GMT  (582kb)", "http://arxiv.org/abs/1408.6746v2", "IEEE 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419, 2014"]], "COMMENTS": "IEEE 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["slobodan beliga", "sanda martin\\v{c}i\\'c-ip\\v{s}i\\'c"], "accepted": false, "id": "1408.6746"}, "pdf": {"name": "1408.6746.pdf", "metadata": {"source": "CRF", "title": "Non-Standard Words as Features for Text Categorization", "authors": ["Slobodan Beliga", "Sanda Martin\u010di\u0107-Ip\u0161i\u0107"], "emails": ["smarti}@inf.uniri.hr"], "sections": [{"heading": null, "text": "It is not the way in which the individual words in each category have been grouped and shaped, but the way in which they are used in each category: official, literary, informative, popular, educational and scientific terms used in three different representations of the SIPEZ collection: in the first representation the frequencies of NSWs are used as characteristics; in the second representation the statistical measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as characteristics, while the third representation combines the first two characteristics. Naive Bayes, CN2, C4.5, kNN, classification Trees and Random Forest algorithms are used in text categorization."}, {"heading": "A. NSW extraction", "text": "Traditionally, normalization in the field of natural language processing (NLP) is the process of converting the entity into its normal form (i.e. lemmas). Normalization is also the first step in the text pre-processing of text-to-speech (TTS) systems. It is carried out in the normalization module, which is responsible for identifying a Commonwealth mark and transforming it into its extended form [5, 6]. Text normalization for Croatian language synthesis is described in [7]. In this work, the rules-based approach from the Croatian economy was used for NSW extraction [7, 8]. The normalization module first recognizes the whales and separates them from the standard words. Then, the recognized economy is transformed into its extended form, which is suitable for the TTS system. In this experiment, the process of transforming the economy into an extended form is not required. Instead, we determine each individual whale and determine its type, as it is then determined in its advanced form, which is proposed for each word in the taxonomy for the non-standard classification of the croatical language."}, {"heading": "B. Text categorization", "text": "Text categorization is charged with assigning one of the predefined classes to a document. Normally, text categorization is performed using a bag model borrowed from the retrieval of information (IR) and mostly based on the TF-IDF model [1-3]. Documents are presented as feature vectors that, according to the bag model, are a disordered list of words with the frequency of their occurrence in a document. Inverted document frequency is used to model the rarity of terms in a document collection. Characteristic vectors are used to train the classifier, usually the Bayesian classifier, the k-closest neighbor (kNN), classification tree or support vector machines (SVM), etc. [1, 3, 9] Some of the techniques for ector dimensionality or feature space reduction are: stopwords and the removal of lemmatization [10] or lemmatization [11]."}, {"heading": "A. Data (SKIPEZ)", "text": "For normalization and categorization purposes, we have prepared the text collection in Croatian - SKIPEZ (Slu\u017ebeno, Knji\u017eevno, Informativno, Popularno, Edukativno, Znanstveno). SKIPEZ comprises 390 texts organized into 6 predefined classes: official, literary, informative, popular, pedagogical and scientific. Text distribution across classes is balanced, so that each class contains exactly 65 texts. SKIPEZ texts were selected according to their relevance to the class and according to the percentage of NSWs included, because we needed a balance between both aspects in order to form a classifier. SKIPEZ statistics are presented in Table I, and further details about SKIPEZ are in [14]. The content of texts in each subclass has been carefully selected to include as many different and representative texts in each class and to contain a representative set of NSWs in order to compile the exhausted list of Croatian NSWs."}, {"heading": "B. NSW Extraction Procedure", "text": "The NSWs were extracted from the SKIPEZ text collections according to the approach presented in [7, 8] and nominal numbers. Here, we have identified only individual NSWs types according to this taxonomy, which are composed of feature vectors consisting of NSWs types. A feature vector for each text document in the collection contains 85 values (calculated from NSW types and special acronyms): STRING part - 15 values: ordinal and nominal roman numbers, simple or compound abbreviations without completion, chemical elements, measurement units, currency units, acronyms, inflected and special acronyms, symbols, emoticons and suffixes. NUMBER part - 21 values: numeric format date, period, time, proportion, dimensions, short and long phone numbers, positive and negative decimals, exponents, ordinals and nominal numbers."}, {"heading": "C. Categorization Results", "text": "We trained Naive Bayes, Classification Tree, kNN, CN2, C4.5, and Random Forest with the most varied classifications. As a measure of classification performance, we have achieved accuracy (1) and random texts (1), where a correct classification means that the learned model predicts the same class as the original class of test case. Results of the worn classification characteristics are shown in the first feature series with the first feature set of knowledge in Table III - with statistical characteristics in Table IV. The experimental results can be analyzed in two ways: by accuracy of the different classifications and by accuracy of the different types of representations in the collection -Random Forest."}], "references": [{"title": "The Text Mining Handbook - Advanced Approaches in Analyzing Unstructured Data", "author": ["R. Feldman", "J. Sanger"], "venue": "New York: Cambridge University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "An Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Mininh the Web - Discovering knowladge from hypertext data", "author": ["S. Chakrabarti"], "venue": "Morgan Kaufmann Publishers", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "A comparative study on Feature Selection in Text Categorisation", "author": ["Y. Yang", "J.O. Pedersen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Normalization of Non-standard Words", "author": ["R Sproat"], "venue": "Computer Speech & Language", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Lightly Supervised Learning of Text Normalization: Russian Number Names", "author": ["R. Sproat"], "venue": "IEEE Workshop on Spoken Language Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Text Normalization for Croatian Speech Synthesis", "author": ["S. Beliga", "S. Martin\u010di\u0107-Ip\u0161i\u0107"], "venue": "Proc. MIPRO junior - Student Papers, 382-387", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "S", "author": ["S. Beliga", "M. Pobar"], "venue": "Martin\u010di\u0107-Ip\u0161i\u0107, Normalization of Non- Standard Words in Croatian Texts,\u00ab u TSD 2011, Plsen", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Web Data Mining - Exploring Hyperlinks", "author": ["B. Liu"], "venue": "Contents, and Usage Data, Berlin: Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Text Categorization and Sorting of Web Search Results", "author": ["M Radovanovi\u0107", "M. Ivanovi\u0107", "Z. Budimac"], "venue": "Computing and Informatics, 28(6), pp. 861\u2013893", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Language morphology offset: Text classification on a Croatian\u2013English parallel corpus", "author": ["M. Malenica", "T. \u0160muc", "J. \u0160najder", "B. Dalbelo Ba\u0161i\u0107"], "venue": "Information Processing & Management, 44(1), 325-339", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Feature subset selection in text-learning", "author": ["D. Mladeni\u0107"], "venue": "Machine Learning ECML98.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Word co-occurrence features for text classification", "author": ["F. Figueiredo at al"], "venue": "Information Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Klasifikacija tekstova temeljem nestandardnih oblika rije\u010di u hrvatskome jeziku", "author": ["S. Beliga"], "venue": "Diplomski rad. Sveu\u010dili\u0161te u Rijeci - Odjel za informatiku", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Vjerojatnost i statistika I i II", "author": ["N. Sarapa"], "venue": "Zagreb: \u0160kolska knjiga", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "Primjenjena statistika", "author": ["I. \u0160o\u0161i\u0107"], "venue": "Zagreb: \u0160kolska knjiga", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In the domain of text categorization, the task is called text categorization (TC) \u2013 given a set of categories (subjects) and a collection of text documents, the process of finding the correct topics for each document [1].", "startOffset": 217, "endOffset": 220}, {"referenceID": 0, "context": "The problem with this approach is the dimensionality of the features vectors, which is equal to the number of different words in the collection [1, 2].", "startOffset": 144, "endOffset": 150}, {"referenceID": 1, "context": "The problem with this approach is the dimensionality of the features vectors, which is equal to the number of different words in the collection [1, 2].", "startOffset": 144, "endOffset": 150}, {"referenceID": 2, "context": "One of the solutions for the dimensionality problem is Feature Selection which can be: heuristic, guided by linguistic and domain knowledge, or statistical [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "Some feature selection approaches ignore terms that are too frequent or too rare according to empirically chosen thresholds [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Therefore, another solution is to use measures such as Information Gain and Chi-Squere, to define relevance of each feature [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 0, "context": "Using word groups, instead of individual words as features, with a high degree of semantic relatedness, proved to be justified in clustering [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "Systematic approach - Latent Semantic Indexing (LSI) is based on this principle [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "The motivation for this work rises from an idea that discarded characters and numbers are sufficient for discrimination of classification of the text categories, reducing the feature vector dimensionality at the same time [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "It is performed in the normalization module, which is responsible to identify a NSW token and to transform it into its expanded form [5, 6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "It is performed in the normalization module, which is responsible to identify a NSW token and to transform it into its expanded form [5, 6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 6, "context": "Text Normalization for Croatian Speech Synthesis is described in [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "In this work, the Croatian NSW taxonomy driven rule based approach was used for NSWs extraction [7, 8].", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "In this work, the Croatian NSW taxonomy driven rule based approach was used for NSWs extraction [7, 8].", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "Instead, we detect each NSW and determine its NSW type as is suggested in taxonomy for classification of nonstandard words in Croatian language [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].", "startOffset": 147, "endOffset": 152}, {"referenceID": 1, "context": "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].", "startOffset": 147, "endOffset": 152}, {"referenceID": 2, "context": "Usually text categorization is performed using a bag-of-words model borrowed from information retrieval (IR), and most often based on TF-IDF model [1-3].", "startOffset": 147, "endOffset": 152}, {"referenceID": 0, "context": "[1, 3, 9].", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[1, 3, 9].", "startOffset": 0, "endOffset": 9}, {"referenceID": 8, "context": "[1, 3, 9].", "startOffset": 0, "endOffset": 9}, {"referenceID": 9, "context": "Some of the techniques for the vector dimensionality or the feature space reduction are: stop words and NSW removal [10], lemmatization or morphological normalization [11] - NLP based principles; selecting the most discriminative subset of features (document frequency, information gain, mutual information, odds ratio, etc.", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "Some of the techniques for the vector dimensionality or the feature space reduction are: stop words and NSW removal [10], lemmatization or morphological normalization [11] - NLP based principles; selecting the most discriminative subset of features (document frequency, information gain, mutual information, odds ratio, etc.", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": ") \u2013 called feature selection [4, 12] - or deriving new smaller set calculated from original features \u2013 called feature extraction [13].", "startOffset": 29, "endOffset": 36}, {"referenceID": 11, "context": ") \u2013 called feature selection [4, 12] - or deriving new smaller set calculated from original features \u2013 called feature extraction [13].", "startOffset": 29, "endOffset": 36}, {"referenceID": 12, "context": ") \u2013 called feature selection [4, 12] - or deriving new smaller set calculated from original features \u2013 called feature extraction [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": ", and additional details about SKIPEZ are in [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "NSW Extraction Procedure The NSWs were extracted from the SKIPEZ text collection according to the approach presented in [7, 8].", "startOffset": 120, "endOffset": 126}, {"referenceID": 7, "context": "NSW Extraction Procedure The NSWs were extracted from the SKIPEZ text collection according to the approach presented in [7, 8].", "startOffset": 120, "endOffset": 126}, {"referenceID": 14, "context": "For each element of the population, therefore each vector, we calculate some of its numerical characteristics called statistical features [15, 16].", "startOffset": 138, "endOffset": 146}, {"referenceID": 15, "context": "For each element of the population, therefore each vector, we calculate some of its numerical characteristics called statistical features [15, 16].", "startOffset": 138, "endOffset": 146}, {"referenceID": 8, "context": "As the measure of classification performance we used accuracy (1) [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Chakrabarti, Yang and Pedersen in [4] state that numerous techniques of selection of features improve the performance of the classifier only marginally.", "startOffset": 34, "endOffset": 37}], "year": 2014, "abstractText": "This paper presents categorization of Croatian texts using Non-Standard Words (NSW) as features. NonStandard Words are: numbers, dates, acronyms, abbreviations, currency, etc. NSWs in Croatian language are determined according to Croatian NSW taxonomy. For the purpose of this research, 390 text documents were collected and formed the SKIPEZ collection with 6 classes: official, literary, informative, popular, educational and scientific. Text categorization experiment was conducted on three different representations of the SKIPEZ collection: in the first representation, the frequencies of NSWs are used as features; in the second representation, the statistic measures of NSWs (variance, coefficient of variation, standard deviation, etc.) are used as features; while the third representation combines the first two feature sets. Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms were used in text categorization experiments. The best categorization results are achieved using the first feature set (NSW frequencies) with the categorization accuracy of 87%. This suggests that the NSWs should be considered as features in highly inflectional languages, such as Croatian. NSW based features reduce the dimensionality of the feature space without standard lemmatization procedures, and therefore the bag-of-NSWs should be considered for further Croatian texts categorization experiments.", "creator": "Microsoft\u00ae Office Word 2007"}}}