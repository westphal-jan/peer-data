{"id": "1408.6988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2014", "title": "An Information Retrieval Approach to Short Text Conversation", "abstract": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \"intelligently\", when combined with a huge repository of conversation data from social media.", "histories": [["v1", "Fri, 29 Aug 2014 12:04:15 GMT  (524kb,D)", "http://arxiv.org/abs/1408.6988v1", "21 pages, 4 figures"]], "COMMENTS": "21 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["zongcheng ji", "zhengdong lu", "hang li"], "accepted": false, "id": "1408.6988"}, "pdf": {"name": "1408.6988.pdf", "metadata": {"source": "CRF", "title": "An Information Retrieval Approach to Short Text Conversation", "authors": ["Zongcheng Jia", "Zhengdong Lub", "Hang Lib"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we address one of its most important sub-problems, the so-called short-text conversation, in which the computer gives a reasonable response to a message from a human being. We use the enormous amount of short-text data available on social media to investigate the problem. We suggest formalizing short-text conversations as a search problem in the first step and using state-of-the-art information technology to perform the task. We examine both the meaning and limitations of the IR approach. Our experiments show that the on-demand model, combined with a huge repository of conversation data from social media, makes the system behave more \"intelligently.\" Keywords: short-text conversation, information repetition, learning to rank, learning to match."}, {"heading": "1. Introduction", "text": "In recent years, the number of people able to remain within the EU has multiplied, both in the EU and in the US. (...) In recent years, the number of people living in the EU has doubled significantly. (...) In the last ten years, the number of people living in the EU has doubled. (...) In the EU, in Europe, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in Europe, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Short Text Conversation", "text": "The early work on modeling dialogues is either rules-based (Wheat Tree, 1966) or learning-based (Litman et al., 2006; Williams and Young, 2007).These approaches do not require data (e.g., rules-based) or little data (e.g., amplification of answers) for training, but a lot of manual effort in building the model, which is usually very costly. Furthermore, the coverage of the systems is unsatisfactory. An alternative approach is to build a dialogue system with a large number of question-answer pairs. Thus, the system in Leuski et al. (2006) and Leuski and the coverage of the systems cannot be satisfied."}, {"heading": "2.2. Search", "text": "Ranking learning and semantic matching are considered to be the most advanced search techniques (Liu, 2009; Li, 2011a, b; Li and Xu, 2014). In a query, documents containing the search terms are first retrieved from the index. Matching between the query and each of the documents is then performed using different models such as the traditional IR model of BM25 (Robertson et al., 1995), the translation model (Berger and Lafferty, 1999) and the latent spatial model (Wu et al., 2013). Matching scores of each document are used as characteristics of the document. Next, the documents are evaluated by the ranking model based on their characteristics. Finally, the documents are sorted according to their rankings. The ranking model is trained in advance based on learning the rank, and the appropriate models are trained in advance using semantic matching techniques (or in general learning-to-match). In this essay, we use the IR Techniques for short texts."}, {"heading": "3. Conversation on Social Media", "text": "Weibo is a microblog service in China, similar to Twitter, where a user can post a short message (referred to as a post in the rest of the article) that is visible to the public or a group of users who follow it. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on each post. Users can attach a short message to a published post, with the same length limit that is referred to as a comment in this essay. Figure 1 shows an example of post and related comments (in Chinese). We argue that the post-comment pairs on Weibo can be a fairly valuable resource for studying short text conversations between users. Comments on a post can have flexible shapes and different topics, as illustrated in Example Table 1. Since a post is a report on the current status of the user (traveling to Hawaii), the comments can be a question about the future status of the user, a request to the user, a greeting to the user, and further text, and so the post appears to be a pair of them, but they are all appropriate in many cases."}, {"heading": "4. Retrieval-based Short Text Conversation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Problem Definition", "text": "Short Text Conversations (STC) are defined as a conversation about two short texts, the former being a human message and the latter a response to the message given by the computer. In the first step, we formalize STC as an information retrieval problem (IR), i.e. we perform a call-based STC. Faced with a message (query), the system retrieves related answers from the large repository of conversation data and returns the most reasonable response. Using advanced IR technologies and a data set with previously unimaginable volume, we would expect the call system to behave almost like a human in each conversation. Formally, for a given query, we select q from the repository of post-comment pairs (p, r) the answer r with the highest placed Score.r = arg max (p, r) Score (q, (p, r) Score (1), where the score is a combination of individual matching comment pairs (q, r) of the highest placed response (p), r with the max placed (p), p (p)."}, {"heading": "4.2. System Architecture", "text": "The system performs on-demand short text conversations in three phases, as illustrated in Figure 2: \u2022 Level I (on-demand), the system uses three fast linear matching models (see Section 6.1) to retrieve a number of candidate-post-comment pairs for the specified query q and to form a reduced candidate set C (reduced) q. \u2022 Level II (on-demand), the system uses more matching models (see Section 6.2-6.5) to further evaluate all comments in C (reduced) q and provides a matching feature set {\u03a6i (q, (p, r), i, i, i, for each candidate comment pair. The matching models are learned offline using techniques known as learning-to-match (see Section 6 for details). \u2022 Level III (ranking), the system uses a linear ranking function defined in Equation (2), with the matching models serving as attributes to further evaluate all the responses (q) and a reduction in the responses (q)."}, {"heading": "4.3. Learning of Ranking Model", "text": "We use linear RankingSVM (Herbrich et al., 1999), a state-of-the-art method of learning ranking to train the ranking model. As training data, we use post-comment pairs, as explained in Section 5.2. From the highlighted data, we derive pairs of preference data (q, (p, r) +, (p, r) \u2212), so that the score (q, (p, r) +) > score (q, (p, r) \u2212) is achieved. Specifically, (p, r) + is selected from the marked positive instances in relation to q, while (p, r) \u2212 is selected from the marked negative instances. We have confirmed that the use of marked negative instances instead of randomly selected instances can produce slightly better results."}, {"heading": "5. Short Text Conversation Dataset", "text": "This section presents the data set used for on-demand STC.5. There are (1) original post-comment pairs used as the on-demand repository, and (2) marked post-comment pairs used for training and testing different on-demand models."}, {"heading": "5.1. Original Post-Comment Pairs", "text": "The data set creation process, as illustrated in Figure 3, consists of three successive steps: (1) searching the user community, (2) searching their posts and related comments, and (3) cleaning up the data with further details described below. 4This is because the negative cases are collected by the top-ranked candidates using several simple retrieval models, and therefore tend to indicate the difference between positive and negative instances. 5We have received permission from Weibo to release the data set for research, and it is available at http: / / data.noahlab.com.hk / conversation /."}, {"heading": "5.1.1. Sampling Strategy", "text": "To make the topic relatively focused, we are using the following sampling strategy for collecting the post-comment pairs. We are initially looking for 3,200 users from a loosely connected community of Natural Language Processing (NLP) and Machine Learning (ML) in China. The community consists mainly of professors, researchers and students. This is done by crawling through followers 6 out of 10 manually selected seed users who are NLP researchers active on Weibo (with an average of no less than 2 posts per day) and popular enough (with no less than 100 followers). We trawl through the posts and related comments (not necessarily from the crawled community) for two months (from April 5, 2013 to June 5, 2013). Topics are relatively limited due to our choice of users, with the most important being: \u2022 Research: Discussions on research ideas, essays, books, tutorials, conferences and researchers in NLP and ML, etc. (physics, physics and physics, and both)."}, {"heading": "5.1.2. Processing, Filtering, and Data Cleaning", "text": "For the scrawled posts and comments, we first perform a four-step filtering of the posts and comments: \u2022 We first remove a post-comment pair if the length of the post is less than 10 Chinese characters or the length of the comment is less than 5 Chinese characters, for two reasons: (1) If the text is too short, it can contain little information that can be reliably captured, e.g. the following example post, two after go.and (2) some of the posts or comments are too general to be of interest in other cases, e.g. the comment in the example below post, which you cannot read. (3 down, two after go.and) Nice restaurant. I would highly recommend it. Everything here is good, except the long waiting line. Comment: \"Wow.6If user A follows user B, A is referred to as B's successor, and B as A's successor."}, {"heading": "5.2. Labeled Post-Comment Pairs", "text": "This year it is more than ever before."}, {"heading": "6. Matching Features", "text": "In this section, we present matching features (calculated from matching models) used in our on-demand STC. First, we introduce three basic linear matching models as baselines, then a translation-based language model (TransLM) to alleviate the lexical gap problem. Next, we describe a deep matching model (DeepMatch) that aligns query and response (Comment) with a deep architecture, and then we explain a topic-word model (TopicWord) to address the thematic orientation of query and response (Comment), and finally, we describe other simple matching features used in our on-demand STC."}, {"heading": "6.1. Basic Linear Matching Models", "text": "We use the following three basic linear matching models for quick search in Level I. In addition, these matching models are also used in Level II to generate three matching characteristics for each post-comment pair."}, {"heading": "6.1.1. Query-Response Similarity", "text": "Here we use a simple vector space model to measure the similarity between a query q and a candidate response rsimQ2R (q, r) = qTr-q-r (3), where q and r each have the TF-IDF vectors of q and r. While it is not necessarily true that a good answer has many common words as a query, this measurement is often helpful to find relevant answers. For example, if the query and candidate response both have the \"National Palace Museum in Taipei,\" this is a strong signal that they are related topics. Unlike other semantic matching characteristics, this simple similarity does not require learning and works on rare words. Our empirical results show that it can often capture the query-response relationship that semantic matching characteristics cannot capture."}, {"heading": "6.1.2. Query-Post Similarity", "text": "The basic idea here is to find posts (messages) that are similar to query q and to use their comments (answers) as candidates. Again, we use the vector space model to measure query-post similarity with Q2P (q, p) = qTp, q, q, p (4), where q and p are the TF-IDF vectors of q and p respectively. The assumption here is that if a post p is similar to query q, the associated comments (answers) may be suitable for q. However, it often fails, especially if an answer to p addresses parts of p that are not included in q, which fortunately can be mitigated by combining with other measures."}, {"heading": "6.1.3. Query-Response Matching in Latent Space", "text": "This special matching function is based on mapping posts and answers in the original vector spaces to a low-dimensional latent space obtained from data. The matching score between a query q and a candidate answer r can be measured as an internal product between their images in latent space. LatentMatch (q, r) = qTLqLTr r (5) This serves to capture the semantic matching between a post and a response that may not be well captured by word-to-word matching. We find the mapping functions Lq and Lr by a large number of query response pairs and a large margin variant of the method in Wu et al. (2013).arg min Lq, Lr i max (1 \u2212 qTi LqL T r ri, 0) s.t. Ln, q 1 \u2264 \u00b51, n = 1, 2,... sepl. \""}, {"heading": "6.2. Translation-based Language Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1. Motivation", "text": "With the three basic matching models and some simple features (see Section 6.5), the on-demand STC model performs relatively well (see Section 7.2), but there are also some instances where the model fails. One of the most serious problems is the lexical gap between the query and the candidates for comments. Table 3 shows a real example of the lexical gap problem. Two candidates are suitable for the query, while their ranking among the top 30 candidates is very low. The main reason for this is that there is no overlap between the candidates \"answers and the query, although there is a common word\" good night \"between the original posts and the query."}, {"heading": "6.2.2. Model Description", "text": "To mitigate the lexical gap problem, we use the state-of-the-art translation-based language model (TransLM) (Xue et al., 2008) with a small modification for retrievable STC reasons. In the face of a pair of answers to question q and a pair of answers to the candidate question (p, r), the ranking function based on TransLM is written as Pmx (w, r) + answer q PTransLM (w, r) (7) PTransLM (w, r)) = (1 \u2212 \u03b1) Pmx (w, r) + Pml (w, r) = trix (w, r) Pmx (w, r) = (p, r) = (p, r) = (p, r) = (p, r) (mp) (mp) (px) (p) (px) (p), p) (px) (px) (p), p) (px) (p), p) (px) (p), p) (p), p), p), p), p (mx) (mp), p), p (p), p (p), p), p (px), p), p (p), p), p (p), p), p (px), p), p), p (p), p), p (p), p), p), p (p), p), p), p (p), p), p), p (mx), p), p), p (p), p), p), p), p (mx), p), p), p), p), p (p), p), p), p (p), p), p), p), p (p), p (mx), p), p (p), p), p), p), p (p), p), p), p (mx), p), p), p (p), p), p), p (p), p), p), p), p ("}, {"heading": "6.2.3. Learning Translation Probabilities", "text": "The performance of the translation-based language model is based on the quality of the word-to-word translation probabilities. We follow the method of Xue et al. (2008) to learn the translation probabilities. In our experiments, original post-comment pairs are used for the training, and the toolkit GIZA + + 8 (Och and Ney, 2003) is used to learn the translation model. There are two different settings of source and target for the construction of the parallel corpus: (1) set the contributions as source and the answers as target, i.e. capture (p, r) 1,..., (r, p) n. According to Xue et al. (2008), a pooling strategy is applied to combine the above two surveys of pairs to form a pooled collection (p, r) 1,..., (r, p) n."}, {"heading": "6.3. Deep Matching Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3.1. Motivation", "text": "The above matching models are linear models. Although these models have proven effective, they are not sufficient to capture the complex matching relationships between query and candidate responses in query models such as texts. We propose to use a new deep neural network model, referred to in the essay as DeepMatch, to model the intricate matching relationships between query and candidate responses in call-based STC.9. This new architecture is essentially based on the following two intuitions: \u2022 Localization: There are outstanding local structures in the semantic space of parallel texts that can be roughly captured by the coexistence pattern of words. However, this locality should not prevent two \"remote\" components from correlate at a higher level and therefore require the hierarchical characteristics of the model; \u2022 Hierarchy: Decision-making for matching has different levels of abstraction. Local decisions that are later combined between global interactions for intersecting and related words."}, {"heading": "6.3.2. Model Description", "text": "The deep matching model (DeepMatch) consists of two parts: (1) many bilinear local matching models and (2) a deep neural network to further combine the local matching models to generate the final matching score. Each local matching model (indexed k) is responsible for a small subset of words in short texts (x and y) and a pair of projection matrices (L (k) x, L (k) y).The score from the kth matching model is given as follows: (k) (x, y) = f (k) ((((x (k)) > L (k) y (k) + b (k)))), k = 1, \u00b7 \u00b7, K (10) 8https: / / code.google.com / p / giza-pp / 9The details can be found in Lu and Li (2013)."}, {"heading": "6.3.3. Model Training", "text": "The formation of DeepMatch is divided into two phases: (1) bilingual topic modeling to find potentially matching subsets (topics) of word pairs and model architecture, and (2) training in the parameters of local topic models and deep neural networks. In this thesis, we train the deep matching model with a ranking-based target. Specifically, we use a large margin target defined by preference pairs in the ranking. Suppose we get the following triples (x, y +, y \u2212) from the oracle, where X matches y + better than Y. The ranking-based target is defined as: L (W, Dtrn) = \u2211 (xi, y + i, y \u2212 i). DtrneW (xi, y \u2212 i), y \u2212 i), DtrneW (y \u2212 i), y \u2212 i, y \u2212 i, y \u2212 i, y \u2212 i, y \u2212 margin, \u2212 we follow the margin (xi), \u2212 xi \u2212 xi, \u2212 xi \u2212 \u2212 we."}, {"heading": "6.4. Topic-Word Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.1. Motivation", "text": "The above matching models are mainly used to show the semantic relevance between the query and the answer, and do not capture the appropriate relationships between the main topics of the query and the answer. Table 4 shows a real example of the problem. The answers of the two best candidates are unsuitable for the query, while the first suitable answer is in 9th place. The main reason for this is that the word \"beginner\" has higher terminology frequencies in the query and the unsuitable answers, and thus dominates their matching results. However, the main topics of the query are not \"beginners,\" but \"beginners,\" and therefore the answers of the two best candidates are not suitable. A possible solution is to identify the topic words of the query and the answers of the candidates and to give more weight to the assignment of the topic words. This could alleviate the problem of topic mismatch and improve the performance of the STC. Table 4 shows the topic words of the query and the candidates \"answers in bold. Once the topic has been identified, the answer to the first question will clearly refer to the other words for the answer."}, {"heading": "6.4.2. Learning Topic Words", "text": "A short text, such as a post or comment on Weibo, usually centers on a specific topic, which is usually captured by a number of words in the text. We refer to the words as subject words. Examples are in Table 4. In this paper, we use a probabilistic approach based on logistic regression to calculate the probability of a word being the subject of a short text. In the logistic regression model, we calculate P (subject | w) as follows, where w denotes a word and a topic a binary variable that represents whether it is a topic or not. Log (subject | w) 1 \u2212 P (subject | w) = ~ x + c (subject) P (subject | w) as follows, a topic that denotes a word and a topic."}, {"heading": "6.4.3. Model Description", "text": "With the trained model, we can assign a probability value to each word in the short text that indicates the probability that the word is a topic word. The question is how we use this information in call-based STCs. In this thesis, we simply take the probability values as term weights and use them to calculate the similarities between a query q and a candidate response r or its original post p with a vector space model that are similar to the query-response similarity and query-post similarity introduced in Section 6.1.simQ2R, with q, p, and r each being the vectors of q, p, and r with the topic P (topic | w) as weight."}, {"heading": "6.5. Other Simple Matching Features", "text": "We also use some other simple match characteristics as follows: \u2022 Longest Common String (LCS): This feature measures the length of the longest common string between query q and candidate response r, which is useful for capturing quotes common in microblog comments, and is also relatively robust against errors in Chinese word segmentation. LCS Q2R (q, r) = | LCS (q, r) | (16) \u2022 Side entry characteristics: These characteristics represent the size, frequency, sum and average of the IDF values of the contiguous words between query q and candidate response r or their original posteriority size (x, y) = | (17) Frequency (x, y) = | | (18) Frequency (x, y), frequency (x, y), frequency (x, y), frequency (x, c) and frequency (x)."}, {"heading": "7. Experiments", "text": "With the data set we have created, we conduct experiments and report on the results of call-based STCs."}, {"heading": "7.1. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1. Evaluation Metrics", "text": "We evaluate the performance of different STC retrieval models using the following two metrics: Mean Average Precision (MAP) and Precision @ 1 (P @ 1). MAP rewards methods that provide appropriate answers upwards and also rewards methods that provide correct ranking of responses. P @ 1 indicates the proportion of appropriate answers among the answers collected upwards of 1. All results given below are based on a 5-fold cross-validation of the 422 queries. We also perform a significance test using a paired t-test with a significant grade of 05. We measure the performance of the logistics regression classifier for learning topic words with regard to accuracy based on a 5-fold cross-validation of the words highlighted in 2008."}, {"heading": "7.1.2. Parameter Settings", "text": "In our experiments, five parameters have to be defined: we match the best parameters with a 5-fold cross-validation. Finally, we set \u03b1 = 0.8 as Jelinek-Mercer smoothing factor in Equation (8), \u03b2 = 0.9 to interpolate the postal model and the response model in Equation (9), \u03b3 = 0.5 to interpolate the Unigram language model and the translation model in Equation (9), we set c = 0 in Equation (12). If we train the weighting of the features in Equation (2) with linear ranking SVM (Herbrich et al., 1999), we use a fixed penalty parameter (i.e. 50), because the performance is relatively insensitive to the choice of the parameter."}, {"heading": "7.2. Results of Basic Linear Matching Models", "text": "First, we evaluate the performance of the three basic linear matching models combined with the simple matching features in learning to classify the framework using Equation (2), the results of which are presented in Table 8. In the table, Q2R stands for the features based on the similarity between query and answer (Section 6.1.1) and the simple matching features based on query and answer (Section 6.5). LatentMatch stands for the latent matching feature introduced in Section 6.1.3. As shown in the table, combining the three basic linear matching models with all the simple matching features results in relatively good performance (row 4) and we refer to this model as the baseline used in the following subsections."}, {"heading": "7.3. Results of Combining all the Features", "text": "Then we integrate TransLM, DeepMatch and TopicWord as matching features into the Learning-to-Rank framework with Equation (2). Table 9 shows the comparison of different combinations of matching features for call-based STC. Baseline stands for the model that combines the three basic linear matching models with all the simple matching features based on the Learning-to-Rank framework (see row 4 in Table 8). The table shows that the three new matching features can significantly improve call performance. If we combine all three new features with baseline, the model performs best, outperforming baseline by 3.3 percent and 6.3 percent with respect to MAP and P @ 1 respectively (line 12 vs. line 5). To clearly see the contributions of the three new features, we make comparisons between the model with and without the individual features. Tables 10, 11 and 12 show the contributions of TransLicMatch TopicWord and TopicWord."}, {"heading": "X 0.621 0.628 0.635 0.641", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "X 0.574 0.587 0.586 0.606", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8. Case Study", "text": "In order to better understand the effectiveness of the matching characteristics, we conduct case studies on the characteristics. We illustrate the results using several examples. Section 8.1 shows the effectiveness of the basic linear matching characteristics. Section 8.2, 8.3 and 8.4 show the effectiveness of using the translation-based language model, deep matching model and theme-word model. Section 8.5 gives some examples that are not handled well with our current model and that we will leave to future work."}, {"heading": "8.1. The Effectiveness of Basic Linear Matching", "text": "The basic linear matching characteristics are mostly vector-based and are quite suitable for grasping semantic relevance, as in Table 13. Matching responses are retrieved mainly because they have words in common with the queries. Experiments also show that we can find interesting and suitable answers that do not have words in common with the query, as the example in Table 14 shows."}, {"heading": "8.2. The Effectiveness of Translation-based Language Model", "text": "The experimental results show that TransLM performs better when used as a feature in the Learning to Rank Framework. Candidate-post-comment pairs that do not share many common words with the query tend to be rated low by the other matching models. However, the translation-based model is able to fill the lexical gap and find and rank the lexically dissimilar but semantically similar post-comment pairs high. Table 15 gives some determined post-comment pairs for a particular query. We can see that the model with TransLM as one of the features can rank the appropriate answers higher than those without TransLM. This is due to the word translation probabilities: T (non-comment pairs for a particular query) = 0.018, T (non-interaction) = 0.01."}, {"heading": "8.3. The Effectiveness of Deep Matching Model", "text": "Table 16 shows some retrieved answers to a given query. The table shows that although the two suitable answers share almost no common words with the query, the model with DeepMatch as one of the features matches them well and can rank them higher than the model without the feature."}, {"heading": "8.4. The Effectiveness of Topic-Word Model", "text": "It is clear from the table that when using TopicWord, the inappropriate answers that do not share topic words with the query are rated lower as one of the characteristics, while the appropriate answer that shares topic words with the query is rated higher as one of the characteristics when using TopicWord. Specifically, the word \"newcomer\" is not a topic word in the query, so it has a low term weight (i.e. the likelihood of being a topic word). Although the word \"newcomer\" is a topic word with a high term weight in the inappropriate answers, the cosmic similarities between the query and the two inappropriate answers are still not high after the use of term weighting of topic words. Furthermore, the appropriate answer is rated higher primarily because it has common topic words \"control,\" \"tool\" with the query that are assigned higher weights."}, {"heading": "8.5. Some Failed Issues", "text": "In this section we will show some problems that cannot be solved well with our current model and leave them to future work."}, {"heading": "8.5.1. Entity Association", "text": "The entity association is only partially addressed by features such as the cosinal similarity of query and answer, where entity names are treated as words, which does not seem to be enough to prevent the following type of error (see Table 18), if the candidate's answer and the query match well elsewhere. In fact, for query 1 in the unsuitable answer 1, we only need to modify the word \"Hrsg. Li\" in the unsuitable answer 1 to the word \"Hrsg. Wang\" to get the expected answer 1 that is suitable for query 1. For query 2, we only need to modify the word \"drawing\" in the unsuitable answer 2 to the word \"Hrsg. (China)\" to get the expected answer 2 that is suitable for query 2."}, {"heading": "8.5.2. Logic Consistency", "text": "Our current model does not directly maintain the logical consistency between the candidate's answer and the query, since the logical consistency requires a deeper analysis of the texts and is therefore difficult to implement. Table 19 shows two examples that are semantically relevant and correct in terms of the speech act, but logically inappropriate."}, {"heading": "9. Conclusions", "text": "Our experiments show that the call-based model works relatively well when combined with a set of carefully designed matching features and a huge repository of call data. This work opens up several interesting directions for future work on STC. When performing the call-based STC, we must consider matching query and response in terms of semantic relevance. In addition, we may also need to consider matching query and response in terms of speech behavior, mood, entity association, logical consistency, and discourse structure. How these factors can be modelled, and how accuracy based on the factors in the STC can be improved, are open and challenging questions."}], "references": [{"title": "Information retrieval as statistical translation", "author": ["A. Berger", "J. Lafferty"], "venue": "Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201999. ACM,", "citeRegEx": "Berger and Lafferty,? \\Q1999\\E", "shortCiteRegEx": "Berger and Lafferty", "year": 1999}, {"title": "Evaluating conversational characters created through question generation", "author": ["G. Chen", "E. Tosch", "R. Artstein", "A. Leuski", "D.R. Traum"], "venue": "FLAIRS Conference", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E.", "Chang", "K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J.", "Jun."], "venue": "Journal of Machine Learning Research 9, 1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Herbrich et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 1999}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["S. Jafarpour", "C.J. Burges", "A. Ritter"], "venue": "Advances in Ranking,", "citeRegEx": "Jafarpour et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. ACL \u201907", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Building effective question answering characters", "author": ["A. Leuski", "R. Patel", "D. Traum", "B. Kennedy"], "venue": "Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue. SigDIAL \u201906. Association for Computational Linguistics,", "citeRegEx": "Leuski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Leuski et al\\.", "year": 2006}, {"title": "Npceditor: Creating virtual human dialogue using information retrieval techniques", "author": ["A. Leuski", "D. Traum"], "venue": "AI Magazine", "citeRegEx": "Leuski and Traum,? \\Q2011\\E", "shortCiteRegEx": "Leuski and Traum", "year": 2011}, {"title": "A short introduction to learning to rank", "author": ["H. Li"], "venue": "IEICE Transactions on Information and Systems", "citeRegEx": "Li,? \\Q2011\\E", "shortCiteRegEx": "Li", "year": 2011}, {"title": "Learning to rank for information retrieval. Foundations and Trends in Information Retrieval", "author": ["PA Stroudsburg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Stroudsburg,? \\Q2009\\E", "shortCiteRegEx": "Stroudsburg", "year": 2009}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney", "Mar"], "venue": "Computational Linguistics", "citeRegEx": "Och et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Och et al\\.", "year": 2011}, {"title": "Data-driven response generation in social media", "author": ["A. 311\u2013318. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "Proceedings of the Conference on Empirical", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A vector space model for automatic indexing", "author": ["G. 109\u2013109. Salton", "A. Wong", "C.S. Yang", "Nov"], "venue": "Commun. ACM", "citeRegEx": "Salton et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1975}, {"title": "The philosophy of information retrieval evaluation. In: Evaluation of cross-language information retrieval", "author": ["E.M. Voorhees"], "venue": "The Knowledge Engineering Review", "citeRegEx": "Voorhees,? \\Q2002\\E", "shortCiteRegEx": "Voorhees", "year": 2002}, {"title": "Eliza - a computer program for the study of natural language communication between man and machine", "author": ["J. Weizenbaum", "Jan"], "venue": "Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Weizenbaum and Jan.,? \\Q1966\\E", "shortCiteRegEx": "Weizenbaum and Jan.", "year": 1966}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["J.D. Williams", "S. Young"], "venue": "Computer Speech & Language", "citeRegEx": "Williams and Young,? \\Q2007\\E", "shortCiteRegEx": "Williams and Young", "year": 2007}, {"title": "Learning bilinear model for matching queries and documents", "author": ["W. 393\u2013422. Wu", "Z. Lu", "H. Li", "Jan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Retrieval models for question and answer archives", "author": ["X. 2519\u20132548. Xue", "J. Jeon", "W.B. Croft"], "venue": "Proceedings of the 31st Annual International ACM", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201908. ACM,", "citeRegEx": "Zhai and Lafferty,? \\Q2001\\E", "shortCiteRegEx": "Zhai and Lafferty", "year": 2001}, {"title": "Hhmm-based chinese lexical analyzer ictclas", "author": ["Zhang", "H.-P", "Yu", "H.-K", "Xiong", "D.-Y", "Q. Liu"], "venue": "Proceedings of the Second SIGHAN", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "One of the major reasons for that is lack of large volumes of real conversation data (Chen et al., 2011; Nouri et al., 2011).", "startOffset": 85, "endOffset": 124}, {"referenceID": 1, "context": "One of the major reasons for that is lack of large volumes of real conversation data (Chen et al., 2011; Nouri et al., 2011). In this paper, we consider a much simplified version of the problem: one round of conversation formed by two short texts, with the former being a message from human and the latter being a response to the message from the computer. We refer to it as short text conversation (STC). Thanks to the extremely large amount of short text conversation data available on social media such as Twitter1 and Weibo2, we anticipate that significant progress could be made in the research on the problem with the use of the big data, much like what has happened in machine translation, community question answering, etc. Modeling a short text conversation is much simpler than modeling a complete dialogue, which often requires several rounds of interactions (e.g., a dialogue system as in Litman et al. (2000)).", "startOffset": 86, "endOffset": 922}, {"referenceID": 15, "context": "Short Text Conversation Early work on modeling dialogues is either rule-based (Weizenbaum, 1966) or learning-based (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007).", "startOffset": 115, "endOffset": 187}, {"referenceID": 5, "context": "For example, the system in Leuski et al. (2006) and Leuski and Traum (2011) selects the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": "For example, the system in Leuski et al. (2006) and Leuski and Traum (2011) selects the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval.", "startOffset": 27, "endOffset": 76}, {"referenceID": 1, "context": "Instead of building knowledge base by hand, Chen et al. (2011) and Nouri et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 1, "context": "Instead of building knowledge base by hand, Chen et al. (2011) and Nouri et al. (2011) propose augmenting a knowledge base with question-answer pairs derived from texts using a question generation tool.", "startOffset": 44, "endOffset": 87}, {"referenceID": 5, "context": "The results show that phrase-based SMT (Koehn et al., 2007) works better than vector space model (VSM) (Salton et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 12, "context": ", 2007) works better than vector space model (VSM) (Salton et al., 1975) in IR in terms of BLEU score (Papineni et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 9, "context": "Ritter et al. (2011) investigate the feasibility of conducting short text conversation by using statistical machine translation (SMT) techniques, as well as millions of naturally occurring conversation data in Twitter.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "A related but slightly different problem has been studied in Jafarpour et al. (2010), as an initial step for building a chatbot, referred to as learning to chat (L2C).", "startOffset": 61, "endOffset": 85}, {"referenceID": 0, "context": ", 1995), translation model (Berger and Lafferty, 1999), and latent space model (Wu et al.", "startOffset": 27, "endOffset": 54}, {"referenceID": 16, "context": ", 1995), translation model (Berger and Lafferty, 1999), and latent space model (Wu et al., 2013).", "startOffset": 79, "endOffset": 96}, {"referenceID": 3, "context": "Learning of Ranking Model We employ linear RankingSVM (Herbrich et al., 1999), a state-of-the-art method of learning to rank, to train the ranking model.", "startOffset": 54, "endOffset": 77}, {"referenceID": 19, "context": "Then, for the remained posts and comments, we remove punctuation marks and emotions, and use ICTCLAS (Zhang et al., 2003) for Chinese word segmentation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "We employ a pooling strategy widely used in information retrieval for getting the instances to label (Voorhees, 2002).", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "We find the mapping functions Lq and Lr through a large number of query-response pairs and a large margin variant of the method in Wu et al. (2013).", "startOffset": 131, "endOffset": 148}, {"referenceID": 17, "context": "Model Description To alleviate the lexical gap problem, we employ the state-of-the-art translation-based language model (TransLM) (Xue et al., 2008) with a small modification for retrieval-based STC.", "startOffset": 130, "endOffset": 148}, {"referenceID": 18, "context": "\u03b1 is the Jelinek-Mercer smoothing factor (Zhai and Lafferty, 2001).", "startOffset": 41, "endOffset": 66}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008).", "startOffset": 164, "endOffset": 182}, {"referenceID": 17, "context": "The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al.", "startOffset": 134, "endOffset": 152}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008). The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al. (2008) does not do that in the answer part.", "startOffset": 165, "endOffset": 438}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008). The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al. (2008) does not do that in the answer part. The reasons are two-fold: (1) we focus on finding suitable responses given a query while Xue et al. (2008) focus on finding similar questions given a query question, and (2) the responses tend to be shorter than the posts in STC while the answers tent to be longer than the questions in CQA.", "startOffset": 165, "endOffset": 582}, {"referenceID": 17, "context": "We follow the method of Xue et al. (2008) to learn the word translation probabilities.", "startOffset": 24, "endOffset": 42}, {"referenceID": 17, "context": "We follow the method of Xue et al. (2008) to learn the word translation probabilities. In our experiments, original post-comment pairs are used for training, and the GIZA++8 (Och and Ney, 2003) toolkit is used to learn the translation model. There are two different settings of source and target for constructing the parallel corpus: (1) set the posts as the source and the responses as the target, i.e., collection (p, r)1, ..., (p, r)n, and (2) set the responses as the source and the posts as the target, i.e., collection (r, p)1, ..., (r, p)n. Following Xue et al. (2008), a pooling strategy is adopted to combine the above two collections of pairs to get a pooled collection (p, r)1, .", "startOffset": 24, "endOffset": 576}, {"referenceID": 8, "context": "com/p/giza-pp/ 9The detail can be found in Lu and Li (2013). 11", "startOffset": 50, "endOffset": 60}, {"referenceID": 19, "context": "We use ICTCLAS (Zhang et al., 2003) to obtain the part of speech and named entity information and LIBLINEAR (Fan et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": ", 2003) to obtain the part of speech and named entity information and LIBLINEAR (Fan et al., 2008) to build the logistic regression model and predict the probability of a word being a topic word.", "startOffset": 80, "endOffset": 98}, {"referenceID": 3, "context": "When training the weighs of features in Equation (2) with linear RankingSVM (Herbrich et al., 1999), we use a fixed penalty parameter (i.", "startOffset": 76, "endOffset": 99}], "year": 2014, "abstractText": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \u201cintelligently\u201d, when combined with a huge repository of conversation data from social media.", "creator": "LaTeX with hyperref package"}}}