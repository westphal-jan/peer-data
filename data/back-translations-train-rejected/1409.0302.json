{"id": "1409.0302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2014", "title": "Team Behavior in Interactive Dynamic Influence Diagrams with Applications to Ad Hoc Teams", "abstract": "Planning for ad hoc teamwork is challenging because it involves agents collaborating without any prior coordination or communication. The focus is on principled methods for a single agent to cooperate with others. This motivates investigating the ad hoc teamwork problem in the context of individual decision making frameworks. However, individual decision making in multiagent settings faces the task of having to reason about other agents' actions, which in turn involves reasoning about others. An established approximation that operationalizes this approach is to bound the infinite nesting from below by introducing level 0 models. We show that a consequence of the finitely-nested modeling is that we may not obtain optimal team solutions in cooperative settings. We address this limitation by including models at level 0 whose solutions involve learning. We demonstrate that the learning integrated into planning in the context of interactive dynamic influence diagrams facilitates optimal team behavior, and is applicable to ad hoc teamwork.", "histories": [["v1", "Mon, 1 Sep 2014 06:53:27 GMT  (348kb)", "http://arxiv.org/abs/1409.0302v1", "8 pages, Appeared in the MSDM Workshop at AAMAS 2014, Extended Abstract version appeared at AAMAS 2014, France"]], "COMMENTS": "8 pages, Appeared in the MSDM Workshop at AAMAS 2014, Extended Abstract version appeared at AAMAS 2014, France", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["muthukumaran chandrasekaran", "prashant doshi", "yifeng zeng", "yingke chen"], "accepted": false, "id": "1409.0302"}, "pdf": {"name": "1409.0302.pdf", "metadata": {"source": "CRF", "title": "Team Behavior in Interactive Dynamic Influence Diagrams with Applications to Ad Hoc Teams", "authors": ["Muthukumaran Chandrasekaran", "Prashant Doshi", "Yifeng Zeng", "Yingke Chen"], "emails": ["mkran@uga.edu", "pdoshi@cs.uga.edu", "y.zeng@tees.ac.uk", "y.chen@qub.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.03 02v1 [cs.MA] 1 SCategories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents, Multiagent SystemsGeneral Term Algorithms, ExperimentationKeywords Multiagent Systems, Ad-hoc Teamwork, Sequential Decision Making and Planning, Reinforcement Learning"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "2. BACKGROUND: INTERACTIVE DIDS", "text": "In the following we outline I-DIDs and refer readers to [25] for further details."}, {"heading": "2.1 Representation", "text": "A traditional DID model of sequential decision-making for a single agent by linking a series of random, decision-making, and utility nodes over multiple time steps. To view multi-agent interactions, I-DIDs introduce a new type of node called a model node (hexagonal node, Mj, l \u2212 1, in Fig. 1 (a))), which represents how another agent j acts as subject-agent i about his own decisions at the level. The model node contains a series of candidate models at level l \u2212 1. A link from the random node, S, to the model node, Mj, l \u2212 1, represents the beliefs of agent i about the models of j. Specifically, it is a probability distribution in the conditional probability table (CPT) of the random node, mod [Mj] -1 (in Fig."}, {"heading": "2.2 Solution", "text": "A level l I-DID of the agent i expands via T-time steps is solved in bottom-up manner. To solve the level l I-DID of the agent, all the models of the lower level l \u2212 1 of the agent j must be solved. The solution of a level l \u2212 1 model, mj, l \u2212 1, is the policy of j, which is a mapping of J's observations in Oj to the optimal decision in Aj given their belief, bj, l \u2212 1. We can then enter J's optimal decisions in the random node, Aj, in each time step, and expand J's models in Mod [Mj, l \u2212 1], which correspond to each pair of J's optimal action and observation. We perform this process for each level l \u2212 1 models of j in each time step and obtain the fully extended level l model. We outline the algorithm for the exact solution of I-DIDs in the figure. 2.The computerized complexity of the growth of the I-level models is mainly due to the exponential D."}, {"heading": "3. TEAMWORK IN INTERACTIVE DIDS", "text": "Each ad hoc agent in the team behaves according to a policy that maps the observation history or beliefs of the agent to the action (s) he or she should perform. We begin by showing that the endlessly nested hierarchy in I-DID does not facilitate ad hoc teamwork. However, extending the traditional model space to include models whose solution is achieved through reinforcement learning offers an opportunity to develop team behavior."}, {"heading": "3.1 Implausibility of Teamwork", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "3.2 Augmented Level 0 Models that Learn", "text": "We present a principle-driven method to induce team behavior by improving the reasoning ability of problem agents at the lower level. Although it is a priori difficult to see the benefit of ascension for Agent j in Fig. 3, it could be experienced by the agent. Specifically, it can explore moving in different directions, including ascension and the benefit from the resulting, possibly indirect, team reward. Subsequently, we can expect an agent to learn strategies that are consistent with optimal team behavior, because the appropriate measures provide large reinforcements. For example, that Agent i moves directly in Fig. 3, j may decide to ascend in his exploration, and thus receive a large reinforcing reward. This observation motivates the formulation of models at Level 0 that use RL to generate the predicted policy for the modeled agent. Essentially, we expect that RL's explorations will multiply the explorations caused by the lack of team work for the Rational policy."}, {"heading": "3.3 Model-Free Learning: Generalized MCESP", "text": "Learning has been used to solve decision-making problems in both single and multi-agent settings. Both model-based and model-based policy approaches exist for solving POMDPs. In the multi-agent context, Banerjee et al. [4] uses a semi-model-based distributed RL for finite horizon DEC POMDPs. Lately, the setting in which learning takes place has been partially observable, while RL approaches that calculate a value table for stateaction pairs are not applicable. We adapt Perkin's Monte Carlo Exploring Starts for POMDPs (MCESP) [18], which have been shown to learn good strategies in fewer iterations, while we do not make prior assumptions about the agent's models to achieve reliable convergence."}, {"heading": "3.4 Augmented I-DID Solutions", "text": "The solution of the extended I-DIDs is comparable to the solution of the traditional I-DIDs, except that the agent's candidate models can be learning models at level 0. We show the revised algorithm in Fig. 6. If the level 0 model is a learning model, the algorithm calls the method LEVEL 0 RL, which is shown in Fig. 4. Otherwise, it follows the same procedure as in Fig. 2 to solve the subordinate models recursively (line 11 in Fig. 4). While we consider in principle a reduced space of agent i's strategies and therefore the learning models of agent j, we can further reduce the political space of agent j by maintaining the top K strategies of j, K > 0 in terms of their expected usefulness (line 11 in Fig. 4)."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "Our experiments show that I-DIDs are complemented by level-0 models that facilitate team behavior, which was previously implausible. In addition, we demonstrate the applicability of I-DIDs to ad hoc teamwork in an environment similar to that of Wu et al. ([24]). We empirically evaluate performance in three well-known cooperative areas where two agents are involved, i and j: 3 x 3 grid meetings (Grid) [6], box-pushing (BP) [21] and multi-access broadcast channel (MABC) [12]. In the BP domain, each agent intends to push either a small or large box into the target region. The reward of the agents is maximum when both cooperatively push the large box to the target. In the MABC problem, the nodes must send messages among themselves across a channel."}, {"heading": "4.1 Teamwork in Finitely-Nested I-DIDs", "text": "Experimental Settings. We implemented the AUGMENTED I-DID algorithm as shown in Fig. 6, including an implementation of the generalized MCESP to perform level 0 RL. We demonstrate the performance of the extended framework for generating team behavior. We compare the expected benefits of Agent i's actions with the values of optimal team policy achieved using an exact algorithm, GMAA * -ICE, for dec-POMDP formulations of the same problem domains. [22] We also compare the values achieved by traditional I-DIDs. All I-DIDs are solved using the Exact Discrimination Modeling (DMU) method. For both traditional and eye DIDs, we have used level 0 models that differ in the original assumptions or framework. We adopt two model weight schemes that are not the same: (a) Uniform: All strategies are uniformly weighted with larger benefits; (b) they are proportional to larger benefits."}, {"heading": "4.2 Applications to Ad Hoc Teams", "text": "Experimental Settings. We test the performance of advanced I-DIDs in ad hoc applications that include different types of teammate, especially when teammates \"strategies are not effective in advancing the common goal (i.e. ad hoc) and compare them to a well-known ad hoc planner, OPAT. Team types include: (a) Random - when the teammate plays over the entire runtime after a randomly generated action sequence. Some predefined random seeds are used to guarantee that each test has the same action sequences. (b) Predefined - when the team plays according to some predefined patterns, the sequences of random actions are with some defined repeated lengths randomly selected at the beginning of each run. For example, if the action pattern is\" 1324 \"and the repeat value is 2, the resulting action sequence will be\" 1133244. \"(c) Optimal - when the team is playing and adaptive."}, {"heading": "BP T=20, look-ahead=3", "text": "The better performance of DID is due in part to the sophisticated belief update that gradually increases the probability on the true model when present in the model space of Agent j, as shown in Fig. 8 for MABC. As expected, both OPAT and Aug. I-DID-based ad-hoc agents perform better when the other agent in the team is optimal compared to random or predefined types. Aug. I-DIDs perform significantly better than OPAT when confronted with optimal teammates for the BP domain, while the results for the other domains are similar. In summary, Aug. I-DID maintains a probability distribution across different types of teammates and updates both the distribution and types over time, which differs from OPAT's concentration on a single optimal behavior of teammates during planning. Consequently, Aug. I-DID allows better adaptability as examined above. Further experiments on dynamics of ADs achieved in OPD-42 = better dynamics of ADs."}, {"heading": "4.3 Scalability Results", "text": "Although we recognize that the learning component (MCESP) is the bottleneck in scaling advanced I-DIDs for major problems, we have been able to achieve the optimal team policy using advanced I-DIDs (as calculated by GMAA * -ICE) in 4-4 raster, BP for T = 4, and MABC for T = 5. On these larger issues, we also noted a significant improvement in the values achieved by advanced I-DIDs compared to their traditional counterparts, as shown in Table 2. In the larger 4-4 raster domain for T = 3, the optimal team value of 29.6 is achieved by the advanced I-DID compared to 19.82 achieved by solving the traditional I-DID. A better replacement for MCESP and other approximation techniques will allow us to further expand advanced I-DIDs."}, {"heading": "5. DISCUSSION AND CONCLUSION", "text": "In this essay, we explain a negative consequence of limited rationality: the agent may not behave as an optimal teammate. In the I-DID framework, which models individual decision-makers who recursively model other actors, we show that the enhanced learning associated with planning enables models to produce sophisticated strategies. For the first time, we see the principle-driven and comprehensive emergence of team behavior in I-DID solutions that facilitate the application of I-DIDs to ad-hoc team environments for which they are simply well suited. We show that integrating learning in the context of I-DIDs helps us solve some fundamental challenges in ad-DID teamwork - building a single autonomous agent that can plan individually in partially observable environments by adapting to different types of teamwork while not making assumptions about its behavior or beliefs and trying to adapt to their true types."}, {"heading": "6. REFERENCES", "text": "[2] S. Albrecht and S. Ramamoorthy. A game-theoretic modeland best-response learning method for ad hoc coordination in multiagent systems (extended abstract). In AAMAS, pp. 1155-1156, 2013. [3] R. J. Aumann. Interactive epistemology II: Probability. International Journal of Game Theory, 28: 301-314, 1999. [4] B. Banerjee, J. Lyle, L. Kraemer, and R. Yellamraju. Solving finite horizon decentralized pomdps by distributed reinforcement learning. In AAMAS Workshop on MSDM, pp. 9-16, 2012. [5] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein."}], "references": [{"title": "Hierarchies of beliefs and common knowledge", "author": ["B. Adam", "E. Dekel"], "venue": "International Journal of Game Theory", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems (extended abstract)", "author": ["S. Albrecht", "S. Ramamoorthy"], "venue": "AAMAS, pages 1155\u20131156", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactive epistemology II: Probability", "author": ["R.J. Aumann"], "venue": "International Journal of Game Theory, 28:301\u2013314", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Solving finite horizon decentralized pomdps by distributed reinforcement learning", "author": ["B. Banerjee", "J. Lyle", "L. Kraemer", "R. Yellamraju"], "venue": "AAMAS Workshop on MSDM, pages 9\u201316", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research, 27(4):819\u2013840", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Bounded policy iteration for decentralized pomdps", "author": ["D.S. Bernstein", "E.A. Hansen", "S. Zilberstein"], "venue": "IJCAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Essays on Foundations of Game Theory", "author": ["K. Binmore"], "venue": "Pittman", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1982}, {"title": "A cognitive hierarchy model of games", "author": ["C.F. Camerer", "T.-H. Ho", "J.-K. Chong"], "venue": "The Quarterly Journal of Economics, 119(3):861\u2013898", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Reinforcement learning with perceptual aliasing: the perceptual distinctions approach", "author": ["L. Chrisman"], "venue": "AAAI, pages 183\u2013188", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Graphical models for interactive pomdps: Representations and solutions", "author": ["P. Doshi", "Y. Zeng", "Q. Chen"], "venue": "JAAMAS, 18(3):376\u2013416", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for sequential planning in multiagent settings", "author": ["P. Gmytrasiewicz", "P. Doshi"], "venue": "JAIR, 24:49\u201379", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["E.A. Hansen", "D.S. Bernstein", "S. Zilberstein"], "venue": "AAAI, pages 709\u2013715", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Interactive pomdp lite: Towards practical planning to predict and exploit intentions for interacting with self-interested agents", "author": ["T.N. Hoang", "K.H. Low"], "venue": "IJCAI, pages 2298\u20132305", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Formulation of bayesian analysis for games with incomplete information", "author": ["J. Mertens", "S. Zamir"], "venue": "International Journal of Game Theory, 14:1\u201329", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1985}, {"title": "K", "author": ["N. Meuleau", "L. Peshkin"], "venue": "eung Kim, and L. P. Kaelbling. Learning finite-state controllers for partially observable environments. In UAI, pages 427\u2013436", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayes-adaptive interactive pomdps", "author": ["B. Ng", "K. Boakye", "C. Meyers", "A. Wang"], "venue": "AAAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "JAAMAS, 11(3):387\u2013434", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning for pomdps based on action values and stochastic optimization", "author": ["T.J. Perkins"], "venue": "AAAI, pages 199\u2013204", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Minimal mental models", "author": ["D. Pynadath", "S. Marsella"], "venue": "AAAI, pages 1038\u20131044", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "The communicative multiagent team decision problem: Analyzing teamwork theories and models", "author": ["D.V. Pynadath", "M. Tambe"], "venue": "Journal of Artificial Intelligence Research, 16:389\u2013423", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved memory-bounded dynamic programming for decentralized pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "UAI", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "The multiagent decision process toolbox: Software for decision-theoretic planning in multiagent systems", "author": ["M. Spaan", "F. Oliehoek"], "venue": "AAMAS Workshop on MSDM, pages 107\u2013121", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Ad hoc autonomous agent teams: Collaboration without pre-coordination", "author": ["P. Stone", "G.A. Kaminka", "S. Kraus", "J.S. Rosenschein"], "venue": "AAAI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Online planning for ad hoc autonomous agent teams", "author": ["F. Wu", "S. Zilberstein", "X. Chen"], "venue": "IJCAI, pages 439\u2013445", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting model equivalences for solving interactive dynamic influence diagrams", "author": ["Y. Zeng", "P. Doshi"], "venue": "JAIR, 43:211\u2013255", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 22, "context": "Ad hoc teamwork involves a team of agents coming together to cooperate without any prior coordination or communication protocols [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "gent team decision problem [20] and the decentralized partially observable Markov decision process (DEC-POMDP) [5] utilize centralized planning and distribution of local policies among agents, which are assumed to have common initial beliefs.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "gent team decision problem [20] and the decentralized partially observable Markov decision process (DEC-POMDP) [5] utilize centralized planning and distribution of local policies among agents, which are assumed to have common initial beliefs.", "startOffset": 111, "endOffset": 114}, {"referenceID": 23, "context": "This includes an algorithm for online planning in ad hoc teams (OPAT) [24] that solves a series of stage games assuming that other agents are optimal with the utility at each stage computed using Monte Carlo tree search.", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Albrecht and Ramamoorthy [2] model the uncertainty about other agents\u2019 types and construct a Harsanyi-Bayesian ad hoc game that is solved online using learning.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "In this regard, recognized frameworks include the interactive POMDP (I-POMDP) [11], its graphical counterpart, interactive dynamic influence diagram (I-DID) [10], and I-POMDP Lite [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "In this regard, recognized frameworks include the interactive POMDP (I-POMDP) [11], its graphical counterpart, interactive dynamic influence diagram (I-DID) [10], and I-POMDP Lite [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "In this regard, recognized frameworks include the interactive POMDP (I-POMDP) [11], its graphical counterpart, interactive dynamic influence diagram (I-DID) [10], and I-POMDP Lite [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "Indeed, Albrecht and Ramamoorthy [2] note the suitability of these frameworks to the problem of ad hoc teamwork but find the complexity challenging.", "startOffset": 33, "endOffset": 36}, {"referenceID": 24, "context": "While recent advances on model equivalence [25] allow frameworks such as I-DIDs to scale, another significant challenge that merits attention is due to the finitely-nested modeling used in these frameworks, which assumes the presence of level 0 models that do not explicitly reason about others [1, 3, 8, 14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "While recent advances on model equivalence [25] allow frameworks such as I-DIDs to scale, another significant challenge that merits attention is due to the finitely-nested modeling used in these frameworks, which assumes the presence of level 0 models that do not explicitly reason about others [1, 3, 8, 14].", "startOffset": 295, "endOffset": 308}, {"referenceID": 2, "context": "While recent advances on model equivalence [25] allow frameworks such as I-DIDs to scale, another significant challenge that merits attention is due to the finitely-nested modeling used in these frameworks, which assumes the presence of level 0 models that do not explicitly reason about others [1, 3, 8, 14].", "startOffset": 295, "endOffset": 308}, {"referenceID": 7, "context": "While recent advances on model equivalence [25] allow frameworks such as I-DIDs to scale, another significant challenge that merits attention is due to the finitely-nested modeling used in these frameworks, which assumes the presence of level 0 models that do not explicitly reason about others [1, 3, 8, 14].", "startOffset": 295, "endOffset": 308}, {"referenceID": 13, "context": "While recent advances on model equivalence [25] allow frameworks such as I-DIDs to scale, another significant challenge that merits attention is due to the finitely-nested modeling used in these frameworks, which assumes the presence of level 0 models that do not explicitly reason about others [1, 3, 8, 14].", "startOffset": 295, "endOffset": 308}, {"referenceID": 23, "context": "[24], and experiment with multiple well-known cooperative domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We sketch I-DIDs below and refer readers to [25] for more details.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "Models that are behaviorally equivalent (BE) [19] \u2013 whose behavioral predictions for the other agent are identical \u2013 could be pruned and a single representative model can be considered.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "3 shows an ad hoc team setting of a two-agent grid meeting problem [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "However, at the same time, this imposition is, (a) motivated by reasons of computability, which allow us to operationalize such a paradigm; and (b) allows us to avoid some self-contradicting, and therefore impossible beliefs, which exist when infinite belief hierarchies are considered [7].", "startOffset": 286, "endOffset": 289}, {"referenceID": 8, "context": "Both model based [9] and model free [15, 18] learning approaches exist for solving POMDPs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 14, "context": "Both model based [9] and model free [15, 18] learning approaches exist for solving POMDPs.", "startOffset": 36, "endOffset": 44}, {"referenceID": 17, "context": "Both model based [9] and model free [15, 18] learning approaches exist for solving POMDPs.", "startOffset": 36, "endOffset": 44}, {"referenceID": 3, "context": "[4] utilized a semi-model based distributed RL for finite horizon DEC-POMDPs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] incorporated model learning in the context of IPOMDPs where the subject agent learns the transition and observation probabilities by augmenting the interactive states with frequency counts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "We adapt Perkin\u2019s Monte Carlo Exploring Starts for POMDPs (MCESP) [18], which has been shown to learn good policies in fewer iterations while making no prior assumptions about the agent\u2019s models in order to achieve convergence.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Agent j\u2019s policy space will be additionally reduced because behaviorally equivalent models \u2013 learning and other models with identical solutions \u2013 will be clustered [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 23, "context": "([24]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 5, "context": "We empirically evaluate the performance in three wellknown cooperative domains involving two agents, i and j: 3 \u00d7 3 grid meeting (Grid) [6], box-pushing (BP) [21], and multi-access broadcast channel (MABC) [12].", "startOffset": 136, "endOffset": 139}, {"referenceID": 20, "context": "We empirically evaluate the performance in three wellknown cooperative domains involving two agents, i and j: 3 \u00d7 3 grid meeting (Grid) [6], box-pushing (BP) [21], and multi-access broadcast channel (MABC) [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "We empirically evaluate the performance in three wellknown cooperative domains involving two agents, i and j: 3 \u00d7 3 grid meeting (Grid) [6], box-pushing (BP) [21], and multi-access broadcast channel (MABC) [12].", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "We compare the expected utility of agent i\u2019s policies with the values of the optimal team policies obtained using an exact algorithm, GMAA*-ICE, for Dec-POMDP formulations of the same problem domains [22].", "startOffset": 200, "endOffset": 204}, {"referenceID": 24, "context": "solved using the exact discriminative model update (DMU) method [25].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Additionally, in order to speed up the generation of RL models at level 0, we implemented an approximate version of our generalized MCESP called the Sample Average Approximation (MCESPSAA) that estimates action values by taking a fixed number of sample trajectories and then comparing the sample averages [18].", "startOffset": 305, "endOffset": 309}, {"referenceID": 17, "context": "4) if no policy changes are recommended after taking n samples of the value of each observation sequence-action pair [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "Integrating learning while planning provides a middle ground (or a bridge) between multiagent planning frameworks such as Dec-POMDPs and joint learning for cooperative domains [17].", "startOffset": 176, "endOffset": 180}], "year": 2013, "abstractText": "Planning for ad hoc teamwork is challenging because it involves agents collaborating without any prior coordination or communication. The focus is on principled methods for a single agent to cooperate with others. This motivates investigating the ad hoc teamwork problem in the context of individual decision making frameworks. However, individual decision making in multiagent settings faces the task of having to reason about other agents\u2019 actions, which in turn involves reasoning about others. An established approximation that operationalizes this approach is to bound the infinite nesting from below by introducing level 0 models. We show that a consequence of the finitely-nested modeling is that we may not obtain optimal team solutions in cooperative settings. We address this limitation by including models at level 0 whose solutions involve learning. We demonstrate that the learning integrated into planning in the context of interactive dynamic influence diagrams facilitates optimal team behavior, and is applicable to ad hoc teamwork.", "creator": "gnuplot 4.2 patchlevel 4 "}}}