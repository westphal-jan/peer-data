{"id": "1409.0813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "Friendly Artificial Intelligence: the Physics Challenge", "abstract": "Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \"Friendly AI\", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is \"meaning\" in a particle arrangement? What is \"life\"? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.", "histories": [["v1", "Tue, 2 Sep 2014 18:20:28 GMT  (8kb)", "https://arxiv.org/abs/1409.0813v1", "3 pages"], ["v2", "Wed, 3 Sep 2014 15:05:07 GMT  (8kb)", "http://arxiv.org/abs/1409.0813v2", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.CY cs.AI", "authors": ["max tegmark"], "accepted": false, "id": "1409.0813"}, "pdf": {"name": "1409.0813.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 9.08 13v2 [cs.CY] 3 Sep 201 4Friendly Artificial Intelligence: the Physics ChallengeMax Tegmark Dept. of Physics & MIT Kavli Institute, Massachusetts Institute of Technology, Cambridge, MA 02139 (as of September 4, 2014) Continuous progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans in the labor market, and perhaps ever. Eliezer Yudkowski and others have explored the possibility that a promising future for humanity could be guaranteed by a super-intelligent \"friendly artificial intelligence\" [1] designed to protect humanity and its values. I will argue that from a physical perspective, where everything is just an array of elementary particles, this could be even more difficult than it appears. Indeed, it might require rigorous reflection on the meaning of life: What does \"life\" mean in a particle array? What is \"life\" that we should strive to shape as our own ethical imperative, i.e. what is the ultimate future?"}, {"heading": "I. THE FRIENDLY AI VISION", "text": "As Irving J. Good pointed out in 1965 [2], an AI that is better than humans at all intellectual tasks could repeatedly and quickly improve its own software and hardware, resulting in an \"intelligence explosion\" that leaves humans far behind. Although we cannot reliably predict what would happen next, as Vernor Vinge [3] points out, Stephen Omohundro argues that we can predict certain aspects of AI's behavior almost independently of what final goals it may have [4], and this idea is reviewed and developed in Nick Bostrom's new book \"Superintelligence.\" [5] As I see it, the basic argument that an AI can maximize its chances of achieving its current goals is as follows: 1. Performance enhancement: (a) Better hardware (b) Better software (c) Better world Model2. Target-retentive 1a would favor both the use of current resources (for sensors, actuators, compilers, and architecture, etc.) as well as improvement."}, {"heading": "II. THE TENSION BETWEEN WORLD MODELING AND GOAL RETENTION", "text": "In fact, most of them are able to survive on their own."}, {"heading": "III. THE FINAL GOAL CONUNDRUM", "text": "In fact, most people who are able to survive on their own are not able to survive on their own, \"he said.\" But it's not that they are able to survive on their own. \"Most people who are able to survive on their own are not able to survive on their own, and so are people who are able to survive on their own."}], "references": [{"title": "Creating Friendly AI 1.0\u201d: The Analysis and Design of Benevolent Goal Architectures", "author": ["Yudkowski", "Eliezer"], "venue": "Machine Intelligence Research Institute,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Speculations Concerning the First Ultraintelligent Machine\u201d, in Advances in Computers\u201d, edited by Franz L", "author": ["Good", "Irving John"], "venue": "Alr and Morris Rubinoff,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1965}, {"title": "The Coming Technological Singularity: How to Survive in the Post-Homan Era\u201d, in Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace, 11-22", "author": ["Vinge", "Vernor"], "venue": "NASA Conference Publication", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "The Basic AI Drives\u201d, in Artificial General Intelligence 2008: proceedings of the First AGI Conference", "author": ["Omohundro", "Stephen M"], "venue": "Frontiers in Artificial Intelligence and Applications 171,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Superintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP", "author": ["Bostrom", "Nick"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart", "Norvig", "Peter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Causal Entropic Forces", "author": ["Wissner-Gross", "Alexander D", "C.E. Freer"], "venue": "Phys. Rev. Lett", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A Theory of Universal Artificial Intelligence based on Algorithmic Complexity", "author": ["Hutter", "Marcus"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Consciousness as Integrated Information: a Provisional Manifesto", "author": ["Tononi", "Giulio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \u201cFriendly AI\u201d [1], designed to safeguard humanity and its values.", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Good pointed out in 1965 [2], an AI that is better than humans at all intellectual tasks could repeatedly and rapidly improve its own software and hardware, resulting in an \u201cintelligence explosion\u201d leaving humans far behind.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 238, "endOffset": 241}, {"referenceID": 4, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 338, "endOffset": 341}, {"referenceID": 5, "context": "Incentive 1b implies improving learning algorithms and the overall architecture for what AI-researchers term an \u201crational agent\u201d [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "This sounds quite plausible: after all, would you choose to get an IQ-boosting brain implant if you knew that it would make you want to kill your loved ones? The argument for incentive 2 forms a cornerstone of the friendly AI vision [1], guaranteeing that a self-improving friendly AI would try its best to remain friendly.", "startOffset": 233, "endOffset": 236}, {"referenceID": 5, "context": "When optimizing a rational agent to attain a goal, limited hardware resources may preclude implementing a perfect algorithm, so that the best choice involves what AI-researchers term \u201climited rationality\u201d: an approximate algorithm that works reasonably well in the restricted context where the agent expects to find itself [6].", "startOffset": 323, "endOffset": 326}, {"referenceID": 4, "context": "Many such challenges have been explored in the friendly-AI literature (see [5] for a superb review), and so far, no generally accepted solution has been found.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "From my physics perspective, a key reason for this is that much of the literature (including Bostrom\u2019s book [5]) uses the concept of a \u201cfinal goal\u201d for the friendly AI, even though such a notion is problematic.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "\u2022 What Alex Wissner-Gross & Cameron Freer term \u201ccausal entropy\u201d [7] (a proxy for future opportunities), which they argue is the hallmark of intelligence.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "\u2022 The ability of the AI to predict the future in the spirit of Marcus Hutter\u2019s AIXI paradigm [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "\u2022 The amount of consciousness in our Universe, which Giulio Tononi has argued corresponds to integrated information [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "[1] Yudkowski, Eliezer 2001, \u201cCreating Friendly AI 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Good, Irving John 1965, \u201cSpeculations Concerning the First Ultraintelligent Machine\u201d, in Advances in Computers\u201d, edited by Franz L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "New York: Academic Press [3] Vinge, Vernor 1993, \u201cThe Coming Technological Singularity: How to Survive in the Post-Homan Era\u201d, in Vision-21:", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "[4] Omohundro, Stephen M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 241, "endOffset": 244}, {"referenceID": 7, "context": "110, 168702 [8] Hutter, Marcus 2000, \u201cA Theory of Universal Artificial Intelligence based on Algorithmic Complexity\u201d, arXiv:cs/0004001 [9] Tononi, Giulio 2008, \u201cConsciousness as Integrated Information: a Provisional Manifesto\u201d, Biol.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "110, 168702 [8] Hutter, Marcus 2000, \u201cA Theory of Universal Artificial Intelligence based on Algorithmic Complexity\u201d, arXiv:cs/0004001 [9] Tononi, Giulio 2008, \u201cConsciousness as Integrated Information: a Provisional Manifesto\u201d, Biol.", "startOffset": 135, "endOffset": 138}], "year": 2014, "abstractText": "Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \u201cFriendly AI\u201d [1], designed to safeguard humanity and its values. I will argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is \u201cmeaning\u201d in a particle arrangement? What is \u201clife\u201d? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.", "creator": "LaTeX with hyperref package"}}}