{"id": "1409.0915", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "An Approach for Text Steganography Based on Markov Chains", "abstract": "A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov - based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided.", "histories": [["v1", "Tue, 2 Sep 2014 22:59:52 GMT  (21kb)", "http://arxiv.org/abs/1409.0915v1", "Presented at 41 JAIIO - WSegI 2012"]], "COMMENTS": "Presented at 41 JAIIO - WSegI 2012", "reviews": [], "SUBJECTS": "cs.MM cs.CL", "authors": ["h hernan moraldo"], "accepted": false, "id": "1409.0915"}, "pdf": {"name": "1409.0915.pdf", "metadata": {"source": "CRF", "title": "An Approach for Text Steganography Based on Markov Chains", "authors": ["H. Hernan Moraldo"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 140 9.09 15v1 [cs.MM] 2 Sep 201 4Keywords: steganography, Markov chain, Markov model, text, linguistics"}, {"heading": "1 Introduction", "text": "Steganography is the field that deals with the problem of sending a message from a sender A to a receiver B through a channel that can be read by a so-called warden, in such a way that the warden does not suspect that the message is there. Steganographic techniques exist to hide messages in images, audio, videos and other media. Specifically, textual steganography examines information that is hidden in texts. There are many techniques for this, as summarized on [1] [2]. One of the simplest steganographic methods for texts works by encoding a fixed amount of bits per word, using a table that maps words to codes, and vice versa. A disadvantage of this trivial technique is that the text is obviously random at the syntactical level, since words are generated in a way that is independent of context."}, {"heading": "2 Related Work", "text": "Many methods of text steganography that are not based on Markov chains are well known, such as NiceText [10], which shows a way to encode cipher text into text using custom styles, context-free grammars and dictionaries; the approach used in [6] [7] is based on Markov chains; during encoding, some data is provided as input, and the system generates text as output using a particular Markov chain; the bridge texts are generated in a way that simulates that they were generated by the Markov chain. However, to avoid complex calculations, the Markov model is simplified by assuming that all probabilities from a given state are the same in another state, which can significantly alter the quality of the texts generated by the Markov chain. For example, words such as \"the\" and \"of course\" are both potential beginnings of a phrase, but the Markov model should not be the more common one and the latter should not be the difference."}, {"heading": "3 Markov Chain Models", "text": "In fact, it is such that most of them are able to survive themselves if they do not see themselves able to survive themselves, and that they are not able to survive themselves if they are not able to survive themselves. In fact, it is such that they are not able to survive themselves. In fact, it is such that they are not able to survive themselves. In fact, it is such that they are unable to survive themselves and survive themselves, as in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the, in the world in the world in the, in the, in the world in the world in the world in the, in the world in the, in the world in the world in the, in the, in the world in the, in the world in the world in the, in the world in the, in the world in the, in the world in the, in the world in the world in the world in the, in the, in the world in the, in the world in the, in the world in the world in the, in the world in the world in the, in the world in the, in the, in the world in the world in the, in the world in the, in the world in the, in the world in the, in"}, {"heading": "4 Fixed-size Steganography", "text": "A main objective in this article is to describe two functions, encode and decode, that are used to generate text from a data input while decode performs the reverse process.The encoding function is not cryptographically secure; it assumes that its input is plain text, or some data that has already been encrypted with an independent system. In the latter case, the input of the encoding text can be encrypted, while the input of the encoding text can be called cipherent text. In the latter case, the input of the encoding text can be called encoded text. We demand that the encoding function is not cryptographically secure; that is, that its input data should be encoded d1 and d2 (d1) = encode (d2) only be encoded if d1 = d2). Also, decode is the inverse of the encoding of the encoding code code, i.e. 1 is required for each encoding system (1)."}, {"heading": "4.1 Mapping of Probabilities to Ranges", "text": "A basic component for encoding and decoding is the function called subranges, which maps all outgoing states from a given state to subranges of a given range. These subranges are a partition of the original range. However, the result is a list that must have some properties that are described below. (sn, rn) The behavior of this function is that it maps the outgoing states of a Markov chain to subranges, s is a state in S, and r is a range of natural numbers. The result is a list that must have some properties that are described below.) The behavior of this function is that it maps the outgoing states of a Markov chain to subranges in a manner that approximates the ratio between the sizes of the different subranges, to the ratio between the probabilities of the respective states. For example, ifmc is the chain in Fig."}, {"heading": "4.2 Encoding Fixed-size Data Using Markov Chains", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "4.3 Decoding of Fixed-size Data Using Markov Chains", "text": "The decoding of fixed-size data is based on subrangeForState, described in Section 4.1. It was previously described as a function that returns the subrange associated with a given state, but it can also be considered a decoder from state to number. In this way, the subrangeForState function (mc, wk, r, wl) decodes a single word state, since the previous state was wk. The decoded value is not a number, but a range of numbers: [a, b] where both a and b are natural numbers. In the face of an input sequence of states or words wt (where w0 is taken as the initial state for the encoding) and an initial range r0 (typically [0, 2n \u2212 1]), we define the sequence of ranges rt = subcoding that will be decoded."}, {"heading": "4.4 Implementation Details", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "5 Variable Size Encoding and Decoding", "text": "The encoding and decoding process described above allows only the decoding of data from a text, since the size of the data is known beforehand. However, it requires the recipient of a steganographic system to know the size of the hidden data before it is decoded. This number is not the data size, but the size used for a header; it is typically a small value like 16 or 32. Texts c1 and c2 are encoded as shown below, with the three arguments version of encodefixed.The header is first encoded in c1, but the size used for a header; it is typically a small value like 16 or 32. Texts c1 and c2 are encoded as shown below, with the three arguments version of encodefixed.The header is first done in the code of c1. This is done using the algorithm for the fixed data encoding, with the fixed encoding of m known both for the encoder and for the decoder = decoder."}, {"heading": "6 Conclusions and Future Research", "text": "This article presented a steganographic method based on Markov chains, which differs from other similar models in that it avoids precision losses in the language model. A reference implementation for this method was also presented. Examples shown in Table 1 could show that the system has very low capacities, but this is only due to the Markov chain used; if the system uses a small Markov chain, it will have low capacities, but if it uses a larger Markov chain, it will typically have a higher capacity.Preliminary results of empirical tests with a large Markov chain calculated from an actual literary text show that the encoded data will be about 6-7 times the size of the original data, with an n value large enough (for very small n but larger than a few bytes, this factor may be higher, e.g. by 9). Since the output generated is text, it can be compressed with a high ratio of text, which is about twice the actual size of the compressed text."}], "references": [{"title": "Linguistic Steganography: Survey, Analysis, and Robustness Concerns for Hiding Information in Text", "author": ["K. Bennett"], "venue": "CERIAS Tech Report 2004-13, Purdue University.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Applying Statistical Methods to Text Steganography", "author": ["I. Nechta", "A. Fionov"], "venue": "CoRR.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "SNOW", "author": ["M. Kwan"], "venue": "http://www.darkside.com.au/snow/manual.html", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Information Hiding Through Errors, A Confusing Approach", "author": ["M. Topkara", "U. Topkara", "M.J. Atallah"], "venue": "Proceedings of the SPIE International Conference on Security, Steganography, and Watermarking of Multimedia Contents.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "TranslationBased Steganography", "author": ["C. Grothoff", "K. Grothoff", "L. Alkhutova", "R. Stutsman", "M. Atallah"], "venue": "Proceedings of the 2005 Information Hiding Workshop (IH 2005). Paper 1624.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Text Steganography System Using Markov Chain Source Model and DES Algorithm", "author": ["W. Dai", "Y. Yu", "Y. Dai", "B. Deng"], "venue": "Journal of Software, vol. 5, issue 7, pp. 785-792.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "BinText Steganography Based on Markov State Transferring Probability", "author": ["W. Dai", "Y. Yu", "B. Deng"], "venue": "2nd International Conference on Interaction Sciences: Information Technology, Culture and Human (ICIS \u201909).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "MarkovTextStego", "author": ["H.H. Moraldo"], "venue": "https://github.com/hmoraldo/markovTextStego", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Hiding the Hidden: a Software System for Concealing Ciphertext as Innocuous Text", "author": ["M. Chapman"], "venue": "Masters thesis, University of Wisconsin-Milwaukee", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "NL Stego", "author": ["C. Siefkes"], "venue": "http://www.siefkes.net/software/nlstego/", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Natural Language Steganography Based on Statistical Text Generation", "author": ["C. Siefkes"], "venue": "http://www.siefkes.net/software/nlstego/slides/slides-nlstego.sxi", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Artificial Intelligence, a Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": "Prentice Hall, 2nd Edition.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden Markov Models and Steganalysis", "author": ["M. Sidorov"], "venue": "2004 workshop on Multimedia and security (MM&Sec \u201904).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Attacks on Lexical Natural Language Steganography Systems", "author": ["C.M. Taskiran", "U. Topkara", "M Topkara", "E.J. Delp"], "venue": "SPIE International Conference on Security, Steganography, and Water-marking of Multimedia Contents.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "There are many techniques for this, as summarized on [1] [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "There are many techniques for this, as summarized on [1] [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "For example SNOW [3] hides information in tabs and spaces at the end of each line, that are usually not visible on text viewers.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "The technique shown in [4] hides information by modifying words in a way that resembles ortographical or typographical errors.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "There are other techniques that rely on translation [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "[6] [7] explore a method for encoding data on this way; [8] shows a simple implementation of a similar concept.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] [7] explore a method for encoding data on this way; [8] shows a simple implementation of a similar concept.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "A reference implementation of the method described here is also included in the open source program MarkovTextStego [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "An example is NiceText [10], which shows a way to encode ciphertext to text, that uses custom styles, Context Free Grammars and dictionaries.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "The approach used in [6] [7] is based on Markov chains.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "The approach used in [6] [7] is based on Markov chains.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "Other Markov - based models or similar models require of similar simplifications of the Markov chain, typically by making all outbound probabilities of each state equal (as in the previous example), or by replacing them by other ones, either explicitly or implicitly through the operation of the encoding algorithm [8] [11] [12].", "startOffset": 319, "endOffset": 323}, {"referenceID": 10, "context": "Other Markov - based models or similar models require of similar simplifications of the Markov chain, typically by making all outbound probabilities of each state equal (as in the previous example), or by replacing them by other ones, either explicitly or implicitly through the operation of the encoding algorithm [8] [11] [12].", "startOffset": 324, "endOffset": 328}, {"referenceID": 11, "context": ", XT ) with values from a finite set S is a Markov chain, if it has the Markov properties [13] [14]: Limited Horizon property:", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": ", XT ) with values from a finite set S is a Markov chain, if it has the Markov properties [13] [14]: Limited Horizon property:", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Markov chains and models are frequently used to model language [13]; when that\u2019s the case, states in the chain are used, for example, to represent words, characters, or n-grams.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Also, Markov models are used in steganography (as described above), and in steganalysis [15] [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Also, Markov models are used in steganography (as described above), and in steganalysis [15] [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "implementation of the system [9] allows using both unigrams and bigrams as states, and it is possible to extend it to support n-grams with n > 2.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "The language model is usually simplified in some way; for example [6] sets all P (x|w) with w fixed, to a fixed k.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 65, "endOffset": 71}, {"referenceID": 1, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "If s = s2, the expected result would be [(s4, [0, 0]), (s5, [1, 3])], where again the length of the subranges matches the proportion of their respective probabilities.", "startOffset": 60, "endOffset": 66}, {"referenceID": 2, "context": "If s = s2, the expected result would be [(s4, [0, 0]), (s5, [1, 3])], where again the length of the subranges matches the proportion of their respective probabilities.", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "01, and the input range to process is r = [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "In this case, it would seem that the best output would map s1 to the full range: the returned value for this would be [(s1, [0, 1])].", "startOffset": 124, "endOffset": 130}, {"referenceID": 0, "context": "Because of this, the only valid results for this example would be [(s1, [0, 0]), (s2, [1, 1])] and a symmetrical one (same ranges but switching states).", "startOffset": 86, "endOffset": 92}, {"referenceID": 0, "context": "Because of this, the only valid results for this example would be [(s1, [0, 0]), (s2, [1, 1])] and a symmetrical one (same ranges but switching states).", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "The reference implementation [9] was used, and the results may vary in other implementations, depending on specific details of the range partitioning algorithm.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "This means that we can use expand to convert short ranges like [01, 10] (in binary) to the longer 4 bit range [0100, 1011], if n = 4.", "startOffset": 63, "endOffset": 71}, {"referenceID": 7, "context": "Example benchmarks and results when running MarkovTextStego [9] with Markov chains generated from War and Peace by Tolstoy.", "startOffset": 60, "endOffset": 63}], "year": 2014, "abstractText": "A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}