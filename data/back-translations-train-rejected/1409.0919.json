{"id": "1409.0919", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "abstract": "This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications.", "histories": [["v1", "Tue, 2 Sep 2014 23:28:22 GMT  (806kb)", "http://arxiv.org/abs/1409.0919v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmad basheer hassanat", "mohammad ali abbadi", "ghada awad altarawneh", "ahmad ali alhasanat"], "accepted": false, "id": "1409.0919"}, "pdf": {"name": "1409.0919.pdf", "metadata": {"source": "META", "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "authors": ["Ahmad Basheer Hassanat", "Mohammad Ali Abbadi", "Ghada Awad Altarawneh", "Ahmad Ali Alhasanat"], "emails": [], "sections": [{"heading": null, "text": "This method is one of the simplest and oldest methods used for pattern classification. However, it often produces efficient results and in certain cases its accuracy is higher than that of other classes. The similarity depends on a specific distance metric, i.e. the performance of the classifier. [4] The KNN classifier categorizes an unlabeled test example using the majority of examples in its closest environment (most similar) neighbors in education. The similarity depends on a specific distance metric, i.e. the performance of the classifier. [5] The KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7] because the technique is very simple and highly efficient in the field of pattern recognition."}], "references": [{"title": "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties", "author": ["E. Fix", "J. Hodges"], "venue": "4, 1951.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1951}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Trans. Inform. Theory, vol. IT-13, pp. 21- 27, 1967.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1967}, {"title": "A Bootstrap Technique for Nearest Neighbor Classifier Design", "author": ["Y. Hamamoto", "S. Uchimura", "S. Tomita"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, vol. 19, no. 1, pp. 73-79, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Voting Over Multiple Condensed Nearest Neoghbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review, vol. 11, pp. 115-132, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207-244, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A Review of Data Classification Using K-Nearest Neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering, vol. 3, no. 6, pp. 354-360, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Survey of Nearest Neighbor  Techniques", "author": ["N. Bhatia", "A. Vandana"], "venue": "(IJCSIS) International Journal of Computer Science and Information Security, vol. 8, no. 2, pp. 302-305, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual Speech Recognition", "author": ["A.B.A. Hassant"], "venue": "Speech Technologies, I. Ipsic, Ed. Rijeka: InTech - Open Access Publisher, 2011, vol. 2, ch. 14.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual Passwords Using Automatic Lip Reading", "author": ["A.B.A. Hassanat"], "venue": "International Journal of Sciences: Basic and Applied Research (IJSBAR), vol. 13, no. 1, pp. 218-231, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "KNN Model- Based Approach in Classification", "author": ["G. Guo", "H. Wang", "D. Bell", "Y. Bi", "K. Greer"], "venue": "Lecture Notes in Computer Science, vol. 2888, pp. 986-996, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "The Condensed Nearest Neighbour Rule", "author": ["P. Hart"], "venue": "IEEE Transactions on Information Theory, vol. 14, pp. 515-516, 1968.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1968}, {"title": "The Reduced Nearest Neighbour Rule", "author": ["G. Gates"], "venue": "IEEE Transactions on Information Theory, vol. 18, pp. 431-433, 1972.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1972}, {"title": "Voting Nearest-Neighbour Subclassifiers", "author": ["M. Kubat", "M. Jr"], "venue": "Proceedings of the 17th International Conference on Machine Learning, ICML-2000, Stanford, CA, 2000, pp. 503-510.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Reduction Techniques for Exemplar-Based Learning Algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine learning, vol. 38, no. 3, pp. 257-286, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Pseudo Nearest Neighbor Rule for Pattern Recognition", "author": ["Y. Zeng", "Y. Yang", "L. Zhou"], "venue": "Expert Systems with  Applications, vol. 36, pp. 3587-3595, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A Modification on K- Nearest Neighbor Classifier", "author": ["H. Parvin", "H. Alizadeh", "B. Minaei"], "venue": "Global Journal of Computer Science and Technology, vol. 10, no. 14, pp. 37-41, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "An Improved kNN Text Classification Algorithm based on Clustering", "author": ["Z. Yong"], "venue": "Journal of Computers, vol. 4, no. 3, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Center-based nearest neighbor classifier", "author": ["Q.-B. Gao", "Z.-Z. Wang"], "venue": "Pattern Recognition, vol. 40, pp. 346-349, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "An Improved KNN Text Classification Algorithm Based on Clustering", "author": ["Z. Yong", "L. Youwen", "X. Shixiong"], "venue": "JOURNAL OF COMPUTERS, vol. 4, no. 3, pp. 230-237, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximate nearest neighbor: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proc. 30th Annu. ACM Symp. Comput. Geometry, 1998, p. 604\u2013613.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "COMMUNICATIONS OF THE ACM, vol. 51, no. 1, pp. 117-122, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Iknn: Informative k-nearest neighbor pattern classification", "author": ["Y. Song", "J. Huang", "D. Zhou", "H. Zha", "C.L. Giles"], "venue": "Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in  Databases, Berlin, 2007, pp. 248-264.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Toward an explanatory similarity measure for nearest-neighbor classification", "author": ["M. Latourrette"], "venue": "Proceedings of the 11th European Conference on Machine  Learning, London, 2000, pp. 238-245.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "An evaluation of statistical approaches to text categorization", "author": ["Y. Yang"], "venue": "Information Retrieval, vol. 1, pp. 69-90, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "Proceedings of SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval, Berkeley, 1999, pp. 42-49.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Choice of the smoothing parameter and efficiency of k-nearest neighbor classification", "author": ["G.G. Enas", "S.C. Choi"], "venue": "Computers & 38  http://sites.google.com/site/ijcsis/ ISSN 1947-5500  (IJCSIS) International Journal of Computer Science and Information Security, Vol. 12, No. 8, August 2014 Mathematics with Applications, vol. 12, no. 2, pp. 235-244, 1986.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1947}, {"title": "Classifier Based on Inverted Indexes of Neighbors", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Institute of Computer Science, Technical Report No. V-1034, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Using Singularity Exponent in Distance Based Classifier", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010), Cairo, 2010, pp. 220-224.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifiers Based on Inverted Distances", "author": ["M. Jirina", "M.J. Jirina"], "venue": "New Fundamental Technologies in Data  Mining, K. Funatsu, Ed. InTech, 2011, vol. 1, ch. 19, pp. 369-387.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE Trans. Evol. Comput., vol. 1, p. 67\u201382, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The nearest neighbor approach was first introduced by [1] and later studied by [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "INTRODUCTION The nearest neighbor approach was first introduced by [1] and later studied by [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers [3] [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers [3] [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "The similarity depends on a specific distance metric, therefore, the performance of the classifier depends significantly on the distance metric used [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "The KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7], because the technique is very simple, and highly efficient in the field of pattern recognition, machine learning, text categorization, data mining, object recognition, etc.", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "The KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7], because the technique is very simple, and highly efficient in the field of pattern recognition, machine learning, text categorization, data mining, object recognition, etc.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "[8] and [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] and [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "There are two major problems inherited from the design of the KNN [10] and [7]: 1.", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "There are two major problems inherited from the design of the KNN [10] and [7]: 1.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 119, "endOffset": 122}, {"referenceID": 12, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Their idea is based on removing the similar redundant examples [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "removing them causes no significant error overall [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "Other works used some hashing techniques to increase classification speed; this includes the work of [20] and [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "Other works used some hashing techniques to increase classification speed; this includes the work of [20] and [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Choosing the optimal K is almost impossible for a variety of problems [22], as the performance of a KNN classifier varies significantly when K is changed as well as the change of distance metric used.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "However, it is shown in the literature that when the examples are not uniformly distributed, determining the value of K in advance becomes difficult [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Their work reduces the size of the training data, and removes the need for choosing the k parameter [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Their experiments (based on 10 data sets from the benchmark corpus [24]) showed that their methods were less sensitive to the change of parameters than the conventional KNN classifier [22].", "startOffset": 184, "endOffset": 188}, {"referenceID": 2, "context": "Their experimental results showed that the nearest neighbor classifier based on the bootstrap samples outperforms the conventional KNN classifiers, mainly when the tested examples are in high dimensions [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 23, "context": "They used large values for the k parameter such as (30, 45 and 60), and the best results of the classifier were included in their results tables [25] and [26].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "They used large values for the k parameter such as (30, 45 and 60), and the best results of the classifier were included in their results tables [25] and [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 25, "context": "Enas and Choi show that the best choice of the k parameter was found to be dependent on several factors, namely, the dimension of the sample space, the size of the space, the covariance structure, as well as the sample proportions [27].", "startOffset": 231, "endOffset": 235}, {"referenceID": 26, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 170, "endOffset": 173}, {"referenceID": 26, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 175, "endOffset": 179}, {"referenceID": 27, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 181, "endOffset": 185}, {"referenceID": 28, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 190, "endOffset": 194}, {"referenceID": 26, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 239, "endOffset": 243}, {"referenceID": 27, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 245, "endOffset": 249}, {"referenceID": 28, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 254, "endOffset": 258}, {"referenceID": 26, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 182, "endOffset": 186}, {"referenceID": 27, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 188, "endOffset": 192}, {"referenceID": 28, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "In addition to the use of a large number of neighbors such as k= 30, 45 and 60, does not help in increasing the accuracy of the KNN classifier as argued by [25] and [26].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In addition to the use of a large number of neighbors such as k= 30, 45 and 60, does not help in increasing the accuracy of the KNN classifier as argued by [25] and [26].", "startOffset": 165, "endOffset": 169}, {"referenceID": 29, "context": "It is well established in the literature [31] and according to the \u2018no free lunch\u2019 theorem [32], there is no optimal classifier that works perfectly for every class of problems, as the performance of the classifier depends mainly on the problem and the data used.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "There is room for enhancing the complexity time of the proposed method using KD-trees [33] or other hashing techniques [20] and [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "There is room for enhancing the complexity time of the proposed method using KD-trees [33] or other hashing techniques [20] and [21].", "startOffset": 128, "endOffset": 132}], "year": 2014, "abstractText": "This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications. KeywordsKNN; supervised learning; machine learning; ensemble learning; nearest neighbor;", "creator": "PDFCreator Version 1.7.3"}}}