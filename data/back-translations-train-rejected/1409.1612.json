{"id": "1409.1612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Semantic clustering of Russian web search results: possibilities and problems", "abstract": "The paper deals with word sense induction from lexical co-occurrence graphs. We construct such graphs on large Russian corpora and then apply this data to cluster the results of Mail.ru search according to meanings of the query. We compare different methods of performing such clustering and different source corpora. Models of applying distributional semantics to big linguistic data are described.", "histories": [["v1", "Thu, 4 Sep 2014 21:09:26 GMT  (627kb,D)", "https://arxiv.org/abs/1409.1612v1", "Presented at Russian Summer School in Information Retrieval (RuSSIR 2014). To be published in Springer Communications in Computer and Information Science series"], ["v2", "Sun, 26 Oct 2014 13:27:11 GMT  (627kb,D)", "http://arxiv.org/abs/1409.1612v2", "Presented at Russian Summer School in Information Retrieval (RuSSIR 2014). To be published in Springer Communications in Computer and Information Science series"]], "COMMENTS": "Presented at Russian Summer School in Information Retrieval (RuSSIR 2014). To be published in Springer Communications in Computer and Information Science series", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["andrey kutuzov"], "accepted": false, "id": "1409.1612"}, "pdf": {"name": "1409.1612.pdf", "metadata": {"source": "META", "title": "Semantic clustering of Russian web search results: possibilities and problems", "authors": ["Andrey Kutuzov"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The problem stems from the obvious fact that many search queries are in some way ambiguous. Thus, search engines strive to diversify their results and present such results as are related to as many search results as possible. Example: Google search for the Russian word \"search results\": 1. Five search results referring to a popular singer, 2. Two search results for a magazine, 3. A search result for http: / / lib.ru, Maxim Moshkov's electronic library, 4. A search result for a correct name. However, these search results are not sorted by their meaning and are simply returned by their relevance, which for many of them seems to be almost the same. The obvious way to capture the search results is by the words that share their snippets."}, {"heading": "2 Related Work", "text": "As explained in the previous section, we are inspired by a basic hypothesis, since the meaning depends on distribution [1], and that the frequency of linguistic phenomena (in our case word coincidence) is important in determining the place of these phenomena in the language system [2]. Our work is also based on the idea that the senses of ambiguous lexical units should be generated automatically from the data itself, rather than from a dictionary. Therefore, we shift our focus from selecting the most appropriate senses from a predefined inventory to the discovery of senses automatically from the raw data that are nature texts. One of the first notes on the practical application of this idea is the text itself."}, {"heading": "3 Building Co-Occurrence Graph", "text": "The first thing we had to do was to select a text corpus to build the graphics on which we build two larger sentences. It is well known that the larger the corpus is, the more co-occurrence information it contains. However, increasing corpus size also leads to exponentially increasing computing time. So, for the sake of time and for the sake of the preliminary nature of our research, we limit ourselves to three smaller, yet acceptable sizes: 1. Corpus of random searches from Mail.ru search engine3 (2 milliontokens), continue. \"The first two items are academic texts of Russian National Corpus2 (written) language in general. They differ in that the first consists of complete texts published under various free and open licenses, while the second is a random sample from the larger sentences of Russian national language."}, {"heading": "4 Building Query Graph", "text": "We experimented with clustering the search results page on a series of sixty ambiguous one-word Russian searches, taken from Analyzethis homonymous queries analyzer4. Analyzethis is a search engine evaluation initiative that offers various search engine performance analyzers, including one for ambiguous or homonymous searches. We combed Mail.ru for these searches, obtained titles and snippets (10 for each search result).The process of the semantic cluster star begins with the construction of the so-called query graph. Here we follow closely [6].First we lemmage all snippets and titles and remove stopwords and the query word itself. Then we construct a graph with all the nouns of snippets and titles as vertex. Then we use one of the large corporate graphs (the ones we built in Section 3) to find words that are strongly linked to the query and those words in the query."}, {"heading": "5 Processing Query Senses and Results", "text": "It is also important that we do not delete vertexes with clusters. This is because the neighbors of such vertexes are not associated with anything but that vertexes. It is also important that we do not delete vertexes with clusters. It is important that we do not delete vertexes with clusters. This is because the neighbors of such vertexes are not associated with anything. What we do is also important that we do not delete vertexes with clusters. It is important that we do not delete vertexes with clusters. This is because the neighbors of such vertexes are not associated with anything. If we remove them, we will be associated with a lot of vertexes. It is also important that we do not delete vertexes with clusters. It is that we do not delete vertexes with clusters. This is because neighbors of such vertexes are not associated with anything. It is that we are associated with anything."}, {"heading": "5.3 Mapping Results to Senses", "text": "Once we have the SERP cluster for the search query, we can combine it with dictionaries for each search result to eventually perform SERP clustering. We do this in a fairly straightforward way. Faced with a set of senses represented by a fixed lemmas and a set of results (snippets and titles), we calculate for each pair of results () and senses () the measure of similarity. \"In the future.\" In the future. \"In the future.\" In the future. \"In the future.\" In the future. \"In the future.\" In the future. \"In the future.\" In the future. \""}, {"heading": "6 Evaluation of SERP clustering", "text": "It is the best method to do this, but for now we will limit ourselves to simply assessing the correctness of the cluster number (that is, the number of meanings). For example, if we believe that there are three sensory perceptions in the SERP, we will classify all results into a cluster, that is, the algorithm is not optimal if the number of cluster entries is as high as it is."}, {"heading": "7 Conclusion and future work", "text": "We have shown that state-of-the-art methods of word induction and search result clustering based on semantic graphs work for Russian data. Applying such methods may bring the presentation of search engine results closer to the actual semantics of the results, not just by mere term frequency ranking, which would allow the user to immediately understand what results in the SERP are actually related to the sense of the query and what other senses exist. We plan to experiment with more ways of processing search results and to start a full human evaluation of the results, and such snippets should provide better data for graph-based sense of the word induction algorithms. We also plan to use more ways of processing search results and to perform a complete human evaluation of the results. Furthermore, it seems profitable to not only use individual words, but also to use composite phrases, as well as neighbors."}], "references": [{"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Frequency of use and the organization of language", "author": ["J. Bybee"], "venue": "Oxford University Press, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Information retrieval based on word senses", "author": ["H. Sch\u00fctze", "J.O. Pedersen"], "venue": "Proceedings 4th Annual Symposium on Document Analysis and Information Retrieval (SDAIR 1995).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "A quick tour of word sense disambiguation, induction and related approaches", "author": ["R. Navigli"], "venue": "SOFSEM 2012: Theory and practice of computer science. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionary word sense distinctions: An enquiry into their nature", "author": ["A. Kilgarriff"], "venue": "Computers and the Humanities 26(5-6)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Clustering and diversifying web search results with graph-based word sense induction", "author": ["A.D. Marco", "R. Navigli"], "venue": "Computational Linguistics 39(3)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Hyperlex: lexical cartography for information retrieval", "author": ["J. V\u00e9ronis"], "venue": "Computer Speech & Language 18(3)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Towards wider multilinguality", "author": ["L. Padr\u00f3", "E. Stanilovsky"], "venue": "eds.: Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Translating collocations for bilingual lexicons: A statistical approach", "author": ["F. Smadja", "K.R. McKeown", "V. Hatzivassiloglou"], "venue": "Computational Linguistics 22(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Collective dynamics of \u2018small-world\u2019networks", "author": ["D.J. Watts", "S.H. Strogatz"], "venue": "Nature 393(6684)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "As stated in the previous Section, we are inspired by a fundamental hypothesis than meaning depends on the distribution [1] and that frequency of linguistic phenomena (in our case, word co-occurrence) is important for determining these phenomena\u2019s place in the system of language [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "As stated in the previous Section, we are inspired by a fundamental hypothesis than meaning depends on the distribution [1] and that frequency of linguistic phenomena (in our case, word co-occurrence) is important for determining these phenomena\u2019s place in the system of language [2].", "startOffset": 280, "endOffset": 283}, {"referenceID": 4, "context": "No dictionary is perfect or comprehensive, because \u2018senses as identified in the dictionary identify points on a continuum of possibilities for how the word is used\u2019 [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "One of the first notes on practical application of this idea to word sense disambiguation and word sense induction is found in [3], where vector representations of word similarity derived from co-occurrence data are used.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Broad review of contemporary (by 2012) state of the field is provided in [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "The main source of methods for our present research is [6], which describes workflow for clustering web search results using graph analysis over co-occurrence networks.", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "query senses we employed Curvature algorithm [6] and Hyperlex algorithm proposed in [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "query senses we employed Curvature algorithm [6] and Hyperlex algorithm proposed in [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "Processed QC with Freeling analyzer [8] to extract lemmas and morphological information for all tokens, 3.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Removed all tokens except nouns, as we restrict ourselves to inducing only nominal senses (the same strategy was applied in [6]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "For each edge we also calculate Dice coefficient [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "It should also be noted that all corpora comply to \u2018small world\u2019 definition [10], because their average path length is approximately the same as in a random graph with the same number of vertexes (NV ) and average degree (AD), while clustering coefficient is significantly higher than it should be in the random graph.", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "Here we closely follow [6].", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "We apply two techniques for that, namely, Curvature from [6] and Hyperlex from [7].", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "We apply two techniques for that, namely, Curvature from [6] and Hyperlex from [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "Hyperlex algorithm described in [7] introduces the notion of \u2018hubs\u2019 within the graph, meaning most inter-connected vertexes and employs the graph\u2019s maximum spanning tree.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Perhaps, these senses are in fact less related to real-life, as even linguists sometimes have trouble matching the \u2018senses\u2019 found in a dictionary and the occurrences found in a corpus [7].", "startOffset": 184, "endOffset": 187}], "year": 2014, "abstractText": "The present paper deals with word sense induction from lexical co-occurrence graphs. We construct such graphs on large Russian corpora and then apply the data to cluster the results of Mail.ru search according to meanings in the query. We compare different methods of performing such clustering and different source corpora. Models of applying distributional semantics to big linguistic data are described.", "creator": "LaTeX with hyperref package"}}}