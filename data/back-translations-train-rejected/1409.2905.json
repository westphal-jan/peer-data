{"id": "1409.2905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "Non-Convex Boosting Overcomes Random Label Noise", "abstract": "The sensitivity of Adaboost to random label noise is a well-studied problem. LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be less sensitive to noise than AdaBoost. We present the results of experiments evaluating these algorithms on both synthetic and real datasets. We compare the performance on each of datasets when the labels are corrupted by different levels of independent label noise. In presence of random label noise, we found that BrownBoost and RobustBoost perform significantly better than AdaBoost and LogitBoost, while the difference between each pair of algorithms is insignificant. We provide an explanation for the difference based on the margin distributions of the algorithms.", "histories": [["v1", "Tue, 9 Sep 2014 21:36:47 GMT  (251kb,D)", "http://arxiv.org/abs/1409.2905v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunsern cheamanunkul", "evan ettinger", "yoav freund"], "accepted": false, "id": "1409.2905"}, "pdf": {"name": "1409.2905.pdf", "metadata": {"source": "CRF", "title": "Non-Convex Boosting Overcomes Random Label Noise", "authors": ["Sunsern Cheamanunkul", "Evan Ettinger"], "emails": ["scheaman@eng.ucsd.edu", "evanettinger@gmail.com", "yfreund@eng.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "Adaboost [10] is a very popular classification learning algorithm. It is a simple and effective algorithm. Although it is generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2]. The random label noise setup is one where we take a data set for which our learning algorithm generates an accurate classifier and flips every label in the training set with a small fixed probability. Note that the classifier, who was a good classifier in the noiseless setup, is still a good classifier. The problem is that in the loud setup the loud examples mislead the learning algorithm and cause it to deviate significantly from the good classification. LogitBoost [7] is assumed to be less sensitive to random noise than Adaboost, but it still falls on a high level of random label functionality [8] that actually uses a random label [8]."}, {"heading": "2 Boosting, margins and convexity", "text": "All non-recursive enhancement algorithms generate a classification rule that represents a threshold for a linear problem (so-called \"base\" classification rules). More precisely, let (x, y), with y {\u2212 1, + 1} therefore denounce a described example. However, let hi: X \u2192 {\u2212 1, + 1} denounce the basic rules. Then, the output of the enhancement algorithm is a rule of formF (x) = character (\u2211 i \u03b1ihi (x))). As it turns out, the sum that is the operator of the drawing function is important for understanding the operation of the enhancement algorithms as well as the generalization errors of the generated classifier. It is convenient to replace the sum with a dot product: i\u03b1ihi (x) = ~ \u03b1 \u00b7 h (x) with ~ \u03b1 and ~ h defined in the natural way."}, {"heading": "2.1 Margins and generalization", "text": "Our ultimate goal in learning classifiers to reduce the test error - the number of errors the classifier makes on the test set. However, since we only have access to the training data, we cannot directly minimize the generalization error. The natural goal of the algorithm is to minimize the training error. Schapire et. al. [11] has shown, however, that there is a better performance measurement for the performance of the increased classifier on the training set. That is, to maximize the number of training examples whose normalized margin is greater than any margin. > 0 Where the positive margin of the example (x, y) is defined to measure the performance of the increased player (x, y) = y ~ \u03b1 \u00b7 h (x). 1The intuition presented and justified in [11] is that large positive margins correspond to trustworthy predictions. Specifically, Theorem 2 in [11] states that the playing ability is centered with the selection of 1 over \u2212 random."}, {"heading": "2.2 Random label noise", "text": "Suppose the weight assigned to the example (x, y) by Adaboost is exponential in the margin area w (x, y) = e \u2212 m (x, y). Suppose c (x) is the best rule for noise-free data. Suppose we now add independent label noise to the dataset. The margin of c (x) for examples whose labels have been reversed will be negative, resulting in a high weight being assigned to the loud examples, resulting in basic classifiers matching the loud examples."}, {"heading": "3 Three non-convex boosting algorithms", "text": "Freund [3, 4, 5] suggested several boosting algorithms that use nonconvex potential functions. We will briefly consider three of these algorithms: Boost-by-majority, BrownBoost, and RobustBoost."}, {"heading": "3.1 Boosting-by-majority", "text": "Boost by Majority (BBM) combines the basic rules using the same weights for each of the rules. Two additional assumptions are made: that the error of each of the basic rules with respect to the corresponding distribution is smaller than a fixed number: 1 / 2 \u2212 \u03b3, and that the number of boosting iterations is known in advance. Combining these three constraints, Freund can throw the study in the form of a mathematical game and find the optimal solution for this game. The result is a potential function that depends on both the margin and the number of steps remaining until the end of the game. Specifically, T should be the total number of iterations and not the current iteration. Let's take as an example what we are currently looking at is (x, y) and that i is the number of existing basic rules that correctly predict on (x, y) (i corresponds to the margin), then the optimal boosting algorithm uses a potential function defined by the following recursion."}, {"heading": "3.2 BrownBoost and RobustBoost", "text": "While BBM is a theoretically optimal boosting algorithm, it is not applicable in practice because it knows the number of steps in advance and gives the same weight to each basic rule. Compare this to Adaboost and LogitBoost, which adjust their step size to the error of the last basic classification. Brownboost [4] overcomes this deficiency by taking the limit of the BBM game, where the number of steps goes infinitely. This limit is not trivial, but the end result is fairly simple. Within this limit, both the margin s and the time are continuous: \u03a6 (s, t) = 12 (1 \u2212 erf (s, t) = 1 (s + 2 \u00b7 \u03b2 (1 \u2212 t) \u221a 2 (1 \u2212 t))) (4) A slightly different limit yields the robust boost potential: \u0445 (s, t) = 12 (1 \u2212 erf (s \u2212 erf (t) \u03c3 (t)))) (5) ce 1cm \u2212 2ct = 1ct and \u2212 2 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 are \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212."}, {"heading": "3.3 Solving the potential function", "text": "RobustBoost and BrownBoost require an additional step in the boosting algorithm. Potential functions change here depending on time, and time is a continuous variable (it is not proportional to the number of iterations). To do this, we have to solve a series of two nonlinear equations into two unknowns for each iteration: the basic rule weight \u03b1 and the time progress \u0394t. We use a standard numerical solver."}, {"heading": "3.4 Setting the parameters", "text": "Unlike Adaboost and Logitboost, the non-convex boosting algorithms require the selection of two parameters. These exist in different forms, but they are all equivalent to selecting the error target and the margin target. The error target corresponds to a guess of the fraction of the examples where we must \"give up.\" While the margin target defines the minimum margin for the examples where we do not give up. In the next section, we propose an adaptive algorithm for selection. The time associated with the n-th iteration of the BrownBoost and RobustBoost algorithms is called tn = \u2211 n i = 1 (0) i. The initial time corresponds to zero t0 = 0 and ti \u2212 1 \u2265 ti increases with each iteration. The end time is defined as t = 1. When the algorithm reaches this time, it stops. In some cases, the setting for it is too low and the setting for it is too high."}, {"heading": "4 Adaptive- Heuristic", "text": "In BrownBoost (BB) and RobustBoost (RB), we must specify the target error rate. Choosing the method can greatly affect the performance of the trained classifier. If set too low or too high, the algorithms often produce a classifier that performs poorly even on the training data. Figure 2 shows the margin distributions of BB and RB, which differ on a data set with 30% label noise after 200 iterations. Note that the classifier, if \u2264 0.30, does not provide examples around zero margin.If the true noise rate is known, a good rule of thumb is to set a little higher than the noise rate. Table 1 summarizes the final time tf and the final training error rate Ef of BB and RB on a synthetic data set with the true noise rate \u03b7 = {0.1, 0.2, 0.3} using two different settings slightly above and below the limit."}, {"heading": "5 Experiments", "text": "In this section we compare the performance of BrownBoost with adaptive (BBA), RobustBoost with adaptive (RBA) to AdaBoost (ADB) and LogLossBoost (LLB) 1 to 3 datasets with and without random label noise. In particular, we take a close look at the margin distributions for each of the boosting algorithms. In addition, we investigate the effects of using positive target margin imbalances on BBA and RBA. We have implemented all the boosting algorithms in MATLAB and used the Optimization Toolbox to numerically solve BB- and RB equations. For RBA, we use \u03c3f = 0.001 for all experiments."}, {"heading": "5.1 Datasets", "text": "We performed experiments on 3 different datasets: LS, Face and Satimage. Each dataset can be described as follows: Firstly, the LS dataset is a synthetic dataset, whose structure is suggested by Long and Servidio in [8]. The dataset has an input x-R21 with binary characters xi-1, + 1 and label y-1, + 1. Each instance is generated as follows. Firstly, the label y is selected so that it is \u2212 1 or + 1 with equal probability. Given y and the border width parameter xi, the characteristics xi are selected according to the following mix distribution: \u2022 Large margin: Probability 1 / 4, we select xi = y for all 1 \u2264 i \u2022 Pullers: Probability 1 / 4, we select xi = y for 1 \u2264 i attributes and xi = \u2212 y for 11 + y = \u2212 y for the face."}, {"heading": "5.2 Results", "text": "We initially have the performance of ADB, LLB, BBA and RBA with different identification noise level from ADLB = 0,0, 0,1, 0,2, 0,3 to LS with 2 settings of the identification parameter \u03b4 1, 3. We used the identification set size N = 1600 and executed each identification error for 200 identifications. For BBA and RBA, the identification parameter RLS has been successfully switched to the correct identification parameter, while ADA and LLB have the higher identification error quotas with respect to the true identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification identification and"}, {"heading": "6 Conclusion", "text": "Our experiments show that Brownboost and Robustboost are significantly more resistant to label noise than Adaboost and LogitBoost. We show how this is related to the progression of margin distribution over time. We show that determining the target error rate is critical to final performance and provide practical heuristics for determining it. Our experiments also show that for loud small training sets, maximizing the margin of examples where we do not \"give up\" is significantly better than minimizing the training error."}], "references": [{"title": "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "author": ["T.G. Dietterich"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inf. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "An Adaptive Version of the Boost by Majority Algorithm", "author": ["Y. Freund"], "venue": "Mach. Learn.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "A more robust boosting algorithm", "author": ["Y. Freund"], "venue": "[stat.ML],", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Experiments with a New Boosting Algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of the 13th International conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Additive Logistic Regression: a Statistical View of Boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R. a. Servedio"], "venue": "Proceedings of the 25th international conference on Machine learning - ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An Empirical Evaluation of Bagging and Boosting", "author": ["R. Maclin", "D. Opitz"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction Adaboost [10] is a very popular classification learning algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 7, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 0, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "LogitBoost [7] is believed to be less sensitive to random noise than Adaboost, but it still falls pray to high levels of random labels noise.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In fact, Servedio and Long [8] proved that, in general, any boosting algorithm that uses a convex potential function can be misled by random label noise.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "Freund [4] suggested a boosting algorithm, called Brownboost, that uses a non-convex potential function and claims to overcome random label", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "[11] defines the \u201cmargin\u201d of an example as: m(x, y) = y~ \u03b1 \u00b7 ~h(x) Thus m(x, y) > 0 if and only if the classification rule is correct on the example (x, y).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Moreover, as was shown in [8], algorithms that minimize convex potential functions can always be fooled by the addition of random label noise.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "[11], showed that there is a better performance measure the performance of the boosted classifier on the training set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "m\u0302(x, y) = y ~ \u03b1 \u00b7 ~h(x) \u2016\u03b1\u20161 The intuition, presented and justified in [11], is that large positive margins correspond to confident predictions.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Specifically, Theorem 2 in [11] states that, with probability 1 \u2212 \u03b4 over the random choice of the training set, the following inequality holds for all \u03b8 > 0", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 2, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 3, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 2, "context": "Brownboost [4] overcomes this deficiency by taking the limit of the BBM game where the number of steps goes to infinity.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "First, LS dataset is a synthetic dataset whose construction is suggested by Long and Servidio in [8].", "startOffset": 97, "endOffset": 100}], "year": 2014, "abstractText": "The sensitivity of Adaboost to random label noise is a well-studied problem. LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be less sensitive to noise than AdaBoost. We present the results of experiments evaluating these algorithms on both synthetic and real datasets. We compare the performance on each of datasets when the labels are corrupted by different levels of independent label noise. In presence of random label noise, we found that BrownBoost and RobustBoost perform significantly better than AdaBoost and LogitBoost, while the difference between each pair of algorithms is insignificant. We provide an explanation for the difference based on the margin distributions of the algorithms.", "creator": "LaTeX with hyperref package"}}}