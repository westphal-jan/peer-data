{"id": "1409.2993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2014", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "abstract": "It has always been a burden to the users of statistical topic models to predetermine the right number of topics, which is a key parameter of most topic models. Conventionally, automatic selection of this parameter is done through either statistical model selection (e.g., cross-validation, AIC, or BIC) or Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These methods either rely on repeated runs of the inference algorithm to search through a large range of parameter values which does not suit the mining of big data, or replace this parameter with alternative parameters that are less intuitive and still hard to be determined. In this paper, we explore to \"eliminate\" this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameter-free topic model extracts topics with a comparable quality comparing to classical topic models with \"manual transmission\". The quality of the topics outperforms those extracted through classical Bayesian nonparametric models.", "histories": [["v1", "Wed, 10 Sep 2014 08:41:35 GMT  (473kb,D)", "http://arxiv.org/abs/1409.2993v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["jian tang", "ming zhang", "qiaozhu mei"], "accepted": false, "id": "1409.2993"}, "pdf": {"name": "1409.2993.pdf", "metadata": {"source": "CRF", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "authors": ["Jian Tang", "Ming Zhang", "Qiaozhu Mei"], "emails": ["tangjian@net.pku.edu.cn", "mzhang@net.pku.edu.cn", "qmei@umich.edu"], "sections": [{"heading": null, "text": "First of all, we present a non-parametric treatment of the PLSA model called non-parametric latent semantic analysis (nPLSA). The nPLSA follow-up method allows for the exploration and comparison of different numbers of topics within a single execution, but remains as simple as PLSA's. This is achieved by replacing the parameter of the number of topics with an alternative parameter that represents the minimum accuracy of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity of the discovered topics or by weak monitoring of the users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics can be further eliminated by two parameter-free treatments: either by monitoring the diversity of the discovered topics or by weak monitoring of the users in the form of an exemplar topic."}, {"heading": "1. INTRODUCTION", "text": "1D eeisrsrteeVnlrsrteeeeteeteeeeoiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiteeteeteeteeteeteeteersrrsrsrrsrrrrrllgteeoiiiiuiuiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiteteteteteteteteteteeteteteeteeterrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2. RELATED WORK", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves, that we are able to stay in the world, \"he said."}, {"heading": "3. PLSA", "text": "The classical parametric topic model of probabilistic latent semantic analysis (PLSA) assumes that each document is a mixture of topics, with each topic corresponding to a multinomial distribution across the word vocabulary. (1) where n (d, w) is the frequency of the word w in document d, p (z | d) is the probability of the topic z in document d, and p (w | z) is the probability of the word w in topic z. According to the principle of maximum probability, the parameters of the model, i.e. p (z | d) and p (z | d) are the probability of the topic z in document d, and p (w | z) the probability of the word is calculated by maximizing the logical probability distribution z."}, {"heading": "4. ELIMINATING THE PARAMETER K", "text": "To eliminate parameter K or automatically select the optimal K, a natural way is to execute the PLSA algorithm with different values from K and then select the value that maximizes the likelihood of a data set retained or a selection criterion for a statistical model, such as AIC and BIC. These methods inevitably require many runs of the algorithm and are therefore computationally expensive. A better way is to explore different topics within a single run of the algorithm. Therefore, in Section 4.1 we introduce a non-parametric variation of the PLSA called non-parametric PLSA (nPLSA), which allows the number of topics to be dynamically increased and thus to be able to compare different values of K within a single run of the algorithm. Although the nPLSA model is still based on a user parameter different from K, it can be eliminated either by using the diversity of topics (Section 4.2) or by monitoring a weak one (Section 4.3)."}, {"heading": "4.1 Nonparametric PLSA", "text": "The non-parametric PLSA (nPLSA) models are motivated by the intuition that a document does not fit well into the current topics, but fits into the new topics. (...) This approach can of course be applied to a collection of documents in a sequential manner. (...) An important question is how to measure the quality of a document by the existing topics. (...) Let us leave a set of topics and the new topic of the document. (...) Let it be a set of topics and the language of the document d. (...) If a new topic is promoted by the document d, an extended topic will be promoted by the document d. (...) An expanded collection of topics would emerge as a result. (...) Of course, the new topic would emerge as a language model of the document d. (...) This decision can be promoted by comparing the documents d. (...) An extended collection of topics would be promoted as a result of the new collection of topics in a sequential man.An important question is how to measure the quality of an existing document. (...)"}, {"heading": "A Theoretical Interpretation", "text": "The procedure of Algorithm 1 is similar to that of the original PLSA model, except that it allows new topics to be generated in the E-step. Although it remains simple, the algorithm has a principled theoretical interpretation. Specifically, one can prove that the entire procedure is actually monotonous by logging the following objective function [d, w). [The coefficient of punishment is sessibly the threshold in algorithm 1 [z | d] p (w | z) \u2212 K, (7) this is the log probability of training data being punished by the number of topics.] The coefficient of punishment is inherently the threshold in algorithm 1. In other words, the introduction of an additional topic leads to the loss of a minimal threshold of probability ratio. [Clearly, a smaller result in a greater number of topics.] Such an objective function is indeed closely linked to the Akaike-M-Information (AIC) [1] in the statistical function of the model selection and the objective means, We also prove that in the objective setting of DP]."}, {"heading": "4.2 Parameter-free nPLSA through Topic Diversity", "text": "The nPLSA model makes it possible to gradually increase the number of topics (the number of topics in a single process). (So the number of topics we are aiming for in each area is very high.) The question is when do we generate new topics, or how do we find a smaller number of topics that are closer together. (The question is when do we stop generating new topics, or do we stop researching a larger variety of topics.) Our treatment makes use of a degree of diversity among the topics to generate new topics. (The question is when do we stop generating new topics, or stop pushing diversity among the topics that cease to generate new topics as a criterion. (20), query proposal [19], and documented proposals [5]. Some existing work has been used as evaluation criteria for the quality of the topics, but not for selecting the optimal number of topics (20)."}, {"heading": "4.3 Parameter-free nPLSA through Weak Supervision", "text": "In practice, we have found that while it is difficult for a user to define the number of topics in a given dataset, it is usually much easier for them to describe an exemplary topic of their expectation (e.g., \"I want to find topics like data mining.\") Therefore, to minimize the burden on a user, we allow them to provide an exemplary topic in the form of a keyword query to guide the algorithm toward the desirable granularity of topics. This query-by-example design makes a much more reasonable assumption about users, because it is a common practice to construct search queries. Let's define the distance of a topic that q is the distance between the query and the topic that is closest to it."}, {"heading": "4.4 Discussion", "text": "Since diversity among topics may not be a convex function in terms of the number of topics, in practice we can wait several iterations until diversity of topics decreases smoothly. An alternative solution is to start parameter-free nPLSA with enough iterations to see how the diversity of topics changes from which we can determine the optimal number of topics. Then, you can simply run a classic theme model using this optimal number of topics. It is similar to determining when d (\u03b8q) reaches the minimum. Sensitivity in terms of the order of the documents. As the nPLSA model introduced in Section 4.1 processes the documents in sequential order, one concern is whether the order of the documents affects the result. We will empirically show that the result is not sensitive to the order of the documents. (See Figure 3 (d): Parameter-free treatment of the nPLSA with both topics diversity or weak monitoring does not depend on how the entire order of the documents is merged."}, {"heading": "5. EXPERIMENTS", "text": "We evaluate the nPLSA model and its parameter-free versions against the classic PLSA and LDA theme models and the Bayesian non-parametric theme model HDP2. We evaluate all models on both synthetic and real data sets."}, {"heading": "5.1 Evaluation Metrics", "text": "On synthetic data, we know the basic truth of the topics. (We refer to the basic truth topics as area truth and the number of topics learned by the algorithm.) The quality of the topics learned can be measured by comparing them with the basic truth topics. (Topic Quality Error.) The quality error measures the error that the learned topics are introduced into the basic truth topics. The quality error of each learned topic is defined as the minimum distance between the topic and the basic truth topics. (Topic Quality Error measures the error that the learned topics are introduced into the basic truth topics.) The quality error of each individual learned topic is defined as the minimum distance between the topic and the basic truth topics. The overall issue of quality error2We take over the implementation of HDP from http: / / www. cmu.u / ceresource.1 The concentrations for both are defined as level 1."}, {"heading": "5.2 Results on Synthetic Dataset", "text": "In fact, most of us are able to stick to the rules that they have set for their purposes over the past few years. (...) Most of us are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...) Most of them are able to achieve their goals. (...)"}, {"heading": "5.3 Results on Real-World Datasets", "text": "In fact, it is the case that most of us are able to go in search of a solution that they have got a grip on. (...) In fact, it is the case that they are able to go in search of a solution. (...) In fact, it is the case that they are able to find a solution. (...) In fact, it is the case that they are able to find a solution. (...) In fact, it is the case that they are able to find a solution. (...) It is as if they are able to find a solution. (...) It is as if they are able to find a solution. (...) It is as if they are able to find a solution."}, {"heading": "7. REFERENCES", "text": "In B. N. Petrov and F. Csaki, editors, Proceedings of 2nd International Symposium on Information Theory, pp. 267-281. Akademiai Kiado, 1973. [2] D. Arthur and S. Vassilvitskii. k-means + +: The advantage of careful seeding. [3] D. M. Blei, T. Griffiths, M. I. Jordan, and J. B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In NIPS, 2003. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet location. J. Mach. Learn. Res., 3: 993-1022, 2003."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Aikake"], "venue": "B. N. Petrov and F. Csaki, editors, Proceedings of 2nd International Symposium on Information Theory, pages 267\u2013281. Akademiai Kiado", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "The use of mmr", "author": ["J. Carbonell", "J. Goldstein"], "venue": "diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335\u2013336, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Relational topic models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "Journal of Machine Learning Research - Proceedings Track, 5:81\u201388", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience, New York, NY, USA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M.D. Hoffman", "D.M. Blei"], "venue": "CoRR, abs/1206.4665", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS, 101", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["L. Hannah", "D.M. Blei", "W.B. Powell"], "venue": "Journal of Machine Learning Research, 12:1923\u20131953", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A K-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics, 28:100\u2013108", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Bayesian Nonparametrics: Principles and Practice", "author": ["N. Hjort", "C. Holmes", "P. Mueller", "S. Walker"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for latent dirichlet allocation", "author": ["M.D. Hoffman", "D.M. Blei", "F.R. Bach"], "venue": "NIPS, pages 856\u2013864", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pages 50\u201357, New York, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Empirical study of topic modeling in twitter", "author": ["L. Hong", "B.D. Davison"], "venue": "Proceedings of the First Workshop on Social Media Analytics, SOMA \u201910, pages 80\u201388, New York, NY, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting k-means: New algorithms via bayesian nonparametrics", "author": ["B. Kulis", "M.I. Jordan"], "venue": "CoRR, abs/1111.0352", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "CIKM, pages 375\u2013384", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic-link lda: joint models of topic and author community", "author": ["Y. Liu", "A. Niculescu-Mizil", "W. Gryc"], "venue": "ICML, pages 665\u2013672, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Diversifying query suggestion results", "author": ["H. Ma", "M.R. Lyu", "I. King"], "venue": "AAAI", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Divrank: the interplay of prestige and diversity in information networks", "author": ["Q. Mei", "J. Guo", "D.R. Radev"], "venue": "KDD, pages 1009\u20131018", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing semantic coherence in topic models", "author": ["D.M. Mimno", "H.M. Wallach", "E.M. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP\u201911, pages 262\u2013272", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizing semantic coherence in topic models", "author": ["D.M. Mimno", "H.M. Wallach", "E.M. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, pages 262\u2013272", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic evaluation of topic coherence", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "NAACL", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating topic models for digital libraries", "author": ["D. Newman", "Y. Noh", "E.M. Talley", "S. Karimi", "T. Baldwin"], "venue": "JCDL, pages 215\u2013224", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "On the problem of the most efficient tests of statistical hypotheses", "author": ["J. Neyman", "E.S. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231:289\u2013337", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1933}, {"title": "Predicting friendship links in social networks using a topic modeling approach", "author": ["R. Parimi", "D. Caragea"], "venue": "PAKDD, pages 75\u201386", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, 6(2):461\u2013464", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1978}, {"title": "Arnetminer: extraction and mining of academic social networks", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang", "Z. Su"], "venue": "KDD, pages 990\u2013998", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Evaluation methods for topic models", "author": ["H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D.M. Mimno"], "venue": "ICML, page 139", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "KDD, pages 448\u2013456", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["C. Wang", "J.W. Paisley", "D.M. Blei"], "venue": "Journal of Machine Learning Research, 15:752\u2013760", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "X. He", "C.H.Q. Ding", "M. Gu", "H.D. Simon"], "venue": "NIPS\u201901, pages 1057\u20131064", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Model-based feedback in the language modeling approach to information retrieval", "author": ["C. Zhai", "J.D. Lafferty"], "venue": "CIKM, pages 403\u2013410", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": ", [14, 4]) are widely adopted to analyze text collections in various domains, such as the Web, scientific literature, social media, and digital humanities.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": ", [14, 4]) are widely adopted to analyze text collections in various domains, such as the Web, scientific literature, social media, and digital humanities.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 205, "endOffset": 211}, {"referenceID": 16, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 232, "endOffset": 236}, {"referenceID": 17, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 5, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 25, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 30, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 294, "endOffset": 298}, {"referenceID": 13, "context": "In practice, parametric models such as the probabilistic latent semantic analysis (PLSA) [14] and the latent Dirichlet allocation (LDA) [4] are still widely preferred.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "In practice, parametric models such as the probabilistic latent semantic analysis (PLSA) [14] and the latent Dirichlet allocation (LDA) [4] are still widely preferred.", "startOffset": 136, "endOffset": 139}, {"referenceID": 24, "context": "\u201d Inspired by the likelihood ratio test [25], we can measure how well a document is described by the current topics using the likelihood ratio of it being generated by the existing topics versus being generated by all plus one more topic.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Traditional clustering algorithms such as k-means [11] and spectral clustering [33], and statistical topic models, such as PLSA and LDA, all require users to specify the number of clusters or topics.", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "Traditional clustering algorithms such as k-means [11] and spectral clustering [33], and statistical topic models, such as PLSA and LDA, all require users to specify the number of clusters or topics.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "One way to choose this parameter is to run the same algorithm on the same data many times with different parameter values, and then choose the best model based on statistical model selection techniques such as the perplexity on held-out data, the Akaike information criterion (AIC) [1], or the Bayesian information criterion (BIC) [27].", "startOffset": 282, "endOffset": 285}, {"referenceID": 26, "context": "One way to choose this parameter is to run the same algorithm on the same data many times with different parameter values, and then choose the best model based on statistical model selection techniques such as the perplexity on held-out data, the Akaike information criterion (AIC) [1], or the Bayesian information criterion (BIC) [27].", "startOffset": 331, "endOffset": 335}, {"referenceID": 28, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 2, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 7, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 9, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 11, "context": "For clustering problems, the Dirichlet process mixture [12] model is widely used as the nonparametric prior of the mixture components.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "In [29], Teh et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3], a hierarchical latent Dirichlet allocation (hLDA) model was proposed based on the Dirichlet process to infer topic hierarchies from the data.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [16], Kulis and Jordan designed the DP-means algorithm, which is a scalable clustering algorithm built on top of k-means and", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Formally, a likelihood ratio test [25] can be used to compare the goodness of fit of two models, with the requirement that one is nested within the other (in our case, \u0398 is nested within \u0398\u2032).", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "This can be achieved by a \u201cfold-in\u201d process [14], which updates the topic distribution of the words in the document, i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "Such an objective function is in fact closely related to the Akaike information criterion (AIC) [1] in statistical model selection and also related to the objective in DP-means [16].", "startOffset": 96, "endOffset": 99}, {"referenceID": 15, "context": "Such an objective function is in fact closely related to the Akaike information criterion (AIC) [1] in statistical model selection and also related to the objective in DP-means [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "Some existing work has used diversity as an evaluation criteria for topic quality [21], but not for selecting the optimum number of topics (compare across different numbers of topics).", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "In practice, one can instantiate the distance function with the L2 distance or the Jensen\u2212Shannon divergence [7] (the symmetric version of the Kullback\u2212Leibler divergence [7]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "In practice, one can instantiate the distance function with the L2 distance or the Jensen\u2212Shannon divergence [7] (the symmetric version of the Kullback\u2212Leibler divergence [7]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "This heuristic is related to the process for sampling seedling points in the KMeans++ algorithm [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 33, "context": "While a query can be as short as a keyword, it is the common practice in information retrieval to estimate a robust \u03b8q though model-based pseudo feedback [34].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 22, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 21, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 23, "context": "In [24], the point-wise mutual information is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Specifically, for each document wj in the held-out dataset, we split the document into two parts wj = (wj1,wj2) and compute the predictive likelihood of the second part wj2 (20% of the words) conditioned on the first 80% of the words and the training data, the same way used in [30].", "startOffset": 278, "endOffset": 282}, {"referenceID": 27, "context": "The first dataset we use is the computer science bibliography DBLP from [28] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "As the length of tweets is too short for topic modeling analysis [15], we adopt the same strategy as [15] by treating each hashtag as a \u201cpseudo document\u201d and concatenate all the tweets containing this hashtag into the same document.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "As the length of tweets is too short for topic modeling analysis [15], we adopt the same strategy as [15] by treating each hashtag as a \u201cpseudo document\u201d and concatenate all the tweets containing this hashtag into the same document.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "In particular, it is possible to develop an online inference procedure of these models similar to the practice in [13] and [32], as well as distributed versions of the algorithms.", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "In particular, it is possible to develop an online inference procedure of these models similar to the practice in [13] and [32], as well as distributed versions of the algorithms.", "startOffset": 123, "endOffset": 127}], "year": 2014, "abstractText": "It has always been a burden to the users of statistical topic models to predetermine the right number of topics, which is a key parameter of most topic models. Conventionally, automatic selection of this parameter is done through either statistical model selection (e.g., cross-validation, AIC, or BIC) or Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These methods either rely on repeated runs of the inference algorithm to search through a large range of parameter values which does not suit the mining of big data, or replace this parameter with alternative parameters that are less intuitive and still hard to be determined. In this paper, we explore to \u201celiminate\u201d this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameterfree topic model extracts topics with a comparable quality comparing to classical topic models with \u201cmanual transmission.\u201d The quality of the topics outperforms those extracted through classical Bayesian nonparametric models. \u2217This study is done when the first author is visiting the University of Michigan. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00.", "creator": "LaTeX with hyperref package"}}}