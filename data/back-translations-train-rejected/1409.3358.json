{"id": "1409.3358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2014", "title": "Building Program Vector Representations for Deep Learning", "abstract": "Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the \"coding criterion\" to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than \"shallow\" methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future.", "histories": [["v1", "Thu, 11 Sep 2014 08:44:28 GMT  (189kb,D)", "http://arxiv.org/abs/1409.3358v1", "This paper was submitted to ICSE'14"]], "COMMENTS": "This paper was submitted to ICSE'14", "reviews": [], "SUBJECTS": "cs.SE cs.LG cs.NE", "authors": ["lili mou", "ge li", "yuxuan liu", "hao peng", "zhi jin", "yan xu", "lu zhang"], "accepted": false, "id": "1409.3358"}, "pdf": {"name": "1409.3358.pdf", "metadata": {"source": "CRF", "title": "Building Program Vector Representations for Deep Learning", "authors": ["Lili Mou", "Ge Li", "Yuxuan Liu", "Hao Peng", "Zhi Jin", "Yan Xu", "Lu Zhang"], "emails": ["zhanglu}@sei.pku.edu.cn", "alandroxu}@gmail.com"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. MOTIVATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. From Machine Learning to Deep Learning", "text": "Traditional approaches to machine learning largely depend on human feature engineering, e.g. [17] for error detection, [18] for cloning detection. Such feature engineering is labeling-intensive and ad hoc tailored to a specific task. Furthermore, evidence in the literature suggests that human-made features can be even worse than automatically learned. Mnih et al. report - for example, in the field of natural language processing (NLP) - that the automatically learned taxonomy of words [19] is better in its application than the famous expert-designed WordNet [20] in [21].Such results place increasing demands on highly automated learning approaches, such as deep learning with very little human engineering. In deep neural networks, program analysis using statistical methods can be easier. For example, in the task of program classification, deep neural networks automatically extract program features that are of interest with very little human engineering."}, {"heading": "B. Barriers of Deep Learning for Program Analysis", "text": "Although deep neural networks are powerful enough to capture complex functions, there are still barriers that need to be overcome before they can be used practically to analyze programs. As all program symbols (e.g. nodes in ASTs) are \"discrete,\" no order is defined on these symbols. However, such discrete symbols cannot be fed directly into a neural network. A possible solution is to map each symbol to a real vector in a dimension. Each element in the vector spontaneously characterizes a specific feature of the symbol, which is why it is also called distributed representation."}, {"heading": "III. BACKGROUND OF DEEP LEARNING AND REPRESENTATION LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Neural Networks", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green of the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the"}, {"heading": "B. Existing Representation Learning Approaches in NLP", "text": "However, they cannot be applied directly to the field such as NLP and program analysis, because words and program symbols are \"discrete.\" However, in areas such as NLP, a word with the index 20 does not become twice as large as the word with the index 10 for each meaningful property. Therefore, if the value is 255, the pixel becomes white. In areas such as NLP, a word with the index 20 is by no means as large as the word with the index 10 for a meaningful property. Therefore, the traditional approaches in NLP, where each word is treated as an atomic unit, are meaningless."}, {"heading": "IV. THE CODING CRITERION FOR PROGRAM REPRESENTATION LEARNING", "text": "In this section we will first discuss the granularities of the program representation. We will be content with the granularity of nodes in abstract syntax trees (ASTs). In Section IV-B we formalize our approach and give the learning objective. In Section IV-C we will present the stochastic gradient descent algorithm for the training."}, {"heading": "A. The Granularity", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "B. Formalization", "text": "In fact, it is as if most people are able to outdo themselves. (...) It is not as if they are able to outdo themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves. (...) It is as if they are able to outtrump themselves."}, {"heading": "C. Training", "text": "The numerical optimization algorithm we use is stochastic gradient descent with dynamics. The model parameters are first computed randomly for each data sample x (i) and its negative sample x (i) c. The coding criterion of the vector representation acquisition - as preparation phase for the analysis of neural programs - is then \"flat,\" whereby error algorithm 1: StochasticGradientDescentWithMomentum input: data samples x (i), i = 1.. N; momentum; learning rate \u03b1 output: model parameters \u0438 = (vec (\u00b7), Wl, Wr, b) can be randomly initialized."}, {"heading": "V. EXPERIMENTS", "text": "These qualitative evaluations give an intuitive idea of our vector representations and then we perform supervised learning in the program classification task. We forward the learned representations to deep neural networks. Experimental results show that meaningful representations as a means of preparation significantly facilitate the training of the network in deep architectures. Furthermore, we achieve a higher accuracy with the deep, tree-based Convolutionary Neural Network compared to basic methods. We regard this as primary proof of the success of deep learning for program analyses. The data set, source codes and learned representations can be downloaded from our project page."}, {"heading": "A. Qualitative Evaluation: Nearest Neighbor Queries and kmeans Clustering", "text": "As we noted in Section IV-B, similar nodes in ASTs (such as ID, constant) should have similar representations. To assess whether our coding criterion for representation learning has achieved this goal, we perform closest neighborhood queries. This seems significant since both are associated with data references. Similar symbols also include ArrayRef, BinaryOp, which are associated with data manipulation. Symbols such as If, During, Break are similar to those associated with control flows. ArrayDecl, PtrDecl are similar to those associated with explanations."}, {"heading": "B. Quantitative Evaluation: Improvement for Supervised Learning", "text": "It is about whether building programs in the first 40 years is beneficial for the tasks of the real world, i.e. whether they will improve the optimization and / or generalization of interests. We feed the representations of people who are able to enter their source codes into the system, which means they are accompanied by a large number of programming problems for students who solve the problems and insert their source codes into the system. OJ system automatically compiles the validity of the source codes. We select four problems for our program task. Source codes (in programming language C) of the four problems are downloaded together with their labels. We divide the data on 3: 1 for education, cross-validation and testing. Figure 4 plots the learning curves for education."}, {"heading": "VI. LOOKING FORWARD TO THE FUTURE", "text": "As the literature shows, deep learning is making breakthroughs in many areas of artificial intelligence. We believe that it will also become an important method in various tasks in the field of program analysis. As a pioneering study, we are dedicated to the following promising research topics in this new field."}, {"heading": "A. Different Perspectives for Program Modeling", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to determine for themselves what they want and what they want. In fact, it is the case that most people are able to determine for themselves what they want and what they want."}, {"heading": "VII. CONCLUSION", "text": "We propose a novel \"coding criterion\" to create vector representations of nodes in ASTs that make deep learning a reality for program analysis; we also feed the learned representations into a deep neural network to classify programs; the experimental results show that our representations successfully capture the similarities and relationships between different nodes in ASTs; the learned representations significantly improve supervised training for deep neural networks both in terms of optimization and generalization; and we conclude that the coding criterion is successful in building program vector representations; the experiments also confirm the feasibility of deep learning for analyzing programs; and show primary evidence of its success in the new field. As a groundbreaking study, we address several fundamental issues, including the perspectives of program modeling, the integration of human priors, and the application of deep learning. To promote further research in the new field, we will publish all our source codes, and believe that our studies will be conducted in this area."}], "references": [{"title": "Software defect prediction using semi-supervised learning with dimension reduction", "author": ["H. Lu", "B. Cukic", "M. Culp"], "venue": "Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting memory leaks through introspective dynamic behavior modelling using machine learning", "author": ["S. Lee", "C. Jung", "S. Pande"], "venue": "Proceedings of 36th International Conference on Software Engineering, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining the execution history of a software system to infer the best time for its adaptation", "author": ["K. Canavera", "N. Esfahani", "S. Malek"], "venue": "Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E. Barr", "Z. Su", "M. Gabel", "P. Devanbu"], "venue": "Proceedings of 34th International Conference on Software Engineering, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["G. Dahl", "A. Mohamed", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine learning, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1\u201340, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature-based detection of bugs in clones", "author": ["D. Steidl", "N. Gode"], "venue": "7th International Workshop on Software Clones, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Syntax tree fingerprinting for source code similarity detection", "author": ["M. Chilowicz", "E. Duris", "G. Roussel"], "venue": "Proceedings of IEEE 17th International Conference on Program Comprehension, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "WordNet: a lexical database for English", "author": ["G. Miller"], "venue": "Communications of the ACM, vol. 38, no. 11, pp. 39\u201341, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "What\u2019s the code?: Automatic classification of source code archives", "author": ["S. Ugurel", "R. Krovetz", "L. Giles"], "venue": "Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K. Murphy"], "venue": "MIT press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Clone detection in software source code using operational similarity of statements", "author": ["R. Kaur", "S. Singh"], "venue": "ACM SIGSOFT Software Engineering Notes, vol. 39, no. 3, pp. 1\u20135, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Source code retrieval using sequence based similarity", "author": ["Y. Udagawa"], "venue": "International Journal of Data Mining & Knowledge Management Process, no. 4, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimizing a search-based code recommendation system", "author": ["N. Murakami", "H. Masuhara"], "venue": "3rd International Workshop on Recommendation Systems for Software Engineering, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine learning, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E. Huang", "R. Socher", "C. Manning", "A. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Studying the language and structure in non-programmers\u2019 solutions to programming problems", "author": ["J. Pane", "C. Ratanamahatana", "B. Myers"], "venue": "International Journal of Human-Computer Studies, vol. 54, no. 2, pp. 237\u2013264, 2001.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1828}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303\u2013314, 1989.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1989}, {"title": "On the power of small-depth threshold circuits", "author": ["J. Hastad", "M. Goldmann"], "venue": "Computational Complexity, vol. 1, no. 2, pp. 113\u2013129, 1991.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1991}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": "Proceedings of International Conference on Artificial Neural Networks, 1995.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1995}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Clone detection using abstract syntax trees", "author": ["I. Baxter", "A. Yahin", "L. Moura", "M. Sant\u2019Anna", "L. Bier"], "venue": "Proceedings of the International Conference on Software Maintenance, 1998.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Clone detection algorithm based on the Abstract Syntax Tree approach", "author": ["F. Lazar", "O. Banias"], "venue": "Proceedings of 9th IEEE International Symposium on Applied Computational Intelligence and Informatic, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized vulnerability extrapolation using abstract syntax trees", "author": ["F. Yamaguchi", "M. Lottmann", "K. Rieck"], "venue": "Proceedings of 28th Annual Computer Security Applications Conference, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop, 2013.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E. Huang", "J. Pennin", "C. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K. Hermann", "P. Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to order things", "author": ["W. Cohen", "R. Schapire", "Y. Singer"], "venue": "Advances in Neural Information Processing Systems, 1998.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1998}, {"title": "Computing semantic relatedness using Wikipedia-based explicit semantic analysis.", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2007}, {"title": "Comparison and evaluation of code clone detection techniques and tools: A qualitative approach", "author": ["C. Roy", "J. Cordy", "R. Koschke"], "venue": "Science of Computer Programming, vol. 74, no. 7, pp. 470\u2013495, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data", "author": ["R. Feldman", "J. Sanger"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Mining succinct and high-coverage API usage patterns from source code", "author": ["J. Wang", "Y. Dang", "H. Zhang", "K. Chen", "T. Xie", "D. Zhang"], "venue": "Proceedings of IEEE Working Conference on Mining Software Repositories, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining API patterns as partial orders from source code: From usage scenarios to specifications", "author": ["M. Acharya", "T. Xie", "J. Pei", "J. Xu"], "venue": "Proc. of ESEC/SIGSOFT FSE, 2007.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning transformational invariants from natural movies", "author": ["C. Cadieu", "B. Olshausen"], "venue": "Advances in Neural Information Processing Systems, 2008.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Slow, decorrelated features for pretraining complex cell-like networks", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "A.N.C. Manning"], "venue": "Advances in Neural Information Processing Systems, 2013.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Leveraging test generation and specification mining for automated bug detection without false positives", "author": ["M. Pradel", "T. Gross"], "venue": "Proceeings of 24th International Conference on Software Engineering, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Machine learning-based program analysis has been studied long in the literature [1], [2], [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "compare programming languages to natural languages and conclude that programs also have rich statistical properties [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "The deep neural network, also known as deep learning, has become one of the prevailing machine learning approaches since 2006 [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 130, "endOffset": 133}, {"referenceID": 9, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "It has made significant breakthroughs in a variety of fields, such as natural language processing [6], [7], image processing [8], [9], speech recognition [10], [11], etc.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Interestingly, with even a unified model, deep learning achieves better performance than state-of-the-art approaches in many heterogeneous tasks [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "Since no proper \u201cpretraining\u201d method is proposed for programs, deep neural networks cannot be trained effectively with pure back propagation [13], [14], [15] because gradients would either vanish or blow up through the deep architecture [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 16, "context": ", [17] for bug detection, [18] for clone detection.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": ", [17] for bug detection, [18] for clone detection.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "report\u2014for example, in the field of natural language processing (NLP)\u2014the automatically learned taxonomy of words [19] is better in their application than the famous WordNet constructed by experts [20] used in [21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 21, "context": "Such deep learning architectures require less human engineering than existing methods like [22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "As pointed out in [23], \u201cmany decision problems can be reduced to classification.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 207, "endOffset": 211}, {"referenceID": 25, "context": "\u2022 bug detection as [17], to which the deep learning approach is to automatically extract features of bugs; \u2022 clone detection as [24], to automatically match the features of two programs; \u2022 code retrieval as [25], to automatically match program features with that of the queries; and \u2022 code recommendation as [26], to automatically predict the probability of the next possible codes, e.", "startOffset": 308, "endOffset": 312}, {"referenceID": 26, "context": "[27] in NLP).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Chances are that we end up with both poor optimization and poor generalization if the network is deep [13], [14], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 104, "endOffset": 107}, {"referenceID": 28, "context": "Hence, many researches focus on the problem of representation learning per se, such as [27], [28], [5], [7], [29] in fields like NLP.", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "However, due to the structural differences between natural languages and programming languages [30], existing representation learning algorithms in NLP are improper for programs.", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "Comprehensive reviews include [31], [32].", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "Comprehensive reviews include [31], [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "It can be proved that a 2-layer2 neural network with sufficient hidden units can approximate arbitrary Boolean or continuous functions, and that a 3-layer network can approximate any function [34].", "startOffset": 192, "endOffset": 196}, {"referenceID": 33, "context": "However, the shallow architecture is inefficient because the number of hidden units may grow exponentially in order to learn complicated (highly non-linear) features of data [35].", "startOffset": 174, "endOffset": 178}, {"referenceID": 30, "context": "The theory of circuits suggests deep architectures would be more efficient to capture complicated features [31].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Few successful researches were reported in early years using deep architectures (except convolutional neural networks [37]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "proposed stacked restricted Boltzmann machine (RBM) as a greedy layer-wise pretraining method for deep neural networks [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 12, "context": "Shortly after that, stacked autoencoders are used for pretraining [13], the criterion of which is to minimize the reconstruction error.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "According to the experiments reported in [13], [14], [15], pretraining helps optimization (minimizing the training error) as well as generalization (minimizing the test error).", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "In [27], they predict the probability of each word given n\u22121 previous words.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "introduce 3 energybased models in [28], where they learn word representations by minimizing the energy (maximizing the likelihood) defined on neighboring words.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "In [21], [19], hierarchical architectures are proposed to reduce the computational cost in calculating the probabilities.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [21], [19], hierarchical architectures are proposed to reduce the computational cost in calculating the probabilities.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "Fast algorithms are then proposed in [38], [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "Fast algorithms are then proposed in [38], [39].", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "Recurrent neural network (RNN) is introduced in order to capture long-term dependencies [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "However, RNN may be very difficult to train since the gradient would either vanish or blow up during back propagation [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "Although some researches explore character-level modeling for NLP [41], it is improper for programming languages.", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 40, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "This level is also used in traditional program analysis like code clone detection [42], [43], vulnerability extrapolation [44], etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 42, "context": "Such researches in NLP is often referred to as compositional semantics [45].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "This is referred to as \u201cdisentangling the underlying factors of variation\u201d in [32].", "startOffset": 78, "endOffset": 82}, {"referenceID": 43, "context": "To solve this problem, one extreme is to apply dynamic pooling [46], [47].", "startOffset": 63, "endOffset": 67}, {"referenceID": 44, "context": "To solve this problem, one extreme is to apply dynamic pooling [46], [47].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "This is also mathematically equivalent to the continuous bagof-words model [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 42, "context": "Another extreme is to assign a different matrix parameter for each different position [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 55, "endOffset": 59}, {"referenceID": 42, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 61, "endOffset": 65}, {"referenceID": 45, "context": "To solve the problem, negative sampling can be applied [12], [45], [48].", "startOffset": 67, "endOffset": 71}, {"referenceID": 46, "context": "Hence, negative sampling method is also sometimes referred to as the pairwise ranking criterion [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Even though heuristic metrics can also be used to measure similarity\u2014like [50] in NLP and [18], [24] in program analysis, which may be useful for code clone detection [51], code retrieval [25]\u2014they fail to capture different aspects of the relationships between different symbols because the similarity is the only outcome of these metrics.", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "The fact that unsupervised pretraining improves supervised learning is also reported in [13], [14], [15], where RBMs and autoencoders are used as pretraining methods for generic data (mainly the MNIST handwritten digit dataset in these papers).", "startOffset": 100, "endOffset": 104}, {"referenceID": 49, "context": "In these baseline methods, we adopt the bag-of-words model, which is a widely-used approach in text classification [52].", "startOffset": 115, "endOffset": 119}, {"referenceID": 50, "context": ", API usage pattern mining [53], [54].", "startOffset": 27, "endOffset": 31}, {"referenceID": 51, "context": ", API usage pattern mining [53], [54].", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "Interestingly, as a bionics-inspired model, deep CNN achieved unexpected high performance [37] before pretraining methods were invented.", "startOffset": 90, "endOffset": 94}, {"referenceID": 52, "context": "Another widely-used domain specific prior in deep learning is slowness [55], [56].", "startOffset": 71, "endOffset": 75}, {"referenceID": 53, "context": "Another widely-used domain specific prior in deep learning is slowness [55], [56].", "startOffset": 77, "endOffset": 81}, {"referenceID": 54, "context": "[57] is an example of neural reasoning for knowledge base.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "For programs, even though all non-trivial properties are undecidable, formal methods can be viewed as an approximation with pure mathematical deduction, often giving the guarantee of either no false-positive, or no false-negative, which may be important to program analysis [58].", "startOffset": 274, "endOffset": 278}], "year": 2014, "abstractText": "Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the \u201ccoding criterion\u201d to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than \u201cshallow\u201d methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future.", "creator": "LaTeX with hyperref package"}}}