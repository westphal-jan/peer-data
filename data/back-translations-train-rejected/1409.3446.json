{"id": "1409.3446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2014", "title": "Consensus-Based Modelling using Distributed Feature Construction", "abstract": "A particularly successful role for Inductive Logic Programming (ILP) is as a tool for discovering useful relational features for subsequent use in a predictive model. Conceptually, the case for using ILP to construct relational features rests on treating these features as functions, the automated discovery of which necessarily requires some form of first-order learning. Practically, there are now several reports in the literature that suggest that augmenting any existing features with ILP-discovered relational features can substantially improve the predictive power of a model. While the approach is straightforward enough, much still needs to be done to scale it up to explore more fully the space of possible features that can be constructed by an ILP system. This is in principle, infinite and in practice, extremely large. Applications have been confined to heuristic or random selections from this space. In this paper, we address this computational difficulty by allowing features to be constructed in a distributed manner. That is, there is a network of computational units, each of which employs an ILP engine to construct some small number of features and then builds a (local) model. We then employ a consensus-based algorithm, in which neighboring nodes share information to update local models. For a category of models (those with convex loss functions), it can be shown that the algorithm will result in all nodes converging to a consensus model. In practice, it may be slow to achieve this convergence. Nevertheless, our results on synthetic and real datasets that suggests that in relatively short time the \"best\" node in the network reaches a model whose predictive accuracy is comparable to that obtained using more computational effort in a non-distributed setting (the best node is identified as the one whose weights converge first).", "histories": [["v1", "Thu, 11 Sep 2014 14:11:02 GMT  (216kb,D)", "http://arxiv.org/abs/1409.3446v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haimonti dutta", "ashwin srinivasan"], "accepted": false, "id": "1409.3446"}, "pdf": {"name": "1409.3446.pdf", "metadata": {"source": "CRF", "title": "Consensus-Based Modelling using Distributed Feature Construction", "authors": ["Haimonti Dutta", "Ashwin Srinivasan"], "emails": ["haimonti@buffalo.edu", "ashwin@iiitd.ac.in"], "sections": [{"heading": null, "text": "? A short 6-page version of this paper was presented at the 24th International Conference on Inductive Logic Programming, held in conjunction with the ECML PKDD, France. In addition, a substantial portion of the work in this paper was authored when the first author was an associate scientist at the Center for Computational Learning Systems (CCLS) at Columbia University, NY.ar Xiv: 140 9.34 46v1 [cs.LG] 1 1 1"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to survive on their own, without there being a process in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, and a process, a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, and a process, respectively, and, respectively, respectively, and, and, respectively, respectively, and, respectively, and, respectively, respectively, respectively, and"}, {"heading": "3 Consensus-Based Model Construction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Description", "text": "Let M have a n \u00b7 m matrix with real rated entries. This matrix represents a data set of n tuples of the form Xi \u00b7 Rm, 1 \u2264 i \u2264 n. Suppose, without loss of generality, this data set was \u00b7 \u00b7 uniquely about k sites S1, S2, \u00b7 \u00b7 \u00b7, weight | \u00b7, SkInput: n \u00b7 mi matrix at each site Si, G (V, E), which includes the underlying communication framework \u00b7 \u00b7 \u00b7 none of the iterations Output: Each site Si has Wi \u2248 Wg [1: mi] for t = 1 to T do (a) Site Si calculates MiW T i locally and estimates the loss function; (b) Site Si gossips with its neighbors Sj \u0432 {Ni} and receives MjWTj for each neighbor; (c) Site Si locally updates its functional estimate as JTI (MiW T i i) +."}, {"heading": "3.2 Algorithm", "text": "The distributed algorithm evolves over discrete time in relation to a \"global\" clock. In addition, each site has its own memory and can perform local calculations (such as calculating the gradient on its local attributes). It stores fi, which is the estimated local function. In addition to its own calculation, sites can receive messages from their neighbors to help evaluate local functionality. Assumption 2: Communication protocols are interconnected. Sites Si are represented by another underlying communication framework represented by a graphical G (V, E), so that each page Si (Si), \u00b7 \u00b7 \u00b7 \u00b7 Si are connected to another page."}, {"heading": "3.3 Convergence", "text": "The proof of convergence of the algorithm makes use of the following concept: In the distributed setting, the process of information exchange between k sites can be modeled as a non-stationary Markov chain. A non-stationary Markov chain is weakly ergodic when dependence on state distribution disappears as time tends toward infinity [60]. A detailed discussion of the convergence of the algorithm is given here.First, we make the following assumptions about the cost function J: Assumption 3: 1. There is J (W t): 0, for each W t: 1. Rm: 2. Lipschitz convergence of convergence J: The function J is continuously differentiable and there is a constant K1, so that there is such a constant that J (W t1) \u2212 J (W t2)."}, {"heading": "4 Empirical Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Aims", "text": "Our goal is to empirically investigate the utility of the consensus-based algorithm we describe. We use model (k, f) to designate the model that returned the consensus-based algorithm in Section 3.2 using k nodes in a network, each of which can rely on an ILP engine to construct at most f-features. In this section, we compare the performance of: model (N, F) (N > 1) with model (1, N \u00b7 F) The latter effectively represents the non-distributed model, where all the features are present on a single centralized node. For convenience, we call the former the distributed model and the latter the centralized model.We intend to investigate whether there is empirical support for the assumption that the performance of the distributed model is better than that of the centralized model. We assume that the performance of a model-construction method by the pair (A), T is given by ascertaining the preestimation of the time (whereby A is an inaccuracy of the predetermined 1)."}, {"heading": "4.2 Materials", "text": "The data for experiments are divided into two categories: 1. Synthetic. We use the problem of R. Michalski's \"traits\" for controlled experiments. Data sets of 1000 examples are obtained for randomly drawn target concepts (see \"Methods\" below).6 For this, we use S. H. Muggleton's Random Generator 7, which defines a random process for generating examples. We will use this data for controlled experiments to test general assumptions about the comparative performance of distributed and centralized models. 2. Real. We report on results of experiments conducted with some well-studied real-world biochemical toxicology problems (mutagenesis [31]; carcinogenesis [32]; and DssTox [?]). Our purpose in investigating the performance of real data is twofold. First, we intend to see if the use of linear models is too restrictive for real-world problems."}, {"heading": "4.3 Method", "text": "In fact, it is in such a way that it is a matter of a manner in which it is a matter of a manner in which it is a matter of a manner in which it is a matter of a manner and in which it is a matter of a manner and in which it is a matter of a manner and in which it is a matter of a manner and in which it is a matter of a manner and in which it is a matter of a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about"}, {"heading": "4.4 Results", "text": "First, we present the main results from the experiments based on synthetic data (shown in Fig.3). The primary observations in these experiments are as follows: (1) on average, because concepts vary, the distributed algorithm appears to achieve higher accuracies than the centralized approach, although the differences for a randomly selected concept cannot be significant; (2) on average, because concepts vary, the time taken to model through the distributed approach appears to be substantially lower than before; and (3) the variation in both accuracies and the time with the distributed approach due to both changes in the concept or due to repetitions of the feature construction appears to be less than the centralized approach. Together, these results suggest that good, stable models can be obtained relatively quickly from the distributed approach, and that the approach could be an efficient alternative to a centralized approach in which all features are constructed through a uniform computer-based unit."}, {"heading": "5 Conclusion", "text": "A particularly effective form of inductive logic programming is its use to construct new functions that can be used to supplement existing descriptors of a dataset. Experimental studies published in the literature have repeatedly shown that the relationship characteristics constructed by an ILP engine can significantly contribute to the analysis of data. Models constructed in this way considered both classification as well as regression and improvements in each case. However, practical difficulties remained. However, the rich language of firststorer logic used by ILP systems generates a very wide range of possible new characteristics. The resulting computer-related difficulties in finding interesting characteristics are not easily overcome by the usual ILP-based methods of speech distortion or limitations. In this paper, we have introduced the first attempt to use a distributed algorithm for the selection of characteristics in the ILP, which also has verifiable guarantees of convergence."}, {"heading": "Acknowledgements", "text": "H.D. is also an adjunct assistant professor at the Institute of Computer Science of the IIIT in Delhi and an associate member of the Institute of Data Sciences of Columbia University in New York. A.S. has also held guest positions at the School of CSE of the University of New South Wales in Sydney and at the Department of Computer Science of Oxford University in Oxford."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["A. Agarwal", "O. Chapelle", "M. Dud\u0301\u0131k", "J. Langford"], "venue": "Journal of Machine Learning Research 15, 1111\u20131133", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining sequential patterns", "author": ["R. Agrawal", "R. Srikant"], "venue": "ICDE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Generalization of pattern-growth methods for sequential pattern mining with gap constraints", "author": ["C. Antunes", "A.L. Oliveira"], "venue": "MLDM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "bitspade: A lattice-based sequential pattern mining algorithm using bitmap representation", "author": ["S. Aseervatham", "A. Osmani", "E. Viennet"], "venue": "ICDM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequential pattern mining using a bitmap representation", "author": ["J. Ayres", "J. Gehrke", "T. Yiu", "J. Flannick"], "venue": "KDD", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Order-optimal consensus through randomized path averaging", "author": ["F. Benezit", "A. Dimakis", "P. Thiran", "M. Vetterli"], "venue": "Information Theory, IEEE Transactions on 56(10), 5150\u20135167", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific, Belmont, MA.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Y. Lechevallier, G. Saporta (eds.) Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT\u20192010), pp. 177\u2013187. Springer, Paris, France", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Optimization for Machine Learning, pp. 351\u2013368. MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Optimization for Machine Learning, pp. 351\u2013368. MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Randomized gossip algorithms", "author": ["S. Boyd", "A. Ghosh", "B. Prabhakar", "D. Shah"], "venue": "IEEE/ACM Trans. Netw. 14(SI), 2508\u20132530", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn. 3(1), 1\u2013122", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "The snow learning architecture", "author": ["A. Carlson", "C. Cumby", "J. Rosen", "D. Roth"], "venue": "Tech. Rep. UIUCDCS-R-99-2101, UIUC Computer Science Department", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Identification of class specific discourse patterns", "author": ["A. Chalamalla", "S. Negi", "L.V. Subramaniam", "G. Ramakrishnan"], "venue": "CIKM, pp. 1193\u20131202", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised distributed feature selection for multi-view object recognition", "author": ["C. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "Tech. Rep. MIT-CSAIL-TR-2008-009, MIT, CSAIL", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Dynamic load balancing for distributed memory multiprocessors", "author": ["G. Cybenko"], "venue": "Proceedings of the Journal of Parallel and Distributed Computing 7, pp. 279\u2013301", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "Note on learning rate schedules for stochastic optimization", "author": ["C. Darken", "J. Moody"], "venue": "Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems 3, NIPS-3, pp. 832\u2013838", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks", "author": ["K. Das", "K. Bhaduri", "H. Kargupta"], "venue": "Knowl. Inf. Syst. 24(3), 341\u2013367", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "J. Mach. Learn. Res. 13(1), 165\u2013202", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Geographic gossip: efficient aggregation for sensor networks", "author": ["A. Dimakis", "A. Sarwate", "M. Wainwright"], "venue": "Information Processing in Sensor Networks, 2006. IPSN 2006. The Fifth International Conference on, pp. 69\u201376", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling", "author": ["J. Duchi", "A. Agarwal", "M. Wainwright"], "venue": "IEEE Transactions on Automatic Control 57(3), 592\u2013606", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Spirit: Sequential pattern mining with regular expression constraints", "author": ["M.N. Garofalakis", "R. Rastogi", "K. Shim"], "venue": "VLDB", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "An l1 regularization framework for optimal rule combination", "author": ["Y. Han", "J. Wang"], "venue": "ECML/PKDD", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient rule ensemble learning using hierarchical kernels", "author": ["P. Jawanpuria", "J.S. Nath", "G. Ramakrishnan"], "venue": "ICML, pp. 161\u2013168", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "The peer sampling service: Experimental evaluation of unstructured gossip-based implementations", "author": ["M. Jelasity", "R. Guerraoui", "A.M. Kermarrec", "M. Steen"], "venue": "Middleware 2004, Lecture Notes in Computer Science, vol. 3231, pp. 79\u201398", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Gossip-based aggregation in large dynamic networks", "author": ["M. Jelasity", "A. Montresor", "\u00d6. Babaoglu"], "venue": "ACM Trans. Comput. Syst. 23(3), 219\u2013252", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining minimal distinguishing subsequence patterns with gap constraints", "author": ["X. Ji", "J. Bailey", "G. Dong"], "venue": "Knowledge and Information Systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "PROCEEDINGS OF THE ELEVENTH INTERNATIONAL Conference on Machine Learning, pp. 121\u2013129. Morgan Kaufmann", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature construction using theoryguided sampling and randomised search", "author": ["S. Joshi", "G. Ramakrishnan", "A. Srinivasan"], "venue": "ILP, pp. 140\u2013157", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Gossip-based computation of aggregate information", "author": ["D. Kempe", "A. Dobra", "J. Gehrke"], "venue": "Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pp. 482\u2013491", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Structure-activity relationships derived by machine learning: the use of atoms and their bond connectivities to predict mutagenicity by inductive logic programming", "author": ["R.D. King", "S.H. Muggleton", "A. Srinivasan", "M.J. Sternberg"], "venue": "Proceedings of the National Academy of Sciences of the United States of America 93(1), 438\u201342", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming", "author": ["R.D. King", "A. Srinivasan"], "venue": "Environmental Health Perspectives 104, pp. 1031\u20131040", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "An application of boosting to graph classification", "author": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Slow learners are fast", "author": ["J. Langford", "A. Smola", "M. Zinkevich"], "venue": "Advances in Neural Information Processing Systems 22, pp. 2331\u20132339", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Inductive inference of vl decision rules", "author": ["J. Larson", "R.S. Michalski"], "venue": "SIGART Bull. (63), 38\u201344", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1977}, {"title": "Feature Selection for Knowledge Discovery and Data Mining", "author": ["H. Liu", "H. Motoda"], "venue": "Kluwer Academic Publishers, Norwell, MA, USA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel gradient distribution in unconstrained optimization", "author": ["L. Mangasarian"], "venue": "SIAM Journal on Control and Optimization 33(6), 1916\u20131925", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1995}, {"title": "Distributed training strategies for the structured perceptron", "author": ["R. McDonald", "K. Hall", "G. Mann"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910, pp. 456\u2013464. Association for Computational Linguistics, Stroudsburg, PA, USA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "PeerSim: A scalable P2P simulator", "author": ["A. Montresor", "M. Jelasity"], "venue": "Proc. of the 9th Int. Conference on Peer-to-Peer (P2P\u201909), pp. 99\u2013100. Seattle, WA", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards efficient named-entity rule induction for customizability", "author": ["A. Nagesh", "G. Ramakrishnan", "L. Chiticariu", "R. Krishnamurthy", "A. Dharkar", "P. Bhattacharyya"], "venue": "EMNLP-CoNLL, pp. 128\u2013138", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Rule ensemble learning using hierarchical kernels in structured output spaces", "author": ["N. Nair", "A. Saha", "G. Ramakrishnan", "S. Krishnaswamy"], "venue": "AAAI", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "Advances in Neural Information Processing Systems 24, 693\u2013701", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative subsequence mining for action classification", "author": ["S. Nowozin", "G. Bakr", "K. Tsuda"], "venue": "CVPR", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Mining sequential patterns by pattern-growth: The prefixspan approach", "author": ["J. Pei"], "venue": "Journal of Machine Learning Research 16-11", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Constraint-based sequential pattern mining: the pattern-growth methods", "author": ["J. Pei", "J. Han", "W. Wang"], "venue": "Journal of Intelligent Information Systems", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "From sequential pattern mining to structured pattern mining: A pattern-growth approach", "author": ["J. Pei", "J. Han", "X.F. Yan"], "venue": "Journal of Computer Science and Technology 9(3), 257\u2013279", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Using ilp to construct features for information extraction from semi-structured text", "author": ["G. Ramakrishnan", "S. Joshi", "S. Balakrishnan", "A. Srinivasan"], "venue": "ILP, pp. 211\u2013224", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to resolve natural language ambiguities: A unified approach", "author": ["D. Roth"], "venue": "Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence, AAAI \u201998/IAAI \u201998, pp. 806\u2013813", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "What kinds of relational features are useful for statistical learning", "author": ["A. Saha", "A. Srinivasan", "G. Ramakrishnan"], "venue": "ILP", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "On the probability of large deviations of random variables", "author": ["I.N. Sanov"], "venue": "Mat. Sbornik, vol. 42, pp. 11\u201344", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1957}, {"title": "Gossip algorithms", "author": ["D. Shah"], "venue": "Found. Trends Netw. 3(1), 1\u2013125", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "An investigation into feature construction to assist word sense disambiguation", "author": ["L. Specia", "A. Srinivasan", "S. Joshi", "G. Ramakrishnan", "M. Graas Volpe Nunes"], "venue": "Machine Learning 76(1), 109\u2013136", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Word sense disambiguation using inductive logic programming", "author": ["L. Specia", "A. Srinivasan", "G. Ramakrishnan", "M. das Gra\u00e7as Volpe Nunes"], "venue": "ILP, pp. 409\u2013423", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "The aleph manual", "author": ["A. Srinivasan"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1999}, {"title": "An empirical study of on-line models for relational data streams", "author": ["A. Srinivasan", "M. Bain"], "venue": "Tech. Rep. 201401, School of Computer Science and Engineering, UNSW", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Parameter screening and optimisation for ilp using designed experiments", "author": ["A. Srinivasan", "G. Ramakrishnan"], "venue": "Journal of Machine Learning Research 12, 627\u2013662", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "An introduction to measure theory", "author": ["T. Tao"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Problems in decentralized decision making and computation", "author": ["J.N. Tsitsiklis"], "venue": "Ph.D. thesis, Department of EECS, MIT", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1984}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans"], "venue": "IEEE Transactions on Automatic Control, vol. AC-31", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1986}, {"title": "Matrix Iterative Analysis", "author": ["R. Varga"], "venue": "Prentice Hall, Englewood Cliffs, NJ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1962}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "NIPS, vol. 4, p. 4", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Nevertheless, there is now a growing body of research that suggests that augmenting any existing features with ILPconstructed relational ones can substantially improve the predictive power of a statistical model (see, for example: [29, 49, 52, 47, 53]).", "startOffset": 231, "endOffset": 251}, {"referenceID": 48, "context": "Nevertheless, there is now a growing body of research that suggests that augmenting any existing features with ILPconstructed relational ones can substantially improve the predictive power of a statistical model (see, for example: [29, 49, 52, 47, 53]).", "startOffset": 231, "endOffset": 251}, {"referenceID": 51, "context": "Nevertheless, there is now a growing body of research that suggests that augmenting any existing features with ILPconstructed relational ones can substantially improve the predictive power of a statistical model (see, for example: [29, 49, 52, 47, 53]).", "startOffset": 231, "endOffset": 251}, {"referenceID": 46, "context": "Nevertheless, there is now a growing body of research that suggests that augmenting any existing features with ILPconstructed relational ones can substantially improve the predictive power of a statistical model (see, for example: [29, 49, 52, 47, 53]).", "startOffset": 231, "endOffset": 251}, {"referenceID": 52, "context": "Nevertheless, there is now a growing body of research that suggests that augmenting any existing features with ILPconstructed relational ones can substantially improve the predictive power of a statistical model (see, for example: [29, 49, 52, 47, 53]).", "startOffset": 231, "endOffset": 251}, {"referenceID": 34, "context": "An illustrative example of the consensus-based approach using Michalski\u2019s \u201cTrains\u201d problem [35].", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "Techniques for selecting from a (large) but finite set of features of known size d has been well-studied within the machine learning, usually under the umbrellaterms of filter-based or wrapper-based methods (see for example, [28, 36]).", "startOffset": 225, "endOffset": 233}, {"referenceID": 35, "context": "Techniques for selecting from a (large) but finite set of features of known size d has been well-studied within the machine learning, usually under the umbrellaterms of filter-based or wrapper-based methods (see for example, [28, 36]).", "startOffset": 225, "endOffset": 233}, {"referenceID": 58, "context": "Early work in decentralized optimization was marked by interest in consensus-based learning, distributed optimization and minimization with the seminal work of Bertsekas, Tsitsiklis and colleagues ([60, 59, 7]).", "startOffset": 198, "endOffset": 209}, {"referenceID": 57, "context": "Early work in decentralized optimization was marked by interest in consensus-based learning, distributed optimization and minimization with the seminal work of Bertsekas, Tsitsiklis and colleagues ([60, 59, 7]).", "startOffset": 198, "endOffset": 209}, {"referenceID": 6, "context": "Early work in decentralized optimization was marked by interest in consensus-based learning, distributed optimization and minimization with the seminal work of Bertsekas, Tsitsiklis and colleagues ([60, 59, 7]).", "startOffset": 198, "endOffset": 209}, {"referenceID": 10, "context": "More recently, researchers have shown that convergence properties of these decentralized algorithms can be related to the network topology by using spectral properties of random walks or path averaging arguments on the underlying graph structure ([11, 51, 20, 6]).", "startOffset": 247, "endOffset": 262}, {"referenceID": 50, "context": "More recently, researchers have shown that convergence properties of these decentralized algorithms can be related to the network topology by using spectral properties of random walks or path averaging arguments on the underlying graph structure ([11, 51, 20, 6]).", "startOffset": 247, "endOffset": 262}, {"referenceID": 19, "context": "More recently, researchers have shown that convergence properties of these decentralized algorithms can be related to the network topology by using spectral properties of random walks or path averaging arguments on the underlying graph structure ([11, 51, 20, 6]).", "startOffset": 247, "endOffset": 262}, {"referenceID": 5, "context": "More recently, researchers have shown that convergence properties of these decentralized algorithms can be related to the network topology by using spectral properties of random walks or path averaging arguments on the underlying graph structure ([11, 51, 20, 6]).", "startOffset": 247, "endOffset": 262}, {"referenceID": 20, "context": "Learning feature subsets in distributed environments using decentralized optimization has become an active area of research ([21, 1, 15]) in recent years.", "startOffset": 125, "endOffset": 136}, {"referenceID": 0, "context": "Learning feature subsets in distributed environments using decentralized optimization has become an active area of research ([21, 1, 15]) in recent years.", "startOffset": 125, "endOffset": 136}, {"referenceID": 14, "context": "Learning feature subsets in distributed environments using decentralized optimization has become an active area of research ([21, 1, 15]) in recent years.", "startOffset": 125, "endOffset": 136}, {"referenceID": 0, "context": "[1] present a system and a set of techniques for learning linear predictors with convex losses on terabyte sized datasets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] present a dual averaging sub-gradient method which maintains and forms weighted averages of sub-gradients in the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Similar techniques for learning linear predictors have been presented elsewhere ([37, 38, 62], [42, 12]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 37, "context": "Similar techniques for learning linear predictors have been presented elsewhere ([37, 38, 62], [42, 12]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 60, "context": "Similar techniques for learning linear predictors have been presented elsewhere ([37, 38, 62], [42, 12]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 41, "context": "Similar techniques for learning linear predictors have been presented elsewhere ([37, 38, 62], [42, 12]).", "startOffset": 95, "endOffset": 103}, {"referenceID": 11, "context": "Similar techniques for learning linear predictors have been presented elsewhere ([37, 38, 62], [42, 12]).", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "In addition, this is a batch algorithm and hence quite different from distributed online learning counterparts ([19, 34, 9]).", "startOffset": 112, "endOffset": 123}, {"referenceID": 33, "context": "In addition, this is a batch algorithm and hence quite different from distributed online learning counterparts ([19, 34, 9]).", "startOffset": 112, "endOffset": 123}, {"referenceID": 8, "context": "In addition, this is a batch algorithm and hence quite different from distributed online learning counterparts ([19, 34, 9]).", "startOffset": 112, "endOffset": 123}, {"referenceID": 17, "context": "[18] show that three popular feature selection criteria \u2013 misclassification gain, gini index and entropy can be learnt in a large peer-to-peer network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 10, "endOffset": 22}, {"referenceID": 42, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 10, "endOffset": 22}, {"referenceID": 32, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 10, "endOffset": 22}, {"referenceID": 28, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 48, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 39, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 13, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 51, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 46, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 52, "context": "Optimally [23, 43, 33] or heuristically [29, 49, 40, 14, 52, 47, 53] solve a discrete optimization problem.", "startOffset": 40, "endOffset": 68}, {"referenceID": 23, "context": "Optimally [24, 41] solve a convex optimization problem with sparsity inducing regularizers; 3.", "startOffset": 10, "endOffset": 18}, {"referenceID": 40, "context": "Optimally [24, 41] solve a convex optimization problem with sparsity inducing regularizers; 3.", "startOffset": 10, "endOffset": 18}, {"referenceID": 43, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 26, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 3, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 2, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 44, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 1, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 45, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 4, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 21, "context": "Compute all relational features that satisfy some quality criterion by systematically and efficiently exploring a prescribed search space [44, 27, 4, 3, 45, 2, 46, 5, 22].", "startOffset": 138, "endOffset": 170}, {"referenceID": 28, "context": "The latter is not the case for a technique like the one proposed in [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 47, "context": "Perhaps of most interest to the work here is the Sparse Network Of Winnow classifers described in [48, 13].", "startOffset": 98, "endOffset": 106}, {"referenceID": 12, "context": "Perhaps of most interest to the work here is the Sparse Network Of Winnow classifers described in [48, 13].", "startOffset": 98, "endOffset": 106}, {"referenceID": 54, "context": "Finally, from the ILP-viewpoint, [55] shows how it is possible to construct Winnow-based models in an infinite-attribute setting using an ILP engine with a stream-based model of the data.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "Taken together, this suggests that a combination of the techniques we propose, and those in [13] can be used to develop linear models that can handle both horizontal partitioning of the data and vertical partitioning of the feature-space.", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "The choice of \u03b1il may be deterministic or randomized and may or may not depend on the time t [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 59, "context": "Theorem 1 Perron-Frobenius [61] Let A be a positive, irreducible matrix such that the rows sum to 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 59, "context": "Since the eigenvalues ofA are bounded by 1, it can be shown that J t i converges to the average function estimate J i if and only if -1 is not an eigen value [61].", "startOffset": 158, "endOffset": 162}, {"referenceID": 59, "context": "If \u03b3 = 1, then system fails to converge [61], [16].", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "If \u03b3 = 1, then system fails to converge [61], [16].", "startOffset": 46, "endOffset": 50}, {"referenceID": 58, "context": "A non-stationary Markov chain is weakly ergodic if the dependence on the state distribution vanishes as time tends to infinity [60].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Assumption 5: Descent Lemma [7] at each site: (a) For every i and t we have,", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Assumption 6: Stochastic Descent Lemma [7] at each site: There exist positive constants K6, K7 and K8 such that: (a) \u2207J(W t i )E[si|S(t)] \u2264 \u2212K6 \u2016 \u2207J(W t i ) \u2016,\u2200t \u2208 T .", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "Assumption 7: Partial Asynchronism [7]) There exists a positive integer B such that: (a) For every i and for every t \u2265 0 at least one of the elements of the set {t, t+ 1, \u00b7 \u00b7 \u00b7 , t+B \u2212 1} belongs to T .", "startOffset": 35, "endOffset": 38}, {"referenceID": 56, "context": "A formal definition (using measure theory [58]) is given below: Let (\u03c3,F , P ) be a probability space.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "Supermartingale Convergence Theorem [7]: Let {Yi} be a sequence of random variables and let {Fi} be a sequence of finite sets of random variables such that Fi \u2282 Fi+1 for each i.", "startOffset": 36, "endOffset": 39}, {"referenceID": 30, "context": "We report results from experiments conducted using some wellstudied real world biochemical toxicology problems (Mutagenesis [31]; Carcinogenesis [32]; and DssTox [?]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 31, "context": "We report results from experiments conducted using some wellstudied real world biochemical toxicology problems (Mutagenesis [31]; Carcinogenesis [32]; and DssTox [?]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "Algorithms and Machines The DFE algorithm has been implemented on a Peer-to-Peer simulator, PeerSim [39].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "The emergent topology from newscast protocol has a very low diameter and is very close to a random graph ([25],[26]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 25, "context": "The emergent topology from newscast protocol has a very low diameter and is very close to a random graph ([25],[26]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 53, "context": "The ILP system used in all experiments is Aleph [54].", "startOffset": 48, "endOffset": 52}, {"referenceID": 55, "context": "The Baseline models are the ones reported in [56] (these are cross-validation estimates, whereas the estimates for Centralised and Distributed models are from holdout sets).", "startOffset": 45, "endOffset": 49}, {"referenceID": 7, "context": "(see for example [8], [10], [17], [57]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "(see for example [8], [10], [17], [57]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "(see for example [8], [10], [17], [57]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "We have some reason to believe that this conjecture may hold in some circumstances, based on the use of Sanov\u2019s theorem [50] and related techniques.", "startOffset": 120, "endOffset": 124}], "year": 2014, "abstractText": "A particularly successful role for Inductive Logic Programming (ILP) is as a tool for discovering useful relational features for subsequent use in a predictive model. Conceptually, the case for using ILP to construct relational features rests on treating these features as functions, the automated discovery of which necessarily requires some form of first-order learning. Practically, there are now several reports in the literature that suggest that augmenting any existing features with ILPdiscovered relational features can substantially improve the predictive power of a model. While the approach is straightforward enough, much still needs to be done to scale it up to explore more fully the space of possible features that can be constructed by an ILP system. This is in principle, infinite and in practice, extremely large. Applications have been confined to heuristic or random selections from this space. In this paper, we address this computational difficulty by allowing features to be constructed in a distributed manner. That is, there is a network of computational units, each of which employs an ILP engine to construct some small number of features and then builds a (local) model. We then employ a consensus-based algorithm, in which neighbouring nodes share information to update local models. For a category of models (those with convex loss functions), it can be shown that the algorithm will result in all nodes converging to a consensus model. In practice, it may be slow to achieve this convergence. Nevertheless, our results on synthetic and real datasets that suggests that in relatively short time the \u201cbest\u201d node in the network reaches a model whose predictive accuracy is comparable to that obtained using more computational effort in a non-distributed setting (the best node is identified as the one whose weights converge first). ? A short 6-page version of this paper was presented at the 24th International Conference on Inductive Logic Programming, held in conjunction with ECML PKDD, France. Furthermore, a significant part of the work in this paper was done when the first author was an Associate Research Scientist at the Center for Computational Learning Systems (CCLS), Columbia University, NY. ar X iv :1 40 9. 34 46 v1 [ cs .L G ] 1 1 Se p 20 14 2 Haimonti Dutta and Ashwin Srinivasan", "creator": "TeX"}}}