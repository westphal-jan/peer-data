{"id": "1409.3653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "On Minimax Optimal Offline Policy Evaluation", "abstract": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.", "histories": [["v1", "Fri, 12 Sep 2014 06:10:15 GMT  (521kb,D)", "http://arxiv.org/abs/1409.3653v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lihong li", "remi munos", "csaba szepesvari"], "accepted": false, "id": "1409.3653"}, "pdf": {"name": "1409.3653.pdf", "metadata": {"source": "CRF", "title": "On Minimax Optimal Offline Policy Evaluation", "authors": ["Lihong Li", "Remi Munos"], "emails": ["lihongli@microsoft.com", "remi.munos@inria.fr", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "This paper examines the non-political evaluation problem of assessing the value of a target policy on the basis of a sample of observations of another policy. First, we consider the case of a multi-armed bandit, determine a lower minimax risk, and analyze the risk of two standard estimators. It is shown and confirmed in the simulation that one is optimal up to a constant minimax, while another, despite its empirical success and popularity, can be arbitrarily inferior. Results are applied to related problems in contextual bandits and Markov decision-making processes with a fixed horizon, and are also related to semi-supervised learning."}, {"heading": "1 Introduction", "text": "It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it is. (it is.) It is. (it is.) It is. (it is. (it is.) It is. (it is.) It is. (it is. (it is.) It is. (it.) It is. (it is. (it is.) It is. (it is. (it.) It is. (it is. (it is.) It is. (it. (it is.) It is. (it. (it is.) It is. (it is. (it.) It is. (it. (it is.). (it is. (it is. (it is.). (it is. (it.) It is. (it. (it.) It is. (it. (it. (it is.). (it is. (it.) It is. (it. (it.). (it is. (it. (it.). (it. (it is.). (it is. (it is. (it.) It is. (it. (it.) It is. (it. (it.). (it. (it is.). (it is.) It is. (it is. (it. (it is.). (it is. (it is. (it.). (it is. (it is.). (it.). (it is. (it.). (it is. (it is.). (it.). (it is. (it. (it. (it.). (it is.). (it is.). (it. (it is."}, {"heading": "2 Multi-armed Bandit", "text": "The data Dn = {(Ai, Ri) 1 \u2264 i \u2264 n are generated by the following process: 1 (Ai, Ri) are independent copies of (A, R), where P (A = a) = \u03c0D (a) and R \u00b2 (\u00b7 | A) are for any unknown family of distributions {1 (Ai, Ri) an \"A\" and known \"E.\" We are also given a known target guideline, and we want to estimate its value: = EA, R \u00b2 (\u00b7 | A) [R] on the basis of the knowledge of Dn, \u03c0D and \u03c0, where the quality of an estimate v \u00b2 is measured on the basis of Dn (and D) on the basis of its average square error, MSE (v \u00b2): V \u00b2 Vector: = E [v \u00b2 vzip \u00b2 \u00b2).Let us define an estimate v \u00b2."}, {"heading": "2.1 A Minimax Lower Bound", "text": "We start with the definition of a Minimax1The data Dn is actually a list, not a sentence. We keep the notation {(Ai, Ri)} 1 \u2264 i \u2264 i \u2264 n for historical reasons. Optimal risk is subject to the assumption that Minimax1The data Dn is a list, not a sentence. We keep the notation {(Ai, Ri)} n for historical reasons. Optimal risk is subject to the number 2 (a) \u2264 2 (a). We consider Minimax1The data Dn is actually a list, not a sentence. (A)"}, {"heading": "2.2 Likelihood Ratio Estimator", "text": "One of the most popular estimators is known in the statistical literature as the estimator of the inclination value (Rosenbaum and Rubin, 1983, 1985) or as the estimator of importance (Bottou et al., 2013). We call him the estimator of the probability value because he estimates the unknown value using probability ratios or weights of importance: v \u00b2 LR (\u03c0, \u03c0D, D n): = 1n n \u00b2 i = 1 (Ai) \u03c0D (Ai) Ri.His distinguishing feature is that he is unbiased: E [v \u00b2 LR (\u03c0, \u03c0D, Dn)] = v\u03c0\u0435, which implies that the MSE is contributed exclusively by the variance of the estimator. The main result in this subsection shows that this estimator does not reach the Minimax lower limit, which is bound to any constant (by forming V2 V1).The proof (given in the appendix) is based on a direct calculation using the total value of the VSE (the VSE), which is greater than the probability (the M2) of the VSE being smaller."}, {"heading": "2.3 Regression Estimator", "text": "For convenience, the REG estimator can also be written in such a way that it problems the reward function and the problem of estimating the reward function as regression. Interestingly, it can be verified by direct calculation, the REG estimator can also be written in such a way that it estimates the reward function. Interestingly, the REG estimator can also be written in such a way that it problems the reward function as regression. Interestingly, the REG estimator can be verified by direct calculation. The REG estimator can also be written in such a way that it problems the reward function and the problem of estimating the reward function as regression. Interestingly, the REG estimator can also be written in such a way."}, {"heading": "2.4 Simulation Results", "text": "This subsection confirms our analysis with simulation results that empirically show the influence of key variables on the MSE of the two estimators. Two sets of experiments are performed that correspond to the left and right ranges in Figure 1. In all experiments, we repeat the process of data generation (with \u03c0D) 10,000 times and calculate the MSE of each estimator. All reward distributions are normal distributions with \u03c32 = 0.01 and different means. We then draw a normalized MSE (MSE multiplied by sample size n) or nMSE, versus n.The first experiment consists of comparing the finite time and asymptotic accuracy of v-LR and v-Reg."}, {"heading": "3 Extensions", "text": "In this section we look at extensions of our previous findings to contextual bandits and Markovian decision-making processes, while implications for semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material."}, {"heading": "3.1 Contextual Bandits", "text": "The problem structure is as follows: In addition to the finite action set A = {1, 2,., K}, we are given a context set X = {1, 2,., M}. A policy is now a map \u03c0: X \u2192 [0, 1] Such a set of guidelines about X and A is given for each x-X number (A), \u03c0 (x), a probability distribution over the action space A (X). The process that generates the data is a probability distribution over the action space A. For the notation-conditional convenience we will p = (Xi, Ai, Ri), n x-number of i-numbers is described as follows: (Xi, Ri), A-numbers are independent copies of (X, A), R) where the X-number of x-numbers A-numbers (A), A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, A-numbers, numbers, numbers, numbers, numbers, numbers, numbers, numbers, A-numbers, numbers, numbers, numbers, numbers, K-numbers, numbers, numbers, numbers, numbers, K-numbers, numbers, numbers, K-numbers, numbers, numbers, K, numbers, numbers, K-numbers, numbers, numbers, K-numbers, K, numbers, numbers, numbers, K-numbers, K-numbers, K-numbers, K-numbers, K-numbers, K-numbers, K-numbers, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K-numbers, K-numbers, K-numbers, K, K-numbers, K-numbers, K-numbers, K, K-numbers, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K-numbers, K, K-numbers, K, K-numbers, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K-numbers, K-numbers, K, K, K-numbers, K, K, K-numbers,"}, {"heading": "3.2 Markov Decision Processes", "text": "Similarly, the results in Section 2 can be naturally extended to a fixed horizon, Markov finite decision-making processes (MDPs) (MDPs). Here, an MDP is defined by a tuple of M = < X, A, P, \u03a6, \u03bd, H >, where X = {1,.., N} is the set of states, A = {1,., K} is the set of actions, P is the transitional core, \u03a6: X \u00b7 A 7 \u2192 R is the reward function, \u03bd is the start-state distribution, and H is the horizon. A policy: X 7 \u2192 [0, 1] K maps states to distributions of actions, and we use \u03c0 (a | x) to determine the probability of an action in state x. In the face of a policy approach (X, A) is a trajectory of length H, denoted T = (X, A, R, R)."}, {"heading": "4 Conclusions", "text": "Although it turned out that the simplest estimator using weights of importance (referred to as LR) is sensitive to the magnitude of weights of importance, it was found that the regression estimator (REG), which estimates the mean rewards for each action, is less exposed to this value. However, while the sensitivity of LR is a \"folkloristic theorem,\" we have not formally seen this result confirmed in the literature. We also found that the REG estimator has different qualities: it is optimal up to a constant minimax, which is the minimum of the square number of actions, K2, and the maximum inverse reward variance. We showed that the dependence on the number of actions in general cannot be removed. There is still a gap of factor K between our lower and upper limits. We suspect that the lower limit has the correct order (which seems to be confirmed by the experiments), while we are not able to evaluate these results."}, {"heading": "A Technical Details", "text": "The appendix collects various results, shown in the main part of the text.A.1 Proof of the second part of theorem 1We provide here a complete proof of the second part of theorem 1. First, we need some background. Let X = (X, A) is a measurable space that is open by the product RK, p (\u00b7). ("}, {"heading": "B Extension to Contextual Bandits", "text": "In this section, we look at an extension of our previous results to finite contextual bandits. As we will soon see, the expansion is seamless. The problem is as follows: In addition to the finite plot A = {1, 2,.., K}, we will also have a context X = {1, 2,., M}. A policy is now a map: X \u2192 [0, 1] A, which represents for each x-X solution a probability distribution over the action space A. For notational convenience, we will use (a-x) instead of the x-x solution (a) (a). The series of measures for X and A is denounced by each x-X solution. The process that describes the data Dn = {(Xi, Ai, Ri), Ri), the 1-solution i-X solution: (Xi, Ai, Ri) are independent copies of the (X, A, R), where X-solution can be found for each x solution."}, {"heading": "C Extension to Markov Decision Processes", "text": "In this section we will consider an extension of the time horizon, finite Markov decision-making processes (MDPs), which are reduced to the bandit problem examined in Section 2. Here, an MDP is represented by a tuple M = < X, A, P, \u03a6, \u03bd, H >, where X = {1,., N} is the set of states, A = {1,., K} is the set of actions, P is the transitional core, \u03a6: X \u00b7 A 7 \u2192 R is the reward function, \u03bd is the start-state distribution, and H is the horizon. A policy: X 7 \u2192 [0, 1] K maps the states to the distribution over actions, and we use the probability of choosing an action in state x. The amount of actions over X and A is indicated by the distribution (X, A)."}, {"heading": "D Connection to Semi-supervised Learning", "text": "The hope is that the large, unlabeled dataset will help to reduce the error of an estimator whose task it is to predict a value that depends on the unknown distribution that the data generates. Clearly, the non-political evaluation problem can be linked to semi-supervised learning: Given the data {(Ai, Ri)} i = 1,..., n generated from \u03c0D and \u03a6, the goal being to predict v\u03c0\u03a6. A large \"unlabeled\" dataset {Aj} j = 1,..., m with m n helps to identify a quantitative index. In fact, it is a fascinating idea to cleverly improve the prediction of v\u03c0D. The most obvious way to use it in the probability quotiator. However, as we have shown, the MSE of the probability quotient can be much larger than the estimator."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Qui\u00f1onero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Two faces of active learning", "author": ["Sanjoy Dasgupta"], "venue": "Theoretical Computer Science,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search", "author": ["V. Heidrich-Meisner", "C. Igel"], "venue": "In ICML,", "citeRegEx": "Heidrich.Meisner and Igel.,? \\Q2009\\E", "shortCiteRegEx": "Heidrich.Meisner and Igel.", "year": 2009}, {"title": "Efficient estimation of average treatment effects using the estimated propensity", "author": ["Keisuke Hirano", "Guido W. Imbens", "Geert Ridder"], "venue": "score. Econometrica,", "citeRegEx": "Hirano et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hirano et al\\.", "year": 2003}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Exploration scavenging", "author": ["John Langford", "Alexander L. Strehl", "Jennifer Wortman"], "venue": "In Proceedings of the Twenty-Fifth International Conference on Machine Learning,", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In Proceedings of the Fourth International Conference on Web Search and Web Data Mining (WSDM-11),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Probability and Computing: Randomized Algorithms and Probabilistic Analysis", "author": ["Michael Mitzenmacher", "Eli Upfal"], "venue": null, "citeRegEx": "Mitzenmacher and Upfal.,? \\Q2005\\E", "shortCiteRegEx": "Mitzenmacher and Upfal.", "year": 2005}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A.Y. Ng", "M. Jordan"], "venue": "In UAI,", "citeRegEx": "Ng and Jordan.,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2000}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Doina Precup", "Richard S. Sutton", "Satinder P. Singh"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Information-based complexity, feedback and dynamics in convex programming", "author": ["Maxim Raginsky", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Raginsky and Rakhlin.,? \\Q2011\\E", "shortCiteRegEx": "Raginsky and Rakhlin.", "year": 2011}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["P. Rosenbaum", "D. Rubin"], "venue": null, "citeRegEx": "Rosenbaum and Rubin.,? \\Q1983\\E", "shortCiteRegEx": "Rosenbaum and Rubin.", "year": 1983}, {"title": "Reducing bias in observational studies using subclassification on the propensity score", "author": ["P. Rosenbaum", "D. Rubin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rosenbaum and Rubin.,? \\Q1985\\E", "shortCiteRegEx": "Rosenbaum and Rubin.", "year": 1985}, {"title": "Input-dependent estimation of generalization error under covariate shift", "author": ["M. Sugiyama", "K. M\u00fcller"], "venue": "Statistics & Decisions,", "citeRegEx": "Sugiyama and M\u00fcller.,? \\Q2005\\E", "shortCiteRegEx": "Sugiyama and M\u00fcller.", "year": 2005}, {"title": "Analysis of kernel mean matching under covariate shift", "author": ["Yaoliang Yu", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Yu and Szepesv\u00e1ri.,? \\Q2012\\E", "shortCiteRegEx": "Yu and Szepesv\u00e1ri.", "year": 2012}, {"title": "Introduction to semi-supervised learning", "author": ["Xiaojin Zhu", "Andrew B. Goldberg"], "venue": null, "citeRegEx": "Zhu and Goldberg.,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Goldberg.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": ", 2010), which is sometimes referred to as offline evaluation in the bandit literature (Li et al., 2011) or counterfactual reasoning (Bottou et al.", "startOffset": 87, "endOffset": 104}, {"referenceID": 1, "context": ", 2011) or counterfactual reasoning (Bottou et al., 2013).", "startOffset": 36, "endOffset": 57}, {"referenceID": 9, "context": ", 2013) and can also be looked as a key building block for policy optimization which, as in supervised learning, can often be reduced to evaluation, as long as the complexity of the policy class is well-controlled (Ng and Jordan, 2000).", "startOffset": 214, "endOffset": 235}, {"referenceID": 2, "context": "In the context of supervised learning, in the covariate shift literature, the problem of estimating losses under changing distributions is crucial for model selection (Sugiyama and M\u00fcller, 2005, Yu and Szepesv\u00e1ri, 2012) and also appears in active learning (Dasgupta, 2011).", "startOffset": 256, "endOffset": 272}, {"referenceID": 4, "context": "Here, the focus is on the two-action (binary) case where the goal is to estimate the difference between the expected rewards of the two actions (Hirano et al., 2003), which is slightly (but not essentially) different than our setting.", "startOffset": 144, "endOffset": 165}, {"referenceID": 4, "context": "As opposed to the statistics literature (Hirano et al., 2003), we are interested in results for finite sample sizes.", "startOffset": 40, "endOffset": 61}, {"referenceID": 4, "context": "The motivation of studying this estimator is both its simplicity and also because it is known that a related estimator is asymptotically efficient (Hirano et al., 2003).", "startOffset": 147, "endOffset": 168}, {"referenceID": 11, "context": ", Raginsky and Rakhlin (2011)).", "startOffset": 2, "endOffset": 30}, {"referenceID": 11, "context": "Now setting p = 6, and applying Lemma 1, Theorem 1 and the \u201cInformation Radius bound\u201d from Raginsky and Rakhlin (2011), we have n \u2265 V1 4\u03b5 .", "startOffset": 91, "endOffset": 119}, {"referenceID": 1, "context": "One of the most popular estimators is known as the propensity score estimator in the statistical literature (Rosenbaum and Rubin, 1983, 1985), or the importance weighting estimator (Bottou et al., 2013).", "startOffset": 181, "endOffset": 202}, {"referenceID": 16, "context": "In this section, we consider extensions of our previous results to contextual bandits and Markovian Decision Processes, while implications to semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material.", "startOffset": 167, "endOffset": 191}], "year": 2014, "abstractText": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.", "creator": "Creator"}}}