{"id": "1409.3813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "Incorporating Semi-supervised Features into Discontinuous Easy-First Constituent Parsing", "abstract": "This paper describes adaptations for EaFi, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014.", "histories": [["v1", "Fri, 12 Sep 2014 18:37:35 GMT  (40kb,D)", "http://arxiv.org/abs/1409.3813v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yannick versley"], "accepted": false, "id": "1409.3813"}, "pdf": {"name": "1409.3813.pdf", "metadata": {"source": "CRF", "title": "Incorporating Semi-supervised Features into Discontinuous Easy-first Constituent Parsing", "authors": ["Yannick Versley"], "emails": ["versley@cl.uni-heidelberg.de"], "sections": [{"heading": "1 Introduction", "text": "The SPMRL Shared Task 2014 (Seddah et al., 2014) adds unlabeled data to the 2013 shared task - dependency and component trees for multiple languages, including discontinuous component trees for Swedish and German - to enable semi-supervised parsing approaches; the following sections explain (i) how multilingual customization using the shared information from dependency and constituency data (using the Universal POS Tagset Mapping (Petrov et al., 2012) in Section 2.1; (ii) the use of word clusters for semi-supervised parsing (Section 3.1; and (iii) the addition of a Bigram dependency language model to the parser (Section 3.2)."}, {"heading": "2 Easy-first parsing of discontinuous constituents", "text": "The EAFI parser uses the simple first parsing approach of Goldberg and Elhalad (2010) for discontinuous parsing of components. It starts with the sequence of terminals with word forms, lemmas, and part-of-speech tags, and gradually applies the parsing action that has been classified as the safest. Since the classifier uses only features from a small window around the action, the number of feature vectors that need to be calculated and evaluated is linear in the number of words, as opposed to approaches that perform parsing based on a dynamic programming approach. To learn the classifier for the next action, the training component of EAFI performs the parsing process until the first error (stop early, cf. Collins and Roark, the actual time consumption is close to the linear. To learn the classifier for the next action, the training component of EAFI performs the parsing process from punctuation to the first error (stop early and duel Collins)."}, {"heading": "2.1 Multilingual Adaptations", "text": "The aforementioned persons are able to position themselves at the head of the party in the same way as they have placed themselves at the head of the party."}, {"heading": "2.2 General tuning", "text": "EaFi uses online learning with a hash kernel to implement the learning of parameters - in particular, AdaGrad updates (Duchi et al., 2011) with forward-backward splitting (FOBOS) for L1 regularization (Duchi and Singer, 2009). Several parameters influence the performance of the parser: \u2022 The size of the weight vector - since a hash kernel is used, collisions of the hash function can have a negative impact on the available dimension on performance. We use a 400MB weight vector that still allows training on machines of modest size, but leaves room for more features than the 80MB weight vector used by Versley (2014). \u2022 The size of the regulation parameter. Since FOBOS does not change the weights between updates, smaller parameters seem to work better than larger ones. Goldberg (2013) puts a value of \u03bb = 0.05% D, due to Alexandre Passo, per week, the number of decisions is better."}, {"heading": "3 Integrating semi-supervised features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Using word clusters", "text": "For discriminatory parsers, such as the dependency parser from Koo et al. (2008), features that use interface shapes are supplemented by duplicate characteristics, where word forms (in whole or in part) are replaced by clusters. A discriminatory framework also allows the use of clusters and reduced clusters. Candito and Seddah (2010) have shown that word clusters can be productively integrated into a generative parser, such as the Berkeley parser, which uses a PCFG with latent Annotations (PCFG-LA). In their case, they add suffixes to the clusters to improve the parser's ability to assign the correct part of the speech days. Since EAFI uses discriminatory parsing of words, we have followed Cmmmmn by providing duplicates of features in which word form characteristics are replaced by features."}, {"heading": "3.2 Using a Dependency Bigram Language Model", "text": "For models with a generative component, self-training (as in McClosky et al., 2006) can offer tangible benefits. In fact, Suzuki et al. (2009) show that it is possible to achieve improvements in parsing dependencies that go beyond what is possible with word clusters when combined with a discriminatory model that uses word clusters with an interplay of generative models that are used as traits. 1Thanks to Djame \u0301 Seddah for providing Brown clusters for these languages. While the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift reduction parser for constitutions. Following Zhu et al., we add features to indicate that for the position pairs (0.1), they do not belong to certain (0.2) words (1.2)."}, {"heading": "4 Experiments", "text": "Among the tree banks used in the SPMRL joint task, German and Swedish have discontinuous components - in this case, German has a large number of them (about ten thousand in the five thousand sentences of the test set), while Swedish has very few (only fifty discontinuous sentences in the 600 sentences of the test set). Based on previous experiments, learning on the larger German data set lasted for 15 epochs, while training on the Swedish data set lasted for 30 epochs."}, {"heading": "5 Results and Discussion", "text": "Table 1 shows how adjustments to the purely monitored part of the EAFI affect the results based on the results for German gold tags. In particular, the data-driven header table and special POS tags have a slightly positive effect. An increase in the weight vector does not seem to have a strong effect, implying that the existing weight vector is sufficient for the set of characteristics used in the experiments. However, a different setting for the regulatory constant results in a fairly large difference (almost + 4%), suggesting that the previous setting was suboptimal. In Tables 2 and 3 we find the monitored initial results together with experiments regarding the use of clusters and their granularity and the use of characteristics based on the Bigram language model. For both Sweden and Germany, we see that the addition of cluster-based characteristics significantly improves the results, with an increase of + 1.3% in the case of German and slightly more than + 3.5% in Sweden for version + 2.6% and less than the predicted cluster-bit results for the 4bit."}, {"heading": "6 Conclusions", "text": "In this paper, we reported on customizations that have the dual goal of first, using the EAFI engine to parse multiple languages, using existing dependency conversions and tagset mappings to provide header rules and lists of closed-class tags; second, to improve these monitored learning outcomes by incorporating features based on data from large corporations without manual annotation, namely Brown clusters and a dependency bigram language model.Experimental results show that these two improvements are well suited to enhancing the capabilities of the parser, while showing that techniques known in dependency parsing can also be used to create parsers for discontinuous constituent structures that work better than existing parsers based on Treebank LCFRS grammars, making them a practical solution for parsing discontinuous structures such as extraposition and crambling."}], "references": [{"title": "Parsing word clusters", "author": ["Candito", "Marie-Helene", "Djam\u00e9 Seddah."], "venue": "Proceedings of the First Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2010).", "citeRegEx": "Candito et al\\.,? 2010", "shortCiteRegEx": "Candito et al\\.", "year": 2010}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Michael", "Brian Roark."], "venue": "ACL-04.", "citeRegEx": "Collins et al\\.,? 2004", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Efficient learning with forward-backward splitting", "author": ["Duchi", "John", "Yoram Singer."], "venue": "Proceedings of Neural Information Processing Systems (NIPS 2009).", "citeRegEx": "Duchi et al\\.,? 2009", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["Dunning", "Ted."], "venue": "Computational Linguistics 19(1):61\u201374.", "citeRegEx": "Dunning and Ted.,? 1993", "shortCiteRegEx": "Dunning and Ted.", "year": 1993}, {"title": "Dynamic-oracle transition-based parsing with calibrated probabilistic output", "author": ["Goldberg", "Yoav."], "venue": "Proceedings of IWPT 2013.", "citeRegEx": "Goldberg and Yoav.,? 2013", "shortCiteRegEx": "Goldberg and Yoav.", "year": 2013}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "author": ["Goldberg", "Yoav", "Michael Elhalad."], "venue": "Proceedings of NAACL-2010.", "citeRegEx": "Goldberg et al\\.,? 2010", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo", "Terry", "Xavier Carreras", "Michael Collins."], "venue": "ACL 2008.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Direct parsing of discontinuous constituents in German", "author": ["Maier", "Wolfgang."], "venue": "Proceedings of the NAACL-HLT First Workshop on Statistical Parsing of Morphologically Rich Languages.", "citeRegEx": "Maier and Wolfgang.,? 2010", "shortCiteRegEx": "Maier and Wolfgang.", "year": 2010}, {"title": "Reranking and self-training for parser adaptation", "author": ["McClosky", "David", "Eugene Charniak", "Mark Johnson."], "venue": "CoLing/ACL 2006.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "An improved oracle for dependency parsing with online reordering", "author": ["Nivre", "Joakim", "Marco Kuhlmann", "Johan Hall."], "venue": "Proceedings of the 11th International Conference on Parsing Technologies (IWPT).", "citeRegEx": "Nivre et al\\.,? 2009", "shortCiteRegEx": "Nivre et al\\.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Petrov", "Slav", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "An empirical study of semisupervised structured conditional models for dependency parsing", "author": ["Suzuki", "Jun", "Hideki Isozaki", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Suzuki et al\\.,? 2009", "shortCiteRegEx": "Suzuki et al\\.", "year": 2009}, {"title": "Efficient parsing with linear context-free rewriting systems", "author": ["van Cranenburgh", "Andreas"], "venue": "EACL", "citeRegEx": "Cranenburgh and Andreas.,? \\Q2012\\E", "shortCiteRegEx": "Cranenburgh and Andreas.", "year": 2012}, {"title": "Experiments with easy-first nonprojective constituent parsing", "author": ["Versley", "Yannick."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Language.", "citeRegEx": "Versley and Yannick.,? 2014", "shortCiteRegEx": "Versley and Yannick.", "year": 2014}, {"title": "Improving shift-reduce constituency parsing with large-scale unlabeled data", "author": ["Zhu", "Muhua", "Jingbo Zhu", "Huizhen Wang."], "venue": "Natural Language Engineering .", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "The following sections explain (i) how multilingual adaptation was performed using the joint information from dependency data and constituency data (with the help of the Universal POS tagset mapping (Petrov et al., 2012) where available), in section 2.", "startOffset": 199, "endOffset": 220}, {"referenceID": 2, "context": "The feature vectors of the erroneous action and of the highest-scoring action are used to perform a regularized AdaGrad update (Duchi et al., 2011).", "startOffset": 127, "endOffset": 147}, {"referenceID": 8, "context": "By using a swap action similar to the online reordering approach of Nivre et al. (2009), EAFI is able to perform nonprojective constituent parsing in sub-quadratic time, with an actual time consumption being close to linear.", "startOffset": 68, "endOffset": 88}, {"referenceID": 11, "context": "We use the Universal Tagset Mapping of Petrov et al. (2012) where it is available to make a three-way split between normal POS tags, closed-class POS tags, and punctuation.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "EaFi uses online learning with a hash kernel to realize the learning of parameters \u2013 in particular, AdaGrad updates (Duchi et al., 2011) with forward-backward splitting (FOBOS) for L1 regularization (Duchi and Singer, 2009).", "startOffset": 116, "endOffset": 136}, {"referenceID": 7, "context": "In discriminative parsers such as the dependency parser of Koo et al. (2008), features that use surface forms are complemented by duplicated features where the word forms are (wholly or in part) replaced by clusters.", "startOffset": 59, "endOffset": 77}, {"referenceID": 7, "context": "In discriminative parsers such as the dependency parser of Koo et al. (2008), features that use surface forms are complemented by duplicated features where the word forms are (wholly or in part) replaced by clusters. A discriminative framework also allows to use both clusters and reduced clusters. Candito and Seddah (2010) have shown that word clusters can productively be incorporated into a generative parser such as the Berkeley Parser, which uses a PCFG with latent annotations (PCFG-LA).", "startOffset": 59, "endOffset": 325}, {"referenceID": 9, "context": "2 Using a Dependency Bigram Language Model For models with a generative component, self-training (as in McClosky et al., 2006) can provide tangible benefits. Indeed, Suzuki et al. (2009) show that it is possible to reach improvements in dependency parsing beyond what is possible with word clustering when combining a discriminative model that uses word clusters with an ensemble of generative models that are used as features.", "startOffset": 104, "endOffset": 187}, {"referenceID": 12, "context": "While the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift-reduce parser for constituents.", "startOffset": 22, "endOffset": 105}, {"referenceID": 12, "context": "While the approach of Suzuki et al. works with a dynamic programming model of parsing, Zhu et al. (2013) show that it is also possible to use lexical dependency statistics learned from a large corpus to improve a state-of-the-art shift-reduce parser for constituents. Following Zhu et al., we add features to indicate, for the position pairs (0,1), (1,2), (0,2), whether they belong to the top-10% quantile of non-zero values for one particular head word (HI), to the top-30% (MI), have a non-zero value (LO), or a zero value (NO). The association scores are either determined on the raw counts (Raw), on proportions normalized on the head word (L1) or scored using theG likelihood ratio of Dunning (1993). The bigram association strength feature is taken both by itself and paired with the POS tags of the words in question.", "startOffset": 22, "endOffset": 706}], "year": 2014, "abstractText": "This paper describes adaptations for EAFI, a parser for easy-first parsing of discontinuous constituents, to adapt it to multiple languages as well as make use of the unlabeled data that was provided as part of the SPMRL shared task 2014.", "creator": "LaTeX with hyperref package"}}}