{"id": "1409.3924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2014", "title": "A study on effectiveness of extreme learning machine", "abstract": "Extreme learning machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM.", "histories": [["v1", "Sat, 13 Sep 2014 07:47:37 GMT  (32kb)", "http://arxiv.org/abs/1409.3924v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yuguang wang", "feilong cao", "yubo yuan"], "accepted": false, "id": "1409.3924"}, "pdf": {"name": "1409.3924.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuguang Wang Feilong Cao", "Yubo Yuan"], "emails": ["feilongcao@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.39 24v1 [cs.NE] 1 3Se pExtreme Learning Machine (ELM), proposed by Huang et al., has shown that a promising learning algorithm has been developed for single-layer hidden neural networks (SLFNs). However, due to the random selection of input weights and distortions, the ELM algorithm sometimes results in the hidden output matrix H of the SLFN not taking full column rank, which reduces the effectiveness of the ELM. This paper discusses the effectiveness of the ELM and proposes an improved algorithm called EELM, which correctly selects the input weights and distortions before calculating the initial weights, thereby ensuring the full column rank H in theory, improving the learning rate (test accuracy, prediction accuracy, learning time) and the robustness characteristics of the networks based on both the classification and the benchmarking of the ELM, as well as the results of the use of the good networks."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live."}, {"heading": "2 Linear inverse problems and Regularization Model of", "text": "(1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (2). (2). (2). (2). (2). (2). (2). (2). (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2).). (2). (2). (2).). (2). (2).). (2). (2). (2).). (2).). (2).). (2).). (2).). (2).). (2).). (2).). (2). (2).). (2). (2).). (2).). (2).).). (2). (2).). (2.).). (2.).). (2.).). (2).).). (2).). (2.). (2.).). (2.). (2.).). (2.).). (2.).).).)"}, {"heading": "3 Extreme learning machine using iterative method", "text": "Based on Theorem 2.2 and Theorem 2.3, this section proposes a more effective method for the formation of SLFNs."}, {"heading": "3.1 Features of extreme learning machine (ELM) algorithm", "text": "The ELM algorithm proposed by Huang et al. can be summarized as follows: ELM algorithm: Given a training set N = {(Xi, ti) | \u0192R d, ti R, i = 1, 2,..., n} and activation function g, hidden node number n0.Step 1: Randomly assign the input weight Wi and bias bi (i = 1, 2,., n0) Step 2: Calculate the hidden output matrix H. Step 3: Calculate the output weight \u03b2 = H \u2020 T, here H \u2020 is the Moore-Penrose generalized inversion of H (see [21] and [25] for more details) and T = (t1, t2,..) T. The ELM algorithm proves in practice to be an extremely fast algorithm \u03b2 = H \u2020 T. This is because it preselects the input weights Wi and the distortions bi the SLFs Ns."}, {"heading": "3.2 Improvement for the effectiveness of extreme learning machine", "text": "After Theorem 2.2 and Theorem 2.3 you can summarize the new extreme learning machine for SLFNs as follows: (max.). We call the new algorithm an effective extreme learning machine. (max.). (max.). (max.). (max.). (max.). (max.). (.). (.). (.). (.). (.). (.). (max.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.).). (.). (.).). (.).). (.).). (.).).). (.).). (.).). (.).). (.).). (.).). (.). (.).). (.).).). (.).). (.).). (.). (.).). (.).). (.). (.).).). (.).). (.).).). (.).). (.).).).). (.).). (.).).). (.).).). (.).).).).).)....).)....)..).....)..)..."}, {"heading": "4 Complexity and performance", "text": "Compared to the second phase of the algorithm, which calculates the initial weights, it can be considered constant. In the rest of this section, the performance of the proposed EELM learning algorithm is measured compared to the ELM algorithm. Simulations for ELM and EELM algorithms are carried out in the Matlab 7.0 environment, which runs at a speed of 1.30 GHz on the Intel Celeron 743 CPU and on the Intel Core 2 Duo CPU. Activation function used in both algorithms is the Gaussian radial base function g (x) = e \u2212 x 2."}, {"heading": "4.1 Benchmarking with a regression problem: approximation of \u2018SinC\u2019", "text": "The target function is as follows: y = f (x) = sin (x) / x x 6 = 0, 1 x = 0, A training set (Xi, ti) and test set (Xi, ti) with 200 samples each are generated, Xi being distributed in the training data in [\u2212 10, 10] with uniform stride length. Xi in the test data is randomly selected in the standard normalized distribution in [\u2212 30, 30]. The reason why the range (\u2212 30, 30] of the test data is longer than that of the training data is that an obvious method for assessing the quality of the learned model is to see how long the predictions given in the model are accurate. The experiment is performed on the basis of this data as follows: There are 200 hidden nodes associated with both the ELM algorithm and the EELM algorithm. 50 attempts have been made for the ELM algorithm."}, {"heading": "4.2 Benchmarking with real-world applications", "text": "In this section, we compare the performance of the proposed ELM algorithms and the ELM algorithms for 5 real problems: 3 classification tasks including diabetes, glass identification (Glass ID), statlog (Landsat Satellite) and 2 regression tasks including housing and slump (Concrete Slump). All data sets come from the UCI repository of machine learning databases [2]. The speculation of each database is presented in Table 2. For databases that have only one data table, as in [9, 22, 23, 26], 75% and 25% of the samples of the problem are randomly selected for training or testing at each triality test. Fifty studies have been conducted for the two algorithms and the results are reported in Table 3, Table 4 and Table 5, which show that ELM can generally achieve a higher test rate for medium and large classification problems than ELM in our simulation."}, {"heading": "5 Discussions and conclusions", "text": "This paper proposed a simple and effective algorithm for the single-layer feeding of neural networks (SLFNs), called the effective extreme learning machine (EELM), to overcome the inadequacies of the extreme learning machine (ELM). However, there are several interesting features of the proposed EELM algorithm compared to the ELM algorithm: (1) The learning speed of EELM is generally faster than ELM. The main difference between EELM and ELM algorithms lies in the selection of the input weights and distortions. The ELM algorithm selects them randomly, which consumes little time. Our EELM algorithm selects the input weights and properly distorts them, which also compares the training time of the output weights. (2) The proposed EELM algorithm is conditioned by the correct selection of the input weights and the biase weights of the neural networks."}], "references": [{"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Trans. Information Theory 44 (2) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "UCI repository of machine learning databases", "author": ["C. Blake", "C. Merz"], "venue": "in: http://www.ics.uci.edu/ \u0303mlearn/MLRepository.html, Department of Information and Computer Sciences, University of California, Irvine, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "The estimate for approximation error of neural networks: a constructive approach", "author": ["F.L. Cao", "T.F. Xie", "Z.B. Xu"], "venue": "Neurocomputing 71 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation problems in system identification with neural networks", "author": ["T.P. Chen"], "venue": "Science in China (series A) 37 (4) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Approximation capability to functions of several variables", "author": ["T.P. Chen", "H. Chen"], "venue": "nonlinear functionals, and operators by radial basis function neural networks, IEEE Trans. Neural Networks, 6 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems", "author": ["T.P. Chen", "H. Chen"], "venue": "IEEE Trans. Neural Networks 6 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Approximation by superpositions of sigmoidal function", "author": ["G. Cybenko"], "venue": "Math. Control Signals Syst. 2 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Q", "author": ["G. Feng", "G.B. Huang"], "venue": "Lin R. Gay, Error minimized extreme learning machine with growth of hidden nodes and incremental learning, IEEE Trans. Neural Networks 20 (8) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "in: International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K.I. Funahashi"], "venue": "Neural Networks 2 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural Networks 4 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning capability and storage capacity of two-hidden-layer feedforward networks", "author": ["G.B. Huang"], "venue": "IEEE Trans. Neural Networks 14 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions", "author": ["G.B. Huang", "H.A. Babri"], "venue": "IEEE Trans. Neural Networks 9 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Convex incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 70 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced random search based incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 71 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "IEEE Trans. Neural Networks 17 (4) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimization method based extreme learning machine for classification", "author": ["G.B. Huang", "X. Ding", "H. Zhou"], "venue": "Neurocomputing, In Press, Accepted Manuscript", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximat any function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks 6 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Fully complex extreme learning machine", "author": ["M.B. Li", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neurocomputing 68 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalized Inverse of Matrices and its Applications", "author": ["C.R. Rao", "S.K. Mitra"], "venue": "Wiley, New York", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1972}, {"title": "An improvement of AdaBoost to avoid overfitting", "author": ["G. R\u00e4tsch", "T. Onoda", "K.R. M\u00fcller"], "venue": "in: Proceedings of the Fifth International Conference on Neural Information Processing ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "A new incremental method for function approximation using feed-forward neural networks", "author": ["E. Romero", "R. Alqu\u00e9zar"], "venue": "in: Proceedings of INNS-IEEE International Joint Conference on Neural Networks ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Online sequential fuzzy extreme learning machine for function approximation and classification problems", "author": ["H.J. Rong", "G.B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE Trans. Systems, Man, and Cybernetics-Part B: Cybernetics 39 (4) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrices: Theory and Applications", "author": ["D. Serre"], "venue": "Springer, New York", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Heterogeneous radial basis function networks", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "in: Proceedings of the International Conference on Neural Networks (ICNN", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Multicategory classification using an extreme learning machine for microarray gene expression cancer diagnosis", "author": ["R. Zhang", "G.B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE/ACM Trans. Computational Biology and Bioinformatics 4 (3) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolutionary extreme learning machine", "author": ["Q.Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.B. Huang"], "venue": "Pattern Recognition 38 (10) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "We refer the reader to [12], [14]-[18], [20], [24], [27] and [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "In 1989, Cybenko [7] and Funahashi [10] proved that any continuous functions can be approximated on a compact set with uniform topology by an SLFN with any continuous, sigmoidal activation function, which made a breakthrough in the artificial neural network field.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "In 1989, Cybenko [7] and Funahashi [10] proved that any continuous functions can be approximated on a compact set with uniform topology by an SLFN with any continuous, sigmoidal activation function, which made a breakthrough in the artificial neural network field.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "Leshno [19] improved the results of Hornik [11] and proved that any continuous functions could be approximated by feedforward networks with a nonpolynomial activation function.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Leshno [19] improved the results of Hornik [11] and proved that any continuous functions could be approximated by feedforward networks with a nonpolynomial activation function.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].", "startOffset": 102, "endOffset": 111}, {"referenceID": 4, "context": "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].", "startOffset": 102, "endOffset": 111}, {"referenceID": 5, "context": "Furthermore, some deep and systematic studies on the condition of activation function can be found in [4, 5, 6].", "startOffset": 102, "endOffset": 111}, {"referenceID": 2, "context": "[3] constructively gave the estimation of upper bounds of approximation for continuous functions by SLFNs with the bounded, strictly monotone and odd activation function, which means that the neural networks can be constructed without training as long as the samples are given.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In practical applications, for function approximation in a finite training set, Huang and Babri [13] showed that an SLFN with at most N hidden nodes and with almost any nonlinear activation function can exactly learn N distinct samples.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "Bartlett [1] pointed out that for feedforward neural networks the smaller the norm of weights and training error are, the better generalization performance the networks tend to have.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "[13, 12], H is called the hidden layer output matrix of the neural networks.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[13, 12], H is called the hidden layer output matrix of the neural networks.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "Step 3: Calculate the output weight \u03b2 by \u03b2 = HT , here H is the MoorePenrose generalized inverse of H (see [21] and [25] for further details) and T = (t1, t2, .", "startOffset": 107, "endOffset": 111}, {"referenceID": 24, "context": "Step 3: Calculate the output weight \u03b2 by \u03b2 = HT , here H is the MoorePenrose generalized inverse of H (see [21] and [25] for further details) and T = (t1, t2, .", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "All the data sets are from UCI repository of machine learning databases [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.", "startOffset": 65, "endOffset": 80}, {"referenceID": 22, "context": "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.", "startOffset": 65, "endOffset": 80}, {"referenceID": 25, "context": "For the databases that have only one data table, as conducted in [9, 22, 23, 26], 75% and 25% of samples in the problem are randomly chosen for training and testing respectively at each trial.", "startOffset": 65, "endOffset": 80}], "year": 2014, "abstractText": "Extreme Learning Machine (ELM), proposed by Huang et al., has been shown a promising learning algorithm for single-hidden layer feedforward neural networks (SLFNs). Nevertheless, because of the random choice of input weights and biases, the ELM algorithm sometimes makes the hidden layer output matrix H of SLFN not full column rank, which lowers the effectiveness of ELM. This paper discusses the effectiveness of ELM and proposes an improved algorithm called EELM that makes a proper selection of the input weights and bias before calculating the output weights, which ensures the full column rank of H in theory. This improves to some extend the learning rate (testing accuracy, prediction accuracy, learning time) and the robustness property of the networks. The experimental results based on both the benchmark function approximation and real-world problems including classification and regression applications show the good performances of EELM.", "creator": "LaTeX with hyperref package"}}}